<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000500">
<title confidence="0.849758">
Perceptually grounded selectional preferences
</title>
<author confidence="0.615884">
Ekaterina Shutova Niket Tandon Gerard de Melo
</author>
<affiliation confidence="0.7094155">
Computer Laboratory Max Planck Institute IIIS
University of Cambridge, UK for Informatics, Germany Tsinghua University, China
</affiliation>
<email confidence="0.992686">
es407@cam.ac.uk ntandon@mpi-inf.mpg.de gdm@demelo.org
</email>
<sectionHeader confidence="0.994667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999893076923077">
Selectional preferences (SPs) are widely
used in NLP as a rich source of semantic
information. While SPs have been tradi-
tionally induced from textual data, human
lexical acquisition is known to rely on both
linguistic and perceptual experience. We
present the first SP learning method that si-
multaneously draws knowledge from text,
images and videos, using image and video
descriptions to obtain visual features. Our
results show that it outperforms linguistic
and visual models in isolation, as well as
the existing SP induction approaches.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932881355932">
Selectional preferences (SPs) are the semantic con-
straints that a predicate places onto its arguments.
This means that certain classes of entities are more
likely to fill the predicate’s argument slot than oth-
ers. For instance, while the sentences “The au-
thors wrote a new paper.” and “The cat is eating
your sausage!” sound natural and describe plausi-
ble real-life situations, the sentences “The carrot
ate the keys.” and “The law sang a driveway.” ap-
pear implausible and difficult to interpret, as the
arguments do not satisfy the verbs’ common pref-
erences. SPs provide generalisations about word
meaning and use and find a wide range of appli-
cations in natural language processing (NLP), in-
cluding word sense disambiguation (Resnik, 1997;
McCarthy and Carroll, 2003; Wagner et al., 2009),
resolving ambiguous syntactic attachments (Hindle
and Rooth, 1993), semantic role labelling (Gildea
and Jurafsky, 2002; Zapirain et al., 2010), natural
language inference (Zanzotto et al., 2006; Pantel
et al., 2007), and figurative language processing
(Fass, 1991; Mason, 2004; Shutova et al., 2013; Li
et al., 2013). Automatic acquisition of SPs from
linguistic data has thus become an active area of
research. The community has investigated a range
of techniques to tackle data sparsity and to per-
form generalisation from observed arguments to
their underlying types, including the use of Word-
Net synsets as SP classes (Resnik, 1993; Li and
Abe, 1998; Clark and Weir, 1999; Abney and Light,
1999; Ciaramita and Johnson, 2000), word cluster-
ing (Rooth et al., 1999; Bergsma et al., 2008; Sun
and Korhonen, 2009), distributional similarity met-
rics (Erk, 2007; Peirsman and Pad´o, 2010), latent
variable models ( O´ S´eaghdha, 2010; Ritter et al.,
2010), and neural networks (Van de Cruys, 2014).
Little research, however, has been concerned
with the sources of knowledge that underlie the
learning of SPs. There is ample evidence in cogni-
tive and neurolinguistics that our concept learning
and semantic representation are grounded in per-
ception and action (Barsalou, 1999; Glenberg and
Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and
Damasio, 2008). This suggests that word mean-
ing and relational knowledge are acquired not only
from linguistic input but also from our experiences
in the physical world. Multi-modal models of word
meaning have thus enjoyed a growing interest in se-
mantics (Bruni et al., 2014), outperforming purely
text-based models in tasks such as similarity es-
timation (Bruni et al., 2014; Kiela et al., 2014),
predicting compositionality (Roller and Schulte
im Walde, 2013), and concept categorization (Sil-
berer and Lapata, 2014). However, to date these
approaches relied on low-level image features such
as color histograms or SIFT keypoints to repre-
sent the meaning of isolated words. To the best
of our knowledge, there has not yet been a multi-
modal semantic approach performing extraction of
</bodyText>
<page confidence="0.957026">
950
</page>
<note confidence="0.977367">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 950–960,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999853666666667">
predicate-argument relations from visual data. In
this paper, we propose the first SP model integrat-
ing information about predicate-argument interac-
tions from text, images, and videos. We expect
it to outperform purely text-based models of SPs,
which suffer from two problems: topic bias and
figurative uses of words. Such bias stems from the
fact that we typically write about abstract topics
and events, resulting in high coverage of abstract
senses of words and comparatively lower coverage
of the original physical senses (Shutova, 2011). For
instance, the verb cut is used predominantly in the
domains of economics and finance and its most fre-
quent direct objects are cost and price, according
to the British National Corpus (BNC) (Burnard,
2007). Predicate-argument distributions acquired
from text thus tend to be skewed in favour of ab-
stract domains and figurative uses, inadequately
reflecting our daily experiences with cutting, which
guide human acquisition of meaning. Integrating
predicate-argument relations observed in the physi-
cal world (in the form of image and video descrip-
tions) with the more abstract text-based relations
is likely to yield a more realistic semantic model,
with real prospects of improving the performance
of NLP applications that rely on SPs.
We use the BNC as an approximation of linguis-
tic knowledge and a large collection of tagged im-
ages and videos from Flickr (www.flickr.com)
as an approximation of perceptual knowledge. The
human-annotated labels that accompany media on
Flickr enable us to acquire predicate-argument co-
occurrence information. Our experiments focus on
verb preferences for their subjects and direct ob-
jects. In summary, our method (1) performs word
sense disambiguation and part-of-speech (PoS) tag-
ging of Flickr tag sequences to extract verb-noun
co-occurrence; (2) clusters nouns to induce SP
classes using linguistic and visual features; (3)
quantifies the strength of preference of a verb for
a given class by interpolating linguistic and visual
SP distributions. We investigate the impact of per-
ceptual information at different levels – from none
(purely text-based model) to 100% (purely visual
model). We evaluate our model directly against a
dataset of human plausibility judgements of verb-
noun pairs, as well as in the context of a semantic
task: metaphor interpretation. Our results show
that the interpolated model combining linguistic
and visual relations outperforms the purely linguis-
tic model in both evaluation settings.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.999764">
2.1 Selectional preference induction
</subsectionHeader>
<bodyText confidence="0.998982551020408">
The widespread interest in automatic acquisition of
SPs was triggered by the work of Resnik (1993),
who treated SPs as probability distributions over all
potential arguments of a predicate, rather than a sin-
gle argument class assigned to the predicate. The
original study used WordNet to define SP classes
and to map the words in the corpus to those classes.
Since then, the field has moved toward automatic
induction of SP classes from corpus data. Rooth et
al. (1999) presented a probabilistic latent variable
model of verb preferences. In their approach, verb-
argument pairs are generated from a latent variable,
which represents a cluster of verb-argument inter-
actions. The latent variable distribution and the
probabilities that a latent variable generates the
verb and the argument are learned from the data
using Expectation Maximization (EM). The latent
variables enable the model to recognise previously
unseen verb-argument pairs. O´ S´eaghdha (2010)
and Ritter et al. (2010) similarly model SPs within a
latent variable framework, but use Latent Dirichlet
Allocation (LDA) to learn the probability distri-
butions, for single-argument and multi-argument
preferences respectively.
Pad´o et al. (2007) and Erk (2007) used simi-
larity metrics to approximate selectional prefer-
ence classes. Their underlying hypothesis is that
a predicate-argument combination (p, a) is felici-
tous if the predicate p is frequently observed in the
data with the arguments a&apos; similar to a. The sys-
tems compute similarities between distributional
representations of arguments in a vector space.
Bergsma et al. (2008) trained an SVM classifier
to discriminate between felicitous and infelicitous
verb-argument pairs. Their training data consisted
of observed verb-argument pairs (positive exam-
ples) with unobserved, randomly-generated ones
(negative examples). They classified nominal ar-
guments of verbs, using their verb co-occurrence
probabilities and information about their semantic
classes as features. Bergsma and Goebel (2011) ex-
tended this method by incorporating image-driven
noun features. They extract color and SIFT key-
point features from images found for a particular
noun via Google image searches and add them to
the feature vectors to classify nouns as felicitous
or infelicitous arguments of a given verb. This
method is the closest in spirit to ours and the only
one so far to investigate the relevance of visual fea-
</bodyText>
<page confidence="0.997046">
951
</page>
<bodyText confidence="0.999969933333333">
tures to lexical preference learning. However, our
work casts the problem in a different framework:
rather than relying on low-level visual properties of
nouns in isolation, we explicitly model interactions
of predicates and arguments within an image or a
video frame.
Van de Cruys (2014) recently presented a deep
learning approach to SP acquisition. He trained
a neural network to discriminate between felic-
itous and infelicitous arguments using the data
constructed of positive (observed) and negative
(randomly-generated) examples for training. The
network weights were optimized by requiring the
model to assign a higher score to an observed pair
than to the unobserved one by a given margin.
</bodyText>
<subsectionHeader confidence="0.974587">
2.2 Multi-modal methods in semantics
</subsectionHeader>
<bodyText confidence="0.999952444444445">
Previous work has used multimodal data to de-
termine distributional similarity or to learn multi-
modal embeddings that project multiple modalities
into the same vector space. Some studies rely on
extensions of LDA to obtain correlations between
words and visual features (Feng and Lapata, 2010;
Roller and Schulte im Walde, 2013). Bruni et al.
(2012) integrated visual features into distributional
similarity models using simple vector concatena-
tion. Instead of generic visual features, Silberer et
al. (2013) relied on supervised learning to train 412
higher-level visual attribute classifiers.
Applications of multimodal embeddings include
zero-shot object detection, i.e. recognizing objects
in images without training data for the object class
(Socher et al., 2013; Frome et al., 2013; Lazaridou
et al., 2014), and automatic generation of image
captions (Kulkarni et al., 2013), video descriptions
(Rohrbach et al., 2013), or tags (Srivastava et al.,
2014). Other applications of multimodal data in-
clude language modeling (Kiros et al., 2014) and
knowledge mining from images (Chen et al., 2013;
Divvala et al., 2014). Young et al. (2014) apply sim-
plification rules to image captions, showing that the
resulting hierarchy of mappings between natural
language expressions and images can be used for
entailment tasks.
</bodyText>
<sectionHeader confidence="0.992717" genericHeader="method">
3 Experimental data
</sectionHeader>
<bodyText confidence="0.999877761904762">
Textual data. We extract linguistic features for
our model from the BNC. In particular, we parse
the corpus using the RASP parser (Briscoe et al.,
2006) and extract subject–verb and verb–object re-
lations from its dependency output. These relations
are then used as features for clustering to obtain SP
classes, as well as to quantify the strength of asso-
ciation between a particular verb and a particular
argument class.
Visual data. For the visual features of our model,
we mine the Yahoo! Webscope Flickr-100M dataset
(Shamma, 2014). Flickr-100M contains 99.3 mil-
lion images and 0.7 million videos with language
tags annotated by users, enabling us to generalise
SPs at a large scale. The tags reflect how humans
describe objects and actions from a visual perspec-
tive. We first stem the tags and remove words that
are absent in WordNet (typically named entities
and misspellings), then identify their PoS based
on their visual context and extract verb–noun co-
occurrences.
</bodyText>
<sectionHeader confidence="0.8612155" genericHeader="method">
4 Identifying visual verb-noun
co-occurrence
</sectionHeader>
<bodyText confidence="0.999955060606061">
In the Flickr-100M dataset, tags are assigned to im-
ages and videos in the form of sets of words, rather
than grammatically coherent sentences. However,
the roles that individual words play are still dis-
cernible from their visual context, as manifested by
the other words in a given set. In order to identify
verbs and nouns co-occurring in the same images,
we propose a list sense disambiguation method that
first maps each word to a set of possible WordNet
senses (accompanied by PoS information) and then
performs a joint optimization on the space of candi-
date word senses, such that their overall similarity
is maximized. This amounts to assigning those
senses and PoS tags to the words in the set that best
fit together.
For a given word i and one of its candidate Word-
Net senses j, we consider an assignment variable
xij and compute a sense frequency-based prior for
it as Pij = 1
1+R, where R is the WordNet rank
of the sense. We then compute a similarity score
Sij,i,j, between all pairs of sense choices for two
words i,i&apos; and their respective candidate senses j,j&apos;.
For these, we rely on WordNet’s taxonomic path-
based similarities (Pedersen et al., 2004) in the case
of noun-noun sense pairs, the Adapted Lesk sim-
ilarity measure for adjective-adjective pairs, and
finally, WordNet verb-groups and VerbNet class
membership (Kipper-Schuler, 2005) for verb-verb
pairs. Note that even parts of speech that are dis-
regarded later on can still be helpful at this stage,
as we aim at a joint optimization over all words.
After the similarities have been obtained for all rel-
</bodyText>
<page confidence="0.989112">
952
</page>
<bodyText confidence="0.9763795">
S into a stochastic matrix P containing transition
probabilities between the vertices in the graph as
</bodyText>
<equation confidence="0.999057">
P = D−1S, (2)
</equation>
<bodyText confidence="0.954275">
evant sense pairs, we maximize the coherence of
the senses of the words in the set as an Integer Lin-
ear Program, using the Gurobi Optimizer (Gurobi
Optimization, 2014) and solving
maximize
</bodyText>
<equation confidence="0.99566">
P Pijxij + P P Sij,i0j0Bij,i0j0
i ij i0j0
subject to
P
j xij ≤ 1 ∀i, xij ∈ {0,1} ∀i,j,
Bij,i0j0 ≤ xij, Bij,i0j0 ≤ xi0j0,
Bij,i0j0 ∈ {0, 1} ∀i, j, iy.
</equation>
<bodyText confidence="0.9996835">
The binary variables Bij,i0j0 are 1 iff xij = 1 and
xi0j0 = 1, indicating that both senses were simulta-
neously chosen. The optimizer disambiguates the
input words by selecting sense tuples x1j, x2j, ...,
from which we can directly obtain the correspond-
ing PoS information. Verb-noun co-occurrence
information is then extracted from the PoS-tagged
sets.
</bodyText>
<sectionHeader confidence="0.974053" genericHeader="method">
5 Selectional preference model
</sectionHeader>
<subsectionHeader confidence="0.999018">
5.1 Acquisition of argument classes
</subsectionHeader>
<bodyText confidence="0.999982928571429">
To address the issue of data sparsity, we generalise
selectional preferences over argument classes, as
opposed to individual arguments. We obtain SP
classes by means of spectral clustering of nouns
with lexico-syntactic features, which has been
shown effective in previous lexical classification
tasks (Brew and Schulte im Walde, 2002; Sun and
Korhonen, 2009).
Spectral clustering partitions the data, relying on
a similarity matrix that records similarities between
all pairs of data points. We use Jensen-Shannon
divergence to measure the similarity between fea-
ture vectors for two nouns, wi and wj, defined as
follows:
</bodyText>
<equation confidence="0.987414">
1 1
diS(wi, wj) = 2dKL(wi||m) + 2dKL(wj||m),
(1)
</equation>
<bodyText confidence="0.998470857142857">
where dKL is the Kullback-Leibler divergence, and
m is the average of wi and wj. We construct the
similarity matrix S computing similarities Sij as
Sij = exp(−diS(wi, wj)). The matrix S then en-
codes a similarity graph G (over our nouns), where
Sij are the adjacency weights. The clustering prob-
lem can then be defined as identifying the optimal
partition, or cut, of the graph into clusters, such
that the intra-cluster weights are high and the inter-
cluster weights are low. We use the multiway nor-
malized cut (MNCut) algorithm of Meila and Shi
(2001) for this purpose. The algorithm transforms
where the degree matrix D is a diagonal matrix
with Dii = PN j=1 Sij. It then computes the K
leading eigenvectors of P, where K is the desired
number of clusters. The graph is partitioned by
finding approximately equal elements in the eigen-
vectors using a simpler clustering algorithm, such
as k-means. Meila and Shi (2001) have shown that
the partition I derived in this way minimizes the
MNCut criterion:
</bodyText>
<equation confidence="0.9995355">
MNCut(I) = XK (1 − P(Ik → Ik|Ik)), (3)
k=1
</equation>
<bodyText confidence="0.999620529411765">
which is the sum of transition probabilities across
different clusters. Since k-means starts from a ran-
dom cluster assignment, we run the algorithm mul-
tiple times and select the partition that minimizes
the cluster distortion, i.e. distances to cluster cen-
troid.
We cluster nouns using linguistic and visual fea-
tures in two independent experiments.
Clustering with linguistic features: We first clus-
ter the 2,000 most frequent nouns in the BNC, us-
ing their grammatical relations as features. The
features consist of verb lemmas appearing in the
subject, direct object and indirect object relations
with the given nouns in the RASP-parsed BNC,
indexed by relation type. The feature vectors are
first constructed from the corpus counts, and sub-
sequently normalized by the sum of the feature
values.
Clustering with visual features: We also clus-
ter the 2,000 most frequent nouns in the Flickr
data. Since our goal is to create argument classes
for verb preferences, we extract co-occurrence fea-
tures that map to verb-noun relations from PoS-
disambiguated image tags. We use the verb lem-
mas co-occurring with the noun in the same images
and videos as features for clustering. The feature
values are again normalised by their sum.
SP classes: Example clusters produced using lin-
guistic and visual features are shown in Figures 1
and 2. Our cluster analysis reveals that the image-
derived clusters tend to capture scene-like relations
(e.g. beach and ocean; guitar and concert), as
opposed to types of entities, yielded by the lin-
guistic features and better suited to generalise over
</bodyText>
<page confidence="0.953028">
953
</page>
<bodyText confidence="0.984338857142857">
as
desire hostility anxiety passion doubt fear curiosity enthusi-
asm impulse instinct emotion feeling suspicion
official officer inspector journalist detective constable police
policeman reporter
book statement account draft guide advertisement document
report article letter
</bodyText>
<figureCaption confidence="0.960158">
Figure 1: Clusters obtained using linguistic fea-
tures
</figureCaption>
<table confidence="0.3509">
pilot aircraft plane airline landing flight wing arrival departure
airport
concert festival music guitar alternative band instrument audi-
ence event performance rock benjamin
cost benefit crisis debt credit customer consumer
</table>
<figureCaption confidence="0.99426">
Figure 2: Clusters obtained using visual features
</figureCaption>
<bodyText confidence="0.99867">
predicate-argument structure. In addition, the im-
age features tend to be sparse for abstract concepts,
reducing both the quality and the coverage of ab-
stract clusters. We thus use the noun clusters de-
rived with linguistic features as an approximation
of SP classes.
</bodyText>
<subsectionHeader confidence="0.999855">
5.2 Quantifying selectional preferences
</subsectionHeader>
<bodyText confidence="0.9978797">
Once the SP classes have been obtained, we need
to quantify the strength of association of a given
verb with each of the classes. We adopt an informa-
tion theoretic measure proposed by Resnik (1993)
for this purpose. Resnik first measures selectional
preference strength (SPS) of a verb in terms of
Kullback-Leibler divergence between the distribu-
tion of noun classes occurring as arguments of this
verb, p(c|v), and the prior distribution of the noun
classes, p(c).
</bodyText>
<equation confidence="0.954065">
SPSR(v) = �p(c |v) log p(c|p(c)), (4)
c
</equation>
<bodyText confidence="0.999895833333333">
where R is the grammatical relation for which SPs
are computed. SPS measures how strongly the
predicate constrains its arguments. Selectional as-
sociation of the verb with a particular argument
class is then defined as a relative contribution of
that argument class to the overall SPS of the verb.
</bodyText>
<equation confidence="0.99665">
AssR(v, c) = SPSR(v)p(c|v) log p(c|v)
1
p(c) (5)
</equation>
<bodyText confidence="0.999955066666667">
We use this measure to quantify verb SPs based
on linguistic and visual co-occurrence information.
We first extract verb-subject and verb-direct object
relations from the RASP-parsed BNC, map the ar-
gument heads to SP classes and quantify selectional
association of a given verb with each SP class, thus
acquiring its base preferences. Since visual verb-
noun co-occurrences do not contain information
about grammatical relations, we rely on linguistic
data to provide a set of base arguments of the verb
for a given grammatical relation. We then interpo-
late the verb-argument probabilities from linguistic
and visual models for the base arguments of the
verb, thus preserving information about grammati-
cal relations.
</bodyText>
<subsectionHeader confidence="0.988767">
5.3 Linguistic and visual model interpolation
</subsectionHeader>
<bodyText confidence="0.999392583333333">
We investigate two model interpolation techniques:
simple linear interpolation and predicate-driven lin-
ear interpolation.
Linear interpolation combines information from
component models by computing a weighted aver-
age of their probabilities. The interpolated probabil-
ity of an event e is derived as pLI(e) = Ei λipi(e),
where pi(e) is the probability of e in the model i
and λi is the interpolation weight defined such that
Ei λi = 1; and λi ∈ [0, 1]. In our experiments, we
interpolate the probabilities p(c) and p(c|v) in the
linguistic (LM) and visual (VM) models, as follows:
</bodyText>
<equation confidence="0.9999885">
pLI(c) `=λLMpLM(c) + λVMpVM(c) ((6)
pLI(c|v) = ^LMpLM(c|v) + λVMpVM(c|v) (7)
</equation>
<bodyText confidence="0.996057875">
We experiment with a number of parameter settings
for λLM and λVM.
Predicate-driven linear interpolation derives
predicate-specific interpolation weights directly
from the data, as opposed to pre-setting them uni-
versally for all verbs. For each predicate v, we com-
pute the interpolation weights based on its promi-
nence in the respective corpus, as follows:
</bodyText>
<equation confidence="0.992769333333333">
reli(v)
λi(v) = (8)
Ek relk(v),
</equation>
<bodyText confidence="0.8284202">
where rel is the relevance function of model i for
verb v, computed as its relative frequency in the
respective corpus: reli(v) = fi(v)
EV fi(v). The interpo-
lation weights for LM and VM are then computed
</bodyText>
<equation confidence="0.99838825">
relLM(v)
λLM(v) = relLM(v) + relVM(v) (9)
relVM(v)
λVM(v) = relLM(v) + relVM(v). (10)
</equation>
<bodyText confidence="0.997155666666667">
The motivation for this approach comes from the
fact that not all verbs are represented equally well
in linguistic and visual data. For instance, while
concrete verbs, such as run, push or throw, are
more likely to be prominent in visual data, abstract
verbs, such as understand or speculate, are best
</bodyText>
<page confidence="0.994545">
954
</page>
<bodyText confidence="0.9990045">
represented in text. Relative linguistic and visual
frequencies of a verb provide a way to estimate the
relevance of linguistic and visual features to its SP
learning.
</bodyText>
<sectionHeader confidence="0.735149" genericHeader="method">
6 Direct evaluation and data analysis
</sectionHeader>
<bodyText confidence="0.9999368">
We evaluate the predicate-argument scores as-
signed by our models against a dataset of hu-
man plausibility judgements of verb-direct object
pairs collected by Keller and Lapata (2003). Their
dataset is balanced with respect to the frequency
of verb-argument relations, as well as their plausi-
bility and implausibility, thus creating a realistic
SP evaluation task. Keller and Lapata selected 30
predicates and matched each of them to three ar-
guments from different co-occurrence frequency
bands according to their BNC counts, e.g. divert
attention (high frequency), divert water (medium)
and divert fruit (low). This constituted their dataset
of Seen verb-noun pairs, 90 in total. Each of the
predicates was then also paired with three randomly
selected arguments with which it did not occur in
the BNC, creating the Unseen dataset. The pairs in
both datasets were then rated for their plausibility
by 27 human subjects, and their judgements were
aggregated into a gold standard. We compare the
verb-argument scores generated by our linguistic
(LSP), visual (VSP) and interpolated (ISP) SP mod-
els against these two datasets in terms of Pearson
correlation coefficient, r, and Spearman rank cor-
relation coefficient, p. The selectional association
score of the cluster to which a given noun belongs
is taken to represent the preference score of the
verb for this noun. If a noun is not present in our
argument clusters, we match it to its nearest clus-
ter, as determined by its distributional similarity
to the cluster centroid in terms of Jensen-Shannon
divergence.
We first compare LSP, VSP and ISP with static
and predicate-driven interpolation weights. The
results, presented in Table 1, demonstrate that
the interpolated model outperforms both LSP and
VSP used on their own. The best performance is
attained with the static interpolation weights of
ALM = 0.8 (r = 0.540; p = 0.728) and ALM = 0.9
(r = 0.548; p = 0.699). This suggests that while
linguistic input plays a crucial role in SP induction
(by providing both semantic and syntactic informa-
tion), visual features further enhance the quality
of SPs, as we expected. Figure 3 shows LSP- and
VSP-acquired direct object preferences of the verb
</bodyText>
<table confidence="0.999303071428572">
Seen p Unseen p
r r
VSP 0.180 0.126 0.118 0.132
ISP: ALM = 0.1 0.279 0.532 0.220 0.371
ISP: ALM = 0.2 0.349 0.556 0.278 0.411
ISP: ALM = 0.3 0.385 0.558 0.305 0.423
ISP: ALM = 0.4 0.410 0.571 0.320 0.428
ISP: ALM = 0.5 0.448 0.579 0.329 0.430
ISP: ALM = 0.6 0.461 0.591 0.330 0.431
ISP: ALM = 0.7 0.523 0.713 0.335 0.431
ISP: ALM = 0.8 0.540 0.728 0.339 0.430
ISP: ALM = 0.9 0.548 0.699 0.342 0.429
ISP: Predicate-driven 0.476 0.597 0.391 0.551
LSP 0.512 0.688 0.412 0.559
</table>
<tableCaption confidence="0.910265">
Table 1: Model comparison on the plausibility data
of Keller and Lapata (2003)
</tableCaption>
<bodyText confidence="0.586013">
LSP: (1) 0.309 expenditure cost risk expense emission budget
spending; (2) 0.201 dividend price rate premium rent rat-
ing salary wages; (3) 0.088 employment investment growth
supplies sale import export production [..]
</bodyText>
<equation confidence="0.323577">
ISP predicate-driven ALM = 0.65
</equation>
<figureCaption confidence="0.991421545454545">
(1) 0.346 expenditure cost risk expense emission budget
spending; (2) 0.211 dividend price rate premium rent rat-
ing salary wages; (3) 0.126 tail collar strand skirt trousers
hair curtain sleeve
VSP: (1) 0.224 tail collar strand skirt trousers hair curtain
sleeve; (2) 0.098 expenditure cost risk expense emission bud-
get spending; (3) 0.090 management delivery maintenance
transport service housing [..]
Figure 3: Top three direct object classes for cut
and their association scores, assigned by different
models
</figureCaption>
<bodyText confidence="0.995846043478261">
cut, as well as the effects of merging the features
in the interpolated model – the verbs’ experiential
arguments (e.g. hair or fabric) are emphasized by
the visual features.
However, the model based on visual features
alone performs poorly on the dataset of Keller and
Lapata (2003). This is partly explained by the fact
that a number of verbs in this dataset are abstract
verbs, whose visual representations in the Flickr
data are sparse. In addition, VSP (as other visual
models used in isolation from text) is not syntax-
aware and is unable to discriminate between differ-
ent types of semantic relations. VSP thus acquires
sets of verb-argument relations that are closer in
nature to scene descriptions and semantic frames
than to lexico-syntactic paradigms. Figure 4 shows
the differences between linguistic and visual ar-
guments of the verb kill ranked by LSP and VSP.
While LSP produces mainly semantic objects of kill,
VSP output contains other types of arguments, such
as weapon (instrument) and death (consequence).
Taking the argument classes produced by the
linguistic model as a basis and then re-ranking
</bodyText>
<page confidence="0.997975">
955
</page>
<figureCaption confidence="0.913647826086956">
LSP: (1) 0.523 girl other woman child person people; (2)
0.164 fleet soldier knight force rebel guard troops crew army
pilot; (3) 0.133 sister daughter parent relative lover cousin
friend wife mother husband brother father; (4) 0.048 being
species sheep animal creature horse baby human fish male
lamb bird rabbit [..]; (5) 0.045 victim bull teenager prisoner
hero gang enemy rider offender youth killer thief [..]
VSP: (1) 0.180 defeat fall death tragedy loss collapse decline
[..]; (2) 0.141 girl other woman child person people; (3) 0.128
abuse suicide killing offence murder breach crime; (4) 0.113
handle weapon horn knife blade stick sword [..]; (5) 0.095
victim bull teenager prisoner hero gang enemy rider offender
youth killer thief [..]
Figure 4: Top five arguments of kill and their asso-
ciation scores, assigned by LSP and VSP
(1) 0.442 drink coffee champagne pint wine beer; (2) 0.182
mixture dose substance drug milk cream alcohol chemical
[..]; (3) 0.091 girl other woman child person people; (4) 0.053
sister daughter parent relative lover cousin friend wife mother
husband brother father; (5) 0.050 drop tear sweat paint blood
water juice
Figure 5: Error analysis: Mixed subjects and direct
objects of drink, assigned by the predicate-driven
</figureCaption>
<bodyText confidence="0.9841679">
ISP
them to incorporate visual statistics helps to avoid
the above problem for the interpolated models,
whose output corresponds to grammatical relations.
However, static interpolation weights (emphasiz-
ing linguistic features over the visual ones for all
verbs equally) outperformed the predicate-driven
interpolation technique, attaining correlations of
r = 0.548 and r = 0.476 respectively. This is
mainly due to the fact that some verbs are over-
represented in the visual data (e.g. the predicate-
driven interpolation weight for the verb drink is
ALM = 0.08). As a result, candidate argument
classes (selected based on syntactically-parsed lin-
guistic input) are ranked predominantly based on
visual statistics. This makes it possible to empha-
size incorrectly parsed arguments (such as subject
relations in the direct object SP distribution and
vice versa). The predicate-driven ISP output for
direct object SPs of drink, for instance, contains
a mixture of subject and direct object classes, as
shown in Figure 5. Using a static model with a
high ALM weight helps to avoid such errors and,
therefore, leads to a better performance.
In order to investigate the composition of the
visual and linguistic datasets, we assess the average
level of concreteness of the verbs and nouns present
in the datasets. We use the concreteness ratings
from the MRC Psycholinguistic Database (Wilson,
1988) for this purpose. In this database, nouns and
</bodyText>
<figureCaption confidence="0.990541">
Figure 6: WordNet top level class distributions for
verbs in the visual and textual corpora
</figureCaption>
<table confidence="0.998996375">
Seen p Unseen p
r r
Rooth et al. (1999)* 0.455 0.487 0.479 0.520
Pad´o et al. (2007)* 0.484 0.490 0.398 0.430
O’Seaghdha (2010) 0.520 0.548 0.564 0.605
VSP 0.180 0.126 0.118 0.132
ISP (best) 0.548 0.699 0.342 0.429
LSP 0.512 0.688 0.412 0.559
</table>
<tableCaption confidence="0.997336">
Table 2: Comparison to other SP induction meth-
ods. *Results reported in O’Seaghdha (2010).
</tableCaption>
<bodyText confidence="0.999691074074074">
verbs are rated for concreteness on a scale from
100 (highly abstract) to 700 (highly concrete). We
map the verbs and nouns in our textual and visual
corpora to their MRC concreteness scores. We then
calculate a dataset-wide concreteness score as an
average of the concreteness scores of individual
verbs and nouns weighted by their frequency in
the respective corpus. The average concreteness
scores in the visual dataset were 506.4 (nouns) and
498.1 (verbs). As expected, they are higher than the
respective scores in the textual data: 433.1 (nouns)
and 363.4 (verbs). In order to compare the types
of actions that are common in each of the datasets,
we map the verbs to their corresponding top level
classes in WordNet. Figure 6 shows the comparison
of prominent verb classes in visual and textual data.
One can see from the Figure that the visual dataset
is well suited for representing motion, perception
and contact, while abstract verbs related to e.g.
communication, cognition, possession or change
are more common in textual data.
We also compare the performance of our models
to existing SP induction methods: the EM-based
clustering method of Rooth et al. (1999), the vec-
tor space similarity-based method of Pad´o et al.
(2007) and the LDA topic modelling approach of
O´ S´eaghdha (2010)1. The best ISP configuration
</bodyText>
<footnote confidence="0.996784">
1Since Rooth et al.’s (1999) and Pad´o et al.’s (2007) models
were not originally evaluated on the same dataset, we use the
</footnote>
<page confidence="0.996675">
956
</page>
<bodyText confidence="0.999895153846154">
(λLM = 0.9) outperforms all of these methods, as
well as our own LSP, on the Seen dataset, con-
firming the positive contribution of visual features.
However, it achieves less success on the Unseen
data, where the methods of O´ S´eaghdha (2010)
and Rooth et al. (1999) are leading. This result
speaks in favour of latent variable models for acqui-
sition of SP estimates for rarely attested predicate-
argument pairs. In turn, this suggests that integrat-
ing our ISP model (that currently outperforms oth-
ers on more common pairs) with such techniques
is likely to improve SP prediction across frequency
bands.
</bodyText>
<sectionHeader confidence="0.97857" genericHeader="method">
7 Task-based evaluation
</sectionHeader>
<bodyText confidence="0.999982259259259">
In order to investigate the applicability of perceptu-
ally grounded SPs in wider NLP, we evaluate them
in the context of an external semantic task – that of
metaphor interpretation. Since metaphor is based
on transferring imagery and knowledge across do-
mains – typically from more familiar domains of
physical experiences to the sphere of vague and
elusive abstract thought – metaphor interpretation
provides an ideal framework for testing perceptu-
ally grounded SPs. Our experiments rely on the
metaphor interpretation method of Shutova (2010),
in which text-derived SPs are a central component
of the system. We replace the SP component with
our LSP and ISP (λLM = 0.8) models and com-
pare their performance in the context of metaphor
interpretation.
Shutova (2010) defined metaphor interpretation
as a paraphrasing task, where literal paraphrases
for metaphorical expressions are derived from cor-
pus data using a set of statistical measures. For
instance, their system interprets the metaphor “a
carelessly leaked report” as “a carelessly disclosed
report”. Focusing on metaphorical verbs in subject
and direct object constructions, Shutova first ap-
plies a maximum likelihood model to extract and
rank candidate paraphrases for the verb given the
context, as follows:
</bodyText>
<equation confidence="0.997391">
rIN n=1 f(wn, i)
P(i, w1, ..., wN) = (f(i))N−1 · � k f(ik), (11)
</equation>
<bodyText confidence="0.999772511627907">
where f(i) is the frequency of the paraphrase on
its own and f(wn, i) the co-occurrence frequency
of the paraphrase with the context word wn. This
results for their re-implementation reported by O’Seaghdha
(2010), who conducted a comprehensive evaluation of SP
models on the plausibility data of Keller and Lapata (2003).
model favours paraphrases that match the given
context best. These candidates are then filtered
based on the presence of shared features with the
metaphorical verb, as defined by their location and
distance in the WordNet hierarchy. All the can-
didates that have a common hypernym with the
metaphorical verb within three levels of the Word-
Net hierarchy are selected. This results in a set of
paraphrases retaining the meaning of the metaphor-
ical verb. However, some of them are still figura-
tively used. Shutova further applies an SP model
to discriminate between figurative and literal para-
phrases, treating a strong selectional preference fit
as a likely indicator of literalness. The candidates
are re-ranked by the SP model, emphasizing the
verbs whose preferences the noun in the context
matches best. We use LSP and ISP scores to per-
form this re-ranking step.
We evaluate the performance of our models on
this task using the metaphor paraphrasing gold stan-
dard of Shutova (2010). The dataset consists of 52
verb metaphors and their human-produced literal
paraphrases. Following Shutova, we evaluate the
performance in terms of mean average precision
(MAP), which measures the ranking quality of GS
paraphrases across the dataset. MAP is defined as
follows:
where M is the number of metaphorical expres-
sions, Nj is the number of correct paraphrases for
the metaphorical expression j, Pji is the precision
at each correct paraphrase (the number of correct
paraphrases among the top i ranks). As compared
to the gold standard, ISP attains a MAP score of
0.65, outperforming both the LSP (MAP = 0.62)
and the original system of Shutova (2010) (MAP
= 0.62), demonstrating the positive contribution of
visual features.
</bodyText>
<sectionHeader confidence="0.984441" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999757333333333">
We have presented the first SP induction method
that simultaneously draws knowledge from text,
images and videos. Our experiments show that it
outperforms linguistic and visual models in iso-
lation, as well as the previous approaches to SP
learning. We believe that this model has a wide
applicability in NLP, where many systems already
rely on automatically induced SPs. It can also
benefit image caption generation systems, which
</bodyText>
<figure confidence="0.5641904">
1
1
M
MAP =
Pji,
Nj
Nj
i=1
M
j=1
</figure>
<page confidence="0.987967">
957
</page>
<bodyText confidence="0.9999731875">
typically focus on objects rather than actions, by
providing information about predicate-argument
structure.
In the future, it would be interesting to derive
the information about predicate-argument relations
from low-level visual features directly. However, to
our knowledge, reliably mapping images to actions
(i.e. verbs) at a large-scale is still a challenging
task. Human-annotated image and video descrip-
tions allow us to investigate what types of verb–
noun relations are in principle present in the visual
data and the ways in which they are different from
the ones found in text. Our results show that visual
data is better suited for capturing physical proper-
ties of concepts as well as containing relations not
explicitly described in text.
The presented interpolation techniques are also
applicable outside multi-modal semantics. For in-
stance, they can be generalised to acquire SPs from
unbalanced corpora of different sizes (e.g. for lan-
guages lacking balanced corpora) or to perform
domain adaptation of SPs. In the future, we would
like to apply SP interpolation to multilingual SP
learning, i.e. integrating data from multiple lan-
guages for more accurate SP induction and project-
ing universal semantic relations to low-resource
languages. It is also interesting to investigate SP
learning at the level of semantic predicates (e.g.
automatically inducing FrameNet-style frames),
where combining the visual and linguistic knowl-
edge is likely to outperform text-based models on
their own.
</bodyText>
<sectionHeader confidence="0.995136" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99992">
Ekaterina Shutova’s research is funded by the
University of Cambridge and the Leverhulme
Trust Early Career Fellowship. Gerard de Melo’s
work is funded by China 973 Program Grants
2011CBA00300, 2011CBA00301, and NSFC
Grants 61033001, 61361136003, 61450110088.
We are grateful to the ACL reviewers for their in-
sightful feedback.
</bodyText>
<sectionHeader confidence="0.997564" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996126245614035">
Steven Abney and Marc Light. 1999. Hiding a Seman-
tic Hierarchy in a Markov Model. In Proceedings of
the Workshop on Unsupervised Learning in Natural
Language Processing, ACL, pages 1–8.
Lisa Aziz-Zadeh and Antonio Damasio. 2008. Embod-
ied semantics for actions: Findings from functional
brain imaging. Journal of Physiology – Paris, 102(1-
3).
Lawrence W. Barsalou. 1999. Perceptual symbol sys-
tems. Behavioral and Brain Sciences, 22(4):577–
609.
Lawrence W. Barsalou. 2008. Grounded cognition.
Annual Review of Psychology, 59(1):617–645.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
Proceedings of RANLP.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference
from unlabeled text. In Proceedings of EMNLP
2008, EMNLP ’08, pages 59–68, Honolulu, Hawaii.
Chris Brew and Sabine Schulte im Walde. 2002. Spec-
tral clustering for German verbs. In Proceedings of
EMNLP, pages 117–124.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL on Interactive presen-
tation sessions, pages 77–80.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL 2012, pages
136–145, Jeju Island, Korea, July. ACL.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
Lou Burnard. 2007. Reference Guide for the British
National Corpus (XML Edition).
Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta.
2013. NEIL: Extracting Visual Knowledge from
Web Data. In Proceedings of ICCV 2013.
Massimiliano Ciaramita and Mark Johnson. 2000. Ex-
plaining away ambiguity: Learning verb selectional
preference with Bayesian networks. In Proceedings
of COLING 2000, pages 187–193.
Stephen Clark and David Weir. 1999. An iterative
approach to estimating frequencies over a seman-
tic hierarchy. In Proceedings of EMNLP/VLC 1999,
pages 258–265.
Santosh Divvala, Ali Farhadi, and Carlos Guestrin.
2014. Learning everything about anything: Webly-
supervised visual concept learning. In Proceedings
of CVPR 2014.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of ACL
2007.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49–90.
</reference>
<page confidence="0.990967">
958
</page>
<reference confidence="0.998342134615385">
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of NAACL 2010, pages 91–99. ACL.
Andrea Frome, Greg Corrado, Jon Shlens, Samy
Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and
Tomas Mikolov. 2013. DeViSE: A deep visual-
semantic embedding model. In Proceedings of NIPS
2013.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28.
Arthur M. Glenberg and Michael P. Kaschak. 2002.
Grounding language in action. Psychonomic Bul-
letin and Review, pages 558–565.
Gurobi Optimization. 2014. Gurobi optimizer refer-
ence manual, version 5.6. Houston, TX, USA.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguis-
tics, 19:103–120.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459–484.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is some-
times more. In Proceedings ofACL 2014, Baltimore,
Maryland.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania, PA.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2014. Multimodal neural language models.
In Proceedings of ICML 2014, pages 595–603.
Girish Kulkarni, Visruth Premraj, Vicente Ordonez,
Sagnik Dhar, Siming Li, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. Babytalk: Un-
derstanding and generating simple image descrip-
tions. IEEE Trans. Pattern Anal. Mach. Intell.,
35(12):2891–2903.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL 2014, pages 1403–
1414. ACL.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle.
Computational Linguistics, 24(2):217–244.
Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013.
Data-driven metaphor recognition and explanation.
Transactions of the Association for Computational
Linguistics, 1:379–390.
Zachary Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23–44.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639–654.
Marina Meila and Jianbo Shi. 2001. A random walks
view of spectral segmentation. In Proceedings ofAI-
STATS.
Diarmuid O´ S´eaghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings ofACL
2010.
Sebastian Pad´o, Ulrike Pad´o, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plau-
sibility judgements. In Proceedings of EMNLP-
CoNLL.
P. Pantel, R. Bhagat, T. Chklovski, and E. Hovy. 2007.
Isp: Learning inferential selectional preferences. In
Proceedings of NAACL 2007.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, pages 38–41.
Y. Peirsman and S. Pad´o. 2010. Cross-lingual induc-
tion of selectional preferences with bilingual vector
spaces. In Proceedings of NAACL 2010, pages 921–
929.
Philip Resnik. 1993. Selection and information: A
class-based approach to lexical relationships. Tech-
nical report, University of Pennsylvania.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In ACL SIGLEX Workshop on Tag-
ging Text with Lexical Semantics, Washington, D.C.
Alan Ritter, Mausam Etzioni, and Oren Etzioni. 2010.
A latent dirichlet allocation method for selectional
preferences. In Proceedings ACL 2010, pages 424–
434.
Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater,
Manfred Pinkal, and Bernt Schiele. 2013. Translat-
ing video content to natural language descriptions.
In Proceedings of ICCV 2013.
Stephen Roller and Sabine Schulte im Walde. 2013.
A Multimodal LDA Model integrating Textual, Cog-
nitive and Visual Modalities. In Proceedings of
EMNLP 2013, pages 1146–1157, Seattle, WA.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings ofACL 1999, pages 104–111.
David Shamma. 2014. One hundred million Cre-
ative Commons Flickr images for research. http:
//labs.yahoo.com/news/yfcc100m/.
</reference>
<page confidence="0.986329">
959
</page>
<reference confidence="0.999808909090909">
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical Metaphor Processing. Com-
putational Linguistics, 39(2).
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Proceedings of
NAACL 2010, pages 1029–1037, Los Angeles, USA.
Ekaterina Shutova. 2011. Computational Approaches
to Figurative Language. Ph.D. thesis, University of
Cambridge, UK.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of ACL 2014, Baltimore,
Maryland.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with vi-
sual attributes. In Proceedings of ACL 2013, pages
572–582.
Richard Socher, Milind Ganjoo, Christopher D. Man-
ning, and Andrew Ng. 2013. Zero-shot learning
through cross-modal transfer. In Proceedings of
NIPS 2013, pages 935–943.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of EMNLP 2009.
Tim Van de Cruys. 2014. A neural network approach
to selectional preference acquisition. In Proceed-
ings of EMNLP 2014.
Wiebke Wagner, Helmut Schmid, and Sabine Schulte
Im Walde. 2009. Verb sense disambiguation us-
ing a predicate-argument clustering model. In Pro-
ceedings of the CogSci Workshop on Semantic Space
Models (DISCO).
M.D. Wilson. 1988. The MRC Psycholinguistic
Database: Machine Readable Dictionary, Version
2. Behavioural Research Methods, Instruments and
Computers, 20:6–11.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. 2014. From image descriptions to vi-
sual denotations. Transactions of the Association
of Computational Linguistics – Volume 2, Issue 1,
pages 67–78.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using
selectional preferences. In Proceedings of COL-
ING/ACL, pages 849–856.
Be˜nat Zapirain, Eneko Agirre, Llu´ıs M`arquez, and Mi-
hai Surdeanu. 2010. Improving semantic role clas-
sification with selectional preferences. In Proceed-
ings of NAACL HLT 2010, pages 373–376.
</reference>
<page confidence="0.997561">
960
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830063">
<title confidence="0.995941">Perceptually grounded selectional preferences</title>
<author confidence="0.998053">Ekaterina Shutova Niket Tandon Gerard de_Melo</author>
<affiliation confidence="0.999554">Computer Laboratory Max Planck Institute IIIS University of Cambridge, UK for Informatics, Germany Tsinghua University,</affiliation>
<email confidence="0.838435">es407@cam.ac.ukntandon@mpi-inf.mpg.degdm@demelo.org</email>
<abstract confidence="0.999758142857143">Selectional preferences (SPs) are widely used in NLP as a rich source of semantic information. While SPs have been traditionally induced from textual data, human lexical acquisition is known to rely on both linguistic and perceptual experience. We present the first SP learning method that simultaneously draws knowledge from text, images and videos, using image and video descriptions to obtain visual features. Our results show that it outperforms linguistic and visual models in isolation, as well as the existing SP induction approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a Semantic Hierarchy in a Markov Model.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Unsupervised Learning in Natural Language Processing, ACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2327" citStr="Abney and Light, 1999" startWordPosition="350" endWordPosition="353"> role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 200</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Steven Abney and Marc Light. 1999. Hiding a Semantic Hierarchy in a Markov Model. In Proceedings of the Workshop on Unsupervised Learning in Natural Language Processing, ACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Aziz-Zadeh</author>
<author>Antonio Damasio</author>
</authors>
<title>Embodied semantics for actions: Findings from functional brain imaging.</title>
<date>2008</date>
<journal>Journal of Physiology – Paris,</journal>
<pages>102--1</pages>
<contexts>
<context position="2975" citStr="Aziz-Zadeh and Damasio, 2008" startWordPosition="449" endWordPosition="452">nson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image features such as color histograms or</context>
</contexts>
<marker>Aziz-Zadeh, Damasio, 2008</marker>
<rawString>Lisa Aziz-Zadeh and Antonio Damasio. 2008. Embodied semantics for actions: Findings from functional brain imaging. Journal of Physiology – Paris, 102(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Perceptual symbol systems.</title>
<date>1999</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>22</volume>
<issue>4</issue>
<pages>609</pages>
<contexts>
<context position="2900" citStr="Barsalou, 1999" startWordPosition="441" endWordPosition="442">ark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date thes</context>
</contexts>
<marker>Barsalou, 1999</marker>
<rawString>Lawrence W. Barsalou. 1999. Perceptual symbol systems. Behavioral and Brain Sciences, 22(4):577– 609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Grounded cognition.</title>
<date>2008</date>
<journal>Annual Review of Psychology,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="2944" citStr="Barsalou, 2008" startWordPosition="447" endWordPosition="448">iaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image featu</context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>Lawrence W. Barsalou. 2008. Grounded cognition. Annual Review of Psychology, 59(1):617–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="8576" citStr="Bergsma and Goebel (2011)" startWordPosition="1293" endWordPosition="1296"> predicate p is frequently observed in the data with the arguments a&apos; similar to a. The systems compute similarities between distributional representations of arguments in a vector space. Bergsma et al. (2008) trained an SVM classifier to discriminate between felicitous and infelicitous verb-argument pairs. Their training data consisted of observed verb-argument pairs (positive examples) with unobserved, randomly-generated ones (negative examples). They classified nominal arguments of verbs, using their verb co-occurrence probabilities and information about their semantic classes as features. Bergsma and Goebel (2011) extended this method by incorporating image-driven noun features. They extract color and SIFT keypoint features from images found for a particular noun via Google image searches and add them to the feature vectors to classify nouns as felicitous or infelicitous arguments of a given verb. This method is the closest in spirit to ours and the only one so far to investigate the relevance of visual fea951 tures to lexical preference learning. However, our work casts the problem in a different framework: rather than relying on low-level visual properties of nouns in isolation, we explicitly model i</context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008, EMNLP ’08,</booktitle>
<pages>59--68</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="2416" citStr="Bergsma et al., 2008" startWordPosition="365" endWordPosition="368">ence (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and rel</context>
<context position="8160" citStr="Bergsma et al. (2008)" startWordPosition="1240" endWordPosition="1243">(2010) similarly model SPs within a latent variable framework, but use Latent Dirichlet Allocation (LDA) to learn the probability distributions, for single-argument and multi-argument preferences respectively. Pad´o et al. (2007) and Erk (2007) used similarity metrics to approximate selectional preference classes. Their underlying hypothesis is that a predicate-argument combination (p, a) is felicitous if the predicate p is frequently observed in the data with the arguments a&apos; similar to a. The systems compute similarities between distributional representations of arguments in a vector space. Bergsma et al. (2008) trained an SVM classifier to discriminate between felicitous and infelicitous verb-argument pairs. Their training data consisted of observed verb-argument pairs (positive examples) with unobserved, randomly-generated ones (negative examples). They classified nominal arguments of verbs, using their verb co-occurrence probabilities and information about their semantic classes as features. Bergsma and Goebel (2011) extended this method by incorporating image-driven noun features. They extract color and SIFT keypoint features from images found for a particular noun via Google image searches and a</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of EMNLP 2008, EMNLP ’08, pages 59–68, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brew</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Spectral clustering for German verbs.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>117--124</pages>
<marker>Brew, Walde, 2002</marker>
<rawString>Chris Brew and Sabine Schulte im Walde. 2002. Spectral clustering for German verbs. In Proceedings of EMNLP, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<pages>77--80</pages>
<contexts>
<context position="11206" citStr="Briscoe et al., 2006" startWordPosition="1701" endWordPosition="1704">al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of association between a particular verb and a particular argument class. Visual data. For the visual features of our model, we mine the Yahoo! Webscope Flickr-100M dataset (Shamma, 2014). Flickr-100M contains 99.3 million images and 0.7 million videos with language tags annotated by users, enabling us to generalise SPs at a large scale. The tags reflect how humans describe objects and actions from a v</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 77–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>136--145</pages>
<publisher>ACL.</publisher>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="10064" citStr="Bruni et al. (2012)" startWordPosition="1530" endWordPosition="1533">cted of positive (observed) and negative (randomly-generated) examples for training. The network weights were optimized by requiring the model to assign a higher score to an observed pair than to the unobserved one by a given margin. 2.2 Multi-modal methods in semantics Previous work has used multimodal data to determine distributional similarity or to learn multimodal embeddings that project multiple modalities into the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et </context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012. Distributional semantics in Technicolor. In Proceedings of ACL 2012, pages 136–145, Jeju Island, Korea, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="3236" citStr="Bruni et al., 2014" startWordPosition="492" endWordPosition="495"> 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image features such as color histograms or SIFT keypoints to represent the meaning of isolated words. To the best of our knowledge, there has not yet been a multimodal semantic approach performing extraction of 950 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<date>2007</date>
<booktitle>Reference Guide for the British National Corpus (XML Edition).</booktitle>
<contexts>
<context position="4760" citStr="Burnard, 2007" startWordPosition="726" endWordPosition="727">ent interactions from text, images, and videos. We expect it to outperform purely text-based models of SPs, which suffer from two problems: topic bias and figurative uses of words. Such bias stems from the fact that we typically write about abstract topics and events, resulting in high coverage of abstract senses of words and comparatively lower coverage of the original physical senses (Shutova, 2011). For instance, the verb cut is used predominantly in the domains of economics and finance and its most frequent direct objects are cost and price, according to the British National Corpus (BNC) (Burnard, 2007). Predicate-argument distributions acquired from text thus tend to be skewed in favour of abstract domains and figurative uses, inadequately reflecting our daily experiences with cutting, which guide human acquisition of meaning. Integrating predicate-argument relations observed in the physical world (in the form of image and video descriptions) with the more abstract text-based relations is likely to yield a more realistic semantic model, with real prospects of improving the performance of NLP applications that rely on SPs. We use the BNC as an approximation of linguistic knowledge and a larg</context>
</contexts>
<marker>Burnard, 2007</marker>
<rawString>Lou Burnard. 2007. Reference Guide for the British National Corpus (XML Edition).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinlei Chen</author>
<author>Abhinav Shrivastava</author>
<author>Abhinav Gupta</author>
</authors>
<title>NEIL: Extracting Visual Knowledge from Web Data.</title>
<date>2013</date>
<booktitle>In Proceedings of ICCV</booktitle>
<contexts>
<context position="10812" citStr="Chen et al., 2013" startWordPosition="1638" endWordPosition="1641">ures, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of associat</context>
</contexts>
<marker>Chen, Shrivastava, Gupta, 2013</marker>
<rawString>Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. 2013. NEIL: Extracting Visual Knowledge from Web Data. In Proceedings of ICCV 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional preference with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>187--193</pages>
<contexts>
<context position="2357" citStr="Ciaramita and Johnson, 2000" startWordPosition="354" endWordPosition="357"> and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh </context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2000. Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. In Proceedings of COLING 2000, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>An iterative approach to estimating frequencies over a semantic hierarchy.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC</booktitle>
<pages>258--265</pages>
<contexts>
<context position="2304" citStr="Clark and Weir, 1999" startWordPosition="346" endWordPosition="349">Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Gl</context>
</contexts>
<marker>Clark, Weir, 1999</marker>
<rawString>Stephen Clark and David Weir. 1999. An iterative approach to estimating frequencies over a semantic hierarchy. In Proceedings of EMNLP/VLC 1999, pages 258–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santosh Divvala</author>
<author>Ali Farhadi</author>
<author>Carlos Guestrin</author>
</authors>
<title>Learning everything about anything: Weblysupervised visual concept learning.</title>
<date>2014</date>
<booktitle>In Proceedings of CVPR</booktitle>
<contexts>
<context position="10835" citStr="Divvala et al., 2014" startWordPosition="1642" endWordPosition="1645">l. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of association between a particula</context>
</contexts>
<marker>Divvala, Farhadi, Guestrin, 2014</marker>
<rawString>Santosh Divvala, Ali Farhadi, and Carlos Guestrin. 2014. Learning everything about anything: Weblysupervised visual concept learning. In Proceedings of CVPR 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2487" citStr="Erk, 2007" startWordPosition="377" endWordPosition="378">ing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also </context>
<context position="7783" citStr="Erk (2007)" startWordPosition="1184" endWordPosition="1185">e, which represents a cluster of verb-argument interactions. The latent variable distribution and the probabilities that a latent variable generates the verb and the argument are learned from the data using Expectation Maximization (EM). The latent variables enable the model to recognise previously unseen verb-argument pairs. O´ S´eaghdha (2010) and Ritter et al. (2010) similarly model SPs within a latent variable framework, but use Latent Dirichlet Allocation (LDA) to learn the probability distributions, for single-argument and multi-argument preferences respectively. Pad´o et al. (2007) and Erk (2007) used similarity metrics to approximate selectional preference classes. Their underlying hypothesis is that a predicate-argument combination (p, a) is felicitous if the predicate p is frequently observed in the data with the arguments a&apos; similar to a. The systems compute similarities between distributional representations of arguments in a vector space. Bergsma et al. (2008) trained an SVM classifier to discriminate between felicitous and infelicitous verb-argument pairs. Their training data consisted of observed verb-argument pairs (positive examples) with unobserved, randomly-generated ones </context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<title>met*: A method for discriminating metonymy and metaphor by computer.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="1893" citStr="Fass, 1991" startWordPosition="279" endWordPosition="280">ay.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peir</context>
</contexts>
<marker>Fass, 1991</marker>
<rawString>Dan Fass. 1991. met*: A method for discriminating metonymy and metaphor by computer. Computational Linguistics, 17(1):49–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010,</booktitle>
<pages>91--99</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10007" citStr="Feng and Lapata, 2010" startWordPosition="1520" endWordPosition="1523">elicitous and infelicitous arguments using the data constructed of positive (observed) and negative (randomly-generated) examples for training. The network weights were optimized by requiring the model to assign a higher score to an observed pair than to the unobserved one by a given margin. 2.2 Multi-modal methods in semantics Previous work has used multimodal data to determine distributional similarity or to learn multimodal embeddings that project multiple modalities into the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video desc</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Proceedings of NAACL 2010, pages 91–99. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg Corrado</author>
<author>Jon Shlens</author>
<author>Samy Bengio</author>
<author>Jeffrey Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A deep visualsemantic embedding model.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="10502" citStr="Frome et al., 2013" startWordPosition="1590" endWordPosition="1593">ace. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A deep visualsemantic embedding model. In Proceedings of NIPS 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<contexts>
<context position="1748" citStr="Gildea and Jurafsky, 2002" startWordPosition="256" endWordPosition="259">“The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johns</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur M Glenberg</author>
<author>Michael P Kaschak</author>
</authors>
<title>Grounding language in action. Psychonomic Bulletin and Review,</title>
<date>2002</date>
<pages>558--565</pages>
<contexts>
<context position="2928" citStr="Glenberg and Kaschak, 2002" startWordPosition="443" endWordPosition="446">99; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-l</context>
</contexts>
<marker>Glenberg, Kaschak, 2002</marker>
<rawString>Arthur M. Glenberg and Michael P. Kaschak. 2002. Grounding language in action. Psychonomic Bulletin and Review, pages 558–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurobi Optimization</author>
</authors>
<title>Gurobi optimizer reference manual, version 5.6.</title>
<date>2014</date>
<location>Houston, TX, USA.</location>
<contexts>
<context position="13935" citStr="Optimization, 2014" startWordPosition="2160" endWordPosition="2161">djective-adjective pairs, and finally, WordNet verb-groups and VerbNet class membership (Kipper-Schuler, 2005) for verb-verb pairs. Note that even parts of speech that are disregarded later on can still be helpful at this stage, as we aim at a joint optimization over all words. After the similarities have been obtained for all rel952 S into a stochastic matrix P containing transition probabilities between the vertices in the graph as P = D−1S, (2) evant sense pairs, we maximize the coherence of the senses of the words in the set as an Integer Linear Program, using the Gurobi Optimizer (Gurobi Optimization, 2014) and solving maximize P Pijxij + P P Sij,i0j0Bij,i0j0 i ij i0j0 subject to P j xij ≤ 1 ∀i, xij ∈ {0,1} ∀i,j, Bij,i0j0 ≤ xij, Bij,i0j0 ≤ xi0j0, Bij,i0j0 ∈ {0, 1} ∀i, j, iy. The binary variables Bij,i0j0 are 1 iff xij = 1 and xi0j0 = 1, indicating that both senses were simultaneously chosen. The optimizer disambiguates the input words by selecting sense tuples x1j, x2j, ..., from which we can directly obtain the corresponding PoS information. Verb-noun co-occurrence information is then extracted from the PoS-tagged sets. 5 Selectional preference model 5.1 Acquisition of argument classes To addre</context>
</contexts>
<marker>Optimization, 2014</marker>
<rawString>Gurobi Optimization. 2014. Gurobi optimizer reference manual, version 5.6. Houston, TX, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<booktitle>Computational Linguistics,</booktitle>
<pages>19--103</pages>
<contexts>
<context position="1696" citStr="Hindle and Rooth, 1993" startWordPosition="249" endWordPosition="252">he sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and We</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19:103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="22374" citStr="Keller and Lapata (2003)" startWordPosition="3514" endWordPosition="3517">bs are represented equally well in linguistic and visual data. For instance, while concrete verbs, such as run, push or throw, are more likely to be prominent in visual data, abstract verbs, such as understand or speculate, are best 954 represented in text. Relative linguistic and visual frequencies of a verb provide a way to estimate the relevance of linguistic and visual features to its SP learning. 6 Direct evaluation and data analysis We evaluate the predicate-argument scores assigned by our models against a dataset of human plausibility judgements of verb-direct object pairs collected by Keller and Lapata (2003). Their dataset is balanced with respect to the frequency of verb-argument relations, as well as their plausibility and implausibility, thus creating a realistic SP evaluation task. Keller and Lapata selected 30 predicates and matched each of them to three arguments from different co-occurrence frequency bands according to their BNC counts, e.g. divert attention (high frequency), divert water (medium) and divert fruit (low). This constituted their dataset of Seen verb-noun pairs, 90 in total. Each of the predicates was then also paired with three randomly selected arguments with which it did n</context>
<context position="24942" citStr="Keller and Lapata (2003)" startWordPosition="3948" endWordPosition="3951">gure 3 shows LSP- and VSP-acquired direct object preferences of the verb Seen p Unseen p r r VSP 0.180 0.126 0.118 0.132 ISP: ALM = 0.1 0.279 0.532 0.220 0.371 ISP: ALM = 0.2 0.349 0.556 0.278 0.411 ISP: ALM = 0.3 0.385 0.558 0.305 0.423 ISP: ALM = 0.4 0.410 0.571 0.320 0.428 ISP: ALM = 0.5 0.448 0.579 0.329 0.430 ISP: ALM = 0.6 0.461 0.591 0.330 0.431 ISP: ALM = 0.7 0.523 0.713 0.335 0.431 ISP: ALM = 0.8 0.540 0.728 0.339 0.430 ISP: ALM = 0.9 0.548 0.699 0.342 0.429 ISP: Predicate-driven 0.476 0.597 0.391 0.551 LSP 0.512 0.688 0.412 0.559 Table 1: Model comparison on the plausibility data of Keller and Lapata (2003) LSP: (1) 0.309 expenditure cost risk expense emission budget spending; (2) 0.201 dividend price rate premium rent rating salary wages; (3) 0.088 employment investment growth supplies sale import export production [..] ISP predicate-driven ALM = 0.65 (1) 0.346 expenditure cost risk expense emission budget spending; (2) 0.211 dividend price rate premium rent rating salary wages; (3) 0.126 tail collar strand skirt trousers hair curtain sleeve VSP: (1) 0.224 tail collar strand skirt trousers hair curtain sleeve; (2) 0.098 expenditure cost risk expense emission budget spending; (3) 0.090 managemen</context>
<context position="33680" citStr="Keller and Lapata (2003)" startWordPosition="5346" endWordPosition="5349">arelessly disclosed report”. Focusing on metaphorical verbs in subject and direct object constructions, Shutova first applies a maximum likelihood model to extract and rank candidate paraphrases for the verb given the context, as follows: rIN n=1 f(wn, i) P(i, w1, ..., wN) = (f(i))N−1 · � k f(ik), (11) where f(i) is the frequency of the paraphrase on its own and f(wn, i) the co-occurrence frequency of the paraphrase with the context word wn. This results for their re-implementation reported by O’Seaghdha (2010), who conducted a comprehensive evaluation of SP models on the plausibility data of Keller and Lapata (2003). model favours paraphrases that match the given context best. These candidates are then filtered based on the presence of shared features with the metaphorical verb, as defined by their location and distance in the WordNet hierarchy. All the candidates that have a common hypernym with the metaphorical verb within three levels of the WordNet hierarchy are selected. This results in a set of paraphrases retaining the meaning of the metaphorical verb. However, some of them are still figuratively used. Shutova further applies an SP model to discriminate between figurative and literal paraphrases, </context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL 2014,</booktitle>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="3356" citStr="Kiela et al., 2014" startWordPosition="511" endWordPosition="514">ere is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image features such as color histograms or SIFT keypoints to represent the meaning of isolated words. To the best of our knowledge, there has not yet been a multimodal semantic approach performing extraction of 950 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 950–960, Beijing, China, July 26-31, 20</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In Proceedings ofACL 2014, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper-Schuler</author>
</authors>
<title>VerbNet: A broadcoverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, PA.</institution>
<contexts>
<context position="13426" citStr="Kipper-Schuler, 2005" startWordPosition="2069" endWordPosition="2070">iven word i and one of its candidate WordNet senses j, we consider an assignment variable xij and compute a sense frequency-based prior for it as Pij = 1 1+R, where R is the WordNet rank of the sense. We then compute a similarity score Sij,i,j, between all pairs of sense choices for two words i,i&apos; and their respective candidate senses j,j&apos;. For these, we rely on WordNet’s taxonomic pathbased similarities (Pedersen et al., 2004) in the case of noun-noun sense pairs, the Adapted Lesk similarity measure for adjective-adjective pairs, and finally, WordNet verb-groups and VerbNet class membership (Kipper-Schuler, 2005) for verb-verb pairs. Note that even parts of speech that are disregarded later on can still be helpful at this stage, as we aim at a joint optimization over all words. After the similarities have been obtained for all rel952 S into a stochastic matrix P containing transition probabilities between the vertices in the graph as P = D−1S, (2) evant sense pairs, we maximize the coherence of the senses of the words in the set as an Integer Linear Program, using the Gurobi Optimizer (Gurobi Optimization, 2014) and solving maximize P Pijxij + P P Sij,i0j0Bij,i0j0 i ij i0j0 subject to P j xij ≤ 1 ∀i, </context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>Karin Kipper-Schuler. 2005. VerbNet: A broadcoverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard S Zemel</author>
</authors>
<title>Multimodal neural language models.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML 2014,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="10760" citStr="Kiros et al., 2014" startWordPosition="1629" endWordPosition="1632">e vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP clas</context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. Multimodal neural language models. In Proceedings of ICML 2014, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Vicente Ordonez</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Babytalk: Understanding and generating simple image descriptions.</title>
<date>2013</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>35</volume>
<issue>12</issue>
<contexts>
<context position="10595" citStr="Kulkarni et al., 2013" startWordPosition="1604" endWordPosition="1607">al features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et</context>
</contexts>
<marker>Kulkarni, Premraj, Ordonez, Dhar, Li, Choi, Berg, Berg, 2013</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2013. Babytalk: Understanding and generating simple image descriptions. IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2891–2903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<pages>1403--1414</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10527" citStr="Lazaridou et al., 2014" startWordPosition="1594" endWordPosition="1597">ly on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. </context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world. In Proceedings of ACL 2014, pages 1403– 1414. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the mdl principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="2282" citStr="Li and Abe, 1998" startWordPosition="342" endWordPosition="345">ments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and acti</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the mdl principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongsong Li</author>
<author>Kenny Q Zhu</author>
<author>Haixun Wang</author>
</authors>
<title>Data-driven metaphor recognition and explanation.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--379</pages>
<contexts>
<context position="1946" citStr="Li et al., 2013" startWordPosition="287" endWordPosition="290">ret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´</context>
</contexts>
<marker>Li, Zhu, Wang, 2013</marker>
<rawString>Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013. Data-driven metaphor recognition and explanation. Transactions of the Association for Computational Linguistics, 1:379–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zachary Mason</author>
</authors>
<title>Cormet: a computational, corpus-based conventional metaphor extraction system.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1906" citStr="Mason, 2004" startWordPosition="281" endWordPosition="282">implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´</context>
</contexts>
<marker>Mason, 2004</marker>
<rawString>Zachary Mason. 2004. Cormet: a computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30(1):23–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1606" citStr="McCarthy and Carroll, 2003" startWordPosition="237" endWordPosition="240">ties are more likely to fill the predicate’s argument slot than others. For instance, while the sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, includ</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
<author>Jianbo Shi</author>
</authors>
<title>A random walks view of spectral segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings ofAISTATS.</booktitle>
<contexts>
<context position="15749" citStr="Meila and Shi (2001)" startWordPosition="2458" endWordPosition="2461">defined as follows: 1 1 diS(wi, wj) = 2dKL(wi||m) + 2dKL(wj||m), (1) where dKL is the Kullback-Leibler divergence, and m is the average of wi and wj. We construct the similarity matrix S computing similarities Sij as Sij = exp(−diS(wi, wj)). The matrix S then encodes a similarity graph G (over our nouns), where Sij are the adjacency weights. The clustering problem can then be defined as identifying the optimal partition, or cut, of the graph into clusters, such that the intra-cluster weights are high and the intercluster weights are low. We use the multiway normalized cut (MNCut) algorithm of Meila and Shi (2001) for this purpose. The algorithm transforms where the degree matrix D is a diagonal matrix with Dii = PN j=1 Sij. It then computes the K leading eigenvectors of P, where K is the desired number of clusters. The graph is partitioned by finding approximately equal elements in the eigenvectors using a simpler clustering algorithm, such as k-means. Meila and Shi (2001) have shown that the partition I derived in this way minimizes the MNCut criterion: MNCut(I) = XK (1 − P(Ik → Ik|Ik)), (3) k=1 which is the sum of transition probabilities across different clusters. Since k-means starts from a random</context>
</contexts>
<marker>Meila, Shi, 2001</marker>
<rawString>Marina Meila and Jianbo Shi. 2001. A random walks view of spectral segmentation. In Proceedings ofAISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL</booktitle>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings ofACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
<author>Katrin Erk</author>
</authors>
<title>Flexible, corpus-based modelling of human plausibility judgements.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<marker>Pad´o, Pad´o, Erk, 2007</marker>
<rawString>Sebastian Pad´o, Ulrike Pad´o, and Katrin Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>R Bhagat</author>
<author>T Chklovski</author>
<author>E Hovy</author>
</authors>
<title>Isp: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="1845" citStr="Pantel et al., 2007" startWordPosition="271" endWordPosition="274">nces “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), di</context>
</contexts>
<marker>Pantel, Bhagat, Chklovski, Hovy, 2007</marker>
<rawString>P. Pantel, R. Bhagat, T. Chklovski, and E. Hovy. 2007. Isp: Learning inferential selectional preferences. In Proceedings of NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet:: Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL</title>
<date>2004</date>
<pages>38--41</pages>
<contexts>
<context position="13236" citStr="Pedersen et al., 2004" startWordPosition="2041" endWordPosition="2044">he space of candidate word senses, such that their overall similarity is maximized. This amounts to assigning those senses and PoS tags to the words in the set that best fit together. For a given word i and one of its candidate WordNet senses j, we consider an assignment variable xij and compute a sense frequency-based prior for it as Pij = 1 1+R, where R is the WordNet rank of the sense. We then compute a similarity score Sij,i,j, between all pairs of sense choices for two words i,i&apos; and their respective candidate senses j,j&apos;. For these, we rely on WordNet’s taxonomic pathbased similarities (Pedersen et al., 2004) in the case of noun-noun sense pairs, the Adapted Lesk similarity measure for adjective-adjective pairs, and finally, WordNet verb-groups and VerbNet class membership (Kipper-Schuler, 2005) for verb-verb pairs. Note that even parts of speech that are disregarded later on can still be helpful at this stage, as we aim at a joint optimization over all words. After the similarities have been obtained for all rel952 S into a stochastic matrix P containing transition probabilities between the vertices in the graph as P = D−1S, (2) evant sense pairs, we maximize the coherence of the senses of the wo</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet:: Similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Peirsman</author>
<author>S Pad´o</author>
</authors>
<title>Cross-lingual induction of selectional preferences with bilingual vector spaces.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010,</booktitle>
<pages>921--929</pages>
<marker>Peirsman, Pad´o, 2010</marker>
<rawString>Y. Peirsman and S. Pad´o. 2010. Cross-lingual induction of selectional preferences with bilingual vector spaces. In Proceedings of NAACL 2010, pages 921– 929.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and information: A class-based approach to lexical relationships.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2264" citStr="Resnik, 1993" startWordPosition="340" endWordPosition="341">ntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in p</context>
<context position="6659" citStr="Resnik (1993)" startWordPosition="1013" endWordPosition="1014">estigate the impact of perceptual information at different levels – from none (purely text-based model) to 100% (purely visual model). We evaluate our model directly against a dataset of human plausibility judgements of verbnoun pairs, as well as in the context of a semantic task: metaphor interpretation. Our results show that the interpolated model combining linguistic and visual relations outperforms the purely linguistic model in both evaluation settings. 2 Related work 2.1 Selectional preference induction The widespread interest in automatic acquisition of SPs was triggered by the work of Resnik (1993), who treated SPs as probability distributions over all potential arguments of a predicate, rather than a single argument class assigned to the predicate. The original study used WordNet to define SP classes and to map the words in the corpus to those classes. Since then, the field has moved toward automatic induction of SP classes from corpus data. Rooth et al. (1999) presented a probabilistic latent variable model of verb preferences. In their approach, verbargument pairs are generated from a latent variable, which represents a cluster of verb-argument interactions. The latent variable distr</context>
<context position="18929" citStr="Resnik (1993)" startWordPosition="2964" endWordPosition="2965"> benjamin cost benefit crisis debt credit customer consumer Figure 2: Clusters obtained using visual features predicate-argument structure. In addition, the image features tend to be sparse for abstract concepts, reducing both the quality and the coverage of abstract clusters. We thus use the noun clusters derived with linguistic features as an approximation of SP classes. 5.2 Quantifying selectional preferences Once the SP classes have been obtained, we need to quantify the strength of association of a given verb with each of the classes. We adopt an information theoretic measure proposed by Resnik (1993) for this purpose. Resnik first measures selectional preference strength (SPS) of a verb in terms of Kullback-Leibler divergence between the distribution of noun classes occurring as arguments of this verb, p(c|v), and the prior distribution of the noun classes, p(c). SPSR(v) = �p(c |v) log p(c|p(c)), (4) c where R is the grammatical relation for which SPs are computed. SPS measures how strongly the predicate constrains its arguments. Selectional association of the verb with a particular argument class is then defined as a relative contribution of that argument class to the overall SPS of the </context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and information: A class-based approach to lexical relationships. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In ACL SIGLEX Workshop on Tagging Text with Lexical Semantics,</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="1578" citStr="Resnik, 1997" startWordPosition="235" endWordPosition="236">lasses of entities are more likely to fill the predicate’s argument slot than others. For instance, while the sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to th</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In ACL SIGLEX Workshop on Tagging Text with Lexical Semantics, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam Etzioni</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings ACL 2010,</booktitle>
<pages>424--434</pages>
<contexts>
<context position="2581" citStr="Ritter et al., 2010" startWordPosition="390" endWordPosition="393">uisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoy</context>
<context position="7545" citStr="Ritter et al. (2010)" startWordPosition="1149" endWordPosition="1152"> then, the field has moved toward automatic induction of SP classes from corpus data. Rooth et al. (1999) presented a probabilistic latent variable model of verb preferences. In their approach, verbargument pairs are generated from a latent variable, which represents a cluster of verb-argument interactions. The latent variable distribution and the probabilities that a latent variable generates the verb and the argument are learned from the data using Expectation Maximization (EM). The latent variables enable the model to recognise previously unseen verb-argument pairs. O´ S´eaghdha (2010) and Ritter et al. (2010) similarly model SPs within a latent variable framework, but use Latent Dirichlet Allocation (LDA) to learn the probability distributions, for single-argument and multi-argument preferences respectively. Pad´o et al. (2007) and Erk (2007) used similarity metrics to approximate selectional preference classes. Their underlying hypothesis is that a predicate-argument combination (p, a) is felicitous if the predicate p is frequently observed in the data with the arguments a&apos; similar to a. The systems compute similarities between distributional representations of arguments in a vector space. Bergsm</context>
</contexts>
<marker>Ritter, Etzioni, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam Etzioni, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings ACL 2010, pages 424– 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Rohrbach</author>
<author>Wei Qiu</author>
<author>Ivan Titov</author>
<author>Stefan Thater</author>
<author>Manfred Pinkal</author>
<author>Bernt Schiele</author>
</authors>
<title>Translating video content to natural language descriptions.</title>
<date>2013</date>
<booktitle>In Proceedings of ICCV</booktitle>
<contexts>
<context position="10639" citStr="Rohrbach et al., 2013" startWordPosition="1610" endWordPosition="1613">nd Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and ver</context>
</contexts>
<marker>Rohrbach, Qiu, Titov, Thater, Pinkal, Schiele, 2013</marker>
<rawString>Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. 2013. Translating video content to natural language descriptions. In Proceedings of ICCV 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP 2013,</booktitle>
<pages>1146--1157</pages>
<location>Seattle, WA.</location>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities. In Proceedings of EMNLP 2013, pages 1146–1157, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>104--111</pages>
<contexts>
<context position="2394" citStr="Rooth et al., 1999" startWordPosition="361" endWordPosition="364">tural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests tha</context>
<context position="7030" citStr="Rooth et al. (1999)" startWordPosition="1074" endWordPosition="1077">guistic and visual relations outperforms the purely linguistic model in both evaluation settings. 2 Related work 2.1 Selectional preference induction The widespread interest in automatic acquisition of SPs was triggered by the work of Resnik (1993), who treated SPs as probability distributions over all potential arguments of a predicate, rather than a single argument class assigned to the predicate. The original study used WordNet to define SP classes and to map the words in the corpus to those classes. Since then, the field has moved toward automatic induction of SP classes from corpus data. Rooth et al. (1999) presented a probabilistic latent variable model of verb preferences. In their approach, verbargument pairs are generated from a latent variable, which represents a cluster of verb-argument interactions. The latent variable distribution and the probabilities that a latent variable generates the verb and the argument are learned from the data using Expectation Maximization (EM). The latent variables enable the model to recognise previously unseen verb-argument pairs. O´ S´eaghdha (2010) and Ritter et al. (2010) similarly model SPs within a latent variable framework, but use Latent Dirichlet All</context>
<context position="29649" citStr="Rooth et al. (1999)" startWordPosition="4693" endWordPosition="4696">ixture of subject and direct object classes, as shown in Figure 5. Using a static model with a high ALM weight helps to avoid such errors and, therefore, leads to a better performance. In order to investigate the composition of the visual and linguistic datasets, we assess the average level of concreteness of the verbs and nouns present in the datasets. We use the concreteness ratings from the MRC Psycholinguistic Database (Wilson, 1988) for this purpose. In this database, nouns and Figure 6: WordNet top level class distributions for verbs in the visual and textual corpora Seen p Unseen p r r Rooth et al. (1999)* 0.455 0.487 0.479 0.520 Pad´o et al. (2007)* 0.484 0.490 0.398 0.430 O’Seaghdha (2010) 0.520 0.548 0.564 0.605 VSP 0.180 0.126 0.118 0.132 ISP (best) 0.548 0.699 0.342 0.429 LSP 0.512 0.688 0.412 0.559 Table 2: Comparison to other SP induction methods. *Results reported in O’Seaghdha (2010). verbs are rated for concreteness on a scale from 100 (highly abstract) to 700 (highly concrete). We map the verbs and nouns in our textual and visual corpora to their MRC concreteness scores. We then calculate a dataset-wide concreteness score as an average of the concreteness scores of individual verbs </context>
<context position="31117" citStr="Rooth et al. (1999)" startWordPosition="4933" endWordPosition="4936">d 363.4 (verbs). In order to compare the types of actions that are common in each of the datasets, we map the verbs to their corresponding top level classes in WordNet. Figure 6 shows the comparison of prominent verb classes in visual and textual data. One can see from the Figure that the visual dataset is well suited for representing motion, perception and contact, while abstract verbs related to e.g. communication, cognition, possession or change are more common in textual data. We also compare the performance of our models to existing SP induction methods: the EM-based clustering method of Rooth et al. (1999), the vector space similarity-based method of Pad´o et al. (2007) and the LDA topic modelling approach of O´ S´eaghdha (2010)1. The best ISP configuration 1Since Rooth et al.’s (1999) and Pad´o et al.’s (2007) models were not originally evaluated on the same dataset, we use the 956 (λLM = 0.9) outperforms all of these methods, as well as our own LSP, on the Seen dataset, confirming the positive contribution of visual features. However, it achieves less success on the Unseen data, where the methods of O´ S´eaghdha (2010) and Rooth et al. (1999) are leading. This result speaks in favour of laten</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings ofACL 1999, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Shamma</author>
</authors>
<title>One hundred million Creative Commons Flickr images for research.</title>
<date>2014</date>
<note>http: //labs.yahoo.com/news/yfcc100m/.</note>
<contexts>
<context position="11588" citStr="Shamma, 2014" startWordPosition="1764" endWordPosition="1765">e expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of association between a particular verb and a particular argument class. Visual data. For the visual features of our model, we mine the Yahoo! Webscope Flickr-100M dataset (Shamma, 2014). Flickr-100M contains 99.3 million images and 0.7 million videos with language tags annotated by users, enabling us to generalise SPs at a large scale. The tags reflect how humans describe objects and actions from a visual perspective. We first stem the tags and remove words that are absent in WordNet (typically named entities and misspellings), then identify their PoS based on their visual context and extract verb–noun cooccurrences. 4 Identifying visual verb-noun co-occurrence In the Flickr-100M dataset, tags are assigned to images and videos in the form of sets of words, rather than gramma</context>
</contexts>
<marker>Shamma, 2014</marker>
<rawString>David Shamma. 2014. One hundred million Creative Commons Flickr images for research. http: //labs.yahoo.com/news/yfcc100m/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Simone Teufel</author>
<author>Anna Korhonen</author>
</authors>
<date>2013</date>
<booktitle>Statistical Metaphor Processing. Computational Linguistics,</booktitle>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1928" citStr="Shutova et al., 2013" startWordPosition="283" endWordPosition="286">nd difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent varia</context>
</contexts>
<marker>Shutova, Teufel, Korhonen, 2013</marker>
<rawString>Ekaterina Shutova, Simone Teufel, and Anna Korhonen. 2013. Statistical Metaphor Processing. Computational Linguistics, 39(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Automatic metaphor interpretation as a paraphrasing task.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010,</booktitle>
<pages>1029--1037</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="32569" citStr="Shutova (2010)" startWordPosition="5171" endWordPosition="5172">o improve SP prediction across frequency bands. 7 Task-based evaluation In order to investigate the applicability of perceptually grounded SPs in wider NLP, we evaluate them in the context of an external semantic task – that of metaphor interpretation. Since metaphor is based on transferring imagery and knowledge across domains – typically from more familiar domains of physical experiences to the sphere of vague and elusive abstract thought – metaphor interpretation provides an ideal framework for testing perceptually grounded SPs. Our experiments rely on the metaphor interpretation method of Shutova (2010), in which text-derived SPs are a central component of the system. We replace the SP component with our LSP and ISP (λLM = 0.8) models and compare their performance in the context of metaphor interpretation. Shutova (2010) defined metaphor interpretation as a paraphrasing task, where literal paraphrases for metaphorical expressions are derived from corpus data using a set of statistical measures. For instance, their system interprets the metaphor “a carelessly leaked report” as “a carelessly disclosed report”. Focusing on metaphorical verbs in subject and direct object constructions, Shutova f</context>
<context position="34664" citStr="Shutova (2010)" startWordPosition="5509" endWordPosition="5510">is results in a set of paraphrases retaining the meaning of the metaphorical verb. However, some of them are still figuratively used. Shutova further applies an SP model to discriminate between figurative and literal paraphrases, treating a strong selectional preference fit as a likely indicator of literalness. The candidates are re-ranked by the SP model, emphasizing the verbs whose preferences the noun in the context matches best. We use LSP and ISP scores to perform this re-ranking step. We evaluate the performance of our models on this task using the metaphor paraphrasing gold standard of Shutova (2010). The dataset consists of 52 verb metaphors and their human-produced literal paraphrases. Following Shutova, we evaluate the performance in terms of mean average precision (MAP), which measures the ranking quality of GS paraphrases across the dataset. MAP is defined as follows: where M is the number of metaphorical expressions, Nj is the number of correct paraphrases for the metaphorical expression j, Pji is the precision at each correct paraphrase (the number of correct paraphrases among the top i ranks). As compared to the gold standard, ISP attains a MAP score of 0.65, outperforming both th</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Proceedings of NAACL 2010, pages 1029–1037, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Computational Approaches to Figurative Language.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge, UK.</institution>
<contexts>
<context position="4550" citStr="Shutova, 2011" startWordPosition="691" endWordPosition="692">hina, July 26-31, 2015. c�2015 Association for Computational Linguistics predicate-argument relations from visual data. In this paper, we propose the first SP model integrating information about predicate-argument interactions from text, images, and videos. We expect it to outperform purely text-based models of SPs, which suffer from two problems: topic bias and figurative uses of words. Such bias stems from the fact that we typically write about abstract topics and events, resulting in high coverage of abstract senses of words and comparatively lower coverage of the original physical senses (Shutova, 2011). For instance, the verb cut is used predominantly in the domains of economics and finance and its most frequent direct objects are cost and price, according to the British National Corpus (BNC) (Burnard, 2007). Predicate-argument distributions acquired from text thus tend to be skewed in favour of abstract domains and figurative uses, inadequately reflecting our daily experiences with cutting, which guide human acquisition of meaning. Integrating predicate-argument relations observed in the physical world (in the form of image and video descriptions) with the more abstract text-based relation</context>
</contexts>
<marker>Shutova, 2011</marker>
<rawString>Ekaterina Shutova. 2011. Computational Approaches to Figurative Language. Ph.D. thesis, University of Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014,</booktitle>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="3477" citStr="Silberer and Lapata, 2014" startWordPosition="526" endWordPosition="530">grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acquired not only from linguistic input but also from our experiences in the physical world. Multi-modal models of word meaning have thus enjoyed a growing interest in semantics (Bruni et al., 2014), outperforming purely text-based models in tasks such as similarity estimation (Bruni et al., 2014; Kiela et al., 2014), predicting compositionality (Roller and Schulte im Walde, 2013), and concept categorization (Silberer and Lapata, 2014). However, to date these approaches relied on low-level image features such as color histograms or SIFT keypoints to represent the meaning of isolated words. To the best of our knowledge, there has not yet been a multimodal semantic approach performing extraction of 950 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 950–960, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics predicate-argument relations from visual data. In this paper, we pro</context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of ACL 2014, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of semantic representation with visual attributes.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<pages>572--582</pages>
<contexts>
<context position="10223" citStr="Silberer et al. (2013)" startWordPosition="1551" endWordPosition="1554">igher score to an observed pair than to the unobserved one by a given margin. 2.2 Multi-modal methods in semantics Previous work has used multimodal data to determine distributional similarity or to learn multimodal embeddings that project multiple modalities into the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala e</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In Proceedings of ACL 2013, pages 572–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS 2013,</booktitle>
<pages>935--943</pages>
<contexts>
<context position="10482" citStr="Socher et al., 2013" startWordPosition="1586" endWordPosition="1589">to the same vector space. Some studies rely on extensions of LDA to obtain correlations between words and visual features (Feng and Lapata, 2010; Roller and Schulte im Walde, 2013). Bruni et al. (2012) integrated visual features into distributional similarity models using simple vector concatenation. Instead of generic visual features, Silberer et al. (2013) relied on supervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract lin</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Christopher D. Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Proceedings of NIPS 2013, pages 935–943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>15--1929</pages>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="2441" citStr="Sun and Korhonen, 2009" startWordPosition="369" endWordPosition="372"> 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun and Korhonen, 2009), distributional similarity metrics (Erk, 2007; Peirsman and Pad´o, 2010), latent variable models ( O´ S´eaghdha, 2010; Ritter et al., 2010), and neural networks (Van de Cruys, 2014). Little research, however, has been concerned with the sources of knowledge that underlie the learning of SPs. There is ample evidence in cognitive and neurolinguistics that our concept learning and semantic representation are grounded in perception and action (Barsalou, 1999; Glenberg and Kaschak, 2002; Barsalou, 2008; Aziz-Zadeh and Damasio, 2008). This suggests that word meaning and relational knowledge are acq</context>
<context position="14886" citStr="Sun and Korhonen, 2009" startWordPosition="2313" endWordPosition="2316"> words by selecting sense tuples x1j, x2j, ..., from which we can directly obtain the corresponding PoS information. Verb-noun co-occurrence information is then extracted from the PoS-tagged sets. 5 Selectional preference model 5.1 Acquisition of argument classes To address the issue of data sparsity, we generalise selectional preferences over argument classes, as opposed to individual arguments. We obtain SP classes by means of spectral clustering of nouns with lexico-syntactic features, which has been shown effective in previous lexical classification tasks (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009). Spectral clustering partitions the data, relying on a similarity matrix that records similarities between all pairs of data points. We use Jensen-Shannon divergence to measure the similarity between feature vectors for two nouns, wi and wj, defined as follows: 1 1 diS(wi, wj) = 2dKL(wi||m) + 2dKL(wj||m), (1) where dKL is the Kullback-Leibler divergence, and m is the average of wi and wj. We construct the similarity matrix S computing similarities Sij as Sij = exp(−diS(wi, wj)). The matrix S then encodes a similarity graph G (over our nouns), where Sij are the adjacency weights. The clusterin</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In Proceedings of EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A neural network approach to selectional preference acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Van de Cruys, 2014</marker>
<rawString>Tim Van de Cruys. 2014. A neural network approach to selectional preference acquisition. In Proceedings of EMNLP 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wiebke Wagner</author>
<author>Helmut Schmid</author>
<author>Sabine Schulte Im Walde</author>
</authors>
<title>Verb sense disambiguation using a predicate-argument clustering model.</title>
<date>2009</date>
<booktitle>In Proceedings of the CogSci Workshop on Semantic Space Models (DISCO).</booktitle>
<contexts>
<context position="1628" citStr="Wagner et al., 2009" startWordPosition="241" endWordPosition="244"> the predicate’s argument slot than others. For instance, while the sentences “The authors wrote a new paper.” and “The cat is eating your sausage!” sound natural and describe plausible real-life situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet</context>
</contexts>
<marker>Wagner, Schmid, Walde, 2009</marker>
<rawString>Wiebke Wagner, Helmut Schmid, and Sabine Schulte Im Walde. 2009. Verb sense disambiguation using a predicate-argument clustering model. In Proceedings of the CogSci Workshop on Semantic Space Models (DISCO).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Wilson</author>
</authors>
<title>The MRC Psycholinguistic Database:</title>
<date>1988</date>
<booktitle>Machine Readable Dictionary, Version 2. Behavioural Research Methods, Instruments and Computers,</booktitle>
<pages>20--6</pages>
<contexts>
<context position="29471" citStr="Wilson, 1988" startWordPosition="4662" endWordPosition="4663"> (such as subject relations in the direct object SP distribution and vice versa). The predicate-driven ISP output for direct object SPs of drink, for instance, contains a mixture of subject and direct object classes, as shown in Figure 5. Using a static model with a high ALM weight helps to avoid such errors and, therefore, leads to a better performance. In order to investigate the composition of the visual and linguistic datasets, we assess the average level of concreteness of the verbs and nouns present in the datasets. We use the concreteness ratings from the MRC Psycholinguistic Database (Wilson, 1988) for this purpose. In this database, nouns and Figure 6: WordNet top level class distributions for verbs in the visual and textual corpora Seen p Unseen p r r Rooth et al. (1999)* 0.455 0.487 0.479 0.520 Pad´o et al. (2007)* 0.484 0.490 0.398 0.430 O’Seaghdha (2010) 0.520 0.548 0.564 0.605 VSP 0.180 0.126 0.118 0.132 ISP (best) 0.548 0.699 0.342 0.429 LSP 0.512 0.688 0.412 0.559 Table 2: Comparison to other SP induction methods. *Results reported in O’Seaghdha (2010). verbs are rated for concreteness on a scale from 100 (highly abstract) to 700 (highly concrete). We map the verbs and nouns in </context>
</contexts>
<marker>Wilson, 1988</marker>
<rawString>M.D. Wilson. 1988. The MRC Psycholinguistic Database: Machine Readable Dictionary, Version 2. Behavioural Research Methods, Instruments and Computers, 20:6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations.</title>
<date>2014</date>
<journal>Transactions of the Association of Computational Linguistics –</journal>
<volume>2</volume>
<pages>67--78</pages>
<contexts>
<context position="10856" citStr="Young et al. (2014)" startWordPosition="1646" endWordPosition="1649">ervised learning to train 412 higher-level visual attribute classifiers. Applications of multimodal embeddings include zero-shot object detection, i.e. recognizing objects in images without training data for the object class (Socher et al., 2013; Frome et al., 2013; Lazaridou et al., 2014), and automatic generation of image captions (Kulkarni et al., 2013), video descriptions (Rohrbach et al., 2013), or tags (Srivastava et al., 2014). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) apply simplification rules to image captions, showing that the resulting hierarchy of mappings between natural language expressions and images can be used for entailment tasks. 3 Experimental data Textual data. We extract linguistic features for our model from the BNC. In particular, we parse the corpus using the RASP parser (Briscoe et al., 2006) and extract subject–verb and verb–object relations from its dependency output. These relations are then used as features for clustering to obtain SP classes, as well as to quantify the strength of association between a particular verb and a particul</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations. Transactions of the Association of Computational Linguistics – Volume 2, Issue 1, pages 67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="1823" citStr="Zanzotto et al., 2006" startWordPosition="267" endWordPosition="270">e situations, the sentences “The carrot ate the keys.” and “The law sang a driveway.” appear implausible and difficult to interpret, as the arguments do not satisfy the verbs’ common preferences. SPs provide generalisations about word meaning and use and find a wide range of applications in natural language processing (NLP), including word sense disambiguation (Resnik, 1997; McCarthy and Carroll, 2003; Wagner et al., 2009), resolving ambiguous syntactic attachments (Hindle and Rooth, 1993), semantic role labelling (Gildea and Jurafsky, 2002; Zapirain et al., 2010), natural language inference (Zanzotto et al., 2006; Pantel et al., 2007), and figurative language processing (Fass, 1991; Mason, 2004; Shutova et al., 2013; Li et al., 2013). Automatic acquisition of SPs from linguistic data has thus become an active area of research. The community has investigated a range of techniques to tackle data sparsity and to perform generalisation from observed arguments to their underlying types, including the use of WordNet synsets as SP classes (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 1999; Abney and Light, 1999; Ciaramita and Johnson, 2000), word clustering (Rooth et al., 1999; Bergsma et al., 2008; Sun a</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In Proceedings of COLING/ACL, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Be˜nat Zapirain</author>
<author>Eneko Agirre</author>
<author>Llu´ıs M`arquez</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Improving semantic role classification with selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>373--376</pages>
<marker>Zapirain, Agirre, M`arquez, Surdeanu, 2010</marker>
<rawString>Be˜nat Zapirain, Eneko Agirre, Llu´ıs M`arquez, and Mihai Surdeanu. 2010. Improving semantic role classification with selectional preferences. In Proceedings of NAACL HLT 2010, pages 373–376.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>