<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.891095">
Online Relative Margin Maximization for Statistical Machine Translation
</title>
<author confidence="0.76246">
Vladimir Eidelman Yuval Marton Philip Resnik
</author>
<affiliation confidence="0.978508666666667">
Computer Science Microsoft Linguistics
and UMIACS City Center Plaza and UMIACS
University of Maryland Bellevue, WA University of Maryland
</affiliation>
<address confidence="0.822092">
College Park, MD yuvalmarton@gmail.com College Park, MD
</address>
<email confidence="0.992828">
vlad@umiacs.umd.edu resnik@umd.edu
</email>
<sectionHeader confidence="0.994536" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999701947368421">
Recent advances in large-margin learning
have shown that better generalization can
be achieved by incorporating higher order
information into the optimization, such as
the spread of the data. However, these so-
lutions are impractical in complex struc-
tured prediction problems such as statis-
tical machine translation. We present an
online gradient-based algorithm for rela-
tive margin maximization, which bounds
the spread of the projected data while max-
imizing the margin. We evaluate our op-
timizer on Chinese-English and Arabic-
English translation tasks, each with small
and large feature sets, and show that our
learner is able to achieve significant im-
provements of 1.2-2 BLEU and 1.7-4.3
TER on average over state-of-the-art opti-
mizers with the large feature set.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976516666667">
The desire to incorporate high-dimensional sparse
feature representations into statistical machine
translation (SMT) models has driven recent re-
search away from Minimum Error Rate Training
(MERT) (Och, 2003), and toward other discrim-
inative methods that can optimize more features.
Examples include minimum risk (Smith and Eis-
ner, 2006), pairwise ranking (PRO) (Hopkins and
May, 2011), RAMPION (Gimpel and Smith, 2012),
and variations of the margin-infused relaxation al-
gorithm (MIRA) (Watanabe et al., 2007; Chiang et
al., 2008; Cherry and Foster, 2012). While the ob-
jective function and optimization method vary for
each optimizer, they can all be broadly described
as learning a linear model, or parameter vector w,
which is used to score alternative translation hy-
potheses.
In every SMT system, and in machine learn-
ing in general, the goal of learning is to find a
model that generalizes well, i.e. one that will yield
good translations for previously unseen sentences.
However, as the dimension of the feature space in-
creases, generalization becomes increasingly diffi-
cult. Since only a small portion of all (sparse) fea-
tures may be observed in a relatively small fixed
set of instances during tuning, we are prone to
overfit the training data. An alternative approach
for solving this problem is estimating discrimina-
tive feature weights directly on the training bi-
text (Tillmann and Zhang, 2006; Blunsom et al.,
2008; Simianer et al., 2012), which is usually sub-
stantially larger than the tuning set, but this is com-
plementary to our goal here of better generaliza-
tion given a fixed size tuning set.
In order to achieve that goal, we need to care-
fully choose what objective to optimize, and how
to perform parameter estimation of w for this ob-
jective. We focus on large-margin methods such
as SVM (Joachims, 1998) and passive-aggressive
algorithms such as MIRA. Intuitively these seek
a w such that the separating distance in geomet-
ric space of two hypotheses is at least as large as
the cost incurred by selecting the incorrect one.
This criterion performs well in practice at find-
ing a linear separator in high-dimensional feature
spaces (Tsochantaridis et al., 2004; Crammer et
al., 2006).
Now, recent advances in machine learning have
shown that the generalization ability of these
learners can be improved by utilizing second or-
der information, as in the Second Order Percep-
tron (Cesa-Bianchi et al., 2005), Gaussian Margin
Machines (Crammer et al., 2009b), confidence-
weighted learning (Dredze and Crammer, 2008),
AROW (Crammer et al., 2009a; Chiang, 2012)
and Relative Margin Machines (RMM) (Shiv-
aswamy and Jebara, 2009b). The latter, RMM,
was introduced as an effective and less computa-
tionally expensive way to incorporate the spread
of the data – second order information about the
</bodyText>
<page confidence="0.944515">
1116
</page>
<note confidence="0.9134575">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99984258">
distance between hypotheses when projected onto
the line defined by the weight vector w.
Unfortunately, not all advances in machine
learning are easy to apply to structured prediction
problems such as SMT; the latter often involve la-
tent variables and surrogate references, resulting
in loss functions that have not been well explored
in machine learning (Mcallester and Keshet, 2011;
Gimpel and Smith, 2012). Although Shivaswamy
and Jebara extended RMM to handle sequen-
tial structured prediction (Shivaswamy and Jebara,
2009a), their batch approach to quadratic opti-
mization, using existing off-the-shelf QP solvers,
does not provide a practical solution: as Taskar et
al. (2006) observe, “off-the-shelf QP solvers tend
to scale poorly with problem and training sam-
ple size” for structured prediction problems.. This
motivates an online gradient-based optimization
approach—an approach that is particularly attrac-
tive because its simple update is well suited for ef-
ficiently processing structured objects with sparse
features (Crammer et al., 2012).
The contributions of this paper include (1) in-
troduction of a loss function for structured RMM
in the SMT setting, with surrogate reference trans-
lations and latent variables; (2) an online gradient-
based solver, RM, with a closed-form parameter
update to optimize the relative margin loss; and
(3) an efficient implementation that integrates well
with the open source cdec SMT system (Dyer et
al., 2010).1 In addition, (4) as our solution is not
dependent on any specific QP solver, it can be
easily incorporated into practically any gradient-
based learning algorithm.
After background discussion on learning in
SMT (§2), we introduce a novel online learning al-
gorithm for relative margin maximization suitable
for SMT (§3). First, we introduce RMM (§3.1) and
propose a latent structured relative margin objec-
tive which incorporates cost-augmented hypothe-
sis selection and latent variables. Then, we de-
rive a simple closed-form online update necessary
to create a large margin solution while simulta-
neously bounding the spread of the projection of
the data (§3.2). Chinese-English translation exper-
iments show that our algorithm, RM, significantly
outperforms strong state-of-the-art optimizers, in
both a basic feature setting and high-dimensional
(sparse) feature space (§4). Additional Arabic-
English experiments further validate these results,
</bodyText>
<footnote confidence="0.90288">
1https://github.com/veidel/cdec
</footnote>
<bodyText confidence="0.99948">
even where previously MERT was shown to be ad-
vantageous (§5). Finally, we discuss the spread
and other key issues of RM (§6), and conclude
with discussion of future work (§7).
</bodyText>
<sectionHeader confidence="0.63572" genericHeader="method">
2 Learning in SMT
</sectionHeader>
<bodyText confidence="0.94237325">
Given an input sentence in the source language
x E X, we want to produce a translation y E Y(x)
using a linear model parameterized by a weight
vector w:
</bodyText>
<equation confidence="0.992377">
(y*, d*) = arg max wTf(x, y, d)
(y,d)∈Y(x),D(x)
</equation>
<bodyText confidence="0.999011892857143">
where wTf(x, y, d) is the weighted feature scor-
ing function, hereafter s(x, y, d), and Y(x) is the
space of possible translations of x. While many
derivations d E D(x) can produce a given transla-
tion, we are only able to observe y; thus we model
d as a latent variable. Although our models are
actually defined over derivations, they are always
paired with translations, so our feature function
f(x, y, d) is defined over derivation–translation
pairs.2 The learning goal is then to estimate w.
The instability of MERT in larger feature
sets (Foster and Kuhn, 2009; Hopkins and May,
2011), has motivated many alternative tuning
methods for SMT. These include strategies based
on batch log-linear models (Tillmann and Zhang,
2006; Blunsom et al., 2008), as well as the in-
troduction of online linear models (Liang et al.,
2006a; Arun and Koehn, 2007).
Recent batch optimizers, PRO and RAMPION,
and Batch-MIRA (Cherry and Foster, 2012), have
been partly motivated by existing MT infrastruc-
tures, as they iterate between decoding the entire
tuning set and optimizing the parameters. PRO
considers tuning a classification problem and em-
ploys a binary classifier to rank pairs of outputs.
RAMPION aims to address the disconnect between
MT and machine learning by optimizing a struc-
tured ramp loss with a concave-convex procedure.
</bodyText>
<subsectionHeader confidence="0.980028">
2.1 Large-Margin Learning
</subsectionHeader>
<bodyText confidence="0.995985">
Online large-margin algorithms, such as MIRA,
have also gained prominence in SMT, thanks to
their ability to learn models in high-dimensional
feature spaces (Watanabe et al., 2007; Chiang et
al., 2009). The usual presentation of MIRA’s opti-
mization problem is given as a quadratic program:
</bodyText>
<footnote confidence="0.847569">
2We may omit d in some equations for clarity.
</footnote>
<page confidence="0.842041">
1117
</page>
<equation confidence="0.882619">
2||w − wt||2 + Cξi
1
s.t. s(xi, yi, d) − s(xi, y0, d) ≥ Δi(y0) − ξi
</equation>
<bodyText confidence="0.998493380952381">
where y0 is the single most violated constraint, the
cost Δi(y) is computed using an external measure
of quality, such as 1-BLEU(yi, y), and a slack vari-
able ξi is introduced to allow for non-separable
instances. C acts as a regularization parameter,
trading off between margin maximization and con-
straint violations.
While solving the optimization problem relies
on computing the margin between the correct out-
put yi, and y0, in SMT our decoder is often inca-
pable of producing the reference translation, i.e.
yi ∈/ Y(xi). We must instead resort to selecting a
surrogate reference, y+ ∈ Y(xi). This issue has
recently received considerable attention (Liang
et al., 2006a; Eidelman, 2012; Chiang, 2012),
with preference given to surrogate references ob-
tained through cost-diminished hypothesis selec-
tion. Thus, y+ is selected based on a combination
of model score and error metric from the k-best
list produced by our current model. A similar se-
lection is made for the cost-augmented hypothesis
</bodyText>
<equation confidence="0.969108363636364">
y− ∈ Y(xi):
(y+, d+) ← arg max s(xi, y, d) − Δi(y)
(y,d)∈Y(xi),D(xi)
(y−, d−) ← arg max s(xi, y, d) + Δi(y)
(y,d)∈Y(xi),D(xi)
In this setting, the optimization problem be-
comes:
1
wt+1 = arg min 2||w − wt||2 + Cξi
w
s.t. δs(xi, y+, y−) ≥ Δi(y−) − Δi(y+) − ξi
</equation>
<bodyText confidence="0.5585015">
where
y+,
y+,
This leads to a varian
</bodyText>
<equation confidence="0.994633214285714">
δs(xi,
y−)=s(xi,
d+)-s(xi,y−,d−)
t of the structured ramp
loss to be optimized:
max (s (xi,y+, d+)
Y(
i),D(
−Δ
∈
x
xi)
(s(xi,y−,d−) + Δi(y−))
(3)
</equation>
<bodyText confidence="0.999959142857143">
The passive-aggressive update (Crammer et al.,
2006), which is used to solve this problem, up-
dates w on each round such that the score of the
correct hypothesis y+ is greater than the score of
the incorrect y− by a margin at least as large as the
cost incurred by predicting the incorrect hypothe-
sis, while keeping the change to w small.
</bodyText>
<figure confidence="0.579047666666667">
(a)
(b)
darker soli
d line, respectively.
(yw, dw)
arg
y,
←
min(y,d)∈Y(xi),D(xi)s(xi,
</figure>
<bodyText confidence="0.977878">
d),
after projecting both onto the line defined by the
weight vector w. For each y0, this projection is
conveniently given by s(xi, y0, d), thus the spread
is calculated as δs(xi, y+, yw).
RMM was introduced as a generalization over
SVM that incorporates both the margin constraint
</bodyText>
<equation confidence="0.996572222222222">
wt+1 = arg min
w
(1)
+ max
(2)
` =
i (y+))
(y+,d+)
(y−,d−)∈Y(xi),D(xi)
</equation>
<figureCaption confidence="0.988143333333333">
Figure 1: (a) RM and large margin solution comparison and
(b) the spread of the projections given by each. RM and large
margin solutions are shown with a darker dotted line and a
</figureCaption>
<sectionHeader confidence="0.468793" genericHeader="method">
3 The Relative Margin Machine in SMT
</sectionHeader>
<subsectionHeader confidence="0.91801">
3.1 Relative Margin Machine
</subsectionHeader>
<bodyText confidence="0.988534285714286">
The margin, the distance between the correct
hypothesis and incorrect one, is defined by
y+, d+) and
It is maxi-
mized by minimizing the norm in SVM, or
analogously, the proximity constraint in MIRA:
arg
</bodyText>
<equation confidence="0.851859333333333">
2
w
wt
</equation>
<bodyText confidence="0.997173866666667">
2. However, theoretical re-
sults supporting large-margin learning, such as the
VC-dimension (Vapnik, 1995) or the Rademacher
bound (Bartlett and Mendelson, 2003) consider
measures of complexity, in addition to the empir-
ical performance, when describing future predic-
tive ability. The measures of complexity usually
take the form of some value on the radius of the
data, such as the ratio of the radius of the data to
the margin (Shivaswamy and Jebara, 2009a). As
radius is a way of measuring spread in any pro-
jection direction, here we will specifically be in-
terested in the the spread of the data as measured
after the projection defined by the learned model
s(xi,
</bodyText>
<equation confidence="0.865834">
s(xi,y−,d−).
minw
||
−
||
w.
</equation>
<bodyText confidence="0.984444">
More formally, the spread is the dis-
tance between y+, and the worst candidate
</bodyText>
<page confidence="0.937547">
1118
</page>
<bodyText confidence="0.996885277777778">
and information regarding the spread of the data.
The relative margin is the ratio of the absolute,
or maximum margin, to the spread of the pro-
jected data. Thus, the RMM learns a large mar-
gin solution relative to the spread of the data, or
in other words, creates a max margin while si-
multaneously bounding the spread of the projected
data. As a concrete example, consider the plot
shown in Figure 1(a), with hypotheses represented
by two-dimensional feature vectors. The point
marked with a circle in the upper right represents
f(xi, y+), while all other squares represent alter-
native incorrect hypotheses f(xi, y&apos;). The large
margin decision boundary is shown with a darker
solid line, while the relative margin solution is
shown with a darker dotted line. The lighter lines
parallel to each define the margins, with the square
at the intersection being f(xi, y−). The bottom
portion of Figure 1(b) presents an alternative view
of each solution, showing the projections of the
hypotheses given the learned model of each. No-
tice that with a large margin solution, although the
distance between y+ and y− is greater, the points
are highly spread, extending far to the left of the
decision boundary.
In contrast, with a relative margin, although
we have a smaller absolute margin, the spread is
smaller, all points being within a smaller distance E
of the decision boundary. The higher the spread of
the projection, the higher the variance of the pro-
jected points, and the greater the likelihood that
we will mislabel a new instance, since the high
variance projections may cross the learned deci-
sion boundary. In higher dimensions, accounting
for the spread becomes even more crucial, as will
be discussed in Section 6.3
Although RMM is theoretically well-founded
and improves practical performance over large-
margin learning in the settings where it was intro-
duced, it is unsuitable for most complex structured
prediction in NLP. Nonetheless, since structured
RMM is a generalization of Structured SVM,
which shares its underlying objective with MIRA,
our intuition is that SMT should be able to benefit
as well. But to take advantage of the second-order
information RMM utilizes for increased general-
izability in SMT, we need a computationally effi-
3The motivation of confidence-weighted estima-
tion (Dredze and Crammer, 2008) and AROW (Crammer
et al., 2009a) is related in spirit. They use second-order
information in the form of a distribution over weights to
change the maximum margin solution.
cient optimization procedure that does not require
batch training or an off-the-shelf QP solver.
</bodyText>
<subsectionHeader confidence="0.997781">
3.2 RM Algorithm
</subsectionHeader>
<bodyText confidence="0.99970975">
We address the above-mentioned limitations by in-
troducing a novel online learning algorithm for
relative margin maximization, RM. The relative
margin solution is obtained by maximizing the
same margin as Equation (2), but now with re-
spect to the distance between y+, and the worst
candidate yw. Thus, the relative margin dictates
trading-off between a large margin as before, and
a small spread of the projection, in other words,
bounding the distance between y+ and yw. The
additional computation required, namely, obtain-
ing yw, is efficient to perform, and has likely al-
ready happened while obtaining the k-best deriva-
tions necessary for the margin update. The online
latent structured soft relative margin optimization
problem is then:
</bodyText>
<equation confidence="0.9991434">
1
wt+1 = arg min 2||w − wt||2 + Cξi + Dτi
w
s.t.: δs(xi, y+, y−) ? Δi(y−) − Δi(y+) − ξi
− B − τi &lt; δs(xi, y+, yw) &lt; B + τi
</equation>
<bodyText confidence="0.999768166666667">
where additional bounding constraints are added
to the usual margin constraints in order to contain
the spread by bounding the difference in projec-
tions. B is an additional parameter; it controls
the spread, trading off between margin maximiza-
tion and spread minimization. Notice that when
B —� oc, the bounding constraints disappear, and
we are left with the original problem in Equa-
tion (2). D, which plays an analogous role to C,
allows penalized violations of the bounding con-
straints.
The dual of Equation (4) can be derived as:
</bodyText>
<equation confidence="0.991239846153846">
Xβy − B β∗y
y∈Y(xi)
−2
1 ~ X
y∈Y(xi) Xαyωi(y+, y) − βyωi(y+, y)
y∈Y(xi)
+ X β∗yωi(y+, y),
y∈Y(xi)
X αy0ωj (y +, y0) − X βy0ωj(y+, y0)
y0∈Y(xj ) y0∈Y(xj)
+
y0∈Y(xj )
X �β∗ y0ωj(y+, y0)
</equation>
<bodyText confidence="0.999499">
where the α Lagrange multiplier corresponds
to the standard margin constraint, while β and
</bodyText>
<equation confidence="0.5103605">
max XL = Xαy − B
α,β,β∗ y∈Y(xi) y∈Y(xi)
</equation>
<page confidence="0.962504">
1119
</page>
<bodyText confidence="0.99478475">
β* each correspond to a bounding constraint,
and wi(y+, y&apos;) corresponds to the difference of
f(xi, y+, d+) and f(xi, y&apos;, d&apos;). The weight up-
date can then be obtained from the dual variables:
</bodyText>
<equation confidence="0.998597">
X X X
αywi(y+, y) − Oywi(y+, y) + O∗ywi(y+, y)
(6)
</equation>
<bodyText confidence="0.988119130434783">
The dual in Equation (5) can be optimized us-
ing a cutting plane algorithm, an effective method
for solving a relaxed optimization problem in
the dual, used in Structured SVM, MIRA, and
RMM (Tsochantaridis et al., 2004; Chiang, 2012;
Shivaswamy and Jebara, 2009a). The cutting
plane presented in Alg. 1 decomposes the overall
problem into subproblems which are solved inde-
pendently by creating working sets Sji , which cor-
respond to the largest violations of either the mar-
gin constraint, or bounding constraints, and itera-
tively satisfying the constraints in each set.
The cutting plane in Alg. 1 makes use of the
the closed-form gradient-based updates we de-
rived for RM presented in Alg. 2. The updates
amount to performing a subgradient descent step
to update w in accordance with the constraints.
Since the constraint matrix of the dual program is
not strictly decomposable across constraint types,
we are in effect solving an approximation of the
original problem.
Algorithm 1 RM Cutting Plane Algorithm
(adapted from (Shivaswamy and Jebara, 2009a))
</bodyText>
<figureCaption confidence="0.389342">
Require: ith training example (xi, yi), weight w, margin
reg. C, bound B, bound reg. D, E, EB
</figureCaption>
<equation confidence="0.823399363636364">
i ~y+
1: S1 i ~y+ , S2 i ~y+ , S3
2: repeat
3: H(y) := Di(y) − Di(y+) − 6s(xi, y+, y)
4: y1 arg maxy∈Y(xi) H(y)
5: y2 arg maxy∈Y(xi) G(y) := 6s(xi, y+, y)
6: y3 arg miny∈Y(xi) −G(y)
7: ξ max {0, maxy∈Si H(y)}
8: V1 H(y1) − ξ − E
9: V2 G(y2) − B − EB
10: V3 −G(y3) − B − EB
</equation>
<listItem confidence="0.597442">
11: j arg maxj0∈{1,2,3} Vj0
12: if Vj &gt; 0 then
13: Sji Sji U {yj}
14: OPTIMIZE(w, S1i , S2i , S3i , C, B) &gt; see Alg. 2
15: end if
16: until S1i , S2i , S3i do not change
</listItem>
<bodyText confidence="0.9982305">
Alternatively, we could utilize a passive-
aggressive updating strategy (Crammer et al.,
2006), which would simply bypass the cutting
plane and select the most violated constraint for
</bodyText>
<figure confidence="0.72767974">
Algorithm 2 RM update with α, β, β*
1: procedure OPTIMIZE(w, S1i , S2i , S3i , C, B)
2: while w changes do
�� &gt; 1 then
3: if ~~S1 i
4: UPDATEMARGIN(w, S1i , C)
5: end if
�� &gt; 1 then
6: if ~~S2 i
7: UPDATEUPPERBOUND(w, S2i , B)
8: end if
�� &gt; 1 then
9: if ~~S3 i
10: UPDATELOWERBOUND(w, S3i , B)
11: end if
12: end while
13: end procedure
14: procedure UPDATEMARGIN(w, S1i , C)
15: αy 0 for all y E S1i
16: α + C
yi
17: for n 1...MaxIter do
18: Select two constraints y, y0 from S1i
&apos;Yα Δi(y0)−Δi(y)−δs(xi, y, y0)
||ω(y,y0)||2
20: &apos;Yα max(−αy, min(αy0, &apos;Yα))
21: αy αy + &apos;Yα ; α0y α0y − &apos;Yα
22: w w + &apos;Yα(w(y, y0))
23: end for
24: end procedure
25: procedure UPDATEUPPERBOUND(w, S2i , B)
26: Oy 0 for all y E S2i
27: for n 1...MaxIter do
28: Select one constraint y from S2i
&apos;Yβ max(0, B−δs(xi,y+,y)
29: ||ω(y+,y)||2 )
30: Oy Oy + &apos;Yβ
31: w w − &apos;Yβ(w(y+, y))
32: end for
33: end procedure
34: procedure UPDATELOWERBOUND(w, S3i , B)
35: O∗y 0 for all y E S3i
36: for n 1...MaxIter do
37: Select one constraint y from S3i
&apos;Yβ∗ max(0, −B−δs(xi,y+,y)
38: ||ω(y+,y)||2 )
39: O∗y O∗y + &apos;Yβ∗
40: w w + &apos;Yβ∗(w(y+, y))
41: end for
42: end procedure
</figure>
<bodyText confidence="0.998562133333333">
each set, if there is one, and perform the corre-
sponding parameter updates in Alg. 2. We re-
fer to the resulting passive-aggressive algorithm as
RM-PA, and the cutting plane version as RM-CP.
Preliminary experiments showed that RM-PA per-
forms on par with RM-CP, thus RM-PA is the one
used in the empirical evaluation below.
A graphical depiction of the passive-aggressive
RM update is presented in Figure 2. The upper
right circle represents y+, while all other squares
represent alternative hypotheses y&apos;. As in the stan-
dard MIRA solution, we select the maximum mar-
gin constraint violator, y−, shown as the triangle,
and update such that the margin is greater than the
cost. Additionally, we select the maximum bound-
</bodyText>
<page confidence="0.934957">
1120
</page>
<figure confidence="0.848792">
Model Score
</figure>
<figureCaption confidence="0.891309833333333">
Figure 2: RM update with margin and bounding con-
straints. The diagonal dotted line depicts cost–margin equi-
librium. The vertical gray dotted line depicts the bound B.
White arrows indicate updates triggered by constraint viola-
tions. Squares are data points in the k-best list not selected
for update in this round.
</figureCaption>
<table confidence="0.958843909090909">
task Corpus Sentences Tokens
En Zh/Ar
training 1.6M 44.4M 40.4M
tune (MT06) 1664 48k 39k
Zh-En MT03 919 28k 24k
MT05 1082 35k 33k
training 1M 23.7M 22.8M
tune (MT06) 1797 55k 49k
Ar-En MT05 1056 36k 33k
MT08 1360 51k 45k
4-gram LM 24M 600M –
</table>
<tableCaption confidence="0.999349">
Table 1: Corpus statistics
</tableCaption>
<bodyText confidence="0.998385333333333">
ing constraint violator, yw, shown as the upside-
down triangle, and update so the distance from y+
is no greater than B.
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.982725">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.99998424">
To evaluate the advantage of explicitly accounting
for the spread of the data, we conducted several
experiments on two Chinese-English translation
test sets, using two different feature sets in each.
For training we used the non-UN and non-HK
Hansards portions of the NIST training corpora,
which was segmented using the Stanford seg-
menter (Tseng et al., 2005). The data statistics are
summarized in the top half of Table 1. The English
data was lowercased, tokenized and aligned using
GIZA++ (Och and Ney, 2003) to obtain bidirec-
tional alignments, which were symmetrized using
the grow-diag-final-and method (Koehn
et al., 2003). We trained a 4-gram LM on the
English side of the corpus with additional words
from non-NYT and non-LAT, randomly selected
portions of the Gigaword v4 corpus, using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996). We used cdec (Dyer et al., 2010) as our
hierarchical phrase-based decoder, and tuned the
parameters of the system to optimize BLEU (Pap-
ineni et al., 2002) on the NIST MT06 corpus.
We applied several competitive optimizers as
baselines: hypergraph-based MERT (Kumar et al.,
2009), k-best variants of MIRA (Crammer et al.,
2006; Chiang et al., 2009), PRO (Hopkins and
May, 2011), and RAMPION (Gimpel and Smith,
2012). The size of the k-best list was set to 500
for RAMPION, MIRA and RM, and 1500 for PRO,
with both PRO and RAMPION utilizing k-best ag-
gregation across iterations. RAMPION settings
were as described in (Gimpel and Smith, 2012),
and PRO settings as described in (Hopkins and
May, 2011), with PRO requiring regularization
tuning in order to be competitive with the other op-
timizers. MIRA and RM were run with 15 paral-
lel learners using iterative parameter mixing (Mc-
Donald et al., 2010). All optimizers were imple-
mented in cdec and use the same system config-
uration, thus the only independent variable is the
optimizer itself. We set C to 0.01, and MaxIter
to 100. We selected the bound step size D, based
on performance on a held-out dev set, to be 0.01
for the basic feature set and 0.1 for the sparse fea-
ture set. The bound constraint B was set to 1.4 The
approximate sentence-level BLEU cost Δi is com-
puted in a manner similar to (Chiang et al., 2009),
namely, in the context of previous 1-best transla-
tions of the tuning set. All results are averaged
over 3 runs.
</bodyText>
<subsectionHeader confidence="0.994835">
4.2 Feature Sets
</subsectionHeader>
<bodyText confidence="0.999975">
We experimented with a small (basic) feature set,
and a large (sparse) feature set. For the small
feature set, we use 14 features, including a lan-
guage model, 5 translation model features, penal-
ties for unknown words, the glue rule, and rule
arity. For experiments with a larger feature set,
we introduced additional lexical and non-lexical
sparse Boolean features of the form commonly
found in the literature (Chiang et al., 2009; Watan-
</bodyText>
<footnote confidence="0.8400194">
4We also conducted an investigation into the setting of the
B parameter. We explored alternative values for B, as well
as scaling it by the current candidate’s cost, and found that
the optimizer is fairly insensitive to these changes, resulting
in only minor differences in BLEU.
</footnote>
<figure confidence="0.993141333333333">
BLEU Score
B
dist &gt; B
Bounding Constraint
dist
cost &gt; margin
Margin Constraint
margin
cost
</figure>
<page confidence="0.885318">
1121
</page>
<equation confidence="0.8570275">
Optimizer Zh Ar
MIRA 35k 37k
PRO 95k 115k
RAMPION 22k 24k
RM 30k 32k
Active+Inactive 3.4M 4.9M
</equation>
<tableCaption confidence="0.98641">
Table 2: Active sparse feature templates
</tableCaption>
<bodyText confidence="0.992944047619048">
abe et al., 2007; Simianer et al., 2012).
Non-lexical features include structural distor-
tion, which captures the dependence between re-
ordering and the size of a filler, and rule shape,
which bins grammar rules by their sequence of
terminals and nonterminals (Chiang et al., 2008).
Lexical features on rules include rule ID, which
fires on a specific grammar rule. We also in-
troduce context-dependent lexical features for the
300 most frequent aligned word pairs (f,e) in the
training corpus, which fire on triples (f,e,f+1) and
(f,e,f−1), capturing when we see f aligned to e,
with f+1 and f−1 occurring to the right or left of f,
respectively. All other words fall into the default
(unk) feature bin. In addition, we have insertion
and deletion features for the 150 most frequently
unaligned target and source words. These feature
templates resulted in a total of 3.4 million possible
features, of which only a fraction were active for
the respective tuning set and optimizer, as shown
in Table 2.
</bodyText>
<subsectionHeader confidence="0.8838">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.898675444444445">
As can be seen from the results in Table 3, our
RM method was the best performer in all Chinese-
English tests according to all measures – up to 1.9
BLEU and 6.6 TER over MIRA – even though we
only optimized for BLEU.5 Surprisingly, it seems
that MIRA did not benefit as much from the sparse
features as RM. The results are especially notable
for the basic feature setting – up to 1.2 BLEU and
4.6 TER improvement over MERT – since MERT
has been shown to be competitive with small num-
bers of features compared to high-dimensional op-
timizers such as MIRA (Chiang et al., 2008).
For the tuning set, the decoder performance was
consistently the lowest with RM, compared to the
5In the small feature set RAMPION yielded similar best
BLEU scores, but worse TER. In preliminary experiments
with a smaller trigram LM, our RM method consistently
yielded the highest scores in all Chinese-English tests – up
to 1.6 BLEU and 6.4 TER from MIRA, the second best per-
former.
other optimizers. We believe this is due to the
RM bounding constraint being more resistant to
overfitting the training data, and thus allowing for
improved generalization. Conversely, while PRO
had the second lowest tuning scores, it seemed to
display signs of underfitting in the basic and large
feature settings.
</bodyText>
<sectionHeader confidence="0.957396" genericHeader="method">
5 Additional Experiments
</sectionHeader>
<bodyText confidence="0.999924068965517">
In order to explore the applicability of our ap-
proach to a wider range of languages, we also eval-
uated its performance on Arabic-English transla-
tion. All experimental details were the same as
above, except those noted below.
For training, we used the non-UN portion of the
NIST training corpora, which was segmented us-
ing an HMM segmenter (Lee et al., 2003). Dataset
statistics are given in the bottom part of Table 1.
The sparse feature templates resulted here in a to-
tal of 4.9 million possible features, of which again
only a fraction were active, as shown in Table 2.
As can be seen in Table 4, in the smaller feature
set, RM and MERT were the best performers, with
the exception that on MT08, MIRA yielded some-
what better (+0.7) BLEU but a somewhat worse
(-0.9) TER score than RM.
On the large feature set, RM is again the best
performer, except, perhaps, a tied BLEU score
with MIRA on MT08, but with a clear 1.8 TER
gain. In both Arabic-English feature sets, MIRA
seems to take the second place, while RAMPION
lags behind, unlike in Chinese-English (§4).6
Interestingly, RM achieved substantially higher
BLEU precision scores in all tests for both lan-
guage pairs. However, this was also usually cou-
pled had a higher brevity penalty (BP) than MIRA,
with the BP increasing slightly when moving to
the sparse setting.
</bodyText>
<sectionHeader confidence="0.997934" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999035">
The trend of the results, summarized as RM gain
over other optimizers averaged over all test sets, is
presented in Table 5. RM shows clear advantage
in both basic and sparse feature sets, over all other
state-of-the-art optimizers. The RM gains are no-
tably higher in the large feature set, which we take
</bodyText>
<footnote confidence="0.632854833333333">
6In our preliminary experiments with the smaller trigram
LM, MERT did better on MT05 in the smaller feature set, and
MIRA had a small advantage in two cases. RAMPION per-
formed similarly to RM on the smaller feature set. RM’s loss
was only up to 0.8 BLEU (0.7 TER) from MERT or MIRA,
while its gains were up to 1.7 BLEU and 2.1 TER over MIRA.
</footnote>
<page confidence="0.960265">
1122
</page>
<table confidence="0.999703875">
Optimizer Tune Small (basic) feature set Tune Large (sparse) feature set
MT03 MT05 MT03 MT05
&apos;BLEU &apos;BLEU , TER &apos;BLEU , TER &apos;BLEU &apos;BLEU , TER &apos;BLEU , TER
MERT 35.4 35.8 60.8 32.4 63.9 - - - - -
MIRA 35.5 35.8 61.1 32.1 64.6 36.6 35.9 60.6 32.1 64.1
PRO 34.1 36.0 60.2 31.7 63.4 35.7 34.8 56.1 31.4 59.1
RAMPION 35.1 36.5 58.6 33.0 61.3 36.7 36.9 57.7 33.3 60.6
RM 31.3 36.5 56.4 33.6 59.3 33.2 37.5 54.6 34.0 57.5
</table>
<tableCaption confidence="0.997673">
Table 3: Performance on Zh-En with basic (left) and sparse (right) feature sets on MT03 and MT05.
</tableCaption>
<table confidence="0.9999395">
Optimizer Tune Small (basic) feature set Tune Large (sparse) feature set
MT05 MT08 MT05 MT08
&apos;BLEU &apos;BLEU , TER &apos;BLEU , TER &apos;BLEU &apos;BLEU , TER &apos;BLEU , TER
MERT 43.8 53.3 40.2 41.0 50.7 - - - - -
MIRA 43.0 52.8 40.8 41.3 50.6 44.4 53.4 40.1 41.8 50.2
PRO 41.5 51.3 41.5 39.4 51.5 46.8 53.2 40.0 41.4 49.7
RAMPION 42.4 52.0 40.8 40.0 50.8 44.6 52.9 40.4 41.0 50.4
RM 38.5 53.3 39.8 40.6 49.7 43.0 55.3 37.5 41.8 48.4
</table>
<tableCaption confidence="0.997516">
Table 4: Performance on Ar-En with basic (left) and sparse (right) feature sets on MT05 and MT08.
</tableCaption>
<table confidence="0.999548833333333">
Optimizer Small set Large set
BLEU TER BLEU TER
MERT 0.4 2.6 - -
MIRA 0.5 3.0 1.4 4.3
PRO 1.4 2.9 2.0 1.7
RAMPION 0.6 1.6 1.2 2.8
</table>
<tableCaption confidence="0.995117">
Table 5: RM gain over other optimizers averaged
over all test sets.
</tableCaption>
<bodyText confidence="0.999881">
as an indication for the importance of bounding
the spread.
Spread analysis: For RM, the average spread
of the projected data in the Chinese-English small
feature set was 0.9±3.6 for all tuning iterations,
and 0.7±2.9 for the iteration with the highest de-
coder performance. In comparison, the spread of
the data for MIRA was 5.9±20.5 for the best it-
eration. In the sparse setting, RM had an aver-
age spread of 0.9±2.4 for the best iteration, while
MIRA had a spread of 14.0±31.1. Similarly,
on Arabic-English, RM had a spread of 0.7±2.4
in the small setting, and 0.82±1.4 in the sparse
setting, while MIRA’s spread was 9.4±26.8 and
11.4±22.1, for the small and sparse settings, re-
spectively. Notice that the average spread for RM
stays about the same when moving to higher di-
mensions, with the variance decreasing in both
cases. For MIRA, however, the average spread
increases in both cases, with the variance being
much higher than RM. For instance, observe that
the spread of MIRA on Chinese grows from 5.9 to
14.0 in the sparse feature setting. While bounding
the spread is useful in the low-dimensional setting
(0.7-1.5 BLEU gain with RM over MIRA as shown
in Table 3), accounting for the spread is even more
crucial with sparse features, where MIRA gains
only up to 0.1 BLEU, while RM gains 1 BLEU.
These results support the claim that our imposed
bound B indeed helps decrease the spread, and
that, in turn, lower spread yields better general-
ization performance.
Error Analysis: The inconclusive advantage
of RM over MIRA (in BLEU vs. TER scores)
on Arabic-English MT08 calls for a closer look.
Therefore we conducted a coarse error analysis
on 15 randomly selected sentences from MERT,
RMM and MIRA, with basic and sparse feature
settings for the latter two. This sample yielded
450 data points for analysis: output of the 5 con-
ditions on 15 sentences scored in 6 violation cate-
gories. The categories were: function word drop,
content word drop, syntactic error (with a reason-
able meaning), semantic error (regardless of syn-
tax), word order issues, and function word mis-
translation and “hallucination”. The purpose of
this analysis was to get a qualitative feel for the
output of each model, and a better idea as to why
we obtained performance improvements. RM no-
</bodyText>
<page confidence="0.96065">
1123
</page>
<bodyText confidence="0.99998075">
ticeably had more word order and excess/wrong
function word issues in the basic feature setting
than any optimizer. However, RM seemed to ben-
efit the most from the sparse features, as its bad
word order rate dropped close to MIRA, and its ex-
cess/wrong function word rate dropped below that
of MIRA with sparse features (MIRA’s rate actu-
ally doubled from its basic feature set). We con-
jecture both these issues will be ameliorated with
syntactic features such as those in Chiang et al.
(2008). This correlates with our observation that
RM’s overall BLEU score is negatively impacted
by the BP, as the BLEU precision scores are no-
ticeably higher.
K-best: RM is potentially more sensitive to the
size and order of the k-best list. While MIRA is
only concerned with the margin between y+ and
y−, RM also accounts for the distance between y+
and y&apos;. It might be the case that a larger k-best, or
revisiting previous strategies for y+ and y− selec-
tion, such as bold updating, local updating (Liang
et al., 2006b), or max-BLEU updating (Tillmann
and Zhang, 2006) might have a greater impact.
Also, we only explored several settings of B, and
there remains a continuum of RM solutions that
trade off between margin and spread in different
ways.
Active features: Perhaps contrary to expecta-
tion, we did not see evidence of a correlation be-
tween the number of active features and optimizer
performance. RAMPION, with the fewest features,
is the closest performer to RM in Chinese, while
MIRA, with a greater number, is the closest on
Arabic. We also notice that while PRO had the
lowest BLEU scores in Chinese, it was competi-
tive in Arabic with the highest number of features.
</bodyText>
<sectionHeader confidence="0.996438" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999962864864865">
We have introduced RM, a novel online margin-
based algorithm designed for optimizing high-
dimensional feature spaces, which introduces con-
straints into a large-margin optimizer that bound
the spread of the projection of the data while max-
imizing the margin. The closed-form online up-
date for our relative margin solution accounts for
surrogate references and latent variables.
Experimentation in statistical MT yielded sig-
nificant improvements over several other state-
of-the-art optimizers, especially in a high-
dimensional feature space (up to 2 BLEU and 4.3
TER on average). Overall, RM achieves the best or
comparable performance according to two scoring
methods in two language pairs, with two test sets
each, in small and large feature settings. More-
over, across conditions, RM always yielded the
best combined TER-BLEU score.7
These improvements are achieved using stan-
dard, relatively small tuning sets, contrasted with
improvements involving sparse features obtained
using much larger tuning sets, on the order of
hundreds of thousands of sentences (Liang et al.,
2006a; Tillmann and Zhang, 2006; Blunsom et al.,
2008; Simianer et al., 2012). Since our approach
is complementary to scaling up the tuning data, in
future work we intend to combine these two meth-
ods. In future work we also intend to explore using
additional sparse features that are known to be use-
ful in translation, e.g. syntactic features explored
by Chiang et al. (2008).
Finally, although motivated by statistical ma-
chine translation, RM is a gradient-based method
that can easily be applied to other problems. We
plan to investigate its utility elsewhere in NLP
(e.g. for parsing) as well as in other domains in-
volving high-dimensional structured prediction.
</bodyText>
<sectionHeader confidence="0.997237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999786625">
We would like to thank Pannaga Shivaswamy for
valuable discussions, and the anonymous review-
ers for their comments. Vladimir Eidelman is sup-
ported by a National Defense Science and Engi-
neering Graduate Fellowship. This work was also
supported in part by the BOLT program of the De-
fense Advanced Research Projects Agency, Con-
tract HR0011-12-C-0015.
</bodyText>
<sectionHeader confidence="0.964544" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.508552583333333">
Abishek Arun and Philipp Koehn. 2007. Online learn-
ing methods for discriminative training of phrase
based statistical machine translation. In MT Summit
XI.
Peter L. Bartlett and Shahar Mendelson. 2003.
Rademacher and gaussian complexities: risk bounds
and structural results. J. Mach. Learn. Res., 3:463–
482, March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, Columbus, Ohio, June.
</bodyText>
<footnote confidence="0.869028">
7We and other researchers often use 12 (TER − BLEU) as a
combined SMT quality metric.
</footnote>
<page confidence="0.987242">
1124
</page>
<reference confidence="0.997468108108109">
Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gen-
tile. 2005. A second-order perceptron algorithm.
SIAM J. Comput., 34(3):640–668, March.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310–318.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Waikiki, Honolulu, Hawaii.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, NAACL ’09, pages 218–226.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. J. Mach. Learn.
Res., 7:551–585.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009a. Adaptive regularization of weight vectors.
In Advances in Neural Information Processing Sys-
tems 22, pages 414–422.
Koby Crammer, Mehryar Mohri, and Fernando Pereira.
2009b. Gaussian margin machines. Journal of
Machine Learning Research - Proceedings Track,
5:105–112.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification
for text categorization. J. Mach. Learn. Res.,
98888:1891–1926, June.
Mark Dredze and Koby Crammer. 2008. Confidence-
weighted linear classification. In In ICML 08: Pro-
ceedings of the 25th international conference on
Machine learning, pages 264–271. ACM.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 242–249, Athens, Greece, March. As-
sociation for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many Rel-
evant Features. In Claire N´edellec and C´eline Rou-
veirol, editors, European Conference on Machine
Learning, pages 137–142, Berlin. Springer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, Stroudsburg, PA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163–171.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, pages
399–406.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
ACL-44, pages 761–768.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006b. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of the 2006 International Conference on Com-
putational Linguistics (COLING) - the Association
for Computational Linguistics (ACL).
David Mcallester and Joseph Keshet. 2011. Gener-
alization bounds and consistency for latent struc-
tural probit and ramp loss. In J. Shawe-Taylor,
</reference>
<page confidence="0.857397">
1125
</page>
<reference confidence="0.999919887323943">
R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 24, pages 2205–2212.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456–464, Los Angeles, California.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
Pannagadatta Shivaswamy and Tony Jebara. 2009a.
Structured prediction with relative margin. In In
International Conference on Machine Learning and
Applications.
Pannagadatta K Shivaswamy and Tony Jebara. 2009b.
Relative margin machines. In In Advances in Neural
Information Processing Systems 21. MIT Press.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Jeju Island, Korea, July.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ben Taskar, Simon Lacoste-Julien, and Michael I. Jor-
dan. 2006. Structured prediction, dual extragradi-
ent and bregman projections. J. Mach. Learn. Res.,
7:1627–1653, December.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of the 2006 International Conference
on Computational Linguistics (COLING) - the Asso-
ciation for Computational Linguistics (ACL).
Huihsin Tseng, Pi-Chuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005. A
conditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, ICML
’04.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.994508">
1126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931912">
<title confidence="0.999928">Online Relative Margin Maximization for Statistical Machine Translation</title>
<author confidence="0.996634">Vladimir Yuval Philip</author>
<affiliation confidence="0.997547">Computer City Center and and Bellevue, University of University of yuvalmarton@gmail.com College Park,</affiliation>
<address confidence="0.984905">College Park, resnik@umd.edu</address>
<email confidence="0.994955">vlad@umiacs.umd.edu</email>
<abstract confidence="0.9980062">Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these solutions are impractical in complex structured prediction problems such as statistical machine translation. We present an online gradient-based algorithm for relative margin maximization, which bounds the spread of the projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and Arabic- English translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant imof 1.2-2 1.7-4.3 average over state-of-the-art optimizers with the large feature set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Alex Conconi</author>
<author>Claudio Gentile</author>
</authors>
<title>A second-order perceptron algorithm.</title>
<date>2005</date>
<journal>SIAM J. Comput.,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="3553" citStr="Cesa-Bianchi et al., 2005" startWordPosition="550" endWordPosition="553">such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypothese</context>
</contexts>
<marker>Cesa-Bianchi, Conconi, Gentile, 2005</marker>
<rawString>Nicol`o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. 2005. A second-order perceptron algorithm. SIAM J. Comput., 34(3):640–668, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="21941" citStr="Chen and Goodman, 1996" startWordPosition="3683" endWordPosition="3686">n-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1689" citStr="Cherry and Foster, 2012" startWordPosition="240" endWordPosition="243">tate-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) features may be observed in a re</context>
<context position="7881" citStr="Cherry and Foster, 2012" startWordPosition="1219" endWordPosition="1222">ned over derivations, they are always paired with translations, so our feature function f(x, y, d) is defined over derivation–translation pairs.2 The learning goal is then to estimate w. The instability of MERT in larger feature sets (Foster and Kuhn, 2009; Hopkins and May, 2011), has motivated many alternative tuning methods for SMT. These include strategies based on batch log-linear models (Tillmann and Zhang, 2006; Blunsom et al., 2008), as well as the introduction of online linear models (Liang et al., 2006a; Arun and Koehn, 2007). Recent batch optimizers, PRO and RAMPION, and Batch-MIRA (Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to rank pairs of outputs. RAMPION aims to address the disconnect between MT and machine learning by optimizing a structured ramp loss with a concave-convex procedure. 2.1 Large-Margin Learning Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; C</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Waikiki, Honolulu, Hawaii.</location>
<contexts>
<context position="1663" citStr="Chiang et al., 2008" startWordPosition="236" endWordPosition="239">TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) featur</context>
<context position="24668" citStr="Chiang et al., 2008" startWordPosition="4151" endWordPosition="4154">ound that the optimizer is fairly insensitive to these changes, resulting in only minor differences in BLEU. BLEU Score B dist &gt; B Bounding Constraint dist cost &gt; margin Margin Constraint margin cost 1121 Optimizer Zh Ar MIRA 35k 37k PRO 95k 115k RAMPION 22k 24k RM 30k 32k Active+Inactive 3.4M 4.9M Table 2: Active sparse feature templates abe et al., 2007; Simianer et al., 2012). Non-lexical features include structural distortion, which captures the dependence between reordering and the size of a filler, and rule shape, which bins grammar rules by their sequence of terminals and nonterminals (Chiang et al., 2008). Lexical features on rules include rule ID, which fires on a specific grammar rule. We also introduce context-dependent lexical features for the 300 most frequent aligned word pairs (f,e) in the training corpus, which fire on triples (f,e,f+1) and (f,e,f−1), capturing when we see f aligned to e, with f+1 and f−1 occurring to the right or left of f, respectively. All other words fall into the default (unk) feature bin. In addition, we have insertion and deletion features for the 150 most frequently unaligned target and source words. These feature templates resulted in a total of 3.4 million po</context>
<context position="25973" citStr="Chiang et al., 2008" startWordPosition="4382" endWordPosition="4385">nd optimizer, as shown in Table 2. 4.3 Results As can be seen from the results in Table 3, our RM method was the best performer in all ChineseEnglish tests according to all measures – up to 1.9 BLEU and 6.6 TER over MIRA – even though we only optimized for BLEU.5 Surprisingly, it seems that MIRA did not benefit as much from the sparse features as RM. The results are especially notable for the basic feature setting – up to 1.2 BLEU and 4.6 TER improvement over MERT – since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al., 2008). For the tuning set, the decoder performance was consistently the lowest with RM, compared to the 5In the small feature set RAMPION yielded similar best BLEU scores, but worse TER. In preliminary experiments with a smaller trigram LM, our RM method consistently yielded the highest scores in all Chinese-English tests – up to 1.6 BLEU and 6.4 TER from MIRA, the second best performer. other optimizers. We believe this is due to the RM bounding constraint being more resistant to overfitting the training data, and thus allowing for improved generalization. Conversely, while PRO had the second lowe</context>
<context position="32671" citStr="Chiang et al. (2008)" startWordPosition="5567" endWordPosition="5570">get a qualitative feel for the output of each model, and a better idea as to why we obtained performance improvements. RM no1123 ticeably had more word order and excess/wrong function word issues in the basic feature setting than any optimizer. However, RM seemed to benefit the most from the sparse features, as its bad word order rate dropped close to MIRA, and its excess/wrong function word rate dropped below that of MIRA with sparse features (MIRA’s rate actually doubled from its basic feature set). We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al. (2008). This correlates with our observation that RM’s overall BLEU score is negatively impacted by the BP, as the BLEU precision scores are noticeably higher. K-best: RM is potentially more sensitive to the size and order of the k-best list. While MIRA is only concerned with the margin between y+ and y−, RM also accounts for the distance between y+ and y&apos;. It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al., 2006b), or max-BLEU updating (Tillmann and Zhang, 2006) might have a greater impact. Also, </context>
<context position="35321" citStr="Chiang et al. (2008)" startWordPosition="6002" endWordPosition="6005">ore.7 These improvements are achieved using standard, relatively small tuning sets, contrasted with improvements involving sparse features obtained using much larger tuning sets, on the order of hundreds of thousands of sentences (Liang et al., 2006a; Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012). Since our approach is complementary to scaling up the tuning data, in future work we intend to combine these two methods. In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al. (2008). Finally, although motivated by statistical machine translation, RM is a gradient-based method that can easily be applied to other problems. We plan to investigate its utility elsewhere in NLP (e.g. for parsing) as well as in other domains involving high-dimensional structured prediction. Acknowledgments We would like to thank Pannaga Shivaswamy for valuable discussions, and the anonymous reviewers for their comments. Vladimir Eidelman is supported by a National Defense Science and Engineering Graduate Fellowship. This work was also supported in part by the BOLT program of the Defense Advance</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Waikiki, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="8500" citStr="Chiang et al., 2009" startWordPosition="1314" endWordPosition="1317">), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to rank pairs of outputs. RAMPION aims to address the disconnect between MT and machine learning by optimizing a structured ramp loss with a concave-convex procedure. 2.1 Large-Margin Learning Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009). The usual presentation of MIRA’s optimization problem is given as a quadratic program: 2We may omit d in some equations for clarity. 1117 2||w − wt||2 + Cξi 1 s.t. s(xi, yi, d) − s(xi, y0, d) ≥ Δi(y0) − ξi where y0 is the single most violated constraint, the cost Δi(y) is computed using an external measure of quality, such as 1-BLEU(yi, y), and a slack variable ξi is introduced to allow for non-separable instances. C acts as a regularization parameter, trading off between margin maximization and constraint violations. While solving the optimization problem relies on computing the margin betw</context>
<context position="22290" citStr="Chiang et al., 2009" startWordPosition="3740" endWordPosition="3743">he grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McDonald et al., 2010). All optimizers were implemented in cdec and use </context>
<context position="23870" citStr="Chiang et al., 2009" startWordPosition="4018" endWordPosition="4021">i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set. All results are averaged over 3 runs. 4.2 Feature Sets We experimented with a small (basic) feature set, and a large (sparse) feature set. For the small feature set, we use 14 features, including a language model, 5 translation model features, penalties for unknown words, the glue rule, and rule arity. For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan4We also conducted an investigation into the setting of the B parameter. We explored alternative values for B, as well as scaling it by the current candidate’s cost, and found that the optimizer is fairly insensitive to these changes, resulting in only minor differences in BLEU. BLEU Score B dist &gt; B Bounding Constraint dist cost &gt; margin Margin Constraint margin cost 1121 Optimizer Zh Ar MIRA 35k 37k PRO 95k 115k RAMPION 22k 24k RM 30k 32k Active+Inactive 3.4M 4.9M Table 2: Active sparse feature templates abe et al., 2007; Simianer et al., 2012). Non-lexical features include structural</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>J. Machine Learning Research.</journal>
<contexts>
<context position="3703" citStr="Chiang, 2012" startWordPosition="573" endWordPosition="574"> hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured pre</context>
<context position="9409" citStr="Chiang, 2012" startWordPosition="1471" endWordPosition="1472">ernal measure of quality, such as 1-BLEU(yi, y), and a slack variable ξi is introduced to allow for non-separable instances. C acts as a regularization parameter, trading off between margin maximization and constraint violations. While solving the optimization problem relies on computing the margin between the correct output yi, and y0, in SMT our decoder is often incapable of producing the reference translation, i.e. yi ∈/ Y(xi). We must instead resort to selecting a surrogate reference, y+ ∈ Y(xi). This issue has recently received considerable attention (Liang et al., 2006a; Eidelman, 2012; Chiang, 2012), with preference given to surrogate references obtained through cost-diminished hypothesis selection. Thus, y+ is selected based on a combination of model score and error metric from the k-best list produced by our current model. A similar selection is made for the cost-augmented hypothesis y− ∈ Y(xi): (y+, d+) ← arg max s(xi, y, d) − Δi(y) (y,d)∈Y(xi),D(xi) (y−, d−) ← arg max s(xi, y, d) + Δi(y) (y,d)∈Y(xi),D(xi) In this setting, the optimization problem becomes: 1 wt+1 = arg min 2||w − wt||2 + Cξi w s.t. δs(xi, y+, y−) ≥ Δi(y−) − Δi(y+) − ξi where y+, y+, This leads to a varian δs(xi, y−)=s</context>
<context position="16949" citStr="Chiang, 2012" startWordPosition="2771" endWordPosition="2772">y0) where the α Lagrange multiplier corresponds to the standard margin constraint, while β and max XL = Xαy − B α,β,β∗ y∈Y(xi) y∈Y(xi) 1119 β* each correspond to a bounding constraint, and wi(y+, y&apos;) corresponds to the difference of f(xi, y+, d+) and f(xi, y&apos;, d&apos;). The weight update can then be obtained from the dual variables: X X X αywi(y+, y) − Oywi(y+, y) + O∗ywi(y+, y) (6) The dual in Equation (5) can be optimized using a cutting plane algorithm, an effective method for solving a relaxed optimization problem in the dual, used in Structured SVM, MIRA, and RMM (Tsochantaridis et al., 2004; Chiang, 2012; Shivaswamy and Jebara, 2009a). The cutting plane presented in Alg. 1 decomposes the overall problem into subproblems which are solved independently by creating working sets Sji , which correspond to the largest violations of either the margin constraint, or bounding constraints, and iteratively satisfying the constraints in each set. The cutting plane in Alg. 1 makes use of the the closed-form gradient-based updates we derived for RM presented in Alg. 2. The updates amount to performing a subgradient descent step to update w in accordance with the constraints. Since the constraint matrix of </context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. J. Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>7--551</pages>
<contexts>
<context position="3333" citStr="Crammer et al., 2006" startWordPosition="515" endWordPosition="518">iven a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the </context>
<context position="10195" citStr="Crammer et al., 2006" startWordPosition="1610" endWordPosition="1613"> error metric from the k-best list produced by our current model. A similar selection is made for the cost-augmented hypothesis y− ∈ Y(xi): (y+, d+) ← arg max s(xi, y, d) − Δi(y) (y,d)∈Y(xi),D(xi) (y−, d−) ← arg max s(xi, y, d) + Δi(y) (y,d)∈Y(xi),D(xi) In this setting, the optimization problem becomes: 1 wt+1 = arg min 2||w − wt||2 + Cξi w s.t. δs(xi, y+, y−) ≥ Δi(y−) − Δi(y+) − ξi where y+, y+, This leads to a varian δs(xi, y−)=s(xi, d+)-s(xi,y−,d−) t of the structured ramp loss to be optimized: max (s (xi,y+, d+) Y( i),D( −Δ ∈ x xi) (s(xi,y−,d−) + Δi(y−)) (3) The passive-aggressive update (Crammer et al., 2006), which is used to solve this problem, updates w on each round such that the score of the correct hypothesis y+ is greater than the score of the incorrect y− by a margin at least as large as the cost incurred by predicting the incorrect hypothesis, while keeping the change to w small. (a) (b) darker soli d line, respectively. (yw, dw) arg y, ← min(y,d)∈Y(xi),D(xi)s(xi, d), after projecting both onto the line defined by the weight vector w. For each y0, this projection is conveniently given by s(xi, y0, d), thus the spread is calculated as δs(xi, y+, yw). RMM was introduced as a generalization </context>
<context position="18401" citStr="Crammer et al., 2006" startWordPosition="3044" endWordPosition="3047"> ith training example (xi, yi), weight w, margin reg. C, bound B, bound reg. D, E, EB i ~y+ 1: S1 i ~y+ , S2 i ~y+ , S3 2: repeat 3: H(y) := Di(y) − Di(y+) − 6s(xi, y+, y) 4: y1 arg maxy∈Y(xi) H(y) 5: y2 arg maxy∈Y(xi) G(y) := 6s(xi, y+, y) 6: y3 arg miny∈Y(xi) −G(y) 7: ξ max {0, maxy∈Si H(y)} 8: V1 H(y1) − ξ − E 9: V2 G(y2) − B − EB 10: V3 −G(y3) − B − EB 11: j arg maxj0∈{1,2,3} Vj0 12: if Vj &gt; 0 then 13: Sji Sji U {yj} 14: OPTIMIZE(w, S1i , S2i , S3i , C, B) &gt; see Alg. 2 15: end if 16: until S1i , S2i , S3i do not change Alternatively, we could utilize a passiveaggressive updating strategy (Crammer et al., 2006), which would simply bypass the cutting plane and select the most violated constraint for Algorithm 2 RM update with α, β, β* 1: procedure OPTIMIZE(w, S1i , S2i , S3i , C, B) 2: while w changes do �� &gt; 1 then 3: if ~~S1 i 4: UPDATEMARGIN(w, S1i , C) 5: end if �� &gt; 1 then 6: if ~~S2 i 7: UPDATEUPPERBOUND(w, S2i , B) 8: end if �� &gt; 1 then 9: if ~~S3 i 10: UPDATELOWERBOUND(w, S3i , B) 11: end if 12: end while 13: end procedure 14: procedure UPDATEMARGIN(w, S1i , C) 15: αy 0 for all y E S1i 16: α + C yi 17: for n 1...MaxIter do 18: Select two constraints y, y0 from S1i &apos;Yα Δi(y0)−Δi(y)−δs(xi, y, y</context>
<context position="22268" citStr="Crammer et al., 2006" startWordPosition="3736" endWordPosition="3739">re symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McDonald et al., 2010). All optimizers were implem</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. J. Mach. Learn. Res., 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Mark Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>414--422</pages>
<contexts>
<context position="3601" citStr="Crammer et al., 2009" startWordPosition="557" endWordPosition="560">gorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the we</context>
<context position="14506" citStr="Crammer et al., 2009" startWordPosition="2341" endWordPosition="2344"> theoretically well-founded and improves practical performance over largemargin learning in the settings where it was introduced, it is unsuitable for most complex structured prediction in NLP. Nonetheless, since structured RMM is a generalization of Structured SVM, which shares its underlying objective with MIRA, our intuition is that SMT should be able to benefit as well. But to take advantage of the second-order information RMM utilizes for increased generalizability in SMT, we need a computationally effi3The motivation of confidence-weighted estimation (Dredze and Crammer, 2008) and AROW (Crammer et al., 2009a) is related in spirit. They use second-order information in the form of a distribution over weights to change the maximum margin solution. cient optimization procedure that does not require batch training or an off-the-shelf QP solver. 3.2 RM Algorithm We address the above-mentioned limitations by introducing a novel online learning algorithm for relative margin maximization, RM. The relative margin solution is obtained by maximizing the same margin as Equation (2), but now with respect to the distance between y+, and the worst candidate yw. Thus, the relative margin dictates trading-off bet</context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2009</marker>
<rawString>Koby Crammer, Alex Kulesza, and Mark Dredze. 2009a. Adaptive regularization of weight vectors. In Advances in Neural Information Processing Systems 22, pages 414–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
</authors>
<title>Gaussian margin machines.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research - Proceedings Track,</journal>
<pages>5--105</pages>
<contexts>
<context position="3601" citStr="Crammer et al., 2009" startWordPosition="557" endWordPosition="560">gorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the we</context>
<context position="14506" citStr="Crammer et al., 2009" startWordPosition="2341" endWordPosition="2344"> theoretically well-founded and improves practical performance over largemargin learning in the settings where it was introduced, it is unsuitable for most complex structured prediction in NLP. Nonetheless, since structured RMM is a generalization of Structured SVM, which shares its underlying objective with MIRA, our intuition is that SMT should be able to benefit as well. But to take advantage of the second-order information RMM utilizes for increased generalizability in SMT, we need a computationally effi3The motivation of confidence-weighted estimation (Dredze and Crammer, 2008) and AROW (Crammer et al., 2009a) is related in spirit. They use second-order information in the form of a distribution over weights to change the maximum margin solution. cient optimization procedure that does not require batch training or an off-the-shelf QP solver. 3.2 RM Algorithm We address the above-mentioned limitations by introducing a novel online learning algorithm for relative margin maximization, RM. The relative margin solution is obtained by maximizing the same margin as Equation (2), but now with respect to the distance between y+, and the worst candidate yw. Thus, the relative margin dictates trading-off bet</context>
</contexts>
<marker>Crammer, Mohri, Pereira, 2009</marker>
<rawString>Koby Crammer, Mehryar Mohri, and Fernando Pereira. 2009b. Gaussian margin machines. Journal of Machine Learning Research - Proceedings Track, 5:105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification for text categorization.</title>
<date>2012</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>98888--1891</pages>
<contexts>
<context position="5176" citStr="Crammer et al., 2012" startWordPosition="787" endWordPosition="790">y and Jebara extended RMM to handle sequential structured prediction (Shivaswamy and Jebara, 2009a), their batch approach to quadratic optimization, using existing off-the-shelf QP solvers, does not provide a practical solution: as Taskar et al. (2006) observe, “off-the-shelf QP solvers tend to scale poorly with problem and training sample size” for structured prediction problems.. This motivates an online gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects with sparse features (Crammer et al., 2012). The contributions of this paper include (1) introduction of a loss function for structured RMM in the SMT setting, with surrogate reference translations and latent variables; (2) an online gradientbased solver, RM, with a closed-form parameter update to optimize the relative margin loss; and (3) an efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010).1 In addition, (4) as our solution is not dependent on any specific QP solver, it can be easily incorporated into practically any gradientbased learning algorithm. After background discussion on </context>
</contexts>
<marker>Crammer, Dredze, Pereira, 2012</marker>
<rawString>Koby Crammer, Mark Dredze, and Fernando Pereira. 2012. Confidence-weighted linear classification for text categorization. J. Mach. Learn. Res., 98888:1891–1926, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Confidenceweighted linear classification. In</title>
<date>2008</date>
<booktitle>In ICML 08: Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>264--271</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3659" citStr="Dredze and Crammer, 2008" startWordPosition="564" endWordPosition="567">h that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine </context>
<context position="14475" citStr="Dredze and Crammer, 2008" startWordPosition="2335" endWordPosition="2338">ussed in Section 6.3 Although RMM is theoretically well-founded and improves practical performance over largemargin learning in the settings where it was introduced, it is unsuitable for most complex structured prediction in NLP. Nonetheless, since structured RMM is a generalization of Structured SVM, which shares its underlying objective with MIRA, our intuition is that SMT should be able to benefit as well. But to take advantage of the second-order information RMM utilizes for increased generalizability in SMT, we need a computationally effi3The motivation of confidence-weighted estimation (Dredze and Crammer, 2008) and AROW (Crammer et al., 2009a) is related in spirit. They use second-order information in the form of a distribution over weights to change the maximum margin solution. cient optimization procedure that does not require batch training or an off-the-shelf QP solver. 3.2 RM Algorithm We address the above-mentioned limitations by introducing a novel online learning algorithm for relative margin maximization, RM. The relative margin solution is obtained by maximizing the same margin as Equation (2), but now with respect to the distance between y+, and the worst candidate yw. Thus, the relative </context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Confidenceweighted linear classification. In In ICML 08: Proceedings of the 25th international conference on Machine learning, pages 264–271. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<contexts>
<context position="5580" citStr="Dyer et al., 2010" startWordPosition="852" endWordPosition="855">ne gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects with sparse features (Crammer et al., 2012). The contributions of this paper include (1) introduction of a loss function for structured RMM in the SMT setting, with surrogate reference translations and latent variables; (2) an online gradientbased solver, RM, with a closed-form parameter update to optimize the relative margin loss; and (3) an efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010).1 In addition, (4) as our solution is not dependent on any specific QP solver, it can be easily incorporated into practically any gradientbased learning algorithm. After background discussion on learning in SMT (§2), we introduce a novel online learning algorithm for relative margin maximization suitable for SMT (§3). First, we introduce RMM (§3.1) and propose a latent structured relative margin objective which incorporates cost-augmented hypothesis selection and latent variables. Then, we derive a simple closed-form online update necessary to create a large margin solution while simultaneous</context>
<context position="21975" citStr="Dyer et al., 2010" startWordPosition="3690" endWordPosition="3693">e NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
</authors>
<title>Optimization strategies for online large-margin learning in machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="9394" citStr="Eidelman, 2012" startWordPosition="1469" endWordPosition="1470">ted using an external measure of quality, such as 1-BLEU(yi, y), and a slack variable ξi is introduced to allow for non-separable instances. C acts as a regularization parameter, trading off between margin maximization and constraint violations. While solving the optimization problem relies on computing the margin between the correct output yi, and y0, in SMT our decoder is often incapable of producing the reference translation, i.e. yi ∈/ Y(xi). We must instead resort to selecting a surrogate reference, y+ ∈ Y(xi). This issue has recently received considerable attention (Liang et al., 2006a; Eidelman, 2012; Chiang, 2012), with preference given to surrogate references obtained through cost-diminished hypothesis selection. Thus, y+ is selected based on a combination of model score and error metric from the k-best list produced by our current model. A similar selection is made for the cost-augmented hypothesis y− ∈ Y(xi): (y+, d+) ← arg max s(xi, y, d) − Δi(y) (y,d)∈Y(xi),D(xi) (y−, d−) ← arg max s(xi, y, d) + Δi(y) (y,d)∈Y(xi),D(xi) In this setting, the optimization problem becomes: 1 wt+1 = arg min 2||w − wt||2 + Cξi w s.t. δs(xi, y+, y−) ≥ Δi(y−) − Δi(y+) − ξi where y+, y+, This leads to a vari</context>
</contexts>
<marker>Eidelman, 2012</marker>
<rawString>Vladimir Eidelman. 2012. Optimization strategies for online large-margin learning in machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Stabilizing minimum error rate training.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>242--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="7513" citStr="Foster and Kuhn, 2009" startWordPosition="1161" endWordPosition="1164">vector w: (y*, d*) = arg max wTf(x, y, d) (y,d)∈Y(x),D(x) where wTf(x, y, d) is the weighted feature scoring function, hereafter s(x, y, d), and Y(x) is the space of possible translations of x. While many derivations d E D(x) can produce a given translation, we are only able to observe y; thus we model d as a latent variable. Although our models are actually defined over derivations, they are always paired with translations, so our feature function f(x, y, d) is defined over derivation–translation pairs.2 The learning goal is then to estimate w. The instability of MERT in larger feature sets (Foster and Kuhn, 2009; Hopkins and May, 2011), has motivated many alternative tuning methods for SMT. These include strategies based on batch log-linear models (Tillmann and Zhang, 2006; Blunsom et al., 2008), as well as the introduction of online linear models (Liang et al., 2006a; Arun and Koehn, 2007). Recent batch optimizers, PRO and RAMPION, and Batch-MIRA (Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to rank</context>
</contexts>
<marker>Foster, Kuhn, 2009</marker>
<rawString>George Foster and Roland Kuhn. 2009. Stabilizing minimum error rate training. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 242–249, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1553" citStr="Gimpel and Smith, 2012" startWordPosition="219" endWordPosition="222">rge feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature spa</context>
<context position="4535" citStr="Gimpel and Smith, 2012" startWordPosition="696" endWordPosition="699">nformation about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent variables and surrogate references, resulting in loss functions that have not been well explored in machine learning (Mcallester and Keshet, 2011; Gimpel and Smith, 2012). Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction (Shivaswamy and Jebara, 2009a), their batch approach to quadratic optimization, using existing off-the-shelf QP solvers, does not provide a practical solution: as Taskar et al. (2006) observe, “off-the-shelf QP solvers tend to scale poorly with problem and training sample size” for structured prediction problems.. This motivates an online gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects wi</context>
<context position="22357" citStr="Gimpel and Smith, 2012" startWordPosition="3751" endWordPosition="3754"> 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McDonald et al., 2010). All optimizers were implemented in cdec and use the same system configuration, thus the only independent variable i</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1519" citStr="Hopkins and May, 2011" startWordPosition="214" endWordPosition="217">ion tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, a</context>
<context position="7537" citStr="Hopkins and May, 2011" startWordPosition="1165" endWordPosition="1168">g max wTf(x, y, d) (y,d)∈Y(x),D(x) where wTf(x, y, d) is the weighted feature scoring function, hereafter s(x, y, d), and Y(x) is the space of possible translations of x. While many derivations d E D(x) can produce a given translation, we are only able to observe y; thus we model d as a latent variable. Although our models are actually defined over derivations, they are always paired with translations, so our feature function f(x, y, d) is defined over derivation–translation pairs.2 The learning goal is then to estimate w. The instability of MERT in larger feature sets (Foster and Kuhn, 2009; Hopkins and May, 2011), has motivated many alternative tuning methods for SMT. These include strategies based on batch log-linear models (Tillmann and Zhang, 2006; Blunsom et al., 2008), as well as the introduction of online linear models (Liang et al., 2006a; Arun and Koehn, 2007). Recent batch optimizers, PRO and RAMPION, and Batch-MIRA (Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to rank pairs of outputs. RAMPI</context>
<context position="22319" citStr="Hopkins and May, 2011" startWordPosition="3745" endWordPosition="3748">od (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McDonald et al., 2010). All optimizers were implemented in cdec and use the same system configuration</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In Claire N´edellec and C´eline Rouveirol, editors, European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<contexts>
<context position="2955" citStr="Joachims, 1998" startWordPosition="456" endWordPosition="457">, we are prone to overfit the training data. An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Claire N´edellec and C´eline Rouveirol, editors, European Conference on Machine Learning, pages 137–142, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21720" citStr="Koehn et al., 2003" startWordPosition="3647" endWordPosition="3650">e the advantage of explicitly accounting for the spread of the data, we conducted several experiments on two Chinese-English translation test sets, using two different feature sets in each. For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011),</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="22221" citStr="Kumar et al., 2009" startWordPosition="3728" endWordPosition="3731">) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McD</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Ossama Emam</author>
<author>Hany Hassan</author>
</authors>
<title>Language model based Arabic word segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<pages>399--406</pages>
<contexts>
<context position="27057" citStr="Lee et al., 2003" startWordPosition="4561" endWordPosition="4564"> resistant to overfitting the training data, and thus allowing for improved generalization. Conversely, while PRO had the second lowest tuning scores, it seemed to display signs of underfitting in the basic and large feature settings. 5 Additional Experiments In order to explore the applicability of our approach to a wider range of languages, we also evaluated its performance on Arabic-English translation. All experimental details were the same as above, except those noted below. For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMM segmenter (Lee et al., 2003). Dataset statistics are given in the bottom part of Table 1. The sparse feature templates resulted here in a total of 4.9 million possible features, of which again only a fraction were active, as shown in Table 2. As can be seen in Table 4, in the smaller feature set, RM and MERT were the best performers, with the exception that on MT08, MIRA yielded somewhat better (+0.7) BLEU but a somewhat worse (-0.9) TER score than RM. On the large feature set, RM is again the best performer, except, perhaps, a tied BLEU score with MIRA on MT08, but with a clear 1.8 TER gain. In both Arabic-English featu</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, and Hany Hassan. 2003. Language model based Arabic word segmentation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, pages 399–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>761--768</pages>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006a. An end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 761–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 International Conference on Computational Linguistics (COLING) - the Association for Computational Linguistics (ACL).</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006b. An end-to-end discriminative approach to machine translation. In Proceedings of the 2006 International Conference on Computational Linguistics (COLING) - the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mcallester</author>
<author>Joseph Keshet</author>
</authors>
<title>Generalization bounds and consistency for latent structural probit and ramp loss.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems 24,</booktitle>
<pages>2205--2212</pages>
<editor>In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="4510" citStr="Mcallester and Keshet, 2011" startWordPosition="692" endWordPosition="695"> of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent variables and surrogate references, resulting in loss functions that have not been well explored in machine learning (Mcallester and Keshet, 2011; Gimpel and Smith, 2012). Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction (Shivaswamy and Jebara, 2009a), their batch approach to quadratic optimization, using existing off-the-shelf QP solvers, does not provide a practical solution: as Taskar et al. (2006) observe, “off-the-shelf QP solvers tend to scale poorly with problem and training sample size” for structured prediction problems.. This motivates an online gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently process</context>
</contexts>
<marker>Mcallester, Keshet, 2011</marker>
<rawString>David Mcallester and Joseph Keshet. 2011. Generalization bounds and consistency for latent structural probit and ramp loss. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2205–2212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>456--464</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="22840" citStr="McDonald et al., 2010" startWordPosition="3834" endWordPosition="3838">09), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing (McDonald et al., 2010). All optimizers were implemented in cdec and use the same system configuration, thus the only independent variable is the optimizer itself. We set C to 0.01, and MaxIter to 100. We selected the bound step size D, based on performance on a held-out dev set, to be 0.01 for the basic feature set and 0.1 for the sparse feature set. The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost Δi is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set. All results are averaged over 3 runs. 4.2 Feature Sets </context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 456–464, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<issue>21</issue>
<pages>pages</pages>
<contexts>
<context position="21603" citStr="Och and Ney, 2003" startWordPosition="3631" endWordPosition="3634">the upsidedown triangle, and update so the distance from y+ is no greater than B. 4 Experiments 4.1 Setup To evaluate the advantage of explicitly accounting for the spread of the data, we conducted several experiments on two Chinese-English translation test sets, using two different feature sets in each. For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (K</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29(21), pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1342" citStr="Och, 2003" startWordPosition="189" endWordPosition="190">rgin maximization, which bounds the spread of the projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in m</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="22097" citStr="Papineni et al., 2002" startWordPosition="3709" endWordPosition="3713">e summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k-best aggregation across iterations. RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pannagadatta Shivaswamy</author>
<author>Tony Jebara</author>
</authors>
<title>Structured prediction with relative margin.</title>
<date>2009</date>
<booktitle>In In International Conference on Machine Learning and Applications.</booktitle>
<contexts>
<context position="3767" citStr="Shivaswamy and Jebara, 2009" startWordPosition="580" endWordPosition="584">red by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent va</context>
<context position="11828" citStr="Shivaswamy and Jebara, 2009" startWordPosition="1896" endWordPosition="1899">correct hypothesis and incorrect one, is defined by y+, d+) and It is maximized by minimizing the norm in SVM, or analogously, the proximity constraint in MIRA: arg 2 w wt 2. However, theoretical results supporting large-margin learning, such as the VC-dimension (Vapnik, 1995) or the Rademacher bound (Bartlett and Mendelson, 2003) consider measures of complexity, in addition to the empirical performance, when describing future predictive ability. The measures of complexity usually take the form of some value on the radius of the data, such as the ratio of the radius of the data to the margin (Shivaswamy and Jebara, 2009a). As radius is a way of measuring spread in any projection direction, here we will specifically be interested in the the spread of the data as measured after the projection defined by the learned model s(xi, s(xi,y−,d−). minw || − || w. More formally, the spread is the distance between y+, and the worst candidate 1118 and information regarding the spread of the data. The relative margin is the ratio of the absolute, or maximum margin, to the spread of the projected data. Thus, the RMM learns a large margin solution relative to the spread of the data, or in other words, creates a max margin w</context>
<context position="16978" citStr="Shivaswamy and Jebara, 2009" startWordPosition="2773" endWordPosition="2776">α Lagrange multiplier corresponds to the standard margin constraint, while β and max XL = Xαy − B α,β,β∗ y∈Y(xi) y∈Y(xi) 1119 β* each correspond to a bounding constraint, and wi(y+, y&apos;) corresponds to the difference of f(xi, y+, d+) and f(xi, y&apos;, d&apos;). The weight update can then be obtained from the dual variables: X X X αywi(y+, y) − Oywi(y+, y) + O∗ywi(y+, y) (6) The dual in Equation (5) can be optimized using a cutting plane algorithm, an effective method for solving a relaxed optimization problem in the dual, used in Structured SVM, MIRA, and RMM (Tsochantaridis et al., 2004; Chiang, 2012; Shivaswamy and Jebara, 2009a). The cutting plane presented in Alg. 1 decomposes the overall problem into subproblems which are solved independently by creating working sets Sji , which correspond to the largest violations of either the margin constraint, or bounding constraints, and iteratively satisfying the constraints in each set. The cutting plane in Alg. 1 makes use of the the closed-form gradient-based updates we derived for RM presented in Alg. 2. The updates amount to performing a subgradient descent step to update w in accordance with the constraints. Since the constraint matrix of the dual program is not stric</context>
</contexts>
<marker>Shivaswamy, Jebara, 2009</marker>
<rawString>Pannagadatta Shivaswamy and Tony Jebara. 2009a. Structured prediction with relative margin. In In International Conference on Machine Learning and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pannagadatta K Shivaswamy</author>
<author>Tony Jebara</author>
</authors>
<title>Relative margin machines. In</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 21.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3767" citStr="Shivaswamy and Jebara, 2009" startWordPosition="580" endWordPosition="584">red by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order information about the 1116 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1116–1126, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics distance between hypotheses when projected onto the line defined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent va</context>
<context position="11828" citStr="Shivaswamy and Jebara, 2009" startWordPosition="1896" endWordPosition="1899">correct hypothesis and incorrect one, is defined by y+, d+) and It is maximized by minimizing the norm in SVM, or analogously, the proximity constraint in MIRA: arg 2 w wt 2. However, theoretical results supporting large-margin learning, such as the VC-dimension (Vapnik, 1995) or the Rademacher bound (Bartlett and Mendelson, 2003) consider measures of complexity, in addition to the empirical performance, when describing future predictive ability. The measures of complexity usually take the form of some value on the radius of the data, such as the ratio of the radius of the data to the margin (Shivaswamy and Jebara, 2009a). As radius is a way of measuring spread in any projection direction, here we will specifically be interested in the the spread of the data as measured after the projection defined by the learned model s(xi, s(xi,y−,d−). minw || − || w. More formally, the spread is the distance between y+, and the worst candidate 1118 and information regarding the spread of the data. The relative margin is the ratio of the absolute, or maximum margin, to the spread of the projected data. Thus, the RMM learns a large margin solution relative to the spread of the data, or in other words, creates a max margin w</context>
<context position="16978" citStr="Shivaswamy and Jebara, 2009" startWordPosition="2773" endWordPosition="2776">α Lagrange multiplier corresponds to the standard margin constraint, while β and max XL = Xαy − B α,β,β∗ y∈Y(xi) y∈Y(xi) 1119 β* each correspond to a bounding constraint, and wi(y+, y&apos;) corresponds to the difference of f(xi, y+, d+) and f(xi, y&apos;, d&apos;). The weight update can then be obtained from the dual variables: X X X αywi(y+, y) − Oywi(y+, y) + O∗ywi(y+, y) (6) The dual in Equation (5) can be optimized using a cutting plane algorithm, an effective method for solving a relaxed optimization problem in the dual, used in Structured SVM, MIRA, and RMM (Tsochantaridis et al., 2004; Chiang, 2012; Shivaswamy and Jebara, 2009a). The cutting plane presented in Alg. 1 decomposes the overall problem into subproblems which are solved independently by creating working sets Sji , which correspond to the largest violations of either the margin constraint, or bounding constraints, and iteratively satisfying the constraints in each set. The cutting plane in Alg. 1 makes use of the the closed-form gradient-based updates we derived for RM presented in Alg. 2. The updates amount to performing a subgradient descent step to update w in accordance with the constraints. Since the constraint matrix of the dual program is not stric</context>
</contexts>
<marker>Shivaswamy, Jebara, 2009</marker>
<rawString>Pannagadatta K Shivaswamy and Tony Jebara. 2009b. Relative margin machines. In In Advances in Neural Information Processing Systems 21. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2582" citStr="Simianer et al., 2012" startWordPosition="387" endWordPosition="390"> general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) features may be observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data. An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This c</context>
<context position="24429" citStr="Simianer et al., 2012" startWordPosition="4114" endWordPosition="4117"> the form commonly found in the literature (Chiang et al., 2009; Watan4We also conducted an investigation into the setting of the B parameter. We explored alternative values for B, as well as scaling it by the current candidate’s cost, and found that the optimizer is fairly insensitive to these changes, resulting in only minor differences in BLEU. BLEU Score B dist &gt; B Bounding Constraint dist cost &gt; margin Margin Constraint margin cost 1121 Optimizer Zh Ar MIRA 35k 37k PRO 95k 115k RAMPION 22k 24k RM 30k 32k Active+Inactive 3.4M 4.9M Table 2: Active sparse feature templates abe et al., 2007; Simianer et al., 2012). Non-lexical features include structural distortion, which captures the dependence between reordering and the size of a filler, and rule shape, which bins grammar rules by their sequence of terminals and nonterminals (Chiang et al., 2008). Lexical features on rules include rule ID, which fires on a specific grammar rule. We also introduce context-dependent lexical features for the 300 most frequent aligned word pairs (f,e) in the training corpus, which fire on triples (f,e,f+1) and (f,e,f−1), capturing when we see f aligned to e, with f+1 and f−1 occurring to the right or left of f, respectiv</context>
<context position="35023" citStr="Simianer et al., 2012" startWordPosition="5950" endWordPosition="5953">ture space (up to 2 BLEU and 4.3 TER on average). Overall, RM achieves the best or comparable performance according to two scoring methods in two language pairs, with two test sets each, in small and large feature settings. Moreover, across conditions, RM always yielded the best combined TER-BLEU score.7 These improvements are achieved using standard, relatively small tuning sets, contrasted with improvements involving sparse features obtained using much larger tuning sets, on the order of hundreds of thousands of sentences (Liang et al., 2006a; Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012). Since our approach is complementary to scaling up the tuning data, in future work we intend to combine these two methods. In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al. (2008). Finally, although motivated by statistical machine translation, RM is a gradient-based method that can easily be applied to other problems. We plan to investigate its utility elsewhere in NLP (e.g. for parsing) as well as in other domains involving high-dimensional structured prediction. Acknowledgm</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1471" citStr="Smith and Eisner, 2006" startWordPosition="206" endWordPosition="210">zer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translat</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Michael I Jordan</author>
</authors>
<title>Structured prediction, dual extragradient and bregman projections.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>7--1627</pages>
<contexts>
<context position="4807" citStr="Taskar et al. (2006)" startWordPosition="735" endWordPosition="738">ined by the weight vector w. Unfortunately, not all advances in machine learning are easy to apply to structured prediction problems such as SMT; the latter often involve latent variables and surrogate references, resulting in loss functions that have not been well explored in machine learning (Mcallester and Keshet, 2011; Gimpel and Smith, 2012). Although Shivaswamy and Jebara extended RMM to handle sequential structured prediction (Shivaswamy and Jebara, 2009a), their batch approach to quadratic optimization, using existing off-the-shelf QP solvers, does not provide a practical solution: as Taskar et al. (2006) observe, “off-the-shelf QP solvers tend to scale poorly with problem and training sample size” for structured prediction problems.. This motivates an online gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects with sparse features (Crammer et al., 2012). The contributions of this paper include (1) introduction of a loss function for structured RMM in the SMT setting, with surrogate reference translations and latent variables; (2) an online gradientbased solver, RM, with a closed-</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Jordan, 2006</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Michael I. Jordan. 2006. Structured prediction, dual extragradient and bregman projections. J. Mach. Learn. Res., 7:1627–1653, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 International Conference on Computational Linguistics (COLING) - the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2536" citStr="Tillmann and Zhang, 2006" startWordPosition="379" endWordPosition="382"> In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) features may be observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data. An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost i</context>
<context position="7677" citStr="Tillmann and Zhang, 2006" startWordPosition="1185" endWordPosition="1188">e of possible translations of x. While many derivations d E D(x) can produce a given translation, we are only able to observe y; thus we model d as a latent variable. Although our models are actually defined over derivations, they are always paired with translations, so our feature function f(x, y, d) is defined over derivation–translation pairs.2 The learning goal is then to estimate w. The instability of MERT in larger feature sets (Foster and Kuhn, 2009; Hopkins and May, 2011), has motivated many alternative tuning methods for SMT. These include strategies based on batch log-linear models (Tillmann and Zhang, 2006; Blunsom et al., 2008), as well as the introduction of online linear models (Liang et al., 2006a; Arun and Koehn, 2007). Recent batch optimizers, PRO and RAMPION, and Batch-MIRA (Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to rank pairs of outputs. RAMPION aims to address the disconnect between MT and machine learning by optimizing a structured ramp loss with a concave-convex procedure. 2.1 </context>
<context position="33235" citStr="Tillmann and Zhang, 2006" startWordPosition="5665" endWordPosition="5668">with syntactic features such as those in Chiang et al. (2008). This correlates with our observation that RM’s overall BLEU score is negatively impacted by the BP, as the BLEU precision scores are noticeably higher. K-best: RM is potentially more sensitive to the size and order of the k-best list. While MIRA is only concerned with the margin between y+ and y−, RM also accounts for the distance between y+ and y&apos;. It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al., 2006b), or max-BLEU updating (Tillmann and Zhang, 2006) might have a greater impact. Also, we only explored several settings of B, and there remains a continuum of RM solutions that trade off between margin and spread in different ways. Active features: Perhaps contrary to expectation, we did not see evidence of a correlation between the number of active features and optimizer performance. RAMPION, with the fewest features, is the closest performer to RM in Chinese, while MIRA, with a greater number, is the closest on Arabic. We also notice that while PRO had the lowest BLEU scores in Chinese, it was competitive in Arabic with the highest number o</context>
<context position="34977" citStr="Tillmann and Zhang, 2006" startWordPosition="5942" endWordPosition="5945"> optimizers, especially in a highdimensional feature space (up to 2 BLEU and 4.3 TER on average). Overall, RM achieves the best or comparable performance according to two scoring methods in two language pairs, with two test sets each, in small and large feature settings. Moreover, across conditions, RM always yielded the best combined TER-BLEU score.7 These improvements are achieved using standard, relatively small tuning sets, contrasted with improvements involving sparse features obtained using much larger tuning sets, on the order of hundreds of thousands of sentences (Liang et al., 2006a; Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012). Since our approach is complementary to scaling up the tuning data, in future work we intend to combine these two methods. In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al. (2008). Finally, although motivated by statistical machine translation, RM is a gradient-based method that can easily be applied to other problems. We plan to investigate its utility elsewhere in NLP (e.g. for parsing) as well as in other domains involving high-</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical MT. In Proceedings of the 2006 International Conference on Computational Linguistics (COLING) - the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pi-Chuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="21451" citStr="Tseng et al., 2005" startWordPosition="3605" endWordPosition="3608">tune (MT06) 1797 55k 49k Ar-En MT05 1056 36k 33k MT08 1360 51k 45k 4-gram LM 24M 600M – Table 1: Corpus statistics ing constraint violator, yw, shown as the upsidedown triangle, and update so the distance from y+ is no greater than B. 4 Experiments 4.1 Setup To evaluate the advantage of explicitly accounting for the spread of the data, we conducted several experiments on two Chinese-English translation test sets, using two different feature sets in each. For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the s</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter. In Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning, ICML ’04.</booktitle>
<contexts>
<context position="3310" citStr="Tsochantaridis et al., 2004" startWordPosition="511" endWordPosition="514">re of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This criterion performs well in practice at finding a linear separator in high-dimensional feature spaces (Tsochantaridis et al., 2004; Crammer et al., 2006). Now, recent advances in machine learning have shown that the generalization ability of these learners can be improved by utilizing second order information, as in the Second Order Perceptron (Cesa-Bianchi et al., 2005), Gaussian Margin Machines (Crammer et al., 2009b), confidenceweighted learning (Dredze and Crammer, 2008), AROW (Crammer et al., 2009a; Chiang, 2012) and Relative Margin Machines (RMM) (Shivaswamy and Jebara, 2009b). The latter, RMM, was introduced as an effective and less computationally expensive way to incorporate the spread of the data – second order</context>
<context position="16935" citStr="Tsochantaridis et al., 2004" startWordPosition="2767" endWordPosition="2770">) + y0∈Y(xj ) X �β∗ y0ωj(y+, y0) where the α Lagrange multiplier corresponds to the standard margin constraint, while β and max XL = Xαy − B α,β,β∗ y∈Y(xi) y∈Y(xi) 1119 β* each correspond to a bounding constraint, and wi(y+, y&apos;) corresponds to the difference of f(xi, y+, d+) and f(xi, y&apos;, d&apos;). The weight update can then be obtained from the dual variables: X X X αywi(y+, y) − Oywi(y+, y) + O∗ywi(y+, y) (6) The dual in Equation (5) can be optimized using a cutting plane algorithm, an effective method for solving a relaxed optimization problem in the dual, used in Structured SVM, MIRA, and RMM (Tsochantaridis et al., 2004; Chiang, 2012; Shivaswamy and Jebara, 2009a). The cutting plane presented in Alg. 1 decomposes the overall problem into subproblems which are solved independently by creating working sets Sji , which correspond to the largest violations of either the margin constraint, or bounding constraints, and iteratively satisfying the constraints in each set. The cutting plane in Alg. 1 makes use of the the closed-form gradient-based updates we derived for RM presented in Alg. 2. The updates amount to performing a subgradient descent step to update w in accordance with the constraints. Since the constra</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international conference on Machine learning, ICML ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="11478" citStr="Vapnik, 1995" startWordPosition="1839" endWordPosition="1840"> (1) + max (2) ` = i (y+)) (y+,d+) (y−,d−)∈Y(xi),D(xi) Figure 1: (a) RM and large margin solution comparison and (b) the spread of the projections given by each. RM and large margin solutions are shown with a darker dotted line and a 3 The Relative Margin Machine in SMT 3.1 Relative Margin Machine The margin, the distance between the correct hypothesis and incorrect one, is defined by y+, d+) and It is maximized by minimizing the norm in SVM, or analogously, the proximity constraint in MIRA: arg 2 w wt 2. However, theoretical results supporting large-margin learning, such as the VC-dimension (Vapnik, 1995) or the Rademacher bound (Bartlett and Mendelson, 2003) consider measures of complexity, in addition to the empirical performance, when describing future predictive ability. The measures of complexity usually take the form of some value on the radius of the data, such as the ratio of the radius of the data to the margin (Shivaswamy and Jebara, 2009a). As radius is a way of measuring spread in any projection direction, here we will specifically be interested in the the spread of the data as measured after the projection defined by the learned model s(xi, s(xi,y−,d−). minw || − || w. More formal</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1642" citStr="Watanabe et al., 2007" startWordPosition="232" endWordPosition="235">1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), RAMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion o</context>
<context position="8478" citStr="Watanabe et al., 2007" startWordPosition="1310" endWordPosition="1313">Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to rank pairs of outputs. RAMPION aims to address the disconnect between MT and machine learning by optimizing a structured ramp loss with a concave-convex procedure. 2.1 Large-Margin Learning Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009). The usual presentation of MIRA’s optimization problem is given as a quadratic program: 2We may omit d in some equations for clarity. 1117 2||w − wt||2 + Cξi 1 s.t. s(xi, yi, d) − s(xi, y0, d) ≥ Δi(y0) − ξi where y0 is the single most violated constraint, the cost Δi(y) is computed using an external measure of quality, such as 1-BLEU(yi, y), and a slack variable ξi is introduced to allow for non-separable instances. C acts as a regularization parameter, trading off between margin maximization and constraint violations. While solving the optimization problem relies on com</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>