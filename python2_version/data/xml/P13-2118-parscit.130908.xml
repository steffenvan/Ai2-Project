<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025382">
<title confidence="0.988884">
Joint Apposition Extraction with Syntactic and Semantic Constraints
</title>
<author confidence="0.987331">
Will Radford and James R. Curran
</author>
<affiliation confidence="0.9853965">
-lab, School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.530847">
NSW, 2006, Australia
</address>
<email confidence="0.997619">
{wradford,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.994764" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999810333333333">
Appositions are adjacent NPs used to add
information to a discourse. We propose
systems exploiting syntactic and seman-
tic constraints to extract appositions from
OntoNotes. Our joint log-linear model
outperforms the state-of-the-art Favre and
Hakkani-T¨ur (2009) model by ∼10% on
Broadcast News, and achieves 54.3% F-
score on multiple genres.
</bodyText>
<sectionHeader confidence="0.998421" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99952868">
Appositions are typically adjacent coreferent noun
phrases (NP) that often add information about
named entities (NEs). The apposition in Figure 1
consists of three comma-separated NPs – the first
NP (HEAD) names an entity and the others (ATTRs)
supply age and profession attributes. Attributes
can be difficult to identify despite characteristic
punctuation cues, as punctuation plays many roles
and attributes may have rich substructure.
While linguists have studied apposition in de-
tail (Quirk et al., 1985; Meyer, 1992), most appo-
sition extraction has been within other tasks, such
as coreference resolution (Luo and Zitouni, 2005;
Culotta et al., 2007) and textual entailment (Roth
and Sammons, 2007). Extraction has rarely been
intrinsically evaluated, with Favre and Hakkani-
T¨ur’s work a notable exception.
We analyze apposition distribution in
OntoNotes 4 (Pradhan et al., 2007) and com-
pare rule-based, classification and parsing
extraction systems. Our best system uses a joint
model to classify pairs of NPs with features
that faithfully encode syntactic and semantic
restrictions on appositions, using parse trees and
WordNet synsets.
</bodyText>
<figure confidence="0.872593333333333">
{John Ake}h , {48}a , {a former vice-president
in charge of legal compliance at American Capital
Management &amp; Research Inc., in Houston,}a , .. .
</figure>
<figureCaption confidence="0.999926">
Figure 1: Example apposition from OntoNotes 4
</figureCaption>
<bodyText confidence="0.999901571428571">
Our approach substantially outperforms Favre
and Hakkani-T¨ur on Broadcast News (BN) at
54.9% F-score and has state-of-the-art perfor-
mance 54.3% F-score across multiple genres. Our
results will immediately help the many systems
that already use apposition extraction components,
such as coreference resolution and IE.
</bodyText>
<sectionHeader confidence="0.987625" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999772791666667">
Apposition is widely studied, but “grammarians
vary in the freedom with which they apply the
term ‘apposition”’ (Quirk et al., 1985). They are
usually composed of two or more adjacent NPs,
hierarchically structured, so one is the head NP
(HEAD) and the rest attributes (ATTRs). They are
often flagged using punctuation in text and pauses
in speech. Pragmatically, they allow an author to
introduce new information and build a shared con-
text (Meyer, 1992).
Quirk et al. propose three tests for apposition: i)
each phrase can be omitted without affecting sen-
tence acceptability, ii) each fulfils the same syntac-
tic function in the resultant sentences, iii) extralin-
guistic reference is unchanged. Strict interpreta-
tions may exclude other information-bearing cases
like pseudo-titles (e.g. ({President}a {Bush}h)NP),
but include some adverbial phrases (e.g. {(John
Smith)NP}h, {(formerly (the president)NP)AP}a). We
adopt the OntoNotes guidelines’ relatively strict
interpretation: “a noun phrase that modifies an
immediately-adjacent noun phrase (these may be
separated by only a comma, colon, or parenthe-
sis).” (BBN, 2004–2007).
</bodyText>
<page confidence="0.981916">
671
</page>
<note confidence="0.5298235">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–677,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.995957666666667">
Unit TRAINF DEVF TESTF TRAIN DEV TEST
Sents. 9,595 976 1,098 48,762 6,894 6,896
Appos. 590 64 68 3,877 502 490
</table>
<tableCaption confidence="0.999813">
Table 1: Sentence and apposition distribution
</tableCaption>
<bodyText confidence="0.999225085106383">
Apposition extraction is a common component
in many NLP tasks: coreference resolution (Luo
and Zitouni, 2005; Culotta et al., 2007; Bengt-
son and Roth, 2008; Poon and Domingos, 2008),
textual entailment (Roth and Sammons, 2007;
Cabrio and Magnini, 2010), sentence simplifica-
tion (Miwa et al., 2010; Candido et al., 2009;
Siddharthan, 2002) and summarization (Nenkova
et al., 2005). Comma ambiguity has been studied
in the RTE (Srikumar et al., 2008) and generation
domains (White and Rajkumar, 2008).
Despite this, few papers to our knowledge ex-
plicitly evaluate apposition extraction. Moreover,
apposition extraction is rarely the main research
goal and descriptions of the methods used are of-
ten accordingly terse or do not match our guide-
lines. Lee et al. (2011) use rules to extract appo-
sitions for coreference resolution, selecting only
those that are explicitly flagged using commas or
parentheses. They do not separately mark HEAD
and ATTR and permit relative clauses as an ATTR.
While such differences capture useful information
for coreference resolution, these methods would
be unfairly disadvantaged in a direct evaluation.
Favre and Hakkani-T¨ur (2009, FHT) directly
evaluate three extraction systems on OntoNotes
2.9 news broadcasts. The first retrains the Berke-
ley parser (Petrov and Klein, 2007) on trees la-
belled with appositions by appending the HEAD
and ATTR suffix to NPs – we refer to this as a La-
belled Berkeley Parser (LBP). The second is a CRF
labelling words using an IOB apposition scheme.
Token, POS, NE and BP-label features are used,
as are presence of speech pauses. The final sys-
tem classifies parse tree phrases using an Adaboost
classifier (Schapire and Singer, 2000) with similar
features.
The LBP, IOB and phrase systems score 41.38%,
32.76% and 40.41%, while their best uses LBP tree
labels as IOB features, scoring 42.31%. Their fo-
cus on BN automated speech recognition (ASR)
output, which precludes punctuation cues, does
not indicate how well the methods perform on tex-
tual genres. Moreover all systems use parsers or
parse-label features and do not completely evalu-
ate non-parser methods for extraction despite in-
cluding baselines.
</bodyText>
<table confidence="0.962050777777778">
Form # % Reverse form # % E%
H t A 2109 55.9 A t H 724 19.2 75.1
A H 482 12.8 H A 205 5.4 93.3
H , A 1843 48.9 A , H 532 14.1 63.0
A H 482 12.9 H A 205 5.4 81.3
H ( A 146 3.9 A ( H 16 0.4 85.6
A : H 94 2.5 H : A 23 0.6 88.7
H -- A 66 1.8 A -- H 35 0.9 91.4
A - H 31 0.8 H - A 21 0.6 92.8
</table>
<tableCaption confidence="0.970047666666667">
Table 2: Apposition forms in TRAIN with abstract
(top) and actual (bottom) tokens, e.g., H t A in-
dicates an HEAD, one token then an ATTR.
</tableCaption>
<sectionHeader confidence="0.9947" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999977043478261">
We use apposition-annotated documents from the
English section of OntoNotes 4 (Weischedel et al.,
2011). We manually adjust appositions that do not
have exactly one HEAD and one or more ATTR1.
Some appositions are nested, and we keep only
“leaf” appositions, removing the higher-level ap-
positions.
We follow the CoNLL-2011 scheme to select
TRAIN, DEV and TEST datasets (Pradhan et al.,
2011). OntoNotes 4 is made up of a wide vari-
ety of sources: broadcast conversation and news,
magazine, newswire and web text. Appositions
are most frequent in newswire (one per 192 words)
and least common in broadcast conversation (one
per 645 words) with the others in between (around
one per 315 words).
We also replicate the OntoNotes 2.9 BN data
used by FHT, selecting the same sentences from
OntoNotes 4 (TRAINF/DEVF/TESTF). We do not
“speechify” our data and take a different approach
to nested apposition. Table 1 shows the distri-
bution of sentences and appositions (HEAD-ATTR
pairs).
</bodyText>
<subsectionHeader confidence="0.999343">
3.1 Analysis
</subsectionHeader>
<bodyText confidence="0.9998857">
Most appositions in TRAIN have one ATTR
(97.4%) with few having two (2.5%) or three
(0.1%). HEADs are typically shorter (median 5
tokens, 95% &lt; 7) than ATTRs (median 7 tokens,
95% &lt; 15). Table 2 shows frequent apposition
forms. Comma-separated apposition is the most
common (63%) and 93% are separated by zero or
one token. HEADs are often composed of NEs:
52% PER and 13% ORG, indicating an entity about
which the ATTR adds information.
</bodyText>
<footnote confidence="0.999137">
1Available at http://schwa.org/resources
</footnote>
<page confidence="0.991001">
672
</page>
<table confidence="0.998093818181818">
Pattern and Example P R F
{ne:PER}h # {pos:NP (pos:IN ne:LOC|ORG|GPE)?}a # 73.1 21.9 33.7
“{Jian Zhang}h, {the head of Chinese delegation}a,”
{pos:DT gaz:role|relation}a #? {ne:PER}h 45.9 9.5 15.8
“{his new wife}a {Camilla}h”
{ne:ORG|GPE}h # {pos:DT pos:NP}a # 60.4 6.0 10.9
“{Capetronic Inc.}h, {a Taiwan electronics maker}a,”
{pos:NP}a # {ne:PER}h # 33.7 4.5 7.9
“{The vicar}a, {W.D. Jones}h,”
{ne:PER}h # {pos:NP pos:POS pos:NP}a # 82.0 4.0 7.7
“{Laurence Tribe}h, {Gore ’s attorney}a,”
</table>
<tableCaption confidence="0.9629795">
Table 3: The top-five patterns by recall in the TRAIN dataset. ‘#’ is a pause (e.g., punctuation), ‘|’ a
disjunction and ‘?’ an optional part. Patterns are used to combine tokens into NPs for pos:NP.
</tableCaption>
<sectionHeader confidence="0.980044" genericHeader="method">
4 Extracting Appositions
</sectionHeader>
<bodyText confidence="0.999936575757576">
We investigate different extraction systems using
a range of syntactic information. Our systems that
use syntactic parses generate candidates (pairs of
NPs: p1 and p2) that are then classified as apposi-
tion or not.
This paper contributes three complementary
techniques for more faithfully modelling apposi-
tion. Any adjacent NPs, disregarding intervening
punctuation, could be considered candidates, how-
ever stronger syntactic constraints that only allow
sibling NP children provide higher precision can-
didate sets. Semantic compatibility features en-
coding that an ATTR provides consistent informa-
tion for its HEAD. A joint classifier models the
complete apposition rather than combining sepa-
rate phrase-wise decisions. Taggers and parsers
are trained on TRAIN and evaluated on DEV or
TEST. We use the C&amp;C tools (Curran and Clark,
2003) for POS and NE tagging and the and the
Berkeley Parser (Petrov and Klein, 2007), trained
with default parameters.
Pattern POS, NE and lexical patterns are used
to extract appositions avoiding parsing’s compu-
tational overhead. Rules are applied indepen-
dently to tokenized and tagged sentences, yield-
ing HEAD-ATTR tuples that are later deduplicated.
The rules were manually derived from TRAIN2 and
Table 3 shows the top five of sixteen rules by re-
call over TRAIN. The “role” gazetteer is the transi-
tive closure of hyponyms of the WordNet (Miller,
1995) synset person.n.01 and “relation” man-
ually constructed (e.g., “father”, “colleague”). Tu-
ples are post-processed to remove spurious appo-
</bodyText>
<footnote confidence="0.949004">
2There is some overlap between TRAIN and DEVF/TESTF
with appositions from the latter used in rule generation.
</footnote>
<bodyText confidence="0.990607205882353">
sitions such as comma-separated NE lists3.
Adjacent NPs This low precision, high recall
baseline assumes all candidates, depending on
generation strategy, are appositions.
Rule We only consider HEADs whose syntactic
head is a PER, ORG, LOC or GPE NE. We formalise
semantic compatibility by requiring the ATTR head
to match a gazetteer dependent on the HEAD’s NE
type. To create PER, ORG and LOC gazetteers,
we identified common ATTR heads in TRAIN and
looked for matching WordNet synsets, selecting
the most general hypernym that was still seman-
tically compatible with the HEAD’s NE type.
Gazetteer words are pluralized using pattern.en
(De Smedt and Daelemans, 2012) and normalised.
We use partitive and NML-aware rules (Collins,
1999; Vadas and Curran, 2007) to extract syntactic
heads from ATTRs. These must match the type-
appropriate gazetteer, with ORG and LOC/GPE
falling back to PER (e.g., “the champion, Apple”).
Extracted tuples are post-processed as for Pat-
tern and reranked by the OntoNotes specificity
scale (i.e., NNP &gt; PRO &gt; Def. NP &gt; Indef. NP
&gt; NP), and the more specific unit is assigned
HEAD. Possible ATTRs further to the left or right
are checked, allowing for cases such as Figure 1.
Labelled Berkeley Parser We train a LBP on
TRAIN and recover appositions from parsed sen-
tences. Without syntactic constraints this is equiv-
alent to FHT’s LBP system (LBPF) and indicated by
† in Tables.
Phrase Each NP is independently classified as
HEAD, ATTR or None. We use a log-linear model
with a SGD optimizer from scikit-learn (Pedregosa
</bodyText>
<footnote confidence="0.995284">
3Full description: http://schwa.org/resources
</footnote>
<page confidence="0.989579">
673
</page>
<table confidence="0.99992225">
Model Full system -syn -sem -both +gold
Pattern 44.8 34.9 39.2 - - - - - - - - - 52.2 39.6 45.1
Adj NPs 11.6 58.0 19.3 3.6 65.1 6.8 - - - - - - 16.0 85.3 27.0
Rule 65.3 46.8 54.5 43.7 50.0 46.7 - - - - - - 79.1 62.0 69.5
LBP 66.3 52.2 58.4 47.8 53.0 †50.3 - - - - - - - - -
Phrase 73.2 45.6 56.2 77.7 41.0 53.7 73.2 44.6 55.4 77.7 40.8 ‡53.5 89.0 58.2 70.4
Joint 66.3 49.0 56.4 68.5 48.6 56.9 70.4 47.0 56.4 68.9 48.0 56.6 87.9 69.5 77.6
Joint LBP 69.6 51.0 58.9 69.6 49.6 57.9 71.5 49.0 58.2 68.3 48.6 56.8 - -
</table>
<tableCaption confidence="0.925025">
Table 4: Results over DEV: each column shows precision, recall and F-score. -syn/-sem/-both show the
impact of removing constraints/features, +gold shows the impact of parse and tagging errors.
</tableCaption>
<bodyText confidence="0.99504">
et al., 2011). The binary features are calculated
from a generated candidate phrase (p) and are the
same as FHT’s phrase system (PhraseF), denoted
‡ in Tables. In addition, we propose the fea-
tures below and to decode classifications, adjacent
apposition-classified NPs are re-ordered by speci-
ficity.
</bodyText>
<listItem confidence="0.9998743">
• p precedes/follows punctuation/interjection
• p starts with a DT or PRP$ (e.g., “{the
director}a” or “{her husband}a”)
• p’s syntactic head matches a NE-specific se-
mantic gazetteer (e.g., “{the famous actor}a”
→ PER, “{investment bank}a” → ORG)
• p’s syntactic head has the POS CD (e.g.,
“{John Smith}h, {34}a, ... ”)
• p’s NE type (e.g., “{John Smith}h” → PER)
• Specificity rank
</listItem>
<bodyText confidence="0.99796375">
Joint The final system classifies pairs of phrases
(p1, p2) as: HEAD-ATTR, ATTR-HEAD or None.
The system uses the phrase model features as
above as well as pairwise features:
</bodyText>
<listItem confidence="0.987607363636364">
• the cross-product of selected features for p1
and p2: gazetteer matches, NE type, speci-
ficity rank. This models the compatibility be-
tween p1 and p2. For example, if the HEAD
has the NE type PER and the ATTR has the
syntactic head in the PER gazetteer, for ex-
ample “{Tom Cruise}h, {famous actor}a,” →
(p1: PER, p2: PER-gaz)
• If semantic features are found in p1 and p2
• p1/p2 specificity (e.g., equal, p1 &gt; p2)
• whether p1 is an acronym of p2 or vice-versa
</listItem>
<sectionHeader confidence="0.999594" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999940914893617">
We evaluate by comparing the extracted HEAD-
ATTR pairs against the gold-standard. Correct
pairs match gold-standard bounds and label. We
report precision (P), recall (R) and F1-score (F).
Table 4 shows our systems’ performance on the
multi-genre DEV dataset, the impact of remov-
ing syntactic constraints, semantic features and
parse/tag error. Pattern performance is reasonable
at 39.2% F-score given its lack of full syntactic
information. All other results use parses and, al-
though it has a low F-score, the Adjacent NPs’
65.1% recall, without syntactic constraints, is the
upper bound for the parse-based systems. Statis-
tical models improve performance, with the joint
models better than the higher-precision phrase
model as the latter must make two independently
correct classification decisions. Our best system
has an F-score of 58.9% using a joint model over
the de-labelled trees produced by the LBP. This
indicates that although our model does not use
the apposition labels from the tree, the tree is a
more suitable structure for extraction. This sys-
tem substantially improves on our implementation
of FHT’s LBPF (†) and PhraseF (‡) systems by 8.6%
and 5.4%4.
Removing syntactic constraints mostly reduces
performance in parse-based systems as the system
must consider lower-quality candidates. The F-
score increase is driven by higher precision at min-
imal cost to recall. Removing semantic features
has less impact and removing both is most detri-
mental to performance. These features have less
impact on joint models; indeed, joint performance
using BP trees increases without the features, per-
haps as joint models already model the syntactic
context.
We evaluate the impact of parser and tagger
error by using gold-standard resources. Gold-
standard tags and trees improve recall in all cases
leading to F-score improvements (+gold). The
pattern system is reasonably robust to automatic
tagging errors, but parse-based models suffer con-
siderably from automatic parses. To compare the
impact of tagging and parsing error, we configure
the joint system to use gold parses and automatic
NE tags and vice versa. Using automatic tags does
not greatly impact performance (-1.3%), whereas
</bodyText>
<footnote confidence="0.994519">
4We do not implement the IOB or use LBP features for
TRAIN as these would require n-fold parser training.
</footnote>
<page confidence="0.995678">
674
</page>
<table confidence="0.99988">
Model P R F
LBPF † 53.1 46.9 49.8
PhraseF ‡ 71.5 30.2 42.5
Pattern 44.8 34.3 38.8
LBP 63.9 45.1 52.9
Joint LBP 66.9 45.7 54.3
</table>
<tableCaption confidence="0.982724">
Table 5: Results over TEST: FHT’s (top) and our
(bottom) systems.
</tableCaption>
<table confidence="0.999948333333333">
Error BP LBP δ
PP Attachment 5,585 5,396 -189
NP Internal Structure 1,483 1,338 -145
Other 3,164 3,064 -100
Clause Attachment 3,960 3,867 -93
Modifier Attachment 1,523 1,700 177
Co-ordination 3,095 3,245 150
NP Attachment 2,615 2,680 65
Total 30,189 29,859 -330
</table>
<tableCaption confidence="0.99927">
Table 6: Selected BP/LBP parse error distribution.
</tableCaption>
<bodyText confidence="0.999474833333334">
using automatic parses causes a drop of around
20% to 57.7%, demonstrating that syntactic infor-
mation is crucial for apposition extraction.
We compare our work with Favre and Hakkani-
T¨ur (2009), training with TRAINF and evaluating
over TESTF– exclusively BN data. Our implemen-
tations of their systems, PhraseF and LBPF, per-
form at 43.6% and 44.1%. Our joint LBP system
is substantially better, scoring 54.9%.
Table 5 shows the performance of our best sys-
tems on the TEST dataset and these follow the
same trend as DEV. Joint LBP performs the best
at 54.3%, 4.5% above LBPF.
Finally, we test whether labelling appositions
can help parsing. We parse DEV trees with LBP
and BP, remove apposition labels and analyse
the impact of labelling using the Berkeley Parser
Analyser (Kummerfeld et al., 2012). Table 6
shows the LBP makes fewer errors, particularly
NP internal structuring, PP and clause attachment
classes at the cost of modifier attachment and co-
ordination errors. Rather than increasing parsing
difficulty, apposition labels seem complementary,
improving performance.
</bodyText>
<sectionHeader confidence="0.995569" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998834">
We present three apposition extraction techniques.
Linguistic tests for apposition motivate strict syn-
tactic constraints on candidates and semantic fea-
tures encode the addition of compatible informa-
tion. Joint models more faithfully capture apposi-
tion structure and our best system achieves state-
of-the-art performance of 54.3%. Our results will
immediately benefit the large number of systems
with apposition extraction components for coref-
erence resolution and IE.
</bodyText>
<sectionHeader confidence="0.951684" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999624714285714">
The authors would like to thank the anonymous re-
viewers for their suggestions. Thanks must also go
to Benoit Favre for his clear writing and help an-
swering our questions as we replicated his dataset
and system. This work has been supported by
ARC Discovery grant DP1097291 and the Capi-
tal Markets CRC Computable News project.
</bodyText>
<sectionHeader confidence="0.886927" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.822293363636364">
BBN. 2004–2007. Co-reference guidelines for en-
glish ontonotes. Technical Report v6.0, BBN
Technologies.
Eric Bengtson and Dan Roth. 2008. Understand-
ing the value of features for coreference resolu-
tion. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 294–303. Association for Com-
putational Linguistics, Honolulu, Hawaii.
Elena Cabrio and Bernardo Magnini. 2010. To-
ward qualitative evaluation of textual entailment
systems. In Coling 2010: Posters, pages 99–
107. Coling 2010 Organizing Committee, Bei-
jing, China.
Arnaldo Candido, Erick Maziero, Lucia Specia,
Caroline Gasperin, Thiago Pardo, and Sandra
Aluisio. 2009. Supporting the adaptation of
texts for poor literacy readers: a text simplifi-
cation editor for brazilian portuguese. In Pro-
ceedings of the Fourth Workshop on Innovative
Use of NLP for Building Educational Applica-
tions, pages 34–42. Association for Computa-
tional Linguistics, Boulder, Colorado.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Aron Culotta, Michael Wick, and Andrew Mc-
Callum. 2007. First-order probabilistic mod-
els for coreference resolution. In Human Lan-
guage Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Proceedings of
the Main Conference, pages 81–88. Association
</bodyText>
<page confidence="0.997609">
675
</page>
<reference confidence="0.99457303030303">
for Computational Linguistics, Rochester, New
York.
James Curran and Stephen Clark. 2003. Language
independent ner using a maximum entropy tag-
ger. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference
on Natural Language Learning at HLT-NAACL
2003, pages 164–167.
Tom De Smedt and Walter Daelemans. 2012. Pat-
tern for python. Journal of Machine Learning
Research, 13:2013–2035.
Benoit Favre and Dilek Hakkani-T¨ur. 2009.
Phrase and word level strategies for detecting
appositions in speech. In Proceedings of Inter-
speech 2009, pages 2711–2714. Brighton, UK.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser show-
down at the wall street corral: An empirical in-
vestigation of error types in parser output. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 1048–1059. Jeju Island, South
Korea.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan
Jurafsky. 2011. Stanford’s multi-pass sieve
coreference resolution system at the conll-
2011 shared task. In Proceedings of the
CoNLL-2011 Shared Task. URL pubs/
conllst2011-coref.pdf.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic
features. In Proceedings of Human Lan-
guage Technology Conference and Conference
on Empirical Methods in Natural Language
Processing, pages 660–667. Association for
Computational Linguistics, Vancouver, British
Columbia, Canada.
Charles F. Meyer. 1992. Apposition in Contem-
porary English. Cambridge University Press,
Cambridge, UK.
George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the
ACM, 38:39–41.
Makoto Miwa, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii. 2010. Entity-focused sentence
simplification for relation extraction. In Pro-
ceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010),
pages 788–796. Coling 2010 Organizing Com-
mittee, Beijing, China.
Ani Nenkova, Advaith Siddharthan, and Kath-
leen McKeown. 2005. Automatically learn-
ing cognitive status for multi-document sum-
marization of newswire. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing, pages 241–248. Associa-
tion for Computational Linguistics, Vancouver,
British Columbia, Canada.
F. Pedregosa, G. Varoquaux, A. Gramfort,
V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Van-
derplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine Learning in Python . Journal
of Machine Learning Research, 12:2825–2830.
Slav Petrov and Dan Klein. 2007. Learning and in-
ference for hierarchically split PCFGs. In Pro-
ceedings of the 22nd AAAI Conference ofArtifi-
cial Intelligence, pages 1642–1645. Vancouver,
Canada.
Hoifung Poon and Pedro Domingos. 2008.
Joint unsupervised coreference resolution with
Markov Logic. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 650–659. Associ-
ation for Computational Linguistics, Honolulu,
Hawaii.
Sameer Pradhan, Lance Ramshaw, Mitchell Mar-
cus, Martha Palmer, Ralph Weischedel, and
Nianwen Xue. 2011. CoNLL-2011 shared
task: Modeling unrestricted coreference in
OntoNotes. In Proceedings of the Fifteenth
Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–27.
Portland, OR USA.
Sameer S. Pradhan, Eduard Hovy, Mitch Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified rela-
tional semantic representation. In Proceedings
of the International Conference on Semantic
Computing, pages 517–526. Washington, DC
USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey
Leech, and Jan Svartvik. 1985. A Comprehen-
sive Grammar of the English Language. Gen-
eral Grammar Series. Longman, London, UK.
</reference>
<page confidence="0.988496">
676
</page>
<reference confidence="0.999244853658536">
Dan Roth and Mark Sammons. 2007. Seman-
tic and logical inference model for textual en-
tailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Para-
phrasing, pages 107–112. Association for Com-
putational Linguistics, Prague.
Robert E. Schapire and Yoram Singer. 2000. Boos-
texter: A boosting-based systemfor text catego-
rization. Machine Learning, 39(2-3):135–168.
Advaith Siddharthan. 2002. Resolving attachment
and clause boundary ambiguities for simplify-
ing relative clause constructs. In Proceedings of
the ACL Student Research Workshop (ACLSRW
2002), pages 60–65. Association for Computa-
tional Linguistics, Philadelphia.
Vivek Srikumar, Roi Reichart, Mark Sammons,
Ari Rappoport, and Dan Roth. 2008. Extraction
of entailed semantic relations through syntax-
based comma resolution. In Proceedings of
ACL-08: HLT, pages 1030–1038. Columbus,
OH USA.
David Vadas and James R. Curran. 2007. Pars-
ing internal noun phrase structure with collins’
models. In Proceedings of the Australasian
Language Technology Workshop 2007, pages
109–116. Melbourne, Australia.
Ralph Weischedel, Martha Palmer, Mitchell Mar-
cus, Eduard Hovy, Sameer Pradhan, Lance
Ramshaw, Nianwen Xue, Ann Taylor, Jeff
Kaufman, Michelle Franchini, Mohammed El-
Bachouti, Robert Belvin, and Ann Houston.
2011. OntoNotes Release 4.0. Technical re-
port, Linguistic Data Consortium, Philadelphia,
PA USA.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for
broad-coverage surface realization with CCG.
In Coling 2008: Proceedings of the workshop
on Grammar Engineering Across Frameworks,
pages 17–24. Coling 2008 Organizing Commit-
tee, Manchester, England.
</reference>
<page confidence="0.998166">
677
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930952">
<title confidence="0.995774">Joint Apposition Extraction with Syntactic and Semantic Constraints</title>
<author confidence="0.97706">R Radford</author>
<affiliation confidence="0.998344">lab, School of Information University of Sydney</affiliation>
<address confidence="0.988394">NSW, 2006, Australia</address>
<abstract confidence="0.9943968">are adjacent used to add information to a discourse. We propose systems exploiting syntactic and semantic constraints to extract appositions from OntoNotes. Our joint log-linear model outperforms the state-of-the-art Favre and (2009) model by on Broadcast News, and achieves 54.3% Fscore on multiple genres.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<institution>for Computational Linguistics,</institution>
<location>Rochester, New York.</location>
<marker></marker>
<rawString>for Computational Linguistics, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent ner using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<pages>164--167</pages>
<editor>Miles Osborne, editors,</editor>
<contexts>
<context position="9353" citStr="Curran and Clark, 2003" startWordPosition="1478" endWordPosition="1481">t. This paper contributes three complementary techniques for more faithfully modelling apposition. Any adjacent NPs, disregarding intervening punctuation, could be considered candidates, however stronger syntactic constraints that only allow sibling NP children provide higher precision candidate sets. Semantic compatibility features encoding that an ATTR provides consistent information for its HEAD. A joint classifier models the complete apposition rather than combining separate phrase-wise decisions. Taggers and parsers are trained on TRAIN and evaluated on DEV or TEST. We use the C&amp;C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters. Pattern POS, NE and lexical patterns are used to extract appositions avoiding parsing’s computational overhead. Rules are applied independently to tokenized and tagged sentences, yielding HEAD-ATTR tuples that are later deduplicated. The rules were manually derived from TRAIN2 and Table 3 shows the top five of sixteen rules by recall over TRAIN. The “role” gazetteer is the transitive closure of hyponyms of the WordNet (Miller, 1995) synset person.n.01 and “relation” manually const</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James Curran and Stephen Clark. 2003. Language independent ner using a maximum entropy tagger. In Walter Daelemans and Miles Osborne, editors, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 164–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom De Smedt</author>
<author>Walter Daelemans</author>
</authors>
<title>Pattern for python.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--2013</pages>
<marker>De Smedt, Daelemans, 2012</marker>
<rawString>Tom De Smedt and Walter Daelemans. 2012. Pattern for python. Journal of Machine Learning Research, 13:2013–2035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Favre</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Phrase and word level strategies for detecting appositions in speech.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>2711--2714</pages>
<location>Brighton, UK.</location>
<marker>Favre, Hakkani-T¨ur, 2009</marker>
<rawString>Benoit Favre and Dilek Hakkani-T¨ur. 2009. Phrase and word level strategies for detecting appositions in speech. In Proceedings of Interspeech 2009, pages 2711–2714. Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser showdown at the wall street corral: An empirical investigation of error types in parser output.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1048--1059</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="17384" citStr="Kummerfeld et al., 2012" startWordPosition="2815" endWordPosition="2818">nd HakkaniT¨ur (2009), training with TRAINF and evaluating over TESTF– exclusively BN data. Our implementations of their systems, PhraseF and LBPF, perform at 43.6% and 44.1%. Our joint LBP system is substantially better, scoring 54.9%. Table 5 shows the performance of our best systems on the TEST dataset and these follow the same trend as DEV. Joint LBP performs the best at 54.3%, 4.5% above LBPF. Finally, we test whether labelling appositions can help parsing. We parse DEV trees with LBP and BP, remove apposition labels and analyse the impact of labelling using the Berkeley Parser Analyser (Kummerfeld et al., 2012). Table 6 shows the LBP makes fewer errors, particularly NP internal structuring, PP and clause attachment classes at the cost of modifier attachment and coordination errors. Rather than increasing parsing difficulty, apposition labels seem complementary, improving performance. 6 Conclusion We present three apposition extraction techniques. Linguistic tests for apposition motivate strict syntactic constraints on candidates and semantic features encode the addition of compatible information. Joint models more faithfully capture apposition structure and our best system achieves stateof-the-art p</context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. 2012. Parser showdown at the wall street corral: An empirical investigation of error types in parser output. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1048–1059. Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the CoNLL-2011 Shared Task. URL pubs/ conllst2011-coref.pdf.</booktitle>
<contexts>
<context position="4490" citStr="Lee et al. (2011)" startWordPosition="661" endWordPosition="664">th, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or parentheses. They do not separately mark HEAD and ATTR and permit relative clauses as an ATTR. While such differences capture useful information for coreference resolution, these methods would be unfairly disadvantaged in a direct evaluation. Favre and Hakkani-T¨ur (2009, FHT) directly evaluate three extraction systems on OntoNotes 2.9 news broadcasts. The first retrains the Berkeley parser (Petrov and Klein, 2007) on trees labelled with appositions by appending the HE</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll2011 shared task. In Proceedings of the CoNLL-2011 Shared Task. URL pubs/ conllst2011-coref.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Imed Zitouni</author>
</authors>
<title>Multilingual coreference resolution with syntactic features.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--667</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1215" citStr="Luo and Zitouni, 2005" startWordPosition="170" endWordPosition="173">are typically adjacent coreferent noun phrases (NP) that often add information about named entities (NEs). The apposition in Figure 1 consists of three comma-separated NPs – the first NP (HEAD) names an entity and the others (ATTRs) supply age and profession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, using parse trees and WordNet synsets. {John Ake}h , {48}a , {a former vice-president in charge of legal compliance at American</context>
<context position="3834" citStr="Luo and Zitouni, 2005" startWordPosition="557" endWordPosition="560">interpretation: “a noun phrase that modifies an immediately-adjacent noun phrase (these may be separated by only a comma, colon, or parenthesis).” (BBN, 2004–2007). 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–677, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly</context>
</contexts>
<marker>Luo, Zitouni, 2005</marker>
<rawString>Xiaoqiang Luo and Imed Zitouni. 2005. Multilingual coreference resolution with syntactic features. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 660–667. Association for Computational Linguistics, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles F Meyer</author>
</authors>
<title>Apposition in Contemporary English.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="1104" citStr="Meyer, 1992" startWordPosition="155" endWordPosition="156">l by ∼10% on Broadcast News, and achieves 54.3% Fscore on multiple genres. 1 Introduction Appositions are typically adjacent coreferent noun phrases (NP) that often add information about named entities (NEs). The apposition in Figure 1 consists of three comma-separated NPs – the first NP (HEAD) names an entity and the others (ATTRs) supply age and profession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, using parse tree</context>
<context position="2704" citStr="Meyer, 1992" startWordPosition="399" endWordPosition="400">es. Our results will immediately help the many systems that already use apposition extraction components, such as coreference resolution and IE. 2 Background Apposition is widely studied, but “grammarians vary in the freedom with which they apply the term ‘apposition”’ (Quirk et al., 1985). They are usually composed of two or more adjacent NPs, hierarchically structured, so one is the head NP (HEAD) and the rest attributes (ATTRs). They are often flagged using punctuation in text and pauses in speech. Pragmatically, they allow an author to introduce new information and build a shared context (Meyer, 1992). Quirk et al. propose three tests for apposition: i) each phrase can be omitted without affecting sentence acceptability, ii) each fulfils the same syntactic function in the resultant sentences, iii) extralinguistic reference is unchanged. Strict interpretations may exclude other information-bearing cases like pseudo-titles (e.g. ({President}a {Bush}h)NP), but include some adverbial phrases (e.g. {(John Smith)NP}h, {(formerly (the president)NP)AP}a). We adopt the OntoNotes guidelines’ relatively strict interpretation: “a noun phrase that modifies an immediately-adjacent noun phrase (these may</context>
</contexts>
<marker>Meyer, 1992</marker>
<rawString>Charles F. Meyer. 1992. Apposition in Contemporary English. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="9904" citStr="Miller, 1995" startWordPosition="1570" endWordPosition="1571"> on DEV or TEST. We use the C&amp;C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters. Pattern POS, NE and lexical patterns are used to extract appositions avoiding parsing’s computational overhead. Rules are applied independently to tokenized and tagged sentences, yielding HEAD-ATTR tuples that are later deduplicated. The rules were manually derived from TRAIN2 and Table 3 shows the top five of sixteen rules by recall over TRAIN. The “role” gazetteer is the transitive closure of hyponyms of the WordNet (Miller, 1995) synset person.n.01 and “relation” manually constructed (e.g., “father”, “colleague”). Tuples are post-processed to remove spurious appo2There is some overlap between TRAIN and DEVF/TESTF with appositions from the latter used in rule generation. sitions such as comma-separated NE lists3. Adjacent NPs This low precision, high recall baseline assumes all candidates, depending on generation strategy, are appositions. Rule We only consider HEADs whose syntactic head is a PER, ORG, LOC or GPE NE. We formalise semantic compatibility by requiring the ATTR head to match a gazetteer dependent on the HE</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Sætre</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Entity-focused sentence simplification for relation extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>788--796</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4022" citStr="Miwa et al., 2010" startWordPosition="587" endWordPosition="590"> Annual Meeting of the Association for Computational Linguistics, pages 671–677, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or paren</context>
</contexts>
<marker>Miwa, Sætre, Miyao, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Sætre, Yusuke Miyao, and Jun’ichi Tsujii. 2010. Entity-focused sentence simplification for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788–796. Coling 2010 Organizing Committee, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Advaith Siddharthan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatically learning cognitive status for multi-document summarization of newswire.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>241--248</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="4105" citStr="Nenkova et al., 2005" startWordPosition="599" endWordPosition="602">, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or parentheses. They do not separately mark HEAD and ATTR and permit relative clauses as an</context>
</contexts>
<marker>Nenkova, Siddharthan, McKeown, 2005</marker>
<rawString>Ani Nenkova, Advaith Siddharthan, and Kathleen McKeown. 2005. Automatically learning cognitive status for multi-document summarization of newswire. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 241–248. Association for Computational Linguistics, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikitlearn: Machine Learning in Python .</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikitlearn: Machine Learning in Python . Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Learning and inference for hierarchically split PCFGs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd AAAI Conference ofArtificial Intelligence,</booktitle>
<pages>1642--1645</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5035" citStr="Petrov and Klein, 2007" startWordPosition="741" endWordPosition="744"> are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or parentheses. They do not separately mark HEAD and ATTR and permit relative clauses as an ATTR. While such differences capture useful information for coreference resolution, these methods would be unfairly disadvantaged in a direct evaluation. Favre and Hakkani-T¨ur (2009, FHT) directly evaluate three extraction systems on OntoNotes 2.9 news broadcasts. The first retrains the Berkeley parser (Petrov and Klein, 2007) on trees labelled with appositions by appending the HEAD and ATTR suffix to NPs – we refer to this as a Labelled Berkeley Parser (LBP). The second is a CRF labelling words using an IOB apposition scheme. Token, POS, NE and BP-label features are used, as are presence of speech pauses. The final system classifies parse tree phrases using an Adaboost classifier (Schapire and Singer, 2000) with similar features. The LBP, IOB and phrase systems score 41.38%, 32.76% and 40.41%, while their best uses LBP tree labels as IOB features, scoring 42.31%. Their focus on BN automated speech recognition (ASR</context>
<context position="9433" citStr="Petrov and Klein, 2007" startWordPosition="1493" endWordPosition="1496">elling apposition. Any adjacent NPs, disregarding intervening punctuation, could be considered candidates, however stronger syntactic constraints that only allow sibling NP children provide higher precision candidate sets. Semantic compatibility features encoding that an ATTR provides consistent information for its HEAD. A joint classifier models the complete apposition rather than combining separate phrase-wise decisions. Taggers and parsers are trained on TRAIN and evaluated on DEV or TEST. We use the C&amp;C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters. Pattern POS, NE and lexical patterns are used to extract appositions avoiding parsing’s computational overhead. Rules are applied independently to tokenized and tagged sentences, yielding HEAD-ATTR tuples that are later deduplicated. The rules were manually derived from TRAIN2 and Table 3 shows the top five of sixteen rules by recall over TRAIN. The “role” gazetteer is the transitive closure of hyponyms of the WordNet (Miller, 1995) synset person.n.01 and “relation” manually constructed (e.g., “father”, “colleague”). Tuples are post-processed to remove spurio</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Learning and inference for hierarchically split PCFGs. In Proceedings of the 22nd AAAI Conference ofArtificial Intelligence, pages 1642–1645. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov Logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>650--659</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3907" citStr="Poon and Domingos, 2008" startWordPosition="570" endWordPosition="573">oun phrase (these may be separated by only a comma, colon, or parenthesis).” (BBN, 2004–2007). 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–677, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to ext</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov Logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650–659. Association for Computational Linguistics, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--27</pages>
<location>Portland, OR USA.</location>
<contexts>
<context position="6720" citStr="Pradhan et al., 2011" startWordPosition="1066" endWordPosition="1069">.5 H : A 23 0.6 88.7 H -- A 66 1.8 A -- H 35 0.9 91.4 A - H 31 0.8 H - A 21 0.6 92.8 Table 2: Apposition forms in TRAIN with abstract (top) and actual (bottom) tokens, e.g., H t A indicates an HEAD, one token then an ATTR. 3 Data We use apposition-annotated documents from the English section of OntoNotes 4 (Weischedel et al., 2011). We manually adjust appositions that do not have exactly one HEAD and one or more ATTR1. Some appositions are nested, and we keep only “leaf” appositions, removing the higher-level appositions. We follow the CoNLL-2011 scheme to select TRAIN, DEV and TEST datasets (Pradhan et al., 2011). OntoNotes 4 is made up of a wide variety of sources: broadcast conversation and news, magazine, newswire and web text. Appositions are most frequent in newswire (one per 192 words) and least common in broadcast conversation (one per 645 words) with the others in between (around one per 315 words). We also replicate the OntoNotes 2.9 BN data used by FHT, selecting the same sentences from OntoNotes 4 (TRAINF/DEVF/TESTF). We do not “speechify” our data and take a different approach to nested apposition. Table 1 shows the distribution of sentences and appositions (HEAD-ATTR pairs). 3.1 Analysis </context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–27. Portland, OR USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Eduard Hovy</author>
<author>Mitch Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: A unified relational semantic representation.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Semantic Computing,</booktitle>
<pages>517--526</pages>
<location>Washington, DC USA.</location>
<contexts>
<context position="1467" citStr="Pradhan et al., 2007" startWordPosition="207" endWordPosition="210">ession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, using parse trees and WordNet synsets. {John Ake}h , {48}a , {a former vice-president in charge of legal compliance at American Capital Management &amp; Research Inc., in Houston,}a , .. . Figure 1: Example apposition from OntoNotes 4 Our approach substantially outperforms Favre and Hakkani-T¨ur on Broadcast News (BN) at 54.9% F-score and has state-of-the-art performance 54.3% F-s</context>
</contexts>
<marker>Pradhan, Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2007</marker>
<rawString>Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007. OntoNotes: A unified relational semantic representation. In Proceedings of the International Conference on Semantic Computing, pages 517–526. Washington, DC USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language. General Grammar Series.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>London, UK.</location>
<contexts>
<context position="1090" citStr="Quirk et al., 1985" startWordPosition="151" endWordPosition="154">ani-T¨ur (2009) model by ∼10% on Broadcast News, and achieves 54.3% Fscore on multiple genres. 1 Introduction Appositions are typically adjacent coreferent noun phrases (NP) that often add information about named entities (NEs). The apposition in Figure 1 consists of three comma-separated NPs – the first NP (HEAD) names an entity and the others (ATTRs) supply age and profession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, us</context>
<context position="2382" citStr="Quirk et al., 1985" startWordPosition="344" endWordPosition="347">-president in charge of legal compliance at American Capital Management &amp; Research Inc., in Houston,}a , .. . Figure 1: Example apposition from OntoNotes 4 Our approach substantially outperforms Favre and Hakkani-T¨ur on Broadcast News (BN) at 54.9% F-score and has state-of-the-art performance 54.3% F-score across multiple genres. Our results will immediately help the many systems that already use apposition extraction components, such as coreference resolution and IE. 2 Background Apposition is widely studied, but “grammarians vary in the freedom with which they apply the term ‘apposition”’ (Quirk et al., 1985). They are usually composed of two or more adjacent NPs, hierarchically structured, so one is the head NP (HEAD) and the rest attributes (ATTRs). They are often flagged using punctuation in text and pauses in speech. Pragmatically, they allow an author to introduce new information and build a shared context (Meyer, 1992). Quirk et al. propose three tests for apposition: i) each phrase can be omitted without affecting sentence acceptability, ii) each fulfils the same syntactic function in the resultant sentences, iii) extralinguistic reference is unchanged. Strict interpretations may exclude ot</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. General Grammar Series. Longman, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>Semantic and logical inference model for textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>107--112</pages>
<location>Prague.</location>
<contexts>
<context position="1286" citStr="Roth and Sammons, 2007" startWordPosition="181" endWordPosition="184">formation about named entities (NEs). The apposition in Figure 1 consists of three comma-separated NPs – the first NP (HEAD) names an entity and the others (ATTRs) supply age and profession attributes. Attributes can be difficult to identify despite characteristic punctuation cues, as punctuation plays many roles and attributes may have rich substructure. While linguists have studied apposition in detail (Quirk et al., 1985; Meyer, 1992), most apposition extraction has been within other tasks, such as coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007) and textual entailment (Roth and Sammons, 2007). Extraction has rarely been intrinsically evaluated, with Favre and HakkaniT¨ur’s work a notable exception. We analyze apposition distribution in OntoNotes 4 (Pradhan et al., 2007) and compare rule-based, classification and parsing extraction systems. Our best system uses a joint model to classify pairs of NPs with features that faithfully encode syntactic and semantic restrictions on appositions, using parse trees and WordNet synsets. {John Ake}h , {48}a , {a former vice-president in charge of legal compliance at American Capital Management &amp; Research Inc., in Houston,}a , .. . Figure 1: Exa</context>
<context position="3951" citStr="Roth and Sammons, 2007" startWordPosition="576" endWordPosition="579">omma, colon, or parenthesis).” (BBN, 2004–2007). 671 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671–677, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution,</context>
</contexts>
<marker>Roth, Sammons, 2007</marker>
<rawString>Dan Roth and Mark Sammons. 2007. Semantic and logical inference model for textual entailment. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 107–112. Association for Computational Linguistics, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based systemfor text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="5424" citStr="Schapire and Singer, 2000" startWordPosition="810" endWordPosition="813">thods would be unfairly disadvantaged in a direct evaluation. Favre and Hakkani-T¨ur (2009, FHT) directly evaluate three extraction systems on OntoNotes 2.9 news broadcasts. The first retrains the Berkeley parser (Petrov and Klein, 2007) on trees labelled with appositions by appending the HEAD and ATTR suffix to NPs – we refer to this as a Labelled Berkeley Parser (LBP). The second is a CRF labelling words using an IOB apposition scheme. Token, POS, NE and BP-label features are used, as are presence of speech pauses. The final system classifies parse tree phrases using an Adaboost classifier (Schapire and Singer, 2000) with similar features. The LBP, IOB and phrase systems score 41.38%, 32.76% and 40.41%, while their best uses LBP tree labels as IOB features, scoring 42.31%. Their focus on BN automated speech recognition (ASR) output, which precludes punctuation cues, does not indicate how well the methods perform on textual genres. Moreover all systems use parsers or parse-label features and do not completely evaluate non-parser methods for extraction despite including baselines. Form # % Reverse form # % E% H t A 2109 55.9 A t H 724 19.2 75.1 A H 482 12.8 H A 205 5.4 93.3 H , A 1843 48.9 A , H 532 14.1 63</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. Boostexter: A boosting-based systemfor text categorization. Machine Learning, 39(2-3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Resolving attachment and clause boundary ambiguities for simplifying relative clause constructs.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Student Research Workshop (ACLSRW</booktitle>
<pages>60--65</pages>
<location>Philadelphia.</location>
<contexts>
<context position="4064" citStr="Siddharthan, 2002" startWordPosition="595" endWordPosition="596">mputational Linguistics, pages 671–677, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or parentheses. They do not separately mark HEAD a</context>
</contexts>
<marker>Siddharthan, 2002</marker>
<rawString>Advaith Siddharthan. 2002. Resolving attachment and clause boundary ambiguities for simplifying relative clause constructs. In Proceedings of the ACL Student Research Workshop (ACLSRW 2002), pages 60–65. Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Roi Reichart</author>
<author>Mark Sammons</author>
<author>Ari Rappoport</author>
<author>Dan Roth</author>
</authors>
<title>Extraction of entailed semantic relations through syntaxbased comma resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1030--1038</pages>
<location>Columbus, OH USA.</location>
<contexts>
<context position="4174" citStr="Srikumar et al., 2008" startWordPosition="611" endWordPosition="614">onal Linguistics Unit TRAINF DEVF TESTF TRAIN DEV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or parentheses. They do not separately mark HEAD and ATTR and permit relative clauses as an ATTR. While such differences capture useful information for corefere</context>
</contexts>
<marker>Srikumar, Reichart, Sammons, Rappoport, Roth, 2008</marker>
<rawString>Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rappoport, and Dan Roth. 2008. Extraction of entailed semantic relations through syntaxbased comma resolution. In Proceedings of ACL-08: HLT, pages 1030–1038. Columbus, OH USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing internal noun phrase structure with collins’ models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>109--116</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="10910" citStr="Vadas and Curran, 2007" startWordPosition="1722" endWordPosition="1725">eration strategy, are appositions. Rule We only consider HEADs whose syntactic head is a PER, ORG, LOC or GPE NE. We formalise semantic compatibility by requiring the ATTR head to match a gazetteer dependent on the HEAD’s NE type. To create PER, ORG and LOC gazetteers, we identified common ATTR heads in TRAIN and looked for matching WordNet synsets, selecting the most general hypernym that was still semantically compatible with the HEAD’s NE type. Gazetteer words are pluralized using pattern.en (De Smedt and Daelemans, 2012) and normalised. We use partitive and NML-aware rules (Collins, 1999; Vadas and Curran, 2007) to extract syntactic heads from ATTRs. These must match the typeappropriate gazetteer, with ORG and LOC/GPE falling back to PER (e.g., “the champion, Apple”). Extracted tuples are post-processed as for Pattern and reranked by the OntoNotes specificity scale (i.e., NNP &gt; PRO &gt; Def. NP &gt; Indef. NP &gt; NP), and the more specific unit is assigned HEAD. Possible ATTRs further to the left or right are checked, allowing for cases such as Figure 1. Labelled Berkeley Parser We train a LBP on TRAIN and recover appositions from parsed sentences. Without syntactic constraints this is equivalent to FHT’s LB</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007. Parsing internal noun phrase structure with collins’ models. In Proceedings of the Australasian Language Technology Workshop 2007, pages 109–116. Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Martha Palmer</author>
<author>Mitchell Marcus</author>
<author>Eduard Hovy</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
</authors>
<date>2011</date>
<booktitle>OntoNotes Release 4.0. Technical report, Linguistic Data Consortium,</booktitle>
<location>Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed ElBachouti, Robert Belvin, and Ann Houston.</location>
<contexts>
<context position="6432" citStr="Weischedel et al., 2011" startWordPosition="1019" endWordPosition="1022"> not completely evaluate non-parser methods for extraction despite including baselines. Form # % Reverse form # % E% H t A 2109 55.9 A t H 724 19.2 75.1 A H 482 12.8 H A 205 5.4 93.3 H , A 1843 48.9 A , H 532 14.1 63.0 A H 482 12.9 H A 205 5.4 81.3 H ( A 146 3.9 A ( H 16 0.4 85.6 A : H 94 2.5 H : A 23 0.6 88.7 H -- A 66 1.8 A -- H 35 0.9 91.4 A - H 31 0.8 H - A 21 0.6 92.8 Table 2: Apposition forms in TRAIN with abstract (top) and actual (bottom) tokens, e.g., H t A indicates an HEAD, one token then an ATTR. 3 Data We use apposition-annotated documents from the English section of OntoNotes 4 (Weischedel et al., 2011). We manually adjust appositions that do not have exactly one HEAD and one or more ATTR1. Some appositions are nested, and we keep only “leaf” appositions, removing the higher-level appositions. We follow the CoNLL-2011 scheme to select TRAIN, DEV and TEST datasets (Pradhan et al., 2011). OntoNotes 4 is made up of a wide variety of sources: broadcast conversation and news, magazine, newswire and web text. Appositions are most frequent in newswire (one per 192 words) and least common in broadcast conversation (one per 645 words) with the others in between (around one per 315 words). We also rep</context>
</contexts>
<marker>Weischedel, Palmer, Marcus, Hovy, Pradhan, Ramshaw, 2011</marker>
<rawString>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed ElBachouti, Robert Belvin, and Ann Houston. 2011. OntoNotes Release 4.0. Technical report, Linguistic Data Consortium, Philadelphia, PA USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>A more precise analysis of punctuation for broad-coverage surface realization with CCG.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks,</booktitle>
<pages>17--24</pages>
<location>Manchester, England.</location>
<contexts>
<context position="4224" citStr="White and Rajkumar, 2008" startWordPosition="618" endWordPosition="621">EV TEST Sents. 9,595 976 1,098 48,762 6,894 6,896 Appos. 590 64 68 3,877 502 490 Table 1: Sentence and apposition distribution Apposition extraction is a common component in many NLP tasks: coreference resolution (Luo and Zitouni, 2005; Culotta et al., 2007; Bengtson and Roth, 2008; Poon and Domingos, 2008), textual entailment (Roth and Sammons, 2007; Cabrio and Magnini, 2010), sentence simplification (Miwa et al., 2010; Candido et al., 2009; Siddharthan, 2002) and summarization (Nenkova et al., 2005). Comma ambiguity has been studied in the RTE (Srikumar et al., 2008) and generation domains (White and Rajkumar, 2008). Despite this, few papers to our knowledge explicitly evaluate apposition extraction. Moreover, apposition extraction is rarely the main research goal and descriptions of the methods used are often accordingly terse or do not match our guidelines. Lee et al. (2011) use rules to extract appositions for coreference resolution, selecting only those that are explicitly flagged using commas or parentheses. They do not separately mark HEAD and ATTR and permit relative clauses as an ATTR. While such differences capture useful information for coreference resolution, these methods would be unfairly di</context>
</contexts>
<marker>White, Rajkumar, 2008</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2008. A more precise analysis of punctuation for broad-coverage surface realization with CCG. In Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 17–24. Coling 2008 Organizing Committee, Manchester, England.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>