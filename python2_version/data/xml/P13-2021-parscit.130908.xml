<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000381">
<title confidence="0.917878">
Arguments and Modifiers from the Learner’s Perspective
</title>
<author confidence="0.804779">
Leon Bergen
</author>
<affiliation confidence="0.357763">
MIT
Brain and Cognitive Science
</affiliation>
<email confidence="0.921966">
bergen@mit.edu
</email>
<note confidence="0.77582">
Edward Gibson
MIT
Brain and Cognitive Science
</note>
<email confidence="0.916947">
egibson@mit.edu
</email>
<note confidence="0.941510333333333">
Timothy J. O’Donnell
MIT
Brain and Cognitive Science
</note>
<email confidence="0.968799">
timod@mit.edu
</email>
<sectionHeader confidence="0.993218" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999749">
We present a model for inducing sen-
tential argument structure, which distin-
guishes arguments from optional modi-
fiers. We use this model to study whether
representing an argument/modifier distinc-
tion helps in learning argument structure,
and whether a linguistically-natural argu-
ment/modifier distinction can be induced
from distributional data alone. Our results
provide evidence for both hypotheses.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995385740740741">
A fundamental challenge facing the language
learner is to determine the content and structure
of the stored units in the lexicon. This problem is
made more difficult by the fact that many lexical
units have argument structure. Consider the verb
put. The sentence, John put the socks is incom-
plete; when hearing such an utterance, a speaker
of English will expect a location to also be speci-
fied: John put the socks in the drawer. Facts such
as these can be captured if the lexical entry for put
also specifies that the verb has three required ar-
guments: (i) who is doing the putting (ii) what is
being put (iii) and the destination of the putting.
The problem of acquiring argument structure is
further complicated by the fact that not all phrases
in a sentence fill an argument role. Instead, many
are modi�ers. Consider the sentence John put the
socks in the drawer at 5 o’clock. The phrase at
5 o’clock occurs here with the verb put, but it is
not an argument. Removing this phrase does not
change the core structure of the PUTTING event,
nor is the sentence incomplete without this phrase.
The distinction between arguments and mod-
ifiers has a long history in traditional grammar
and is leveraged in many modern theories of syn-
tax (Haegeman, 1994; Steedman, 2001; Sag et
al., 2003). Despite the ubiquity of the distinc-
</bodyText>
<figureCaption confidence="0.944778">
Figure 1: The VP’s in these sentences only share
structure if we separate arguments from modifiers.
</figureCaption>
<bodyText confidence="0.999970321428571">
tion in syntax, however, there is a lack of consen-
sus on the necessary and sufficient conditions for
argumenthood (Sch¨utze, 1995; Sch¨utze and Gib-
son, 1999). It remains unclear whether the argu-
ment/modifier distinction is purely semantic or is
also represented in syntax, whether it is binary or
graded, and what effects argument/modifierhood
have on the distribution of linguistic forms.
In this work, we take a new approach to these
problems. We propose that the argument/modifier
distinction is inferred on a phrase–by–phrase basis
using probabilistic inference. Crucially, allowing
the learner to separate the core argument structure
of phrases from peripheral modifier content in-
creases the generalizability of argument construc-
tions. For example, the two sentences in Figure 1
intuitively share the same argument structures, but
this overlap can only be identified if the preposi-
tional phrase, “at 5 o’clock,” is treated as a modi-
fier. Thus representing the argument/modifier dis-
tinction can help the learner find useful argument
structures which generalize robustly.
Although, like the majority of theorists, we
agree that the argument/adjunct distinction is fun-
damentally semantic, in this work we focus on its
distributional correlates. Does the optionality of
modifier phrases help the learner acquire lexical
items with the right argument structure?
</bodyText>
<sectionHeader confidence="0.987487" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999749">
We adopt an approach where the lexicon consists
of an inventory of stored tree fragments. These
</bodyText>
<figure confidence="0.990671681818182">
put
the socks
put
the socks
S
NP
VP
John
John
NP
VP
3
V
NP
PP
V
NP
PP
PP
in the drawer
in the drawer
at 5 o’clock
</figure>
<page confidence="0.97514">
115
</page>
<bodyText confidence="0.95849825">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115–119,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
tree fragments encode the necessary phrase types
(i.e., arguments) that must be present in a struc-
ture before it is complete. In this system, sen-
tences are generated by recursive substitution of
tree fragments at the frontier argument nodes of
other tree fragments. This approach extends work
on learning probabilistic Tree–Substitution Gram-
mars (TSGs) (Post and Gildea, 2009; Cohn et al.,
2010; O’Donnell, 2011; O’Donnell et al., 2011).1
To model modification, we introduce a second
structure–building operation, adjunction. While
substitution must be licensed by the existence
of an argument node, adjunction can insert con-
stituents into well–formed trees. Many syntactic
theories have made use of an adjunction operation
to model modification. Here, we adopt the variant
known as sister–adjunction (Rambow et al., 1995;
Chiang and Bikel, 2002) which can insert a con-
stituent as the sister to any node in an existing tree.
In order to derive the complete tree for a sen-
tence, starting from an S root node, we recursively
sample arguments and modifiers as follows.2 For
every nonterminal node on the frontier of our
derivation, we sample an elementary tree from our
lexicon to substitute into this node. As already
noted, these elementary trees represent the argu-
ment structure of our tree. Then, for each argu-
ment nonterminal on the tree’s interior, we sister-
adjoin one or more modifier nodes, which them-
selves are built by the same recursive process.
Figure 2 illustrates two derivations of the
same tree, one in standard TSG without sister-
adjunction, and one in our model. In the TSG
derivation, at top, an elementary tree with four ar-
guments – including the intuitively optional tem-
poral PP – is used as the backbone for the deriva-
tion. The four phrases filling these arguments
are then substituted into the elementary tree, as
indicated by arrows. In the bottom derivation,
which uses sister–adjunction, an elementary tree
with only three arguments is used as the back-
bone. While the right-most temporal PP needed
to be an argument of the elementary tree in the
TSG derivation, the bottom derivation uses sister-
adjunction to insert this PP as a child of the VP.
Sister–adjunction therefore allows us to use an ar-
1Note that we depart from many discussions of argument
structure in that we do not require that every stored fragment
has a head word. In effect, we allow completely abstract
phrasal constructions to also have argument structures.
2Our generative model is related to the generative model
for Tree–Adjoining Grammars proposed in (Chiang, 2000)
the socks in the drawer at 5 o’clock
the socks in the drawer at 5 o’clock
</bodyText>
<figureCaption confidence="0.6586275">
Figure 2: The first part of the figure shows how
to derive the tree in TSG, while the second part
shows how to use sister-adjunction to derive the
same tree in our model.
</figureCaption>
<bodyText confidence="0.999938789473685">
gument structure that matches the true argument
structure of the verb “put.”
This figure illustrates how derivations in our
model can have a greater degree of generalizabil-
ity than those in a standard TSG. Sister–adjunction
will be used to derive children which are not part
of the core argument structure, meaning that a
greater variety of structures can be derived by a
combination of common argument structures and
sister-adjoined modifiers. Importantly, this makes
the learning problem for our model less sparse
than for TSGs; our model can derive the trees in a
corpus using fewer types of elementary trees than
a TSG. As a result, the distribution over these ele-
mentary trees is easier to estimate.
To understand what role modifiers play during
learning, we will develop a learning model that
can induce the lexicon and modifier contexts used
by our generative model.
</bodyText>
<sectionHeader confidence="0.991731" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999885111111111">
Our model extends earlier work on induction
of Bayesian TSGs (Post and Gildea, 2009;
O’Donnell, 2011; Cohn et al., 2010). The model
uses a Bayesian non–parametric distribution—the
Pitman-Yor Process, to place a prior over the lex-
icon of elementary trees. This distribution allows
the complexity of the lexicon to grow to arbitrary
size with the input, while still enforcing a bias for
more compact lexicons.
</bodyText>
<figure confidence="0.998396862068965">
Joh
��
NP
VP
S
John
V
put
NP
� ��
��
PP
� ��
��
PP
��
���
S
Joh
��
John
PP
NP
V
NP
VP
put � �� � ��
�� �� ��
���
</figure>
<page confidence="0.977272">
116
</page>
<equation confidence="0.890629333333333">
For each nonterminal c, we define:
Gc|ac,bc,PE ∼ PYP(ac,bc,PE(·|c)) (1)
e|c, Gc ∼ Gc, (2)
</equation>
<bodyText confidence="0.997300375">
where PE(·|c) is a context free distribution over
elementary trees rooted at c, and e is an elementary
tree.
The context-free distribution over elementary
trees PE(e|c) is defined by:
The distribution G at the root of the hierarchy is
not conditioned on any prior context. We define G
by:
</bodyText>
<equation confidence="0.814838">
G ∼ DP(α, Multinomial(m)) (6)
</equation>
<bodyText confidence="0.9970818">
where m is a vector with entries for each nonter-
minal, and where we sample m ∼ Dir(1,...,1).
To perform inference, we developed a local
Gibbs sampler which generalizes the one proposed
by (Cohn et al., 2010).
</bodyText>
<equation confidence="0.84336275">
H H Hscf Pc�(α|c�), 4 Results
PE(e|c) = (1−sci) cl-+αEe
iEI(e) fEF(e)
(3)
</equation>
<bodyText confidence="0.999942173913044">
where I(e) is the set of internal nodes in e, F(e) is
the set of frontier nodes, ci is the nonterminal cat-
egory associated with node i, and sc is the proba-
bility that we stop expanding at a node c. For this
paper, the parameters sc are set to 0.5.
In addition to defining a distribution over ele-
mentary trees, we also define a distribution which
governs modification via sister–adjunction. To
sample a modifier, we first decide whether or not
to sister–adjoin into location l in a tree. Following
this step, we sample a modifier category (e.g., a
PP) conditioned on the location l’s context: its par-
ent and left siblings. Because contexts are sparse,
we use a backoff scheme based on hierarchical
Dirichlet processes similar to the ngram backoff
schemes defined in (Teh, 2006; Goldwater et al.,
2006). Let c be a nonterminal node in a tree de-
rived by substitution into argument positions. The
node c will have n ≥ 1 children derived by ar-
gument substitution: d0,..., dn. In order to sister–
adjoin between two of these children di, di+1, we
recursively sample nonterminals si,1, ..., si,k until
we hit a STOP symbol:
</bodyText>
<equation confidence="0.9977766">
Pa(si,1, ..., si,k, STOP|C0) (4)
k
H Pa(si,j|Cj) · (1 − PCj(STOP))
j=1
· PCk+1(STOP)
</equation>
<bodyText confidence="0.999058875">
where Cj = d1, s1,1, ..., di, si,1, ..., si,j−1, c is the
context for the j’th modifier between these chil-
dren. The distribution over sister–adjoined non-
terminals is defined using a hierarchical Dirichlet
process to implement backoff in a prefix tree over
contexts. We define the distribution G(ql, ..., q1)
over sister–adjoined nonterminals si,j given the
context ql, ..., q1 by:
</bodyText>
<equation confidence="0.9571">
G(ql, ..., q1) ∼ DP(α, G(ql−1, ..., q1)). (5)
</equation>
<bodyText confidence="0.999962461538462">
We evaluate our model in two ways. First,
we examine whether representing the argu-
ment/modifier distinction increases the ability of
the model to learn highly generalizable elemen-
tary trees that can be used as argument structures
across a variety of sentences. Second, we ask
whether our model is able to induce the correct
argument/modifier distinction according to a lin-
guistic gold–standard. We trained our model on
sections 2–21 of the WSJ part of the Penn Tree-
bank (Marcus et al., 1999). The model was trained
on the trees in this corpus, without any further an-
notations for substitution or modification.
To address the first question, we compared the
structure of the grammar learned by our model to
a grammar learned by a version of our model with-
out sister–adjunction (i.e., a TSG similar to the
one used in Cohn et al.). Our model should find
more common structure among the trees in the in-
put corpus, and therefore it should learn a set of el-
ementary trees which are more complex and more
widely shared across sentences. We evaluated this
hypothesis by analyzing the average complexity
of the most probable elementary trees learned by
these models. As Table 1 shows, our model dis-
covers elementary trees that have greater depth
and more nodes than those found by the TSG. In
addition, our model accounts for a larger portion
of the corpus with fewer rules: the top 50, 100, and
200 most common elementary trees in our model’s
lexicon account for a greater portion of the corpus
than the corresponding sets in the TSG.
Figure 3 illustrates a representative example
from the corpus. By using sister-adjuntion to sepa-
rate the ADVP node from the rest of the sentence’s
derivation, our model was able to use a common
depth-3 elementary tree to derive the backbone of
the sentence. In contrast, the TSG cannot give the
same derivation, as it needs to include the ADVP
</bodyText>
<page confidence="0.987389">
117
</page>
<figure confidence="0.666248">
switched
</figure>
<figureCaption confidence="0.994103">
Figure 3: Part of a derivation found by our model.
</figureCaption>
<table confidence="0.99975675">
Model Rank Avg tree Avg tree #Tokens
depth size
Modifier 50 1.59 3.42 97282
TSG 50 1.38 2.98 88023
Modifier 100 1.84 3.98 134205
TSG 100 1.58 3.38 116404
Modifier 200 1.97 4.27 170524
TSG 200 1.77 3.84 146040
</table>
<tableCaption confidence="0.998851">
Table 1: This table shows the average depth and
</tableCaption>
<bodyText confidence="0.991232696969697">
node count for elementary trees in our model and
the TSG. The results are shown for the 50, 100,
and 200 most frequent types of elementary trees.
node in the elementary tree; this wider elementary
tree is much less common in the corpus.
We next examined whether our model learned
to correctly identify modifiers in the corpus. Un-
fortunately, marking for argument/modifiers in the
Penn Treebank is incomplete, and is limited to
certain adverbials, e.g. locative and temporal
PP’s. To supplement this markup, we made use of
the corpus of (Kaeshammer and Demberg, 2012).
This corpus adds annotations indicating, for each
node in the Penn Treebank, whether that node is
a modifier. This corpus was compiled by com-
bining information from Propbank (Palmer et al.,
2005) with a set of heuristics, as well as the NP-
branching structures proposed in (Vadas and Cur-
ran, 2007). It is important to note that this corpus
can only serve as a rough benchmark for evalua-
tion of our model, as the heuristics used in its de-
velopment did not always follow the correct lin-
guistic analysis; the corpus was originally con-
structed for an alternative application in compu-
tational linguistics, for which non–linguistically-
natural analyses were sometimes convenient. Our
model was trained on this corpus, after it had been
stripped of argument/modifier annotations.
We compare our model’s performance to a ran-
dom baseline. Our model constrains every non-
terminal to have at least one argument child, and
our Gibbs sampler initializes argument/modifier
choices randomly subject to this constraint. We
</bodyText>
<table confidence="0.994589333333333">
Model Precision Recall #Guessed #Correct
Random 0.27 0.19 298394 82702
Modifier 0.62 0.15 108382 67516
</table>
<tableCaption confidence="0.8677175">
Table 2: This table shows precision and recall in
identifying modifier nodes in the corpus.
</tableCaption>
<bodyText confidence="0.9998956875">
therefore calculated the probability that a node
that was randomly initialized as a modifier was in
fact a modifier, i.e. the precision of random ini-
tialization. Next, we looked at the precision of
our model following training. Table 2 shows that
among nodes that were labeled as modifiers, 0.27
were labeled correctly before training and 0.62
were labeled correctly after. This table also shows
the recall performance for our model decreased by
0.04. Some of this decrease is due to limitations of
the gold standard; for example, our model learns
to classify infinitives and auxiliary verbs as argu-
ments — consistent with standard linguistic anal-
yses — whereas the gold standard classifies these
as modifiers. Future work will investigate how the
metric used for evaluation can be improved.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="method">
5 Summary
</sectionHeader>
<bodyText confidence="0.999994772727273">
We have investigated the role of the argu-
ment/modifier distinction in learning. We first
looked at whether introducing this distinction
helps in generalizing from an input corpus.
Our model, which represents modification using
sister–adjunction, learns a richer lexicon than a
model without modification, and its lexicon pro-
vides a more compact representation of the in-
put corpus. We next looked at whether the tra-
ditional linguistic classification of arguments and
modifiers can be induced from distributional in-
formation. Without supervision from the correct
labelings of modifiers, our model learned to iden-
tify modifiers more accurately than chance. This
suggests that although the argument/modifier dis-
tinction is traditionally drawn without reference to
distributional properties, the distributional corre-
lates of this distinction are sufficient to partially
reconstruct it from a corpus. Taken together, these
results suggest that representing the difference be-
tween arguments and modifiers may make it easier
to acquire a language’s argument structure.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9959905">
We thank Vera Demberg for providing the gold
standard, and Tom Wasow for helpful comments.
</bodyText>
<figure confidence="0.982577357142857">
8
NP
VP
ADVP
who
��
simply
VP
VBD
PP
Most of those who left stock funds
PP��
������ into money market funds
���
</figure>
<page confidence="0.96599">
118
</page>
<note confidence="0.8208985">
Carson T Sch¨utze and Edward Gibson. 1999. Ar-
gumenthood and english prepositional phrase at-
tachment. Journal of Memory and Language,
40(3):409–431.
References
David Chiang and Daniel Bikel. 2002. Recovering
latent information in treebanks. In Proceedings of
COLING 2002.
</note>
<reference confidence="0.999502129032258">
David Chiang. 2000. Staistical parsing with an
automatically–extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree–substitution grammars. Jour-
nal of Machine Learning Research, 11:3053–3096.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Interpolating between types and to-
kens by estimating power–law generators. In Ad-
vances in Neural Information Processing Systems
18, Cambridge, Ma. MIT Press.
Liliane Haegeman. 1994. Government &amp; Binding The-
ory. Blackwell.
Mirian Kaeshammer and Vera Demberg. 2012. Ger-
man and English treebanks and lexica for tree–
adjoining grammars. In Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2012).
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank–
3. Technical report, Linguistic Data Consortium,
Philadelphia.
Timothy J. O’Donnell, Jesse Snedeker, Joshua B.
Tenenbaum, and Noah D. Goodman. 2011. Pro-
ductivity and reuse in language. In Proceedings of
the 33rd Annual Conference of the Cognitive Science
Society.
Timothy J. O’Donnell. 2011. Productivity and Reuse
in Language. Ph.D. thesis, Harvard University.
Martha Palmer, P. Kingsbury, and Daniel Gildea. 2005.
The proposition bank. Computational Linguistics,
31(1):71–106.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D–tree grammars. In Proceedings of the
33rd annual meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
CSLI, Stanford, CA, 2 edition.
Carson T. Sch¨utze. 1995. PP attachment and argu-
menthood. Technical report, Papers on language
processing and acquisition, MIT working papers in
linguistics, Cambridge, Ma.
Mark Steedman. 2001. The syntactic process. The
MIT press.
Yee Whye Teh. 2006. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical Report TRA2/06,
National University of Singapore, School of Com-
puting.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th annual meeting of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
</reference>
<page confidence="0.999092">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.044145">
<title confidence="0.686666333333333">Arguments and Modifiers from the Learner’s Perspective Leon Brain and Cognitive</title>
<email confidence="0.987027">bergen@mit.edu</email>
<note confidence="0.3707175">Edward Brain and Cognitive</note>
<email confidence="0.997292">egibson@mit.edu</email>
<author confidence="0.997876">J Timothy</author>
<affiliation confidence="0.366845">Brain and Cognitive</affiliation>
<email confidence="0.996818">timod@mit.edu</email>
<abstract confidence="0.999162727272727">We present a model for inducing sentential argument structure, which distinguishes arguments from optional modifiers. We use this model to study whether representing an argument/modifier distinction helps in learning argument structure, and whether a linguistically-natural argument/modifier distinction can be induced from distributional data alone. Our results provide evidence for both hypotheses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Staistical parsing with an automatically–extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6424" citStr="Chiang, 2000" startWordPosition="1031" endWordPosition="1032">guments is used as the backbone. While the right-most temporal PP needed to be an argument of the elementary tree in the TSG derivation, the bottom derivation uses sisteradjunction to insert this PP as a child of the VP. Sister–adjunction therefore allows us to use an ar1Note that we depart from many discussions of argument structure in that we do not require that every stored fragment has a head word. In effect, we allow completely abstract phrasal constructions to also have argument structures. 2Our generative model is related to the generative model for Tree–Adjoining Grammars proposed in (Chiang, 2000) the socks in the drawer at 5 o’clock the socks in the drawer at 5 o’clock Figure 2: The first part of the figure shows how to derive the tree in TSG, while the second part shows how to use sister-adjunction to derive the same tree in our model. gument structure that matches the true argument structure of the verb “put.” This figure illustrates how derivations in our model can have a greater degree of generalizability than those in a standard TSG. Sister–adjunction will be used to derive children which are not part of the core argument structure, meaning that a greater variety of structures ca</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Staistical parsing with an automatically–extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree–substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--3053</pages>
<contexts>
<context position="4258" citStr="Cohn et al., 2010" startWordPosition="674" endWordPosition="677">awer in the drawer at 5 o’clock 115 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115–119, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tree fragments encode the necessary phrase types (i.e., arguments) that must be present in a structure before it is complete. In this system, sentences are generated by recursive substitution of tree fragments at the frontier argument nodes of other tree fragments. This approach extends work on learning probabilistic Tree–Substitution Grammars (TSGs) (Post and Gildea, 2009; Cohn et al., 2010; O’Donnell, 2011; O’Donnell et al., 2011).1 To model modification, we introduce a second structure–building operation, adjunction. While substitution must be licensed by the existence of an argument node, adjunction can insert constituents into well–formed trees. Many syntactic theories have made use of an adjunction operation to model modification. Here, we adopt the variant known as sister–adjunction (Rambow et al., 1995; Chiang and Bikel, 2002) which can insert a constituent as the sister to any node in an existing tree. In order to derive the complete tree for a sentence, starting from an</context>
<context position="7672" citStr="Cohn et al., 2010" startWordPosition="1243" endWordPosition="1246">of common argument structures and sister-adjoined modifiers. Importantly, this makes the learning problem for our model less sparse than for TSGs; our model can derive the trees in a corpus using fewer types of elementary trees than a TSG. As a result, the distribution over these elementary trees is easier to estimate. To understand what role modifiers play during learning, we will develop a learning model that can induce the lexicon and modifier contexts used by our generative model. 3 Model Our model extends earlier work on induction of Bayesian TSGs (Post and Gildea, 2009; O’Donnell, 2011; Cohn et al., 2010). The model uses a Bayesian non–parametric distribution—the Pitman-Yor Process, to place a prior over the lexicon of elementary trees. This distribution allows the complexity of the lexicon to grow to arbitrary size with the input, while still enforcing a bias for more compact lexicons. Joh �� NP VP S John V put NP � �� �� PP � �� �� PP �� ��� S Joh �� John PP NP V NP VP put � �� � �� �� �� �� ��� 116 For each nonterminal c, we define: Gc|ac,bc,PE ∼ PYP(ac,bc,PE(·|c)) (1) e|c, Gc ∼ Gc, (2) where PE(·|c) is a context free distribution over elementary trees rooted at c, and e is an elementary tr</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree–substitution grammars. Journal of Machine Learning Research, 11:3053–3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power–law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<publisher>MIT Press.</publisher>
<location>Cambridge, Ma.</location>
<contexts>
<context position="9569" citStr="Goldwater et al., 2006" startWordPosition="1585" endWordPosition="1588">panding at a node c. For this paper, the parameters sc are set to 0.5. In addition to defining a distribution over elementary trees, we also define a distribution which governs modification via sister–adjunction. To sample a modifier, we first decide whether or not to sister–adjoin into location l in a tree. Following this step, we sample a modifier category (e.g., a PP) conditioned on the location l’s context: its parent and left siblings. Because contexts are sparse, we use a backoff scheme based on hierarchical Dirichlet processes similar to the ngram backoff schemes defined in (Teh, 2006; Goldwater et al., 2006). Let c be a nonterminal node in a tree derived by substitution into argument positions. The node c will have n ≥ 1 children derived by argument substitution: d0,..., dn. In order to sister– adjoin between two of these children di, di+1, we recursively sample nonterminals si,1, ..., si,k until we hit a STOP symbol: Pa(si,1, ..., si,k, STOP|C0) (4) k H Pa(si,j|Cj) · (1 − PCj(STOP)) j=1 · PCk+1(STOP) where Cj = d1, s1,1, ..., di, si,1, ..., si,j−1, c is the context for the j’th modifier between these children. The distribution over sister–adjoined nonterminals is defined using a hierarchical Dir</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power–law generators. In Advances in Neural Information Processing Systems 18, Cambridge, Ma. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liliane Haegeman</author>
</authors>
<title>Government &amp; Binding Theory.</title>
<date>1994</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1918" citStr="Haegeman, 1994" startWordPosition="309" endWordPosition="310">he putting. The problem of acquiring argument structure is further complicated by the fact that not all phrases in a sentence fill an argument role. Instead, many are modi�ers. Consider the sentence John put the socks in the drawer at 5 o’clock. The phrase at 5 o’clock occurs here with the verb put, but it is not an argument. Removing this phrase does not change the core structure of the PUTTING event, nor is the sentence incomplete without this phrase. The distinction between arguments and modifiers has a long history in traditional grammar and is leveraged in many modern theories of syntax (Haegeman, 1994; Steedman, 2001; Sag et al., 2003). Despite the ubiquity of the distincFigure 1: The VP’s in these sentences only share structure if we separate arguments from modifiers. tion in syntax, however, there is a lack of consensus on the necessary and sufficient conditions for argumenthood (Sch¨utze, 1995; Sch¨utze and Gibson, 1999). It remains unclear whether the argument/modifier distinction is purely semantic or is also represented in syntax, whether it is binary or graded, and what effects argument/modifierhood have on the distribution of linguistic forms. In this work, we take a new approach t</context>
</contexts>
<marker>Haegeman, 1994</marker>
<rawString>Liliane Haegeman. 1994. Government &amp; Binding Theory. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirian Kaeshammer</author>
<author>Vera Demberg</author>
</authors>
<title>German and English treebanks and lexica for tree– adjoining grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="13157" citStr="Kaeshammer and Demberg, 2012" startWordPosition="2204" endWordPosition="2207">77 3.84 146040 Table 1: This table shows the average depth and node count for elementary trees in our model and the TSG. The results are shown for the 50, 100, and 200 most frequent types of elementary trees. node in the elementary tree; this wider elementary tree is much less common in the corpus. We next examined whether our model learned to correctly identify modifiers in the corpus. Unfortunately, marking for argument/modifiers in the Penn Treebank is incomplete, and is limited to certain adverbials, e.g. locative and temporal PP’s. To supplement this markup, we made use of the corpus of (Kaeshammer and Demberg, 2012). This corpus adds annotations indicating, for each node in the Penn Treebank, whether that node is a modifier. This corpus was compiled by combining information from Propbank (Palmer et al., 2005) with a set of heuristics, as well as the NPbranching structures proposed in (Vadas and Curran, 2007). It is important to note that this corpus can only serve as a rough benchmark for evaluation of our model, as the heuristics used in its development did not always follow the correct linguistic analysis; the corpus was originally constructed for an alternative application in computational linguistics</context>
</contexts>
<marker>Kaeshammer, Demberg, 2012</marker>
<rawString>Mirian Kaeshammer and Vera Demberg. 2012. German and English treebanks and lexica for tree– adjoining grammars. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank– 3. Technical report, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="10889" citStr="Marcus et al., 1999" startWordPosition="1809" endWordPosition="1812"> ..., q1) over sister–adjoined nonterminals si,j given the context ql, ..., q1 by: G(ql, ..., q1) ∼ DP(α, G(ql−1, ..., q1)). (5) We evaluate our model in two ways. First, we examine whether representing the argument/modifier distinction increases the ability of the model to learn highly generalizable elementary trees that can be used as argument structures across a variety of sentences. Second, we ask whether our model is able to induce the correct argument/modifier distinction according to a linguistic gold–standard. We trained our model on sections 2–21 of the WSJ part of the Penn Treebank (Marcus et al., 1999). The model was trained on the trees in this corpus, without any further annotations for substitution or modification. To address the first question, we compared the structure of the grammar learned by our model to a grammar learned by a version of our model without sister–adjunction (i.e., a TSG similar to the one used in Cohn et al.). Our model should find more common structure among the trees in the input corpus, and therefore it should learn a set of elementary trees which are more complex and more widely shared across sentences. We evaluated this hypothesis by analyzing the average comple</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank– 3. Technical report, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J O’Donnell</author>
<author>Jesse Snedeker</author>
<author>Joshua B Tenenbaum</author>
<author>Noah D Goodman</author>
</authors>
<title>Productivity and reuse in language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Conference of the Cognitive Science Society.</booktitle>
<marker>O’Donnell, Snedeker, Tenenbaum, Goodman, 2011</marker>
<rawString>Timothy J. O’Donnell, Jesse Snedeker, Joshua B. Tenenbaum, and Noah D. Goodman. 2011. Productivity and reuse in language. In Proceedings of the 33rd Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy J O’Donnell</author>
</authors>
<date>2011</date>
<booktitle>Productivity and Reuse in Language. Ph.D. thesis,</booktitle>
<institution>Harvard University.</institution>
<marker>O’Donnell, 2011</marker>
<rawString>Timothy J. O’Donnell. 2011. Productivity and Reuse in Language. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>P Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The proposition bank.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="13354" citStr="Palmer et al., 2005" startWordPosition="2236" endWordPosition="2239">s. node in the elementary tree; this wider elementary tree is much less common in the corpus. We next examined whether our model learned to correctly identify modifiers in the corpus. Unfortunately, marking for argument/modifiers in the Penn Treebank is incomplete, and is limited to certain adverbials, e.g. locative and temporal PP’s. To supplement this markup, we made use of the corpus of (Kaeshammer and Demberg, 2012). This corpus adds annotations indicating, for each node in the Penn Treebank, whether that node is a modifier. This corpus was compiled by combining information from Propbank (Palmer et al., 2005) with a set of heuristics, as well as the NPbranching structures proposed in (Vadas and Curran, 2007). It is important to note that this corpus can only serve as a rough benchmark for evaluation of our model, as the heuristics used in its development did not always follow the correct linguistic analysis; the corpus was originally constructed for an alternative application in computational linguistics, for which non–linguisticallynatural analyses were sometimes convenient. Our model was trained on this corpus, after it had been stripped of argument/modifier annotations. We compare our model’s p</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, P. Kingsbury, and Daniel Gildea. 2005. The proposition bank. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="4239" citStr="Post and Gildea, 2009" startWordPosition="670" endWordPosition="673">PP V NP PP PP in the drawer in the drawer at 5 o’clock 115 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 115–119, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tree fragments encode the necessary phrase types (i.e., arguments) that must be present in a structure before it is complete. In this system, sentences are generated by recursive substitution of tree fragments at the frontier argument nodes of other tree fragments. This approach extends work on learning probabilistic Tree–Substitution Grammars (TSGs) (Post and Gildea, 2009; Cohn et al., 2010; O’Donnell, 2011; O’Donnell et al., 2011).1 To model modification, we introduce a second structure–building operation, adjunction. While substitution must be licensed by the existence of an argument node, adjunction can insert constituents into well–formed trees. Many syntactic theories have made use of an adjunction operation to model modification. Here, we adopt the variant known as sister–adjunction (Rambow et al., 1995; Chiang and Bikel, 2002) which can insert a constituent as the sister to any node in an existing tree. In order to derive the complete tree for a sentenc</context>
<context position="7635" citStr="Post and Gildea, 2009" startWordPosition="1237" endWordPosition="1240">uctures can be derived by a combination of common argument structures and sister-adjoined modifiers. Importantly, this makes the learning problem for our model less sparse than for TSGs; our model can derive the trees in a corpus using fewer types of elementary trees than a TSG. As a result, the distribution over these elementary trees is easier to estimate. To understand what role modifiers play during learning, we will develop a learning model that can induce the lexicon and modifier contexts used by our generative model. 3 Model Our model extends earlier work on induction of Bayesian TSGs (Post and Gildea, 2009; O’Donnell, 2011; Cohn et al., 2010). The model uses a Bayesian non–parametric distribution—the Pitman-Yor Process, to place a prior over the lexicon of elementary trees. This distribution allows the complexity of the lexicon to grow to arbitrary size with the input, while still enforcing a bias for more compact lexicons. Joh �� NP VP S John V put NP � �� �� PP � �� �� PP �� ��� S Joh �� John PP NP V NP VP put � �� � �� �� �� �� ��� 116 For each nonterminal c, we define: Gc|ac,bc,PE ∼ PYP(ac,bc,PE(·|c)) (1) e|c, Gc ∼ Gc, (2) where PE(·|c) is a context free distribution over elementary trees r</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D–tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4685" citStr="Rambow et al., 1995" startWordPosition="735" endWordPosition="738">agments at the frontier argument nodes of other tree fragments. This approach extends work on learning probabilistic Tree–Substitution Grammars (TSGs) (Post and Gildea, 2009; Cohn et al., 2010; O’Donnell, 2011; O’Donnell et al., 2011).1 To model modification, we introduce a second structure–building operation, adjunction. While substitution must be licensed by the existence of an argument node, adjunction can insert constituents into well–formed trees. Many syntactic theories have made use of an adjunction operation to model modification. Here, we adopt the variant known as sister–adjunction (Rambow et al., 1995; Chiang and Bikel, 2002) which can insert a constituent as the sister to any node in an existing tree. In order to derive the complete tree for a sentence, starting from an S root node, we recursively sample arguments and modifiers as follows.2 For every nonterminal node on the frontier of our derivation, we sample an elementary tree from our lexicon to substitute into this node. As already noted, these elementary trees represent the argument structure of our tree. Then, for each argument nonterminal on the tree’s interior, we sisteradjoin one or more modifier nodes, which themselves are buil</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>Owen Rambow, K. Vijay-Shanker, and David Weir. 1995. D–tree grammars. In Proceedings of the 33rd annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
<author>Emily M Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction.</title>
<date>2003</date>
<volume>2</volume>
<pages>edition.</pages>
<location>CSLI, Stanford, CA,</location>
<contexts>
<context position="1953" citStr="Sag et al., 2003" startWordPosition="313" endWordPosition="316">ring argument structure is further complicated by the fact that not all phrases in a sentence fill an argument role. Instead, many are modi�ers. Consider the sentence John put the socks in the drawer at 5 o’clock. The phrase at 5 o’clock occurs here with the verb put, but it is not an argument. Removing this phrase does not change the core structure of the PUTTING event, nor is the sentence incomplete without this phrase. The distinction between arguments and modifiers has a long history in traditional grammar and is leveraged in many modern theories of syntax (Haegeman, 1994; Steedman, 2001; Sag et al., 2003). Despite the ubiquity of the distincFigure 1: The VP’s in these sentences only share structure if we separate arguments from modifiers. tion in syntax, however, there is a lack of consensus on the necessary and sufficient conditions for argumenthood (Sch¨utze, 1995; Sch¨utze and Gibson, 1999). It remains unclear whether the argument/modifier distinction is purely semantic or is also represented in syntax, whether it is binary or graded, and what effects argument/modifierhood have on the distribution of linguistic forms. In this work, we take a new approach to these problems. We propose that t</context>
</contexts>
<marker>Sag, Wasow, Bender, 2003</marker>
<rawString>Ivan A. Sag, Thomas Wasow, and Emily M. Bender. 2003. Syntactic Theory: A Formal Introduction. CSLI, Stanford, CA, 2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carson T Sch¨utze</author>
</authors>
<title>PP attachment and argumenthood. Technical report, Papers on language processing and acquisition, MIT working papers in linguistics,</title>
<date>1995</date>
<location>Cambridge, Ma.</location>
<marker>Sch¨utze, 1995</marker>
<rawString>Carson T. Sch¨utze. 1995. PP attachment and argumenthood. Technical report, Papers on language processing and acquisition, MIT working papers in linguistics, Cambridge, Ma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2001</date>
<publisher>The MIT press.</publisher>
<contexts>
<context position="1934" citStr="Steedman, 2001" startWordPosition="311" endWordPosition="312">problem of acquiring argument structure is further complicated by the fact that not all phrases in a sentence fill an argument role. Instead, many are modi�ers. Consider the sentence John put the socks in the drawer at 5 o’clock. The phrase at 5 o’clock occurs here with the verb put, but it is not an argument. Removing this phrase does not change the core structure of the PUTTING event, nor is the sentence incomplete without this phrase. The distinction between arguments and modifiers has a long history in traditional grammar and is leveraged in many modern theories of syntax (Haegeman, 1994; Steedman, 2001; Sag et al., 2003). Despite the ubiquity of the distincFigure 1: The VP’s in these sentences only share structure if we separate arguments from modifiers. tion in syntax, however, there is a lack of consensus on the necessary and sufficient conditions for argumenthood (Sch¨utze, 1995; Sch¨utze and Gibson, 1999). It remains unclear whether the argument/modifier distinction is purely semantic or is also represented in syntax, whether it is binary or graded, and what effects argument/modifierhood have on the distribution of linguistic forms. In this work, we take a new approach to these problems</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The syntactic process. The MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Bayesian interpretation of interpolated Kneser-Ney.</title>
<date>2006</date>
<tech>Technical Report TRA2/06,</tech>
<institution>National University of Singapore, School of Computing.</institution>
<contexts>
<context position="9544" citStr="Teh, 2006" startWordPosition="1583" endWordPosition="1584"> we stop expanding at a node c. For this paper, the parameters sc are set to 0.5. In addition to defining a distribution over elementary trees, we also define a distribution which governs modification via sister–adjunction. To sample a modifier, we first decide whether or not to sister–adjoin into location l in a tree. Following this step, we sample a modifier category (e.g., a PP) conditioned on the location l’s context: its parent and left siblings. Because contexts are sparse, we use a backoff scheme based on hierarchical Dirichlet processes similar to the ngram backoff schemes defined in (Teh, 2006; Goldwater et al., 2006). Let c be a nonterminal node in a tree derived by substitution into argument positions. The node c will have n ≥ 1 children derived by argument substitution: d0,..., dn. In order to sister– adjoin between two of these children di, di+1, we recursively sample nonterminals si,1, ..., si,k until we hit a STOP symbol: Pa(si,1, ..., si,k, STOP|C0) (4) k H Pa(si,j|Cj) · (1 − PCj(STOP)) j=1 · PCk+1(STOP) where Cj = d1, s1,1, ..., di, si,1, ..., si,j−1, c is the context for the j’th modifier between these children. The distribution over sister–adjoined nonterminals is defined</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A Bayesian interpretation of interpolated Kneser-Ney. Technical Report TRA2/06, National University of Singapore, School of Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the penn treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13455" citStr="Vadas and Curran, 2007" startWordPosition="2254" endWordPosition="2258">ext examined whether our model learned to correctly identify modifiers in the corpus. Unfortunately, marking for argument/modifiers in the Penn Treebank is incomplete, and is limited to certain adverbials, e.g. locative and temporal PP’s. To supplement this markup, we made use of the corpus of (Kaeshammer and Demberg, 2012). This corpus adds annotations indicating, for each node in the Penn Treebank, whether that node is a modifier. This corpus was compiled by combining information from Propbank (Palmer et al., 2005) with a set of heuristics, as well as the NPbranching structures proposed in (Vadas and Curran, 2007). It is important to note that this corpus can only serve as a rough benchmark for evaluation of our model, as the heuristics used in its development did not always follow the correct linguistic analysis; the corpus was originally constructed for an alternative application in computational linguistics, for which non–linguisticallynatural analyses were sometimes convenient. Our model was trained on this corpus, after it had been stripped of argument/modifier annotations. We compare our model’s performance to a random baseline. Our model constrains every nonterminal to have at least one argument</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James Curran. 2007. Adding noun phrase structure to the penn treebank. In Proceedings of the 45th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>