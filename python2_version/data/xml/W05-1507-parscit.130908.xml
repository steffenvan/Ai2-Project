<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.693454">
Machine Translation as Lexicalized Parsing with Hooks
</title>
<author confidence="0.991537">
Liang Huang Hao Zhang and Daniel Gildea
</author>
<affiliation confidence="0.814558333333333">
Dept. of Computer &amp; Information Science Computer Science Department
University of Pennsylvania University of Rochester
Philadelphia, PA 19104 Rochester, NY 14627
</affiliation>
<sectionHeader confidence="0.963126" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999627">
We adapt the “hook” trick for speeding up
bilexical parsing to the decoding problem
for machine translation models that are
based on combining a synchronous con-
text free grammar as the translation model
with an n-gram language model. This
dynamic programming technique yields
lower complexity algorithms than have
previously been described for an impor-
tant class of translation models.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99929971875">
In a number of recently proposed synchronous
grammar formalisms, machine translation of new
sentences can be thought of as a form of parsing on
the input sentence. The parsing process, however,
is complicated by the interaction of the context-free
translation model with an m-graml language model
in the output language. While such formalisms ad-
mit dynamic programming solutions having poly-
nomial complexity, the degree of the polynomial is
prohibitively high.
In this paper we explore parallels between transla-
tion and monolingual parsing with lexicalized gram-
mars. Chart items in translation must be augmented
with words from the output language in order to cap-
ture language model state. This can be thought of as
a form of lexicalization with some similarity to that
of head-driven lexicalized grammars, despite being
unrelated to any notion of syntactic head. We show
&apos;We speak of m-gram language models to avoid confusion
with n, which here is the length of the input sentence for trans-
lation.
that techniques for parsing with lexicalized gram-
mars can be adapted to the translation problem, re-
ducing the complexity of decoding with an inversion
transduction grammar and a bigram language model
from O(n7) to O(n6). We present background on
this translation model as well as the use of the tech-
nique in bilexicalized parsing before describing the
new algorithm in detail. We then extend the al-
gorithm to general m-gram language models, and
to general synchronous context-free grammars for
translation.
</bodyText>
<sectionHeader confidence="0.9776045" genericHeader="method">
2 Machine Translation using Inversion
Transduction Grammar
</sectionHeader>
<bodyText confidence="0.999428333333333">
The Inversion Transduction Grammar (ITG) of Wu
(1997) is a type of context-free grammar (CFG) for
generating two languages synchronously. To model
the translational equivalence within a sentence pair,
ITG employs a synchronous rewriting mechanism to
relate two sentences recursively. To deal with the
syntactic divergence between two languages, ITG
allows the inversion of rewriting order going from
one language to another at any recursive level. ITG
in Chomsky normal form consists of unary produc-
tion rules that are responsible for generating word
pairs:
</bodyText>
<equation confidence="0.933870666666667">
X i e/f
X i e/c
X i c/f
</equation>
<bodyText confidence="0.99993825">
where e is a source language word, f is a foreign lan-
guage word, and c means the null token, and binary
production rules in two forms that are responsible
for generating syntactic subtree pairs:
</bodyText>
<equation confidence="0.748227">
X i [Y Z]
</equation>
<page confidence="0.99274">
65
</page>
<note confidence="0.755436">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 65–73,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.709872">
and
</bodyText>
<equation confidence="0.912046">
X —* (Y Z)
</equation>
<bodyText confidence="0.999938416666667">
The rules with square brackets enclosing the
right-hand side expand the left-hand side symbol
into the two symbols on the right-hand side in the
same order in the two languages, whereas the rules
with angled brackets expand the left hand side sym-
bol into the two right-hand side symbols in reverse
order in the two languages. The first class of rules
is called straight rule. The second class of rules is
called inverted rule.
One special case of 2-normal ITG is the so-called
Bracketing Transduction Grammar (BTG) which
has only one nonterminal A and two binary rules
</bodyText>
<figure confidence="0.716858666666667">
A —* [AA]
and
A —* (AA)
</figure>
<bodyText confidence="0.99969924">
By mixing instances of the inverted rule with
those of the straight rule hierarchically, BTG can
meet the alignment requirements of different lan-
guage pairs. There exists a more elaborate version
of BTG that has 4 nonterminals working together
to guarantee the property of one-to-one correspon-
dence between alignments and synchronous parse
trees. Table 1 lists the rules of this BTG. In the
discussion of this paper, we will consider ITG in 2-
normal form.
By associating probabilities or weights with the
bitext production rules, ITG becomes suitable for
weighted deduction over bitext. Given a sentence
pair, searching for the Viterbi synchronous parse
tree, of which the alignment is a byproduct, turns out
to be a two-dimensional extension of PCFG parsing,
having time complexity of O(n6), where n is the
length of the English string and the foreign language
string. A more interesting variant of parsing over bi-
text space is the asymmetrical case in which only the
foreign language string is given so that Viterbi pars-
ing involves finding the English string “on the fly”.
The process of finding the source string given its tar-
get counterpart is decoding. Using ITG, decoding is
a form of parsing.
</bodyText>
<subsectionHeader confidence="0.891074">
2.1 ITG Decoding
</subsectionHeader>
<bodyText confidence="0.998667916666667">
Wu (1996) presented a polynomial-time algorithm
for decoding ITG combined with an m-gram lan-
guage model. Such language models are commonly
used in noisy channel models of translation, which
find the best English translation e of a foreign sen-
tence f by finding the sentence e that maximizes the
product of the translation model P (f|e) and the lan-
guage model P(e).
It is worth noting that since we have specified ITG
as a joint model generating both e and f, a language
model is not theoretically necessary. Given a foreign
sentence f, one can find the best translation e*:
</bodyText>
<equation confidence="0.9624995">
e* = argmax
e
= argmax
e
</equation>
<bodyText confidence="0.997551">
by approximating the sum over parses q with the
probability of the Viterbi parse:
</bodyText>
<equation confidence="0.919357">
e* = argmax max P(e, f, q)
e q
</equation>
<bodyText confidence="0.999807928571429">
This optimal translation can be computed in using
standard CKY parsing over f by initializing the
chart with an item for each possible translation of
each foreign word in f, and then applying ITG rules
from the bottom up.
However, ITG’s independence assumptions are
too strong to use the ITG probability alone for ma-
chine translation. In particular, the context-free as-
sumption that each foreign word’s translation is cho-
sen independently will lead to simply choosing each
foreign word’s single most probable English trans-
lation with no reordering. In practice it is beneficial
to combine the probability given by ITG with a local
m-gram language model for English:
</bodyText>
<equation confidence="0.883239">
e* = argmax max P(e, f, q)Pl.(e)α
e q
</equation>
<bodyText confidence="0.999943833333333">
with some constant language model weight a. The
language model will lead to more fluent output by
influencing both the choice of English words and the
reordering, through the choice of straight or inverted
rules. While the use of a language model compli-
cates the CKY-based algorithm for finding the best
translation, a dynamic programming solution is still
possible. We extend the algorithm by storing in each
chart item the English boundary words that will af-
fect the m-gram probabilities as the item’s English
string is concatenated with the string from an adja-
cent item. Due to the locality of m-gram language
</bodyText>
<figure confidence="0.919601071428571">
P(e, f)
� P(e, f, q)
q
66
Structural Rules Lexical Rules
S → A B → hAAi C → ei/f�
S → B B → hBAi C → E/f�
S → C B → hCAi C → ei/E
A → [AB] B → hACi
A → [BB] B → hBCi
A → [CB] B → hCCi
A → [AC]
A → [BC]
A → [CC]
</figure>
<tableCaption confidence="0.99046">
Table 1: Unambiguous BTG
</tableCaption>
<bodyText confidence="0.9998294">
model, only m−1 boundary words need to be stored
to compute the new m-grams produced by combin-
ing two substrings. Figure 1 illustrates the combi-
nation of two substrings into a larger one in straight
order and inverted order.
</bodyText>
<sectionHeader confidence="0.957665" genericHeader="method">
3 Hook Trick for Bilexical Parsing
</sectionHeader>
<bodyText confidence="0.9999749">
A traditional CFG generates words at the bottom of
a parse tree and uses nonterminals as abstract rep-
resentations of substrings to build higher level tree
nodes. Nonterminals can be made more specific to
the actual substrings they are covering by associ-
ating a representative word from the nonterminal’s
yield. When the maximum number of lexicalized
nonterminals in any rule is two, a CFG is bilexical.
A typical bilexical CFG in Chomsky normal form
has two types of rule templates:
</bodyText>
<equation confidence="0.925876">
A[h] → B[h]C[h&apos;]
or
A[h] → B[h&apos;]C[h]
</equation>
<bodyText confidence="0.999980535714286">
depending on which child is the head child that
agrees with the parent on head word selection.
Bilexical CFG is at the heart of most modern statisti-
cal parsers (Collins, 1997; Charniak, 1997), because
the statistics associated with word-specific rules are
more informative for disambiguation purposes. If
we use A[i, j, h] to represent a lexicalized con-
stituent, Q(·) to represent the Viterbi score function
applicable to any constituent, and P(·) to represent
the rule probability function applicable to any rule,
Figure 2 shows the equation for the dynamic pro-
gramming computation of the Viterbi parse. The two
terms of the outermost max operator are symmetric
cases for heads coming from left and right. Contain-
ing five free variables i,j,k,h&apos;,h, ranging over 1 to
n, the length of input sentence, both terms can be
instantiated in n5 possible ways, implying that the
complexity of the parsing algorithm is O(n5).
Eisner and Satta (1999) pointed out we don’t have
to enumerate k and h&apos; simultaneously. The trick,
shown in mathematical form in Figure 2 (bottom) is
very simple. When maximizing over h&apos;, j is irrele-
vant. After getting the intermediate result of maxi-
mizing over h&apos;, we have one less free variable than
before. Throughout the two steps, the maximum
number of interacting variables is 4, implying that
the algorithmic complexity is O(n4) after binarizing
the factors cleverly. The intermediate result
</bodyText>
<equation confidence="0.904704333333333">
max[Q(B[i, k, h&apos;]) · P(A[h] → B[h&apos;]C[h])]
h&apos;,B
A
</equation>
<bodyText confidence="0.901386875">
can be represented pictorially as
same trick works for the second max term in
Equation 1. The intermediate result coming from
binarizing the second term can be visualized as
A
B[h]
k j. The shape of the intermediate re-
sults gave rise to the nickname of “hook”. Melamed
(2003) discussed the applicability of the hook trick
for parsing bilexical multitext grammars. The anal-
ysis of the hook trick in this section shows that it is
essentially an algebraic manipulation. We will for-
mulate the ITG Viterbi decoding algorithm in a dy-
namic programming equation in the following sec-
tion and apply the same algebraic manipulation to
produce hooks that are suitable for ITG decoding.
</bodyText>
<sectionHeader confidence="0.922036" genericHeader="method">
4 Hook Trick for ITG Decoding
</sectionHeader>
<bodyText confidence="0.9998485">
We start from the bigram case, in which each de-
coding constituent keeps a left boundary word and
</bodyText>
<figure confidence="0.6800985">
C[h]
i k . The
67
X
[Y Z]
X
&lt;Y Z &gt;
u21
u22 v21 v22 u11 u12 v11 v12
u11 u12 v11v12 u21u22 v21 v22
s S t s S t
(a) (b)
</figure>
<figureCaption confidence="0.979294">
Figure 1: ITG decoding using 3-gram language model. Two boundary words need to be kept on the left (u)
and right (v) of each constituent. In (a), two constituents Y and Z spanning substrings s, 5 and 5, t of the
input are combined using a straight rule X → [Y Z]. In (b), two constituents are combined using a inverted
rule X → hY Zi. The dashed line boxes enclosing three words are the trigrams produced from combining
two substrings.
</figureCaption>
<figure confidence="0.919867">
Q(A[i, j, h]) = max { k,h&apos;,B,C } (1)
[ �
max Q(B[i, k, h&apos;]) · Q(C[k, j, h]) · P (A[h] → B[h&apos;]C[h]) ,
[ �
max Q(B[i,k,h]) · Q(C[k,j,h&apos;]) · P(A[h] → B[h]C[h&apos;])
k,h&apos;,B,C
[ �
max Q(B[i,k,h&apos;]) · Q(C[k,j,h]) · P(A[h] → B[h&apos;]C[h])
k,h&apos;,B,C
= max L [ � �
k,C max Q(B[i,k,h&apos;]) · P(A[h] → B[h&apos;]C[h]) · Q(C[k,j, h])
h&apos;,B
</figure>
<figureCaption confidence="0.999803">
Figure 2: Equation for bilexical parsing (top), with an efficient factorization (bottom)
</figureCaption>
<bodyText confidence="0.999646083333333">
a right boundary word. The dynamic programming
equation is shown in Figure 3 (top) where i,j,k range
over 1 to n, the length of input foreign sentence, and
u,v,v1,u2 (or u,v,v2,u1) range over 1 to V , the size
of English vocabulary. Usually we will constrain the
vocabulary to be a subset of words that are probable
translations of the foreign words in the input sen-
tence. So V is proportional to n. There are seven
free variables related to input size for doing the max-
imization computation. Hence the algorithmic com-
plexity is O(n�).
The two terms in Figure 3 (top) within the first
level of the max operator, corresponding to straight
rules and inverted rules, are analogous to the two
terms in Equation 1. Figure 3 (bottom) shows how to
decompose the first term; the same method applies
to the second term. Counting the free variables en-
closed in the innermost max operator, we get five: i,
k, u, v1, and u2. The decomposition eliminates one
free variable, v1. In the outermost level, there are
six free variables left. The maximum number of in-
teracting variables is six overall. So, we reduced the
complexity of ITG decoding using bigram language
model from O(n�) to O(n6).
</bodyText>
<equation confidence="0.9718415">
X
u u2 Z
</equation>
<bodyText confidence="0.98163575">
The hooks i k that we have built for de-
coding with a bigram language model turn out to be
similar to the hooks for bilexical parsing if we focus
on the two boundary words v1 and u2 (or v2 and u1)
</bodyText>
<page confidence="0.984117">
68
</page>
<figure confidence="0.716738793103448">
β(X[i,j, u, v]) = max
⎧
⎨⎪⎪⎪
⎪⎪⎪⎩
⎫
⎪⎪⎪⎬
⎭⎪⎪ ⎪
(2)
k,v1,u2,Y,Z [ · P(X —* [Y Z]) · bigram(v1, u2) ] ,
β
i
k,
1
β
k,
,
·
(Y
,
u,
])
(Z[
u2, v])
max
max
2,u1,Y,Z
1
· P(X —* (Y Z)) · bigram(v2, u1) J
f
</figure>
<equation confidence="0.897667714285714">
β(Y [i, k, u1, v]) · β(Z[k, j, u, v2])
h i
max β(Y [i,k,u,v1]) · β(Z[k,j,u2,v]) · P(X —* [Y Z]) · bigram(v1,u2)
k,v1,u2,Y,Z
= max [v1,Y
max hβ(Y [i,k,u,v1]) · P(X —* [Y Z]) · bigram(v1,u2)i · β(Z[k, j,u2,v])]
k,u2,Z
</equation>
<figureCaption confidence="0.997943">
Figure 3: Equation for ITG decoding (top), with an efficient factorization (bottom)
</figureCaption>
<bodyText confidence="0.998021714285714">
that are interacting between two adjacent decoding
constituents and relate them with the h&apos; and h that
are interacting in bilexical parsing. In terms of al-
gebraic manipulation, we are also rearranging three
factors (ignoring the non-lexical rules), trying to re-
duce the maximum number of interacting variables
in any computation step.
</bodyText>
<subsectionHeader confidence="0.990393">
4.1 Generalization to m-gram Cases
</subsectionHeader>
<bodyText confidence="0.99903653125">
In this section, we will demonstrate how to use the
hook trick for trigram decoding which leads us to a
general hook trick for any m-gram decoding case.
We will work only on straight rules and use icons
of constituents and hooks to make the equations eas-
ier to interpret.
The straightforward dynamic programming equa-
tion is:
By counting the variables that are dependent
on input sentence length on the right hand side
of the equation, we know that the straightfor-
ward algorithm’s complexity is O(n11). The max-
imization computation is over four factors that
are dependent on n: β(Y [i, k, u1, u2, v11, v12]),
β(Z[k,j, u21, u22, v1, v2]), trigram(v11, v12, u21),
and trigram(v12, u21, u22). As before, our goal is
to cleverly bracket the factors.
By bracketing trigram(v11, v12, u21) and
β(Y [i, k, u1, u2, v11, v12]) together and maximizing
over v11 and Y , we can build the the level-1 hook:
The complexity is O(n7).
Grouping the level-1 hook and
trigram(v12, u21, u22), maximizing over v12,
we can build the level-2 hook:
The complexity is O(n7). Finally,
we can use the level-2 hook to com-
bine with Z[k, j, u21, u22, v1, v2] to build
X[i, j, u1, u2, v1, v2]. The complexity is O(n9)
after reducing v11 and v12 in the first two steps.
Using the hook trick, we have reduced the com-
plexity of ITG decoding using bigrams from O(n7)
to O(n6), and from O(n11) to O(n9) for trigram
</bodyText>
<equation confidence="0.998995559322034">
[
]
X
Y Z
u1u2 v11v12 u21u22 v1v2
j= max
v11,v12,u21,u22,
k,Y,Z
i k j (3)
i
X
u1u2 v1v2
X
u21
v12
Z]
= max
v11,Y
X
Y Z
]
u21
v11v12
[
u1u2
[
u1u2
i k
i k
]
= max
v12
X
[
Z
u21
u1u2
u22
i k
X
[
Z
]
u1u2
v12 u22
u21
i k
u1u2 u21u22 v1v2
j = max
u21,u22,k,Z
i k j
i
(4)
X
[
Z
]
X
u1u2 v1v2
</equation>
<page confidence="0.98248">
69
</page>
<bodyText confidence="0.999821583333333">
case. We conclude that for m-gram decoding of
ITG, the hook trick can change the the time com-
plexity from O(n3+4(m−1)) to O(n3+3(m−1)). To
get an intuition of the reduction, we can compare
Equation 3 with Equation 4. The variables v11 and
v12 in Equation 3, which are independent of v1 and
v2 for maximizing the product have been concealed
under the level-2 hook in Equation 4. In general,
by building m − 1 intermediate hooks, we can re-
duce m − 1 free variables in the final combination
step, hence having the reduction from 4(m − 1) to
3(m − 1).
</bodyText>
<sectionHeader confidence="0.98555" genericHeader="method">
5 Generalization to Non-binary Bitext
Grammars
</sectionHeader>
<bodyText confidence="0.992669492063492">
Although we have presented our algorithm as a de-
coder for the binary-branching case of Inversion
Transduction Grammar, the same factorization tech-
nique can be applied to more complex synchronous
grammars. In this general case, items in the dy-
namic programming chart may need to represent
non-contiguous span in either the input or output
language. Because synchronous grammars with in-
creasing numbers of children on the right hand side
of each production form an infinite, non-collapsing
hierarchy, there is no upper bound on the number
of discontinuous spans that may need to be repre-
sented (Aho and Ullman, 1972). One can, however,
choose to factor the grammar into binary branching
rules in one of the two languages, meaning that dis-
continuous spans will only be necessary in the other
language.
If we assume m is larger than 2, it is likely that
the language model combinations dominate com-
putation. In this case, it is advantageous to factor
the grammar in order to make it binary in the out-
put language, meaning that the subrules will only
need to represent adjacent spans in the output lan-
guage. Then the hook technique will work in the
same way, yielding O(n2(m−1)) distinct types of
items with respect to language model state, and
3(m−1) free indices to enumerate when combining
a hook with a complete constituent to build a new
item. However, a larger number of indices point-
ing into the input language will be needed now that
items can cover discontinuous spans. If the gram-
mar factorization yields rules with at most R spans
in the input language, there may be O(n2R) dis-
tinct types of chart items with respect to the input
language, because each span has an index for its
beginning and ending points in the input sentence.
Now the upper bound of the number of free in-
dices with respect to the input language is 2R + 1,
because otherwise if one rule needs 2R + 2 in-
dices, say i1, · · · , i2R+2, then there are R + 1 spans
(i1, i2), · · · , (i2R+1, i2R+2), which contradicts the
above assumption. Thus the time complexity at the
input language side is O(n2R+1), yielding a total al-
gorithmic complexity of O(n3(m−1)+(2R+1)).
To be more concrete, we will work through a 4-
ary translation rule, using a bigram language model.
The standard DP equation is:
This 4-ary rule is a representative difficult case.
The underlying alignment pattern for this rule is as
follows:
It is a rule that cannot be binarized in the bitext
space using ITG rules. We can only binarize it in
one dimension and leave the other dimension having
discontinuous spans. Without applying binarization
and hook trick, decoding parsing with it according
to Equation 5 requires time complexity of O(n13).
However, we can build the following partial con-
stituents and hooks to do the combination gradually.
The first step finishes a hook by consuming one
bigram. Its time complexity is O(n5):
The second step utilizes the hook we just built and
builds a partial constituent. The time complexity is
O(n7):
</bodyText>
<figure confidence="0.98452025">
u v3 u1 v1 u4 v4 u2 v
j = max
v3,u1,v1,u4,v4,u2,
k1,k2,k3,
B,C,D,E
i k1 k2 k3 j (5)
i
A
B C D E
A
u v
C
E
A
B
D
C D E B C D E
u v3 u1
k2 k3 = max k2 k3
v3,B
A
A
u
u1
</figure>
<page confidence="0.959123">
70
</page>
<bodyText confidence="0.966610571428571">
By “eating” another bigram, we build the second
hook using O(n7):
pruning. If we build hooks looking for all words
in the vocabulary whenever a complete constituent
is added to the chart, we will build many hooks
that are never used, because partial hypotheses with
many of the boundary words specified by the hooks
may never be constructed due to pruning. In-
stead of actively building hooks, which are inter-
mediate results, we can build them only when we
need them and then cache them for future use. To
make this idea concrete, we sketch the code for bi-
gram integrated decoding using ITG as in Algo-
rithm 1. It is worthy of noting that for clarity we
</bodyText>
<figure confidence="0.987768454545455">
v’ v
Z
D E D E
i k1 k2 k3 = max i k1 k2 k3
v1
A
A
u u4
u v1 u4
D E
u v1
C D E
u u1 v1
i k1 k2 k3 = max
u1,C
i k1 k2 k3
A
A
We use the last hook. This step has higher com- are building hooks in shape of k j , instead
plexity: O(n8): X
Y v’
v
A
u v4
k j
as we have been showing in the
E
D E
of
u
A
u4
v4
</figure>
<equation confidence="0.688663">
i k1 k2 j = max
u4,ks,D
</equation>
<bodyText confidence="0.996877818181818">
The last bigram involved in the 4-ary rule is com-
pleted and leads to the third hook, with time com-
plexity of O(n7):
previous sections. That is, the probability for the
grammar rule is multiplied in when a complete con-
stituent is built, rather than when a hook is created.
If we choose the original representation, we would
have to create both straight hooks and inverted hooks
because the straight rules and inverted rules are to be
merged with the “core” hooks, creating more speci-
fied hooks.
</bodyText>
<figure confidence="0.984123375">
i k1 k2 k3 j
A
u v4 u2
7 Conclusion
E
A
E
u u2
</figure>
<bodyText confidence="0.9103295">
The final combination is O(n7):
The overall complexity has been reduced to
O(n8) after using binarization on the output side and
using the hook trick all the way to the end. The result
is one instance of our general analysis: here R = 2,
m = 2, and 3(m − 1) + (2R + 1) = 8.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="conclusions">
6 Implementation
</sectionHeader>
<bodyText confidence="0.999978">
The implementation of the hook trick in a practi-
cal decoder is complicated by the interaction with
By showing the parallels between lexicalization for
language model state and lexicalization for syntac-
tic heads, we have demonstrated more efficient al-
gorithms for previously described models of ma-
chine translation. Decoding for Inversion Transduc-
tion Grammar with a bigram language model can be
done in O(n6) time. This is the same complexity
as the ITG alignment algorithm used by Wu (1997)
and others, meaning complete Viterbi decoding is
possible without pruning for realistic-length sen-
tences. More generally, ITG with an m-gram lan-
guage model is O(n3+3(m−1)), and a synchronous
context-free grammar with at most R spans in the
input language is O(n3(m−1)+(2R+1)). While this
improves on previous algorithms, the degree in n
is probably still too high for complete search to
be practical with such models. The interaction of
the hook technique with pruning is an interesting
</bodyText>
<figure confidence="0.981965266666667">
i k1 k2
j
A
u
u2
E
v
j = max
u2,k1,k2,E
i
A
u v
i k1 k2
j = max i k1 k2 j
v4
</figure>
<page confidence="0.575927">
71
</page>
<reference confidence="0.879875585365854">
Algorithm 1 ITGDecode(Nt)
for all s, t such that 0 &lt; s &lt; t &lt; Nt do
for all S such that s &lt; S &lt; t do
D straight rule
for all rules X —* [Y Z] E G do
for all (Y, u1, v1) possible for the span of (s, S) do
D a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is required
if not exist hooks(S, t, Z, v1) then
build hooks(S, t, Z, v1)
end if
for all v2 possible for the hooks in (S, t, Z, v1) do
D combining a hook and a hypothesis, using straight rule
β(s, t, X, u1, v2) =
n o
max β(s, t, X, u1, v2), β(s, S, Y, u1, v1) � β+(S, t, Z, v1, v2) � P(X —* [Y Z])
end for
end for
end for
D inverted rule
for all rules X —* (Y Z) E G do
for all (Z, u2, v2) possible for the span of (S, t) do
D a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is required
if not exist hooks(s, S, Y, v2) then
build hooks(s, S, Y, v2)
end if
for all v1 possible for the hooks in (s, S, Y, v2) do
D combining a hook and a hypothesis, using inverted rule
β(s, t, X, u2, v1) = 1
max 5 (S, t, X, u2, v1), β(S, t, Z, u2, v2) β+(s, S, Y, v2, v1) P(X —* (Y Z)) Y
end for ll 11
end for
end for
end for
end for
routine build hooks(s, t, X, v&apos;)
for all (X, u, v) possible for the span of (s, t) do
D combining a bigram with a hypothesis
β+(s, t, X, v&apos;, v) =
n o
max β+(s, t, X, v&apos;, v), bigram(v&apos;, u) � β(s, t, X, u, v)
end for
</reference>
<page confidence="0.999152">
72
</page>
<bodyText confidence="0.999846">
area for future work. Building the chart items with
hooks may take more time than it saves if many of
the hooks are never combined with complete con-
stituents due to aggressive pruning. However, it may
be possible to look at the contents of the chart in or-
der to build only those hooks which are likely to be
useful.
</bodyText>
<sectionHeader confidence="0.998896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999610296296296">
Aho, Albert V. and Jeffery D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume 1.
Englewood Cliffs, NJ: Prentice-Hall.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence (AAAI-97), pages 598–603, Menlo
Park, August. AAAI Press.
Collins, Michael. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Conference ofthe Association for Compu-
tational Linguistics (ACL-97), pages 16–23, Madrid,
Spain.
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and head automa-
ton grammars. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics.
Melamed, I. Dan. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of the 2003 Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL-03), Edmonton.
Wu, Dekai. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In 34th Annual Meeting
of the Association for Computational Linguistics.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
</reference>
<page confidence="0.999297">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982983">
<title confidence="0.999785">Machine Translation as Lexicalized Parsing with Hooks</title>
<author confidence="0.999095">Huang Hao Zhang Gildea</author>
<affiliation confidence="0.9999855">Dept. of Computer &amp; Information Science Computer Science Department University of Pennsylvania University of Rochester</affiliation>
<address confidence="0.99774">Philadelphia, PA 19104 Rochester, NY 14627</address>
<abstract confidence="0.998752181818182">We adapt the “hook” trick for speeding up bilexical parsing to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model an language model. This dynamic programming technique yields lower complexity algorithms than have previously been described for an important class of translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Algorithm 1 ITGDecode(Nt) for all s, t such that 0 &lt; s &lt; t &lt; Nt do for all S such that s &lt; S &lt; t do D straight rule for all rules X —* [Y Z] E G do for all (Y, u1, v1) possible for the span of (s, S) do D a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is required if not exist hooks(S, t, Z, v1) then build hooks(S, t, Z, v1) end if for all v2 possible for the hooks in (S, t, Z, v1) do D combining a hook and a hypothesis, using straight rule β(s, t,</title>
<journal>X —* (Y Z) E G</journal>
<booktitle>X, u1, v2) = n o max β(s, t, X, u1, v2), β(s, S, Y, u1, v1) � β+(S, t, Z, v1, v2) � P(X —* [Y Z]) end</booktitle>
<marker></marker>
<rawString>Algorithm 1 ITGDecode(Nt) for all s, t such that 0 &lt; s &lt; t &lt; Nt do for all S such that s &lt; S &lt; t do D straight rule for all rules X —* [Y Z] E G do for all (Y, u1, v1) possible for the span of (s, S) do D a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is required if not exist hooks(S, t, Z, v1) then build hooks(S, t, Z, v1) end if for all v2 possible for the hooks in (S, t, Z, v1) do D combining a hook and a hypothesis, using straight rule β(s, t, X, u1, v2) = n o max β(s, t, X, u1, v2), β(s, S, Y, u1, v1) � β+(S, t, Z, v1, v2) � P(X —* [Y Z]) end for end for end for D inverted rule for all rules X —* (Y Z) E G do for all (Z, u2, v2) possible for the span of (S, t) do D a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is required if not exist hooks(s, S, Y, v2) then build hooks(s, S, Y, v2) end if for all v1 possible for the hooks in (s, S, Y, v2) do D combining a hook and a hypothesis, using inverted rule β(s, t, X, u2, v1) = 1 max 5 (S, t, X, u2, v1), β(S, t, Z, u2, v2) β+(s, S, Y, v2, v1) P(X —* (Y Z)) Y end for ll 11 end for end for end for end for routine build hooks(s, t, X, v&apos;)</rawString>
</citation>
<citation valid="false">
<authors>
<author>for all</author>
</authors>
<title>v) possible for the span of (s, t) do D combining a bigram with a hypothesis β+(s, t, X, v&apos;, v) = n o max β+(s,</title>
<note>t, X, v&apos;, v), bigram(v&apos;, u) � β(s, t, X, u, v) end for</note>
<marker>all, </marker>
<rawString>for all (X, u, v) possible for the span of (s, t) do D combining a bigram with a hypothesis β+(s, t, X, v&apos;, v) = n o max β+(s, t, X, v&apos;, v), bigram(v&apos;, u) � β(s, t, X, u, v) end for</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert V Aho</author>
<author>Jeffery D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling,</booktitle>
<volume>1</volume>
<publisher>Prentice-Hall.</publisher>
<location>Englewood Cliffs, NJ:</location>
<contexts>
<context position="16410" citStr="Aho and Ullman, 1972" startWordPosition="2879" endWordPosition="2882">ammars Although we have presented our algorithm as a decoder for the binary-branching case of Inversion Transduction Grammar, the same factorization technique can be applied to more complex synchronous grammars. In this general case, items in the dynamic programming chart may need to represent non-contiguous span in either the input or output language. Because synchronous grammars with increasing numbers of children on the right hand side of each production form an infinite, non-collapsing hierarchy, there is no upper bound on the number of discontinuous spans that may need to be represented (Aho and Ullman, 1972). One can, however, choose to factor the grammar into binary branching rules in one of the two languages, meaning that discontinuous spans will only be necessary in the other language. If we assume m is larger than 2, it is likely that the language model combinations dominate computation. In this case, it is advantageous to factor the grammar in order to make it binary in the output language, meaning that the subrules will only need to represent adjacent spans in the output language. Then the hook technique will work in the same way, yielding O(n2(m−1)) distinct types of items with respect to </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Albert V. and Jeffery D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume 1. Englewood Cliffs, NJ: Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97),</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park,</location>
<contexts>
<context position="8251" citStr="Charniak, 1997" startWordPosition="1389" endWordPosition="1390">t representations of substrings to build higher level tree nodes. Nonterminals can be made more specific to the actual substrings they are covering by associating a representative word from the nonterminal’s yield. When the maximum number of lexicalized nonterminals in any rule is two, a CFG is bilexical. A typical bilexical CFG in Chomsky normal form has two types of rule templates: A[h] → B[h]C[h&apos;] or A[h] → B[h&apos;]C[h] depending on which child is the head child that agrees with the parent on head word selection. Bilexical CFG is at the heart of most modern statistical parsers (Collins, 1997; Charniak, 1997), because the statistics associated with word-specific rules are more informative for disambiguation purposes. If we use A[i, j, h] to represent a lexicalized constituent, Q(·) to represent the Viterbi score function applicable to any constituent, and P(·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h&apos;,h, ranging over 1 to n, the length of input </context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), pages 598–603, Menlo Park, August. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Conference ofthe Association for Computational Linguistics (ACL-97),</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="8234" citStr="Collins, 1997" startWordPosition="1387" endWordPosition="1388">nals as abstract representations of substrings to build higher level tree nodes. Nonterminals can be made more specific to the actual substrings they are covering by associating a representative word from the nonterminal’s yield. When the maximum number of lexicalized nonterminals in any rule is two, a CFG is bilexical. A typical bilexical CFG in Chomsky normal form has two types of rule templates: A[h] → B[h]C[h&apos;] or A[h] → B[h&apos;]C[h] depending on which child is the head child that agrees with the parent on head word selection. Bilexical CFG is at the heart of most modern statistical parsers (Collins, 1997; Charniak, 1997), because the statistics associated with word-specific rules are more informative for disambiguation purposes. If we use A[i, j, h] to represent a lexicalized constituent, Q(·) to represent the Viterbi score function applicable to any constituent, and P(·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h&apos;,h, ranging over 1 to n, the</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Conference ofthe Association for Computational Linguistics (ACL-97), pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9000" citStr="Eisner and Satta (1999)" startWordPosition="1507" endWordPosition="1510">j, h] to represent a lexicalized constituent, Q(·) to represent the Viterbi score function applicable to any constituent, and P(·) to represent the rule probability function applicable to any rule, Figure 2 shows the equation for the dynamic programming computation of the Viterbi parse. The two terms of the outermost max operator are symmetric cases for heads coming from left and right. Containing five free variables i,j,k,h&apos;,h, ranging over 1 to n, the length of input sentence, both terms can be instantiated in n5 possible ways, implying that the complexity of the parsing algorithm is O(n5). Eisner and Satta (1999) pointed out we don’t have to enumerate k and h&apos; simultaneously. The trick, shown in mathematical form in Figure 2 (bottom) is very simple. When maximizing over h&apos;, j is irrelevant. After getting the intermediate result of maximizing over h&apos;, we have one less free variable than before. Throughout the two steps, the maximum number of interacting variables is 4, implying that the algorithmic complexity is O(n4) after binarizing the factors cleverly. The intermediate result max[Q(B[i, k, h&apos;]) · P(A[h] → B[h&apos;]C[h])] h&apos;,B A can be represented pictorially as same trick works for the second max term </context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Eisner, Jason and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Multitext grammars and synchronous parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="9800" citStr="Melamed (2003)" startWordPosition="1644" endWordPosition="1645">r getting the intermediate result of maximizing over h&apos;, we have one less free variable than before. Throughout the two steps, the maximum number of interacting variables is 4, implying that the algorithmic complexity is O(n4) after binarizing the factors cleverly. The intermediate result max[Q(B[i, k, h&apos;]) · P(A[h] → B[h&apos;]C[h])] h&apos;,B A can be represented pictorially as same trick works for the second max term in Equation 1. The intermediate result coming from binarizing the second term can be visualized as A B[h] k j. The shape of the intermediate results gave rise to the nickname of “hook”. Melamed (2003) discussed the applicability of the hook trick for parsing bilexical multitext grammars. The analysis of the hook trick in this section shows that it is essentially an algebraic manipulation. We will formulate the ITG Viterbi decoding algorithm in a dynamic programming equation in the following section and apply the same algebraic manipulation to produce hooks that are suitable for ITG decoding. 4 Hook Trick for ITG Decoding We start from the bigram case, in which each decoding constituent keeps a left boundary word and C[h] i k . The 67 X [Y Z] X &lt;Y Z &gt; u21 u22 v21 v22 u11 u12 v11 v12 u11 u12</context>
</contexts>
<marker>Melamed, 2003</marker>
<rawString>Melamed, I. Dan. 2003. Multitext grammars and synchronous parsers. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In 34th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5024" citStr="Wu (1996)" startWordPosition="807" endWordPosition="808">g for the Viterbi synchronous parse tree, of which the alignment is a byproduct, turns out to be a two-dimensional extension of PCFG parsing, having time complexity of O(n6), where n is the length of the English string and the foreign language string. A more interesting variant of parsing over bitext space is the asymmetrical case in which only the foreign language string is given so that Viterbi parsing involves finding the English string “on the fly”. The process of finding the source string given its target counterpart is decoding. Using ITG, decoding is a form of parsing. 2.1 ITG Decoding Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram language model. Such language models are commonly used in noisy channel models of translation, which find the best English translation e of a foreign sentence f by finding the sentence e that maximizes the product of the translation model P (f|e) and the language model P(e). It is worth noting that since we have specified ITG as a joint model generating both e and f, a language model is not theoretically necessary. Given a foreign sentence f, one can find the best translation e*: e* = argmax e = argmax e by approxi</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Wu, Dekai. 1996. A polynomial-time algorithm for statistical machine translation. In 34th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2284" citStr="Wu (1997)" startWordPosition="350" endWordPosition="351">iques for parsing with lexicalized grammars can be adapted to the translation problem, reducing the complexity of decoding with an inversion transduction grammar and a bigram language model from O(n7) to O(n6). We present background on this translation model as well as the use of the technique in bilexicalized parsing before describing the new algorithm in detail. We then extend the algorithm to general m-gram language models, and to general synchronous context-free grammars for translation. 2 Machine Translation using Inversion Transduction Grammar The Inversion Transduction Grammar (ITG) of Wu (1997) is a type of context-free grammar (CFG) for generating two languages synchronously. To model the translational equivalence within a sentence pair, ITG employs a synchronous rewriting mechanism to relate two sentences recursively. To deal with the syntactic divergence between two languages, ITG allows the inversion of rewriting order going from one language to another at any recursive level. ITG in Chomsky normal form consists of unary production rules that are responsible for generating word pairs: X i e/f X i e/c X i c/f where e is a source language word, f is a foreign language word, and c </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Wu, Dekai. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>