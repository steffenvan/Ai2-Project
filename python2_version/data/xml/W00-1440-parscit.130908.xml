<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103497">
<title confidence="0.430258">
Appendix II: Discussion Panel on Evaluation in Generation
Research
</title>
<note confidence="0.301805">
Moderator: lndedeet Mani
</note>
<bodyText confidence="0.99779748">
Evaluation is critical in offering feedback on progress to both developers and _potential consumers
of NLG technology. However, evaluation has thus far not been as well-established in NLG as it
has become in NUL This panel will discuss evaluation methods and resources. It is aimed at
building a better understanding of NLG evaluation methods, arid hopefully arriving at steps to
facilitate future evaluations.
Applicable evaluation methods can be derived from work in NLG as well as Text Summarization
and Machine Translation. The evaluation methods include intrinsic methods which test the
generation system in itself, and extrinsic methods which test the generation system in relation to
some other task.
Intrinsic methods can include assessing coverage of different varieties of generation input, the
quality of the generated output, and comparison of generated output against reference output at
some level (e.g., by using subjective grading, comparison against templates, or comparing human
correctness in answering questions based on each type of output, etc.) Of course, a fundamental
problem in evaluating NLG is that there may be many acceptable outputs.
Extrinsic methods can include measuring efficiency in executing generated instructions (e.g., how
easy was it to install the component by following the generated manual?), assessing the relevance
of generated output to some information need or goal (e.g., are the generated business letters
effective?), its impact on a system in which it is embedded (e.g., how much does the generation
help the question answering system?), measuring the amount of effort required to post-edit the
output (e.g., how much do the generated briefings need to be fixed up?), etc.
As the generation technology becomes more mature, it is useful to assess end-user acceptability
of generated output, extensibility and portability, throughput, cost-benefit measures, etc. it is also
interesting for evaluations address both features important to the overall task, as well as features
unique to NL generation.
Participants will address the following issues:
</bodyText>
<listItem confidence="0.88362425">
I. What evaluation methods are applicable to NLG?
2. What are the pros and cons of NLO evaluations you have carried out?
3. Can we construct corpora to help evaluate NI.G systems?
4. What steps can we collectively take to improve the role of evaluation in NLG?
</listItem>
<page confidence="0.998135">
273
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.122271">
<title confidence="0.69631">Appendix II: Discussion Panel on Evaluation in Generation Research</title>
<abstract confidence="0.984860033333333">Moderator: lndedeet Mani Evaluation is critical in offering feedback on progress to both developers and _potential consumers of NLG technology. However, evaluation has thus far not been as well-established in NLG as it has become in NUL This panel will discuss evaluation methods and resources. It is aimed at building a better understanding of NLG evaluation methods, arid hopefully arriving at steps to facilitate future evaluations. Applicable evaluation methods can be derived from work in NLG as well as Text Summarization and Machine Translation. The evaluation methods include intrinsic methods which test the generation system in itself, and extrinsic methods which test the generation system in relation to some other task. Intrinsic methods can include assessing coverage of different varieties of generation input, the quality of the generated output, and comparison of generated output against reference output at level using subjective grading, comparison against templates, or comparing human correctness in answering questions based on each type of output, etc.) Of course, a fundamental problem in evaluating NLG is that there may be many acceptable outputs. methods can include measuring efficiency in executing generated instructions easy was it to install the component by following the generated manual?), assessing the relevance generated output to some information need or goal the generated business letters its impact on a system in which it is embedded much does the generation help the question answering system?), measuring the amount of effort required to post-edit the much do the generated need to be fixed up?), etc. As the generation technology becomes more mature, it is useful to assess end-user acceptability of generated output, extensibility and portability, throughput, cost-benefit measures, etc. it is also interesting for evaluations address both features important to the overall task, as well as features unique to NL generation. Participants will address the following issues: I. What evaluation methods are applicable to NLG? 2. What are the pros and cons of NLO evaluations you have carried out? Can we construct corpora to help evaluate systems? 4. What steps can we collectively take to improve the role of evaluation in NLG?</abstract>
<intro confidence="0.597489">273</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>