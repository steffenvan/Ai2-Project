<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026155">
<title confidence="0.998991">
Designing Special Post-processing Rules for SVM-based
Chinese Word Segmentation
</title>
<author confidence="0.988472">
Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, Jingbo Zhu
</author>
<affiliation confidence="0.958012">
Natural Language Processing Lab
Northeastern University
</affiliation>
<address confidence="0.952531">
No.3-11, Wenhua Road, Shenyang, Liaoning, China, 110004
</address>
<email confidence="0.9202325">
{zhumh, wangyl, wangzx, wanghz}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.993116" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960666666667">
We participated in the Third Interna-
tional Chinese Word Segmentation Bake-
off. Specifically, we evaluated our Chi-
nese word segmenter NEUCipSeg in
the close track, on all four corpora,
namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Mi-
crosoft Research (MSRA), and Univer-
sity of Pennsylvania/University of Col-
orado (UPENN). Based on Support Vec-
tor Machines (SVMs), a basic segmenter
is designed regarding Chinese word seg-
mentation as a problem of character-based
tagging. Moreover, we proposed post-
processing rules specially taking into ac-
count the properties of results brought
out by the basic segmenter. Our system
achieved good ranks in all four corpora.
</bodyText>
<sectionHeader confidence="0.919245" genericHeader="method">
1 SVM-based Chinese Word Segmenter
</sectionHeader>
<bodyText confidence="0.999977454545455">
We built out segmentation system following (Xue
and Shen, 2003), regarding Chinese word segmen-
tation as a problem of character-based tagging.
Instead of Maximum Entropy, we utilized Sup-
port Vector Machines as an alternate. SVMs are
a state-of-the-art learning algorithm, owing their
success mainly to the ability in control of general-
ization error upper-bound, and the smooth integra-
tion with kernel methods. See details in (Vapnik,
1995). We adopted svm-light1 as the specific
implementation of the model.
</bodyText>
<subsectionHeader confidence="0.995229">
1.1 Problem Formalization
</subsectionHeader>
<bodyText confidence="0.979754">
By formalizing Chinese word segmentation into
the problem of character-based tagging, we as-
</bodyText>
<footnote confidence="0.934683">
1http://svmlight.joachims.org/
</footnote>
<bodyText confidence="0.999655285714286">
signed each character to one and only one of the
four classes: word-prefix, word-suffix,
word-stem and single-character. For
example, given a two-word sequence“4.AjR
A”, the Chinese words for ”Southeast Asia(4.
AjR) people(A) ”, the character “4.”is as-
signed to the category word-prefix, indicating
the beginning of a word; “Aj”is assigned to the
category word-stem, indicating the middle po-
sition of a word; “R”belongs to the category
word-suffix, meaning the ending of a Chinese
word; and last, “A”is assigned to the category
single-character, indicating that the single
character itself is a word.
</bodyText>
<subsectionHeader confidence="0.990451">
1.2 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999651333333333">
We utilized four of the five basic feature templates
suggested in (Low et al. , 2005), described as
follows:
</bodyText>
<listItem confidence="0.99836025">
• Cn(n = −2,−1,0,1,2)
• CnCn + 1(n = −2, −1, 0,1)
• P.(C0)
• T(C−2)T (C−1)T (C0)T (C1)T (C2)
</listItem>
<bodyText confidence="0.999474583333333">
where C refers to a Chinese character. The first
two templates specify a context window with the
size of five characters, where C0 stands for the
current character: the former describes individual
characters and the latter presents bigrams within
the context window. The third template checks
if current character is a punctuation or not, and
the last one encodes characters’ type, including
four types: numbers, dates, English letters and
the type representing other characters. See de-
tail description and the example in (Low et al.
, 2005). We dropped template C−1C1, since,
</bodyText>
<page confidence="0.965277">
217
</page>
<bodyText confidence="0.960214555555555">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 217–220,
Sydney, July 2006. c�2006 Association for Computational Linguistics
in experiments, it seemed not to perform well
when incorporated by SVMs. Slightly different
from (Low et al. , 2005), character set repre-
senting dates are expanded to include “日”,
“月”,“年”,“时”,“分”,“秒”,
the Chinese characters for “day”, “month”, “year”,
“hour”,“minute”,“second”, respectively.
</bodyText>
<sectionHeader confidence="0.954874" genericHeader="method">
2 Post-processing Rules
</sectionHeader>
<bodyText confidence="0.999058590909091">
Segmentation results of SVM-based segmenter
have their particular properties. In respect to the
properties of segmentation results produced by the
SVM-based segmenter, we extracted solely from
training data comprehensive and effective post-
processing rules, which are grouped into two cate-
gories: The rules, termed IV rules, make ef-
forts to fix segmentation errors of character se-
quences, which appear both in training and test-
ing data; Rules seek to recall some OOV(Out
Of Vocabulary) words, termed OOV rules. In
practice, we sampled out a subset from train-
ing dataset as a development set for the analysis
of segmentation results produced by SVM-based
segmenter. Note that, in the following, we defined
Vocabulary to be the collection of words ap-
pearing in training dataset and Segmentation
Unit to be any isolated character sequence as-
sumed to be a valid word by a segmenter. A
segmentation unit can be a correctly seg-
mented word or an incorrectly segmented charac-
ter sequence.
</bodyText>
<subsectionHeader confidence="0.950202">
2.1 IV Rules
</subsectionHeader>
<bodyText confidence="0.955124210526316">
The following rules are named IV rules, pur-
suing the consistence between segmentation re-
sults and training data. The intuition underlying
the rules is that since training data give somewhat
specific descriptions for most of the words in it, a
character sequence in testing data should be seg-
mented in accordance with training data as much
as possible.
Ahead of post-processing, all words in the
training data are grouped into two distinct sets:
the uniquity set, which consists of words
with unique segmentation in training data and the
ambiguity set, which includes words having
more than one distinct segmentations in training
data. For example, the character sequence “新世
纪”has two kinds of segmentations, as “新
纪”(new century) and “新世纪”(as a compo-
nent of some Named-Entity, such as the name of a
restaurant).
• For each word in the uniquity set, check
whether it is wrongly segmented into more
than one segmentation units by the SVM-
based segmenter. If true, the continuous seg-
mentation units corresponding to the word
are grouped into the united one. The in-
tuition underlying this post-processing rule
is that SVM-based segmenter prefers two-
character words or single-character words
when confronting the case that the segmenter
has low self-confidence in some character-
sequence segmentation. For example, “复
制品”(duplicate) was segmented as “复
制 品 ”and “ 统 一 ”(unify) was split
into “ 统 一 ”. This phenomenon is
caused by the imbalanced data distribution.
Specifically, characters belonging to category
word-stem are much less than other three
categories.
</bodyText>
<listItem confidence="0.990805866666667">
• For each segmentation unit in the result
produced by SVM-based segmenter, check
whether the unit can be segmented into more
than one IV words and, meanwhile, the words
exist in a successive form for at least once in
training data . If true, replace the segmen-
tation unit with corresponding continuously
existing words. The intuition underlying this
rule is that SVM-based segmenter tends to
combine a word with some suffix, such as
“者” 、“人”, two Chinese characters
representing “person”. For example, “报
名 者”(Person in registration) tends to be
grouped as a single unit.
• For any sequence in the ambiguity set, such
</listItem>
<bodyText confidence="0.944972642857143">
as “ 新 世 纪 ”, check if the correct seg-
mentation can be determined by the con-
text surrounding the sequence. Without los-
ing the generality, in the following explana-
tion, we assume each sequence in the am-
biguity set has two distinct segmentations.
we collected from training data the word
preceding a sequence where each existence
of the sequence has one of its segmenta-
tions, into a collection, named preceding
word set, and, correspondingly, the fol-
lowing word into another set, which is
termed following word set. Analog-
ically, we can produce preceding word
</bodyText>
<page confidence="0.993126">
218
</page>
<bodyText confidence="0.9994513">
set and following word set for an-
other case of segmentation. When an am-
biguous sequence appears in testing data, the
surrounding context (in fact, just one preced-
ing word and a following word) is extracted.
If the context has overlapping with either of
the pre-extracted contexts of the same se-
quence which are from training data, the seg-
mentation corresponding to one of the con-
texts is retained.
</bodyText>
<listItem confidence="0.8623262">
• More over, we took a look into the annotation
errors existing in training data. We assume
there unavoidably exist some annotation mis-
takes. For example, in UPENN, the sequence
“中美”(abbreviation for China and Amer-
ica) exists, for eighty-seven times, as a whole
word and only one time, exists as “中 美”.
We regarded the segmentation “中 美”as
an annotation error. Generally, when the ra-
tio of two kinds of segmentations is greater
than a pre-determined threshold (the value is
set seven in our system), the sequence is re-
moved from the ambiguity set and added as
a word of unique segmentation into the uniq-
uity set.
</listItem>
<subsectionHeader confidence="0.993615">
2.2 OOV Rules
</subsectionHeader>
<bodyText confidence="0.995500857142857">
The following rules are termed OOV rules,
since they are utilized to recall some of the
wrongly segmented OOV words. A OOV word
is frequently segmented into two continuous OOV
segmentation units. For example, the OOV
word“ 梵 蒂 冈 ”(Vatican) was frequently seg-
mented as “ 梵 蒂 冈 ”, where both “
蒂 ”and “ 冈 ”are OOV character sequences.
Continuous OOVs present a strong clue of po-
tential segmentation errors. A rule is designed
to merge some of continuous OOVs into a cor-
rect segmentation unit. The designed rule is ap-
plicable to all four corpora. Moreover, since dis-
tinction between different segmentation standards
frequently leads to very different segmentation of
a same OOV words in different corpora, we de-
signed rules particularly for MSRA and UPENN
respectively, to recall more OOVs.
• For two continuous OOVs, check whether
at least one of them is a single-character
word. If true, group the continuous OOVs
into a segmentation unit. The reason for
the constraint of at least one of continuous
OOVs being single-character word is that not
all continuous OOVs should be combined,
for example, “德商 拜耳”, both “德
商”(Germany merchant) and“拜耳”(the
company name) are OOVs, but this sequence
is a valid segmentation unit. On the other
hand, we assume appropriately that most of
the cases for character being single-character
word have been covered by training data.
That is, once a single character is a OOV seg-
mentation unit, there exists a segmentation
error with high possibility.
</bodyText>
<listItem confidence="0.554447421052632">
• MSRA has very different segmentation stan-
dard from other three corpora, mainly be-
cause it requires to group several continuous
words together into a Name Entity. For ex-
ample, the word“中国外交部”(the Min-
istry of Foreign Affairs of China) appear-
ing in MSRA is generally annotated into two
words in other corpora, as“中国”(China)
and “外交部”(the Ministry of Foreign Af-
fairs). In our system, we first gathered all
the words from the training data whose length
are greater than six Chinese characters, filter-
ing out dates and numbers, which was cov-
eredby Finite State Automation as
a pre-processing stage. For each words col-
lected, regard the first two and three charac-
ters as NE prefix, which indicates the be-
ginning of a Name Entity. The collection of
prefixes is termed Sp(�,fZ�,;). Analogously, the
collection S3(.ffz�,;) of suffixes is brought up
in the same way. Obviously not all the pre-
fixes (suffixes) are good indicators for Name
Entities. Partly inheriting from (Brill, 1995),
we applied error-driven learning to filter pre-
fixes in Sp and suffixes in S3. Specifically,
if a prefix and a suffix are both matched in
a sequence, all the characters between them,
together with the prefix and the suffix, are
merged into a single segmentation unit. The
resulted unit is compared with corresponding
sequence in training data. If they were not ex-
actly matched, the prefix and suffix were re-
moved from collections respectively. Finally
resulted Sp and S3 are utilized to recognize
Name Entities in the initial segmentation re-
sults.
• UPENN has different segmentation standard
from other three corpora in that, for some
</listItem>
<page confidence="0.992943">
219
</page>
<table confidence="0.9989402">
Corpus R P F Roov Rzv
AS 0.949 0.940 0.944 0.694 0.960
MSRA 0.955 0.956 0.956 0.650 0.966
UPENN 0.940 0.914 0.927 0.634 0.969
CITYU 0.965 0.971 0.968 0.719 0.981
</table>
<tableCaption confidence="0.999866">
Table 1: Our official SIGHAN bakeoff results
</tableCaption>
<bodyText confidence="0.999911111111111">
Locations, such as “北京市”(Beijing
) and Organizations, such as “外Scf
部 ”(the Ministry of Foreign Affairs), the
last Chinese character presents a clue that
the character with high possibility is a suf-
fix of some words. In fact, SVM-based seg-
menter sometimes mistakenly split an OOV
word into a segmentation unit followed by a
suffix. Thus, when some suffixes exist as a
single-character segmentation unit, it should
be grouped with the preceding segmentation
unit. Undoubtedly not all suffixes are appro-
priate to this rule. To gather a clean collec-
tion of suffixes, we first clustered together the
words with the same suffix, filtering accord-
ing to the number of instances in each clus-
ter. Second, the same as above, error-driven
method is utilized to retain effective suffixes.
</bodyText>
<sectionHeader confidence="0.996749" genericHeader="method">
3 Evaluation Results
</sectionHeader>
<bodyText confidence="0.99971428">
We evaluated the Chinese word segmentation
system in the close track, on all four cor-
pora, namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Microsoft Re-
search (MSRA), and University of Pennsylva-
nia/University of Colorado (UPENN). The results
are depicted in Table 1, where columns R,P and
F refer to Recall, Precision, F measure
respectively, and ROOV , RIV for the recall of out-
of-vocabulary words and in-vocabulary words.
In addition to final results reported in Bake-
off, we also conducted a series of experiments to
evaluate the contributions of TV rules and OOV
rules. The experimental results are showed in
Table 2, where V1, V2, V3 represent versions
of our segmenters, which compose differently of
components. In detail, V1 represents the basic
SVM-based segmenter; V2 represents the seg-
menter which applied IV rules following SVM-
based segmentation; V3 represents the segmenter
composing of all the components, that is, includ-
ing SVM-based segmenter, IV rules and OOV
rules. Since the OOV ratio is much lower than IV
correspondence, the improvement made by OOV
rules is not so dramatic as IV rules.
</bodyText>
<table confidence="0.9971622">
Corpus V1 v2 v3
AS 0.932 0.94 0.944
MSRA 0.939 0.954 0.956
UPENN 0.914 0.923 0.927
CITYU 0.955 0.966 0.968
</table>
<tableCaption confidence="0.9677395">
Table 2: Word segmentation accuracy(F Measure)
resulted from post-processing rules
</tableCaption>
<sectionHeader confidence="0.992256" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999977666666667">
We added post-processing rules to SVM-based
segmenter. By doing so, we our segmentation sys-
tem achieved comparable results in the close track,
on all four corpora. But on the other hand, post-
processing rules have the problems of confliction,
which limits the number of rules. We expect to
transform rules into features of SVM-based seg-
menter, thus incorporating information carried by
rules in a more elaborate manner.
</bodyText>
<sectionHeader confidence="0.997625" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98786625">
This research was supported in part by the Na-
tional Natural Science Foundation of China(No.
60473140) and by Program for New Century Ex-
cellent Talents in University(No. NCET-05-0287).
</bodyText>
<sectionHeader confidence="0.998213" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9871392">
Nianwen Xue and Libin Shen. 2003. Chinese Word
segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing,pages 176-179.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Berlin: Springer-Verlag.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In Proceeding of the Fifth SIGHAN
Workshop on Chinese Language Processing, pages
161-164.
Eric.Brill. 1995. Transformation-based error-driven
learning and natural language processing:A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543-565.
</reference>
<page confidence="0.99743">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003025">
<title confidence="0.999462">Designing Special Post-processing Rules for SVM-based Chinese Word Segmentation</title>
<author confidence="0.670126">Muhua Zhu</author>
<author confidence="0.670126">Yilin Wang</author>
<author confidence="0.670126">Zhenxing Wang</author>
<author confidence="0.670126">Huizhen Wang</author>
<author confidence="0.670126">Jingbo</author>
<title confidence="0.620664">Natural Language Processing Northeastern</title>
<address confidence="0.869942">No.3-11, Wenhua Road, Shenyang, Liaoning, China,</address>
<email confidence="0.9291875">wangyl,wangzx,zhujingbo@mail.neu.edu.cn</email>
<abstract confidence="0.988376457680252">We participated in the Third International Chinese Word Segmentation Bakeoff. Specifically, we evaluated our Chiword segmenter the close track, on all four corpora, namely Academis Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSRA), and University of Pennsylvania/University of Colorado (UPENN). Based on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging. Moreover, we proposed postprocessing rules specially taking into account the properties of results brought out by the basic segmenter. Our system achieved good ranks in all four corpora. 1 SVM-based Chinese Word Segmenter We built out segmentation system following (Xue and Shen, 2003), regarding Chinese word segmentation as a problem of character-based tagging. Instead of Maximum Entropy, we utilized Support Vector Machines as an alternate. SVMs are a state-of-the-art learning algorithm, owing their success mainly to the ability in control of generalization error upper-bound, and the smooth integration with kernel methods. See details in (Vapnik, We adopted as the specific implementation of the model. 1.1 Problem Formalization By formalizing Chinese word segmentation into problem of character-based tagging, we assigned each character to one and only one of the classes: For given a two-word the Chinese words for ”Southeast ”, the character asto the category indicating beginning of a word; assigned to the indicating the middle poof a word; to the category meaning the ending of a Chinese and last, assigned to the category indicating that the single character itself is a word. 1.2 Feature Templates We utilized four of the five basic feature templates suggested in (Low et al. , 2005), described as follows: • • • • to a Chinese character. The first two templates specify a context window with the of five characters, where for the current character: the former describes individual characters and the latter presents bigrams within the context window. The third template checks if current character is a punctuation or not, and the last one encodes characters’ type, including four types: numbers, dates, English letters and the type representing other characters. See detail description and the example in (Low et al. 2005). We dropped template since, 217 of the Fifth SIGHAN Workshop on Chinese Language pages July 2006. Association for Computational Linguistics in experiments, it seemed not to perform well when incorporated by SVMs. Slightly different from (Low et al. , 2005), character set repredates are expanded to include the Chinese characters for “day”, “month”, “year”, “hour”,“minute”,“second”, respectively. 2 Post-processing Rules Segmentation results of SVM-based segmenter have their particular properties. In respect to the properties of segmentation results produced by the SVM-based segmenter, we extracted solely from training data comprehensive and effective postprocessing rules, which are grouped into two cate- The rules, termed make efforts to fix segmentation errors of character sequences, which appear both in training and testing data; Rules seek to recall some OOV(Out Vocabulary) words, termed In practice, we sampled out a subset from training dataset as a development set for the analysis of segmentation results produced by SVM-based segmenter. Note that, in the following, we defined be the collection of words apin training dataset and be any isolated character sequence asto be a valid word by a segmenter. unit be a correctly segmented word or an incorrectly segmented character sequence. 2.1 IV Rules following rules are named pursuing the consistence between segmentation results and training data. The intuition underlying the rules is that since training data give somewhat specific descriptions for most of the words in it, a character sequence in testing data should be segmented in accordance with training data as much as possible. Ahead of post-processing, all words in the training data are grouped into two distinct sets: which consists of words with unique segmentation in training data and the which includes words having more than one distinct segmentations in training For example, the character sequence “新世 two kinds of segmentations, as century) and a component of some Named-Entity, such as the name of a restaurant). • For each word in the uniquity set, check whether it is wrongly segmented into more than one segmentation units by the SVMbased segmenter. If true, the continuous segmentation units corresponding to the word are grouped into the united one. The intuition underlying this post-processing rule is that SVM-based segmenter prefers twocharacter words or single-character words when confronting the case that the segmenter has low self-confidence in some charactersegmentation. For example, “复 was segmented as “复 品 统 一 was split 统 一 This phenomenon is caused by the imbalanced data distribution. Specifically, characters belonging to category much less than other three categories. • For each segmentation unit in the result produced by SVM-based segmenter, check whether the unit can be segmented into more than one IV words and, meanwhile, the words exist in a successive form for at least once in training data . If true, replace the segmentation unit with corresponding continuously existing words. The intuition underlying this rule is that SVM-based segmenter tends to combine a word with some suffix, such as two Chinese characters “person”. For example, “报 in registration) tends to be grouped as a single unit. • For any sequence in the ambiguity set, such 新 世 纪 check if the correct segmentation can be determined by the context surrounding the sequence. Without losing the generality, in the following explanation, we assume each sequence in the ambiguity set has two distinct segmentations. we collected from training data the word preceding a sequence where each existence of the sequence has one of its segmentainto a collection, named and, correspondingly, the following word into another set, which is word Analogwe can produce word 218 word set another case of segmentation. When an ambiguous sequence appears in testing data, the surrounding context (in fact, just one preceding word and a following word) is extracted. If the context has overlapping with either of the pre-extracted contexts of the same sequence which are from training data, the segmentation corresponding to one of the contexts is retained. • More over, we took a look into the annotation errors existing in training data. We assume there unavoidably exist some annotation mistakes. For example, in UPENN, the sequence for China and America) exists, for eighty-seven times, as a whole and only one time, exists as regarded the segmentation an annotation error. Generally, when the ratio of two kinds of segmentations is greater than a pre-determined threshold (the value is set seven in our system), the sequence is removed from the ambiguity set and added as a word of unique segmentation into the uniquity set. 2.2 OOV Rules following rules are termed since they are utilized to recall some of the wrongly segmented OOV words. A OOV word is frequently segmented into two continuous OOV segmentation units. For example, the OOV 梵 蒂 冈 was frequently segas 梵 蒂 冈 where both 冈 OOV character sequences. Continuous OOVs present a strong clue of potential segmentation errors. A rule is designed to merge some of continuous OOVs into a correct segmentation unit. The designed rule is applicable to all four corpora. Moreover, since distinction between different segmentation standards frequently leads to very different segmentation of a same OOV words in different corpora, we designed rules particularly for MSRA and UPENN respectively, to recall more OOVs. • For two continuous OOVs, check whether at least one of them is a single-character word. If true, group the continuous OOVs into a segmentation unit. The reason for the constraint of at least one of continuous OOVs being single-character word is that not all continuous OOVs should be combined, example, both “德 merchant) company name) are OOVs, but this sequence is a valid segmentation unit. On the other hand, we assume appropriately that most of the cases for character being single-character word have been covered by training data. That is, once a single character is a OOV segmentation unit, there exists a segmentation error with high possibility. • MSRA has very different segmentation standard from other three corpora, mainly because it requires to group several continuous words together into a Name Entity. For exthe Ministry of Foreign Affairs of China) appearing in MSRA is generally annotated into two in other corpora, Ministry of Foreign Affairs). In our system, we first gathered all the words from the training data whose length are greater than six Chinese characters, filtering out dates and numbers, which was cov- State Automation a pre-processing stage. For each words collected, regard the first two and three characas which indicates the beginning of a Name Entity. The collection of is termed Analogously, the suffixes is brought up in the same way. Obviously not all the prefixes (suffixes) are good indicators for Name Entities. Partly inheriting from (Brill, 1995), we applied error-driven learning to filter prein and suffixes in Specifically, if a prefix and a suffix are both matched in a sequence, all the characters between them, together with the prefix and the suffix, are merged into a single segmentation unit. The resulted unit is compared with corresponding sequence in training data. If they were not exactly matched, the prefix and suffix were removed from collections respectively. Finally and are utilized to recognize Name Entities in the initial segmentation results. • UPENN has different segmentation standard from other three corpora in that, for some 219 Corpus R P F AS 0.949 0.940 0.944 0.694 0.960 MSRA 0.955 0.956 0.956 0.650 0.966 UPENN 0.940 0.914 0.927 0.634 0.969 CITYU 0.965 0.971 0.968 0.719 0.981 Table 1: Our official SIGHAN bakeoff results such as and such as Ministry of Foreign Affairs), the last Chinese character presents a clue that the character with high possibility is a suffix of some words. In fact, SVM-based segmenter sometimes mistakenly split an OOV word into a segmentation unit followed by a suffix. Thus, when some suffixes exist as a single-character segmentation unit, it should be grouped with the preceding segmentation unit. Undoubtedly not all suffixes are appropriate to this rule. To gather a clean collection of suffixes, we first clustered together the words with the same suffix, filtering according to the number of instances in each cluster. Second, the same as above, error-driven method is utilized to retain effective suffixes. 3 Evaluation Results We evaluated the Chinese word segmentation system in the close track, on all four corpora, namely Academis Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSRA), and University of Pennsylvania/University of Colorado (UPENN). The results depicted in Table 1, where columns to measure and the recall of outof-vocabulary words and in-vocabulary words. In addition to final results reported in Bakeoff, we also conducted a series of experiments to the contributions of rules The experimental results are showed in Table 2, where V1, V2, V3 represent versions of our segmenters, which compose differently of components. In detail, V1 represents the basic SVM-based segmenter; V2 represents the segmenter which applied IV rules following SVMbased segmentation; V3 represents the segmenter composing of all the components, that is, including SVM-based segmenter, IV rules and OOV rules. Since the OOV ratio is much lower than IV correspondence, the improvement made by OOV rules is not so dramatic as IV rules.</abstract>
<note confidence="0.673159">Corpus V1 v2 v3 AS 0.932 0.94 0.944 MSRA 0.939 0.954 0.956 UPENN 0.914 0.923 0.927 CITYU 0.955 0.966 0.968 Table 2: Word segmentation accuracy(F Measure) resulted from post-processing rules</note>
<abstract confidence="0.9874088">4 Conclusions and future work We added post-processing rules to SVM-based segmenter. By doing so, we our segmentation system achieved comparable results in the close track, on all four corpora. But on the other hand, postprocessing rules have the problems of confliction, which limits the number of rules. We expect to transform rules into features of SVM-based segmenter, thus incorporating information carried by rules in a more elaborate manner.</abstract>
<note confidence="0.971050384615385">Acknowledgements This research was supported in part by the National Natural Science Foundation of China(No. 60473140) and by Program for New Century Excellent Talents in University(No. NCET-05-0287). References Nianwen Xue and Libin Shen. 2003. Chinese Word as LMR tagging. Proceedings of the Second SIGHAN Workshop on Chinese Lan- 176-179. Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag. Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.</note>
<title confidence="0.828888">A Maximum Entropy Approach to Chinese Word Proceeding of the Fifth SIGHAN on Chinese Language pages</title>
<abstract confidence="0.9911555">161-164. Eric.Brill. 1995. Transformation-based error-driven learning and natural language processing:A case in part-of-speech tagging. Lin-</abstract>
<address confidence="0.593606">220</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese Word segmentation as LMR tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,pages</booktitle>
<pages>176--179</pages>
<contexts>
<context position="1110" citStr="Xue and Shen, 2003" startWordPosition="155" endWordPosition="158"> track, on all four corpora, namely Academis Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSRA), and University of Pennsylvania/University of Colorado (UPENN). Based on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging. Moreover, we proposed postprocessing rules specially taking into account the properties of results brought out by the basic segmenter. Our system achieved good ranks in all four corpora. 1 SVM-based Chinese Word Segmenter We built out segmentation system following (Xue and Shen, 2003), regarding Chinese word segmentation as a problem of character-based tagging. Instead of Maximum Entropy, we utilized Support Vector Machines as an alternate. SVMs are a state-of-the-art learning algorithm, owing their success mainly to the ability in control of generalization error upper-bound, and the smooth integration with kernel methods. See details in (Vapnik, 1995). We adopted svm-light1 as the specific implementation of the model. 1.1 Problem Formalization By formalizing Chinese word segmentation into the problem of character-based tagging, we as1http://svmlight.joachims.org/ signed e</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese Word segmentation as LMR tagging. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,pages 176-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<location>Berlin:</location>
<contexts>
<context position="1485" citStr="Vapnik, 1995" startWordPosition="214" endWordPosition="215">ially taking into account the properties of results brought out by the basic segmenter. Our system achieved good ranks in all four corpora. 1 SVM-based Chinese Word Segmenter We built out segmentation system following (Xue and Shen, 2003), regarding Chinese word segmentation as a problem of character-based tagging. Instead of Maximum Entropy, we utilized Support Vector Machines as an alternate. SVMs are a state-of-the-art learning algorithm, owing their success mainly to the ability in control of generalization error upper-bound, and the smooth integration with kernel methods. See details in (Vapnik, 1995). We adopted svm-light1 as the specific implementation of the model. 1.1 Problem Formalization By formalizing Chinese word segmentation into the problem of character-based tagging, we as1http://svmlight.joachims.org/ signed each character to one and only one of the four classes: word-prefix, word-suffix, word-stem and single-character. For example, given a two-word sequence“4.AjR A”, the Chinese words for ”Southeast Asia(4. AjR) people(A) ”, the character “4.”is assigned to the category word-prefix, indicating the beginning of a word; “Aj”is assigned to the category word-stem, indicating the m</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation.</title>
<date>2005</date>
<booktitle>In Proceeding of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005. A Maximum Entropy Approach to Chinese Word Segmentation. In Proceeding of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing:A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="10793" citStr="Brill, 1995" startWordPosition="1732" endWordPosition="1733">s). In our system, we first gathered all the words from the training data whose length are greater than six Chinese characters, filtering out dates and numbers, which was coveredby Finite State Automation as a pre-processing stage. For each words collected, regard the first two and three characters as NE prefix, which indicates the beginning of a Name Entity. The collection of prefixes is termed Sp(�,fZ�,;). Analogously, the collection S3(.ffz�,;) of suffixes is brought up in the same way. Obviously not all the prefixes (suffixes) are good indicators for Name Entities. Partly inheriting from (Brill, 1995), we applied error-driven learning to filter prefixes in Sp and suffixes in S3. Specifically, if a prefix and a suffix are both matched in a sequence, all the characters between them, together with the prefix and the suffix, are merged into a single segmentation unit. The resulted unit is compared with corresponding sequence in training data. If they were not exactly matched, the prefix and suffix were removed from collections respectively. Finally resulted Sp and S3 are utilized to recognize Name Entities in the initial segmentation results. • UPENN has different segmentation standard from ot</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric.Brill. 1995. Transformation-based error-driven learning and natural language processing:A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>