<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011246">
<title confidence="0.998746">
Source-side Syntactic Reordering Patterns with Functional Words for
Improved Phrase-based SMT
</title>
<author confidence="0.917583">
Jie Jiang, Jinhua Du, Andy Way
</author>
<affiliation confidence="0.914866">
CNGL, School of Computing, Dublin City University
</affiliation>
<email confidence="0.99743">
{jjiang,jdu,away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.99735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833925925926">
Inspired by previous source-side syntactic
reordering methods for SMT, this paper
focuses on using automatically learned
syntactic reordering patterns with func-
tional words which indicate structural re-
orderings between the source and target
language. This approach takes advan-
tage of phrase alignments and source-side
parse trees for pattern extraction, and then
filters out those patterns without func-
tional words. Word lattices transformed
by the generated patterns are fed into PB-
SMT systems to incorporate potential re-
orderings from the inputs. Experiments
are carried out on a medium-sized cor-
pus for a Chinese–English SMT task. The
proposed method outperforms the base-
line system by 1.38% relative on a ran-
domly selected testset and 10.45% rela-
tive on the NIST 2008 testset in terms
of BLEU score. Furthermore, a system
with just 61.88% of the patterns filtered
by functional words obtains a comparable
performance with the unfiltered one on the
randomly selected testset, and achieves
1.74% relative improvements on the NIST
2008 testset.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999026761904762">
Previous work has shown that the problem of
structural differences between language pairs in
SMT can be alleviated by source-side syntactic
reordering. Taking account for the integration
with SMT systems, these methods can be divided
into two different kinds of approaches (Elming,
2008): the deterministic reordering and the non-
deterministic reordering approach.
To carry out the deterministic approach, syntac-
tic reordering is performed uniformly on the train-
ing, devset and testset before being fed into the
SMT systems, so that only the reordered source
sentences are dealt with while building during
the SMT system. In this case, most work is fo-
cused on methods to extract and to apply syntac-
tic reordering patterns which come from manually
created rules (Collins et al., 2005; Wang et al.,
2007a), or via an automatic extraction process tak-
ing advantage of parse trees (Collins et al., 2005;
Habash, 2007). Because reordered source sen-
tence cannot be undone by the SMT decoders (Al-
Onaizan et al., 2006), which implies a systematic
error for this approach, classifiers (Chang et al.,
2009b; Du &amp; Way, 2010) are utilized to obtain
high-performance reordering for some specialized
syntactic structures (e.g. DE construction in Chi-
nese).
On the other hand, the non-deterministic ap-
proach leaves the decisions to the decoders to
choose appropriate source-side reorderings. This
is more flexible because both the original and
reordered source sentences are presented in the
inputs. Word lattices generated from syntactic
structures for N-gram-based SMT is presented
in (Crego et al., 2007). In (Zhang et al., 2007a;
Zhang et al., 2007b), chunks and POS tags are
used to extract reordering rules, while the gener-
ated word lattices are weighted by language mod-
els and reordering models. Rules created from a
syntactic parser are also utilized to form weighted
n-best lists which are fed into the decoder (Li et
al., 2007). Furthermore, (Elming, 2008; Elm-
</bodyText>
<page confidence="0.992491">
19
</page>
<note confidence="0.9818625">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19–27,
COLING 2010, Beijing, August 2010.
</note>
<bodyText confidence="0.996956513888889">
ing, 2009) uses syntactic rules to score the output
word order, both on English–Danish and English–
Arabic tasks. Syntactic reordering information is
also considered as an extra feature to improve PB-
SMT in (Chang et al., 2009b) for the Chinese–
English task. These results confirmed the effec-
tiveness of syntactic reorderings.
However, for the particular case of Chinese
source inputs, although the DE construction has
been addressed for both PBSMT and HPBSMT
systems in (Chang et al., 2009b; Du &amp; Way,
2010), as indicated by (Wang et al., 2007a), there
are still lots of unexamined structures that im-
ply source-side reordering, especially in the non-
deterministic approach. As specified in (Xue,
2005), these include the bei-construction, ba-
construction, three kinds of de-construction (in-
cluding DE construction) and general preposition
constructions. Such structures are referred with
functional words in this paper, and all the con-
structions can be identified by their correspond-
ing tags in the Penn Chinese TreeBank. It is in-
teresting to investigate these functional words for
the syntactic reordering task since most of them
tend to produce structural reordering between the
source and target sentences.
Another related work is to filter the bilingual
phrase pairs with closed-class words (S´anchez-
Martinez, 2009). By taking account of the word
alignments and word types, the filtering process
reduces the phrase tables by up to a third, but still
provide a system with competitive performance
compared to the baseline. Similarly, our idea is to
use special type of words for the filtering purpose
on the syntactic reordering patterns.
In this paper, our objective is to exploit
these functional words for source-side syntac-
tic reordering of Chinese–English SMT in the
non-deterministic approach. Our assumption is
that syntactic reordering patterns with functional
words are the most effective ones, and others can
be pruned for both speed and performance.
To validate this assumption, three systems are
compared in this paper: a baseline PBSMT sys-
tem, a syntactic reordering system with all pat-
terns extracted from a corpus, and a syntactic re-
ordering system with patterns filtered with func-
tional words. To accomplish this, firstly the lat-
tice scoring approach (Jiang et al., 2010) is uti-
lized to discover non-monotonic phrase align-
ments, and then syntactic reordering patterns are
extracted from source-side parse trees. After that,
functional word tags specified in (Xue, 2005) are
adopted to perform pattern filtering. Finally, both
the unfiltered pattern set and the filtered one are
used to transform inputs into word lattices to
present potential reorderings for improving PB-
SMT system. A comparison between the three
systems is carried out to examine the performance
of syntactic reordering as well as the usefulness of
functional words for pattern filtering.
The rest of this paper is organized as follows:
in section 2 we describe the extraction process of
syntactic reordering patterns, including the lattice
scoring approach and the extraction procedures.
Then section 3 presents the filtering process used
to obtain patterns with functional words. After
that, section 4 shows the generation of word lat-
tices with patterns, and experimental setup and re-
sults included related discussion are presented in
section 5. Finally, we give our conclusion and av-
enues for future work in section 6.
</bodyText>
<sectionHeader confidence="0.991568" genericHeader="method">
2 Syntactic reordering patterns
extraction
</sectionHeader>
<bodyText confidence="0.999960411764706">
Instead of top-down approaches such as (Wang
et al., 2007a; Chang et al., 2009a), we use a
bottom-up approach similar to (Xia et al., 2004;
Crego et al., 2007) to extract syntactic reordering
patterns from non-monotonic phrase alignments
and source-side parse trees. The following steps
are carried out to extract syntactic reordering pat-
terns: 1) the lattice scoring approach proposed
in (Jiang et al., 2010) is used to obtain phrase
alignments from the training corpus; 2) reorder-
ing regions from the non-monotonic phrase align-
ments are used to identify minimum treelets for
pattern extraction; and 3) the treelets are trans-
formed into syntactic reordering patterns which
are then weighted by their occurrences in the
training corpus. Details of each of these steps are
presented in the rest of this section.
</bodyText>
<subsectionHeader confidence="0.997476">
2.1 Lattice scoring for phrase alignments
</subsectionHeader>
<bodyText confidence="0.9998195">
The lattice scoring approach is proposed in (Jiang
et al., 2010) for the SMT data cleaning task.
</bodyText>
<page confidence="0.990126">
20
</page>
<bodyText confidence="0.999902523809524">
To clean the training corpus, word alignments
are used to obtain approximate decoding results,
which are then used to calculate BLEU (Papineni
et al., 2002) scores to filter out low-scoring sen-
tences pairs. The following steps are taken in
the lattice scoring approach: 1) train an initial
PBSMT model; 2) collect anchor pairs contain-
ing source and target phrase positions from word
alignments generated in the training phase; 3)
build source-side lattices from the anchor pairs
and the translation model; 4) search on the source-
side lattices to obtain approximate decoding re-
sults; 5) calculate BLEU scores for the purpose of
data cleaning.
Note that the source-side lattices in step 3 come
from anchor pairs, so each edge in the lattices con-
tain both the source and target phrase positions.
Thus the outputs of step 4 contain phrase align-
ments on the training corpus. These phrase align-
ments are used to identify non-monotonic areas
for the extraction of reordering patterns.
</bodyText>
<subsectionHeader confidence="0.999812">
2.2 Reordering patterns
</subsectionHeader>
<bodyText confidence="0.997387833333333">
Non-monotonic regions of the phrase alignments
are examined as potential source-side reorderings.
By taking a bottom-up approach, the reordering
regions are identified and mapped to minimum
treelets on the source parse trees. After that, syn-
tactic reordering patterns are transformed from
these minimum treelets.
In this paper, reordering regions A and B indi-
cating swapping operations on the source side are
only considered as potential source-side reorder-
ings. Thus, given reordering regions AB, this im-
plies (1):
</bodyText>
<equation confidence="0.992236">
AB ⇒ BA (1)
</equation>
<bodyText confidence="0.999692166666667">
on the source-side word sequences. Referring to
the phrase alignment extraction in the last section,
each non-monotonic phrase alignment produces
one reordering region. Furthermore, for each re-
ordering region identified, all of its sub-areas in-
dicating non-monotonic alignments are also at-
tempted to produce more reordering regions.
To represent the reordering region using syn-
tactic structure, given the extracted reordering re-
gions AB, the following steps are taken to map
them onto the source-side parse trees, and to gen-
erate corresponding patterns:
</bodyText>
<listItem confidence="0.999640578947368">
1. Generate a parse tree for each of the source
sentences. The Berkeley parser (Petrov,
2006) is used in this paper. To obtain sim-
pler tree structures, right-binarization is per-
formed on the parse trees, while tags gener-
ated from binarization are not distinguished
from the original ones (e.g. @V P and V P
are the same).
2. Map reordering regions AB onto the parse
trees. Denote NA as the set of leaf nodes in
region A and NB for region B. The mapping
is carried out on the parse tree to find a mini-
mum treelet T, which satisfies the following
two criteria: 1) there must exist a path from
each node in NA ∪ NB to the root node of
T; 2) each leaf node of T can only be the
ancestor of nodes in NA or NB (or none of
them).
3. Traverse T in pre-order to obtain syntactic
</listItem>
<bodyText confidence="0.951232555555556">
reordering pattern P. Label all the leaf nodes
of T with A or B as reorder options, which
indicate that the descendants of nodes with
label A are supposed to be swapped with
those with label B.
Instead of using subtrees, we use treelets to
refer the located parse tree substructures, since
treelets do not necessarily go down to leaf nodes.
Since phrase alignments cannot always be per-
fectly matched with parse trees, we also expand
AB to the right and/or the left side with a limited
number of words to find a minimum treelet. In
this situation, a minimum number of ancestors of
expanded tree nodes are kept in T but they are as-
signed the same labels as those from which they
have been expanded. In this case, the expanded
tree nodes are considered as the context nodes of
syntactic reordering patterns.
Figure 1 illustrates the extraction process. Note
the symbol @ indicates the right-binarization sym-
bols (e.g. @V P in the figure). In the figure, tree
T (surrounded by dashed lines) is the minimum
treelet mapped from the reordering region AB.
Leaf node NP is labeled by A, V P is labeled by
B, and the context node P is also labeled by A.
Leaf nodes labeled A or B are collected into node
sequences LA or LB to indicate the reordering op-
</bodyText>
<page confidence="0.995039">
21
</page>
<figure confidence="0.9982875">
T
A B
</figure>
<figureCaption confidence="0.99997">
Figure 1: Reordering pattern extraction
</figureCaption>
<bodyText confidence="0.9819375">
erations. Thus the syntactic reordering pattern P
is obtained from T as in (2):
</bodyText>
<equation confidence="0.8850015">
P = {V P (PP (P NP) V P)|O = {LA, LB}}
(2)
</equation>
<bodyText confidence="0.999976">
where the first part of P is the V P with its tree
structure, and the second part O indicates the re-
ordering scheme, which implies that source words
corresponding with descendants of LA are sup-
posed to be swapped with those of LB.
</bodyText>
<subsectionHeader confidence="0.995884">
2.3 Pattern weights estimation
</subsectionHeader>
<bodyText confidence="0.99982875">
We use preo to represent the chance of reordering
when a treelet is located by a pattern on the parse
tree. It is estimated by the number of reorderings
for each of the occurrences of the pattern as in (3):
</bodyText>
<equation confidence="0.9518565">
preo(P) = count{reorderings of P}
count{observation of P} (3)
</equation>
<bodyText confidence="0.998909">
By contrast, one syntactic pattern P usually con-
tains several reordering schemes (specified in for-
mula (2)), each of them weighted as in (4):
</bodyText>
<equation confidence="0.966561">
count{reorderings of O in P}
w(O,P) =
count{reorderings of P}
(4)
</equation>
<bodyText confidence="0.984675">
Generally, a syntactic reordering pattern is ex-
pressed as in (5):
</bodyText>
<equation confidence="0.97568">
P = {tree  |preo  |O1, w1, · · · , On, wn} (5)
</equation>
<bodyText confidence="0.999968666666667">
where tree is the tree structures of the pattern,
preo is the reordering probability, Oi and wi are
the reordering schemes and weights (1 ≤ i ≤ n).
</bodyText>
<sectionHeader confidence="0.90816" genericHeader="method">
3 Patterns with functional words
</sectionHeader>
<bodyText confidence="0.99985472">
Some of the patterns extracted may not benefit
the final system since the extraction process is
controlled by phrase alignments rather than syn-
tactic knowledge. Inspired by the study of DE
constructions (Chang et al., 2009a; Du &amp; Way,
2010), we assume that syntactic reorderings are
indicated by functional words for the Chinese–
English task. To incorporate the knowledge of
functional words into the extracted patterns, in-
stead of directly specifying the syntactic struc-
ture from the linguistic aspects, we use functional
word tags to filter the extracted patterns. In this
case, we assume that all patterns containing func-
tional words tend to produce meaningful syntactic
reorderings. Thus the filtered patterns carry the re-
ordering information from the phrase alignments
as well as the linguistic knowledge. Thus the
noise produced in phrase alignments and the size
of pattern set can be reduced, so that the speed and
the performance of the system can be improved.
The functional word tags used in this paper are
shown in Table 1, which come from (Xue, 2005).
We choose them as functional words because nor-
mally they imply word reorders between Chinese
and English sentence pairs.
</bodyText>
<table confidence="0.998898777777778">
Tag Description
BA ba-construction
DEC de (1st kind) in a relative-clause
DEG associative de (1st kind)
DER de (2nd kind) in V-de const. &amp; V-de-R
DEV de (3rd kind) before VP
LB bei in long bei-construction
P preposition excluding bei and ba
SB bei in short bei-construction
</table>
<tableCaption confidence="0.9331775">
Table 1: Syntactic reordering tags for functional
words
</tableCaption>
<bodyText confidence="0.995458857142857">
Note that there are three kinds of de-
constructions, but only the first kind is the DE
construction in (Chang et al., 2009a; Du &amp; Way,
2010). After the filtering process, both the unfil-
tered pattern set and the filtered one are used to
build different syntactic reordering PBSMT sys-
tems for comparison purpose.
</bodyText>
<page confidence="0.994276">
22
</page>
<sectionHeader confidence="0.974993" genericHeader="method">
4 Word lattice construction
</sectionHeader>
<bodyText confidence="0.992267625">
Both the devset and testset are transformed into
word lattices by the extracted patterns to incor-
porate potential reorderings. Figure 2 illustrates
this process: treelet T′ is matched with a pat-
tern, then its leaf nodes {a1, · · · am} ∈ LA (span-
ning {w1, · · · , wp}) are swapped with leaf nodes
{b1, · · · , bn} ∈ LB (spanning {v1, · · · , vq}) on
the generated paths in the word lattice.
</bodyText>
<figureCaption confidence="0.85606">
Figure 2: Incorporating potential reorderings into
lattices
</figureCaption>
<bodyText confidence="0.9999825">
We sort the matched patterns by preo in formula
(5), and only apply a pre-defined number of re-
orderings for each sentence. For each lattice node,
if we denote E0 as the edge from the original sen-
tence, while patterns {P1, · · · , Pi, · · · , Pk} are ap-
plied to this node, then E0 is weighted as in (6):
</bodyText>
<equation confidence="0.96977">
{(1 − α) k∗ {1 − preo(Pi)}}
(6)
</equation>
<bodyText confidence="0.9997314">
where preo(Pi) is the pattern weight in formula
(3), and α is the base probability to avoid E0 be-
ing equal to zero. Suppose {Es, · · · , Es+r−1} are
generated by r reordering schemes of Pi, then Ej
is weighted as in (7):
</bodyText>
<equation confidence="0.9998395">
w(Ej) = (1 −α) k∗preo(Pi)∗ ws−j+1(Pi)
�r t=1 wt(Pi) (7)
</equation>
<bodyText confidence="0.99993675">
where wt(Pi) is the reordering scheme in formula
(5), and s &lt;= j &lt; s + r. Reordering patterns
with the same root lattice node share equal proba-
bilities in formula (6) and (7).
</bodyText>
<sectionHeader confidence="0.991999" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.999984916666667">
We conducted our experiments on a medium-sized
corpus FBIS (a multilingual paragraph-aligned
corpus with LDC resource number LDC2003E14)
for the Chinese–English SMT task. The Cham-
pollion aligner (Ma, 2006) is utilized to perform
sentence alignment. A total number of 256,911
sentence pairs are obtained, while 2,000 pairs for
devset and 2,000 pairs for testset are randomly se-
lected, which we call FBIS set. The rest of the
data is used as the training corpus.
The baseline system is Moses (Koehn et
al., 2007), and GIZA++1 is used to perform
word alignment. Minimum error rate training
(MERT) (Och, 2003) is carried out for tuning. A
5-gram language model built via SRILM2 is used
for all the experiments in this paper.
Experiments results are reported on two differ-
ent sets: the FBIS set and the NIST set. For the
NIST set, the NIST 2005 testset (1,082 sentences)
is used as the devset, and the NIST 2008 test-
set (1,357 sentences) is used as the testset. The
FBIS set contains only one reference translation
for both devset and testset, while NIST set has
four references.
</bodyText>
<subsectionHeader confidence="0.7911985">
5.1 Pattern extraction and filtering with
functional words
</subsectionHeader>
<bodyText confidence="0.999878647058824">
The lattice scoring approach is carried out with
the same baseline system as specified above to
produce the phrase alignments. The initial PB-
SMT system in the lattice scoring approach is
tuned with the FBIS devset to obtain the weights.
As specified in section 2.1, phrase alignments are
generated in the step 4 of the lattice scoring ap-
proach.
From the generated phrase alignments and
source-side parse trees of the training corpus,
we obtain 48,285 syntactic reordering patterns
(57,861 reordering schemes) with an average
number of 11.02 non-terminals. For computa-
tional efficiency, any patterns with number of non-
terminal less than 3 and more than 9 are pruned.
This procedure leaves 18,169 syntactic reordering
patterns (22,850 reordering schemes) with a aver-
</bodyText>
<footnote confidence="0.9580525">
1http://fjoch.com/GIZA++.html
2http://www.speech.sri.com/projects/srilm/
</footnote>
<figure confidence="0.9880400625">
T
Sub parse tree
matched with
a pattern
a1
bn
am b1
Source side
sentence
... ...
... ...
w1 w2 ... wp v1 v2 ... vq
w1 w2 ... wp v1 v2 ... vq
... ...
Generated
lattice
</figure>
<equation confidence="0.733343666666667">
v2 ... vq w1 w2 ...
v1
wp
k
w(E0) = α +
i=1
</equation>
<page confidence="0.98731">
23
</page>
<bodyText confidence="0.999836071428572">
age number of 7.6 non-terminals. This pattern set
is used to built the syntactic reordering PBSMT
system without pattern filtering, which here after
we call the ‘unfiltered system’.
Using the tags specified in Table 1, the ex-
tracted syntactic reordering patterns without func-
tional words are filtered out, while only 6,926 syn-
tactic reordering patterns (with 9,572 reordering
schemes) are retained. Thus the pattern set are
reduced by 61.88%, and over half of them are
pruned by the functional word tags. The filtered
pattern set is used to build the syntactic reorder-
ing PBSMT system with pattern filtering, which
we refer as the ‘filtered system’.
</bodyText>
<table confidence="0.9995422">
Type Tag Patterns Percent
ba-const. BA 222 3.20%
LB 97
bei-const. SB 96 2.79%
de-const. (1st) DEC 1662 60.11%
DEG 2501
de-const. (2nd) DER 52 0.75%
de-const. (3rd) DEV 178 2.57%
preposition P 2591 37.41%
excl. ba &amp; bei
</table>
<tableCaption confidence="0.8918955">
Table 2: Statistics on the number of patterns for
each type of functional word
</tableCaption>
<bodyText confidence="0.999821428571429">
Statistics on the patterns with respect to func-
tional word types are shown in Table 2. The num-
ber of patterns for each functional word in the fil-
tered pattern set are illustrated, and percentages of
functional word types are also reported. Note that
some patterns contain more than one kind of func-
tional word, so that the percentages of functional
word types do not sum to one.
As demonstrated in Table 2, the first kind of de-
construction takes up 60.11% of the filtered pat-
tern set, and is the main type of patterns used in
our experiment. This indicates that more than half
of the patterns are closely related to the DE con-
struction examined in (Chang et al., 2009b; Du
&amp; Way, 2010). However, the general preposi-
tion construction (excluding bei and ba) accounts
for 37.41% of the filtered patterns, which implies
that it is also a major source of syntactic reorder-
ing. By contrast, other constructions have much
smaller amount of percentages, so have a minor
impact on our experiments.
</bodyText>
<subsectionHeader confidence="0.999391">
5.2 Word lattice construction
</subsectionHeader>
<bodyText confidence="0.999911833333333">
As specified in section 4, for both unfiltered and
the filtered systems, both the devset and testset
are converted into word lattices with the unfiltered
and filtered syntactic reordering patterns respec-
tively. To avoid a dramatic increase in size of the
lattices, the following constraints are applied: for
each source sentence, the maximum number of re-
ordering schemes is 30, and the maximum span of
a pattern is 30.
For the lattice construction, the base probabil-
ity in (6) and (7) is set to 0.05. The two syntac-
tic reordering PBSMT systems also incorporate
the built-in reordering models (distance-based and
lexical reordering) of Moses, and their weights in
the log-linear model are tuned with respect to the
devsets.
The effects of the pattern filtering by functional
words are also reported in Table 3. For both the
FBIS and NIST sets, the average number of nodes
in word lattices are illustrated before and after pat-
tern filtering. From the table, it is clear that the
pattern filtering procedure dramatically reduces
the input size for the PBSMT system. The reduc-
tion is up to 37.99% for the NIST testset.
</bodyText>
<table confidence="0.9989618">
Data set Unfiltered Filtered Reduced
FBIS dev 183.13 131.38 28.26%
FBIS test 183.68 136.56 25.65%
NIST dev 175.78 115.89 34.07%
NIST test 149.13 92.48 37.99%
</table>
<tableCaption confidence="0.980958">
Table 3: Comparison of the average number of
nodes in word lattices
</tableCaption>
<subsectionHeader confidence="0.977806">
5.3 Results on FBIS set
</subsectionHeader>
<bodyText confidence="0.9998107">
Three systems are compared on the FBIS set:
the baseline PBSMT system, and the syntactic
reordering systems with and without pattern fil-
tering. Since the built-in reordering models of
Moses are enabled, several values of the distor-
tion limit (DL) parameter are chosen to validate
consistency. The evaluation results on the FBIS
set are shown in Table 4.
As shown in Table 4, the syntactic reordering
systems with and without pattern filtering outper-
</bodyText>
<page confidence="0.998204">
24
</page>
<table confidence="0.999611071428571">
System DL BLEU NIST METE
Baseline 0 22.32 6.45 52.51
6 23.67 6.63 54.07
10 24.52 6.66 54.04
12 24.57 6.69 54.31
Unfiltered 0 23.92 6.60 54.30
6 24.57 6.68 54.64
10 24.98 6.71 54.67
12 24.84 6.69 54.65
Filtered 0 23.71 6.60 54.11
6 24.65 6.68 54.61
10 24.87 6.71 54.84
24.91
12 6.7 54.51
</table>
<tableCaption confidence="0.979232">
Table 4: Results on FBIS testset (DL = distortion
limit, METE=METEOR)
</tableCaption>
<bodyText confidence="0.997471971428572">
form the baseline system for each of the distortion
limit parameters in terms of the BLEU, NIST and
METEOR scores (scores in bold face). By con-
trast, the filtered systems has a comparable perfor-
mance with the unfiltered system: for some of the
distortion limits, the filtered systems even outper-
forms the unfiltered system (scores in bold face,
e.g. BLEU and NIST for DL=12, METEOR for
DL=10).
The best performance of the baseline system
is obtained with distortion limit 12 (underlined);
the best performance of the unfiltered system is
achieved with distortion limit 10 (underlined);
while for the filtered system, the best BLEU score
is accomplished with distortion limit 12 (under-
lined), and the best NIST and METEOR scores
are shown with distortion limit 10 (underlined).
Thus the unfiltered system outperforms the base-
line by 0.41 (1.67% relative) BLEU points, 0.02
(0.30% relative) NIST points and 0.36 (0.66%
relative) METEOR points. By contrast, the fil-
tered system outperforms the baseline by 0.34
(1.38% relative) BLEU points, 0.02 (0.30% rel-
ative) NIST points and 0.53 (0.98% relative) ME-
TEOR points.
Compared with the unfiltered system, pattern
filtering with functional words degrades perfor-
mance by 0.07 (0.28% relative) in term of BLEU,
but improves the system by 0.17 (0.31% rela-
tive) in term of METEOR, while the two systems
achieved the same best NIST score.
These results indicates that the filtered system
has a comparable performance with the unfiltered
one on the FBIS set, while both of them outper-
form the baseline.
</bodyText>
<subsectionHeader confidence="0.950224">
5.4 Results on NIST set
</subsectionHeader>
<bodyText confidence="0.9913245">
The evaluation results on the NIST set are illus-
trated in Table 5.
</bodyText>
<table confidence="0.998215411764706">
System DL BLEU NIST METE
Baseline 0 14.43 5.75 45.03
6 15.61 5.88 45.75
10 15.73 5.78 45.27
12 15.89 6.16 45.88
Unfiltered 0 16.77 6.54 47.16
6 17.25 6.67 47.65
10 47.78
17.15 6.64
16.88 6.56
12 47.17
Filtered 0 16.79 6.64 47.67
6 17.55 6.71 48.06
10 6.72 48.15
17.51
17.37
12 6.72 48.08
</table>
<tableCaption confidence="0.969632">
Table 5: Results on NIST testset (DL = distortion
limit, METE=METEOR)
</tableCaption>
<bodyText confidence="0.999659869565218">
From Table 5, the unfiltered system outper-
forms the baseline system for each of the distor-
tion limits in terms of the BLEU, NIST and ME-
TEOR scores (scores in bold face). By contrast,
the filtered system also outperform the unfiltered
system for each of the distortion limits in terms of
the three evaluation methods (scores in bold face).
The best performance of the baseline system
is obtained with distortion limit 12 (underlined),
while the best performance of the unfiltered sys-
tem is obtained with distortion limit 6 for BLEU
and NIST, and 10 for METEOR (underlined). For
the filtered system, the best BLEU score is shown
with distortion limit 6, and the best NIST and ME-
TEOR scores are accomplished with distortion
limit 10 (underlined). Thus the unfiltered system
outperforms the baseline by 1.36 (8.56% relative)
BLEU points, 0.51 (8.28% relative) NIST points
and 1.90 (4.14% relative) METEOR points. By
contrast, the filtered system outperforms the base-
line by 1.66 (10.45% relative) BLEU points, 0.56
(9.52% relative) NIST points and 2.27 (4.95% rel-
ative) METEOR points.
</bodyText>
<page confidence="0.99286">
25
</page>
<bodyText confidence="0.999887333333333">
Compared with the unfiltered system, patterns
with functional words boost the performance by
0.30 (1.74% relative) in term of BLEU, 0.05
(0.75% relative) in term of NIST, and 0.37 (0.77%
relative) in term of METEOR.
These results demonstrate that the pattern filter-
ing improves the syntactic reordering system on
the NIST set, while both of them significantly out-
perform the baseline.
</bodyText>
<subsectionHeader confidence="0.651946">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999996709677419">
Experiments in the previous sections demonstrate
that: 1) the two syntactic reordering systems im-
prove the PBSMT system by providing potential
reorderings obtained from phrase alignments and
parse trees; 2) patterns with functional words play
a major role in the syntactic reordering process,
and filtering the patterns with functional words
maintains or even improves the system perfor-
mance for Chinese–English SMT task. Further-
more, as shown in the previous section, pattern
filtering prunes the whole pattern set by 61.88%
and also reduces the sizes of word lattices by up
to 37.99%, thus the whole syntactic reordering
procedure for the original inputs as well as the
tuning/decoding steps are sped up dramatically,
which make the proposed methods more useful in
the real world, especially for online SMT systems.
From the statistics on the filtered pattern set
in Table 2, we also argue that the first kind
of de-construction and general preposition (ex-
cluding bei and ba) are the main sources of
Chinese–English syntactic reordering. Previous
work (Chang et al., 2009b; Du &amp; Way, 2010)
showed the advantages of dealing with the DE
construction. In our experiments too, even though
all the patterns are automatically extracted from
phrase alignments, these two constructions still
dominate the filtered pattern set. This result con-
firms the effectiveness of previous work on DE
construction, and also highlights the importance
of the general preposition construction in this task.
</bodyText>
<sectionHeader confidence="0.996423" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999970695652174">
Syntactic reordering patterns with functional
words are examined in this paper. The aim is to
exploit these functional words within the syntactic
reordering patterns extracted from phrase align-
ments and parse trees. Three systems are com-
pared: a baseline PBSMT system, a syntactic re-
ordering system with all patterns extracted from a
corpus and a syntactic reordering system with pat-
terns filtered with functional words. Evaluation
results on a medium-sized corpus showed that the
two syntactic reordering systems consistently out-
perform the baseline system. The pattern filtering
with functional words prunes 61.88% of patterns,
but still maintains a comparable performance with
the unfiltered one on the randomly select testset,
and even obtains 1.74% relative improvement on
the NIST 2008 testset.
In future work, the structures of patterns con-
taining functional words will be investigated to
obtain fine-grained analysis on such words in this
task. Furthermore, experiments on larger corpora
as well as on other language pairs will also be car-
ried out to validation our method.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9966452">
This research is supported by Science Foundation
Ireland (Grant 07/CE/I1142) as part of the Centre
for Next Generation Localisation (www.cngl.ie) at
Dublin City University. Thanks to Yanjun Ma for
the sentence-aligned FBIS corpus.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997011285714286">
Yaser Al-Onaizan and Kishore Papineni 2006. Dis-
tortion models for statistical machine translation.
Coling-ACL 2006: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pages 529-536, Sydney,
Australia.
Pi-Chuan Chang, Dan Jurafsky, and Christopher
D.Manning 2009a. Disambiguating DE for
Chinese–English machine translation. Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 215-223, Athens, Greece.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009b. Discriminative
reordering with Chinese grammatical features. Pro-
ceedings of SSST-3: Third Workshop on Syntax and
Structure in Statistical Translation, pages 51-59,
Boulder, CO.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. ACL-2005: 43rd Annual meeting of the
</reference>
<page confidence="0.97984">
26
</page>
<reference confidence="0.998682165048544">
Association for Computational Linguistics, pages
531-540, University of Michigan, Ann Arbor, MI.
Josep M. Crego, and Jos´e B. Mari˜no. 2007. Syntax-
enhanced N-gram-based SMT. MT Summit XI,
pages 111-118, Copenhagen, Denmark.
Jinhua Du and Andy Way. 2010. The Impact of
Source-Side Syntactic Reordering on Hierarchical
Phrase-based SMT. EAMT 2010: 14th Annual Con-
ference of the European Association for Machine
Translation, Saint-Rapha¨el, France.
Jakob Elming. 2008. Syntactic reordering integrated
with phrase-based SMT. Coling 2008: 22nd In-
ternational Conference on Computational Linguis-
tics, Proceedings of the conference, pages 209-216,
Manchester, UK.
Jakob Elming, and Nizar Habash. 2009. Syntac-
tic reordering for English-Arabic phrase-based ma-
chine translation. Proceedings of the EACL 2009
Workhop on Computational Approaches to Semitic
Languages, pages 69-77, Athens, Greece.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. MT Summit XI, pages
215-222, Copenhagen, Denmark.
Jie Jiang, Andy Way, Julie Carson-Berndsen. 2010.
Lattice Score-Based Data Cleaning For Phrase-
Based Statistical Machine Translation. EAMT
2010: 14th Annual Conference of the European As-
sociation for Machine Translation, Saint-Rapha¨el,
France.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007: proceedings of demo and
poster sessions, pp. 177-180, Prague, Czech Repub-
lic.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. ACL 2007: proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 720-727, Prague, Czech
Republic.
Xiaoyi Ma. 2006. Champollion: A Robust Paral-
lel Text Sentence Aligner. LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation, pp.489-492, Genova, Italy.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. ACL-2003:
41st Annual meeting of the Association for Compu-
tational Linguistics, pp. 160-167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation. ACL-2002:
40th Annual meeting of the Association for Compu-
tational Linguistics, pp.311-318, Philadelphia, PA.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. Coling-ACL 2006:
Proceedings ofthe 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 433-440, Sydney, Australia.
Felipe S´anchez-Martinez and Andy Way. 2009.
Marker-based filtering of bilingual phrase pairs for
SMT. EAMT-2009: Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation, pages 144-151, Barcelona,
Spain.
Chao Wang, Michael Collins, and Philipp Koehn.
2007a. Chinese syntactic reordering for statistical
machine translation. EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
737-745, Prague, Czech Republic.
Fei Xia, and Michael McCord 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. Coling 2004: 20th International
Conference on Computational Linguistics, pages
508-514, University of Geneva, Switzerland.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2), pages 207-238.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
Proceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP, pages 333-341, Suntec, Singa-
pore.
Yuqi Zhang, Richard Zens, and Hermann Ney 2007a.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statisti-
cal machine translation. SSST, NAACL-HLT-2007
AMTA Workshop on Syntax and Structure in Statis-
tical Translation, pages 1-8, Rochester, NY.
Yuqi Zhang, Richard Zens, and Hermann Ney 2007b.
Improved chunk-level reordering for statistical ma-
chine translation. IWSLT 2007: International Work-
shop on Spoken Language Translation, pages 21-28,
Trento, Italy.
</reference>
<page confidence="0.9988">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.716360">
<title confidence="0.991715">Source-side Syntactic Reordering Patterns with Functional Words Improved Phrase-based SMT</title>
<author confidence="0.999606">Jie Jiang</author>
<author confidence="0.999606">Jinhua Du</author>
<author confidence="0.999606">Andy Way</author>
<affiliation confidence="0.997462">CNGL, School of Computing, Dublin City University</affiliation>
<abstract confidence="0.990176428571429">Inspired by previous source-side syntactic reordering methods for SMT, this paper focuses on using automatically learned syntactic reordering patterns with functional words which indicate structural reorderings between the source and target language. This approach takes advantage of phrase alignments and source-side parse trees for pattern extraction, and then filters out those patterns without functional words. Word lattices transformed by the generated patterns are fed into PB- SMT systems to incorporate potential reorderings from the inputs. Experiments are carried out on a medium-sized corpus for a Chinese–English SMT task. The proposed method outperforms the baseline system by 1.38% relative on a randomly selected testset and 10.45% relative on the NIST 2008 testset in terms of BLEU score. Furthermore, a system with just 61.88% of the patterns filtered by functional words obtains a comparable performance with the unfiltered one on the randomly selected testset, and achieves 1.74% relative improvements on the NIST 2008 testset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation. Coling-ACL</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--536</pages>
<location>Sydney, Australia.</location>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni 2006. Distortion models for statistical machine translation. Coling-ACL 2006: Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 529-536, Sydney, Australia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Pi-Chuan Chang</author>
<author>Dan Jurafsky</author>
</authors>
<title>and Christopher D.Manning 2009a. Disambiguating DE for Chinese–English machine translation.</title>
<booktitle>Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>215--223</pages>
<location>Athens, Greece.</location>
<marker>Chang, Jurafsky, </marker>
<rawString>Pi-Chuan Chang, Dan Jurafsky, and Christopher D.Manning 2009a. Disambiguating DE for Chinese–English machine translation. Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 215-223, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Discriminative reordering with Chinese grammatical features.</title>
<date>2009</date>
<booktitle>Proceedings of SSST-3: Third Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>51--59</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="2373" citStr="Chang et al., 2009" startWordPosition="360" endWordPosition="363">d testset before being fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et al., 2005; Habash, 2007). Because reordered source sentence cannot be undone by the SMT decoders (AlOnaizan et al., 2006), which implies a systematic error for this approach, classifiers (Chang et al., 2009b; Du &amp; Way, 2010) are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese). On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from syntactic structures for N-gram-based SMT is presented in (Crego et al., 2007). In (Zhang et al., 2007a; Zhang et al., 2007b), chunks and POS tags are used to extract reordering rule</context>
<context position="3599" citStr="Chang et al., 2009" startWordPosition="552" endWordPosition="555"> the generated word lattices are weighted by language models and reordering models. Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li et al., 2007). Furthermore, (Elming, 2008; Elm19 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19–27, COLING 2010, Beijing, August 2010. ing, 2009) uses syntactic rules to score the output word order, both on English–Danish and English– Arabic tasks. Syntactic reordering information is also considered as an extra feature to improve PBSMT in (Chang et al., 2009b) for the Chinese– English task. These results confirmed the effectiveness of syntactic reorderings. However, for the particular case of Chinese source inputs, although the DE construction has been addressed for both PBSMT and HPBSMT systems in (Chang et al., 2009b; Du &amp; Way, 2010), as indicated by (Wang et al., 2007a), there are still lots of unexamined structures that imply source-side reordering, especially in the nondeterministic approach. As specified in (Xue, 2005), these include the bei-construction, baconstruction, three kinds of de-construction (including DE construction) and general</context>
<context position="6903" citStr="Chang et al., 2009" startWordPosition="1066" endWordPosition="1069">as follows: in section 2 we describe the extraction process of syntactic reordering patterns, including the lattice scoring approach and the extraction procedures. Then section 3 presents the filtering process used to obtain patterns with functional words. After that, section 4 shows the generation of word lattices with patterns, and experimental setup and results included related discussion are presented in section 5. Finally, we give our conclusion and avenues for future work in section 6. 2 Syntactic reordering patterns extraction Instead of top-down approaches such as (Wang et al., 2007a; Chang et al., 2009a), we use a bottom-up approach similar to (Xia et al., 2004; Crego et al., 2007) to extract syntactic reordering patterns from non-monotonic phrase alignments and source-side parse trees. The following steps are carried out to extract syntactic reordering patterns: 1) the lattice scoring approach proposed in (Jiang et al., 2010) is used to obtain phrase alignments from the training corpus; 2) reordering regions from the non-monotonic phrase alignments are used to identify minimum treelets for pattern extraction; and 3) the treelets are transformed into syntactic reordering patterns which are </context>
<context position="13299" citStr="Chang et al., 2009" startWordPosition="2171" endWordPosition="2174">2)), each of them weighted as in (4): count{reorderings of O in P} w(O,P) = count{reorderings of P} (4) Generally, a syntactic reordering pattern is expressed as in (5): P = {tree |preo |O1, w1, · · · , On, wn} (5) where tree is the tree structures of the pattern, preo is the reordering probability, Oi and wi are the reordering schemes and weights (1 ≤ i ≤ n). 3 Patterns with functional words Some of the patterns extracted may not benefit the final system since the extraction process is controlled by phrase alignments rather than syntactic knowledge. Inspired by the study of DE constructions (Chang et al., 2009a; Du &amp; Way, 2010), we assume that syntactic reorderings are indicated by functional words for the Chinese– English task. To incorporate the knowledge of functional words into the extracted patterns, instead of directly specifying the syntactic structure from the linguistic aspects, we use functional word tags to filter the extracted patterns. In this case, we assume that all patterns containing functional words tend to produce meaningful syntactic reorderings. Thus the filtered patterns carry the reordering information from the phrase alignments as well as the linguistic knowledge. Thus the n</context>
<context position="14714" citStr="Chang et al., 2009" startWordPosition="2405" endWordPosition="2408">hown in Table 1, which come from (Xue, 2005). We choose them as functional words because normally they imply word reorders between Chinese and English sentence pairs. Tag Description BA ba-construction DEC de (1st kind) in a relative-clause DEG associative de (1st kind) DER de (2nd kind) in V-de const. &amp; V-de-R DEV de (3rd kind) before VP LB bei in long bei-construction P preposition excluding bei and ba SB bei in short bei-construction Table 1: Syntactic reordering tags for functional words Note that there are three kinds of deconstructions, but only the first kind is the DE construction in (Chang et al., 2009a; Du &amp; Way, 2010). After the filtering process, both the unfiltered pattern set and the filtered one are used to build different syntactic reordering PBSMT systems for comparison purpose. 22 4 Word lattice construction Both the devset and testset are transformed into word lattices by the extracted patterns to incorporate potential reorderings. Figure 2 illustrates this process: treelet T′ is matched with a pattern, then its leaf nodes {a1, · · · am} ∈ LA (spanning {w1, · · · , wp}) are swapped with leaf nodes {b1, · · · , bn} ∈ LB (spanning {v1, · · · , vq}) on the generated paths in the word</context>
<context position="19999" citStr="Chang et al., 2009" startWordPosition="3339" endWordPosition="3342">onal word types are shown in Table 2. The number of patterns for each functional word in the filtered pattern set are illustrated, and percentages of functional word types are also reported. Note that some patterns contain more than one kind of functional word, so that the percentages of functional word types do not sum to one. As demonstrated in Table 2, the first kind of deconstruction takes up 60.11% of the filtered pattern set, and is the main type of patterns used in our experiment. This indicates that more than half of the patterns are closely related to the DE construction examined in (Chang et al., 2009b; Du &amp; Way, 2010). However, the general preposition construction (excluding bei and ba) accounts for 37.41% of the filtered patterns, which implies that it is also a major source of syntactic reordering. By contrast, other constructions have much smaller amount of percentages, so have a minor impact on our experiments. 5.2 Word lattice construction As specified in section 4, for both unfiltered and the filtered systems, both the devset and testset are converted into word lattices with the unfiltered and filtered syntactic reordering patterns respectively. To avoid a dramatic increase in size </context>
<context position="27072" citStr="Chang et al., 2009" startWordPosition="4510" endWordPosition="4513">ious section, pattern filtering prunes the whole pattern set by 61.88% and also reduces the sizes of word lattices by up to 37.99%, thus the whole syntactic reordering procedure for the original inputs as well as the tuning/decoding steps are sped up dramatically, which make the proposed methods more useful in the real world, especially for online SMT systems. From the statistics on the filtered pattern set in Table 2, we also argue that the first kind of de-construction and general preposition (excluding bei and ba) are the main sources of Chinese–English syntactic reordering. Previous work (Chang et al., 2009b; Du &amp; Way, 2010) showed the advantages of dealing with the DE construction. In our experiments too, even though all the patterns are automatically extracted from phrase alignments, these two constructions still dominate the filtered pattern set. This result confirms the effectiveness of previous work on DE construction, and also highlights the importance of the general preposition construction in this task. 6 Conclusion and future work Syntactic reordering patterns with functional words are examined in this paper. The aim is to exploit these functional words within the syntactic reordering p</context>
</contexts>
<marker>Chang, Tseng, Jurafsky, Manning, 2009</marker>
<rawString>Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2009b. Discriminative reordering with Chinese grammatical features. Proceedings of SSST-3: Third Workshop on Syntax and Structure in Statistical Translation, pages 51-59, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>ACL-2005: 43rd Annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>531--540</pages>
<institution>University of Michigan,</institution>
<location>Ann Arbor, MI.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. ACL-2005: 43rd Annual meeting of the Association for Computational Linguistics, pages 531-540, University of Michigan, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<date>2007</date>
<booktitle>Syntaxenhanced N-gram-based SMT. MT Summit XI,</booktitle>
<pages>111--118</pages>
<location>Copenhagen, Denmark.</location>
<marker>Crego, Mari˜no, 2007</marker>
<rawString>Josep M. Crego, and Jos´e B. Mari˜no. 2007. Syntaxenhanced N-gram-based SMT. MT Summit XI, pages 111-118, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinhua Du</author>
<author>Andy Way</author>
</authors>
<title>The Impact of Source-Side Syntactic Reordering on Hierarchical Phrase-based</title>
<date>2010</date>
<booktitle>SMT. EAMT 2010: 14th Annual Conference of the European Association for Machine Translation,</booktitle>
<location>Saint-Rapha¨el, France.</location>
<contexts>
<context position="2391" citStr="Du &amp; Way, 2010" startWordPosition="364" endWordPosition="367">g fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et al., 2005; Habash, 2007). Because reordered source sentence cannot be undone by the SMT decoders (AlOnaizan et al., 2006), which implies a systematic error for this approach, classifiers (Chang et al., 2009b; Du &amp; Way, 2010) are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese). On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from syntactic structures for N-gram-based SMT is presented in (Crego et al., 2007). In (Zhang et al., 2007a; Zhang et al., 2007b), chunks and POS tags are used to extract reordering rules, while the gener</context>
<context position="3882" citStr="Du &amp; Way, 2010" startWordPosition="597" endWordPosition="600">rkshop on Syntax and Structure in Statistical Translation, pages 19–27, COLING 2010, Beijing, August 2010. ing, 2009) uses syntactic rules to score the output word order, both on English–Danish and English– Arabic tasks. Syntactic reordering information is also considered as an extra feature to improve PBSMT in (Chang et al., 2009b) for the Chinese– English task. These results confirmed the effectiveness of syntactic reorderings. However, for the particular case of Chinese source inputs, although the DE construction has been addressed for both PBSMT and HPBSMT systems in (Chang et al., 2009b; Du &amp; Way, 2010), as indicated by (Wang et al., 2007a), there are still lots of unexamined structures that imply source-side reordering, especially in the nondeterministic approach. As specified in (Xue, 2005), these include the bei-construction, baconstruction, three kinds of de-construction (including DE construction) and general preposition constructions. Such structures are referred with functional words in this paper, and all the constructions can be identified by their corresponding tags in the Penn Chinese TreeBank. It is interesting to investigate these functional words for the syntactic reordering ta</context>
<context position="13317" citStr="Du &amp; Way, 2010" startWordPosition="2175" endWordPosition="2178">ghted as in (4): count{reorderings of O in P} w(O,P) = count{reorderings of P} (4) Generally, a syntactic reordering pattern is expressed as in (5): P = {tree |preo |O1, w1, · · · , On, wn} (5) where tree is the tree structures of the pattern, preo is the reordering probability, Oi and wi are the reordering schemes and weights (1 ≤ i ≤ n). 3 Patterns with functional words Some of the patterns extracted may not benefit the final system since the extraction process is controlled by phrase alignments rather than syntactic knowledge. Inspired by the study of DE constructions (Chang et al., 2009a; Du &amp; Way, 2010), we assume that syntactic reorderings are indicated by functional words for the Chinese– English task. To incorporate the knowledge of functional words into the extracted patterns, instead of directly specifying the syntactic structure from the linguistic aspects, we use functional word tags to filter the extracted patterns. In this case, we assume that all patterns containing functional words tend to produce meaningful syntactic reorderings. Thus the filtered patterns carry the reordering information from the phrase alignments as well as the linguistic knowledge. Thus the noise produced in p</context>
<context position="14732" citStr="Du &amp; Way, 2010" startWordPosition="2409" endWordPosition="2412">h come from (Xue, 2005). We choose them as functional words because normally they imply word reorders between Chinese and English sentence pairs. Tag Description BA ba-construction DEC de (1st kind) in a relative-clause DEG associative de (1st kind) DER de (2nd kind) in V-de const. &amp; V-de-R DEV de (3rd kind) before VP LB bei in long bei-construction P preposition excluding bei and ba SB bei in short bei-construction Table 1: Syntactic reordering tags for functional words Note that there are three kinds of deconstructions, but only the first kind is the DE construction in (Chang et al., 2009a; Du &amp; Way, 2010). After the filtering process, both the unfiltered pattern set and the filtered one are used to build different syntactic reordering PBSMT systems for comparison purpose. 22 4 Word lattice construction Both the devset and testset are transformed into word lattices by the extracted patterns to incorporate potential reorderings. Figure 2 illustrates this process: treelet T′ is matched with a pattern, then its leaf nodes {a1, · · · am} ∈ LA (spanning {w1, · · · , wp}) are swapped with leaf nodes {b1, · · · , bn} ∈ LB (spanning {v1, · · · , vq}) on the generated paths in the word lattice. Figure 2</context>
<context position="20017" citStr="Du &amp; Way, 2010" startWordPosition="3343" endWordPosition="3346">hown in Table 2. The number of patterns for each functional word in the filtered pattern set are illustrated, and percentages of functional word types are also reported. Note that some patterns contain more than one kind of functional word, so that the percentages of functional word types do not sum to one. As demonstrated in Table 2, the first kind of deconstruction takes up 60.11% of the filtered pattern set, and is the main type of patterns used in our experiment. This indicates that more than half of the patterns are closely related to the DE construction examined in (Chang et al., 2009b; Du &amp; Way, 2010). However, the general preposition construction (excluding bei and ba) accounts for 37.41% of the filtered patterns, which implies that it is also a major source of syntactic reordering. By contrast, other constructions have much smaller amount of percentages, so have a minor impact on our experiments. 5.2 Word lattice construction As specified in section 4, for both unfiltered and the filtered systems, both the devset and testset are converted into word lattices with the unfiltered and filtered syntactic reordering patterns respectively. To avoid a dramatic increase in size of the lattices, t</context>
<context position="27090" citStr="Du &amp; Way, 2010" startWordPosition="4514" endWordPosition="4517"> filtering prunes the whole pattern set by 61.88% and also reduces the sizes of word lattices by up to 37.99%, thus the whole syntactic reordering procedure for the original inputs as well as the tuning/decoding steps are sped up dramatically, which make the proposed methods more useful in the real world, especially for online SMT systems. From the statistics on the filtered pattern set in Table 2, we also argue that the first kind of de-construction and general preposition (excluding bei and ba) are the main sources of Chinese–English syntactic reordering. Previous work (Chang et al., 2009b; Du &amp; Way, 2010) showed the advantages of dealing with the DE construction. In our experiments too, even though all the patterns are automatically extracted from phrase alignments, these two constructions still dominate the filtered pattern set. This result confirms the effectiveness of previous work on DE construction, and also highlights the importance of the general preposition construction in this task. 6 Conclusion and future work Syntactic reordering patterns with functional words are examined in this paper. The aim is to exploit these functional words within the syntactic reordering patterns extracted </context>
</contexts>
<marker>Du, Way, 2010</marker>
<rawString>Jinhua Du and Andy Way. 2010. The Impact of Source-Side Syntactic Reordering on Hierarchical Phrase-based SMT. EAMT 2010: 14th Annual Conference of the European Association for Machine Translation, Saint-Rapha¨el, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
</authors>
<title>Syntactic reordering integrated with phrase-based SMT. Coling</title>
<date>2008</date>
<booktitle>22nd International Conference on Computational Linguistics, Proceedings of the conference,</booktitle>
<pages>209--216</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="1567" citStr="Elming, 2008" startWordPosition="230" endWordPosition="231">and 10.45% relative on the NIST 2008 testset in terms of BLEU score. Furthermore, a system with just 61.88% of the patterns filtered by functional words obtains a comparable performance with the unfiltered one on the randomly selected testset, and achieves 1.74% relative improvements on the NIST 2008 testset. 1 Introduction Previous work has shown that the problem of structural differences between language pairs in SMT can be alleviated by source-side syntactic reordering. Taking account for the integration with SMT systems, these methods can be divided into two different kinds of approaches (Elming, 2008): the deterministic reordering and the nondeterministic reordering approach. To carry out the deterministic approach, syntactic reordering is performed uniformly on the training, devset and testset before being fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et </context>
<context position="3227" citStr="Elming, 2008" startWordPosition="496" endWordPosition="497">e appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from syntactic structures for N-gram-based SMT is presented in (Crego et al., 2007). In (Zhang et al., 2007a; Zhang et al., 2007b), chunks and POS tags are used to extract reordering rules, while the generated word lattices are weighted by language models and reordering models. Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li et al., 2007). Furthermore, (Elming, 2008; Elm19 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19–27, COLING 2010, Beijing, August 2010. ing, 2009) uses syntactic rules to score the output word order, both on English–Danish and English– Arabic tasks. Syntactic reordering information is also considered as an extra feature to improve PBSMT in (Chang et al., 2009b) for the Chinese– English task. These results confirmed the effectiveness of syntactic reorderings. However, for the particular case of Chinese source inputs, although the DE construction has been addressed for both PBSMT and </context>
</contexts>
<marker>Elming, 2008</marker>
<rawString>Jakob Elming. 2008. Syntactic reordering integrated with phrase-based SMT. Coling 2008: 22nd International Conference on Computational Linguistics, Proceedings of the conference, pages 209-216, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Nizar Habash</author>
</authors>
<title>Syntactic reordering for English-Arabic phrase-based machine translation.</title>
<date>2009</date>
<booktitle>Proceedings of the EACL 2009 Workhop on Computational Approaches to Semitic Languages,</booktitle>
<pages>69--77</pages>
<location>Athens, Greece.</location>
<marker>Elming, Habash, 2009</marker>
<rawString>Jakob Elming, and Nizar Habash. 2009. Syntactic reordering for English-Arabic phrase-based machine translation. Proceedings of the EACL 2009 Workhop on Computational Approaches to Semitic Languages, pages 69-77, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Syntactic preprocessing for statistical machine translation. MT Summit XI,</title>
<date>2007</date>
<pages>215--222</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2191" citStr="Habash, 2007" startWordPosition="332" endWordPosition="333">ministic reordering and the nondeterministic reordering approach. To carry out the deterministic approach, syntactic reordering is performed uniformly on the training, devset and testset before being fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et al., 2005; Habash, 2007). Because reordered source sentence cannot be undone by the SMT decoders (AlOnaizan et al., 2006), which implies a systematic error for this approach, classifiers (Chang et al., 2009b; Du &amp; Way, 2010) are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese). On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from </context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. MT Summit XI, pages 215-222, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Jiang</author>
<author>Andy Way</author>
<author>Julie Carson-Berndsen</author>
</authors>
<title>Lattice Score-Based Data Cleaning For PhraseBased Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>EAMT 2010: 14th Annual Conference of the European Association for Machine Translation,</booktitle>
<location>Saint-Rapha¨el, France.</location>
<contexts>
<context position="5663" citStr="Jiang et al., 2010" startWordPosition="873" endWordPosition="876">is to exploit these functional words for source-side syntactic reordering of Chinese–English SMT in the non-deterministic approach. Our assumption is that syntactic reordering patterns with functional words are the most effective ones, and others can be pruned for both speed and performance. To validate this assumption, three systems are compared in this paper: a baseline PBSMT system, a syntactic reordering system with all patterns extracted from a corpus, and a syntactic reordering system with patterns filtered with functional words. To accomplish this, firstly the lattice scoring approach (Jiang et al., 2010) is utilized to discover non-monotonic phrase alignments, and then syntactic reordering patterns are extracted from source-side parse trees. After that, functional word tags specified in (Xue, 2005) are adopted to perform pattern filtering. Finally, both the unfiltered pattern set and the filtered one are used to transform inputs into word lattices to present potential reorderings for improving PBSMT system. A comparison between the three systems is carried out to examine the performance of syntactic reordering as well as the usefulness of functional words for pattern filtering. The rest of th</context>
<context position="7234" citStr="Jiang et al., 2010" startWordPosition="1117" endWordPosition="1120">, and experimental setup and results included related discussion are presented in section 5. Finally, we give our conclusion and avenues for future work in section 6. 2 Syntactic reordering patterns extraction Instead of top-down approaches such as (Wang et al., 2007a; Chang et al., 2009a), we use a bottom-up approach similar to (Xia et al., 2004; Crego et al., 2007) to extract syntactic reordering patterns from non-monotonic phrase alignments and source-side parse trees. The following steps are carried out to extract syntactic reordering patterns: 1) the lattice scoring approach proposed in (Jiang et al., 2010) is used to obtain phrase alignments from the training corpus; 2) reordering regions from the non-monotonic phrase alignments are used to identify minimum treelets for pattern extraction; and 3) the treelets are transformed into syntactic reordering patterns which are then weighted by their occurrences in the training corpus. Details of each of these steps are presented in the rest of this section. 2.1 Lattice scoring for phrase alignments The lattice scoring approach is proposed in (Jiang et al., 2010) for the SMT data cleaning task. 20 To clean the training corpus, word alignments are used t</context>
</contexts>
<marker>Jiang, Way, Carson-Berndsen, 2010</marker>
<rawString>Jie Jiang, Andy Way, Julie Carson-Berndsen. 2010. Lattice Score-Based Data Cleaning For PhraseBased Statistical Machine Translation. EAMT 2010: 14th Annual Conference of the European Association for Machine Translation, Saint-Rapha¨el, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. ACL 2007: proceedings of demo and poster sessions,</title>
<date>2007</date>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="16708" citStr="Koehn et al., 2007" startWordPosition="2775" endWordPosition="2778"> with the same root lattice node share equal probabilities in formula (6) and (7). 5 Experiments and results We conducted our experiments on a medium-sized corpus FBIS (a multilingual paragraph-aligned corpus with LDC resource number LDC2003E14) for the Chinese–English SMT task. The Champollion aligner (Ma, 2006) is utilized to perform sentence alignment. A total number of 256,911 sentence pairs are obtained, while 2,000 pairs for devset and 2,000 pairs for testset are randomly selected, which we call FBIS set. The rest of the data is used as the training corpus. The baseline system is Moses (Koehn et al., 2007), and GIZA++1 is used to perform word alignment. Minimum error rate training (MERT) (Och, 2003) is carried out for tuning. A 5-gram language model built via SRILM2 is used for all the experiments in this paper. Experiments results are reported on two different sets: the FBIS set and the NIST set. For the NIST set, the NIST 2005 testset (1,082 sentences) is used as the devset, and the NIST 2008 testset (1,357 sentences) is used as the testset. The FBIS set contains only one reference translation for both devset and testset, while NIST set has four references. 5.1 Pattern extraction and filterin</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL 2007: proceedings of demo and poster sessions, pp. 177-180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Minghui Li</author>
<author>Yi Guan</author>
</authors>
<title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>ACL 2007: proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>720--727</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3199" citStr="Li et al., 2007" startWordPosition="491" endWordPosition="494">cisions to the decoders to choose appropriate source-side reorderings. This is more flexible because both the original and reordered source sentences are presented in the inputs. Word lattices generated from syntactic structures for N-gram-based SMT is presented in (Crego et al., 2007). In (Zhang et al., 2007a; Zhang et al., 2007b), chunks and POS tags are used to extract reordering rules, while the generated word lattices are weighted by language models and reordering models. Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li et al., 2007). Furthermore, (Elming, 2008; Elm19 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19–27, COLING 2010, Beijing, August 2010. ing, 2009) uses syntactic rules to score the output word order, both on English–Danish and English– Arabic tasks. Syntactic reordering information is also considered as an extra feature to improve PBSMT in (Chang et al., 2009b) for the Chinese– English task. These results confirmed the effectiveness of syntactic reorderings. However, for the particular case of Chinese source inputs, although the DE construction has been a</context>
</contexts>
<marker>Li, Zhang, Li, Zhou, Li, Guan, 2007</marker>
<rawString>Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li, and Yi Guan 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. ACL 2007: proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 720-727, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
</authors>
<title>Champollion: A Robust Parallel Text Sentence Aligner. LREC</title>
<date>2006</date>
<booktitle>Fifth International Conference on Language Resources and Evaluation,</booktitle>
<pages>489--492</pages>
<location>Genova, Italy.</location>
<contexts>
<context position="16403" citStr="Ma, 2006" startWordPosition="2723" endWordPosition="2724">ability to avoid E0 being equal to zero. Suppose {Es, · · · , Es+r−1} are generated by r reordering schemes of Pi, then Ej is weighted as in (7): w(Ej) = (1 −α) k∗preo(Pi)∗ ws−j+1(Pi) �r t=1 wt(Pi) (7) where wt(Pi) is the reordering scheme in formula (5), and s &lt;= j &lt; s + r. Reordering patterns with the same root lattice node share equal probabilities in formula (6) and (7). 5 Experiments and results We conducted our experiments on a medium-sized corpus FBIS (a multilingual paragraph-aligned corpus with LDC resource number LDC2003E14) for the Chinese–English SMT task. The Champollion aligner (Ma, 2006) is utilized to perform sentence alignment. A total number of 256,911 sentence pairs are obtained, while 2,000 pairs for devset and 2,000 pairs for testset are randomly selected, which we call FBIS set. The rest of the data is used as the training corpus. The baseline system is Moses (Koehn et al., 2007), and GIZA++1 is used to perform word alignment. Minimum error rate training (MERT) (Och, 2003) is carried out for tuning. A 5-gram language model built via SRILM2 is used for all the experiments in this paper. Experiments results are reported on two different sets: the FBIS set and the NIST se</context>
</contexts>
<marker>Ma, 2006</marker>
<rawString>Xiaoyi Ma. 2006. Champollion: A Robust Parallel Text Sentence Aligner. LREC 2006: Fifth International Conference on Language Resources and Evaluation, pp.489-492, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training</title>
<date>2003</date>
<booktitle>in Statistical Machine Translation. ACL-2003: 41st Annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="16803" citStr="Och, 2003" startWordPosition="2792" endWordPosition="2793">ults We conducted our experiments on a medium-sized corpus FBIS (a multilingual paragraph-aligned corpus with LDC resource number LDC2003E14) for the Chinese–English SMT task. The Champollion aligner (Ma, 2006) is utilized to perform sentence alignment. A total number of 256,911 sentence pairs are obtained, while 2,000 pairs for devset and 2,000 pairs for testset are randomly selected, which we call FBIS set. The rest of the data is used as the training corpus. The baseline system is Moses (Koehn et al., 2007), and GIZA++1 is used to perform word alignment. Minimum error rate training (MERT) (Och, 2003) is carried out for tuning. A 5-gram language model built via SRILM2 is used for all the experiments in this paper. Experiments results are reported on two different sets: the FBIS set and the NIST set. For the NIST set, the NIST 2005 testset (1,082 sentences) is used as the devset, and the NIST 2008 testset (1,357 sentences) is used as the testset. The FBIS set contains only one reference translation for both devset and testset, while NIST set has four references. 5.1 Pattern extraction and filtering with functional words The lattice scoring approach is carried out with the same baseline syst</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. ACL-2003: 41st Annual meeting of the Association for Computational Linguistics, pp. 160-167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method For Automatic Evaluation</title>
<date>2002</date>
<booktitle>of Machine Translation. ACL-2002: 40th Annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7934" citStr="Papineni et al., 2002" startWordPosition="1231" endWordPosition="1234">egions from the non-monotonic phrase alignments are used to identify minimum treelets for pattern extraction; and 3) the treelets are transformed into syntactic reordering patterns which are then weighted by their occurrences in the training corpus. Details of each of these steps are presented in the rest of this section. 2.1 Lattice scoring for phrase alignments The lattice scoring approach is proposed in (Jiang et al., 2010) for the SMT data cleaning task. 20 To clean the training corpus, word alignments are used to obtain approximate decoding results, which are then used to calculate BLEU (Papineni et al., 2002) scores to filter out low-scoring sentences pairs. The following steps are taken in the lattice scoring approach: 1) train an initial PBSMT model; 2) collect anchor pairs containing source and target phrase positions from word alignments generated in the training phase; 3) build source-side lattices from the anchor pairs and the translation model; 4) search on the sourceside lattices to obtain approximate decoding results; 5) calculate BLEU scores for the purpose of data cleaning. Note that the source-side lattices in step 3 come from anchor pairs, so each edge in the lattices contain both the</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method For Automatic Evaluation of Machine Translation. ACL-2002: 40th Annual meeting of the Association for Computational Linguistics, pp.311-318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation. Coling-ACL</title>
<date>2006</date>
<booktitle>Proceedings ofthe 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia.</location>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. Coling-ACL 2006: Proceedings ofthe 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433-440, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felipe S´anchez-Martinez</author>
<author>Andy Way</author>
</authors>
<title>Marker-based filtering of bilingual phrase pairs for SMT.</title>
<date>2009</date>
<booktitle>EAMT-2009: Proceedings of the 13th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>144--151</pages>
<location>Barcelona,</location>
<marker>S´anchez-Martinez, Way, 2009</marker>
<rawString>Felipe S´anchez-Martinez and Andy Way. 2009. Marker-based filtering of bilingual phrase pairs for SMT. EAMT-2009: Proceedings of the 13th Annual Conference of the European Association for Machine Translation, pages 144-151, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>EMNLP-CoNLL-2007: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>737--745</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2080" citStr="Wang et al., 2007" startWordPosition="312" endWordPosition="315">ion with SMT systems, these methods can be divided into two different kinds of approaches (Elming, 2008): the deterministic reordering and the nondeterministic reordering approach. To carry out the deterministic approach, syntactic reordering is performed uniformly on the training, devset and testset before being fed into the SMT systems, so that only the reordered source sentences are dealt with while building during the SMT system. In this case, most work is focused on methods to extract and to apply syntactic reordering patterns which come from manually created rules (Collins et al., 2005; Wang et al., 2007a), or via an automatic extraction process taking advantage of parse trees (Collins et al., 2005; Habash, 2007). Because reordered source sentence cannot be undone by the SMT decoders (AlOnaizan et al., 2006), which implies a systematic error for this approach, classifiers (Chang et al., 2009b; Du &amp; Way, 2010) are utilized to obtain high-performance reordering for some specialized syntactic structures (e.g. DE construction in Chinese). On the other hand, the non-deterministic approach leaves the decisions to the decoders to choose appropriate source-side reorderings. This is more flexible beca</context>
<context position="3918" citStr="Wang et al., 2007" startWordPosition="604" endWordPosition="607">Statistical Translation, pages 19–27, COLING 2010, Beijing, August 2010. ing, 2009) uses syntactic rules to score the output word order, both on English–Danish and English– Arabic tasks. Syntactic reordering information is also considered as an extra feature to improve PBSMT in (Chang et al., 2009b) for the Chinese– English task. These results confirmed the effectiveness of syntactic reorderings. However, for the particular case of Chinese source inputs, although the DE construction has been addressed for both PBSMT and HPBSMT systems in (Chang et al., 2009b; Du &amp; Way, 2010), as indicated by (Wang et al., 2007a), there are still lots of unexamined structures that imply source-side reordering, especially in the nondeterministic approach. As specified in (Xue, 2005), these include the bei-construction, baconstruction, three kinds of de-construction (including DE construction) and general preposition constructions. Such structures are referred with functional words in this paper, and all the constructions can be identified by their corresponding tags in the Penn Chinese TreeBank. It is interesting to investigate these functional words for the syntactic reordering task since most of them tend to produc</context>
<context position="6882" citStr="Wang et al., 2007" startWordPosition="1062" endWordPosition="1065"> paper is organized as follows: in section 2 we describe the extraction process of syntactic reordering patterns, including the lattice scoring approach and the extraction procedures. Then section 3 presents the filtering process used to obtain patterns with functional words. After that, section 4 shows the generation of word lattices with patterns, and experimental setup and results included related discussion are presented in section 5. Finally, we give our conclusion and avenues for future work in section 6. 2 Syntactic reordering patterns extraction Instead of top-down approaches such as (Wang et al., 2007a; Chang et al., 2009a), we use a bottom-up approach similar to (Xia et al., 2004; Crego et al., 2007) to extract syntactic reordering patterns from non-monotonic phrase alignments and source-side parse trees. The following steps are carried out to extract syntactic reordering patterns: 1) the lattice scoring approach proposed in (Jiang et al., 2010) is used to obtain phrase alignments from the training corpus; 2) reordering regions from the non-monotonic phrase alignments are used to identify minimum treelets for pattern extraction; and 3) the treelets are transformed into syntactic reorderin</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007a. Chinese syntactic reordering for statistical machine translation. EMNLP-CoNLL-2007: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 737-745, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>Coling 2004: 20th International Conference on Computational Linguistics,</booktitle>
<pages>508--514</pages>
<institution>University of Geneva, Switzerland.</institution>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia, and Michael McCord 2004. Improving a statistical MT system with automatically learned rewrite patterns. Coling 2004: 20th International Conference on Computational Linguistics, pages 508-514, University of Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<pages>207--238</pages>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2), pages 207-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase-based statistical machine translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP,</booktitle>
<pages>333--341</pages>
<location>Suntec, Singapore.</location>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och, and Hermann Ney. 2002. Phrase-based statistical machine translation. Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pages 333-341, Suntec, Singapore.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>2007a. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation.</title>
<booktitle>SSST, NAACL-HLT-2007 AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>1--8</pages>
<location>Rochester, NY.</location>
<marker>Zhang, Zens, Ney, </marker>
<rawString>Yuqi Zhang, Richard Zens, and Hermann Ney 2007a. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. SSST, NAACL-HLT-2007 AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1-8, Rochester, NY.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yuqi Zhang</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>2007b. Improved chunk-level reordering for statistical machine translation.</title>
<booktitle>IWSLT 2007: International Workshop on Spoken Language Translation,</booktitle>
<pages>21--28</pages>
<location>Trento, Italy.</location>
<marker>Zhang, Zens, Ney, </marker>
<rawString>Yuqi Zhang, Richard Zens, and Hermann Ney 2007b. Improved chunk-level reordering for statistical machine translation. IWSLT 2007: International Workshop on Spoken Language Translation, pages 21-28, Trento, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>