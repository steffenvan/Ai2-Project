<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038600">
<title confidence="0.9731775">
SemEval-2007 Task 11: English Lexical Sample Task
via English-Chinese Parallel Text
</title>
<author confidence="0.975747">
Hwee Tou Ng and Yee Seng Chan
</author>
<affiliation confidence="0.924721">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.996538">
{nght, chanys}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833083333333">
We made use of parallel texts to gather train-
ing and test examples for the English lexi-
cal sample task. Two tracks were organized
for our task. The first track used examples
gathered from an LDC corpus, while the
second track used examples gathered from
a Web corpus. In this paper, we describe
the process of gathering examples from the
parallel corpora, the differences with similar
tasks in previous SENSEVAL evaluations,
and present the results of participating sys-
tems.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994001923077">
As part of the SemEval-2007 evaluation exercise, we
organized an English lexical sample task for word
sense disambiguation (WSD), where the sense-
annotated examples were semi-automatically gath-
ered from word-aligned English-Chinese parallel
texts. Two tracks were organized for this task, each
gathering data from a different corpus. In this paper,
we describe our motivation for organizing the task,
our task framework, and the results of participants.
Past research has shown that supervised learning
is one of the most successful approaches to WSD.
However, this approach involves the collection of
a large text corpus in which each ambiguous word
has been annotated with the correct sense to serve as
training data. Due to the expensive annotation pro-
cess, only a handful of manually sense-tagged cor-
pora are available.
An effort to alleviate the training data bottle-
neck is the Open Mind Word Expert (OMWE)
project (Chklovski and Mihalcea, 2002) to collect
sense-tagged data from Internet users. Data gath-
ered through the OMWE project were used in the
SENSEVAL-3 English lexical sample task. In that
task, WordNet-1.7.1 was used as the sense inven-
tory for nouns and adjectives, while Wordsmythl
was used as the sense inventory for verbs.
Another source of potential training data is par-
allel texts. Our past research in (Ng et al., 2003;
Chan and Ng, 2005) has shown that examples gath-
ered from parallel texts are useful for WSD. Briefly,
after manually assigning appropriate Chinese trans-
lations to each sense of an English word, the English
side of a word-aligned parallel text can then serve as
the training data, as they are considered to have been
disambiguated and “sense-tagged” by the appropri-
ate Chinese translations.
Using the above approach, we gathered the train-
ing and test examples for our task from parallel texts.
Note that our examples are collected without manu-
ally annotating each individual ambiguous word oc-
currence, allowing us to gather our examples in a
much shorter time. This contrasts with the setting of
the English lexical sample task in previous SENSE-
VAL evaluations. In the English lexical sample task
of SENSEVAL-2, the sense tagged data were cre-
ated through manual annotation by trained lexicog-
raphers. In SENSEVAL-3, the data were gathered
through manual sense annotation by Internet users.
In the next section, we describe in more detail
the process of gathering examples from parallel texts
and the two different parallel corpora we used. We
then give a brief description of each of the partici-
</bodyText>
<footnote confidence="0.989838">
1http://www.wordsmyth.net
</footnote>
<page confidence="0.992721">
54
</page>
<bodyText confidence="0.939234">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58,
Prague, June 2007. c�2007 Association for Computational Linguistics
pating systems. In Section 4, we present the results
obtained by the participants, before concluding in
Section 5.
</bodyText>
<sectionHeader confidence="0.817042" genericHeader="method">
2 Gathering Examples from Parallel
Corpora
</sectionHeader>
<bodyText confidence="0.999148219512195">
To gather examples from parallel corpora, we fol-
lowed the approach in (Ng et al., 2003). Briefly, af-
ter ensuring the corpora were sentence-aligned, we
tokenized the English texts and performed word seg-
mentation on the Chinese texts (Low et al., 2005).
We then made use of the GIZA++ software (Och and
Ney, 2000) to perform word alignment on the paral-
lel corpora. Then, we assigned some possible Chi-
nese translations to each sense of an English word
w. From the word alignment output of GIZA++, we
selected those occurrences of w which were aligned
to one of the Chinese translations chosen. The En-
glish side of these occurrences served as training
data for w, as they were considered to have been dis-
ambiguated and “sense-tagged” by the appropriate
Chinese translations. The English half of the par-
allel texts (each ambiguous English word and its 3-
sentence context) were used as the training and test
material to set up our English lexical sample task.
Note that in our approach, the sense distinction
is decided by the different Chinese translations as-
signed to each sense of a word. This is thus
similar to the multilingual lexical sample task in
SENSEVAL-3 (Chklovski et al., 2004), except that
our training and test examples are collected with-
out manually annotating each individual ambiguous
word occurrence. The average time needed to assign
Chinese translations for one noun and one adjective
is 20 minutes and 25 minutes respectively. This is
a relatively short time, compared to the effort other-
wise needed to manually sense annotate individual
word occurrences. Also, once the Chinese transla-
tions are assigned, more examples can be automat-
ically gathered as more parallel texts become avail-
able.
We note that frequently occurring words are usu-
ally highly polysemous and hard to disambiguate.
To maximize the benefits of our work, we gathered
training data from parallel texts for a set of most fre-
quently occurring noun and adjective types in the
Brown Corpus. Also, similar to the SENSEVAL-3
</bodyText>
<table confidence="0.998515428571429">
Dataset Avg. no. Avg. no. of examples
of senses
Training Test
LDC noun 5.2 197.6 98.5
LDC adjective 3.9 125.6 62.9
Web noun 3.5 182.0 91.3
Web adjective 2.8 88.8 44.6
</table>
<tableCaption confidence="0.9834115">
Table 1: Average number of senses, training exam-
ples, and test examples per word.
</tableCaption>
<bodyText confidence="0.7474285">
English lexical sample task, we used WordNet-1.7.1
as our sense inventory.
</bodyText>
<subsectionHeader confidence="0.79551">
2.1 LDC Corpus
</subsectionHeader>
<bodyText confidence="0.9998524">
We have two tracks for this task, each track using a
different corpus. The first corpus is the Chinese En-
glish News Magazine Parallel Text (LDC2005T10),
which is an English-Chinese parallel corpus avail-
able from the Linguistic Data Consortium (LDC).
From this parallel corpus, we gathered examples
for 50 English words (25 nouns and 25 adjectives)
using the method described above. From the gath-
ered examples of each word, we randomly selected
training and test examples, where the number of
training examples is about twice the number of test
examples.
The rows LDC noun and LDC adjective in Table
1 give some statistics about the examples. For in-
stance, each noun has an average of 197.6 training
and 98.5 test examples and these examples repre-
sent an average of 5.2 senses per noun.2 Participants
taking part in this track need to have access to this
LDC corpus in order to access the training and test
material in this track.
</bodyText>
<subsectionHeader confidence="0.997509">
2.2 Web Corpus
</subsectionHeader>
<bodyText confidence="0.999618777777778">
Since not all interested participants may have access
to the LDC corpus described in the previous sub-
section, the second track of this task makes use of
English-Chinese documents gathered from the URL
pairs given by the STRAND Bilingual Databases.3
STRAND (Resnik and Smith, 2003) is a system that
acquires document pairs in parallel translation auto-
matically from the Web. Using this corpus, we gath-
ered examples for 40 English words (20 nouns and
</bodyText>
<footnote confidence="0.999823">
2Only senses present in the examples are counted.
3http://www.umiacs.umd.edu/—resnik/strand
</footnote>
<page confidence="0.995696">
55
</page>
<bodyText confidence="0.923640625">
20 adjectives).
The rows Web noun and Web adjective in Table 1
show that we selected an average of 182.0 training
and 91.3 test examples for each noun and these ex-
amples represent an average of 3.5 senses per noun.
We note that the average number of senses per word
for the Web corpus is slightly lower than that of the
LDC corpus.
</bodyText>
<subsectionHeader confidence="0.999707">
2.3 Annotation Accuracy
</subsectionHeader>
<bodyText confidence="0.999981826086957">
To measure the annotation accuracy of examples
gathered from the LDC corpus, we examined a ran-
dom selection of 100 examples each from 5 nouns
and 5 adjectives. From these 1,000 examples, we
measured a sense annotation accuracy of 84.7%.
These 10 words have an average of 8.6 senses per
word in the WordNet-1.7.1 sense inventory. As de-
scribed in (Ng et al., 2003), when several senses
of an English word are translated by the same Chi-
nese word, we can collapse these senses to obtain a
coarser-grained, lumped sense inventory. If we do
this and measure the sense annotation accuracy with
respect to a coarser-grained, lumped sense inventory,
these 10 words will have an average of 6.5 senses per
word and an annotation accuracy of 94.7%.
For the Web corpus, we similarly examined a ran-
dom selection of 100 examples each from 5 nouns
and 5 adjectives. These 10 words have an average of
6.5 senses per word in WordNet-1.7.1 and the 1,000
examples have an average sense annotation accuracy
of 85.0%. After sense collapsing, annotation ac-
curacy is 95.3% with an average of 4.8 senses per
word.
</bodyText>
<subsectionHeader confidence="0.8221035">
2.4 Training and Test Data from Different
Documents
</subsectionHeader>
<bodyText confidence="0.999962444444444">
In our previous work (Ng et al., 2003), we conducted
experiments on the nouns of SENSEVAL-2 English
lexical sample task. We found that there were cases
where the same document contributed both training
and test examples and this inflated the WSD accu-
racy figures. To avoid this, during our preparation
of the LDC and Web data, we made sure that a doc-
ument contributed only either training or test exam-
ples, but not both.
</bodyText>
<sectionHeader confidence="0.936758" genericHeader="method">
3 Participating Systems
</sectionHeader>
<bodyText confidence="0.999985">
Three teams participated in the Web corpus track
of our task, with each team employing one system.
There were no participants in the LDC corpus track,
possibly due to the licensing issues involved. All
participating systems employed supervised learning
and only used the training examples provided by us.
</bodyText>
<subsectionHeader confidence="0.999116">
3.1 CITYU-HIF
</subsectionHeader>
<bodyText confidence="0.99998">
The CITYU-HIF team from the City University of
Hong Kong trained a naive Bayes (NB) classifier
for each target word to be disambiguated, using
knowledge sources such as parts-of-speech (POS) of
neighboring words and single words in the surround-
ing context. They also experimented with using dif-
ferent sets of features for each target word.
</bodyText>
<subsectionHeader confidence="0.999112">
3.2 HIT-IR-WSD
</subsectionHeader>
<bodyText confidence="0.999950571428571">
The system submitted by the HIT-IR-WSD team
from Harbin Institute of Technology used Support
Vector Machines (SVM) with a linear kernel func-
tion as the learning algorithm. Knowledge sources
used included POS of surrounding words, local col-
locations, single words in the surrounding context,
and syntactic relations.
</bodyText>
<subsectionHeader confidence="0.991294">
3.3 PKU
</subsectionHeader>
<bodyText confidence="0.999957666666667">
The system submitted by the PKU team from Peking
University used a combination of SVM and maxi-
mum entropy classifiers. Knowledge sources used
included POS of surrounding words, local colloca-
tions, and single words in the surrounding context.
Feature selection was done by ignoring word fea-
tures with certain associated POS tags and by se-
lecting the subset of features based on their entropy
values.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999881444444445">
As all participating systems gave only one answer
for each test example, recall equals precision and
we will only report micro-average recall on the Web
corpus track in this section.
Table 2 gives the overall results obtained by each
of the systems when evaluated on all the test exam-
ples of the Web corpus. We note that all the par-
ticipants obtained scores which exceed the baseline
heuristic of tagging all test examples with the most
</bodyText>
<page confidence="0.99211">
56
</page>
<table confidence="0.9831322">
System ID Contact author Learning algorithm Score
HIT-IR-WSD Yuhang Guo, &lt;astronaut@ir.hit.edu.cn&gt; SVM 0.819
PKU Peng Jin, &lt;jandp@pku.edu.cn&gt; SVM and maximum entropy 0.815
CITYU-HIF Oi Yee Kwong, &lt;rlolivia@cityu.edu.hk&gt; NB 0.753
MFS – Most frequent sense baseline 0.689
</table>
<tableCaption confidence="0.939721">
Table 2: Overall micro-average scores of the participants and the most frequent sense (MFS) baseline.
</tableCaption>
<table confidence="0.999932136363636">
Noun MFS CITYU-HIF HIT-IR-WSD PKU Adjective MFS CITYU-HIF HIT-IR-WSD PKU
age 0.486 0.643 0.743 0.700 ancient 0.778 0.667 0.778 0.741
area 0.480 0.693 0.773 0.773 bad 0.857 0.857 0.905 0.905
body 0.872 0.897 0.910 0.923 common 0.533 0.567 0.533 0.633
change 0.411 0.400 0.578 0.611 early 0.769 0.846 0.769 0.769
director 0.580 0.890 0.960 0.960 educational 0.911 0.911 0.911 0.911
experience 0.830 0.830 0.880 0.840 free 0.760 0.792 0.854 0.917
future 0.889 0.889 0.990 0.990 high 0.630 0.926 0.815 0.852
interest 0.308 0.165 0.813 0.780 human 0.872 0.987 0.962 0.962
issue 0.651 0.711 0.892 0.855 little 0.450 0.750 0.650 0.650
life 0.820 0.830 0.860 0.740 long 0.667 0.690 0.786 0.714
material 0.719 0.719 0.781 0.641 major 0.870 0.902 0.880 0.913
need 0.907 0.907 0.918 0.918 medical 0.738 0.787 0.800 0.725
performance 0.410 0.570 0.690 0.700 national 0.267 0.467 0.667 0.700
program 0.590 0.590 0.730 0.690 new 0.441 0.441 0.529 0.559
report 0.870 0.840 0.880 0.870 present 0.875 0.917 0.875 0.875
system 0.510 0.700 0.610 0.730 rare 0.727 0.818 0.727 0.909
time 0.455 0.673 0.733 0.693 serious 0.879 0.879 0.879 0.879
today 0.800 0.750 0.800 0.780 simple 0.795 0.818 0.864 0.864
water 0.882 0.921 0.868 0.895 small 0.714 0.929 0.893 0.929
work 0.644 0.743 0.842 0.891 third 0.888 0.988 0.963 0.963
Micro-avg 0.656 0.719 0.813 0.802 Micro-avg 0.757 0.823 0.831 0.842
</table>
<tableCaption confidence="0.99712">
Table 3: Micro-average scores of the most frequent
</tableCaption>
<bodyText confidence="0.995304461538462">
sense baseline and the various participants on each
noun.
frequent sense (MFS) in the training data. This sug-
gests that the Chinese translations assigned to senses
of the ambiguous words are appropriate and provide
sense distinctions which are clear enough for effec-
tive classifiers to be learned.
In Table 3 and Table 4, we show the scores ob-
tained by each system on each of the 20 nouns and
20 adjectives. For comparison purposes, we also
show the corresponding MFS score of each word.
Paired t-test on the results of the top two systems
show no significant difference between them.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.997642923076923">
We organized an English lexical sample task using
examples gathered from parallel texts. Unlike the
English lexical task of previous SENSEVAL evalua-
tions where each example is manually annotated, we
Table 4: Micro-average scores of the most frequent
sense baseline and the various participants on each
adjective.
only need to assign appropriate Chinese translations
to each sense of a word. Once this is done, we auto-
matically gather training and test examples from the
parallel texts. All the participating systems of our
task obtain results that are significantly better than
the most frequent sense baseline.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.78591">
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076).
</bodyText>
<sectionHeader confidence="0.988436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.94875125">
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
Proceedings ofAAAI05, pages 1037–1042, Pittsburgh,
Pennsylvania, USA.
</reference>
<page confidence="0.981574">
57
</page>
<reference confidence="0.997932125">
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proceedings of ACL02 Workshop on Word Sense
Disambiguation: Recent Successes and Future Direc-
tions, pages 116–122, Philadelphia, USA.
Timothy Chklovski, Rada Mihalcea, Ted Pedersen, and
Amruta Purandare. 2004. The SENSEVAL-3 multi-
lingual English-Hindi lexical sample task. In Proceed-
ings of SENSEVAL-3, pages 5–8, Barcelona, Spain.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 161–
164, Jeju Island, Korea.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL03, pages
455–462, Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceeedings ofACL00,
pages 440–447, Hong Kong.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
</reference>
<page confidence="0.999258">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.267372">
<title confidence="0.903937">SemEval-2007 Task 11: English Lexical Sample Task via English-Chinese Parallel Text</title>
<author confidence="0.998289">Tou Ng Seng Chan</author>
<affiliation confidence="0.999656">Department of Computer Science National University of Singapore</affiliation>
<address confidence="0.384481">3 Science Drive 2, Singapore 117543</address>
<abstract confidence="0.988993538461538">We made use of parallel texts to gather training and test examples for the English lexical sample task. Two tracks were organized for our task. The first track used examples gathered from an LDC corpus, while the second track used examples gathered from a Web corpus. In this paper, we describe the process of gathering examples from the parallel corpora, the differences with similar tasks in previous SENSEVAL evaluations, and present the results of participating systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Scaling up word sense disambiguation via parallel texts.</title>
<date>2005</date>
<booktitle>In Proceedings ofAAAI05,</booktitle>
<pages>1037--1042</pages>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="2104" citStr="Chan and Ng, 2005" startWordPosition="330" endWordPosition="333">notation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task. In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmythl was used as the sense inventory for verbs. Another source of potential training data is parallel texts. Our past research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English side of a word-aligned parallel text can then serve as the training data, as they are considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. Using the above approach, we gathered the training and test examples for our task from parallel texts. Note that our examples are collected without manually annotating each individual ambiguous word occurrence, allowing us to gat</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings ofAAAI05, pages 1037–1042, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a sense tagged corpus with Open Mind Word Expert.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>116--122</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1693" citStr="Chklovski and Mihalcea, 2002" startWordPosition="260" endWordPosition="263">rent corpus. In this paper, we describe our motivation for organizing the task, our task framework, and the results of participants. Past research has shown that supervised learning is one of the most successful approaches to WSD. However, this approach involves the collection of a large text corpus in which each ambiguous word has been annotated with the correct sense to serve as training data. Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task. In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmythl was used as the sense inventory for verbs. Another source of potential training data is parallel texts. Our past research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English si</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Timothy Chklovski and Rada Mihalcea. 2002. Building a sense tagged corpus with Open Mind Word Expert. In Proceedings of ACL02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 116–122, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
<author>Amruta Purandare</author>
</authors>
<title>The SENSEVAL-3 multilingual English-Hindi lexical sample task.</title>
<date>2004</date>
<booktitle>In Proceedings of SENSEVAL-3,</booktitle>
<pages>5--8</pages>
<location>Barcelona,</location>
<contexts>
<context position="4818" citStr="Chklovski et al., 2004" startWordPosition="770" endWordPosition="773">hinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. The English half of the parallel texts (each ambiguous English word and its 3- sentence context) were used as the training and test material to set up our English lexical sample task. Note that in our approach, the sense distinction is decided by the different Chinese translations assigned to each sense of a word. This is thus similar to the multilingual lexical sample task in SENSEVAL-3 (Chklovski et al., 2004), except that our training and test examples are collected without manually annotating each individual ambiguous word occurrence. The average time needed to assign Chinese translations for one noun and one adjective is 20 minutes and 25 minutes respectively. This is a relatively short time, compared to the effort otherwise needed to manually sense annotate individual word occurrences. Also, once the Chinese translations are assigned, more examples can be automatically gathered as more parallel texts become available. We note that frequently occurring words are usually highly polysemous and har</context>
</contexts>
<marker>Chklovski, Mihalcea, Pedersen, Purandare, 2004</marker>
<rawString>Timothy Chklovski, Rada Mihalcea, Ted Pedersen, and Amruta Purandare. 2004. The SENSEVAL-3 multilingual English-Hindi lexical sample task. In Proceedings of SENSEVAL-3, pages 5–8, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>Jeju Island,</location>
<contexts>
<context position="3882" citStr="Low et al., 2005" startWordPosition="609" endWordPosition="612">f the partici1http://www.wordsmyth.net 54 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58, Prague, June 2007. c�2007 Association for Computational Linguistics pating systems. In Section 4, we present the results obtained by the participants, before concluding in Section 5. 2 Gathering Examples from Parallel Corpora To gather examples from parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. The English half of the parallel texts (each ambiguous English word and its 3- </context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161– 164, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL03,</booktitle>
<pages>455--462</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2084" citStr="Ng et al., 2003" startWordPosition="326" endWordPosition="329"> the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottleneck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. Data gathered through the OMWE project were used in the SENSEVAL-3 English lexical sample task. In that task, WordNet-1.7.1 was used as the sense inventory for nouns and adjectives, while Wordsmythl was used as the sense inventory for verbs. Another source of potential training data is parallel texts. Our past research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. Briefly, after manually assigning appropriate Chinese translations to each sense of an English word, the English side of a word-aligned parallel text can then serve as the training data, as they are considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. Using the above approach, we gathered the training and test examples for our task from parallel texts. Note that our examples are collected without manually annotating each individual ambiguous word occurrence</context>
<context position="3719" citStr="Ng et al., 2003" startWordPosition="583" endWordPosition="586">be in more detail the process of gathering examples from parallel texts and the two different parallel corpora we used. We then give a brief description of each of the partici1http://www.wordsmyth.net 54 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58, Prague, June 2007. c�2007 Association for Computational Linguistics pating systems. In Section 4, we present the results obtained by the participants, before concluding in Section 5. 2 Gathering Examples from Parallel Corpora To gather examples from parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to </context>
<context position="8184" citStr="Ng et al., 2003" startWordPosition="1336" endWordPosition="1339">.0 training and 91.3 test examples for each noun and these examples represent an average of 3.5 senses per noun. We note that the average number of senses per word for the Web corpus is slightly lower than that of the LDC corpus. 2.3 Annotation Accuracy To measure the annotation accuracy of examples gathered from the LDC corpus, we examined a random selection of 100 examples each from 5 nouns and 5 adjectives. From these 1,000 examples, we measured a sense annotation accuracy of 84.7%. These 10 words have an average of 8.6 senses per word in the WordNet-1.7.1 sense inventory. As described in (Ng et al., 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory. If we do this and measure the sense annotation accuracy with respect to a coarser-grained, lumped sense inventory, these 10 words will have an average of 6.5 senses per word and an annotation accuracy of 94.7%. For the Web corpus, we similarly examined a random selection of 100 examples each from 5 nouns and 5 adjectives. These 10 words have an average of 6.5 senses per word in WordNet-1.7.1 and the 1,000 examples have an average se</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of ACL03, pages 455–462, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceeedings ofACL00,</booktitle>
<pages>440--447</pages>
<location>Hong Kong.</location>
<contexts>
<context position="3943" citStr="Och and Ney, 2000" startWordPosition="621" endWordPosition="624"> 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54–58, Prague, June 2007. c�2007 Association for Computational Linguistics pating systems. In Section 4, we present the results obtained by the participants, before concluding in Section 5. 2 Gathering Examples from Parallel Corpora To gather examples from parallel corpora, we followed the approach in (Ng et al., 2003). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005). We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora. Then, we assigned some possible Chinese translations to each sense of an English word w. From the word alignment output of GIZA++, we selected those occurrences of w which were aligned to one of the Chinese translations chosen. The English side of these occurrences served as training data for w, as they were considered to have been disambiguated and “sense-tagged” by the appropriate Chinese translations. The English half of the parallel texts (each ambiguous English word and its 3- sentence context) were used as the training and test material</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceeedings ofACL00, pages 440–447, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="7202" citStr="Resnik and Smith, 2003" startWordPosition="1167" endWordPosition="1170">ive some statistics about the examples. For instance, each noun has an average of 197.6 training and 98.5 test examples and these examples represent an average of 5.2 senses per noun.2 Participants taking part in this track need to have access to this LDC corpus in order to access the training and test material in this track. 2.2 Web Corpus Since not all interested participants may have access to the LDC corpus described in the previous subsection, the second track of this task makes use of English-Chinese documents gathered from the URL pairs given by the STRAND Bilingual Databases.3 STRAND (Resnik and Smith, 2003) is a system that acquires document pairs in parallel translation automatically from the Web. Using this corpus, we gathered examples for 40 English words (20 nouns and 2Only senses present in the examples are counted. 3http://www.umiacs.umd.edu/—resnik/strand 55 20 adjectives). The rows Web noun and Web adjective in Table 1 show that we selected an average of 182.0 training and 91.3 test examples for each noun and these examples represent an average of 3.5 senses per noun. We note that the average number of senses per word for the Web corpus is slightly lower than that of the LDC corpus. 2.3 </context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>