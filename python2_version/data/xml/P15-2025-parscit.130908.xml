<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003531">
<title confidence="0.992743">
Representation Based Translation Evaluation Metrics
</title>
<author confidence="0.972769">
Boxing Chen and Hongyu Guo
</author>
<affiliation confidence="0.7809535">
National Research Council Canada
first.last@nrc-cnrc.gc.ca
</affiliation>
<sectionHeader confidence="0.985225" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969157894737">
Precisely evaluating the quality of a trans-
lation against human references is a chal-
lenging task due to the flexible word or-
dering of a sentence and the existence of
a large number of synonyms for words.
This paper proposes to evaluate transla-
tions with distributed representations of
words and sentences. We study several
metrics based on word and sentence repre-
sentations and their combination. Experi-
ments on the WMT metric task shows that
the metric based on the combined repre-
sentations achieves the best performance,
outperforming the state-of-the-art transla-
tion metrics by a large margin. In particu-
lar, training the distributed representations
only needs a reasonable amount of mono-
lingual, unlabeled data that is not neces-
sary drawn from the test domain.
</bodyText>
<sectionHeader confidence="0.998111" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99967062295082">
Automatic machine translation (MT) evaluation
metrics measure the quality of the translations
against human references. They allow rapid com-
parisons between different systems and enable the
tuning of parameter values during system train-
ing. Many machine translation metrics have been
proposed in recent years, such as BLEU (Pap-
ineni et al., 2002), NIST (Doddington, 2002), TER
(Snover et al., 2006), Meteor (Banerjee and Lavie,
2005) and its extensions, and the MEANT family
(Lo and Wu, 2011), amongst others.
Precisely evaluating translation, however, is not
easy. This is mainly caused by the flexible word
ordering and the existence of the large number
of synonyms for words. One straightforward so-
lution to improve the evaluation quality is to in-
crease the number of various references. Never-
theless, it is expensive to create multiple refer-
ences. In order to catch synonym matches be-
tween the translations and references, synonym
dictionaries or paraphrasing tables have been used.
For example, Meteor (Banerjee and Lavie, 2005)
uses WordNet (Miller, 1995); TER-Plus (Snover et
al., 2009) and Meteor Universal (Denkowski and
Lavie, 2014) deploy paraphrasing tables. These
dictionaries have helped to improve the accuracy
of the evaluation; however, not all languages have
synonym dictionaries or paraphrasing tables, espe-
cially for those low resource languages.
This paper leverages recent developments on
distributed representations to address the above
mentioned two challenges. A distributed represen-
tation maps each word or sentence to a continu-
ous, low dimensional space, where words or sen-
tences having similar syntactic and semantic prop-
erties are close to one another (Bengio et al., 2003;
Socher et al., 2011; Socher et al., 2013; Mikolov
et al., 2013). For example, the words vacation
and holiday are close to each other in the vector
space, but both are far from the word business in
that space.
We propose to evaluate the translations with dif-
ferent word and sentence representations. Specif-
ically, we investigate the use of three widely de-
ployed representations: one-hot representations,
distributed word representations learned from a
neural network model, and distributed sentence
representations computed with recursive auto-
encoder. In particular, to leverage the different ad-
vantages and focuses, in terms of benefiting eval-
uation, of various representations, we concatenate
the three representations to form one vector rep-
resentation for each sentence. Our experiments
on the WMT metric task show that the metric
based on the concatenated representation outper-
forms several state-of-the-art machine translation
metrics, by a large margin on both segment and
system-level. Furthermore, our results also indi-
cate that the representation based metrics are ro-
bust to a variety of training conditions, such as the
data volume and domain.
</bodyText>
<page confidence="0.524601">
150
</page>
<note confidence="0.811298333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 150–155,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.994276" genericHeader="introduction">
2 Representations
</sectionHeader>
<bodyText confidence="0.999436833333333">
A representation, in the context of NLP, is a math-
ematical object associated with each word, sen-
tence, or document. This object is typically a vec-
tor where each element’s value describes, to some
degree, the semantic or syntactic properties of the
associated word, sentence, or document. Using
word or phrase representations as extra features
has been proven to be an effective and simple
way to improve the predictive performance of an
NLP system (Turian et al., 2010; Cherry and Guo,
2015). Our evaluation metrics are based on three
widely used representations, as discussed next.
</bodyText>
<subsectionHeader confidence="0.972695">
2.1 One-hot Representations
</subsectionHeader>
<bodyText confidence="0.999912642857143">
Conventionally, a word is represented by a one-hot
vector. In a one-hot representation, a vocabulary
is first defined, and then each word in the vocabu-
lary is assigned a symbolic ID. In this scenario, for
each word, the feature vector has the same length
as the size of the vocabulary, and only one dimen-
sion that corresponds to the word is on, such as
a vector with one element set to 1 and all others
set to 0. This feature representation has been tra-
ditionally used for many NLP systems. On the
other hand, recent years have witnessed that sim-
ply plugging in distributed word vectors as real-
valued features is an effective way to improve a
NLP system (Turian et al., 2010).
</bodyText>
<subsectionHeader confidence="0.995231">
2.2 Distributed Word Representations
</subsectionHeader>
<bodyText confidence="0.9999045">
Distributed word representations, also called word
embeddings, map each word deterministically to a
real-valued, dense vector (Bengio et al., 2003). A
widely used approach for generating useful word
vectors is developed by (Mikolov et al., 2013).
This method scales very well to very large training
corpora. Their skip-gram model, which we adopt
here, learns word vectors that are good at predict-
ing the words in a context window surrounding it.
A very promising perspective of such distributed
representation is that words that have similar con-
texts, and therefore similar syntactic and semantic
properties, will tend to be near one another in the
low-dimensional vector space.
</bodyText>
<subsectionHeader confidence="0.999458">
2.3 Sentence Vector Representations
</subsectionHeader>
<bodyText confidence="0.999987333333333">
Word level representation often cannot properly
capture more complex linguistic phenomena in a
sentence or multi-word phrase. Therefore, we
adopt an effective and efficient method for multi-
word phrase distributed representation, namely the
greedy unsupervised recursive auto-encoder strat-
egy (RAE) (Socher et al., 2011). This method
works under an unsupervised setting. In particular,
it does not rely on a parsing tree structure in order
to generate sentence level vectors. This character-
istic makes it very desirable for applying it to the
outputs of machine translation systems. This is be-
cause the outputs of translation systems are often
not syntactically correct sentences; parsing them
is possible to introduce unexpected noise.
For a given sentence, the greedy unsupervised
RAE greedily searches a pair of words that re-
sults in minimal reconstruction error by an auto-
encoder. The corresponding hidden vector of the
auto-encoder (denoted as the two children’s par-
ent vector), which has the same size as that of the
two child vectors, is then used to replace the two
children vectors. This process repeats and treats
the new parent vector like any other word vectors.
In such a recursive manner, the parent vector gen-
erated from the word pool with only two vectors
left will be used as the vector representation for
the whole sentence. Interested readers are referred
to (Socher et al., 2011) for detailed discussions of
the strategy.
</bodyText>
<subsectionHeader confidence="0.997108">
2.4 Combined Representations
</subsectionHeader>
<bodyText confidence="0.999991473684211">
Each of the above mentioned representations has
a different strength in terms of encoding syntactic
and semantic contextual information for a given
sentence. Specifically, the one-hot representation
is able to reflect the particular words that occur
in the sentence. The word embeddings can rec-
ognize synonyms of words appearing in the sen-
tence, through the co-occurrence information en-
coded in the vector’s representation. Finally, the
RAE vector can encode the composed semantic
information of the given sentence. These obser-
vations suggest that it is beneficial to take various
types of representations into account.
The most straightforward way to integrate mul-
tiple vectors is using concatenation. In our studies
here, we first compute the sentence-level one-hot,
word embedding, and RAE representations. Next,
we concatenate the three sentence-level represen-
tations to form one vector for each sentence.
</bodyText>
<sectionHeader confidence="0.966223" genericHeader="method">
3 Representations Based Metrics
</sectionHeader>
<bodyText confidence="0.9990795">
Our translation evaluation metrics are built on the
four representations as discussed in Section 2.
</bodyText>
<page confidence="0.662941">
151
</page>
<bodyText confidence="0.9999645">
Consider we have the sentence representations
for the translations (t) and references (r), the
translation quality is measured with a similarity
score computed with Cosine function and a length
penalty. Suppose the size of the vector is N, we
calculate the quality as follows.
</bodyText>
<equation confidence="0.995708888888889">
PScore(t, r) = Cosα(t, r) x Plen (1)
i=N
i=1 vi(t) · vi (r)
Cos(t, r) = qPi=N qPi=N (2)
i=1 v2 i (t) i=1 v2 i (r)
(
exp(1 − lr/lt) if (lt &lt; lr)
Plen = exp(1 − lt/lr) if (lt lr) (3)
&gt;
</equation>
<bodyText confidence="0.9997164">
where α is a free parameter, vi(.) is the value of
the vector element, Plen is the length penalty, and
lr, lt are length of the translation and reference,
respectively.
In the scenarios of there exist multiple refer-
ences, we compute the score with each reference,
then choose the highest one. Also, we treat the
document-level score as the weighted average of
sentence-level scores, with the weights being the
reference lengths, as follows.
</bodyText>
<equation confidence="0.996473">
PD i=1 len(ri)Scorei
PScored = (4)
D
i=1 len(ri)
</equation>
<bodyText confidence="0.999900714285714">
where Scorei denotes the score of sentence i, and
D is the size of the document in sentences. With
these score equations, we then can formulate our
five presentations based metrics as follows.
For the one-hot representation metric, once we
have the representations of the words and n-grams,
we sum all the vectors to obtain the representation
of the sentence. For efficiency, we only keep the
entries which are not both zero in the reference
and translation vectors. After we generate the two
vectors for both translation and reference, we then
compute the score using Equation 1.
For the word embedding based metric, we first
learn the word vector representation using the
code provided by (Mikolov et al., 2013) 1. Next,
following (Zou et al., 2013), we average the word
embeddings of all words in the sentence to obtain
the representation of the sentence.
As discussed in Section 2.4, the three sentence-
level one-hot, word embedding and RAE repre-
sentations have different strength when they are
</bodyText>
<footnote confidence="0.642913">
1https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.9997767">
used to compare two sentences. In our metric here,
each of the three vectors is first scaled with a par-
ticular weight (learned on dev data) and then the
vectors are concatenated. With these concatena-
tion vectors, we then calculate the similarity score
using Equation 1.
For comparison, we also combine the strength
of the three representations using weighted aver-
age of the three metrics computed. Weights are
tuned using development data.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99998396969697">
We conducted experiments on the WMT met-
ric task data. Development sets include WMT
2011 all-to-English, and English-to-all submis-
sions. Test sets contain WMT 2012, and WMT
2013 all-to-English, plus 2012, 2013 English-
to-all submissions. The languages “all” include
French, Spanish, German and Czech. For training
the word embedding and recursive auto-encoder
model, we used WMT 2013 training data 2.
We compared our metrics with smoothed BLEU
(mteval-v13a), TER 3, Meteor v1.0 4, and Meteor
Universal (i.e. v1.5) 5. We used the default set-
tings for all these four metrics.
When considering the representation based met-
rics, we tuned all the parameters to maximize the
system-level γ score for all representation based
metrics on the dev sets. We tuned the weights
for combining the three vectors automatically, us-
ing the downhill simplex method as described in
(Press et al., 2002). The weights are 1 for the
RAE vector, about 0.1 for the word embedding
vector, and around 0.01 for the one-hot vector, re-
spectively. We tuned other parameters manually.
Specifically, we set n equal to 2 for the one-hot
n-gram representation, the vector size of the re-
cursive auto-encoder to 10, and the vector size of
word embeddings to 80.
Following WMT 2013’s metric task (Mach´aˇcek
and Bojar, 2013), to measure the correlation with
human judgment, we use Kendall’s rank correla-
tion coefficient τ for the segment level, and Pear-
son’s correlation coefficient (γ in the below tables
and figures) for the system-level respectively.
</bodyText>
<footnote confidence="0.99580475">
2http://www.statmt.org/wmt13/translation-task.html
3http://www.cs.umd.edu/ snover/tercom/
4http://www.cs.cmu.edu/ alavie/METEOR/
5Meteor universal package does not include paraphrasing
</footnote>
<tableCaption confidence="0.759265">
table for other target language except English, so we did not
</tableCaption>
<table confidence="0.983414076923077">
run Out-of-English experiments for this metric.
152
Into-Eng Out-of-Eng
metric seg τ sys γ seg τ sys γ
BLEU 0.220 0.751 0.179 0.736
TER 0.211 0.742 0.175 0.745
Meteor 0.228 0.824 0.180 0.778
Met. Uni. 0.249 0.808 – –
One-hot 0.235 0.795 0.183 0.773
Word emb. 0.212 0.818 0.175 0.788
RAE vec. 0.203 0.856 0.171 0.780
Comb. rep. 0.259 0.874 0.191 0.832
Wghted avg. 0.247 0.863 0.185 0.798
</table>
<tableCaption confidence="0.990094">
Table 1: Correlations with human judgment on
WMT data for Into-English and Out-of-English
task. Results are averaged on all test sets.
</tableCaption>
<subsectionHeader confidence="0.989898">
4.1 General Performance
</subsectionHeader>
<bodyText confidence="0.99997055">
We first report the main experimental results con-
ducted on the Into-English and Out-of-English
tasks. Results in Tables 1 suggest that metrics
based on three single representations all obtained
comparable or better performance than BLEU,
TER and Meteor. In particular, the metric based
on recursive auto-encoder outperformed the other
testing metrics on system-level. When combin-
ing the strengths of the three representations, our
experimental results show that the metric based
on the combined representation outperformed all
state-of-the-art metrics by a large margin on both
segment- and system-level.
Regarding the evaluation speed of the represen-
tation metrics, it took around 1 minute to score
about 2000 sentences with the above settings on
a machine with a 2.33GHz Intel CPU. It is worth
noting that if we increase the vector size of the
RAE model and word embeddings, longer execu-
tion time is expected for the scoring processes.
</bodyText>
<figure confidence="0.638366">
20K 80K 320K 1.28M 5.12M
number of training sentences
</figure>
<figureCaption confidence="0.68845725">
Figure 1: Correlations with human judgment on
WMT data for Into-English task for combined rep-
resentation based metric when increasing the size
of the training data.
</figureCaption>
<subsectionHeader confidence="0.984346">
4.2 Effect of the Training Data Size
</subsectionHeader>
<bodyText confidence="0.9999936">
In our second experiment, we measure the per-
formance on the Into-English task and increase
the training data from 20K sentences to 11 mil-
lion sentences. The sentences are randomly se-
lected from the whole training data, which in-
clude the English side of WMT 2013 French-to-
English parallel data (“Europarl v7”, “News Com-
mentary” and “UN Corpus”). The results are re-
ported in Figure 1. From this figure, one can con-
clude that the performance improves with the in-
creasing of the training data, however, when more
than 1.28M sentences are used, the performance
stabilizes. This result indicates that training a sta-
ble and good model for our metric does not need a
huge amount of training data.
</bodyText>
<subsectionHeader confidence="0.999122">
4.3 Sensitivity to Data Across Domains
</subsectionHeader>
<bodyText confidence="0.999975590909091">
The last experiment aimed at the following ques-
tion: should the test domain be consistent with the
training domain? In this experiment, we sampled
three training sets from different domain data sets
in equal number (136K) of sentences: Europarl
(EP), News Commentary (NC), and United Na-
tion proceedings (UN), while the test domain re-
mains the same, i.e., the news domain. The met-
ric trained on NC domain data achieved slightly
higher segment-level τ score (0.181 vs 0.178 for
EP, 0.176 for UN) and system-level Pearson’s cor-
relation score γ (0.821 vs 0.820 for EP, 0.817
for UN). Nevertheless, the results are consistent
across domains. This is explainable: although the
same test sentence may have different representa-
tions w.r.t. the training domain, the distance be-
tween the translation and its reference may stay
consistent. Practically, the training and test data
not necessary being in the same domain is a very
attractive characteristic for the translation metrics.
It means that we do not have to train the word em-
beddings and RAE model for each testing domain.
</bodyText>
<subsectionHeader confidence="0.999361">
4.4 Cope with Word Ordering and Synonym
</subsectionHeader>
<bodyText confidence="0.999781666666667">
In order to better understand why metrics based on
combined representations can achieve better cor-
relation with human judgment than other metrics,
we select, in Table 2, some interesting examples
for further analysis.
Consider, for instance, the first reference (de-
noted as “1 R” in Table 2) and their translations. If
we replace the word vacation in the reference with
words business and holiday, respectively, then we
</bodyText>
<figure confidence="0.985158714285714">
seg τ
sys γ
0.8
Into−English
0.6
0.4
0.2
</figure>
<page confidence="0.497245">
153
</page>
<table confidence="0.8676335">
id sentence BLEU rep.
1 R i had a wonderful vacation in italy – –
1 H1 i had a wonderful business in italy 0.489 0.555
1 H2 i had a wonderful holiday in italy 0.489 0.865
1 H3 in italy i had a wonderful vacation 0.707 0.804
1 H4 vacation in i had a wonderful italy 0.508 0.305
2 R but the decision was not his to make – –
2 H1 but it is not up to him to decide 0.063 0.652
2 H2 but the decision not him to take 0.241 0.620
2 H3 but the decision was not the to make 0.595 0.612
3 R they were set to go on trial in jan – –
3 H1 they should appear in court in jan 0.109 0.498
3 H2 the trial was scheduled in jan 0.109 0.454
3 H3 the procedures were prepared in jan 0.109 0.445
</table>
<tableCaption confidence="0.963379">
Table 2: Examples evaluated with smoothed BLEU and combined representation based metric. Examples
</tableCaption>
<bodyText confidence="0.992029304347826">
2-3 are picked up from the real test sets; human judgment ranks H1 better than H2, and H2 better than H3
for each of these example sentences. The combined representation based metric better matches human
judgment than BLEU does.
have hypothesis 1 and hypothesis 2, denoted as “1
H1” and “1 H2”, respectively, in Table 2 . In this
scenario, the metric BLEU assigns the same score
of 0.489 for these two translations. In contrast, the
representation based metric associates hypothesis
2 with a much higher score than that of hypothesis
1, namely 0.865 and 0.555, respectively. In other
words, the score for hypothesis 2 is close to one,
suggesting that the RAE based metric considers
this translation is almost identical to the reference.
The reason here is that the vector representations
for the two words are very near to one another in
the vector space. Consequently, the representation
based metric treats the holiday as a synonym of
vacation, which matches human’s judgment per-
fectly.
Let us continue with this example. Suppose, in
hypothesis 3, we reorder the phrase in italy. The
representation based metric still considers this to
be a good translation with respect to the reference,
thus associating a very close score as that of the
reference, namely 0.804. The reason for represen-
tation metric’s correct judgment is that H3 and the
reference, in the vector space, embed very similar
semantic knowledge, although they have different
word orderings. Now let us take this example a
bit further. We randomly mess up the words in the
reference, resulting in hypothesis 4 (denoted as “1
H4” as shown in Table 2). In such scenario, the
representation metric score drops sharply because
the syntactic and semantic information embedded
in the vector space is very different from the refer-
ence. Interestingly, the BLEU metric still consider
this translation is not a very bad translation.
We made up the first example sentence for il-
lustrative purpose, however, the examples 2-3 are
picked up from the real test sets. According to
the human judgment, hypothesis 1 (H1) is better
than hypothesis 2 (H2); hypothesis 2 is better than
hypothesis 3 (H3) for each of these example sen-
tences. These results indicate that the combined
representation based metric better matches the hu-
man judgment than BLEU does.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978769230769">
We studied a series of translation evaluation
metrics based on three widely used representa-
tions. Experiments on the WMT metric task in-
dicate that the representation metrics obtain bet-
ter correlations with human judgment on both
system-level and segment-level, compared to pop-
ular translation evaluation metrics such as BLEU,
Meteor, Meteor Universal, and TER. Also, the
representation-based metrics use only monolin-
gual, unlabeled data for training; such data are
easy to obtain. Furthermore, the proposed metrics
are robust to various training conditions, such as
the data size and domain.
</bodyText>
<sectionHeader confidence="0.995538" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9818615">
The authors would like to thank Colin Cherry and
Roland Kuhn for useful discussions.
</bodyText>
<page confidence="0.975494">
154
</page>
<sectionHeader confidence="0.995394" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997166378947368">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
Colin Cherry and Hongyu Guo. 2015. The unreason-
able effectiveness of word representations for twit-
ter named entity recognition. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies.
Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, pages
376–380, Baltimore, Maryland, USA, June. Associ-
ation for Computational Linguistics.
G. Doddington. 2002. Authomatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Human
Language Technology Conference, page 128132,
San Diego, CA.
Chi-kiu Lo and Dekai Wu. 2011. Meant: An inexpen-
sive, high-accuracy, semi-automatic metric for eval-
uating translation utility based on semantic roles. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 220–229, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Matouˇs Mach´aˇcek and Ondˇrej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111–
3119.
George A. Miller. 1995. Wordnet: A lexical database
for english. Comunications of the ACM, 38:39–41.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318,
Philadelphia, July. ACL.
W. Press, S. Teukolsky, W. Vetterling, and B. Flannery.
2002. Numerical Recipes in C++. Cambridge Uni-
versity Press, Cambridge, UK.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation for Machine Transla-
tion in the Americas.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Ter-plus: Paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. In Machine Translation, volume 23, pages
117–127.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’11, pages 151–161,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts Potts. 2013. Recursive deep
models for semantic compositionality over a senti-
ment treebank. In EMNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In ACL, pages 384–394.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393–
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.956816">
155
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835369">
<title confidence="0.995358">Representation Based Translation Evaluation Metrics</title>
<author confidence="0.880625">Boxing Chen</author>
<author confidence="0.880625">Hongyu</author>
<affiliation confidence="0.946703">National Research Council</affiliation>
<email confidence="0.991107">first.last@nrc-cnrc.gc.ca</email>
<abstract confidence="0.99838365">Precisely evaluating the quality of a translation against human references is a challenging task due to the flexible word ordering of a sentence and the existence of a large number of synonyms for words. This paper proposes to evaluate translations with distributed representations of words and sentences. We study several metrics based on word and sentence representations and their combination. Experiments on the WMT metric task shows that the metric based on the combined representations achieves the best performance, outperforming the state-of-the-art translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1360" citStr="Banerjee and Lavie, 2005" startWordPosition="202" endWordPosition="205"> margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 Introduction Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used. For example, Meteor (Banerjee and Lavie, 2005)</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2628" citStr="Bengio et al., 2003" startWordPosition="396" endWordPosition="399"> al., 2009) and Meteor Universal (Denkowski and Lavie, 2014) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 2013; Mikolov et al., 2013). For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space. We propose to evaluate the translations with different word and sentence representations. Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder. In particular, to leverage the different advant</context>
<context position="5553" citStr="Bengio et al., 2003" startWordPosition="856" endWordPosition="859">e same length as the size of the vocabulary, and only one dimension that corresponds to the word is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been traditionally used for many NLP systems. On the other hand, recent years have witnessed that simply plugging in distributed word vectors as realvalued features is an effective way to improve a NLP system (Turian et al., 2010). 2.2 Distributed Word Representations Distributed word representations, also called word embeddings, map each word deterministically to a real-valued, dense vector (Bengio et al., 2003). A widely used approach for generating useful word vectors is developed by (Mikolov et al., 2013). This method scales very well to very large training corpora. Their skip-gram model, which we adopt here, learns word vectors that are good at predicting the words in a context window surrounding it. A very promising perspective of such distributed representation is that words that have similar contexts, and therefore similar syntactic and semantic properties, will tend to be near one another in the low-dimensional vector space. 2.3 Sentence Vector Representations Word level representation often </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Hongyu Guo</author>
</authors>
<title>The unreasonable effectiveness of word representations for twitter named entity recognition.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="4572" citStr="Cherry and Guo, 2015" startWordPosition="693" endWordPosition="696">hort Papers), pages 150–155, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Representations A representation, in the context of NLP, is a mathematical object associated with each word, sentence, or document. This object is typically a vector where each element’s value describes, to some degree, the semantic or syntactic properties of the associated word, sentence, or document. Using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an NLP system (Turian et al., 2010; Cherry and Guo, 2015). Our evaluation metrics are based on three widely used representations, as discussed next. 2.1 One-hot Representations Conventionally, a word is represented by a one-hot vector. In a one-hot representation, a vocabulary is first defined, and then each word in the vocabulary is assigned a symbolic ID. In this scenario, for each word, the feature vector has the same length as the size of the vocabulary, and only one dimension that corresponds to the word is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been traditionally used for many NL</context>
</contexts>
<marker>Cherry, Guo, 2015</marker>
<rawString>Colin Cherry and Hongyu Guo. 2015. The unreasonable effectiveness of word representations for twitter named entity recognition. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>376--380</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2069" citStr="Denkowski and Lavie, 2014" startWordPosition="313" endWordPosition="316">ely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used. For example, Meteor (Banerjee and Lavie, 2005) uses WordNet (Miller, 1995); TER-Plus (Snover et al., 2009) and Meteor Universal (Denkowski and Lavie, 2014) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 201</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Authomatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>128132</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="1298" citStr="Doddington, 2002" startWordPosition="194" endWordPosition="195">ng the state-of-the-art translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 Introduction Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Authomatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Human Language Technology Conference, page 128132, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Meant: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>220--229</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1419" citStr="Lo and Wu, 2011" startWordPosition="213" endWordPosition="216">y needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 Introduction Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used. For example, Meteor (Banerjee and Lavie, 2005) uses WordNet (Miller, 1995); TER-Plus (Snover et al., 2009</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. 2011. Meant: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 220–229, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouˇs Mach´aˇcek</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´aˇcek, Bojar, 2013</marker>
<rawString>Matouˇs Mach´aˇcek and Ondˇrej Bojar. 2013. Results of the WMT13 metrics shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, Nevada, United States.,</location>
<contexts>
<context position="2693" citStr="Mikolov et al., 2013" startWordPosition="408" endWordPosition="411">ploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 2013; Mikolov et al., 2013). For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space. We propose to evaluate the translations with different word and sentence representations. Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder. In particular, to leverage the different advantages and focuses, in terms of benefiting evaluation, of various r</context>
<context position="5651" citStr="Mikolov et al., 2013" startWordPosition="872" endWordPosition="875">is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been traditionally used for many NLP systems. On the other hand, recent years have witnessed that simply plugging in distributed word vectors as realvalued features is an effective way to improve a NLP system (Turian et al., 2010). 2.2 Distributed Word Representations Distributed word representations, also called word embeddings, map each word deterministically to a real-valued, dense vector (Bengio et al., 2003). A widely used approach for generating useful word vectors is developed by (Mikolov et al., 2013). This method scales very well to very large training corpora. Their skip-gram model, which we adopt here, learns word vectors that are good at predicting the words in a context window surrounding it. A very promising perspective of such distributed representation is that words that have similar contexts, and therefore similar syntactic and semantic properties, will tend to be near one another in the low-dimensional vector space. 2.3 Sentence Vector Representations Word level representation often cannot properly capture more complex linguistic phenomena in a sentence or multi-word phrase. Ther</context>
<context position="10300" citStr="Mikolov et al., 2013" startWordPosition="1623" endWordPosition="1626">. With these score equations, we then can formulate our five presentations based metrics as follows. For the one-hot representation metric, once we have the representations of the words and n-grams, we sum all the vectors to obtain the representation of the sentence. For efficiency, we only keep the entries which are not both zero in the reference and translation vectors. After we generate the two vectors for both translation and reference, we then compute the score using Equation 1. For the word embedding based metric, we first learn the word vector representation using the code provided by (Mikolov et al., 2013) 1. Next, following (Zou et al., 2013), we average the word embeddings of all words in the sentence to obtain the representation of the sentence. As discussed in Section 2.4, the three sentencelevel one-hot, word embedding and RAE representations have different strength when they are 1https://code.google.com/p/word2vec/ used to compare two sentences. In our metric here, each of the three vectors is first scaled with a particular weight (learned on dev data) and then the vectors are concatenated. With these concatenation vectors, we then calculate the similarity score using Equation 1. For comp</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 3111– 3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Comunications of the ACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="1988" citStr="Miller, 1995" startWordPosition="303" endWordPosition="304">ions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used. For example, Meteor (Banerjee and Lavie, 2005) uses WordNet (Miller, 1995); TER-Plus (Snover et al., 2009) and Meteor Universal (Denkowski and Lavie, 2014) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are c</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Comunications of the ACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia,</location>
<contexts>
<context position="1273" citStr="Papineni et al., 2002" startWordPosition="188" endWordPosition="192"> best performance, outperforming the state-of-the-art translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 Introduction Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionarie</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Press</author>
<author>S Teukolsky</author>
<author>W Vetterling</author>
<author>B Flannery</author>
</authors>
<title>Numerical Recipes in C++.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="11956" citStr="Press et al., 2002" startWordPosition="1885" endWordPosition="1888">de French, Spanish, German and Czech. For training the word embedding and recursive auto-encoder model, we used WMT 2013 training data 2. We compared our metrics with smoothed BLEU (mteval-v13a), TER 3, Meteor v1.0 4, and Meteor Universal (i.e. v1.5) 5. We used the default settings for all these four metrics. When considering the representation based metrics, we tuned all the parameters to maximize the system-level γ score for all representation based metrics on the dev sets. We tuned the weights for combining the three vectors automatically, using the downhill simplex method as described in (Press et al., 2002). The weights are 1 for the RAE vector, about 0.1 for the word embedding vector, and around 0.01 for the one-hot vector, respectively. We tuned other parameters manually. Specifically, we set n equal to 2 for the one-hot n-gram representation, the vector size of the recursive auto-encoder to 10, and the vector size of word embeddings to 80. Following WMT 2013’s metric task (Mach´aˇcek and Bojar, 2013), to measure the correlation with human judgment, we use Kendall’s rank correlation coefficient τ for the segment level, and Pearson’s correlation coefficient (γ in the below tables and figures) f</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2002</marker>
<rawString>W. Press, S. Teukolsky, W. Vetterling, and B. Flannery. 2002. Numerical Recipes in C++. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="1325" citStr="Snover et al., 2006" startWordPosition="197" endWordPosition="200">translation metrics by a large margin. In particular, training the distributed representations only needs a reasonable amount of monolingual, unlabeled data that is not necessary drawn from the test domain. 1 Introduction Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid comparisons between different systems and enable the tuning of parameter values during system training. Many machine translation metrics have been proposed in recent years, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), Meteor (Banerjee and Lavie, 2005) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used. For example</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Ter-plus: Paraphrase, semantic, and alignment enhancements to translation edit rate.</title>
<date>2009</date>
<booktitle>In Machine Translation,</booktitle>
<volume>23</volume>
<pages>117--127</pages>
<contexts>
<context position="2020" citStr="Snover et al., 2009" startWordPosition="306" endWordPosition="309">y (Lo and Wu, 2011), amongst others. Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward solution to improve the evaluation quality is to increase the number of various references. Nevertheless, it is expensive to create multiple references. In order to catch synonym matches between the translations and references, synonym dictionaries or paraphrasing tables have been used. For example, Meteor (Banerjee and Lavie, 2005) uses WordNet (Miller, 1995); TER-Plus (Snover et al., 2009) and Meteor Universal (Denkowski and Lavie, 2014) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et a</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Ter-plus: Paraphrase, semantic, and alignment enhancements to translation edit rate. In Machine Translation, volume 23, pages 117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2649" citStr="Socher et al., 2011" startWordPosition="400" endWordPosition="403">r Universal (Denkowski and Lavie, 2014) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 2013; Mikolov et al., 2013). For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space. We propose to evaluate the translations with different word and sentence representations. Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder. In particular, to leverage the different advantages and focuses, in </context>
<context position="6440" citStr="Socher et al., 2011" startWordPosition="989" endWordPosition="992"> context window surrounding it. A very promising perspective of such distributed representation is that words that have similar contexts, and therefore similar syntactic and semantic properties, will tend to be near one another in the low-dimensional vector space. 2.3 Sentence Vector Representations Word level representation often cannot properly capture more complex linguistic phenomena in a sentence or multi-word phrase. Therefore, we adopt an effective and efficient method for multiword phrase distributed representation, namely the greedy unsupervised recursive auto-encoder strategy (RAE) (Socher et al., 2011). This method works under an unsupervised setting. In particular, it does not rely on a parsing tree structure in order to generate sentence level vectors. This characteristic makes it very desirable for applying it to the outputs of machine translation systems. This is because the outputs of translation systems are often not syntactically correct sentences; parsing them is possible to introduce unexpected noise. For a given sentence, the greedy unsupervised RAE greedily searches a pair of words that results in minimal reconstruction error by an autoencoder. The corresponding hidden vector of </context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 151–161, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2670" citStr="Socher et al., 2013" startWordPosition="404" endWordPosition="407">i and Lavie, 2014) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, especially for those low resource languages. This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed representation maps each word or sentence to a continuous, low dimensional space, where words or sentences having similar syntactic and semantic properties are close to one another (Bengio et al., 2003; Socher et al., 2011; Socher et al., 2013; Mikolov et al., 2013). For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space. We propose to evaluate the translations with different word and sentence representations. Specifically, we investigate the use of three widely deployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive autoencoder. In particular, to leverage the different advantages and focuses, in terms of benefiting e</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="4549" citStr="Turian et al., 2010" startWordPosition="689" endWordPosition="692">anguage Processing (Short Papers), pages 150–155, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Representations A representation, in the context of NLP, is a mathematical object associated with each word, sentence, or document. This object is typically a vector where each element’s value describes, to some degree, the semantic or syntactic properties of the associated word, sentence, or document. Using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an NLP system (Turian et al., 2010; Cherry and Guo, 2015). Our evaluation metrics are based on three widely used representations, as discussed next. 2.1 One-hot Representations Conventionally, a word is represented by a one-hot vector. In a one-hot representation, a vocabulary is first defined, and then each word in the vocabulary is assigned a symbolic ID. In this scenario, for each word, the feature vector has the same length as the size of the vocabulary, and only one dimension that corresponds to the word is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been traditi</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semisupervised learning. In ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="10338" citStr="Zou et al., 2013" startWordPosition="1630" endWordPosition="1633">formulate our five presentations based metrics as follows. For the one-hot representation metric, once we have the representations of the words and n-grams, we sum all the vectors to obtain the representation of the sentence. For efficiency, we only keep the entries which are not both zero in the reference and translation vectors. After we generate the two vectors for both translation and reference, we then compute the score using Equation 1. For the word embedding based metric, we first learn the word vector representation using the code provided by (Mikolov et al., 2013) 1. Next, following (Zou et al., 2013), we average the word embeddings of all words in the sentence to obtain the representation of the sentence. As discussed in Section 2.4, the three sentencelevel one-hot, word embedding and RAE representations have different strength when they are 1https://code.google.com/p/word2vec/ used to compare two sentences. In our metric here, each of the three vectors is first scaled with a particular weight (learned on dev data) and then the vectors are concatenated. With these concatenation vectors, we then calculate the similarity score using Equation 1. For comparison, we also combine the strength o</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393– 1398, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>