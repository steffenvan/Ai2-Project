<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001486">
<title confidence="0.9978765">
FBK-HLT: An Application of Semantic Textual Similarity for Answer
Selection in Community Question Answering
</title>
<author confidence="0.990638">
Ngoc Phuoc An Vo
</author>
<affiliation confidence="0.710469666666667">
University of Trento,
Fondazione Bruno Kessler
Trento, Italy
</affiliation>
<email confidence="0.995771">
ngoc@fbk.eu
</email>
<author confidence="0.99765">
Simone Magnolini
</author>
<affiliation confidence="0.710679666666667">
University of Brescia,
Fondazione Bruno Kessler
Trento, Italy
</affiliation>
<email confidence="0.996028">
magnolini@fbk.eu
</email>
<author confidence="0.526498">
Octavian Popescu
</author>
<affiliation confidence="0.280868">
IBM Research, T.J. Watson
Yorktown, US
</affiliation>
<email confidence="0.994077">
o.popescu@us.ibm.com
</email>
<sectionHeader confidence="0.995567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937666666667">
This paper reports the description and perfor-
mance of our system, FBK-HLT, participating
in the SemEval 2015, Task #3 &amp;quot;Answer Se-
lection in Community Question Answering&amp;quot;
for English, for both subtasks. We submit two
runs with different classifiers in combining typ-
ical features (lexical similarity, string similar-
ity, word n-grams, etc.) with machine transla-
tion evaluation metrics and with some ad hoc
features (e.g user overlapping, spam filtering).
We outperform the baseline system and achieve
interesting results on both subtasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999831076923077">
Answer selection is an important task inside the wider
task of question answering that represents at the mo-
ment a topic of great interest for research and busi-
ness as well. Analyzing social data like answers
given inside a forum is a way to maximize the value
of this type of knowledge source that is usually af-
fected by a very noisy information due to out of topic
spam, double posting, cross posting or other issues.
Recognizing useful posts from bad ones, and auto-
matically detecting the main polarity of answers to
a given question is a way to treat an amount of data
that otherwise might be difficult to handle.
A promising way to provide insight into these ques-
tions was brought forward as Shared Task #3 in the
SemEval-2015 campaign for &amp;quot;Answer Selection in
Community Question Answering&amp;quot; (Màrquez et al.,
2015) for English and Arabic languages. In the Sub-
task A, each system is given a set of questions in
which each one contains some data like posting date,
author’s Id, a set of comments, at least one, but usu-
ally more; then the participating the system has to
classify comments as good, bad or potential accord-
ing to their relevance with the question. In Subtask B,
a subset of these questions are predefined as yes/no
questions, system has to classify them into yes, no or
unsure classes based on the individual good answers.
We participate in this shared task (only in English)
with a system composing several different features
using a multiclass classifier. We are interested in
finding out whether similarity, machine translation
evaluation metrics and task specific techniques could
increase the accuracy of our system. In this paper,
we outline our method and present the results for
the answer selection task; the paper is organized as
follows: Section 2 presents the System Description,
Section 3 describes the Experiment Settings, Section
4 reports the Evaluations, Section 5 is the Error Anal-
ysis and finally, Section 6 presents the Conclusions
and Future Work.
</bodyText>
<sectionHeader confidence="0.973768" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999872909090909">
In order to build our system, we extract and adopt
several different linguistic features from a Semantic
Textual Similarity (STS) system (Vo et al., 2015) and
then consolidate them by a multiclass classifier. Dif-
ferent features can be used independently or together
with others to measure the semantic similarity and
recognize the paraphrase of a given sentence pair as
well as to evaluate the significance of each feature
to the accuracy of system’s predictions. Hence, the
system is expandable and scalable for adopting more
useful features aiming for improving the accuracy.
</bodyText>
<page confidence="0.967745">
231
</page>
<note confidence="0.5107705">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 231–235,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.990489">
2.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999699105263158">
As data preprocessing is a crucial step for preparing
useful information to be learned by the system, we
focus the beginning of our work trying to simplify
data without losing information. Our system is based
on semantic similarity, so it needs pairs of sentences
to compare; we pair-up every question with all of its
comments, one by one, e.g. a question with five com-
ments becomes five pairs of sentences composed by
the question and five different comments. Questions
and comments are composed by subject and body,
so for questions, we merge the subject and body to-
gether if the subject does not occur inside the body;
and for comments, we also check if the comment’s
subject is not identical to question’s subject with the
prefix RE:. As the forum data also contains lot of
informal writing, we normalize them by applying a
simple filter that substitutes common abbreviation:
&amp;quot;u - you&amp;quot;; &amp;quot;r - are&amp;quot;; &amp;quot;ur - your&amp;quot;; &amp;quot;Iam - I am&amp;quot;; &amp;quot;any1 -
anyone&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.999166">
2.2 Syntactic Generalization
</subsectionHeader>
<bodyText confidence="0.999981">
Given a pair of parse trees, the Syntactic Generaliza-
tion (SG) (Galitsky, 2013) finds a set of maximal com-
mon subtrees. The toolkit &amp;quot;relevance-based-on-parse-
trees&amp;quot; is an open-source project, which evaluates text
relevance by using syntactic, parse-tree-based simi-
larity measure.1 It measures the similarity between
two sentences by finding a set of maximal common
subtree for a pair of parse trees, using representa-
tion of constituency parse trees via chunking. Each
type of phrases (NP, VP, PRP etc.) will be aligned
and subject to generalization. It uses the OpenNLP
system to derive constituent trees for generalization
(chunker and parser).2 As it is an unsupervised ap-
proach, we apply the tool directly to the preprocessed
texts to compute the similarity of syntactic structure
of sentence pairs.
</bodyText>
<sectionHeader confidence="0.597609" genericHeader="method">
2.3 Machine Learning Evaluation Metric -
METEOR
</sectionHeader>
<bodyText confidence="0.999825">
We also use evaluation metrics for machine transla-
tion as suggested in (Madnani et al., 2012) for para-
phrase recognition on Microsoft Research paraphrase
corpus (MSRP) (Dolan et al., 2004). In machine
</bodyText>
<footnote confidence="0.9999115">
1https://code.google.com/p/relevance-based-on-parse-trees/
2https://opennlp.apache.org
</footnote>
<bodyText confidence="0.999361866666666">
translation, the evaluation metric scores the hypothe-
ses by aligning them to one or more reference trans-
lations. We take into consideration to use all the
eight metrics proposed, but we find that adding some
of them without a careful process of training on the
dataset may decrease the performance of the system.
We use the latest version of METEOR (Denkowski
and Lavie, 2014) that finds alignments between sen-
tences based on exact, stem, synonym and paraphrase
matches between words and phrases. We used the
system as distributed on its website, using only the
&amp;quot;norm&amp;quot; option that tokenizes and normalizes punctu-
ation and lowercase as suggested by documentation.3
We compute the word alignment scores between ques-
tions and comments.
</bodyText>
<subsectionHeader confidence="0.948486">
2.4 Weighted Matrix Factorization (WMF)
</subsectionHeader>
<bodyText confidence="0.999931111111111">
WMF (Guo and Diab, 2012) is a dimension reduction
model to extract nuanced and robust latent vectors
for short texts/sentences. To overcome the sparsity
problem in short texts/sentences (e.g. 10 words on
average), the missing words, a feature that Latent Se-
mantic Analysis (LSA) and Latent Dirichlet Alloca-
tion (LDA) typically overlook, is explicitly modeled.
We use the pipeline to compute the similarity scores
for question-comment pairs.4
</bodyText>
<subsectionHeader confidence="0.980105">
2.5 User Overlapping
</subsectionHeader>
<bodyText confidence="0.999994">
We extract a simple binary feature focused on com-
ment’s author. We suppose that question’s author is
not usually as same as comment’s author, so if a ques-
tion has one or more comments associated with the
same question’s author, these comments are probably
descriptions or explanations about the question. We
label 1 for comments made by the same question’s
author and 0 otherwise.
</bodyText>
<subsectionHeader confidence="0.985244">
2.6 Spam Filtering - JFilter
</subsectionHeader>
<bodyText confidence="0.999948875">
Recognizing good comments from bad comment is
a task somehow similar to spam filtering, to capture
this feature, we use a Java implementation, Jfilter
(Francesco Saverio Profiti, 2007), based on a fuzzy
version of the Rocchio algorithm (Rocchio, 1971).
This system uses a classifier that needs training, so
to avoid overfitting, from the training and develop-
ment datasets, we randomly choose a subset of good
</bodyText>
<footnote confidence="0.9999005">
3http://www.cs.cmu.edu/%7Ealavie/METEOR/index.html
4http://www.cs.columbia.edu/%7Eweiwei/code.html
</footnote>
<page confidence="0.917336">
232
</page>
<table confidence="0.9991906">
Accuracy F1 (G) F1 (B) F1 (D) F1 (P) F1 (NE) F1 (O) F1 WM
Baseline 53.19 0.694 0 0 0 0 0 0.369
1-against-all 60.06 0.731 0.189 0.545 0 0 0 0.523
Random Correction Code 59.02 0.722 0.319 0.540 0 0 0 0.539
Exhausted Correction Code 60.00 0.731 0.18 0.545 0 0 0 0.521
</table>
<tableCaption confidence="0.897193">
Table 1: Result obtained using different classification algorithms for Subtask A (G good; B bad; D dialog; P potential;
NE not-English; O other; WM Weighted Mean) on Development dataset.
</tableCaption>
<table confidence="0.999438285714286">
Accuracy F1 (Yes) F1 (No) F1 (Unsure) F1 (Not-Applicable) F1 WM
Standard Features 44.4444 0.316 0 0.077 0.593 0.355
Standard Features 45.4444 0.327 0 0.08 0.589 0.358
+ Subtask A output
Standard Features
+ Subtask A 70.7071 0.667 0 0.069 1 0.635
gold-standard labels
</table>
<tableCaption confidence="0.999598">
Table 2: Subtask B system performances on Development dataset.
</tableCaption>
<bodyText confidence="0.999503625">
comments to use as non-spam dataset; in contrast, we
select a subset of bad and potential to use as spam
dataset to train JFilter. This configuration was used
to train our system during development; for the final
run with test dataset, we train JFilter with both devel-
opment and training datasets. JFilter gives a binary
judgment (HAM or SPAM) which is used as a feature
for our system in Subtask A.
</bodyText>
<sectionHeader confidence="0.995378" genericHeader="method">
3 Experiment Settings
</sectionHeader>
<bodyText confidence="0.999841294117647">
We use the machine learning toolkit WEKA (Hall et
al., 2009) to obtain robust and efficient implemen-
tation of different classifiers, as well as to reduce
develop time of the system. For Subtask A, we build
one model using all the features described in Section
2. Table 1 reports some experiments in which we
select a good classifier to optimize both the Accuracy
and F1-score of the system. During the development,
we select the default implementation &amp;quot;1-against-all&amp;quot;
classification algorithm (with logistic regression) for
both subtasks.
For Subtask B, we make some modifications to the
system due to some important differences between
two subtasks. As the question classification depends
on the quality of its comments, we substitute the
spam filtering feature by the comments’ labels from
Subtask A system’s output. In order to examine this
hypothesis, we firstly use the gold-standard labels of
comments from Subtask A as a feature for the ques-
tion classification in Subtask B. The high Accuracy
and F1-score from this setting proves our hypothesis
correct. To avoid the overfitting, we again use only
the label predictions from Subtask A as a feature for
our Subtask B system. Table 2 shows that a precise
output from Subtask A can significantly benefit the
performance of Subtask B system.
As Subtask B does not focus on comment labeling,
but question labeling, to achieve this purpose after
classifying all comments as yes, no, unsure or Not
Applicable, we simply aggregate comments of every
question with a majority vote. We label a question as
yes if the majority of its comments are classified as
yes, the same for no; if there no major judgment of
either yes or no, the question is classified as unsure.
</bodyText>
<table confidence="0.7670776">
Subtask A Subtask B
Mac F1 Acc Mac F1
JAIST 57.19 72.52
VectorSlu 63.7
FBK-HLT 47.32 69.13 27.8
</table>
<tableCaption confidence="0.998442">
Table 3: Evaluation Results on Subtasks A and B.
</tableCaption>
<figure confidence="0.96285625">
Team
Acc
72.0
40.0
</figure>
<page confidence="0.994725">
233
</page>
<table confidence="0.9998208">
Team Accuracy F1 (G) F1 (B) F1 (D) F1 (P) Macro F1
JAIST (3-classes) 72.67 79.11 78.29 0 14.48 57.29
HLT-FBK (3-classes) 69.13 75.80 66.15 0 0 47.32
JAIST (4-classes) 59.62 76.52 40.38 57.21 18.41 48.13
HLT-FBK (4-classes) 62.40 75.80 43.42 51.23 0 42.61
</table>
<tableCaption confidence="0.90893">
Table 4: Subtask A - Comparison with best system for 3-classes and 4-classes evaluation (G good; B bad; D dialog; P
potential; Macro F1).
</tableCaption>
<table confidence="0.99981">
Team Accuracy F1 (Yes) F1 (No) F1 (Unsure) Macro F1
VectorSlu 72.0 83.87 57.14 50.0 63.67
FBK-HLT 40.0 50.0 0.0 33.33 27.78
</table>
<tableCaption confidence="0.999532">
Table 5: Subtask B - Comparison with best system.
</tableCaption>
<sectionHeader confidence="0.994055" genericHeader="method">
4 Evaluations
</sectionHeader>
<bodyText confidence="0.998783555555556">
We submit only one run for both subtasks (English
language) using the &amp;quot;1-against-all&amp;quot; classification al-
gorithms. In Subtask A, we achieve good results,
especially, we are ranked 4th out of 12 teams in Ac-
curacy. In Subtask B, as we only apply the simple
approach &amp;quot;majority vote&amp;quot;, the result is reasonable as
expected. Table 3 shows our performance in both
subtasks in regard to the best systems, both in Macro
F1 and Accuracy measures.
</bodyText>
<sectionHeader confidence="0.998068" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.99998728125">
In this section, we conduct an analysis of our sys-
tem’s performance on test dataset. In Subtask A,
our analysis consists of some comparison between
our system and the best system, JAIST. According
to results in Table 4, for the evaluation on 3-classes
(good, bad, and potential), our system is dramati-
cally penalized by low performance on detecting bad
comments, besides, it is not able to classify the po-
tential ones. This particular class of comments is
very small in training dataset. There are 50.45% for
good comments, 41.09% for bad and only 8.25% for
potential. During the development, as we decide to
optimize the Accuracy and F1 weighted on the num-
ber of comments, this decision misleads our system
to ignore this small class. Hence, in order to improve
the system performance, we may need to search for
a specific feature for potential comments like what
we did with user overlapping for dialog ones. For the
evaluation on 4-classes (good, bad, dialog and po-
tential), our system performance rises significantly,
our system shows a good capability to distinguish
between dialog and other comments.
In Subtask B, the performance comparison in Ta-
ble 5 shows that our system achieves reasonable per-
formance on the Yes and Unsure classes, but has no
capability to capture the No class. Moreover, most
of the instances of No class have been misclassified
as Unsure class. This shows an unclear separation
between these two classes which confuses the system.
Thus, to fix this issue, we need to find more specific
features which may help to distinguish the No class
and others.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999920117647059">
In this paper, we describe our system participating
in the SemEval 2015, Task #3 &amp;quot;Answer Selection
in Community Question Answering&amp;quot; in English, for
both subtasks. We present a supervised system which
considers multiple linguistic features such as lexical,
string and some task-specific features. Our perfor-
mance is much above the baseline and shows some
interesting properties in specific scenarios. We also
show some error analysis in which we investigate
the limit and drawback of our system on specific
comment and question classes.
For future work, we expect to study to exploit more
useful features, especially, task-related features, to
improve the classification performance on potential
labeled comments and No labeled questions, which
will lead to a significant improvement of the overall
performance.
</bodyText>
<page confidence="0.996719">
234
</page>
<sectionHeader confidence="0.995867" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777930232558">
Michael Denkowski and Alon Lavie. 2014. Meteor uni-
versal: Language specific translation evaluation for any
target language. In Proceedings of the EACL 2014
Workshop on Statistical Machine Translation.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, page 350.
Claudio Biancalana Francesco Saverio Profiti. 2007. Jfil-
ter: un filtro antispam intelligente in java. Mokabyte,
(124). in Italian.
Boris Galitsky. 2013. Machine learning of syntactic parse
trees for search and classification of text. Engineer-
ing Applications of Artificial Intelligence, 26(3):1072–
1091.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 864–872.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten. 2009.
The weka data mining software: an update. ACM
SIGKDD explorations newsletter, 11(1):10–18.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 182–190.
Lluís Màrquez, James Glass, Walid Magdy, Alessandro
Moschitti, Preslav Nakov, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in community
question answering. In Proceedings of the 9th Inter-
national Workshop on Semantic Evaluation (SemEval
2015).
Joseph John Rocchio. 1971. Relevance feedback in infor-
mation retrieval.
Ngoc Phuoc An Vo, Simone Magnolini, and Octavian
Popescu. 2015. FBK-HLT: A new framework for
semantic textual similarity. In Proceedings of the
9th International Workshop on Semantic Evaluation
(SemEval-2015), Denver, US.
</reference>
<page confidence="0.998522">
235
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.090101">
<title confidence="0.9984105">FBK-HLT: An Application of Semantic Textual Similarity for Selection in Community Question Answering</title>
<author confidence="0.928828">Ngoc Phuoc An</author>
<affiliation confidence="0.87297">University of Fondazione Bruno</affiliation>
<address confidence="0.563121">Trento,</address>
<email confidence="0.995669">ngoc@fbk.eu</email>
<author confidence="0.975497">Simone</author>
<affiliation confidence="0.837339666666667">University of Fondazione Bruno Trento,</affiliation>
<email confidence="0.993749">magnolini@fbk.eu</email>
<author confidence="0.713013">Octavian</author>
<affiliation confidence="0.77952">IBM Research, T.J. Yorktown,</affiliation>
<email confidence="0.999864">o.popescu@us.ibm.com</email>
<abstract confidence="0.999640923076923">This paper reports the description and performance of our system, FBK-HLT, participating in the SemEval 2015, Task #3 &amp;quot;Answer Selection in Community Question Answering&amp;quot; for English, for both subtasks. We submit two runs with different classifiers in combining typical features (lexical similarity, string similarity, word n-grams, etc.) with machine translation evaluation metrics and with some ad hoc features (e.g user overlapping, spam filtering). We outperform the baseline system and achieve interesting results on both subtasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="6212" citStr="Denkowski and Lavie, 2014" startWordPosition="980" endWordPosition="983">hine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine 1https://code.google.com/p/relevance-based-on-parse-trees/ 2https://opennlp.apache.org translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &amp;quot;norm&amp;quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.3 We compute the word alignment scores between questions and comments. 2.4 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<contexts>
<context position="5736" citStr="Dolan et al., 2004" startWordPosition="912" endWordPosition="915"> of constituency parse trees via chunking. Each type of phrases (NP, VP, PRP etc.) will be aligned and subject to generalization. It uses the OpenNLP system to derive constituent trees for generalization (chunker and parser).2 As it is an unsupervised approach, we apply the tool directly to the preprocessed texts to compute the similarity of syntactic structure of sentence pairs. 2.3 Machine Learning Evaluation Metric - METEOR We also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine 1https://code.google.com/p/relevance-based-on-parse-trees/ 2https://opennlp.apache.org translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Biancalana</author>
</authors>
<title>Francesco Saverio Profiti.</title>
<date>2007</date>
<journal>Mokabyte,</journal>
<volume>124</volume>
<note>in Italian.</note>
<marker>Biancalana, 2007</marker>
<rawString>Claudio Biancalana Francesco Saverio Profiti. 2007. Jfilter: un filtro antispam intelligente in java. Mokabyte, (124). in Italian.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Galitsky</author>
</authors>
<title>Machine learning of syntactic parse trees for search and classification of text.</title>
<date>2013</date>
<journal>Engineering Applications of Artificial Intelligence,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>1091</pages>
<contexts>
<context position="4777" citStr="Galitsky, 2013" startWordPosition="765" endWordPosition="766">e different comments. Questions and comments are composed by subject and body, so for questions, we merge the subject and body together if the subject does not occur inside the body; and for comments, we also check if the comment’s subject is not identical to question’s subject with the prefix RE:. As the forum data also contains lot of informal writing, we normalize them by applying a simple filter that substitutes common abbreviation: &amp;quot;u - you&amp;quot;; &amp;quot;r - are&amp;quot;; &amp;quot;ur - your&amp;quot;; &amp;quot;Iam - I am&amp;quot;; &amp;quot;any1 - anyone&amp;quot;. 2.2 Syntactic Generalization Given a pair of parse trees, the Syntactic Generalization (SG) (Galitsky, 2013) finds a set of maximal common subtrees. The toolkit &amp;quot;relevance-based-on-parsetrees&amp;quot; is an open-source project, which evaluates text relevance by using syntactic, parse-tree-based similarity measure.1 It measures the similarity between two sentences by finding a set of maximal common subtree for a pair of parse trees, using representation of constituency parse trees via chunking. Each type of phrases (NP, VP, PRP etc.) will be aligned and subject to generalization. It uses the OpenNLP system to derive constituent trees for generalization (chunker and parser).2 As it is an unsupervised approach</context>
</contexts>
<marker>Galitsky, 2013</marker>
<rawString>Boris Galitsky. 2013. Machine learning of syntactic parse trees for search and classification of text. Engineering Applications of Artificial Intelligence, 26(3):1072– 1091.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>864--872</pages>
<contexts>
<context position="6633" citStr="Guo and Diab, 2012" startWordPosition="1045" endWordPosition="1048"> but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &amp;quot;norm&amp;quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.3 We compute the word alignment scores between questions and comments. 2.4 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on average), the missing words, a feature that Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) typically overlook, is explicitly modeled. We use the pipeline to compute the similarity scores for question-comment pairs.4 2.5 User Overlapping We extract a simple binary feature focused on comment’s author. We suppose that question’s author is not usually as same as comment’s author, so if a question ha</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="9258" citStr="Hall et al., 2009" startWordPosition="1469" endWordPosition="1472"> Features + Subtask A 70.7071 0.667 0 0.069 1 0.635 gold-standard labels Table 2: Subtask B system performances on Development dataset. comments to use as non-spam dataset; in contrast, we select a subset of bad and potential to use as spam dataset to train JFilter. This configuration was used to train our system during development; for the final run with test dataset, we train JFilter with both development and training datasets. JFilter gives a binary judgment (HAM or SPAM) which is used as a feature for our system in Subtask A. 3 Experiment Settings We use the machine learning toolkit WEKA (Hall et al., 2009) to obtain robust and efficient implementation of different classifiers, as well as to reduce develop time of the system. For Subtask A, we build one model using all the features described in Section 2. Table 1 reports some experiments in which we select a good classifier to optimize both the Accuracy and F1-score of the system. During the development, we select the default implementation &amp;quot;1-against-all&amp;quot; classification algorithm (with logistic regression) for both subtasks. For Subtask B, we make some modifications to the system due to some important differences between two subtasks. As the qu</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="5641" citStr="Madnani et al., 2012" startWordPosition="898" endWordPosition="901">tences by finding a set of maximal common subtree for a pair of parse trees, using representation of constituency parse trees via chunking. Each type of phrases (NP, VP, PRP etc.) will be aligned and subject to generalization. It uses the OpenNLP system to derive constituent trees for generalization (chunker and parser).2 As it is an unsupervised approach, we apply the tool directly to the preprocessed texts to compute the similarity of syntactic structure of sentence pairs. 2.3 Machine Learning Evaluation Metric - METEOR We also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine 1https://code.google.com/p/relevance-based-on-parse-trees/ 2https://opennlp.apache.org translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments betwee</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Màrquez</author>
<author>James Glass</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Bilal Randeree</author>
</authors>
<title>Semeval-2015 task 3: Answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1747" citStr="Màrquez et al., 2015" startWordPosition="269" endWordPosition="272">ike answers given inside a forum is a way to maximize the value of this type of knowledge source that is usually affected by a very noisy information due to out of topic spam, double posting, cross posting or other issues. Recognizing useful posts from bad ones, and automatically detecting the main polarity of answers to a given question is a way to treat an amount of data that otherwise might be difficult to handle. A promising way to provide insight into these questions was brought forward as Shared Task #3 in the SemEval-2015 campaign for &amp;quot;Answer Selection in Community Question Answering&amp;quot; (Màrquez et al., 2015) for English and Arabic languages. In the Subtask A, each system is given a set of questions in which each one contains some data like posting date, author’s Id, a set of comments, at least one, but usually more; then the participating the system has to classify comments as good, bad or potential according to their relevance with the question. In Subtask B, a subset of these questions are predefined as yes/no questions, system has to classify them into yes, no or unsure classes based on the individual good answers. We participate in this shared task (only in English) with a system composing se</context>
</contexts>
<marker>Màrquez, Glass, Magdy, Moschitti, Nakov, Randeree, 2015</marker>
<rawString>Lluís Màrquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. 2015. Semeval-2015 task 3: Answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph John Rocchio</author>
</authors>
<date>1971</date>
<note>Relevance feedback in information retrieval.</note>
<contexts>
<context position="7733" citStr="Rocchio, 1971" startWordPosition="1220" endWordPosition="1221">omment’s author. We suppose that question’s author is not usually as same as comment’s author, so if a question has one or more comments associated with the same question’s author, these comments are probably descriptions or explanations about the question. We label 1 for comments made by the same question’s author and 0 otherwise. 2.6 Spam Filtering - JFilter Recognizing good comments from bad comment is a task somehow similar to spam filtering, to capture this feature, we use a Java implementation, Jfilter (Francesco Saverio Profiti, 2007), based on a fuzzy version of the Rocchio algorithm (Rocchio, 1971). This system uses a classifier that needs training, so to avoid overfitting, from the training and development datasets, we randomly choose a subset of good 3http://www.cs.cmu.edu/%7Ealavie/METEOR/index.html 4http://www.cs.columbia.edu/%7Eweiwei/code.html 232 Accuracy F1 (G) F1 (B) F1 (D) F1 (P) F1 (NE) F1 (O) F1 WM Baseline 53.19 0.694 0 0 0 0 0 0.369 1-against-all 60.06 0.731 0.189 0.545 0 0 0 0.523 Random Correction Code 59.02 0.722 0.319 0.540 0 0 0 0.539 Exhausted Correction Code 60.00 0.731 0.18 0.545 0 0 0 0.521 Table 1: Result obtained using different classification algorithms for Sub</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>Joseph John Rocchio. 1971. Relevance feedback in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Phuoc An Vo</author>
<author>Simone Magnolini</author>
<author>Octavian Popescu</author>
</authors>
<title>FBK-HLT: A new framework for semantic textual similarity.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval-2015),</booktitle>
<location>Denver, US.</location>
<contexts>
<context position="3089" citStr="Vo et al., 2015" startWordPosition="489" endWordPosition="492">on evaluation metrics and task specific techniques could increase the accuracy of our system. In this paper, we outline our method and present the results for the answer selection task; the paper is organized as follows: Section 2 presents the System Description, Section 3 describes the Experiment Settings, Section 4 reports the Evaluations, Section 5 is the Error Analysis and finally, Section 6 presents the Conclusions and Future Work. 2 System Description In order to build our system, we extract and adopt several different linguistic features from a Semantic Textual Similarity (STS) system (Vo et al., 2015) and then consolidate them by a multiclass classifier. Different features can be used independently or together with others to measure the semantic similarity and recognize the paraphrase of a given sentence pair as well as to evaluate the significance of each feature to the accuracy of system’s predictions. Hence, the system is expandable and scalable for adopting more useful features aiming for improving the accuracy. 231 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 231–235, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational L</context>
</contexts>
<marker>Vo, Magnolini, Popescu, 2015</marker>
<rawString>Ngoc Phuoc An Vo, Simone Magnolini, and Octavian Popescu. 2015. FBK-HLT: A new framework for semantic textual similarity. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval-2015), Denver, US.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>