<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.989071">
Measuring Sentiment Annotation Complexity of Text
</title>
<author confidence="0.9855465">
Aditya Joshi1,2,3∗ Abhijit Mishra1 Nivvedan Senthamilselvan1
Pushpak Bhattacharyya1
</author>
<affiliation confidence="0.9166695">
1IIT Bombay, India, 2Monash University, Australia
3IITB-Monash Research Academy, India
</affiliation>
<email confidence="0.992551">
{adityaj, abhijitmishra, nivvedan, pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.97268932">
The effort required for a human annota-
tor to detect sentiment is not uniform for
all texts, irrespective of his/her expertise.
We aim to predict a score that quantifies
this effort, using linguistic properties of
the text. Our proposed metric is called
Sentiment Annotation Complexity (SAC).
As for training data, since any direct judg-
ment of complexity by a human annota-
tor is fraught with subjectivity, we rely on
cognitive evidence from eye-tracking. The
sentences in our dataset are labeled with
SAC scores derived from eye-fixation du-
ration. Using linguistic features and anno-
tated SACs, we train a regressor that pre-
dicts the SAC with a best mean error rate of
22.02% for five-fold cross-validation. We
also study the correlation between a hu-
man annotator’s perception of complexity
and a machine’s confidence in polarity de-
termination. The merit of our work lies in
(a) deciding the sentiment annotation cost
in, for example, a crowdsourcing setting,
(b) choosing the right classifier for senti-
ment prediction.
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996973537037038">
The effort required by a human annotator to de-
tect sentiment is not uniform for all texts. Com-
pare the hypothetical tweet “Just what I wanted: a
good pizza.” with “Just what I wanted: a cold
pizza.”. The two are lexically and structurally
similar. However, because of the sarcasm in the
second tweet (in “cold” pizza, an undesirable sit-
uation followed by a positive sentiment phrase
“just what I wanted”, as discussed in Riloff et al.
(2013)), it is more complex than the first for senti-
ment annotation. Thus, independent of how good
-∗ Aditya is funded by the TCS Research Fellowship Pro-
gram.
the annotator is, there are sentences which will be
perceived to be more complex than others. With
regard to this, we introduce a metric called senti-
ment annotation complexity (SAC). The SAC of a
given piece of text (sentences, in our case) can be
predicted using the linguistic properties of the text
as features.
The primary question is whether such complex-
ity measurement is necessary at all. Fort et al
(2012) describe the necessity of annotation com-
plexity measurement in manual annotation tasks.
Measuring annotation complexity is beneficial in
annotation crowdsourcing. If the complexity of
the text can be estimated even before the annota-
tion begins, the pricing model can be fine-tuned
(pay less for sentences that are easy to annotate,
for example). Also, in terms of an automatic SA
engine which has multiple classifiers in its ensem-
ble, a classifier may be chosen based on the com-
plexity of sentiment annotation (for example, use
a rule-based classifier for simple sentences and a
more complex classifier for other sentences). Our
metric adds value to sentiment annotation and sen-
timent analysis, in these two ways. The fact that
sentiment expression may be complex is evident
from a study of comparative sentences by Gana-
pathibhotla and Liu (2008), sarcasm by Riloff et
al. (2013), thwarting by Ramteke et al. (2013) or
implicit sentiment by Balahur et al. (2011). To
the best of our knowledge, there is no general ap-
proach to “measure” how complex a piece of text
is, in terms of sentiment annotation.
The central challenge here is to annotate a data
set with SAC. To measure the “actual” time spent
by an annotator on a piece of text, we use an eye-
tracker to record eye-fixation duration: the time
for which the annotator has actually focused on
the sentence during annotation. Eye-tracking an-
notations have been used to study the cognitive as-
pects of language processing tasks like translation
by Dragsted (2010) and sense disambiguation by
</bodyText>
<page confidence="0.97901">
36
</page>
<bodyText confidence="0.907086071428571">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36–41,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Joshi et al. (2011). Mishra et al. (2013) present a
technique to determine translation difficulty index.
The work closest to ours is by Scott et al. (2011)
who use eye-tracking to study the role of emotion
words in reading.
The novelty of our work is three-fold: (a) The
proposition of a metric to measure complexity of
sentiment annotation, (b) The adaptation of past
work that uses eye-tracking for NLP in the con-
text of sentiment annotation, (c) The learning of
regressors that automatically predict SAC using
linguistic features.
</bodyText>
<sectionHeader confidence="0.9752275" genericHeader="introduction">
2 Understanding Sentiment Annotation
Complexity
</sectionHeader>
<bodyText confidence="0.999953038461538">
The process of sentiment annotation consists of
two sub-processes: comprehension (where the an-
notator understands the content) and sentiment
judgment (where the annotator identifies the sen-
timent). The complexity in sentiment annotation
stems from an interplay of the two and we expect
SAC to capture the combined complexity of both
the sub-processes. In this section, we describe
how complexity may be introduced in sentiment
annotation in different classical layers of NLP.
The simplest form of sentiment annotation com-
plexity is at the lexical level. Consider the sen-
tence “It is messy, uncouth, incomprehensible, vi-
cious and absurd”. The sentiment words used
in this sentence are uncommon, resulting in com-
plexity.
The next level of sentiment annotation com-
plexity arises due to syntactic complexity. Con-
sider the review: “A somewhat crudely con-
structed but gripping, questing look at a person so
racked with self-loathing, he becomes an enemy to
his own race.”. An annotator will face difficulty
in comprehension as well as sentiment judgment
due to the complicated phrasal structure in this re-
view. Implicit expression of sentiment introduces
complexity at the semantic and pragmatic level.
Sarcasm expressed in “It’s like an all-star salute to
disney’s cheesy commercialism” leads to difficulty
in sentiment annotation because of positive words
like “an all-star salute”.
Manual annotation of complexity scores may
not be intuitive and reliable. Hence, we use a cog-
nitive technique to create our annotated dataset.
The underlying idea is: if we monitor annotation
of two textual units of equal length, the more com-
plex unit will take longer to annotate, and hence,
should have a higher SAC. Using the idea of “an-
notation time” linked with complexity, we devise a
technique to create a dataset annotated with SAC.
It may be thought that inter-annotator agree-
ment (IAA) provides implicit annotation: the
higher the agreement, the easier the piece of text
is for sentiment annotation. However, in case of
multiple expert annotators, this agreement is ex-
pected to be high for most sentences, due to the
expertise. For example, all five annotators agree
with the label for 60% sentences in our data set.
However, the duration for these sentences has a
mean of 0.38 seconds and a standard deviation of
0.27 seconds. This indicates that although IAA is
easy to compute, it does not determine sentiment
annotation complexity of text in itself.
</bodyText>
<sectionHeader confidence="0.9825905" genericHeader="method">
3 Creation of dataset annotated with
SAC
</sectionHeader>
<bodyText confidence="0.999950555555556">
We wish to predict sentiment annotation complex-
ity of the text using a supervised technique. As
stated above, the time-to-annotate is one good can-
didate. However, “simple time measurement” is
not reliable because the annotator may spend time
not doing any annotation due to fatigue or distrac-
tion. To accurately record the time, we use an
eye-tracking device that measures the “duration of
eye-fixations1”. Another attribute recorded by the
eye-tracker that may have been used is “saccade
duration2”. However, saccade duration is not sig-
nificant for annotation of short text, as in our case.
Hence, the SAC labels of our dataset are fixation
durations with appropriate normalization.
It may be noted that the eye-tracking device is
used only to annotate training data. The actual
prediction of SAC is done using linguistic features
alone.
</bodyText>
<subsectionHeader confidence="0.9999">
3.1 Eye-tracking Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999921888888889">
We use a sentiment-annotated data set consisting
of movie reviews by (Pang and Lee, 2005) and
tweets from http://help.sentiment140.
com/for-students. A total of 1059 sen-
tences (566 from a movie corpus, 493 from a twit-
ter corpus) are selected.
We then obtain two kinds of annotation from
five paid annotators: (a) sentiment (positive, nega-
tive and objective), (b) eye-movement as recorded
</bodyText>
<footnote confidence="0.991569666666667">
1A long stay of the visual gaze on a single location.
2A rapid movement of the eyes between positions of rest
on the sentence.
</footnote>
<page confidence="0.999114">
37
</page>
<figureCaption confidence="0.999621">
Figure 1: Gaze-data recording using Translog-II
</figureCaption>
<bodyText confidence="0.961581">
by an eye-tracker. They are given a set of instruc-
tions beforehand and can seek clarifications. This
experiment is conducted as follows:
</bodyText>
<listItem confidence="0.990474684210526">
1. A sentence is displayed to the annotator on
the screen. The annotator verbally states the
sentiment of this sentence, before (s)he can
proceed to the next.
2. While the annotator reads the sentence, a
remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to a Translog II soft-
ware (Carl, 2012) in order to record the data.
A snapshot of the software is shown in fig-
ure 1. The dots and circles represent position
of eyes and fixations of the annotator respec-
tively.
3. The experiment then continues in modules of
50 sentences at a time. This is to prevent fa-
tigue over a period of time. Thus, each an-
notator participates in this experiment over a
number of sittings.
</listItem>
<bodyText confidence="0.999307772727273">
We ensure the quality of our dataset in different
ways: (a) Our annotators are instructed to avoid
unnecessary head movements and eye-movements
outside the experiment environment. (b) To min-
imize noise due to head movements further, they
are also asked to state the annotation verbally,
which was then manually recorded, (c) Our an-
notators are students between the ages 20-24 with
English as the primary language of academic in-
struction and have secured a TOEFL iBT score of
110 or above.
We understand that sentiment is nuanced- to-
wards a target, through constructs like sarcasm and
presence of multiple entities. However, we want to
capture the most natural form of sentiment anno-
tation. So, the guidelines are kept to a bare mini-
mum of “annotating a sentence as positive, nega-
tive and objective as per the speaker”. This exper-
iment results in a data set of 1059 sentences with
a fixation duration recorded for each sentence-
annotator pair3 The multi-rater kappa IAA for sen-
timent annotation is 0.686.
</bodyText>
<subsectionHeader confidence="0.999726">
3.2 Calculating SAC from eye-tracked data
</subsectionHeader>
<bodyText confidence="0.9998446">
We now need to annotate each sentence with a
SAC. We extract fixation durations of the five an-
notators for each of the annotated sentences. A
single SAC score for sentence s for N annotators
is computed as follows:
</bodyText>
<equation confidence="0.9959334">
SAC(s) = 1
N
where,
z(n, dur(s, n)) = dur(s,n)−p(dur(n))
v(dur(n))
</equation>
<bodyText confidence="0.999985809523809">
In the above formula, N is the total number of an-
notators while n corresponds to a specific annota-
tor. dur(s, n) is the fixation duration of annotator
n on sentence s. len(s) is the number of words
in sentence s. This normalization over number
of words assumes that long sentences may have
high dur(s, n) but do not necessarily have high
SACs. p(dur(n)), Q(dur(n)) is the mean and
standard deviation of fixation durations for anno-
tator n across all sentences. z(n, .) is a function
that z-normalizes the value for annotator n to stan-
dardize the deviation due to reading speeds. We
convert the SAC values to a scale of 1-10 using
min-max normalization. To understand how the
formula records sentiment annotation complexity,
consider the SACs of examples in section 2. The
sentence “it is messy , uncouth , incomprehensi-
ble , vicious and absurd” has a SAC of 3.3. On the
other hand, the SAC for the sarcastic sentence “it’s
like an all-star salute to disney’s cheesy commer-
cialism.” is 8.3.
</bodyText>
<sectionHeader confidence="0.997005" genericHeader="method">
4 Predictive Framework for SAC
</sectionHeader>
<bodyText confidence="0.999994714285714">
The previous section shows how gold labels for
SAC can be obtained using eye-tracking experi-
ments. This section describes our predictive for
SAC that uses four categories of linguistic fea-
tures: lexical, syntactic, semantic and sentiment-
related in order to capture the subprocesses of an-
notation as described in section 2.
</bodyText>
<subsectionHeader confidence="0.986131">
4.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.9968545">
The linguistic features described in Table 3.2 are
extracted from the input sentences. Some of these
</bodyText>
<footnote confidence="0.986868">
3The complete eye-tracking data is available at:http://
www.cfilt.iitb.ac.in/˜cognitive-nlp/.
</footnote>
<equation confidence="0.790019333333333">
z(n,dur(s,n))
len(s)
N
E
n=1
(1)
</equation>
<page confidence="0.97668">
38
</page>
<bodyText confidence="0.782998888888889">
Feature Description
Lexical
- Word Count
- Degree of polysemy Average number of Wordnet senses per word
- Mean Word Length Average number of characters per word (commonly used in readability studies
as in the case of Pascual et al. (2005))
- %ge of nouns and adjs.
- %ge of Out-of-
vocabulary words
</bodyText>
<subsectionHeader confidence="0.600226">
Syntactic
</subsectionHeader>
<bodyText confidence="0.90781275">
- Dependency Distance Average distance of all pairs of dependent words in the sentence (Lin, 1996)
- Non-terminal to Ter- Ratio of the number of non-terminals to the number of terminals in the con-
minal ratio stituency parse of a sentence
Semantic
</bodyText>
<listItem confidence="0.527169090909091">
- Discourse connectors Number of discourse connectors
- Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence
- Perplexity Trigram perplexity using language models trained on a mixture of sentences
from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus
(mentioned in Sections 3 and 5)
Sentiment-related (Computed using SentiWordNet (Esuli et al., 2006))
- Subjective Word
Count
- Subjective Score Sum of SentiWordNet scores of all words
- Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts
as one sentiment flip
</listItem>
<tableCaption confidence="0.994438">
Table 1: Linguistic Features for the Predictive Framework
</tableCaption>
<bodyText confidence="0.99950147368421">
features are extracted using Stanford Core NLP 4
tools and NLTK (Bird et al., 2009). Words that
do not appear in Academic Word List 5 and Gen-
eral Service List 6 are treated as out-of-vocabulary
words. The training data consists of 1059 tuples,
with 13 features and gold labels from eye-tracking
experiments.
To predict SAC, we use Support Vector Regres-
sion (SVR) (Joachims, 2006). Since we do not
have any information about the nature of the rela-
tionship between the features and SAC, choosing
SVR allows us to try multiple kernels. We carry
out a 5-fold cross validation for both in-domain
and cross-domain settings, to validate that the re-
gressor does not overfit. The model thus learned is
evaluated using: (a) Error metrics namely, Mean
Squared Error estimate, Mean Absolute Error esti-
mate and Mean Percentage Error. (b) the Pearson
correlation coefficient between the gold and pre-
</bodyText>
<footnote confidence="0.99902675">
4http://nlp.stanford.edu/software/
corenlp.shtml
5www.victoria.ac.nz/lals/resources/academicwordlist/
6www.jbauman.com/gsl.html
</footnote>
<bodyText confidence="0.28728">
dicted SAC.
</bodyText>
<sectionHeader confidence="0.791558" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999984473684211">
The results are tabulated in Table 2. Our obser-
vation is that a quadratic kernel performs slightly
better than linear. The correlation values are pos-
itive and indicate that even if the predicted scores
are not as accurate as desired, the system is capa-
ble of ranking sentences in the correct order based
on their sentiment complexity. The mean percent-
age error (MPE) of the regressors ranges between
22-38.21%. The cross-domain MPE is higher than
the rest, as expected.
To understand how each of the features per-
forms, we conducted ablation tests by con-
sidering one feature at a time. Based on
the MPE values, the best features are: Mean
word length (MPE=27.54%), Degree of Polysemy
(MPE=36.83%) and %ge of nouns and adjectives
(MPE=38.55%). To our surprise, word count per-
forms the worst (MPE=85.44%). This is unlike
tasks like translation where length has been shown
</bodyText>
<page confidence="0.99863">
39
</page>
<table confidence="0.999519">
Kernel Linear Quadratic Cross Domain Linear
Domain Mixed Movie Twitter Mixed Movie Twitter Movie Twitter
MSE 1.79 1.55 1.99 1.68 1.53 1.88 3.17 2.24
MAE 0.93 0.89 0.95 0.91 0.88 0.93 1.39 1.19
MPE 22.49% 23.8% 25.45% 22.02% 23.8% 25% 35.01% 38.21%
Correlation 0.54 0.38 0.56 0.57 0.37 0.6 0.38 0.46
</table>
<tableCaption confidence="0.996509">
Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation using
</tableCaption>
<bodyText confidence="0.941716666666667">
Mean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimates
and correlation with the gold labels.
to be one of the best predictors in translation dif-
ficulty (Mishra et al., 2013). We believe that for
sentiment annotation, longer sentences may have
more lexical clues that help detect the sentiment
more easily. Note that some errors may be intro-
duced in feature extraction due to limitations of
the NLP tools.
</bodyText>
<sectionHeader confidence="0.998898" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99970445">
Our proposed metric measures complexity of sen-
timent annotation, as perceived by human annota-
tors. It would be worthwhile to study the human-
machine correlation to see if what is difficult for
a machine is also difficult for a human. In other
words, the goal is to show that the confidence
scores of a sentiment classifier are negatively cor-
related with SAC.
We use three sentiment classification tech-
niques: Naive Bayes, MaxEnt and SVM with un-
igrams, bigrams and trigrams as features. The
training datasets used are: a) 10000 movie reviews
from Amazon Corpus (McAuley et. al, 2013) and
b) 20000 tweets from the twitter corpus (same as
mentioned in section 3). Using NLTK and Scikit-
learn7 with default settings, we generate six posi-
tive/negative classifiers, for all possible combina-
tions of the three models and two datasets.
The confidence score of a classifier8 for given
text t is computed as follows:
</bodyText>
<equation confidence="0.4906372">
P : Probability of predicted class
P if predicted
polarity is correct
1 − P otherwise
(2)
</equation>
<footnote confidence="0.991985">
7http://scikit-learn.org/stable/
8In case of SVM, the probability of predicted class is com-
puted as given in Platt (1999).
</footnote>
<table confidence="0.999848142857143">
Classifier (Corpus) Correlation
Naive Bayes (Movie) -0.06 (73.35)
Naive Bayes (Twitter) -0.13 (71.18)
MaxEnt (Movie) -0.29 (72.17)
MaxEnt (Twitter) -0.26 (71.68)
SVM (Movie) -0.24 (66.27)
SVM (Twitter) -0.19 (73.15)
</table>
<tableCaption confidence="0.700903">
Table 3: Correlation between confidence of the
classifiers with SAC; Numbers in parentheses in-
dicate classifier accuracy (%)
</tableCaption>
<bodyText confidence="0.990452125">
Table 3 presents the accuracy of the classifiers
along with the correlations between the confidence
score and observed SAC values. MaxEnt has the
highest negative correlation of -0.29 and -0.26.
For both domains, we observe a weak yet nega-
tive correlation which suggests that the perception
of difficulty by the classifiers are in line with that
of humans, as captured through SAC.
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="conclusions">
6 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999850352941176">
We presented a metric called Sentiment Annota-
tion Complexity (SAC), a metric in SA research
that has been unexplored until now. First, the pro-
cess of data preparation through eye tracking, la-
beled with the SAC score was elaborated. Using
this data set and a set of linguistic features, we
trained a regression model to predict SAC. Our
predictive framework for SAC resulted in a mean
percentage error of 22.02%, and a moderate corre-
lation of 0.57 between the predicted and observed
SAC values. Finally, we observe a negative corre-
lation between the classifier confidence scores and
a SAC, as expected. As a future work, we would
like to investigate how SAC of a test sentence can
be used to choose a classifier from an ensemble,
and to determine the pre-processing steps (entity-
relationship extraction, for example).
</bodyText>
<figure confidence="0.952655">
Confidence(t) =
⎧
⎨
⎩
</figure>
<page confidence="0.988788">
40
</page>
<sectionHeader confidence="0.988992" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998751920792079">
Balahur, Alexandra and Hermida, Jes´us M and Mon-
toyo, Andr´es. 2011. Detecting implicit expressions
of sentiment in text based on commonsense knowl-
edge. Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis,53-60.
Batali, John and Searle, John R. 1995. The Rediscov-
ery of the Mind. Artif. Intell., Vol. 77, 177-193.
Steven Bird and Ewan Klein and Edward Loper. 2009.
Natural Language Processing with Python O’Reilly
Media.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation.
Dragsted, B. 2010. 2010. Co-ordination of reading
and writing processes in translation. Contribution
to Translation and Cognition. Shreve, G. and An-
gelone, E.(eds.)Cognitive Science Society.
Esuli, Andrea and Sebastiani, Fabrizio. 2006. Sen-
tiwordnet: A publicly available lexical resource for
opinion mining. Proceedings of LREC, vol. 6, 417-
422.
Fellbaum, Christiane 1998. WordNet: An electronic
lexical database. 1998. Cambridge. MA: MIT Press.
Fort, Kar¨en and Nazarenko, Adeline and Rosset, So-
phie et al 2012. Modeling the complexity of manual
annotation tasks: A grid of analysis Proceedings of
the International Conference on Computational Lin-
guistics.
Ganapathibhotla, G and Liu, Bing. 2008. Identifying
preferred entities in comparative sentences. 22nd In-
ternational Conference on Computational Linguis-
tics (COLING).
Gonz´alez-Ib´a˜nez, Roberto and Muresan, Smaranda and
Wacholder, Nina 2011. Identifying Sarcasm in
Twitter: A Closer Look. ACL (Short Papers) 581-
586.
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Martınez-G´omez, Pascual and Aizawa, Akiko. 2013.
Diagnosing Causes of Reading Difficulty using
Bayesian Networks International Joint Conference
on Natural Language Processing, 13831391.
McAuley, Julian John and Leskovec, Jure 2013 From
amateurs to connoisseurs: modeling the evolution of
user expertise through online reviews. Proceedings
of the 22nd international conference on World Wide
Web.
Mishra, Abhijit and Bhattacharyya, Pushpak and Carl,
Michael. 2013. Automatically Predicting Sentence
Translation Difficulty Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), 346-351.
Narayanan, Ramanathan and Liu, Bing and Choudhary,
Alok 2009. Sentiment Analysis of Conditional Sen-
tences. Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
180-189.
Pang, Bo and Lee, Lillian. 2008. Opinion mining and
sentiment analysis Foundations and trends in infor-
mation retrieval, vol. 2, 1-135.
Pang, Bo and Lee, Lillian. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, 115-124.
Platt, John and others. 1999. Probabilistic outputs for
support vector machines and comparisons to regular-
ized likelihood methods Advances in large margin
classifiers, vol. 10, 61-74.
Ramteke, Ankit and Malu, Akshat and Bhattacharyya,
Pushpak and Nath, J. Saketha 2013. Detect-
ing Turnarounds in Sentiment Analysis: Thwarting
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), 860-865.
Riloff, Ellen and Qadir, Ashequl and Surve, Prafulla
and De Silva, Lalindra and Gilbert, Nathan and
Huang, Ruihong 2013. Sarcasm as Contrast be-
tween a Positive Sentiment and Negative Situation
Conference on Empirical Methods in Natural Lan-
guage Processing, Seattle, USA.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Scott G. , O Donnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783-792
Siegel, Sidney and N. J. Castellan, Jr. 1988. Nonpara-
metric Statistics for the Behavioral Sciences. Second
edition. McGraw-Hill.
</reference>
<page confidence="0.999427">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.808351">
<title confidence="0.998271">Measuring Sentiment Annotation Complexity of Text</title>
<author confidence="0.972437">Abhijit Nivvedan</author>
<affiliation confidence="0.99076">Bombay, India, University,</affiliation>
<address confidence="0.915813">Research Academy, India</address>
<email confidence="0.975339">abhijitmishra,nivvedan,</email>
<abstract confidence="0.997204307692308">The effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise. We aim to predict a score that quantifies this effort, using linguistic properties of the text. Our proposed metric is called Annotation Complexity As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking. The sentences in our dataset are labeled with scores derived from du- Using linguistic features and anno- SACs, we train a regressor that prethe SAC a best mean error rate of 22.02% for five-fold cross-validation. We also study the correlation between a human annotator’s perception of complexity and a machine’s confidence in polarity determination. The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Jes´us M Hermida</author>
<author>Andr´es Montoyo</author>
</authors>
<title>Detecting implicit expressions of sentiment in text based on commonsense knowledge.</title>
<date>2011</date>
<booktitle>Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,53-60.</booktitle>
<contexts>
<context position="3287" citStr="Balahur et al. (2011)" startWordPosition="521" endWordPosition="524">le). Also, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences). Our metric adds value to sentiment annotation and sentiment analysis, in these two ways. The fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008), sarcasm by Riloff et al. (2013), thwarting by Ramteke et al. (2013) or implicit sentiment by Balahur et al. (2011). To the best of our knowledge, there is no general approach to “measure” how complex a piece of text is, in terms of sentiment annotation. The central challenge here is to annotate a data set with SAC. To measure the “actual” time spent by an annotator on a piece of text, we use an eyetracker to record eye-fixation duration: the time for which the annotator has actually focused on the sentence during annotation. Eye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted (2010) and sense disambiguation by 36 Proceedings of t</context>
</contexts>
<marker>Balahur, Hermida, Montoyo, 2011</marker>
<rawString>Balahur, Alexandra and Hermida, Jes´us M and Montoyo, Andr´es. 2011. Detecting implicit expressions of sentiment in text based on commonsense knowledge. Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis,53-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Batali</author>
<author>John R Searle</author>
</authors>
<date>1995</date>
<journal>The Rediscovery of the Mind. Artif. Intell.,</journal>
<volume>77</volume>
<pages>177--193</pages>
<marker>Batali, Searle, 1995</marker>
<rawString>Batali, John and Searle, John R. 1995. The Rediscovery of the Mind. Artif. Intell., Vol. 77, 177-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python O’Reilly Media.</booktitle>
<contexts>
<context position="13717" citStr="Bird et al., 2009" startWordPosition="2227" endWordPosition="2230">ntence - Perplexity Trigram perplexity using language models trained on a mixture of sentences from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus (mentioned in Sections 3 and 5) Sentiment-related (Computed using SentiWordNet (Esuli et al., 2006)) - Subjective Word Count - Subjective Score Sum of SentiWordNet scores of all words - Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts as one sentiment flip Table 1: Linguistic Features for the Predictive Framework features are extracted using Stanford Core NLP 4 tools and NLTK (Bird et al., 2009). Words that do not appear in Academic Word List 5 and General Service List 6 are treated as out-of-vocabulary words. The training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments. To predict SAC, we use Support Vector Regression (SVR) (Joachims, 2006). Since we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels. We carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit. The model thus l</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird and Ewan Klein and Edward Loper. 2009. Natural Language Processing with Python O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carl</author>
</authors>
<title>Translog-II: A Program for Recording User Activity Data for Empirical Reading and Writing Research.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation, European Language Resources Association.</booktitle>
<contexts>
<context position="9101" citStr="Carl, 2012" startWordPosition="1457" endWordPosition="1458">es between positions of rest on the sentence. 37 Figure 1: Gaze-data recording using Translog-II by an eye-tracker. They are given a set of instructions beforehand and can seek clarifications. This experiment is conducted as follows: 1. A sentence is displayed to the annotator on the screen. The annotator verbally states the sentiment of this sentence, before (s)he can proceed to the next. 2. While the annotator reads the sentence, a remote eye-tracker (Model: Tobii TX 300, Sampling rate: 300Hz) records the eyemovement data of the annotator. The eyetracker is linked to a Translog II software (Carl, 2012) in order to record the data. A snapshot of the software is shown in figure 1. The dots and circles represent position of eyes and fixations of the annotator respectively. 3. The experiment then continues in modules of 50 sentences at a time. This is to prevent fatigue over a period of time. Thus, each annotator participates in this experiment over a number of sittings. We ensure the quality of our dataset in different ways: (a) Our annotators are instructed to avoid unnecessary head movements and eye-movements outside the experiment environment. (b) To minimize noise due to head movements fur</context>
</contexts>
<marker>Carl, 2012</marker>
<rawString>Carl, M. 2012. Translog-II: A Program for Recording User Activity Data for Empirical Reading and Writing Research. In Proceedings of the Eight International Conference on Language Resources and Evaluation, European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dragsted</author>
</authors>
<title>Co-ordination of reading and writing processes in translation. Contribution to Translation and Cognition.</title>
<date>2010</date>
<editor>Shreve, G. and Angelone,</editor>
<publisher>E.(eds.)Cognitive Science Society.</publisher>
<contexts>
<context position="3839" citStr="Dragsted (2010)" startWordPosition="619" endWordPosition="620">ke et al. (2013) or implicit sentiment by Balahur et al. (2011). To the best of our knowledge, there is no general approach to “measure” how complex a piece of text is, in terms of sentiment annotation. The central challenge here is to annotate a data set with SAC. To measure the “actual” time spent by an annotator on a piece of text, we use an eyetracker to record eye-fixation duration: the time for which the annotator has actually focused on the sentence during annotation. Eye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted (2010) and sense disambiguation by 36 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36–41, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Joshi et al. (2011). Mishra et al. (2013) present a technique to determine translation difficulty index. The work closest to ours is by Scott et al. (2011) who use eye-tracking to study the role of emotion words in reading. The novelty of our work is three-fold: (a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptatio</context>
</contexts>
<marker>Dragsted, 2010</marker>
<rawString>Dragsted, B. 2010. 2010. Co-ordination of reading and writing processes in translation. Contribution to Translation and Cognition. Shreve, G. and Angelone, E.(eds.)Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>417--422</pages>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, Andrea and Sebastiani, Fabrizio. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. Proceedings of LREC, vol. 6, 417-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge. MA:</location>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane 1998. WordNet: An electronic lexical database. 1998. Cambridge. MA: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kar¨en Fort</author>
<author>Adeline Nazarenko</author>
<author>Rosset</author>
</authors>
<title>Sophie et al 2012. Modeling the complexity of manual annotation tasks: A grid of analysis</title>
<booktitle>Proceedings of the International Conference on Computational Linguistics.</booktitle>
<marker>Fort, Nazarenko, Rosset, </marker>
<rawString>Fort, Kar¨en and Nazarenko, Adeline and Rosset, Sophie et al 2012. Modeling the complexity of manual annotation tasks: A grid of analysis Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ganapathibhotla</author>
<author>Bing Liu</author>
</authors>
<title>Identifying preferred entities in comparative sentences.</title>
<date>2008</date>
<booktitle>22nd International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="3171" citStr="Ganapathibhotla and Liu (2008)" startWordPosition="500" endWordPosition="504">efore the annotation begins, the pricing model can be fine-tuned (pay less for sentences that are easy to annotate, for example). Also, in terms of an automatic SA engine which has multiple classifiers in its ensemble, a classifier may be chosen based on the complexity of sentiment annotation (for example, use a rule-based classifier for simple sentences and a more complex classifier for other sentences). Our metric adds value to sentiment annotation and sentiment analysis, in these two ways. The fact that sentiment expression may be complex is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008), sarcasm by Riloff et al. (2013), thwarting by Ramteke et al. (2013) or implicit sentiment by Balahur et al. (2011). To the best of our knowledge, there is no general approach to “measure” how complex a piece of text is, in terms of sentiment annotation. The central challenge here is to annotate a data set with SAC. To measure the “actual” time spent by an annotator on a piece of text, we use an eyetracker to record eye-fixation duration: the time for which the annotator has actually focused on the sentence during annotation. Eye-tracking annotations have been used to study the cognitive aspe</context>
</contexts>
<marker>Ganapathibhotla, Liu, 2008</marker>
<rawString>Ganapathibhotla, G and Liu, Bing. 2008. Identifying preferred entities in comparative sentences. 22nd International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying Sarcasm in Twitter: A Closer Look.</title>
<date>2011</date>
<journal>ACL (Short Papers)</journal>
<pages>581--586</pages>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Gonz´alez-Ib´a˜nez, Roberto and Muresan, Smaranda and Wacholder, Nina 2011. Identifying Sarcasm in Twitter: A Closer Look. ACL (Short Papers) 581-586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training Linear SVMs in Linear Time</title>
<date>2006</date>
<booktitle>Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="14013" citStr="Joachims, 2006" startWordPosition="2278" endWordPosition="2279">bjective Score Sum of SentiWordNet scores of all words - Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts as one sentiment flip Table 1: Linguistic Features for the Predictive Framework features are extracted using Stanford Core NLP 4 tools and NLTK (Bird et al., 2009). Words that do not appear in Academic Word List 5 and General Service List 6 are treated as out-of-vocabulary words. The training data consists of 1059 tuples, with 13 features and gold labels from eye-tracking experiments. To predict SAC, we use Support Vector Regression (SVR) (Joachims, 2006). Since we do not have any information about the nature of the relationship between the features and SAC, choosing SVR allows us to try multiple kernels. We carry out a 5-fold cross validation for both in-domain and cross-domain settings, to validate that the regressor does not overfit. The model thus learned is evaluated using: (a) Error metrics namely, Mean Squared Error estimate, Mean Absolute Error estimate and Mean Percentage Error. (b) the Pearson correlation coefficient between the gold and pre4http://nlp.stanford.edu/software/ corenlp.shtml 5www.victoria.ac.nz/lals/resources/academicwo</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Joachims, T. 2006 Training Linear SVMs in Linear Time Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>On the structural complexity of natural language sentences.</title>
<date>1996</date>
<booktitle>Proceeding of the 16th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>729733</pages>
<contexts>
<context position="12801" citStr="Lin, 1996" startWordPosition="2083" endWordPosition="2084">scribed in Table 3.2 are extracted from the input sentences. Some of these 3The complete eye-tracking data is available at:http:// www.cfilt.iitb.ac.in/˜cognitive-nlp/. z(n,dur(s,n)) len(s) N E n=1 (1) 38 Feature Description Lexical - Word Count - Degree of polysemy Average number of Wordnet senses per word - Mean Word Length Average number of characters per word (commonly used in readability studies as in the case of Pascual et al. (2005)) - %ge of nouns and adjs. - %ge of Out-ofvocabulary words Syntactic - Dependency Distance Average distance of all pairs of dependent words in the sentence (Lin, 1996) - Non-terminal to Ter- Ratio of the number of non-terminals to the number of terminals in the conminal ratio stituency parse of a sentence Semantic - Discourse connectors Number of discourse connectors - Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence - Perplexity Trigram perplexity using language models trained on a mixture of sentences from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus (mentioned in Sections 3 and 5) Sentiment-related (Computed using SentiWordNet (Esuli et al., 2006)) - Subjective Word Count - Subje</context>
</contexts>
<marker>Lin, 1996</marker>
<rawString>Lin, D. 1996 On the structural complexity of natural language sentences. Proceeding of the 16th International Conference on Computational Linguistics (COLING), pp. 729733.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascual Martınez-G´omez</author>
<author>Akiko Aizawa</author>
</authors>
<title>Diagnosing Causes of Reading Difficulty using</title>
<date>2013</date>
<booktitle>Bayesian Networks International Joint Conference on Natural Language Processing,</booktitle>
<pages>13831391</pages>
<marker>Martınez-G´omez, Aizawa, 2013</marker>
<rawString>Martınez-G´omez, Pascual and Aizawa, Akiko. 2013. Diagnosing Causes of Reading Difficulty using Bayesian Networks International Joint Conference on Natural Language Processing, 13831391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian John McAuley</author>
<author>Jure Leskovec</author>
</authors>
<title>From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews.</title>
<date>2013</date>
<booktitle>Proceedings of the 22nd international conference on World Wide Web.</booktitle>
<marker>McAuley, Leskovec, 2013</marker>
<rawString>McAuley, Julian John and Leskovec, Jure 2013 From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. Proceedings of the 22nd international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhijit Mishra</author>
<author>Pushpak Bhattacharyya</author>
<author>Michael Carl</author>
</authors>
<title>Automatically Predicting Sentence Translation Difficulty</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>346--351</pages>
<contexts>
<context position="4121" citStr="Mishra et al. (2013)" startWordPosition="657" endWordPosition="660">the “actual” time spent by an annotator on a piece of text, we use an eyetracker to record eye-fixation duration: the time for which the annotator has actually focused on the sentence during annotation. Eye-tracking annotations have been used to study the cognitive aspects of language processing tasks like translation by Dragsted (2010) and sense disambiguation by 36 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36–41, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Joshi et al. (2011). Mishra et al. (2013) present a technique to determine translation difficulty index. The work closest to ours is by Scott et al. (2011) who use eye-tracking to study the role of emotion words in reading. The novelty of our work is three-fold: (a) The proposition of a metric to measure complexity of sentiment annotation, (b) The adaptation of past work that uses eye-tracking for NLP in the context of sentiment annotation, (c) The learning of regressors that automatically predict SAC using linguistic features. 2 Understanding Sentiment Annotation Complexity The process of sentiment annotation consists of two sub-pro</context>
<context position="16154" citStr="Mishra et al., 2013" startWordPosition="2612" endWordPosition="2615">shown 39 Kernel Linear Quadratic Cross Domain Linear Domain Mixed Movie Twitter Mixed Movie Twitter Movie Twitter MSE 1.79 1.55 1.99 1.68 1.53 1.88 3.17 2.24 MAE 0.93 0.89 0.95 0.91 0.88 0.93 1.39 1.19 MPE 22.49% 23.8% 25.45% 22.02% 23.8% 25% 35.01% 38.21% Correlation 0.54 0.38 0.56 0.57 0.37 0.6 0.38 0.46 Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation using Mean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimates and correlation with the gold labels. to be one of the best predictors in translation difficulty (Mishra et al., 2013). We believe that for sentiment annotation, longer sentences may have more lexical clues that help detect the sentiment more easily. Note that some errors may be introduced in feature extraction due to limitations of the NLP tools. 5 Discussion Our proposed metric measures complexity of sentiment annotation, as perceived by human annotators. It would be worthwhile to study the humanmachine correlation to see if what is difficult for a machine is also difficult for a human. In other words, the goal is to show that the confidence scores of a sentiment classifier are negatively correlated with SA</context>
</contexts>
<marker>Mishra, Bhattacharyya, Carl, 2013</marker>
<rawString>Mishra, Abhijit and Bhattacharyya, Pushpak and Carl, Michael. 2013. Automatically Predicting Sentence Translation Difficulty Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 346-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramanathan Narayanan</author>
<author>Bing Liu</author>
<author>Alok Choudhary</author>
</authors>
<title>Sentiment Analysis of Conditional Sentences.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>180--189</pages>
<marker>Narayanan, Liu, Choudhary, 2009</marker>
<rawString>Narayanan, Ramanathan and Liu, Bing and Choudhary, Alok 2009. Sentiment Analysis of Conditional Sentences. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, 180-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis Foundations and trends in information retrieval,</title>
<date>2008</date>
<volume>2</volume>
<pages>1--135</pages>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, Bo and Lee, Lillian. 2008. Opinion mining and sentiment analysis Foundations and trends in information retrieval, vol. 2, 1-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="8110" citStr="Pang and Lee, 2005" startWordPosition="1291" endWordPosition="1294">acking device that measures the “duration of eye-fixations1”. Another attribute recorded by the eye-tracker that may have been used is “saccade duration2”. However, saccade duration is not significant for annotation of short text, as in our case. Hence, the SAC labels of our dataset are fixation durations with appropriate normalization. It may be noted that the eye-tracking device is used only to annotate training data. The actual prediction of SAC is done using linguistic features alone. 3.1 Eye-tracking Experimental Setup We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http://help.sentiment140. com/for-students. A total of 1059 sentences (566 from a movie corpus, 493 from a twitter corpus) are selected. We then obtain two kinds of annotation from five paid annotators: (a) sentiment (positive, negative and objective), (b) eye-movement as recorded 1A long stay of the visual gaze on a single location. 2A rapid movement of the eyes between positions of rest on the sentence. 37 Figure 1: Gaze-data recording using Translog-II by an eye-tracker. They are given a set of instructions beforehand and can seek clarifications. This experiment is conducte</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Pang, Bo and Lee, Lillian. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, 115-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
<author>others</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods Advances in large margin classifiers,</title>
<date>1999</date>
<volume>10</volume>
<pages>61--74</pages>
<marker>Platt, others, 1999</marker>
<rawString>Platt, John and others. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods Advances in large margin classifiers, vol. 10, 61-74.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ankit Ramteke</author>
<author>Akshat Malu</author>
<author>Pushpak Bhattacharyya</author>
<author>J Nath</author>
</authors>
<title>Saketha 2013. Detecting Turnarounds in Sentiment Analysis: Thwarting</title>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>860--865</pages>
<marker>Ramteke, Malu, Bhattacharyya, Nath, </marker>
<rawString>Ramteke, Ankit and Malu, Akshat and Bhattacharyya, Pushpak and Nath, J. Saketha 2013. Detecting Turnarounds in Sentiment Analysis: Thwarting Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 860-865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Ashequl Qadir</author>
<author>Prafulla Surve</author>
<author>Lalindra De Silva</author>
<author>Nathan Gilbert</author>
<author>Ruihong Huang</author>
</authors>
<date>2013</date>
<booktitle>Sarcasm as Contrast between a Positive Sentiment and Negative Situation Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, USA.</location>
<marker>Riloff, Qadir, Surve, De Silva, Gilbert, Huang, 2013</marker>
<rawString>Riloff, Ellen and Qadir, Ashequl and Surve, Prafulla and De Silva, Lalindra and Gilbert, Nathan and Huang, Ruihong 2013. Sarcasm as Contrast between a Positive Sentiment and Negative Situation Conference on Empirical Methods in Natural Language Processing, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salil Joshi</author>
<author>Diptesh Kanojia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>More than meets the eye: Study of Human Cognition</title>
<date>2013</date>
<booktitle>in Sense Annotation. NAACL HLT 2013,</booktitle>
<location>Atlanta, USA.</location>
<marker>Joshi, Kanojia, Bhattacharyya, 2013</marker>
<rawString>Salil Joshi, Diptesh Kanojia and Pushpak Bhattacharyya. 2013. More than meets the eye: Study of Human Cognition in Sense Annotation. NAACL HLT 2013, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Donnell P</author>
<author>S Sereno</author>
</authors>
<title>Emotion Words Affect Eye Fixations During Reading.</title>
<date>2012</date>
<journal>Journal of Experimental Psychology:Learning, Memory, and Cognition</journal>
<volume>38</volume>
<pages>783--792</pages>
<marker>P, Sereno, 2012</marker>
<rawString>Scott G. , O Donnell P and Sereno S. 2012. Emotion Words Affect Eye Fixations During Reading. Journal of Experimental Psychology:Learning, Memory, and Cognition 2012, Vol. 38, No. 3, 783-792</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences. Second edition.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel, Sidney and N. J. Castellan, Jr. 1988. Nonparametric Statistics for the Behavioral Sciences. Second edition. McGraw-Hill.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>