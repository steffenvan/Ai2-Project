<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9480785">
THE TEXT SYSTEM FOR NATURAL LANGUAGE GENERATION:
AN OVERVIEW*
</note>
<author confidence="0.797796">
Kathleen R. McKeown
</author>
<affiliation confidence="0.952783">
Dept. of Computer &amp; Information Science
The Moore School
University of Pennsylvania
</affiliation>
<address confidence="0.616035">
Philadelphia, Pa. 19104
</address>
<email confidence="0.500562">
ABSTRACT
</email>
<bodyText confidence="0.998024722222222">
Computer-based generation of natural language
requires consideration of two different types of
problems: 1) determining the content and textual
shape of what is to be said, and 2) transforming
that message into English. A computational
solution to the problems of deciding what to say
and how to organize it effectively is proposed
that relies on an interaction between structural
and semantic processes. Schemas, which encode
aspects of discourse structure, are used to guide
the generation process. A focusing mechanism
monitors the use of the schemes, providing
constraints on what can be said at any point.
These mechanisms have been implemented as part of
a generation method within the context of a
natural language database system, addressing the
specific problem of responding to questions about
database structure.
</bodyText>
<sectionHeader confidence="0.981993" genericHeader="abstract">
1.0 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999970944444445">
Deciding what to say and how to organize it
effectively are two issues of particular
importance to the generation of natural language
text. In the past, researchers have concentrated
on local issues concerning the syntactic and
lexical choices involved in transforming a
pre-determined message into natural language. The
research described here emphasizes a computational
solution to the more global problems of
determining the content and textual shape of what
is to be said. More specifically, my goals have
been the development and application of principles
of discourse structure, discourse coherency, and
relevancy criterion to the computer generation of
text. These principles have been realized in the
TEXT system, reported on in this paper.
The main features of the generation method
used in TEXT include 1) an ability to select
relevant information, 2) a system for pairing
rhetorical techniques (such as analogy) with
discourse purposes (such as defining terms) and
3) a focusing mechanism. Rhetorical techniques,
which encode aspects of discourse structure, guide
the selection of information for inclusion in the
text from a relevant knowledge pool - a subset of
his work was partially supported by National
Science Foundation grant #MCS81-07290.
the knowledge base which contains information
relevant to the discourse purpose. The focusing
mechanism helps maintain discourse coherency. It
aids in the organization of the message by
constraining the selection of information to be
talked about next to that which ties in with the
previous discourse in an appropriate way. These
processes are described in more detail after
setting out the framework of the system.
</bodyText>
<sectionHeader confidence="0.92414" genericHeader="keywords">
2.0 APPLICATION
</sectionHeader>
<bodyText confidence="0.95393155882353">
In order to test generation principles, the
TEXT system was developed as part of a natural
language interface to a database system,
addressing the specific problem of generating
answers to questions about database structure.
Three classes of questions have been considered:
questions about information available in the
database, requests for definitions, and questions
about the differences between database entities
[MCKEOWN 80]. In this context, input questions
provide the initial motivation for speaking.
Although the specific application of
answering questions about database structure was
used primarily for testing principles about text
generation, it is a feature that many users of
such systems would like. Several experiments
([MALHOTRA 75], [TENNANT 79]) have shown that
users often ask questions to familiarize
themselves with the database structure before
proceeding to make requests about the database
contents. The three classes of questions
considered for this system were among those shown
to be needed in a natural language database
system.
Implementation of the TEXT system for natural
language generation used a portion of the Office
of Naval Research (ONR) database containing
information about vehicles and destructive
devices. Some examples of questions that can be
asked of the system include:
&gt; What is a frigate?
&gt; What do you know about submarines?
&gt; What is the difference between a whisky
and a kitty hawk?
</bodyText>
<page confidence="0.998735">
113
</page>
<bodyText confidence="0.99482965">
The kind of generation of which the system is
capable is illustrated by the response it
generates to question (A) below.
A) What kind of data do you have?
All entities in the ONR database have DB
attributes REMARKS. There are 2 types of
entities in the ONR database: destructive
devices and vehicles. The vehicle has DB
attributes that provide information on
SPEED-INDICES and TRAVEL-MEANS. The
destructive device has DB attributes that
provide information on LETHAL-INDICES.
TEXT does not itself contain a facility for
interpreting a user&apos;s questions. Questions must
be phrased using a simple functional notation
(shown below) which corresponds to the types of
questions that can be asked . It is assumed that
a component could be built to perform this type of
task and that the decisions it must make would not
affect the performance of the generation system.
</bodyText>
<listItem confidence="0.996844333333333">
1. (definition &lt;e))
2. (information &lt;e&gt;)
3. (differense &lt;el&gt; &lt;e2&gt;)
</listItem>
<bodyText confidence="0.9993965">
where &lt;e&gt;, &lt;el&gt;, &lt;e2&gt; represent entities in the
database.
</bodyText>
<sectionHeader confidence="0.745602" genericHeader="introduction">
3.0 SYSTEM OVERVIEW
</sectionHeader>
<bodyText confidence="0.999616916666667">
In answering a question about database
structure, TEXT identifies those rhetorical
techniques that could be used for presenting an
appropriate answer. On the basis of the input
question, semantic processes produce a relevant
knowledge pool. A characterization of the
information in this pool is then used to select a
single partially ordered set of rhetorical
techniques from the various possibilities. A
formal representation of the answer (called a
&amp;quot;message&amp;quot;) is constructed by selecting
propositions from the relevant knowledge pool
which match the rhetorical techniques in the given
set. The focusing mechanism monitors the matching
process; where there are choices for what to say
next (i.e. - either alternative techniques are
possible or a single technique matches several
propositions in the knowledge pool), the focusing
mechanism selects that proposition which ties in
most closely with the previous discourse. Once
the message has been constructed, the system
passes the message to a tactical component
[BOSSIE 811 which uses a functional grammar
[KAY 79] to translate the message into English.
</bodyText>
<sectionHeader confidence="0.601538" genericHeader="method">
4.0 KNOWLEDGE BASE
</sectionHeader>
<bodyText confidence="0.9988325">
Answering questions about the structure of
the database requires access to a high-level
description of the classes of objects in the
database, their properties, and the relationships
between them. The knowledge base used for the
TEXT system is a standard database model which
draws primarily from representations developed by
Chen [CHEN 761, the Smiths [SMITH and SMITH 77],
Schubert [SCHUBERT et. al. 79], and Lee and
Gerritsen [LEE and GERRITSEN 78]. The main
features of TEXT&apos;s knowledge base are entities,
relations, attributes, a generalization hierarchy,
a topic hierarchy, distinguishing descriptive
attributes, supporting database attributes, and
based database attributes.
Entities, relations, and attributes are based
on the Chen entity-relationship model. A
generalization hierarchy on entities
[SMITH and SMITH 77], (LEE and GERRITSEN 781, and
a topic hierarchy on attributes
[SCHUBERT et. al. 79] are also used. In the topic
hierarchy, attributes such as MAXIMUM SPEED,
MINIMUM SPEED, and ECONOMIC _SPEED are generalized
as SPEED INDICES. In the generalization
hierarchy, entities such as SHIP and SUBMARINE are
generalized as WATER-GOING VEHICLE. The
generalization hierarchy includes both
generalizations of entities for which physical
records exist in the database (database entity
classes) and sub-types of these entities. The
sub-types were generated automatically by a system
developed by McCoy [MCCOY 82].
An additional feature of the knowledge base
represents the basis for each split in the
hierarchy [LEE and GERRITSEN 78]. For
generalizations of the database entity classes,
partitions are made on the basis of different
attributes possessed, termed supporting db
attributes. For sub-types of the database entity
classes, partitions are made on the basis of
different values possessed for given, shared
attributes, termed based db attributes.
Additional descriptive information that
distinguishes sub-classes of an entity are
captured in distinguishing descriptive attributes
(DDAs). For generalizations of the database
entity classes, such WAS capture real-world
characteristics of the entities. Figure 1 shows
the DDAs and supporting db attributes for two
generalizations. (See [MCCOY 82] for discussion
of information associated with sub-types of
database entity classes).
</bodyText>
<page confidence="0.995938">
114
</page>
<figure confidence="0.7727566">
TRAVEL-MEDIUM
SURFACE (DDA) uNDERWATER (DDA)
-DRAFT, DISPLACEMENT -DEPTH, MAXIMUM
(supporting dbs) SUBMERGED SPEED
(supporting dbs)
</figure>
<figureCaption confidence="0.793178">
FIGURE 1 DDAs and supporting db attributes
</figureCaption>
<sectionHeader confidence="0.756338" genericHeader="method">
5.0 SELECTING RELEVANT INFORMATION
</sectionHeader>
<bodyText confidence="0.999973677419355">
The first step in answering a question is to
circumscribe a subset of the knowledge base
containing that information which is relevant to
the question. This then provides limits on what
information need be considered when deciding what
to say. All information that might be relevant to
the answer is included in the partition, but all
information in the partition need not be included
in the answer. The partitioned subset is called
the relevant knowledge pool. It is similar to
what Grosz has called Rglobal focus&amp;quot; [GROSZ 77]
since its contents are focused throughout the
course of an answer.
The relevant knowledge pool is constructed by
a fairly simple process. For requests for
definitions or available information, the area
around the questioned object containing the
information immediately associated with the entity
(e.g. its superordinates, sub-types, and
attributes) is circumscribed and partitioned from
the remaining knowledge base. For questions about
the difference between entities, the information
included in the relevant knowledge pool depends on
how close in the generalization hierarchy the two
entities are. For entities that are very similar,
detailed attributive information is included. For
entities that are very different, only generic
class information is included. A combination of
this information is included for entities falling
between these two extremes. (See (MCKEOWN 821 for
further details).
</bodyText>
<sectionHeader confidence="0.86179" genericHeader="method">
6.0 RHETORICAL PREDICATES
</sectionHeader>
<bodyText confidence="0.991880492063492">
Rhetorical predicates are the means which a
speaker has for describing information. They
characterize the different types of predicating
acts s/he may use and delineate the structural
relation between propositions in a text. Some
examples are &amp;quot;analogy&amp;quot; (comparison with a familiar
object), &amp;quot;constituency&amp;quot; (description of sub-parts
or sub-types), and &amp;quot;attributive&amp;quot; (associating
properties with an entity or event). Linguistic
discussion of such predicates (e.g. (GRIMES 75],
[SHEPHERD 26]) indicates that some combinations
are preferable to others. Moreover, Grimes claims
that predicates are recursive and can be used to
identify the organization of text on any level
(i.e. - proposition, sentence, paragraph, or
longer sequence of text), although he does not
show how.
I have examined texts and transcripts and
have found that not only are certain combinations
of rhetorical techniques more likely than others,
certain ones are more appropriate in some
discourse situations than others. For example, I
found that objects were frequently defined by
employing same combination of the following means:
(1) identifying an item as a member of some
generic class, (2) describing an object&apos;s
function, attributes, and constituency (either
physical or class), (3) making analogies to
familiar objects, and (4) providing examples.
These techniques were rarely used in random order;
for instance, it was common to identify an item as
a member of some generic class before providing
examples.
In the TEXT system, these types of standard
patterns of discourse structure have been captured
in schemas associated with explicit discourse
purposes. The schemes loosely identify normal
patterns of usage. They are not intended to serve
as grammars of text. The schema shown below
serves the purpose providing providing definitions:
Identification Schema
identification (class&amp;attribute/function)
(analogy/constituency/attributivel*
(particular-illustration/evidencel+
{amplification/analogy/attributive)
(particular-illustration/evidence}
Here, &amp;quot;( )&amp;quot; indicates optionality, &amp;quot;/&amp;quot;
indicates alternatives, &amp;quot;+&amp;quot; indicates that the
item may appear 1-n times, and &amp;quot;*&amp;quot; indicates that
the item may appear 0-n times. The order of the
predicates indicates that the normal pattern of
definitions is an identifying protion followed
by any number of descriptive predicates. The
speaker then provides one or more examples and can
optionally close with some additional descriptive
information and possibly another example.
TEXT&apos;s response to the question &amp;quot;What is a
ship?&amp;quot; (shown below) was generated using the
identification schema. The sentences are numbered
to show the correspondence between each sentence
and the predicate it corresponds to in the
instantiated schema (the numbers do not occur in
the actual output).
</bodyText>
<page confidence="0.99005">
115
</page>
<figure confidence="0.570765285714286">
(definition SHIP)
Schema selected: identification
1) identification
2) evidence
3) attributive
4) particular-illustration
1) A ship is a water-going vehicle that
</figure>
<bodyText confidence="0.923201823529412">
travels on the surface. 2) Its surface-going
capabilities are provided by the DB attributes
DISPLACEMENT and DRAFT. 3) Other DB
attributes of the Ship include MAXIMUM SPEED,
&apos;PROPULSION, FUEL (FUEL CAPACITY- and
FUEL TYPE), DIMENSIONS, SPEED-DEPENDENT RANGE
and - OFFICIAL NAME. 4) The DCWNES, for
example, has MAXIMUM SPEED of 29, PROPULSION
of STMTURGRD, FUEL -a 810 (FUEL CAPACITY) and
BNKR (FUEL TYPE), DIMENSIONS of 25 (DRAFT), 46
(BEAM), and 438 (LENGTH) and
SPEED DEPENDENT RANGE of 4200 (ECONOMIC RANGE)
and 00 (ENDURANCE RANGE).
Another strategy commonly used in the
expository texts examined was to describe an
entity or event in terms of its sub-parts or
sub-classes. This strategy involves:
</bodyText>
<listItem confidence="0.942209">
1) presenting identificational or attributive
information about the entity or event,
2) presenting its sub-parts or sub-classes,
3) discussing attributive or identificational
</listItem>
<bodyText confidence="0.755783571428571">
information with optional evidence about each of
the sub-classes in turn, and 4) oliiiEriafri
returning to the oriiinal entity with additional
attributive or analogical information. The
constituency schema, shown below, encodes the
techniques used in this strategy.
The Constituency Schema
</bodyText>
<equation confidence="0.889340428571429">
attributive/identification (entity)
constituency (entity)
{ attributive/identification
(sub-classl, sub-class2,..)
{evidence
(sub-classl, sub-class2, ...)} }+
{attributive/analogy (entity) }
</equation>
<bodyText confidence="0.916144">
TEXT&apos;s response to the question &amp;quot;What do you
know about vehicles?&amp;quot; was generated using the
constituency schema. It is shown below along with
the predicates that were instantiated for the
answer.
(information VEHICLE)
</bodyText>
<listItem confidence="0.780848166666667">
Schema selected: constituency
1) attributive
2) constituency
3) attributive
4) attributive
5) attributive
</listItem>
<bodyText confidence="0.992525095238095">
1) The vehicle has DB attributes that
provide information on SPEED INDICES and
TRAVEL MEANS. 2) There are 2 types of
vehicles in the OUR database: aircraft and
water-going vehicles. 3) The water-going
vehicle has DB attributes that provide
information on TRAVEL MEANS and
WATER GOING_OPERATION. 4) The aircraft has DB&apos;
attributes that provide information on
TRAVEL MEANS, FLIGHT RADIUS, CEILING and ROLE.
Other 5B attributes of the vehicle include
FUEL( FUEL CAPACITY and FUEL TYPE) and FLAG.
Two other strategies were identified in the
texts examined. These are encoded in the
attributive schema, which is used to provide
detailed information about a particular aspect of
an entity, and the compare and contrast schema,
which encodes a strategy for contrasting two
entities using a description of their similarities
and their differences. For more detail on these
strategies, see (MCKEOWN 82].
</bodyText>
<sectionHeader confidence="0.545208" genericHeader="method">
7.0 USE OF THE SCHEMAS
</sectionHeader>
<bodyText confidence="0.9999918">
As noted earlier, an examination of texts
revealed that different strategies were used in
different situations. In TEXT, this association
of technique with discourse purpose is achieved by
associating the different schemes with different
question-types. For example, if the question
involves defining a term, a different set of
schemes (and therefore rhetorical techniques) is
chosen than if the question involves describing
the type of information available in the database.
The identification schema can be used in
response to a request for a definition. The
purpose of the attributive schema is to provide
detailed information about one particular aspect
of any concept and it can therefore be used in
response to a request for information. In
situations where an object or concept can be
described in terms of its sub-parts or
sub-classes, the constituency schema is used. It
may be selected in response to requests for either
definitions or information. The compare and
contrast schema is used in response to a guest-EFL
about the difference between objects. A summary
of the assignment of schemes to question-types is
shown in Figure 2.
</bodyText>
<page confidence="0.990119">
116
</page>
<bodyText confidence="0.3985205">
(definition GUIDED)
Schemas used for TEXT
</bodyText>
<listItem confidence="0.9040405">
1. identification
-requests for definitions
2. attributive
-requests for available information
3. constituency
-requests for definitions
-requests for available information
4. compare and contrast
</listItem>
<bodyText confidence="0.966066523809524">
-requests about the differences
between objects
FIGURE 2
Once a question has been posed to TEXT, a
schema must be selected for the response structure
which will then be used to control the decisions
involved in deciding what to say when. On the
basis of the given question, a set of schemes is
selected as possible structures for the response.
This set includes those schemes associated with
the given question-type (see Figure 2 above). A
single schema is selected out of this set on the
basis of the information available to answer the
question.
For example, in response to requests for
definitions, the constituency schema is selected
when the relevant knowledge pool contains a rich
description of the questioned object&apos;s sub-classes
and less information about the object itself.
When this is not the case, the identification
schema is used. The test for what kind of
information is available is a relatively simple
one. If the questioned object occurs at a higher
level in the hierarchy than a pre-determined
level, the constituency schema is used. Note that
the higher an entity occurs in the hierarchy, the
less descriptive information is available about
the entity itself. More information is available
about its sub-parts since fewer common features
are associated with entities higher in the
hierarchy.
This type of semantic and structural
interaction means that a different schema may be
used for answering the same type of question. An
earlier example showed that the identification
schema was selected by the TEXT system in response
to a request for a definition of a ship. In
response to a request for a definition of a guided
projectile (shown below), the constituency schema
is selected since more information is available
about the sub-classes of the guided projectile
than about the guided projectile itself.
</bodyText>
<listItem confidence="0.9058268">
Schema selected: Constituency
1) identification
2) conkituency
3) identification
4) identification
5) evidence
6) evidence
7) attributive
1) A guided projectile is a projectile
that is self-propelled. 2) There are 2 types
</listItem>
<bodyText confidence="0.932308333333333">
of guided projectiles in the ONR database:
torpedoes and missiles. 3) The missile has a
target location in the air or on the earth&apos;s
surface. 4) The torpedo has an underwater
target location. 5) The missile&apos;s target
location is indicated by the DB attribute
DESCRIPTION and the missile&apos;s flight
capabilities are provided by the DB attribute
ALTITUDE. 6) The torpedo&apos;s underwater
capabilities are provided by the DB attributes
under DEPTH (for example,
MAXIMUM OPERATING DEPTH). 7) The guided
projectile has DB attributes
TIME TO TARGET &amp; UNITS, HORZ RANGE &amp; UNITS and
_
NAME&apos;:
Once a schema has been selected, it is filled
by matching the predicates it contains against the
relevant knowledge pool. The semantics of each
predicate define the type of information it can
match in the knowledge pool. The semantics
defined for TEXT are particular to the database
query domain and would have to be redefined if the
schemes were to be used in another type of system
(such as a tutorial system, for example). The
semantics are not particular, however, to the
domain of the database. When transferring the
system from one database to another, the predicate
semantics would not have to be altered.
A proposition is an instantiated predicate;
predicate arguments have been filled with values
from the knowledge base. An instantiation of the
identification predicate is shown below along with
its eventual translation.
Instantiated predicate:
(identification (OCEAN-ESCORT CRUISER) SHIP
(non-restrictive TRAVEL-MODE SURFACE))
Eventual translation:
The ocean escort and the cruiser are surface
ships.
The schema is filled by stepping through it,
using the predicate semantics to select
information which matches the predicate arguments.
In places where alternative predicates occur in
the schema, all alternatives are matched against
the relevant knowledge pool producing a set of
propositions. The focus constraints are used to
select the most appropriate proposition.
</bodyText>
<page confidence="0.995597">
117
</page>
<bodyText confidence="0.999951666666667">
The schemes were implemented using a
formalism similar to an augmented transition
network (ATN). Taking an arc corresponds to the
selection of a proposition for the answer. States
correspond to filled stages of the schema. The
main difference between the TEXT system
implementation and a usual ATN, however, is in the
control of alternatives. Instead of uncontrolled
backtracking, TEXT uses one state lookahead. From
a given state, it explores all possible next
states and chooses among them using a function
that encodes the focus constraints. This use of
one state lookahead increases the efficiency of
the strategic component since it eliminates
unbounded non-determinism.
</bodyText>
<sectionHeader confidence="0.739988" genericHeader="method">
8.0 FOCUSING MECHANISM
</sectionHeader>
<bodyText confidence="0.999960423529412">
So far, a speaker has been shown to be
limited in many ways. Fbr example, s/he is
limited by the goal s/he is trying to achieve in
the current speech act. TEXT&apos;s goal is to answer
the user&apos;s current question. TO achieve that
goal, the speaker has limited his/her scope of
attention to a set of objects relevant to this
goal, as represented by global focus or the
relevant knowledge pool. The speaker is also
limited by his/her higher-level plan of how to
achieve the goal. In TEXT, this plan is the
chosen schema. Within these constraints, however,
a speaker may still run into the problem of
deciding what to say next.
A focusing mechanism is used to provide
further constraints on what can be said. The
focus constraints used in TEXT are immediate,
since they use the most recent proposition
(corresponding to a sentence in the English
answer) to constrain the next utterance. Thus, as
the text is constructed, it is used to constrain
what can be said next.
Sidner [SIDNER 79] used three pieces of
information for tracking immediate focus: the
immediate focus of a sentence (represented by the
current focus - CF), the elements of a sentence
which are potential candidates for a change in
focus (represented by a potential focus list -
PFL), and past immediate foci (represented by a
focus stack). She showed that a speaker has the
.
TOTTOwing options from one sentence to the next:
1) to continue focusing on the same thing, 2) to
focus on one of the items introduced in the last
sentence, 3) to return to a previous topic in
which case the focus stack is popped, or 4) to
focus on an item implicitly related to any of
these three options. Sidner&apos;s work on focusing
concerned the interpretation of anaphora. She
says nothing about which of these four options is
preferred over others since in interpretation the
choice has already been made.
For generation, &apos;‘,:wever, a speaker may have
to choose between these options at any point,
given all that s/he wants to say. The speaker may
be faced with the following choices:
1) continuing to talk about the same thing
(current-focus equals current-focus of the
previous sentence) or starting to talk about
something introduced in the last sentence
(current-focus is a member of potential-focus-list
of the previous sentence) and 2) continuing to
talk about the same thing (current focus remains
the same) or returning to a topic of previous
discussion (current focus is a member of the
focus-stack).
When faced with the choice of remaining on
the same topic or switching to one just
introduced, I claim a speaker&apos;s preference is to
switch. If the speaker has something to say about
an item just introduced and does not present it
next, s/he must go to the trouble of
re-introducing it later on. If s/he does present
information about the new item first, however,
s/he can easily continue where s/he left off by
following Sidner&apos;s legal option #3. Thus, for
reasons of efficiency, the speaker should shift
focus to talk about an item just introduced when
s/he has something to say about it.
When faced with the choice of continuing to
talk about the same thing or returning to a
previous topic of conversation, I claim a
speaker&apos;s preference is to remain on the same
topic. Having at some point shifted focus to the
current focus, the speaker has opened a topic for
conversation. By shifting back to the earlier
focus, the speaker closes this new topic, implying
that s/he has nothing more to say about it when in
fact, s/he does. Therefore, the speaker should
maintain the current focus when possible in order
to avoid false implication of a finished topic.
These two guidelines for changing and
maintaining focus during the process of generating
language provide an ordering on the three basic
legal focus moves that Sidner specifies:
</bodyText>
<listItem confidence="0.984497555555556">
1. change focus to member of previous
potential focus list if possible -
CF (new sentence) is a member of PFL
(last sentence)
2. maintain focus if possible -
CF (new sentence) = CF (last sentence)
3. return to topic of previous discussion -
CF (new sentence) is a member of
focus-stack
</listItem>
<bodyText confidence="0.998538916666667">
I have not investigated the problem of
incorporating focus moves to items implicitly
associated with either current foci, potential
focus list members, or previous foci into this
scheme. This remains a topic for future research.
Even these guidelines, however, do not appear
to be enough to ensure a connected discourse.
Although a speaker may decide to focus on a
specific entity, s/he may want to convey
information about several properties of that
entity. S/he will describe related properties of
the entity before describing other properties.
</bodyText>
<page confidence="0.99543">
118
</page>
<bodyText confidence="0.999763777777778">
Thus, strands of semantic connectivity will occur
at more than one level of the discourse.
An example of this phenomenon is given in
dialogues (A) and (B) below. In both, the
discourse is focusing on a single entity (the
balloon), but in (A) properties that must be
talked about are presented randomly. In (B), a
related set of properties (color) is discussed
before the next set (size). (8), as a result, is
more connected than (A).
(A) The balloon was red and white striped.
Because this balloon was designed to carry
men, it had to be large. It had a silver
circle at the top to reflect heat. In fact,
it was larger than any balloon John had ever
seen.
(B) The balloon was red and white striped. It
had a silver circle at the top to reflect
heat. Because this balloon was designed to
carry men, it had to be large. In fact, it
was larger than any balloon John had ever
seen.
In the generation process, this phenomenon is
accounted for by further constraining the choice
of what to talk about next to the proposition with
the greatest number of links to the potential
focus list.
</bodyText>
<subsectionHeader confidence="0.82777">
8.1 Use Of The Focus Constraints
</subsectionHeader>
<bodyText confidence="0.987525418604651">
TEXT uses the legal focus moves identified by
Sidner by only matching schema predicates against
propositions which have an argument that can be
focused in satisfaction of the legal options.
Thus, the matching process itself is constrained
by the focus mechanism. The focus preferences
developed for generation are used to select
between remaining options.
These options occur in TEXT when a predicate
matches more than one piece of information in the
relevant knowledge pool or when more than one
alternative in a schema can be satisfied. In such
cases, the focus guidelines are used to select the
most appropriate proposition. When options exist,
all propositions are selected which have as
focused argument a member of the previous PFL. If
none exist, then all propositions are selected
whose focused argument is the previous
current-focus. If none exist, then all
propositions are selected whose focused argument
is a member of the focus-stack. If these
filtering steps do not narrow down the
possibilities to a single proposition, that
proposition with the greatest number of links to
the previous PFL is selected for the answer. The
focus and potential focus list of each proposition
is maintained and passed to the tactical component
for use in selecting syntactic constructions and
pronominalization.
Interaction of the focus constraints with the
schemas means that although the same schema may be
selected for different answers, it can be
instantiated ° in different ways. Recall that the
identification schema was selected in response to
the question &amp;quot;Shat is a ship?&amp;quot; and the four
predicates, identification, evidence, attributive,
and particular-illustration, were instantiated.
The identification schema was also selected in
response to the question &amp;quot;What is an aircraft
carrier?&amp;quot;, but different predicates were
instantiated as a result of the focus constraints:
(definition AIRCRAFT-CARRIER)
Schema selected: identification
</bodyText>
<listItem confidence="0.9643471">
1) identification
2) analogy
3) particular-illustration
4) amplification
5) evidence
1) An aircraft carrier is a surface ship
with a DISPLACEMENT between 78000 and 80800
and a LENGTH between 1039 and 1063.
2) Aircraft carriers have a greater LENGTH
than all other ships and a greater
</listItem>
<bodyText confidence="0.9006732">
DISPLACEMENT than most other ships. 3) Mine
warfare ships, for example, have a
DISPLACEMENT of 320 and a LENGTH of 144.
4) All aircraft carriers in the ONR database
have REMARKS of 0, FUEL TYPE of BNKR, FLAG of
BLBL, BEAM of 252, ENDURANCE RANGE of 4000,
ECONOMIC SPEED of 12, ENDURANCE SPEED of 30
and PROPULSION of STMTURGRD. 5)-A ship is
classified as an aircraft carrier if the
characters 1 through 2 of its HULL NO are CV.
</bodyText>
<sectionHeader confidence="0.960655" genericHeader="method">
9.0 FUTURE DIRECTIONS
</sectionHeader>
<bodyText confidence="0.999701333333333">
Several possibilities for further development
of the research described here include 1) the use
of the same strategies for responding to questions
about attributes, events, and relations as well as
to questions about entities, 2) investigation of
strategies needed for responding to questions
about the system processes (e.g. How is
manufacturer&apos;s cost determined?) or system
capabilities (e.g. Can you handle ellipsis?),
3) responding to presuppositional failure as well
as to direct questions, and 4) the incorporation
of a user model in the generation process
(currently TEXT assumes a static casual, naive
user and gears its responses to this
characterization). This last feature could be
- used, among other ways, in determining the amount
of detail required (see (MCKEOWN 82] for
discussion of the recursive use of the schemes).
</bodyText>
<page confidence="0.99843">
119
</page>
<sectionHeader confidence="0.92819" genericHeader="conclusions">
10.0 CONCLUSION
</sectionHeader>
<bodyText confidence="0.999970793103448">
The TEXT system successfully incorporates
principles of relevancy criteria, discourse
structure, and focus constraints into a method for
generating English text of paragraph length.
Previous work on focus of attention has been
extended for the task of generation to provide
constraints on what to say next. Knowledge about
discourse structure has been encoded into schemes
that are used to guide the generation process.
The use of these two interacting mechanisms
constitutes a departure from earlier generation
systems. The approach taken in this research is
that the generation process should not simply
trace the knowledge representation to produce
text. Instead, communicative strategies people
are familiar with are used to effectively convey
information. This means that the same information
may be described in different ways on different
occasions.
The result is a system which constructs and
orders a message in response to a given question.
Although the system was designed to generate
answers to questions about database structure (a
feature lacking in most natural language database
systems), the same techniques and principles could
be used in other application areas (for example,
computer assisted instruction systems, expert
systems, etc.) where generation of language is
needed.
</bodyText>
<sectionHeader confidence="0.98749" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999888833333333">
I would like to thank Aravind Joshi, Bonnie
Webber, Kathleen McCoy, and Eric Mays for their
invaluable comments on the style and content of
this paper. Thanks also goes to Kathleen McCoy
and Steven Bossie for their roles in implementing
portions of the system.
</bodyText>
<sectionHeader confidence="0.997981" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9895647">
[BOSSIE 82]. Bossier S., &amp;quot;A tactical model for
text generation: sentence generation using a
functional grammar,&amp;quot; forthcoming M. S. thesis,
University of Pennsylvania, Philadelphia, Pa.,
1982.
[CHEN 76]. Chen, P. P. S., &amp;quot;The
entity-relationship model - towards a unified view
of data.&amp;quot; ACM Transactions on Database Systems,
Vol. 1, No. 1. (1976).
[GRIMES 75]. Grimes, J. E. The Thread
Discourse. Mouton, The Hague, PiFig. (1975).
[GROSZ 77]. Grosz, B. J., &amp;quot;The representation and
use of focus in dialogue understanding.&amp;quot; Technical
note 151, Stanford Research Institute, Menlo Park,
Ca. (1977).
[LEE and GERRITSEN 78]. Gee, R. M. and
R. Gerritsen, &amp;quot;Extended semantics LOC
generalization hierarchies&amp;quot;, in Proceedings of the
1978 ACM-SIGMOD International conference on
Management of Data, Austin, Tex., 1978.
[KAY 79]. Kay, M. &amp;quot;Functional grammar.&amp;quot;
Proceedings of the 5th Annual Meeting of the
Berkeley LingilistIE-soFrety. (1979).
[MALHOTRA 75]. Malhotra, A. &amp;quot;Design criteria for
a knowledge-based English language system for
management: an experimental analysis.&amp;quot; MAC
TR-146, MIT, Cambridge, Mass. (1975).
[MCCOY 82]. McCoy, K. F., &amp;quot;Augmenting a database
knowledge representation for natural language
generation,&amp;quot; in Proc. of the 20th Annual
Conference of the Assoirition-Tbr ComputiEn=
Linguistics, Tbronto, Canada, 1982.
[MCKEOWN 80]. McKeown, K. R., &amp;quot;Generating
relevant explanations: natural language responses
to questions about database structure.&amp;quot; in
Proceedings of AAAI, Stanford Univ., Stanford, Ca.
(1980). pp. 306-9.
(MCKEOWN 82]. McKeown, K. R., &amp;quot;Generating natural
language text in response to questions about
database structure.&amp;quot; Ph.D. dissertation,
University of Pennsylvania, Philadelphia, Pa.
1982.
[SHEPHERD 26]. Shepherd, H. R., The Fine Art of
Writing, The Macmillan Co., New York, N. Y., 1926.
[SIDNER 79]. Sidner, C. L., &amp;quot;Towards a
computational theory of definite anaphora
comprehension in English discourse.&amp;quot; Ph.D.
dissertation, MIT Al Technical Report #TR-537,
Cambridge, Mass. (1979).
[SMITH and SMITH 77]. Smith, J. M. and
</reference>
<bodyText confidence="0.751933">
Smith, D. C. P., &amp;quot;Database abstractions:
aggregation and generalization.&amp;quot; University of
Utah, ACM Transactions on Database Systems, Vol.
2, #2, June 1977, pp. 105-33.
[TENNANT 79]. Tennant, H., &amp;quot;Experience with the
evaluation of natural language question
answerers.&amp;quot; Working paper #18, Univ. of Illinois,
Urbana-Champaign, Ill. (1979).
</bodyText>
<page confidence="0.994068">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000163">
<title confidence="0.9737445">THE TEXT SYSTEM FOR NATURAL LANGUAGE GENERATION: AN OVERVIEW*</title>
<author confidence="0.999996">Kathleen R McKeown</author>
<affiliation confidence="0.995807333333333">Dept. of Computer &amp; Information Science School University of Pennsylvania</affiliation>
<address confidence="0.998916">Philadelphia, Pa. 19104</address>
<abstract confidence="0.998971608108108">Computer-based generation of natural language requires consideration of two different types of problems: 1) determining the content and textual shape of what is to be said, and 2) transforming that message into English. A computational solution to the problems of deciding what to say and how to organize it effectively is proposed that relies on an interaction between structural and semantic processes. Schemas, which encode aspects of discourse structure, are used to guide the generation process. A focusing mechanism monitors the use of the schemes, providing constraints on what can be said at any point. These mechanisms have been implemented as part of a generation method within the context of a natural language database system, addressing the specific problem of responding to questions about database structure. Deciding what to say and how to organize it effectively are two issues of particular importance to the generation of natural language text. In the past, researchers have concentrated on local issues concerning the syntactic and lexical choices involved in transforming a pre-determined message into natural language. The research described here emphasizes a computational solution to the more global problems of determining the content and textual shape of what is to be said. More specifically, my goals have been the development and application of principles of discourse structure, discourse coherency, and relevancy criterion to the computer generation of text. These principles have been realized in the TEXT system, reported on in this paper. The main features of the generation method used in TEXT include 1) an ability to select relevant information, 2) a system for pairing rhetorical techniques (such as analogy) with discourse purposes (such as defining terms) and 3) a focusing mechanism. Rhetorical techniques, which encode aspects of discourse structure, guide the selection of information for inclusion in the from a relevantknowledge pool a subset of his work was partially supported by National Science Foundation grant #MCS81-07290. the knowledge base which contains information relevant to the discourse purpose. The focusing mechanism helps maintain discourse coherency. It aids in the organization of the message by constraining the selection of information to be talked about next to that which ties in with the previous discourse in an appropriate way. These processes are described in more detail after setting out the framework of the system. In order to test generation principles, the TEXT system was developed as part of a natural language interface to a database system, addressing the specific problem of generating answers to questions about database structure. Three classes of questions have been considered: questions about information available in the database, requests for definitions, and questions about the differences between database entities [MCKEOWN 80]. In this context, input questions provide the initial motivation for speaking. Although the specific application of answering questions about database structure was used primarily for testing principles about text generation, it is a feature that many users of such systems would like. Several experiments ([MALHOTRA 75], [TENNANT 79]) have shown that users often ask questions to familiarize themselves with the database structure before proceeding to make requests about the database contents. The three classes of questions considered for this system were among those shown to be needed in a natural language database system. Implementation of the TEXT system for natural language generation used a portion of the Office of Naval Research (ONR) database containing information about vehicles and destructive devices. Some examples of questions that can be asked of the system include: &gt; What is a frigate? &gt; What do you know about submarines? &gt; What is the difference between a whisky and a kitty hawk? 113 The kind of generation of which the system is capable is illustrated by the response it generates to question (A) below. A) What kind of data do you have? All entities in the ONR database have DB attributes REMARKS. There are 2 types of entities in the ONR database: destructive devices and vehicles. The vehicle has DB attributes that provide information on SPEED-INDICES and TRAVEL-MEANS. The destructive device has DB attributes that provide information on LETHAL-INDICES. TEXT does not itself contain a facility for interpreting a user&apos;s questions. Questions must be phrased using a simple functional notation (shown below) which corresponds to the types of questions that can be asked . It is assumed that a component could be built to perform this type of task and that the decisions it must make would not affect the performance of the generation system. 1. (definition &lt;e)) 2. (information &lt;e&gt;) 3. (differense &lt;el&gt; &lt;e2&gt;) where &lt;e&gt;, &lt;el&gt;, &lt;e2&gt; represent entities in the database. OVERVIEW In answering a question about database structure, TEXT identifies those rhetorical techniques that could be used for presenting an appropriate answer. On the basis of the input question, semantic processes produce a relevant knowledge pool. A characterization of the information in this pool is then used to select a single partially ordered set of rhetorical techniques from the various possibilities. A formal representation of the answer (called a &amp;quot;message&amp;quot;) is constructed by selecting propositions from the relevant knowledge pool which match the rhetorical techniques in the given set. The focusing mechanism monitors the matching process; where there are choices for what to say next (i.e. either alternative techniques are possible or a single technique matches several propositions in the knowledge pool), the focusing mechanism selects that proposition which ties in most closely with the previous discourse. Once the message has been constructed, the system passes the message to a tactical component [BOSSIE 811 which uses a functional grammar [KAY 79] to translate the message into English. KNOWLEDGEBASE Answering questions about the structure of the database requires access to a high-level description of the classes of objects in the database, their properties, and the relationships between them. The knowledge base used for the TEXT system is a standard database model which draws primarily from representations developed by</abstract>
<author confidence="0.864007333333333">Chen</author>
<abstract confidence="0.989340007861636">features of TEXT&apos;s knowledge base are entities, relations, attributes, a generalization hierarchy, a topic hierarchy, distinguishing descriptive attributes, supporting database attributes, and based database attributes. Entities, relations, and attributes are based on the Chen entity-relationship model. A hierarchyon entities [SMITH and SMITH 77], (LEE and GERRITSEN 781, and hierarchyon attributes [SCHUBERT et. al. 79] are also used. In the topic hierarchy, attributes such as MAXIMUM SPEED, MINIMUM SPEED, and ECONOMIC _SPEED are generalized as SPEED INDICES. In the generalization hierarchy, entities such as SHIP and SUBMARINE are generalized as WATER-GOING VEHICLE. The generalization hierarchy includes both generalizations of entities for which physical records exist in the database (database entity classes) and sub-types of these entities. The sub-types were generated automatically by a system developed by McCoy [MCCOY 82]. An additional feature of the knowledge base represents the basis for each split in the hierarchy [LEE and GERRITSEN 78]. For generalizationsof the database entity classes, partitions are made on the basis of different possessed, termed supportingdb attributes.For sub-typesof the database entity classes, partitions are made on the basis of different values possessed for given, shared termed based db attributes. Additional descriptive information that distinguishes sub-classes of an entity are in descriptive attributes (DDAs). For generalizations of the database entity classes, such WAS capture real-world characteristics of the entities. Figure 1 shows the DDAs and supporting db attributes for two generalizations. (See [MCCOY 82] for discussion of information associated with sub-types of database entity classes). 114 TRAVEL-MEDIUM SURFACE (DDA) uNDERWATER (DDA) -DRAFT, DISPLACEMENT -DEPTH, MAXIMUM (supporting dbs) SUBMERGED SPEED (supporting dbs) FIGURE 1 DDAs and supporting db attributes SELECTINGRELEVANT INFORMATION The first step in answering a question is to circumscribe a subset of the knowledge base containing that information which is relevant to the question. This then provides limits on what information need be considered when deciding what to say. All information that might be relevant to the answer is included in the partition, but all information in the partition need not be included in the answer. The partitioned subset is called knowledgepool. It is similar to Grosz has called focus&amp;quot; 77] since its contents are focused throughout the course of an answer. The relevant knowledge pool is constructed by a fairly simple process. For requests for definitions or available information, the area around the questioned object containing the information immediately associated with the entity (e.g. its superordinates, sub-types, and attributes) is circumscribed and partitioned from the remaining knowledge base. For questions about the difference between entities, the information included in the relevant knowledge pool depends on how close in the generalization hierarchy the two entities are. For entities that are very similar, detailed attributive information is included. For entities that are very different, only generic class information is included. A combination of this information is included for entities falling between these two extremes. (See (MCKEOWN 821 for further details). RHETORICALPREDICATES Rhetorical predicates are the means which a speaker has for describing information. They characterize the different types of predicating acts s/he may use and delineate the structural relation between propositions in a text. Some examples are &amp;quot;analogy&amp;quot; (comparison with a familiar object), &amp;quot;constituency&amp;quot; (description of sub-parts or sub-types), and &amp;quot;attributive&amp;quot; (associating properties with an entity or event). Linguistic of such predicates (e.g. 75], [SHEPHERD 26]) indicates that some combinations are preferable to others. Moreover, Grimes claims that predicates are recursive and can be used to identify the organization of text on any level (i.e. proposition, sentence, paragraph, or longer sequence of text), although he does not show how. I have examined texts and transcripts and have found that not only are certain combinations of rhetorical techniques more likely than others, certain ones are more appropriate in some discourse situations than others. For example, I that objects were frequently definedby employing same combination of the following means: (1) identifying an item as a member of some generic class, (2) describing an object&apos;s function, attributes, and constituency (either physical or class), (3) making analogies to familiar objects, and (4) providing examples. These techniques were rarely used in random order; for instance, it was common to identify an item as a member of some generic class before providing examples. In the TEXT system, these types of standard patterns of discourse structure have been captured in schemas associated with explicit discourse purposes. The schemes loosely identify normal of usage. Theyare not intendedto serve as grammars of text. The schema shown below the providing definitions: Identification Schema identification (class&amp;attribute/function) (analogy/constituency/attributivel* (particular-illustration/evidencel+ {amplification/analogy/attributive) (particular-illustration/evidence} Here, &amp;quot;( )&amp;quot; indicates optionality, &amp;quot;/&amp;quot; indicates alternatives, &amp;quot;+&amp;quot; indicates that the item may appear 1-n times, and &amp;quot;*&amp;quot; indicates that the item may appear 0-n times. The order of the predicates indicates that the normal pattern of definitions is an identifying protion followed by any number of descriptive predicates. The speaker then provides one or more examples and can optionally close with some additional descriptive information and possibly another example. TEXT&apos;s response to the question &amp;quot;What is a ship?&amp;quot; (shown below) was generated using the identificationschema. The sentences are numbered to show the correspondence between each sentence and the predicate it corresponds to in the instantiated schema (the numbers do not occur in the actual output). 115 Schema selected: identification 1) identification 2) evidence 3) attributive 4) particular-illustration 1) A ship is a water-going vehicle that travels on the surface. 2) Its surface-going are provided by the Other DB of the Ship include SPEED, FUEL (FUEL and TYPE), DIMENSIONS, RANGE - NAME. The for has MAXIMUM 29, FUEL (FUEL CAPACITY) (FUEL TYPE), DIMENSIONS 25 438 and DEPENDENT RANGE 4200 RANGE) (ENDURANCE RANGE). Another strategy commonly used in the expository texts examined was to describe an entity or event in terms of its sub-parts or sub-classes. This strategy involves: 1) presenting identificational or attributive information about the entity or event, 2) presenting its sub-parts or sub-classes, 3) discussing attributive or identificational information with optional evidence about each of sub-classesin turn, and 4) the with additional attributive or analogical information. The constituencyschema, shown below, encodes the techniques used in this strategy. The Constituency Schema attributive/identification (entity) constituency (entity) { attributive/identification (sub-classl, sub-class2,..) {evidence sub-class2, {attributive/analogy (entity) } to the question &amp;quot;What do you know about vehicles?&amp;quot; was generated using the constituency schema. It is shown below along with the predicates that were instantiated for the answer. Schema selected: constituency 1) attributive 2) constituency 3) attributive 4) attributive 5) attributive The vehicle has that information on INDICES MEANS. There are 2 types of in the aircraft and water-going vehicles. 3) The water-going has that provide on MEANS and GOING_OPERATION. The aircraft has DB&apos; attributes that provide information on MEANS, FLIGHT RADIUS, CEILING ROLE. of the vehicle include FUEL and TYPE) Two other strategies were identified in the texts examined. These are encoded in the attributiveschema, which is used to provide detailed information about a particular aspect of entity, and the compareand contrastschema, which encodes a strategy for contrasting two entities using a description of their similarities and their differences. For more detail on these see USE OF THE As noted earlier, an examination of texts revealed that different strategies were used in situations. In association of technique with discourse purpose is achieved by associating the different schemes with different question-types. For example, if the question involves defining a term, a different set of schemes (and therefore rhetorical techniques) is chosen than if the question involves describing the type of information available in the database. identificationschema can be used in response to a request for a definition. The of the attributiveschema is to provide detailed information about one particular aspect of any concept and it can therefore be used in response to a request for information. In situations where an object or concept can be described in terms of its sub-parts or the constituencyschema is used. It may be selected in response to requests for either or information. The compareand schema is used in response to a about the difference between objects. A summary of the assignment of schemes to question-types is shown in Figure 2. 116 (definition GUIDED) Schemasused for TEXT 1. identification -requests for definitions 2. attributive -requests for available information 3. constituency -requests for definitions -requests for available information 4. compare and contrast -requests about the differences between objects FIGURE 2 Once a question has been posed to TEXT, a schema must be selected for the response structure which will then be used to control the decisions involved in deciding what to say when. On the basis of the given question, a set of schemes is selected as possible structures for the response. This set includes those schemes associated with the given question-type (see Figure 2 above). A single schema is selected out of this set on the basis of the information available to answer the question. For example, in response to requests for definitions, the constituency schema is selected when the relevant knowledge pool contains a rich description of the questioned object&apos;s sub-classes and less information about the object itself. When this is not the case, the identification schema is used. The test for what kind of information is available is a relatively simple one. If the questioned object occurs at a higher level in the hierarchy than a pre-determined level, the constituency schema is used. Note that the higher an entity occurs in the hierarchy, the less descriptive information is available about the entity itself. More information is available about its sub-parts since fewer common features are associated with entities higher in the hierarchy. This type of semantic and structural interaction means that a different schema may be used for answering the same type of question. An earlier example showed that the identification schema was selected by the TEXT system in response to a request for a definition of a ship. In response to a request for a definition of a guided (shown below), the constituencyschema is selected since more information is available about the sub-classes of the guided projectile than about the guided projectile itself. Schema selected: Constituency 1) identification 2) conkituency 3) identification 4) identification 5) evidence 6) evidence 7) attributive 1) A guided projectile is a projectile that is self-propelled. 2) There are 2 types of guided projectiles in the ONR database: torpedoes and missiles. 3) The missile has a target location in the air or on the earth&apos;s surface. 4) The torpedo has an underwater target location. 5) The missile&apos;s target location is indicated by the DB attribute DESCRIPTION and the missile&apos;s flight capabilities are provided by the DB attribute ALTITUDE. 6) The torpedo&apos;s underwater capabilities are provided by the DB attributes under DEPTH (for example, MAXIMUM OPERATING DEPTH). 7) The guided has attributes TIME TO TARGET &amp; UNITS, HORZ RANGE &amp; UNITS and _ NAME&apos;: Once a schema has been selected, it is filled by matching the predicates it contains against the relevant knowledge pool. The semantics of each predicate define the type of information it can match in the knowledge pool. The semantics defined for TEXT are particular to the database query domain and would have to be redefined if the schemes were to be used in another type of system (such as a tutorial system, for example). The semantics are not particular, however, to the domainof the database. When transferring the system from one database to another, the predicate semantics would not have to be altered. A proposition is an instantiated predicate; predicate arguments have been filled with values from the knowledge base. An instantiation of the identification predicate is shown below along with its eventual translation. Instantiated predicate: (identification (OCEAN-ESCORT CRUISER) SHIP (non-restrictive TRAVEL-MODE SURFACE)) Eventual translation: The ocean escort and the cruiser are surface ships. The schema is filled by stepping through it, using the predicate semantics to select information which matches the predicate arguments. places where alternative predicates occur the schema, all alternatives are matched against the relevant knowledge pool producing a set of propositions. The focus constraints are used to select the most appropriate proposition. 117 The schemes were implemented using a formalism similar to an augmented transition network (ATN). Taking an arc corresponds to the selection of a proposition for the answer. States correspond to filled stages of the schema. The main difference between the TEXT system implementation and a usual ATN, however, is in the control of alternatives. Instead of uncontrolled backtracking, TEXT uses one state lookahead. From a given state, it explores all possible next states and chooses among them using a function that encodes the focus constraints. This use of one state lookahead increases the efficiency of the strategic component since it eliminates unbounded non-determinism. MECHANISM So far, a speaker has been shown to be limited in many ways. Fbr example, s/he is goal s/he is trying to achieve in the current speech act. TEXT&apos;s goal is to answer the user&apos;s current question. TO achieve that goal, the speaker has limited his/her scope of attention to a set of objects relevant to this goal, as represented by global focus or the relevant knowledge pool. The speaker is also limited by his/her higher-level plan of how to achieve the goal. In TEXT, this plan is the chosen schema. Within these constraints, however, a speaker may still run into the problem of deciding what to say next. A focusing mechanism is used to provide further constraints on what can be said. The focus constraints used in TEXT are immediate, since they use the most recent proposition (corresponding to a sentence in the English answer) to constrain the next utterance. Thus, as the text is constructed, it is used to constrain what can be said next. Sidner [SIDNER 79] used three pieces of information for tracking immediate focus: the immediate focus of a sentence (represented by the current focus - CF), the elements of a sentence which are potential candidates for a change in focus (represented by a potential focus list - PFL), and past immediate foci (represented by a focus stack). She showed that a speaker has the . from one sentence to the next: 1) to continue focusing on the same thing, 2) to focus on one of the items introduced in the last sentence, 3) to return to a previous topic in which case the focus stack is popped, or 4) to focus on an item implicitly related to any of these three options. Sidner&apos;s work on focusing concerned the interpretation of anaphora. She says nothing about which of these four options is preferred over others since in interpretation the choice has already been made. For generation, &apos;‘,:wever, a speaker may have to choose between these options at any point, given all that s/he wants to say. The speaker may be faced with the following choices: 1) continuing to talk about the same thing (current-focus equals current-focus of the previous sentence) or starting to talk about something introduced in the last sentence (current-focus is a member of potential-focus-list of the previous sentence) and 2) continuing to talk about the same thing (current focus remains the same) or returning to a topic of previous discussion (current focus is a member of the focus-stack). When faced with the choice of remaining on the same topic or switching to one just introduced, I claim a speaker&apos;s preference is to switch. If the speaker has something to say about an item just introduced and does not present it next, s/he must go to the trouble of re-introducing it later on. If s/he does present information about the new item first, however, s/he can easily continue where s/he left off by following Sidner&apos;s legal option #3. Thus, for reasons of efficiency, the speaker should shift focus to talk about an item just introduced when s/he has something to say about it. When faced with the choice of continuing to talk about the same thing or returning to a previous topic of conversation, I claim a speaker&apos;s preference is to remain on the same topic. Having at some point shifted focus to the current focus, the speaker has opened a topic for conversation. By shifting back to the earlier focus, the speaker closes this new topic, implying that s/he has nothing more to say about it when in fact, s/he does. Therefore, the speaker should maintain the current focus when possible in order to avoid false implication of a finished topic. These two guidelines for changing and maintaining focus during the process of generating language provide an ordering on the three basic legal focus moves that Sidner specifies: 1. change focus to member of previous potential focus list if possible - CF (new sentence) is a member of PFL (last sentence) 2. maintain focus if possible - CF (new sentence) = CF (last sentence) 3. return to topic of previous discussion - CF (new sentence) is a member of focus-stack I have not investigated the problem of incorporating focus moves to items implicitly associated with either current foci, potential focus list members, or previous foci into this scheme. This remains a topic for future research. Even these guidelines, however, do not appear to be enough to ensure a connected discourse. Although a speaker may decide to focus on a specific entity, s/he may want to convey information about several properties of that entity. S/he will describe related properties of the entity before describing other properties. 118 Thus, strands of semantic connectivity will occur at more than one level of the discourse. An example of this phenomenon is given in dialogues (A) and (B) below. In both, the discourse is focusing on a single entity (the balloon), but in (A) properties that must be talked about are presented randomly. In (B), a related set of properties (color) is discussed before the next set (size). (8), as a result, is more connected than (A). (A) The balloon was red and white striped. Because this balloon was designed to carry men, it had to be large. It had a silver circle at the top to reflect heat. In fact, it was larger than any balloon John had ever seen. (B) The balloon was red and white striped. It had a silver circle at the top to reflect heat. Because this balloon was designed to carry men, it had to be large. In fact, it was larger than any balloon John had ever seen. In the generation process, this phenomenon is accounted for by further constraining the choice of what to talk about next to the proposition with the greatest number of links to the potential focus list. Use Of The Focus TEXT uses the legal focus moves identified by Sidner by only matching schema predicates against propositions which have an argument that can be focused in satisfaction of the legal options. Thus, the matching process itself is constrained by the focus mechanism. The focus preferences developed for generation are used to select between remaining options. These options occur in TEXT when a predicate matches more than one piece of information in the relevant knowledge pool or when more than one alternative in a schema can be satisfied. In such cases, the focus guidelines are used to select the most appropriate proposition. When options exist, all propositions are selected which have as focused argument a member of the previous PFL. If none exist, then all propositions are selected whose focused argument is the previous current-focus. If none exist, then all propositions are selected whose focused argument is a member of the focus-stack. If these filtering steps do not narrow down the possibilities to a single proposition, that proposition with the greatest number of links to the previous PFL is selected for the answer. The focus and potential focus list of each proposition is maintained and passed to the tactical component for use in selecting syntactic constructions and pronominalization. Interaction of the focus constraints with the schemas means that although the same schema may be selected for different answers, it can be instantiated ° in different ways. Recall that the identificationschema was selected in response to the question &amp;quot;Shat is a ship?&amp;quot; and the four evidence, attributive, particular-illustration,were instantiated. identificationschema was also selected in response to the question &amp;quot;What is an aircraft carrier?&amp;quot;, but different predicates were instantiated as a result of the focus constraints: (definition AIRCRAFT-CARRIER) Schema selected: identification 1) identification 2) analogy 3) particular-illustration 4) amplification 5) evidence 1) An aircraft carrier is a surface ship with a DISPLACEMENT between 78000 and 80800 and a LENGTH between 1039 and 1063. 2) Aircraft carriers have a greater LENGTH than all other ships and a greater DISPLACEMENT than most other ships. 3) Mine warfare ships, for example, have a DISPLACEMENT of 320 and a LENGTH of 144. 4) All aircraft carriers in the ONR database have REMARKS of 0, FUEL TYPE of BNKR, FLAG of BLBL, BEAM of 252, ENDURANCE RANGE of 4000, ECONOMIC SPEED of 12, ENDURANCE SPEED of 30 PROPULSION of STMTURGRD. ship is classified as an aircraft carrier if the characters 1 through 2 of its HULL NO are CV. DIRECTIONS Several possibilities for further development of the research described here include 1) the use of the same strategies for responding to questions about attributes, events, and relations as well as to questions about entities, 2) investigation of strategies needed for responding to questions about the system processes (e.g. How is manufacturer&apos;s cost determined?) or system capabilities (e.g. Can you handle ellipsis?), 3) responding to presuppositional failure as well as to direct questions, and 4) the incorporation of a user model in the generation process (currently TEXT assumes a static casual, naive user and gears its responses to this characterization). This last feature could be used, among other ways, in determining the amount of detail required (see (MCKEOWN 82] for discussion of the recursive use of the schemes). 119 The TEXT system successfully incorporates principles of relevancy criteria, discourse structure, and focus constraints into a method for generating English text of paragraph length. Previous work on focus of attention has been extended for the task of generation to provide constraints on what to say next. Knowledge about discourse structure has been encoded into schemes that are used to guide the generation process. The use of these two interacting mechanisms constitutes a departure from earlier generation systems. The approach taken in this research is that the generation process should not simply trace the knowledge representation to produce text. Instead, communicative strategies people are familiar with are used to effectively convey information. This means that the same information may be described in different ways on different occasions. The result is a system which constructs and orders a message in response to a given question. Although the system was designed to generate answers to questions about database structure (a feature lacking in most natural language database systems), the same techniques and principles could be used in other application areas (for example, computer assisted instruction systems, expert systems, etc.) where generation of language is needed. Acknowledgements I would like to thank Aravind Joshi, Bonnie Webber, Kathleen McCoy, and Eric Mays for their invaluable comments on the style and content of this paper. Thanks also goes to Kathleen McCoy and Steven Bossie for their roles in implementing portions of the system. References 82]. S., &amp;quot;A tactical model for text generation: sentence generation using a functional grammar,&amp;quot; forthcoming M. S. thesis, University of Pennsylvania, Philadelphia, Pa., 1982. [CHEN 76]. Chen, P. P. S., &amp;quot;The entity-relationship model towards a unified view data.&amp;quot; ACM Transactionson Systems,</abstract>
<note confidence="0.972195">Vol. 1, No. 1. (1976). 75]. Grimes, J. E. The Discourse. Mouton, The Hague, PiFig. (1975). [GROSZ 77]. Grosz, B. J., &amp;quot;The representation and use of focus in dialogue understanding.&amp;quot; Technical note 151, Stanford Research Institute, Menlo Park, Ca. (1977). [LEE and GERRITSEN 78]. Gee, R. M. and R. Gerritsen, &amp;quot;Extended semantics LOC hierarchies&amp;quot;, in Proceedingsof the International conferenceon of Data,Austin, Tex., 1978. [KAY 79]. Kay, M. &amp;quot;Functional grammar.&amp;quot; Proceedingsof the 5th Meetingof the (1979).</note>
<author confidence="0.38708">A Design criteria for Malhotra</author>
<abstract confidence="0.7704299375">a knowledge-based English language system for management: an experimental analysis.&amp;quot; MAC TR-146, MIT, Cambridge, Mass. (1975). [MCCOY 82]. McCoy, K. F., &amp;quot;Augmenting a database knowledge representation for natural language in Proc.of the 20th Annual Conferenceof the ComputiEn= Linguistics,Tbronto, Canada, 1982. [MCKEOWN 80]. McKeown, K. R., &amp;quot;Generating relevant explanations: natural language responses to questions about database structure.&amp;quot; in Proceedingsof AAAI, Stanford Univ., (1980). pp. 306-9. (MCKEOWN 82]. McKeown, K. R., &amp;quot;Generating natural language text in response to questions about database structure.&amp;quot; Ph.D. dissertation,</abstract>
<note confidence="0.549669631578947">University of Pennsylvania, Philadelphia, Pa. 1982. [SHEPHERD 26]. Shepherd, H. R., The Fine Art of Writing, The Macmillan Co., New York, N. Y., 1926. [SIDNER 79]. Sidner, C. L., &amp;quot;Towards a computational theory of definite anaphora comprehension in English discourse.&amp;quot; Ph.D. dissertation, MIT Al Technical Report #TR-537, Cambridge, Mass. (1979). [SMITH and SMITH 77]. Smith, J. M. and Smith, D. C. P., &amp;quot;Database abstractions: aggregation and generalization.&amp;quot; University of ACM Transactionson Systems,Vol. 2, #2, June 1977, pp. 105-33. [TENNANT 79]. Tennant, H., &amp;quot;Experience with the evaluation of natural language question answerers.&amp;quot; Working paper #18, Univ. of Illinois, Urbana-Champaign, Ill. (1979). 120</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bossier</author>
</authors>
<title>A tactical model for text generation: sentence generation using a functional grammar,&amp;quot;</title>
<date>1982</date>
<journal>forthcoming M. S. thesis,</journal>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, Pa.,</location>
<marker>[BOSSIE 82]</marker>
<rawString>. Bossier S., &amp;quot;A tactical model for text generation: sentence generation using a functional grammar,&amp;quot; forthcoming M. S. thesis, University of Pennsylvania, Philadelphia, Pa., 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P S Chen</author>
</authors>
<title>The entity-relationship model - towards a unified view of data.&amp;quot;</title>
<date>1976</date>
<journal>ACM Transactions on Database Systems,</journal>
<volume>1</volume>
<marker>[CHEN 76]</marker>
<rawString>. Chen, P. P. S., &amp;quot;The entity-relationship model - towards a unified view of data.&amp;quot; ACM Transactions on Database Systems, Vol. 1, No. 1. (1976).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Grimes</author>
</authors>
<title>The Thread Discourse.</title>
<date>1975</date>
<location>Mouton, The Hague, PiFig.</location>
<marker>[GRIMES 75]</marker>
<rawString>. Grimes, J. E. The Thread Discourse. Mouton, The Hague, PiFig. (1975).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>The representation and use of focus in dialogue understanding.&amp;quot; Technical note 151, Stanford Research Institute,</title>
<date>1977</date>
<location>Menlo Park, Ca.</location>
<marker>[GROSZ 77]</marker>
<rawString>. Grosz, B. J., &amp;quot;The representation and use of focus in dialogue understanding.&amp;quot; Technical note 151, Stanford Research Institute, Menlo Park, Ca. (1977).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Gee</author>
<author>R Gerritsen</author>
</authors>
<title>Extended semantics LOC generalization hierarchies&amp;quot;,</title>
<date>1978</date>
<booktitle>in Proceedings of the 1978 ACM-SIGMOD International conference on Management of Data,</booktitle>
<location>Austin, Tex.,</location>
<marker>[LEE and GERRITSEN 78]</marker>
<rawString>. Gee, R. M. and R. Gerritsen, &amp;quot;Extended semantics LOC generalization hierarchies&amp;quot;, in Proceedings of the 1978 ACM-SIGMOD International conference on Management of Data, Austin, Tex., 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional grammar.&amp;quot;</title>
<date>1979</date>
<booktitle>Proceedings of the 5th Annual Meeting of the Berkeley LingilistIE-soFrety.</booktitle>
<marker>[KAY 79]</marker>
<rawString>. Kay, M. &amp;quot;Functional grammar.&amp;quot; Proceedings of the 5th Annual Meeting of the Berkeley LingilistIE-soFrety. (1979).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Malhotra</author>
</authors>
<title>Design criteria for a knowledge-based English language system for management: an experimental analysis.&amp;quot; MAC TR-146, MIT,</title>
<date>1975</date>
<location>Cambridge, Mass.</location>
<marker>[MALHOTRA 75]</marker>
<rawString>. Malhotra, A. &amp;quot;Design criteria for a knowledge-based English language system for management: an experimental analysis.&amp;quot; MAC TR-146, MIT, Cambridge, Mass. (1975).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K F McCoy</author>
</authors>
<title>Augmenting a database knowledge representation for natural language generation,&amp;quot;</title>
<date>1982</date>
<booktitle>in Proc. of the 20th Annual Conference of the Assoirition-Tbr ComputiEn= Linguistics,</booktitle>
<location>Tbronto, Canada,</location>
<marker>[MCCOY 82]</marker>
<rawString>. McCoy, K. F., &amp;quot;Augmenting a database knowledge representation for natural language generation,&amp;quot; in Proc. of the 20th Annual Conference of the Assoirition-Tbr ComputiEn= Linguistics, Tbronto, Canada, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Generating relevant explanations: natural language responses to questions about database structure.&amp;quot;</title>
<date>1980</date>
<booktitle>in Proceedings of AAAI,</booktitle>
<pages>306--9</pages>
<institution>University of Pennsylvania,</institution>
<location>Stanford Univ., Stanford, Ca.</location>
<note>Ph.D. dissertation,</note>
<marker>[MCKEOWN 80]</marker>
<rawString>. McKeown, K. R., &amp;quot;Generating relevant explanations: natural language responses to questions about database structure.&amp;quot; in Proceedings of AAAI, Stanford Univ., Stanford, Ca. (1980). pp. 306-9. (MCKEOWN 82]. McKeown, K. R., &amp;quot;Generating natural language text in response to questions about database structure.&amp;quot; Ph.D. dissertation, University of Pennsylvania, Philadelphia, Pa. 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H R Shepherd</author>
</authors>
<title>The Fine Art of Writing,</title>
<date>1926</date>
<publisher>The Macmillan Co.,</publisher>
<location>New York, N. Y.,</location>
<marker>[SHEPHERD 26]</marker>
<rawString>. Shepherd, H. R., The Fine Art of Writing, The Macmillan Co., New York, N. Y., 1926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
</authors>
<title>Towards a computational theory of definite anaphora comprehension in English discourse.&amp;quot;</title>
<date>1979</date>
<tech>Technical Report #TR-537,</tech>
<institution>MIT Al</institution>
<location>Cambridge, Mass.</location>
<note>Ph.D. dissertation,</note>
<marker>[SIDNER 79]</marker>
<rawString>. Sidner, C. L., &amp;quot;Towards a computational theory of definite anaphora comprehension in English discourse.&amp;quot; Ph.D. dissertation, MIT Al Technical Report #TR-537, Cambridge, Mass. (1979).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J M Smith</author>
</authors>
<note>and</note>
<marker>[SMITH and SMITH 77]</marker>
<rawString>. Smith, J. M. and</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>