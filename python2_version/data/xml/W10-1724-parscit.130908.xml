<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015981">
<title confidence="0.882269">
Linear Inversion Transduction Grammar Alignments
as a Second Translation Path
</title>
<author confidence="0.940376">
Markus SAERS and Joakim NIVRE
</author>
<affiliation confidence="0.90315675">
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
</affiliation>
<email confidence="0.989867">
first.last@lingfil.uu.se
</email>
<sectionHeader confidence="0.997237" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999501666666667">
We explore the possibility of using
Stochastic Bracketing Linear Inversion
Transduction Grammars for a full-scale
German–English translation task, both on
their own and in conjunction with align-
ments induced with GIZA++. The ratio-
nale for transduction grammars, the details
of the system and some results are pre-
sented.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999399631578947">
Lately, there has been some interest in using In-
version Transduction Grammars (ITGs) for align-
ment purposes. The main problem with ITGs is the
time complexity, O(Gn6) doesn’t scale well. By
limiting the grammar to a bracketing ITG (BITG),
the grammar constant (G) can be eliminated, but
O(n6) is still prohibitive for large data sets.
There has been some work on approximate in-
ference of ITGs. Zhang et al. (2008) present a
method for evaluating spans in the sentence pair
to determine whether they should be excluded or
not. The algorithm has a best case time com-
plexity of O(n3). Saers, Nivre &amp; Wu (2009) in-
troduce a beam pruning scheme, which reduces
time complexity to O(bn3). They also show
that severe pruning is possible without significant
deterioration in alignment quality (as measured
by downstream translation quality). Haghighi et
al. (2009) use a simpler aligner as guidance for
pruning, which reduces the time complexity by
two orders of magnitude. Their work also par-
tially implements the phrasal ITGs for translation-
driven segmentation introduced in Wu (1997), al-
though they only allow for one-to-many align-
ments, rather than many-to-many alignments. A
more extreme approach is taken in Saers, Nivre
&amp; Wu (2010). Not only is the search severely
pruned, but the grammar itself is limited to a lin-
earized form, getting rid of branching within a sin-
gle parse. Although a small deterioration in down-
stream translation quality is noted (compared to
harshly pruned SBITGs), the grammar can be in-
duced in linear time.
In this paper we apply SBLITGs to a full size
German–English WMT’10 translation task. We
also use differentiated translation paths to com-
bine SBLITG translation models with a standard
GIZA++ translation model.
</bodyText>
<sectionHeader confidence="0.987709" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999981827586207">
A transduction grammar is a grammar that gener-
ates a pair of languages. In a transduction gram-
mar, the terminal symbols consist of pairs of to-
kens where the first is taken from the vocabulary
of one of the languages, and the second from the
vocabulary of the other. Transduction grammars
have to our knowledge been restricted to trans-
duce between languages no more complex than
context-free languages (CFLs). Transduction be-
tween CFLs was first described in Lewis &amp; Stearns
(1968), and then further explored in Aho &amp; Ull-
man (1972). The main motivation for explor-
ing this was to build programming language com-
pilers, which essentially translate between source
code and machine code. There are two types of
transduction grammars between CFLs described in
the computer science literature: simple transduc-
tion grammars (STGs) and syntax-directed trans-
duction grammars (SDTGs). The difference be-
tween them is that STGs are monotone, whereas
SDTGs allow unlimited reordering in rule produc-
tions. Both allow the use of singletons to insert
and delete tokens from either language. A sin-
gleton is a biterminal where one of the tokens is
the empty string (E). Neither STGs nor SDTGs
are intuitively useful in translating natural lan-
guages, since STGs have no way to model reorder-
ing, and SDTGs require exponential time to be in-
duced from examples (parallel corpora). Since
</bodyText>
<figure confidence="0.942329666666667">
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
</figure>
<page confidence="0.93199">
167
</page>
<note confidence="0.4441105">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167–171,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999322724137931">
compilers in general work on well defined, manu-
ally specified programming languages, there is no
need to induce them from examples, so the expo-
nential complexity is not a problem in this setting
– SDTGs can transduce in O(n3) time, so once the
grammar is known they can be used to translate
efficiently.
In natural language translation, the grammar is
generally not known, in fact, state-of-the art trans-
lation systems rely heavily on machine learning.
For transduction grammars, this means that they
have to be induced from parallel corpora.
An inversion transduction grammar (ITG)
strikes a good balance between STGs and SDTGs,
as it allows some reordering, while requiring only
polynomial time to be induced from parallel cor-
pora. The allowed reordering is either the iden-
tity permutation of the production, or the inver-
sion permutation. Restricting the permutations in
this way ensures that an ITG can be expressed in
two-normal form, which is the key property for
avoiding exponential time complexity in biparsing
(parsing of a sentence pair).
An ITG in two-normal form (representing the
transduction between L1 and L2) is written with
identity productions in square brackets, and in-
verted productions in angle brackets. Each such
rule can be construed to represent two (one L1 and
one L2) synchronized CFG rules:
</bodyText>
<equation confidence="0.66708625">
ITGLI L2 CFGL1 CFGL2
A → [BC] A → B C A → B C
A → h B C i A → B C A → C B
A → e/f A → e A → f
</equation>
<bodyText confidence="0.9998426">
Inducing an ITG from a parallel corpus is still slow,
as the time complexity is O(Gn6). Several ways
to get around this has been proposed (Zhang et al.,
2008; Haghighi et al., 2009; Saers et al., 2009;
Saers et al., 2010).
Taking a closer look at the linear ITGs (Saers et
al., 2010), there are five rules in normal form. De-
composing these five rule types into monolingual
rule types reveals that the monolingual grammars
are linear grammars (LGs):
</bodyText>
<equation confidence="0.601491">
LITGL1�L2 LGL1 LGL2
</equation>
<bodyText confidence="0.919232086956522">
A → [ e/f C ] A → e C A → f C
A → [ B e/f ] A → B e A → B f
A → h e/f C i A → e C A → C f
A → h B e/f i A → B e A → f B
A → E/E A → E A → E
This means that LITGs are transduction grammars
that transduce between linear languages.
There is also a nice parallel in search time com-
plexities between CFGs and ITGs on the one hand,
and LGs and LITGs on the other. Searching for
all possible parses given a sentence is O(n3) for
CFGs, and O(n2) for LGs. Searching for all possi-
ble biparses given a bisentence is O(n6) for ITGs,
and O(n4) for LITGs. This is consistent with
thinking of biparsing as finding every L2 parse for
every L1 parse. Biparsing consists of assigning a
joint structure to a sentence pair, rather than as-
signing a structure to a sentence.
In this paper, only stochastic bracketing gram-
mars (SBITGs and SBLITGs) were used. A brack-
eting grammar has only one nonterminal symbol,
denoted X. A stochastic grammar is one where
each rule is associated with a probability, such that
</bodyText>
<equation confidence="0.99419375">
� �
��
∀X p(X → �) � 1 �
�
</equation>
<bodyText confidence="0.999918666666667">
While training a Stochastic Bracketing ITG
(SBITG) or LITG (SBLITG) with EM, expectations
of probabilities over the biparse-forest are calcu-
lated. These expectations approach the true prob-
abilities, and can be used as approximations. The
probabilities over the biparse-forest can be used
to select the one-best parse-tree, which in turn
forces an alignment over the sentence pair. The
alignments given by SBITGs and SBLITGs has been
shown to give better translation quality than bidi-
rectional IBM-models, when applied to short sen-
tence corpora (Saers and Wu, 2009; Saers et al.,
2009; Saers et al., 2010). In this paper we ex-
plore whether this hold for SBLITGs on standard
sentence corpora.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="method">
3 Setup
</sectionHeader>
<bodyText confidence="0.998355384615385">
The baseline system for the shared task was a
phrase based translation model based on bidi-
rectional IBM- (Brown et al., 1993) and HMM-
models (Vogel et al., 1996) combined with the
grow-diag-final-and heuristic. This is
computed with the GIZA++ tool (Och and Ney,
2003) and the Moses toolkit (Koehn et al., 2007).
The language model was a 5-gram SRILM (Stol-
cke, 2002). Parameters in the final translation sys-
tem were determined with Minimum Error-Rate
Training (Och, 2003), and translation quality was
assessed with the automatic measures BLEU (Pap-
ineni et al., 2002) and NIST (Doddington, 2002).
</bodyText>
<page confidence="0.979984">
168
</page>
<table confidence="0.998920166666667">
Corpus Type Size
German–English Europarl out of domain 1,219,343 sentence pairs
German–English news commentary in-domain 86,941 sentence pairs
English news commentary in-domain 48,653,884 sentences
German–English news commentary in-domain tuning data 2,051 sentence pairs
German–English news commentary in-domain test data 2,489 sentence pairs
</table>
<tableCaption confidence="0.997267">
Table 1: Corpora available for the German–English translation task after baseline cleaning.
</tableCaption>
<table confidence="0.999849375">
System BLEU NIST
GIZA++ 17.88 5.9748
SBLITG 17.61 5.8846
SBLITG (only Europarl) 17.46 5.8491
SBLITG (only news) 15.49 5.4987
GIZA++ and SBLITG 17.66 5.9650
GIZA++ and SBLITG (only Europarl) 17.58 5.9819
GIZA++ and SBLITG (only news) 17.48 5.9693
</table>
<tableCaption confidence="0.996907">
Table 2: Results for the German–English translation task.
</tableCaption>
<bodyText confidence="0.999972325">
We chose to focus on the German–English
translation task. The corpora resources available
for that task is summarized in Table 1. We used the
entire news commentary monolingual data con-
catenated with the English side of the Europarl
bilingual data to train the language model. In ret-
rospect, this was probably a bad choice, as others
seem to prefer the use of two language models in-
stead.
We contrasted the baseline system with pure
SBLITG systems trained on different parts of the
training data, as well as combined systems, where
the SBLITG systems were combined with the base-
line system. The combination was done by adding
the SBLITG translation model as a second transla-
tion path to the base line system.
To train our SBLITG systems, we used the algo-
rithm described in Saers et al. (2010). We set the
beam size parameter to 50, and ran expectation-
maximization for 10 iterations or until the log-
probability of the training corpus started deterio-
rating. After the grammar was induced we ob-
tained the one-best parse for each sentence pair,
which also dictates a word alignment over that
sentence pair, which we used instead of the word
alignments provided by GIZA++. From that point,
training did not differ from the baseline procedure.
We trained a total of three pure SBLITG system,
one with only the news commentary part of the
corpus, one with only the Europarl part, and one
with both. We also combined all three SBLITG
systems with the baseline system to see whether
the additional translation paths would help.
The system we submitted corresponds to the
“GIZA++ and SBLITG (only news)” system, but
with RandLM (Talbot and Osborne, 2007) as lan-
guage model rather than SRILM. This was because
we lacked the necessary RAM resources to calcu-
late the full SRILM model before the system sub-
mission deadline.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999970444444444">
The results for the development test set are sum-
marized in Table 2. The submitted system
achieved a BLEU score of 0.1759 and a NIST
score of 5.9579 for cased output on this year’s test
set (these numbers are not comparable to those
in Table 2). To our surprise, adding the addi-
tional phrases as a second translation path does
not seem to help. Instead a small deterioration
in BLEU is noted (0.22–0.40 points), whereas the
differences in NIST are mixed (-0.0098–+0.0071
points). Over all the variations were very small.
The pure SBLITG systems perform consistently
below baseline, which could indicate that the
grammar class is unable to capture the reorderings
found in longer sentence pairs adequately in one
parse. The variation between the pure SBLITG sys-
tems can be explained by the size of the training
data: more data – better quality.
</bodyText>
<page confidence="0.998824">
169
</page>
<sectionHeader confidence="0.998359" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99994">
We tried to use SBLITGs as word aligners on full
size sentences, which has not been done to date,
and noted that the formalism seems unable to ac-
count for the full complexity of longer sentence
pairs. We also tried combining the translation
models acquired with SBLITG alignments to the
baseline system, and noted very small differences,
tending to a deterioration in quality. The fact that
SBLITGs seem unable to capture the complex re-
lationship between an English and a German sen-
tence in one parse means that we need to find ei-
ther some more complex model or some way to
use the entire parse forest to arrive at the align-
ment.
</bodyText>
<sectionHeader confidence="0.994345" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999973785714286">
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under
research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency. The com-
putations were performed on UPPMAX resources
under project p2007020.
</bodyText>
<sectionHeader confidence="0.999126" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999425391891892">
Jeffrey D. Aho, Alfred V. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Englewood Cliffs, NJ.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of Human
Language Technology conference (HLT-2002), San
Diego, California.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 923–
931, Suntec, Singapore, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-
directed transduction. Journal of the Association for
Computing Machinery, 15(3):465–488.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160–167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, July.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from Stochas-
tic Inversion Transduction Grammars. In Proceed-
ings of the Third Workshop on Syntax and Structure
in Statistical Translation (SSST-3) at NAACL HLT
2009, pages 28–36, Boulder, Colorado, June.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Transduc-
tion Grammars with a cubic time biparsing algo-
rithm. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT’09), pages
29–32, Paris, France, October.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings of
Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, California, June.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
512–519, Prague, Czech Republic, June.
</reference>
<page confidence="0.969993">
170
</page>
<reference confidence="0.999347384615385">
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836–841, Mor-
ristown, New Jersey.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97–105,
Columbus, Ohio, June.
</reference>
<page confidence="0.998208">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.136673">
<title confidence="0.866226">Linear Inversion Transduction Grammar</title>
<author confidence="0.28231">as a Second Translation Path</author>
<affiliation confidence="0.8972675">Computational Linguistics Dept. of Linguistics and</affiliation>
<address confidence="0.589471">Uppsala</address>
<abstract confidence="0.9826099">We explore the possibility of using Stochastic Bracketing Linear Inversion Transduction Grammars for a full-scale German–English translation task, both on their own and in conjunction with aligninduced with The rationale for transduction grammars, the details of the system and some results are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeffrey D Aho</author>
<author>Alfred V Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. PrenticeHalll,</booktitle>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="2851" citStr="Aho &amp; Ullman (1972)" startWordPosition="451" endWordPosition="455">combine SBLITG translation models with a standard GIZA++ translation model. 2 Background A transduction grammar is a grammar that generates a pair of languages. In a transduction grammar, the terminal symbols consist of pairs of tokens where the first is taken from the vocabulary of one of the languages, and the second from the vocabulary of the other. Transduction grammars have to our knowledge been restricted to transduce between languages no more complex than context-free languages (CFLs). Transduction between CFLs was first described in Lewis &amp; Stearns (1968), and then further explored in Aho &amp; Ullman (1972). The main motivation for exploring this was to build programming language compilers, which essentially translate between source code and machine code. There are two types of transduction grammars between CFLs described in the computer science literature: simple transduction grammars (STGs) and syntax-directed transduction grammars (SDTGs). The difference between them is that STGs are monotone, whereas SDTGs allow unlimited reordering in rule productions. Both allow the use of singletons to insert and delete tokens from either language. A singleton is a biterminal where one of the tokens is th</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Jeffrey D. Aho, Alfred V. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. PrenticeHalll, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7714" citStr="Brown et al., 1993" startWordPosition="1315" endWordPosition="1318">n be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,94</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology conference (HLT-2002),</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="8182" citStr="Doddington, 2002" startWordPosition="1392" endWordPosition="1393">ntence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news commentary in-domain 48,653,884 sentences German–English news commentary in-domain tuning data 2,051 sentence pairs German–English news commentary in-domain test data 2,489 sentence pairs Table 1: Corpora available for the German–English translation task after baseline cleaning. System BLEU NIST GIZA++ 17.88 5.9748 SBLITG 17.61 5.8846 SBLITG (only Europarl) 17.46 5.8491 SBLITG (only news) 15.49 5.4987 GIZA++ and SBLITG 17.66 5.9650 GI</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of Human Language Technology conference (HLT-2002), San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>923--931</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1424" citStr="Haghighi et al. (2009)" startWordPosition="215" endWordPosition="218">BITG), the grammar constant (G) can be eliminated, but O(n6) is still prohibitive for large data sets. There has been some work on approximate inference of ITGs. Zhang et al. (2008) present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3). Saers, Nivre &amp; Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in Wu (1997), although they only allow for one-to-many alignments, rather than many-to-many alignments. A more extreme approach is taken in Saers, Nivre &amp; Wu (2010). Not only is the search severely pruned, but the grammar itself is limited to a linearized form, getting rid of branching within a single parse. Although a small deterioration in downstream translation quality is noted (compa</context>
<context position="5585" citStr="Haghighi et al., 2009" startWordPosition="910" endWordPosition="913">iding exponential time complexity in biparsing (parsing of a sentence pair). An ITG in two-normal form (representing the transduction between L1 and L2) is written with identity productions in square brackets, and inverted productions in angle brackets. Each such rule can be construed to represent two (one L1 and one L2) synchronized CFG rules: ITGLI L2 CFGL1 CFGL2 A → [BC] A → B C A → B C A → h B C i A → B C A → C B A → e/f A → e A → f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITGL1�L2 LGL1 LGL2 A → [ e/f C ] A → e C A → f C A → [ B e/f ] A → B e A → B f A → h e/f C i A → e C A → C f A → h B e/f i A → B e A → f B A → E/E A → E A → E This means that LITGs are transduction grammars that transduce between linear languages. There is also a nice parallel in search time complexities between CFGs and ITGs on</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 923– 931, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7899" citStr="Koehn et al., 2007" startWordPosition="1346" endWordPosition="1349">ments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news commentary in-domain 48,653,884 sentences German–English news commentary in-domain tuning data 2,051 sentence pairs German–English news commentary in-domai</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M Lewis</author>
<author>Richard E Stearns</author>
</authors>
<title>Syntaxdirected transduction.</title>
<date>1968</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="2801" citStr="Lewis &amp; Stearns (1968)" startWordPosition="442" endWordPosition="445">ask. We also use differentiated translation paths to combine SBLITG translation models with a standard GIZA++ translation model. 2 Background A transduction grammar is a grammar that generates a pair of languages. In a transduction grammar, the terminal symbols consist of pairs of tokens where the first is taken from the vocabulary of one of the languages, and the second from the vocabulary of the other. Transduction grammars have to our knowledge been restricted to transduce between languages no more complex than context-free languages (CFLs). Transduction between CFLs was first described in Lewis &amp; Stearns (1968), and then further explored in Aho &amp; Ullman (1972). The main motivation for exploring this was to build programming language compilers, which essentially translate between source code and machine code. There are two types of transduction grammars between CFLs described in the computer science literature: simple transduction grammars (STGs) and syntax-directed transduction grammars (SDTGs). The difference between them is that STGs are monotone, whereas SDTGs allow unlimited reordering in rule productions. Both allow the use of singletons to insert and delete tokens from either language. A singl</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Philip M. Lewis and Richard E. Stearns. 1968. Syntaxdirected transduction. Journal of the Association for Computing Machinery, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7856" citStr="Och and Ney, 2003" startWordPosition="1338" endWordPosition="1341">lignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news commentary in-domain 48,653,884 sentences German–English news commentary in-domain tuning data 2,051 sentence pa</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="8059" citStr="Och, 2003" startWordPosition="1373" endWordPosition="1374">2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news commentary in-domain 48,653,884 sentences German–English news commentary in-domain tuning data 2,051 sentence pairs German–English news commentary in-domain test data 2,489 sentence pairs Table 1: Corpora available for the German–English translation task after baseline cleaning. System BLEU NIST GIZA++ 17.88 5.974</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="8154" citStr="Papineni et al., 2002" startWordPosition="1385" endWordPosition="1389">s hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news commentary in-domain 48,653,884 sentences German–English news commentary in-domain tuning data 2,051 sentence pairs German–English news commentary in-domain test data 2,489 sentence pairs Table 1: Corpora available for the German–English translation task after baseline cleaning. System BLEU NIST GIZA++ 17.88 5.9748 SBLITG 17.61 5.8846 SBLITG (only Europarl) 17.46 5.8491 SBLITG (only news) 15.49 5.4987 GIZA+</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrasebased translation via word alignments from Stochastic Inversion Transduction Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009,</booktitle>
<pages>28--36</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="7453" citStr="Saers and Wu, 2009" startWordPosition="1268" endWordPosition="1271">th a probability, such that � � �� ∀X p(X → �) � 1 � � While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och,</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>Markus Saers and Dekai Wu. 2009. Improving phrasebased translation via word alignments from Stochastic Inversion Transduction Grammars. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009, pages 28–36, Boulder, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning Stochastic Bracketing Inversion Transduction Grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>29--32</pages>
<location>Paris, France,</location>
<contexts>
<context position="5605" citStr="Saers et al., 2009" startWordPosition="914" endWordPosition="917">complexity in biparsing (parsing of a sentence pair). An ITG in two-normal form (representing the transduction between L1 and L2) is written with identity productions in square brackets, and inverted productions in angle brackets. Each such rule can be construed to represent two (one L1 and one L2) synchronized CFG rules: ITGLI L2 CFGL1 CFGL2 A → [BC] A → B C A → B C A → h B C i A → B C A → C B A → e/f A → e A → f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITGL1�L2 LGL1 LGL2 A → [ e/f C ] A → e C A → f C A → [ B e/f ] A → B e A → B f A → h e/f C i A → e C A → C f A → h B e/f i A → B e A → f B A → E/E A → E A → E This means that LITGs are transduction grammars that transduce between linear languages. There is also a nice parallel in search time complexities between CFGs and ITGs on the one hand, and L</context>
<context position="7473" citStr="Saers et al., 2009" startWordPosition="1272" endWordPosition="1275">ch that � � �� ∀X p(X → �) � 1 � � While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translat</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. 2009. Learning Stochastic Bracketing Inversion Transduction Grammars with a cubic time biparsing algorithm. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 29–32, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Word alignment with Stochastic Bracketing Linear Inversion Transduction Grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles, California,</location>
<contexts>
<context position="5626" citStr="Saers et al., 2010" startWordPosition="918" endWordPosition="921">ing (parsing of a sentence pair). An ITG in two-normal form (representing the transduction between L1 and L2) is written with identity productions in square brackets, and inverted productions in angle brackets. Each such rule can be construed to represent two (one L1 and one L2) synchronized CFG rules: ITGLI L2 CFGL1 CFGL2 A → [BC] A → B C A → B C A → h B C i A → B C A → C B A → e/f A → e A → f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITGL1�L2 LGL1 LGL2 A → [ e/f C ] A → e C A → f C A → [ B e/f ] A → B e A → B f A → h e/f C i A → e C A → C f A → h B e/f i A → B e A → f B A → E/E A → E A → E This means that LITGs are transduction grammars that transduce between linear languages. There is also a nice parallel in search time complexities between CFGs and ITGs on the one hand, and LGs and LITGs on the o</context>
<context position="7494" citStr="Saers et al., 2010" startWordPosition="1276" endWordPosition="1279">X → �) � 1 � � While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was asses</context>
<context position="9720" citStr="Saers et al. (2010)" startWordPosition="1628" endWordPosition="1631">ual data concatenated with the English side of the Europarl bilingual data to train the language model. In retrospect, this was probably a bad choice, as others seem to prefer the use of two language models instead. We contrasted the baseline system with pure SBLITG systems trained on different parts of the training data, as well as combined systems, where the SBLITG systems were combined with the baseline system. The combination was done by adding the SBLITG translation model as a second translation path to the base line system. To train our SBLITG systems, we used the algorithm described in Saers et al. (2010). We set the beam size parameter to 50, and ran expectationmaximization for 10 iterations or until the logprobability of the training corpus started deteriorating. After the grammar was induced we obtained the one-best parse for each sentence pair, which also dictates a word alignment over that sentence pair, which we used instead of the word alignments provided by GIZA++. From that point, training did not differ from the baseline procedure. We trained a total of three pure SBLITG system, one with only the news commentary part of the corpus, one with only the Europarl part, and one with both. </context>
</contexts>
<marker>Saers, Nivre, Wu, 2010</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Word alignment with Stochastic Bracketing Linear Inversion Transduction Grammar. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="7954" citStr="Stolcke, 2002" startWordPosition="1357" endWordPosition="1359">ter translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news commentary in-domain 48,653,884 sentences German–English news commentary in-domain tuning data 2,051 sentence pairs German–English news commentary in-domain test data 2,489 sentence pairs Table 1: Corpora avail</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>512--519</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10572" citStr="Talbot and Osborne, 2007" startWordPosition="1771" endWordPosition="1774">r each sentence pair, which also dictates a word alignment over that sentence pair, which we used instead of the word alignments provided by GIZA++. From that point, training did not differ from the baseline procedure. We trained a total of three pure SBLITG system, one with only the news commentary part of the corpus, one with only the Europarl part, and one with both. We also combined all three SBLITG systems with the baseline system to see whether the additional translation paths would help. The system we submitted corresponds to the “GIZA++ and SBLITG (only news)” system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. This was because we lacked the necessary RAM resources to calculate the full SRILM model before the system submission deadline. 4 Results The results for the development test set are summarized in Table 2. The submitted system achieved a BLEU score of 0.1759 and a NIST score of 5.9579 for cased output on this year’s test set (these numbers are not comparable to those in Table 2). To our surprise, adding the additional phrases as a second translation path does not seem to help. Instead a small deterioration in BLEU is noted (0.22–0.40 points), whereas the d</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512–519, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>836--841</pages>
<location>Morristown, New Jersey.</location>
<contexts>
<context position="7749" citStr="Vogel et al., 1996" startWordPosition="1322" endWordPosition="1325">obabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus Type Size German–English Europarl out of domain 1,219,343 sentence pairs German–English news commentary in-domain 86,941 sentence pairs English news comme</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, pages 836–841, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1646" citStr="Wu (1997)" startWordPosition="252" endWordPosition="253"> pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3). Saers, Nivre &amp; Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in Wu (1997), although they only allow for one-to-many alignments, rather than many-to-many alignments. A more extreme approach is taken in Saers, Nivre &amp; Wu (2010). Not only is the search severely pruned, but the grammar itself is limited to a linearized form, getting rid of branching within a single parse. Although a small deterioration in downstream translation quality is noted (compared to harshly pruned SBITGs), the grammar can be induced in linear time. In this paper we apply SBLITGs to a full size German–English WMT’10 translation task. We also use differentiated translation paths to combine SBLITG</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="983" citStr="Zhang et al. (2008)" startWordPosition="144" endWordPosition="147">translation task, both on their own and in conjunction with alignments induced with GIZA++. The rationale for transduction grammars, the details of the system and some results are presented. 1 Introduction Lately, there has been some interest in using Inversion Transduction Grammars (ITGs) for alignment purposes. The main problem with ITGs is the time complexity, O(Gn6) doesn’t scale well. By limiting the grammar to a bracketing ITG (BITG), the grammar constant (G) can be eliminated, but O(n6) is still prohibitive for large data sets. There has been some work on approximate inference of ITGs. Zhang et al. (2008) present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3). Saers, Nivre &amp; Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal </context>
<context position="5562" citStr="Zhang et al., 2008" startWordPosition="906" endWordPosition="909">key property for avoiding exponential time complexity in biparsing (parsing of a sentence pair). An ITG in two-normal form (representing the transduction between L1 and L2) is written with identity productions in square brackets, and inverted productions in angle brackets. Each such rule can be construed to represent two (one L1 and one L2) synchronized CFG rules: ITGLI L2 CFGL1 CFGL2 A → [BC] A → B C A → B C A → h B C i A → B C A → C B A → e/f A → e A → f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITGL1�L2 LGL1 LGL2 A → [ e/f C ] A → e C A → f C A → [ B e/f ] A → B e A → B f A → h e/f C i A → e C A → C f A → h B e/f i A → B e A → f B A → E/E A → E A → E This means that LITGs are transduction grammars that transduce between linear languages. There is also a nice parallel in search time complexities b</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proceedings of ACL-08: HLT, pages 97–105, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>