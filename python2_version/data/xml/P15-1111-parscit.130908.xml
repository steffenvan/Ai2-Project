<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001969">
<title confidence="0.993325">
Identifying Cascading Errors using Constraints in Dependency Parsing
</title>
<author confidence="0.999439">
Dominick Ng and James R. Curran
</author>
<affiliation confidence="0.998432">
a-lab, School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.667655">
NSW 2006, Australia
</address>
<email confidence="0.999147">
{dominick.ng,james.r.curran}@sydney.edu.au
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99979765">
Dependency parsers are usually evaluated
on attachment accuracy. Whilst easily in-
terpreted, the metric does not illustrate
the cascading impact of errors, where the
parser chooses an incorrect arc, and is sub-
sequently forced to choose further incor-
rect arcs elsewhere in the parse.
We apply arc-level constraints to MST-
parser and ZPar, enforcing the correct
analysis of specific error classes, whilst
otherwise continuing with decoding. We
investigate the direct and indirect impact
of applying constraints to the parser. Er-
roneous NP and punctuation attachments
cause the most cascading errors, while in-
correct PP and coordination attachments
are frequent but less influential. Punctu-
ation is especially challenging, as it has
long been ignored in parsing, and serves
a variety of disparate syntactic roles.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967096153846">
Dependency parsers are evaluated using word-
level attachment accuracy. Whilst comparable
across systems, this does not provide insight into
why the parser makes certain errors, or whether
certain misattachments are caused by other errors.
For example, incorrectly identifying a modifier
head may only introduce a single attachment error,
while misplacing the root of a sentence will create
substantially more errors elsewhere. In projective
dependency parsing, erroneous arcs can also force
the parser to select other incorrect arcs.
Kummerfeld et al. (2012) propose a static post-
parsing analysis to categorise groups of bracket er-
rors in constituency parsing into higher level error
classes such as clause attachment. However, this
cannot account for cascading changes resulting
from repairing errors, or limitations which may
prevent the parser from applying a repair. It is un-
clear whether the parser will apply the repair op-
eration in its entirety, or if it will introduce other
changes in response to the repairs.
We develop an evaluation procedure to evalu-
ate the influence of each error class in dependency
parsing without making assumptions about how
the parser will behave. We define error classes
based on dependency labels, and use the depen-
dencies in each class as arc constraints specifying
the correct head and label for particular words in
each sentence. We adapt parsers to apply these
constraints, whilst otherwise proceeding with de-
coding under their grammar and model. By eval-
uating performance with and without constraints,
we can directly observe the cascading impact of
each error class on each the parser.
We implement our procedure for the graph-
based MSTparser (McDonald and Pereira, 2006)
and the transition-based ZPar (Zhang and Clark,
2011) using basic Stanford dependencies over the
OntoNotes 4.0 release of the WSJ Penn Treebank
data. Our results show that erroneously attach-
ing NPs, PPs, modifiers, and punctuation have the
largest overall impact on UAS. Of those, NPs and
punctuation have the most substantial cascading
impact, indicating that these errors have the most
effect on the remainder of the parse. Enforcing
correct punctuation arcs has a particularly large
impact on accuracy, even though most evaluation
scripts ignore punctuation. We find that punctu-
ation arcs are commonly misplaced by large dis-
tances in the final parse, crossing over and forcing
other arcs to be incorrect in the process.
We will make our source code available, and
</bodyText>
<page confidence="0.940021">
1148
</page>
<note confidence="0.983992666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1148–1158,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.994646941176471">
nsubjpass
root
punct
det
amod
auxpass
auxpass
cc det
ccomp
nn
advmod
punct
nn
nsubj
neg
The LME stocks decline was about as expected , but the Comex gain was n’t .
The LME stocks decline was about as expected , but the Comex gain was n’t .
</figure>
<figureCaption confidence="0.9920535">
Figure 1: MSTparser output (top) and the gold parse (bottom) for a WSJ 22 sentence. MSTparser pro-
duces two independent errors: an NP bracketing error (red, dotted), and an incorrect root (blue, dashed).
</figureCaption>
<figure confidence="0.979586142857143">
nn nn nsubj
det
root
ccomp
advmod
punct
advmod
cc
conj
punct
det
nn
nsubj
neg
</figure>
<table confidence="0.728311333333333">
Parser UAS LAS usent lsent
MSTparser 91.3 87.5 41.3 26.1
ZPar 91.7 89.3 45.1 35.9
</table>
<tableCaption confidence="0.8654265">
Table 1: Baseline UAS and LAS scores on Stanford
dependencies over WSJ 22.
</tableCaption>
<bodyText confidence="0.9846065">
hope that our findings will drive efforts address-
ing the remaining dependency parsing errors.
</bodyText>
<sectionHeader confidence="0.983857" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.9998474">
Table 1 summarises the performance of MST-
parser and ZPar on Stanford dependencies over
OntoNotes 4 WSJ 22. ZPar performs slightly
better than MSTparser on UAS, and substantially
better on LAS. However, these numbers do not
show what types of errors are being made by each
parser, what errors remain to be addressed, or hint
at what underlying problems cause each error.
Figure 1 depicts a WSJ 22 sentence as parsed by
MSTparser, and the gold parse. The UAS is 47.1%,
with 8 of 17 arcs correct. By contrast, ZPar (parse
not shown) scores 94.1%, with the sole attachment
error being on LME (as with MSTparser). While
there are nine incorrect arcs overall, MSTparser
seems to have made only two underlying errors:
</bodyText>
<listItem confidence="0.996468666666667">
• LME attached to decline rather than stocks (NP
internal). Correcting this repairs one error;
• expected being chosen as the root rather than
was. Correcting the root and moving all at-
tachments to it from the old root repairs the
remaining eight errors.
</listItem>
<bodyText confidence="0.981939181818182">
Intuitively, it seems that the impact of the NP
error is limited. By contrast, the root selection
error has a substantial impact on the second half
of the sentence, causing a misplaced subject, mis-
attached punctuation, and incorrect coordination.
These cascaded errors appear to be caused by the
incorrect root.
What we do not know is whether these intu-
itions actually hold. Many dependency parsers,
including MSTparser and ZPar, construct trees by
repeatedly combining fragments together until a
spanning analysis is found, using a small window
of information to make each arc decision. An er-
ror in one part of the tree may have no influence
on a different part of the tree. Alternatively, er-
rors may exert long-range influence — particularly
if there are higher-order features or algorithmic
constraints such as projectivity over the tree. As
parsing algorithms are complex, we wish to repair
various error types in isolation without otherwise
making assumptions regarding the subsequent ac-
tions of the parser.
</bodyText>
<sectionHeader confidence="0.971086" genericHeader="method">
3 Applying Constraints
</sectionHeader>
<bodyText confidence="0.999923857142857">
We investigate how each parser behaves when cer-
tain errors in the tree are corrected. We force each
parser to select the correct head and label for spe-
cific words, but otherwise allow it to construct its
best parse. Given a set of constraints, each of
which lists a word with the head and label to which
it must be attached, we investigate two measures:
</bodyText>
<listItem confidence="0.96672025">
1. errors directly corrected by the constraints,
called the constrained accuracy impact;
2. the indirect impact of the constraints, includ-
ing errors indirectly corrected, and correct
</listItem>
<page confidence="0.995938">
1149
</page>
<bodyText confidence="0.999127421686747">
arcs indirectly destroyed, together called the
cascaded accuracy impact.
The constrained accuracy impact tells us how
often the parser makes errors in the set of words
covered by the constraints. The cascaded accu-
racy impact is less predictable, as it describes what
effect the errors made over the constrained set of
arcs have over the rest of the sentence. It is the
influence of the set of constraints over the other
attachments, which may be mediated through pro-
jectivity requirements, or changes in the context
used for other parsing decisions.
The core of our procedure is adapting each
parser to accept a set of constraints. Following
Kummerfeld et al. (2012), we define meaningful
error classes grouped with the operations that re-
pair them. In dependency parsing, error classes
are groups of Stanford dependency labels, rather
than groups of node repair operations. The Stan-
ford labels provide a rich distinction in NP internal
structure, clauses, and modifiers, and map well to
the error categories of Kummerfeld et al. (2012),
allowing us to avoid excessive heuristics in the
mapping process. Our technique can be applied to
other dependency schemes such as LTH (Johans-
son and Nugues, 2007) by defining new mappings
from labels to error types.
The difficulty of the mapping task depends on
the intricacies of each formalism. The major
challenge with LTH dependencies is the enormous
skew towards the nominal modifier NMOD label.
This label occurs 11,335 times in WSJ 22, more
than twice as frequently as the next most fre-
quent punctuation P. By contrast, the most com-
mon Stanford label is punctuation, at 4,731 occur-
rences. The NMOD label is split into many smaller,
but more informative nominal labels in the Stan-
ford scheme, making it better suited for our goal
of error analysis.
The label grouping was performed with refer-
ence to the Stanford dependencies manual v2.04
(de Marneffe and Manning, 2008, updated 2012).
For each error class, we generate a set of con-
straints over WSJ 22 for all words with a gold-
standard label in the set associated with the class.
Our types are defined as follows:
NP attachment: any label specifically attaching
an NP, includes appos, dobj, iobj, nsubj,
nsubjpass, pobj, and xsubj.
NP internal: any label marking nominal struc-
ture (not including adjectival modifiers), in-
cludes abbrev, det, nn, number, poss,
possessive, and predet.
PP attachment: any label attaching a preposi-
tional phrase, includes prep. Also includes
pcomp if the POS of the word is TO or IN.
Clause attachment: any label attaching a
clause, includes advcl, ccomp, csubj,
csubjpass, purpcl, rcmod, and xcomp.
Also includes pcomp if the POS of the word is
not TO or IN.
Modifier attachment: any label attaching
an adverbial or adjectival modifier, includes
advmod, amod, infmod, npadvmod, num,
partmod, quantmod, and tmod.
Coordination attachment: conj, cc, and
preconj.
Root attachment: the root label.
Punctuation attachment: the punct label.
Other attachment: all other Stanford labels,
specifically acomp, attr, aux, auxpass,
complm, cop, dep, expl, mark, mwe, neg,
parataxis, prt, ref, and rel.
For example, Root constraints specify sentence
roots, while PP constraints specify heads of prepo-
sitional phrases.
One deficiency of our implementation is that we
apply constraints to all arcs of a particular error
type in each sentence, and do not isolate multiple
instances of the same error class in a sentence. We
do this since applying single constraints to a sen-
tence at a time would require substantial modifica-
tions to the standard evaluation regime.
</bodyText>
<subsectionHeader confidence="0.99584">
3.1 MSTparser implementation
</subsectionHeader>
<bodyText confidence="0.999978866666667">
MSTparser is a graph-based, second-order parser
that uses Eisner (1996)’s algorithm for projective
decoding (McDonald and Pereira, 2006).1 Eis-
ner’s algorithm constructs and caches subtrees
which span progressively larger sections of the
sentence. These spans are marked either as com-
plete, consisting of a head, a dependent, and all
of the descendants of that head to one side, or in-
complete, consisting of a head, a dependent, and
an unfilled region where additional tokens may be
attached. Dependencies are formed between the
head and dependent in each complete span, while
label assignment occurs as a separate process.
We enforce constraints by allowing complete
spans to be formed only from constrained tokens
</bodyText>
<footnote confidence="0.777172">
1As the variant of Stanford dependencies we use are pro-
jective, we did not use non-projective decoding.
</footnote>
<page confidence="0.982294">
1150
</page>
<bodyText confidence="0.999910857142857">
to their correct heads with the correct labels. Any
complete span between an incorrect head and the
constrained token is forbidden. The algorithm is
forced to choose the constrained spans as it builds
the parse; these constraints have no impact on the
parser’s coverage as all possible head selections
are considered.
</bodyText>
<subsectionHeader confidence="0.998309">
3.2 ZPar implementation
</subsectionHeader>
<bodyText confidence="0.999989921052632">
ZPar is an arc-eager transition-based parser
(Zhang and Clark, 2011) that uses an incremen-
tal process with a stack storing partial parse states
(Nivre et al., 2004). Each state represents tokens
that may accept further arcs. The tokens of a sen-
tence are initially stored in a buffer, and at each
point during parsing, the parser decides whether
or not to create an arc between the front token of
the buffer and the top token on the stack.
We apply constraints in a similar way to Nivre
et al. (2014). Arc creation actions are factored on
the dependency label to be assigned to the arc.
ZPar scores each possible action using a percep-
tron model over features from the front of the
buffer and the top of the stack (as well as some ad-
ditional context features which refer to previously
created states). The highest scoring actions and
their resulting states are kept in a beam; during
parsing, ZPar finds the optimal action for all items
in the beam, and retains the highest scoring new
states at each step.
We disallow any arc creation action that would
create an arc that conflicts with any constraints.
Due to the use of beam search, it is possible for all
of the partial states containing the constrained arcs
to be evicted from the beam if they score lower
under the model than other states. When this hap-
pens, the parser will fail to find an analysis for the
sentence, as no head will exist in the beam for the
constrained token. We have deliberately chosen to
not address this issue as any solution (e.g. increas-
ing the beam size from its default of 64) would
change the decisions of the parser and model.
We verified that our modifications were work-
ing correctly for both parsers by passing in zero
constraints (checking that the output matched the
baseline performance), and every possible con-
straint (checking that the output scored 100%).
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999911830188679">
Kummerfeld et al. (2012) perform a comprehen-
sive classification of constituency bracket errors
and their cascading impact, and their work is
philosophically similar to ours. They associate
groups of bracket errors in the parse with abstract
error classes, and identify the tree operations that
repair these error types, such as the insertion, dele-
tion, or substitution of nodes in the parse tree.
The error classes in a particular parser’s output
are identified through a heuristic procedure that re-
peatedly applies the operation repairing the largest
number of bracket errors. This approach differs
from our methodology as it is a static post-process
that assumes the parser would respond perfectly to
each repair, when it is possible that the parser may
not perform the repair in full, or even be incapable
of constructing the repaired tree.
McDonald and Nivre (2011) perform an in-
depth comparison of the graph-based MSTparser
and transition-based MaltParser. However, Malt-
Parser uses support vector machines to determinis-
tically predict the next transition, rather than stor-
ing the most probable options in a beam like ZPar.
Additionally, they do not focus on the cascad-
ing impact of errors, and instead concentrate on
higher-level error classification (e.g. by POS tag,
labels and dependency lengths) in lieu of examin-
ing how the parsers respond to forced corrections.
Nivre et al. (2014) describe several uses for arc-
level constraints in transition-based parsing. How-
ever, these applications focus on improving pars-
ing accuracy when constraints can be readily iden-
tified, e.g. imperatives at the beginning of a sen-
tence are likely to be the root. We focus our con-
straints on evaluation, attempting to identify im-
portant sources of error in dependency parsers.
Our constraint-based approach shares similar-
ities to oracle training and decoding methods,
where an external source of truth is used to ver-
ify parser decisions. An oracle source of parser
actions is a necessary component for training
transition-based parsers (Nivre, 2009). Oracle de-
coding, where a system is forced to produce cor-
rect output if possible, can be used to assess its up-
per performance bounds (Ng and Curran, 2012).
Constraining the parser’s internal search space
is akin to an optimal pruning operation. Char-
niak and Johnson (2005) use a coarse-to-fine, it-
erative pruning approach for efficiently generat-
ing high-quality n-best parses for a discriminative
reranker. Rush and Petrov (2012) use a similar
coarse-to-fine algorithm with vine grammars (Eis-
ner and Smith, 2005) to accelerate graph-based de-
</bodyText>
<page confidence="0.976091">
1151
</page>
<note confidence="0.608585">
The LME stocks decline was about as expected , but the Comex gain was n’t .
</note>
<figureCaption confidence="0.984671333333333">
Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to
its correct value. The incorrect noun phrase error is not affected by the constraint (dashed, red), six
attachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple).
</figureCaption>
<figure confidence="0.919404357142857">
det
amod
nn nsubj prep
root
punct
pobj
advmod
cc
conj
punct
det
nn
nsubj
neg
</figure>
<table confidence="0.993118">
Constraints ✓ ✗ Remaining Errors
None 8 9 see Figure 1
nn 9 8 All except decline --+ LME
root 14 3 decline --+ LME
about --+ expected
was --+ about
punct 14 3 decline --+ LME
about --+ expected
was --+ about
ccomp 16 1 decline --+ LME
</table>
<tableCaption confidence="0.99025">
Table 2: Correct and incorrect arcs, and the re-
</tableCaption>
<bodyText confidence="0.978727916666667">
maining errors after applying various sets of con-
straints to the sentence in Figure 1.
pendency parsing, achieving parsing speeds close
to linear-time transition parsers despite encoding
more complex features. Supertagging (Clark and
Curran, 2007) and chart pruning (Zhang et al.,
2010) have been used to constrain the search space
of a CCG parser, and to remove unlikely or for-
bidden spans from repeated consideration. In our
work, we use pruning not for parsing speed, but
evaluation, and so we prune items based on gold-
standard constraints rather than heuristics.
</bodyText>
<sectionHeader confidence="0.999217" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.994930581395349">
We use the training (sections 2-21) and develop-
ment (section 22) data from the OntoNotes 4.0
release of the Penn Treebank WSJ data (Mar-
cus et al., 1993), as supplied by the SANCL
2012 Shared Task on Parsing the Web (Petrov
and McDonald, 2012). OntoNotes annotates en-
riched NP structure compared to the Penn Tree-
bank (Weischedel et al., 2011), meaning that deter-
mining NP attachments is less trivial. We changed
all marker tokens in the corpus (e.g. -LRB- and
-LCB-) to their equivalent unescaped punctuation
marks to ensure correct evaluation. The corpus has
been converted to basic Stanford dependencies us-
ing the Stanford Parser v2.0,2 and part-of-speech
2nlp.stanford.edu/software/lex-parser.shtml
tagged using MXPOST (Ratnaparkhi, 1996). A
model trained on WSJ sections 2-21 was used to
tag the development set, and 10-fold jackknife
training was used to tag the training data.
We implement a custom evaluation script to
facilitate a straightforward comparative analysis
between the unconstrained and constrained out-
put. The script is based on and produces identi-
cal scores to eval06.pl, the official evaluation
for the CoNLL-X Shared Task on Multilingual
Dependency Parsing (Buchholz and Marsi, 2006).
We ignore punctuation as defined by eval06.pl
in our evaluation; experiments with constraints
over punctuation tokens constrain those tokens in
the parse, but ignore them during evaluation.
We run the modified parsers over WSJ 22 with
and without each set of constraints. We ex-
amine the overall unlabeled and labeled attach-
ment scores (UAS and LAS), as well as identify-
ing the contribution to the overall UAS improve-
ment from directly (constrained) and indirectly
corrected arcs.
MSTparser uses coarse-grained tags and fine-
grained POS tags in its features, both of which
were provided by the CoNLL-X Shared Task. We
approximate the coarse-grained POS tags by taking
the first character of the MXPOST-assigned POS
tag, a technique also used by Bansal et al. (2014)3.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.998127333333333">
Figure 2 and Table 2 show the impact of applying
constraints on tokens with various labels to MST-
parser for the sentence in Figure 1. Enforcing the
gold nn arc between decline and LME repairs that
noun phrase error, but does not affect any of the
other errors. Conversely, enforcing the gold root
arc does not affect the noun phrase error, but re-
pairs nearly every other error in the parse. Un-
fortunately, the constrained root arc introduces
</bodyText>
<footnote confidence="0.92452">
3Mohit Bansal, p.c.
</footnote>
<page confidence="0.869818">
1152
</page>
<table confidence="0.999565416666667">
Error class cover eff eff % disp UAS LAS ΔUAS Δc Δu
Baseline 100.0 - - - 91.3 87.5 - - -
NP attachment 95.6 312 4.9 5.2 94.1 90.7 2.3 1.2 1.1
NP internal 98.2 206 3.2 2.8 92.6 89.2 1.1 0.8 0.3
Modifier attachment 96.8 321 7.9 3.8 93.4 90.3 1.7 1.2 0.5
PP attachment 98.3 378 13.1 4.3 93.2 89.5 1.7 1.4 0.3
Coordination attachment 97.7 238 16.0 5.8 92.9 89.5 1.3 0.9 0.4
Clause attachment 96.7 228 17.9 6.9 93.0 89.6 1.4 0.9 0.5
Root attachment 99.1 77 5.8 9.3 92.2 88.3 0.8 0.3 0.5
Punctuation attachment 93.2 469 14.2 7.4 93.9 89.9 1.8 0.1 1.7
Other attachment 94.3 210 7.0 6.1 93.5 90.8 1.4 0.8 0.6
All attachments 98.5 2912 9.3 5.8 100.0 100.0 8.6 8.6 0.0
</table>
<tableCaption confidence="0.942463">
Table 3: The coverage, effective constraints and percentage, error displacement, UAS, LAS, ΔUAS over
the corrected arcs, and the constrained and cascaded Δ for MSTparser over WSJ 22 (covered by ZPar).
</tableCaption>
<table confidence="0.99972275">
Error class cover eff eff % disp UAS LAS ΔUAS Δc Δu
Baseline 100.0 - - - 91.7 89.2 - - -
NP attachment 95.6 277 4.3 4.8 94.9 92.7 2.4 1.0 1.4
NP internal 98.2 197 3.0 3.0 93.2 91.1 1.2 0.7 0.5
Modifier attachment 96.8 303 7.5 3.9 94.0 92.3 1.8 1.1 0.7
PP attachment 98.3 357 12.4 3.9 93.8 91.4 1.7 1.3 0.4
Coordination attachment 97.7 240 16.2 5.8 93.5 91.1 1.3 0.9 0.4
Clause attachment 96.7 166 13.0 5.6 93.4 91.2 1.2 0.6 0.6
Root attachment 99.1 57 4.3 9.9 92.4 89.9 0.5 0.2 0.3
Punctuation attachment 93.2 430 13.0 7.3 94.5 92.1 1.6 0.2 1.5
Other attachment 94.3 187 6.3 5.5 94.2 92.7 1.3 0.7 0.6
All attachments 98.5 2760 8.8 5.8 100.0 100.0 8.0 8.0 0.0
</table>
<tableCaption confidence="0.986317">
Table 4: The coverage, effective constraints and percentage, error displacement, UAS, LAS, ΔUAS over
</tableCaption>
<bodyText confidence="0.995263538461539">
the baseline, and the constrained and cascaded Δ for ZPar over WSJ 22.
two new errors, with the parser incorrectly attach-
ing the clausal complement headed by expected
and the modifier headed by about. In fact, cor-
recting the ccomp arc in isolation rather than the
root arc leads to MSTparser producing the full
correct analysis for the second half of the sentence
(though again, it does not repair the separate noun
phrase error). This example highlights why we
have chosen to implement our evaluation as a set
of constraints in the parser, rather than Kummer-
feld et al. (2012)’s post-processing approach, as
we cannot know that the parser will react as we
expect it to when repairing errors.
Tables 3 and 4 summarise our results on MST-
parser and ZPar, calculated over the sentences
covered by ZPar in WSJ 22. Results over the full
WSJ 22 for MSTparser were consistent with these
figures. We focus on discussing UAS results in this
paper, since LAS results are consistent.
The UAS of constrained arcs in each experiment
is the expected 100%. Effective constraints repair
an error in the baseline, and the effective constraint
percentage is this figure expressed as a percentage,
i.e. the error rate. Error displacement is the av-
erage number of words that effective constraints
moved an attachment point. The overall ΔUAS
improvement is divided into Δc, the constrained
impact, and Δu, the cascaded impact.
It is important to note that a parser may make
a substantial number of mistakes on a particular
error class (large effective constraint percentage),
but correcting those mistakes may have very little
cascading impact (small Δc), limiting the overall
ΔUAS improvement. Conversely, there may be a
class with a small effective constraint percentage,
but a large ΔUAS due to a large cascading impact
from the corrections, or simply because the class
contains more constraints.
</bodyText>
<page confidence="0.972417">
1153
</page>
<subsectionHeader confidence="0.98583">
6.1 Overall Parser Comparison
</subsectionHeader>
<bodyText confidence="0.999978826086957">
When applying all constraints, ZPar has a 8.8%
effective constraint percentage compared to 9.3%
for MSTparser. This is directly related to the UAS
difference between the parsers. Aside from coor-
dination, where the parsers made a nearly identi-
cal number of errors, ZPar is more accurate across
the board. It makes substantially fewer mistakes
on clause attachments, punctuation dependencies,
and NP attachments, whilst maintaining a small
advantage across all of the other categories.
The relative rank of the effective constraint per-
centage per error category is similar across the
parsers, with PP attachment, punctuation, modi-
fiers, and coordination recording the largest num-
ber of effective constraints, and thus the most er-
rors. This illustrates that the behaviour of both
parsers is very consistent, despite one considering
every possible attachment point, and the other us-
ing a linear transition-based beam search. ZPar
is able to make fewer mistakes across each error
category, suggesting that the beam search pruning
is maintaining more desired states than the graph-
based parser is able to rank during its search.
ZPar’s coverage is 98.5% when applying all
constraints. However, as the number of con-
straints is reduced, coverage also drops. This
seems counter-intuitive, but applying more con-
straints eliminates more undesired states, leaving
more space in the beam for satisfying states. Re-
ducing the number of constraints permits more
states which do not yet violate a constraint, but
only yield undesired states later.
Punctuation constraints have the largest impact
on coverage, reducing it to 93.2%. NP attach-
ments, clauses, and modifier attachments also in-
cur substantial coverage reductions. This suggests
that ZPar’s performance will degrade substantially
over the sentences which it cannot cover, as they
must contain constructions which are dispreferred
by the model and fall out of the beam. Constraints
with the smallest effect on coverage include root
attachments, which only occur once per sentence
and are rarely incorrect, and NP internal and PP at-
tachments. For the latter two, the small displace-
ments suggest that alternate attachment points of-
ten lie within the same projective span.
</bodyText>
<subsectionHeader confidence="0.998569">
6.2 Noun phrases
</subsectionHeader>
<bodyText confidence="0.999971641509435">
Applying NP attachment constraints causes a 4.4%
drop in coverage for ZPar, and the effective con-
straint percentage is below 5% for both parsers.
However, these constraints still result in the largest
DUAS for both parsers, at 2.6% for MSTparser
and 2.2% for ZPar. This reflects the prevalence
of NP attachments in the corpus.
DUAS is split evenly between correcting con-
strained (1.4%) and cascaded arcs (1.2%) for
MSTparser, while it skews towards cascaded arcs
for ZPar (1.0% and 1.4%). Most error classes
skew in the other direction, while repairing one NP
attachment error typically repairs another non-NP
attachment error.
For NP internal attachments, both parsers have
a similar error rate, with 206 effective constraints
for MSTparser and 197 for ZPar. Although this is
the second largest class, applying these constraints
gives the second smallest Du for both parsers.
This implies that determining NP internal struc-
ture is a strength, even with the more complex
OntoNotes 4 NP structure. Dc is also small for
both parsers, reinforcing the limited displacement
and cascading impact of NP internal errors.
Despite fewer effective constraints (i.e. less
errors to fix), ZPar exhibits more cascading re-
pair than MSTparser using both NP and NP in-
ternal constraints. This will be a common theme
through this evaluation: the transition-based ZPar
is better at propagating effective constraints into
cascaded impact than the graph-based MSTparser,
even though ZPar almost always begins with fewer
effective constraints due to its better baseline per-
formance. One possibility to explain this is that
the beam is actually pruning away other erroneous
states, while the graph-based MSTparser must still
consider all of them.
Table 5 summarises the error classes of cor-
rected cascaded arcs for the two NP constraint
types, which are closely related. NP attachment
constraints directly identify the head of the NP as
well as its correct attachment, providing strong
cues for determining the internal structure. NP in-
ternal constraints implicitly identify the head of an
NP. We can see that for both types of constraints,
many of the cascaded corrections come from the
other NP error class.
Table 5 also shows that, compared to MST-
parser, ZPar repairs nearly twice as many NP inter-
nal and coordination errors when using NP attach-
ment constraints, and vice versa when using NP
internal constraints. This suggests that ZPar has
more difficulty identifying the correct heads for
</bodyText>
<page confidence="0.979944">
1154
</page>
<table confidence="0.999873916666667">
Error class NP attachment NP internal
MSTparser ZPar MSTparser ZPar
NP attachment - - 45 69
NP internal 43 80 - -
Modifier attachment 65 68 24 30
PP attachment 26 36 2 10
Coordination attachment 37 67 20 41
Clause attachment 59 65 1 1
Root attachment 24 21 2 1
Punctuation attachment 79 80 26 41
Other attachment 68 76 7 11
Total 401 493 127 204
</table>
<tableCaption confidence="0.817072">
Table 5: The number of unconstrained errors repaired per error class when enforcing NP attachment and
NP internal constraints for MSTparser and ZPar over WSJ 22.
</tableCaption>
<bodyText confidence="0.9777235">
nominal coordination, and often chooses a word
which should be a nominal modifier instead.
</bodyText>
<subsectionHeader confidence="0.999688">
6.3 Coordination, Modifiers and PPs
</subsectionHeader>
<bodyText confidence="0.999961476190476">
These categories are large error classes for both
parsers, with constraints leading to UAS improve-
ments of 1.3 to 1.7%.
PPs and coordination have high effective con-
straint percentages relative to the other error
classes for both parsers. However, they are also
amongst the most isolated errors, with only 0.3%
and 0.4% Δu for MSTparser and ZPar respec-
tively. These errors also have minimal impact on
ZPar’s coverage. Both classes seem to have rela-
tively contained attachment options within a lim-
ited projective span. The small error displace-
ments reinforce this idea.
Modifiers are relatively isolated errors for MST-
parser (0.5% Δu), but less so for ZPar (0.7% Δu).
There are substantially more modifier constraints
than PP or coordination, despite all yielding a sim-
ilar UAS increase. This suggests that modifiers are
actually relatively well analysed by both parsers,
but there are so many of them that they form a
large source of error.
</bodyText>
<subsectionHeader confidence="0.999195">
6.4 Clause attachment
</subsectionHeader>
<bodyText confidence="0.999955933333334">
MSTparser performs substantially worse than
ZPar on clause attachments, with an effective con-
straint percentage of 17.9% compared to 13.0%,
and Δc of 0.9% compared with 0.6%. MST-
parser’s error rate is the worst of any error class
on clause attachments, while it is second to coor-
dination attachments for ZPar. Attaching clauses
is very challenging for dependency parsers, partic-
ularly considering the small size of the class.
ZPar again achieves a slightly larger cascaded
impact than MSTparser (0.6% to 0.5%), despite
having far fewer effective constraints. This im-
plies that the additional clause errors being made
by MSTparser are largely self-contained, as they
have not triggered a corresponding increase in Δu.
</bodyText>
<subsectionHeader confidence="0.998801">
6.5 Root attachment
</subsectionHeader>
<bodyText confidence="0.998924777777778">
Both parsers make few root attachment errors,
though MSTparser is less accurate than ZPar.
However, root constraints provide the largest UAS
improvement per number of constraints for both
parsers. Root errors are also the most displaced of
any error class, at 9.3 words for MSTparser and
9.9 for ZPar. When the root is incorrect, it is of-
ten very far from its correct location, and causes
substantial cascading errors.
</bodyText>
<subsectionHeader confidence="0.997446">
6.6 Punctuation
</subsectionHeader>
<bodyText confidence="0.9999756">
Despite ignoring punctuation dependencies in
evaluation, applying punctuation constraints led to
substantial UAS improvements. On MSTparser,
Δu is 0.1% (due to some punctuation not being
excluded from evaluation), but Δc is 1.7%. On
ZPar, the equivalent metrics are 0.2% and 1.5%.
Enforcing correct punctuation has a disproportion-
ate impact on the remainder of the parse.
For both parsers, punctuation errors occur more
frequently than any other error type, with 469
and 430 effective constraints respectively (though
the majority of these corrected errors are on non-
evaluated arcs). ZPar’s coverage is worst of all
when enforcing punctuation constraints, suggest-
ing that the remaining uncovered sentences will
</bodyText>
<page confidence="0.968665">
1155
</page>
<table confidence="0.9998264">
Error class MSTparser ZPar
NP attachment 75 51
NP internal 25 27
Modifier attachment 33 43
PP attachment 45 55
Coordination attachment 87 106
Clause attachment 66 48
Root attachment 59 27
Other attachment 65 69
Total 455 426
</table>
<tableCaption confidence="0.820098666666667">
Table 6: The number of unconstrained errors re-
paired per error class when enforcing punctuation
constraints for MSTparser and ZPar.
</tableCaption>
<bodyText confidence="0.997960130434783">
contain even more punctuation errors.
Incorrect punctuation heads are displaced from
their correct locations by 7.4 words for MSTparser
and 7.3 words for ZPar on average, second only to
root attachments. Given that we are using projec-
tive parsers and a projective grammar, the large av-
erage displacement caused by errors indicates that
punctuation affects and is in turn affected by the
requirement for non-crossing arcs.
Table 6 summarises the error classes of the re-
paired cascaded arcs when punctuation constraints
are applied. MSTparser has a more even distri-
bution of repairs, while ZPar’s repairs are con-
centrated in coordination attachment. This shows
that MSTparser is relatively better at coordination
as a proportion of its overall performance com-
pared to ZPar. It also indicates that the majority of
punctuation errors in both parsers (and especially
ZPar) stem from incorrectly identified coordina-
tion markers such as commas.
Punctuation is commonly ignored in depen-
dency parser evaluation (Yamada and Matsumoto,
2003; Buchholz and Marsi, 2006), and they are
inconsistently treated across different grammars.
Our results show that enforcing the correct punc-
tuation attachments in a sentence has a substan-
tial cascading impact, suggesting that punctua-
tion errors are highly correlated with errors else-
where in the analysis. Given the broad simi-
larities between Stanford dependencies and other
dependency schemes commonly used in parsing
(Søgaard, 2013), we anticipate that the problems
with roots and punctuation will carry across dif-
ferent treebanks and schemes.
Punctuation is often placed at phrasal bound-
aries and serves to split sentences into smaller sec-
tions within a projective parser. Graph-based and
transition-based parsers, both of which use a lim-
ited local context to make parsing decisions, are
equally prone to the cascading impact of erroneous
punctuation. Removing the confounding presence
of punctuation from parsing and treating attach-
ment as a global post-process may help to allevi-
ate these issues. Alternatively, more punctuation-
specific features to account for its myriad roles in
syntax could serve to improve performance.
</bodyText>
<sectionHeader confidence="0.998866" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999994533333333">
We have developed a procedure to classify the im-
portance of errors in dependency parsers without
any assumptions on how the parser will respond
to attachment repairs. Our approach constrains the
parser to allow only correct arcs for certain tokens,
whilst allowing it to otherwise form the parse that
it thinks is best. Compared to Kummerfeld et al.
(2012), we can observe exactly how the parser re-
sponds to the parse repairs, though at the cost of
requiring modifications to the parser itself.
Our results show that noun phrases remain chal-
lenging for dependency parsers, both in choosing
the correct head, and in determining the internal
structure. Punctuation, despite being commonly
ignored in parsers and evaluation, causes substan-
tial cascading errors when misattached.
We are extending our work to other popular de-
pendency parsers and non-projective parsing algo-
rithms, and hope to develop features to improve
and mitigate the cascading impact of punctuation
attachment errors in parsing. Given that con-
stituency parsers perform strongly when converted
to dependencies (Cer et al., 2010; Petrov and Mc-
Donald, 2012), it would also be interesting to in-
vestigate how they perform on our metrics.
We implement a robust procedure to identify
the cascading impact of dependency parser errors.
Our results provide insights into which errors are
most damaging in parsing, and will drive further
improvements in parsing accuracy.
</bodyText>
<sectionHeader confidence="0.997833" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.914465">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representa-
tions for dependency parsing. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (ACL-14), pages
809–815. Baltimore, Maryland, USA.
</reference>
<page confidence="0.946737">
1156
</page>
<reference confidence="0.996331551020408">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency
Parsing. In Proceedings of the Tenth Conference
on Computational Natural Language Learning
(CoNLL-06), pages 149–164.
Daniel Cer, Marie-Catherine de Marneffe, Dan
Jurafsky, and Chris Manning. 2010. Parsing
to Stanford Dependencies: Trade-offs between
Speed and Accuracy. In Proceedings of the Sev-
enth International Conference on Language Re-
sources and Evaluation (LREC-10).
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Dis-
criminative Reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-05), pages
173–180.
Stephen Clark and James R. Curran. 2007.
Formalism-Independent Parser Evaluation with
CCG and DepBank. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL-07), pages 248–255.
Prague, Czech Republic.
Marie-Catherine de Marneffe and Christopher D.
Manning. 2008. Stanford dependencies manual.
Technical report, Stanford University.
Jason Eisner. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
Proceedings of the 16th International Confer-
ence on Computational Linguistics (COLING-
96), pages 340–345.
Jason Eisner and A. Noah Smith. 2005. Parsing
with Soft and Hard Constraints on Dependency
Length. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technology (IWPT-
05), pages 30–41.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended Constituent-to-dependency Conversion
for English. In Proceedings of the 16th
Nordic Conference of Computational Linguis-
tics (NODALIDA-07), pages 105–112. Tartu,
Estonia.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser Show-
down at the Wall Street Corral: An Empirical
Investigation of Error Types in Parser Output.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning (EMNLP-CoNLL-12), pages 1048–
1059.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313–330.
Ryan McDonald and Joakim Nivre. 2011. Analyz-
ing and Integrating Dependency Parsers. Com-
putational Linguistics, 37(1):197–230.
Ryan McDonald and Fernando Pereira. 2006.
Online Learning of Approximate Dependency
Parsing Algorithms. In Proceedings of the
11th Conference of the European Chapter of
the Association for Computational Linguistics
(EACL-06), pages 81–88.
Dominick Ng and James R. Curran. 2012. Depen-
dency Hashing for n-best CCG Parsing. In Pro-
ceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
12), pages 497–505.
Joakim Nivre. 2009. Non-Projective Dependency
Parsing in Expected Linear Time. In Proceed-
ings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language
Processing of the AFNLP (ACL-09), pages 351–
359.
Joakim Nivre, Yoav Goldberg, and Ryan McDon-
ald. 2014. Constrained arc-eager dependency
parsing. Computational Linguistics, 40(2):249–
257.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In HLT-
NAACL 2004 Workshop: Eighth Conference
on Computational Natural Language Learning
(CoNLL-04), pages 49–56.
Slav Petrov and Ryan McDonald. 2012. Overview
of the 2012 Shared Task on Parsing the Web.
In Notes of the First Workshop on the Syntactic
Analysis of Non-Canonical Language (SANCL-
12).
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Proceed-
ings of the 1996 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-
96), pages 133–142.
Alexander Rush and Slav Petrov. 2012. Vine Prun-
ing for Efficient Multi-Pass Dependency Pars-
</reference>
<page confidence="0.875576">
1157
</page>
<reference confidence="0.999396371428572">
ing. In Proceedings of the 2012 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Lan-
guage Technologies (NAACL-12), pages 498–
507.
Anders Søgaard. 2013. An Empirical Study of
Differences between Conversion Schemes and
Annotation Guidelines. In Proceedings of the
2nd International Conference on Dependency
Linguistics, pages 298–307.
Ralph Weischedel, Eduard Hovy, Mitchell Mar-
cus, Martha Palmer, Robert Belvin, Sameer
Pradhan, Lance Ramshaw, and Nianwen Xue.
2011. OntoNotes: A Large Training Corpus for
Enhanced Processing. In Joseph Olive, Caitlin
Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Ma-
chine Translation: DARPA Global Autonomous
Language Exploitation, pages 54–63. Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical Dependency Analysis with Support Vec-
tor Machines. In Proceedings of the 8th In-
ternational Workshop of Parsing Technologies
(IWPT-03), pages 196–206.
Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt
Van Wyk, James R. Curran, and Laura Rimell.
2010. Chart Pruning for Fast Lexicalised-
Grammar Parsing. In Proceedings of the
23rd International Conference on Computa-
tional Linguistics (COLING-10), pages 1471–
1479.
Yue Zhang and Stephen Clark. 2011. Syntactic
Processing Using the Generalized Perceptron
and Beam Search. Computational Linguistics,
37(1):105–151.
</reference>
<page confidence="0.995266">
1158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510428">
<title confidence="0.999791">Identifying Cascading Errors using Constraints in Dependency Parsing</title>
<author confidence="0.999941">Ng R Curran</author>
<affiliation confidence="0.9852">a-lab, School of Information University of</affiliation>
<address confidence="0.513887">NSW 2006,</address>
<abstract confidence="0.999786333333333">Dependency parsers are usually evaluated on attachment accuracy. Whilst easily interpreted, the metric does not illustrate the cascading impact of errors, where the parser chooses an incorrect arc, and is subsequently forced to choose further incorrect arcs elsewhere in the parse. We apply arc-level constraints to MSTparser and ZPar, enforcing the correct analysis of specific error classes, whilst otherwise continuing with decoding. We investigate the direct and indirect impact of applying constraints to the parser. Erpunctuation attachments cause the most cascading errors, while incoordination attachments are frequent but less influential. Punctuation is especially challenging, as it has long been ignored in parsing, and serves a variety of disparate syntactic roles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14),</booktitle>
<pages>809--815</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="19634" citStr="Bansal et al. (2014)" startWordPosition="3177" endWordPosition="3180"> the parse, but ignore them during evaluation. We run the modified parsers over WSJ 22 with and without each set of constraints. We examine the overall unlabeled and labeled attachment scores (UAS and LAS), as well as identifying the contribution to the overall UAS improvement from directly (constrained) and indirectly corrected arcs. MSTparser uses coarse-grained tags and finegrained POS tags in its features, both of which were provided by the CoNLL-X Shared Task. We approximate the coarse-grained POS tags by taking the first character of the MXPOST-assigned POS tag, a technique also used by Bansal et al. (2014)3. 6 Results Figure 2 and Table 2 show the impact of applying constraints on tokens with various labels to MSTparser for the sentence in Figure 1. Enforcing the gold nn arc between decline and LME repairs that noun phrase error, but does not affect any of the other errors. Conversely, enforcing the gold root arc does not affect the noun phrase error, but repairs nearly every other error in the parse. Unfortunately, the constrained root arc introduces 3Mohit Bansal, p.c. 1152 Error class cover eff eff % disp UAS LAS ΔUAS Δc Δu Baseline 100.0 - - - 91.3 87.5 - - - NP attachment 95.6 312 4.9 5.2 </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14), pages 809–815. Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-06),</booktitle>
<pages>149--164</pages>
<contexts>
<context position="18869" citStr="Buchholz and Marsi, 2006" startWordPosition="3054" endWordPosition="3057">ic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2nlp.stanford.edu/software/lex-parser.shtml tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife training was used to tag the training data. We implement a custom evaluation script to facilitate a straightforward comparative analysis between the unconstrained and constrained output. The script is based on and produces identical scores to eval06.pl, the official evaluation for the CoNLL-X Shared Task on Multilingual Dependency Parsing (Buchholz and Marsi, 2006). We ignore punctuation as defined by eval06.pl in our evaluation; experiments with constraints over punctuation tokens constrain those tokens in the parse, but ignore them during evaluation. We run the modified parsers over WSJ 22 with and without each set of constraints. We examine the overall unlabeled and labeled attachment scores (UAS and LAS), as well as identifying the contribution to the overall UAS improvement from directly (constrained) and indirectly corrected arcs. MSTparser uses coarse-grained tags and finegrained POS tags in its features, both of which were provided by the CoNLL-</context>
<context position="33250" citStr="Buchholz and Marsi, 2006" startWordPosition="5410" endWordPosition="5413">ummarises the error classes of the repaired cascaded arcs when punctuation constraints are applied. MSTparser has a more even distribution of repairs, while ZPar’s repairs are concentrated in coordination attachment. This shows that MSTparser is relatively better at coordination as a proportion of its overall performance compared to ZPar. It also indicates that the majority of punctuation errors in both parsers (and especially ZPar) stem from incorrectly identified coordination markers such as commas. Punctuation is commonly ignored in dependency parser evaluation (Yamada and Matsumoto, 2003; Buchholz and Marsi, 2006), and they are inconsistently treated across different grammars. Our results show that enforcing the correct punctuation attachments in a sentence has a substantial cascading impact, suggesting that punctuation errors are highly correlated with errors elsewhere in the analysis. Given the broad similarities between Stanford dependencies and other dependency schemes commonly used in parsing (Søgaard, 2013), we anticipate that the problems with roots and punctuation will carry across different treebanks and schemes. Punctuation is often placed at phrasal boundaries and serves to split sentences i</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX Shared Task on Multilingual Dependency Parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-06), pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Dan Jurafsky</author>
<author>Chris Manning</author>
</authors>
<title>Parsing to Stanford Dependencies: Trade-offs between Speed and Accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC-10).</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Dan Jurafsky, and Chris Manning. 2010. Parsing to Stanford Dependencies: Trade-offs between Speed and Accuracy. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="16077" citStr="Charniak and Johnson (2005)" startWordPosition="2597" endWordPosition="2601">luation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 The LME stocks decline was about as expected , but the Comex gain was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect noun phrase error is not affected by the constraint (dashed, red), six attachment errors are repaired (solid, blue), and </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalism-Independent Parser Evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07),</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="17331" citStr="Clark and Curran, 2007" startWordPosition="2811" endWordPosition="2814">dotted, purple). det amod nn nsubj prep root punct pobj advmod cc conj punct det nn nsubj neg Constraints ✓ ✗ Remaining Errors None 8 9 see Figure 1 nn 9 8 All except decline --+ LME root 14 3 decline --+ LME about --+ expected was --+ about punct 14 3 decline --+ LME about --+ expected was --+ about ccomp 16 1 decline --+ LME Table 2: Correct and incorrect arcs, and the remaining errors after applying various sets of constraints to the sentence in Figure 1. pendency parsing, achieving parsing speeds close to linear-time transition parsers despite encoding more complex features. Supertagging (Clark and Curran, 2007) and chart pruning (Zhang et al., 2010) have been used to constrain the search space of a CCG parser, and to remove unlikely or forbidden spans from repeated consideration. In our work, we use pruning not for parsing speed, but evaluation, and so we prune items based on goldstandard constraints rather than heuristics. 5 Evaluation We use the training (sections 2-21) and development (section 22) data from the OntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enr</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Formalism-Independent Parser Evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07), pages 248–255. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. Stanford dependencies manual. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING96),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="10810" citStr="Eisner (1996)" startWordPosition="1730" endWordPosition="1731">, expl, mark, mwe, neg, parataxis, prt, ref, and rel. For example, Root constraints specify sentence roots, while PP constraints specify heads of prepositional phrases. One deficiency of our implementation is that we apply constraints to all arcs of a particular error type in each sentence, and do not isolate multiple instances of the same error class in a sentence. We do this since applying single constraints to a sentence at a time would require substantial modifications to the standard evaluation regime. 3.1 MSTparser implementation MSTparser is a graph-based, second-order parser that uses Eisner (1996)’s algorithm for projective decoding (McDonald and Pereira, 2006).1 Eisner’s algorithm constructs and caches subtrees which span progressively larger sections of the sentence. These spans are marked either as complete, consisting of a head, a dependent, and all of the descendants of that head to one side, or incomplete, consisting of a head, a dependent, and an unfilled region where additional tokens may be attached. Dependencies are formed between the head and dependent in each complete span, while label assignment occurs as a separate process. We enforce constraints by allowing complete span</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING96), pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>A Noah Smith</author>
</authors>
<title>Parsing with Soft and Hard Constraints on Dependency Length.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology (IWPT05),</booktitle>
<pages>30--41</pages>
<contexts>
<context position="16317" citStr="Eisner and Smith, 2005" startWordPosition="2632" endWordPosition="2636">n oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 The LME stocks decline was about as expected , but the Comex gain was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect noun phrase error is not affected by the constraint (dashed, red), six attachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple). det amod nn nsubj prep root punct pobj advmod cc conj punct det nn nsubj neg Constraints ✓ ✗ Remaining Errors None 8 9 see Figure 1 nn 9 8 All except decline --+ LME root 14 3 decline --+ LME </context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>Jason Eisner and A. Noah Smith. 2005. Parsing with Soft and Hard Constraints on Dependency Length. In Proceedings of the Ninth International Workshop on Parsing Technology (IWPT05), pages 30–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended Constituent-to-dependency Conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA-07),</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="8337" citStr="Johansson and Nugues, 2007" startWordPosition="1330" endWordPosition="1334">re is adapting each parser to accept a set of constraints. Following Kummerfeld et al. (2012), we define meaningful error classes grouped with the operations that repair them. In dependency parsing, error classes are groups of Stanford dependency labels, rather than groups of node repair operations. The Stanford labels provide a rich distinction in NP internal structure, clauses, and modifiers, and map well to the error categories of Kummerfeld et al. (2012), allowing us to avoid excessive heuristics in the mapping process. Our technique can be applied to other dependency schemes such as LTH (Johansson and Nugues, 2007) by defining new mappings from labels to error types. The difficulty of the mapping task depends on the intricacies of each formalism. The major challenge with LTH dependencies is the enormous skew towards the nominal modifier NMOD label. This label occurs 11,335 times in WSJ 22, more than twice as frequently as the next most frequent punctuation P. By contrast, the most common Stanford label is punctuation, at 4,731 occurrences. The NMOD label is split into many smaller, but more informative nominal labels in the Stanford scheme, making it better suited for our goal of error analysis. The lab</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended Constituent-to-dependency Conversion for English. In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA-07), pages 105–112. Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-12),</booktitle>
<pages>1048--1059</pages>
<contexts>
<context position="1617" citStr="Kummerfeld et al. (2012)" startWordPosition="229" endWordPosition="232"> and serves a variety of disparate syntactic roles. 1 Introduction Dependency parsers are evaluated using wordlevel attachment accuracy. Whilst comparable across systems, this does not provide insight into why the parser makes certain errors, or whether certain misattachments are caused by other errors. For example, incorrectly identifying a modifier head may only introduce a single attachment error, while misplacing the root of a sentence will create substantially more errors elsewhere. In projective dependency parsing, erroneous arcs can also force the parser to select other incorrect arcs. Kummerfeld et al. (2012) propose a static postparsing analysis to categorise groups of bracket errors in constituency parsing into higher level error classes such as clause attachment. However, this cannot account for cascading changes resulting from repairing errors, or limitations which may prevent the parser from applying a repair. It is unclear whether the parser will apply the repair operation in its entirety, or if it will introduce other changes in response to the repairs. We develop an evaluation procedure to evaluate the influence of each error class in dependency parsing without making assumptions about how</context>
<context position="7803" citStr="Kummerfeld et al. (2012)" startWordPosition="1246" endWordPosition="1249"> the cascaded accuracy impact. The constrained accuracy impact tells us how often the parser makes errors in the set of words covered by the constraints. The cascaded accuracy impact is less predictable, as it describes what effect the errors made over the constrained set of arcs have over the rest of the sentence. It is the influence of the set of constraints over the other attachments, which may be mediated through projectivity requirements, or changes in the context used for other parsing decisions. The core of our procedure is adapting each parser to accept a set of constraints. Following Kummerfeld et al. (2012), we define meaningful error classes grouped with the operations that repair them. In dependency parsing, error classes are groups of Stanford dependency labels, rather than groups of node repair operations. The Stanford labels provide a rich distinction in NP internal structure, clauses, and modifiers, and map well to the error categories of Kummerfeld et al. (2012), allowing us to avoid excessive heuristics in the mapping process. Our technique can be applied to other dependency schemes such as LTH (Johansson and Nugues, 2007) by defining new mappings from labels to error types. The difficul</context>
<context position="13791" citStr="Kummerfeld et al. (2012)" startWordPosition="2234" endWordPosition="2237">er the model than other states. When this happens, the parser will fail to find an analysis for the sentence, as no head will exist in the beam for the constrained token. We have deliberately chosen to not address this issue as any solution (e.g. increasing the beam size from its default of 64) would change the decisions of the parser and model. We verified that our modifications were working correctly for both parsers by passing in zero constraints (checking that the output matched the baseline performance), and every possible constraint (checking that the output scored 100%). 4 Related Work Kummerfeld et al. (2012) perform a comprehensive classification of constituency bracket errors and their cascading impact, and their work is philosophically similar to ours. They associate groups of bracket errors in the parse with abstract error classes, and identify the tree operations that repair these error types, such as the insertion, deletion, or substitution of nodes in the parse tree. The error classes in a particular parser’s output are identified through a heuristic procedure that repeatedly applies the operation repairing the largest number of bracket errors. This approach differs from our methodology as </context>
<context position="22311" citStr="Kummerfeld et al. (2012)" startWordPosition="3669" endWordPosition="3673">ntage, error displacement, UAS, LAS, ΔUAS over the baseline, and the constrained and cascaded Δ for ZPar over WSJ 22. two new errors, with the parser incorrectly attaching the clausal complement headed by expected and the modifier headed by about. In fact, correcting the ccomp arc in isolation rather than the root arc leads to MSTparser producing the full correct analysis for the second half of the sentence (though again, it does not repair the separate noun phrase error). This example highlights why we have chosen to implement our evaluation as a set of constraints in the parser, rather than Kummerfeld et al. (2012)’s post-processing approach, as we cannot know that the parser will react as we expect it to when repairing errors. Tables 3 and 4 summarise our results on MSTparser and ZPar, calculated over the sentences covered by ZPar in WSJ 22. Results over the full WSJ 22 for MSTparser were consistent with these figures. We focus on discussing UAS results in this paper, since LAS results are consistent. The UAS of constrained arcs in each experiment is the expected 100%. Effective constraints repair an error in the baseline, and the effective constraint percentage is this figure expressed as a percentage</context>
<context position="34721" citStr="Kummerfeld et al. (2012)" startWordPosition="5637" endWordPosition="5640">nding presence of punctuation from parsing and treating attachment as a global post-process may help to alleviate these issues. Alternatively, more punctuationspecific features to account for its myriad roles in syntax could serve to improve performance. 7 Conclusion We have developed a procedure to classify the importance of errors in dependency parsers without any assumptions on how the parser will respond to attachment repairs. Our approach constrains the parser to allow only correct arcs for certain tokens, whilst allowing it to otherwise form the parse that it thinks is best. Compared to Kummerfeld et al. (2012), we can observe exactly how the parser responds to the parse repairs, though at the cost of requiring modifications to the parser itself. Our results show that noun phrases remain challenging for dependency parsers, both in choosing the correct head, and in determining the internal structure. Punctuation, despite being commonly ignored in parsers and evaluation, causes substantial cascading errors when misattached. We are extending our work to other popular dependency parsers and non-projective parsing algorithms, and hope to develop features to improve and mitigate the cascading impact of pu</context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. 2012. Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-12), pages 1048– 1059.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="17816" citStr="Marcus et al., 1993" startWordPosition="2895" endWordPosition="2899">ng parsing speeds close to linear-time transition parsers despite encoding more complex features. Supertagging (Clark and Curran, 2007) and chart pruning (Zhang et al., 2010) have been used to constrain the search space of a CCG parser, and to remove unlikely or forbidden spans from repeated consideration. In our work, we use pruning not for parsing speed, but evaluation, and so we prune items based on goldstandard constraints rather than heuristics. 5 Evaluation We use the training (sections 2-21) and development (section 22) data from the OntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enriched NP structure compared to the Penn Treebank (Weischedel et al., 2011), meaning that determining NP attachments is less trivial. We changed all marker tokens in the corpus (e.g. -LRB- and -LCB-) to their equivalent unescaped punctuation marks to ensure correct evaluation. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2nlp.stanford.edu/software/lex-parser.shtml tagged using MXPOST (Ratnaparkhi, 1996). A model tr</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing and Integrating Dependency Parsers.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="14636" citStr="McDonald and Nivre (2011)" startWordPosition="2368" endWordPosition="2371">or classes, and identify the tree operations that repair these error types, such as the insertion, deletion, or substitution of nodes in the parse tree. The error classes in a particular parser’s output are identified through a heuristic procedure that repeatedly applies the operation repairing the largest number of bracket errors. This approach differs from our methodology as it is a static post-process that assumes the parser would respond perfectly to each repair, when it is possible that the parser may not perform the repair in full, or even be incapable of constructing the repaired tree. McDonald and Nivre (2011) perform an indepth comparison of the graph-based MSTparser and transition-based MaltParser. However, MaltParser uses support vector machines to deterministically predict the next transition, rather than storing the most probable options in a beam like ZPar. Additionally, they do not focus on the cascading impact of errors, and instead concentrate on higher-level error classification (e.g. by POS tag, labels and dependency lengths) in lieu of examining how the parsers respond to forced corrections. Nivre et al. (2014) describe several uses for arclevel constraints in transition-based parsing. </context>
</contexts>
<marker>McDonald, Nivre, 2011</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2011. Analyzing and Integrating Dependency Parsers. Computational Linguistics, 37(1):197–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2772" citStr="McDonald and Pereira, 2006" startWordPosition="415" endWordPosition="418">h error class in dependency parsing without making assumptions about how the parser will behave. We define error classes based on dependency labels, and use the dependencies in each class as arc constraints specifying the correct head and label for particular words in each sentence. We adapt parsers to apply these constraints, whilst otherwise proceeding with decoding under their grammar and model. By evaluating performance with and without constraints, we can directly observe the cascading impact of each error class on each the parser. We implement our procedure for the graphbased MSTparser (McDonald and Pereira, 2006) and the transition-based ZPar (Zhang and Clark, 2011) using basic Stanford dependencies over the OntoNotes 4.0 release of the WSJ Penn Treebank data. Our results show that erroneously attaching NPs, PPs, modifiers, and punctuation have the largest overall impact on UAS. Of those, NPs and punctuation have the most substantial cascading impact, indicating that these errors have the most effect on the remainder of the parse. Enforcing correct punctuation arcs has a particularly large impact on accuracy, even though most evaluation scripts ignore punctuation. We find that punctuation arcs are com</context>
<context position="10875" citStr="McDonald and Pereira, 2006" startWordPosition="1736" endWordPosition="1739">l. For example, Root constraints specify sentence roots, while PP constraints specify heads of prepositional phrases. One deficiency of our implementation is that we apply constraints to all arcs of a particular error type in each sentence, and do not isolate multiple instances of the same error class in a sentence. We do this since applying single constraints to a sentence at a time would require substantial modifications to the standard evaluation regime. 3.1 MSTparser implementation MSTparser is a graph-based, second-order parser that uses Eisner (1996)’s algorithm for projective decoding (McDonald and Pereira, 2006).1 Eisner’s algorithm constructs and caches subtrees which span progressively larger sections of the sentence. These spans are marked either as complete, consisting of a head, a dependent, and all of the descendants of that head to one side, or incomplete, consisting of a head, a dependent, and an unfilled region where additional tokens may be attached. Dependencies are formed between the head and dependent in each complete span, while label assignment occurs as a separate process. We enforce constraints by allowing complete spans to be formed only from constrained tokens 1As the variant of St</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominick Ng</author>
<author>James R Curran</author>
</authors>
<title>Dependency Hashing for n-best CCG Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL12),</booktitle>
<pages>497--505</pages>
<contexts>
<context position="15959" citStr="Ng and Curran, 2012" startWordPosition="2580" endWordPosition="2583">ied, e.g. imperatives at the beginning of a sentence are likely to be the root. We focus our constraints on evaluation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 The LME stocks decline was about as expected , but the Comex gain was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect nou</context>
</contexts>
<marker>Ng, Curran, 2012</marker>
<rawString>Dominick Ng and James R. Curran. 2012. Dependency Hashing for n-best CCG Parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL12), pages 497–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-Projective Dependency Parsing in Expected Linear Time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-09),</booktitle>
<pages>351--359</pages>
<contexts>
<context position="15804" citStr="Nivre, 2009" startWordPosition="2553" endWordPosition="2554">el constraints in transition-based parsing. However, these applications focus on improving parsing accuracy when constraints can be readily identified, e.g. imperatives at the beginning of a sentence are likely to be the root. We focus our constraints on evaluation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 The LME stocks decline was about as expected , but th</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-Projective Dependency Parsing in Expected Linear Time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-09), pages 351– 359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Yoav Goldberg</author>
<author>Ryan McDonald</author>
</authors>
<title>Constrained arc-eager dependency parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>2</issue>
<pages>257</pages>
<contexts>
<context position="12404" citStr="Nivre et al. (2014)" startWordPosition="1991" endWordPosition="1994">traints have no impact on the parser’s coverage as all possible head selections are considered. 3.2 ZPar implementation ZPar is an arc-eager transition-based parser (Zhang and Clark, 2011) that uses an incremental process with a stack storing partial parse states (Nivre et al., 2004). Each state represents tokens that may accept further arcs. The tokens of a sentence are initially stored in a buffer, and at each point during parsing, the parser decides whether or not to create an arc between the front token of the buffer and the top token on the stack. We apply constraints in a similar way to Nivre et al. (2014). Arc creation actions are factored on the dependency label to be assigned to the arc. ZPar scores each possible action using a perceptron model over features from the front of the buffer and the top of the stack (as well as some additional context features which refer to previously created states). The highest scoring actions and their resulting states are kept in a beam; during parsing, ZPar finds the optimal action for all items in the beam, and retains the highest scoring new states at each step. We disallow any arc creation action that would create an arc that conflicts with any constrain</context>
<context position="15159" citStr="Nivre et al. (2014)" startWordPosition="2450" endWordPosition="2453">epair in full, or even be incapable of constructing the repaired tree. McDonald and Nivre (2011) perform an indepth comparison of the graph-based MSTparser and transition-based MaltParser. However, MaltParser uses support vector machines to deterministically predict the next transition, rather than storing the most probable options in a beam like ZPar. Additionally, they do not focus on the cascading impact of errors, and instead concentrate on higher-level error classification (e.g. by POS tag, labels and dependency lengths) in lieu of examining how the parsers respond to forced corrections. Nivre et al. (2014) describe several uses for arclevel constraints in transition-based parsing. However, these applications focus on improving parsing accuracy when constraints can be readily identified, e.g. imperatives at the beginning of a sentence are likely to be the root. We focus our constraints on evaluation, attempting to identify important sources of error in dependency parsers. Our constraint-based approach shares similarities to oracle training and decoding methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for tr</context>
</contexts>
<marker>Nivre, Goldberg, McDonald, 2014</marker>
<rawString>Joakim Nivre, Yoav Goldberg, and Ryan McDonald. 2014. Constrained arc-eager dependency parsing. Computational Linguistics, 40(2):249– 257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-Based Dependency Parsing.</title>
<date>2004</date>
<booktitle>In HLTNAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-04),</booktitle>
<pages>49--56</pages>
<contexts>
<context position="12069" citStr="Nivre et al., 2004" startWordPosition="1928" endWordPosition="1931">ens 1As the variant of Stanford dependencies we use are projective, we did not use non-projective decoding. 1150 to their correct heads with the correct labels. Any complete span between an incorrect head and the constrained token is forbidden. The algorithm is forced to choose the constrained spans as it builds the parse; these constraints have no impact on the parser’s coverage as all possible head selections are considered. 3.2 ZPar implementation ZPar is an arc-eager transition-based parser (Zhang and Clark, 2011) that uses an incremental process with a stack storing partial parse states (Nivre et al., 2004). Each state represents tokens that may accept further arcs. The tokens of a sentence are initially stored in a buffer, and at each point during parsing, the parser decides whether or not to create an arc between the front token of the buffer and the top token on the stack. We apply constraints in a similar way to Nivre et al. (2014). Arc creation actions are factored on the dependency label to be assigned to the arc. ZPar scores each possible action using a perceptron model over features from the front of the buffer and the top of the stack (as well as some additional context features which r</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-Based Dependency Parsing. In HLTNAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-04), pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 Shared Task on Parsing the Web.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL12).</booktitle>
<contexts>
<context position="17906" citStr="Petrov and McDonald, 2012" startWordPosition="2912" endWordPosition="2915">lex features. Supertagging (Clark and Curran, 2007) and chart pruning (Zhang et al., 2010) have been used to constrain the search space of a CCG parser, and to remove unlikely or forbidden spans from repeated consideration. In our work, we use pruning not for parsing speed, but evaluation, and so we prune items based on goldstandard constraints rather than heuristics. 5 Evaluation We use the training (sections 2-21) and development (section 22) data from the OntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enriched NP structure compared to the Penn Treebank (Weischedel et al., 2011), meaning that determining NP attachments is less trivial. We changed all marker tokens in the corpus (e.g. -LRB- and -LCB-) to their equivalent unescaped punctuation marks to ensure correct evaluation. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2nlp.stanford.edu/software/lex-parser.shtml tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife trai</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In Notes of the First Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing (EMNLP96),</booktitle>
<pages>133--142</pages>
<contexts>
<context position="18404" citStr="Ratnaparkhi, 1996" startWordPosition="2984" endWordPosition="2985">WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enriched NP structure compared to the Penn Treebank (Weischedel et al., 2011), meaning that determining NP attachments is less trivial. We changed all marker tokens in the corpus (e.g. -LRB- and -LCB-) to their equivalent unescaped punctuation marks to ensure correct evaluation. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2nlp.stanford.edu/software/lex-parser.shtml tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife training was used to tag the training data. We implement a custom evaluation script to facilitate a straightforward comparative analysis between the unconstrained and constrained output. The script is based on and produces identical scores to eval06.pl, the official evaluation for the CoNLL-X Shared Task on Multilingual Dependency Parsing (Buchholz and Marsi, 2006). We ignore punctuation as defined by eval06.pl in our evaluation; experiments with constraints over punctuation tokens constrain those</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing (EMNLP96), pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine Pruning for Efficient Multi-Pass Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-12),</booktitle>
<pages>498--507</pages>
<contexts>
<context position="16234" citStr="Rush and Petrov (2012)" startWordPosition="2620" endWordPosition="2623">g methods, where an external source of truth is used to verify parser decisions. An oracle source of parser actions is a necessary component for training transition-based parsers (Nivre, 2009). Oracle decoding, where a system is forced to produce correct output if possible, can be used to assess its upper performance bounds (Ng and Curran, 2012). Constraining the parser’s internal search space is akin to an optimal pruning operation. Charniak and Johnson (2005) use a coarse-to-fine, iterative pruning approach for efficiently generating high-quality n-best parses for a discriminative reranker. Rush and Petrov (2012) use a similar coarse-to-fine algorithm with vine grammars (Eisner and Smith, 2005) to accelerate graph-based de1151 The LME stocks decline was about as expected , but the Comex gain was n’t . Figure 2: MSTparser output for the sentence in Figure 1, where the root dependency is forced to its correct value. The incorrect noun phrase error is not affected by the constraint (dashed, red), six attachment errors are repaired (solid, blue), and two new errors are introduced (dotted, purple). det amod nn nsubj prep root punct pobj advmod cc conj punct det nn nsubj neg Constraints ✓ ✗ Remaining Errors</context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander Rush and Slav Petrov. 2012. Vine Pruning for Efficient Multi-Pass Dependency Parsing. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-12), pages 498– 507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>An Empirical Study of Differences between Conversion Schemes and Annotation Guidelines.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd International Conference on Dependency Linguistics,</booktitle>
<pages>298--307</pages>
<contexts>
<context position="33657" citStr="Søgaard, 2013" startWordPosition="5472" endWordPosition="5473">d especially ZPar) stem from incorrectly identified coordination markers such as commas. Punctuation is commonly ignored in dependency parser evaluation (Yamada and Matsumoto, 2003; Buchholz and Marsi, 2006), and they are inconsistently treated across different grammars. Our results show that enforcing the correct punctuation attachments in a sentence has a substantial cascading impact, suggesting that punctuation errors are highly correlated with errors elsewhere in the analysis. Given the broad similarities between Stanford dependencies and other dependency schemes commonly used in parsing (Søgaard, 2013), we anticipate that the problems with roots and punctuation will carry across different treebanks and schemes. Punctuation is often placed at phrasal boundaries and serves to split sentences into smaller sections within a projective parser. Graph-based and transition-based parsers, both of which use a limited local context to make parsing decisions, are equally prone to the cascading impact of erroneous punctuation. Removing the confounding presence of punctuation from parsing and treating attachment as a global post-process may help to alleviate these issues. Alternatively, more punctuations</context>
</contexts>
<marker>Søgaard, 2013</marker>
<rawString>Anders Søgaard. 2013. An Empirical Study of Differences between Conversion Schemes and Annotation Guidelines. In Proceedings of the 2nd International Conference on Dependency Linguistics, pages 298–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Robert Belvin</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Nianwen Xue</author>
</authors>
<title>OntoNotes: A Large Training Corpus for Enhanced Processing.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation,</booktitle>
<pages>54--63</pages>
<editor>In Joseph Olive, Caitlin Christianson, and John McCary, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="18005" citStr="Weischedel et al., 2011" startWordPosition="2928" endWordPosition="2931">used to constrain the search space of a CCG parser, and to remove unlikely or forbidden spans from repeated consideration. In our work, we use pruning not for parsing speed, but evaluation, and so we prune items based on goldstandard constraints rather than heuristics. 5 Evaluation We use the training (sections 2-21) and development (section 22) data from the OntoNotes 4.0 release of the Penn Treebank WSJ data (Marcus et al., 1993), as supplied by the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). OntoNotes annotates enriched NP structure compared to the Penn Treebank (Weischedel et al., 2011), meaning that determining NP attachments is less trivial. We changed all marker tokens in the corpus (e.g. -LRB- and -LCB-) to their equivalent unescaped punctuation marks to ensure correct evaluation. The corpus has been converted to basic Stanford dependencies using the Stanford Parser v2.0,2 and part-of-speech 2nlp.stanford.edu/software/lex-parser.shtml tagged using MXPOST (Ratnaparkhi, 1996). A model trained on WSJ sections 2-21 was used to tag the development set, and 10-fold jackknife training was used to tag the training data. We implement a custom evaluation script to facilitate a str</context>
</contexts>
<marker>Weischedel, Hovy, Marcus, Palmer, Belvin, Pradhan, Ramshaw, Xue, 2011</marker>
<rawString>Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A Large Training Corpus for Enhanced Processing. In Joseph Olive, Caitlin Christianson, and John McCary, editors, Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation, pages 54–63. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical Dependency Analysis with Support Vector Machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop of Parsing Technologies (IWPT-03),</booktitle>
<pages>196--206</pages>
<contexts>
<context position="33223" citStr="Yamada and Matsumoto, 2003" startWordPosition="5406" endWordPosition="5409">non-crossing arcs. Table 6 summarises the error classes of the repaired cascaded arcs when punctuation constraints are applied. MSTparser has a more even distribution of repairs, while ZPar’s repairs are concentrated in coordination attachment. This shows that MSTparser is relatively better at coordination as a proportion of its overall performance compared to ZPar. It also indicates that the majority of punctuation errors in both parsers (and especially ZPar) stem from incorrectly identified coordination markers such as commas. Punctuation is commonly ignored in dependency parser evaluation (Yamada and Matsumoto, 2003; Buchholz and Marsi, 2006), and they are inconsistently treated across different grammars. Our results show that enforcing the correct punctuation attachments in a sentence has a substantial cascading impact, suggesting that punctuation errors are highly correlated with errors elsewhere in the analysis. Given the broad similarities between Stanford dependencies and other dependency schemes commonly used in parsing (Søgaard, 2013), we anticipate that the problems with roots and punctuation will carry across different treebanks and schemes. Punctuation is often placed at phrasal boundaries and </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical Dependency Analysis with Support Vector Machines. In Proceedings of the 8th International Workshop of Parsing Technologies (IWPT-03), pages 196–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Byung-Gyu Ahn</author>
<author>Stephen Clark</author>
<author>Curt Van Wyk</author>
<author>James R Curran</author>
<author>Laura Rimell</author>
</authors>
<title>Chart Pruning for Fast LexicalisedGrammar Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>1471--1479</pages>
<marker>Zhang, Ahn, Clark, Van Wyk, Curran, Rimell, 2010</marker>
<rawString>Yue Zhang, Byung-Gyu Ahn, Stephen Clark, Curt Van Wyk, James R. Curran, and Laura Rimell. 2010. Chart Pruning for Fast LexicalisedGrammar Parsing. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 1471– 1479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<date>2011</date>
<booktitle>Syntactic Processing Using the Generalized Perceptron and Beam Search. Computational Linguistics,</booktitle>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2826" citStr="Zhang and Clark, 2011" startWordPosition="423" endWordPosition="426">ons about how the parser will behave. We define error classes based on dependency labels, and use the dependencies in each class as arc constraints specifying the correct head and label for particular words in each sentence. We adapt parsers to apply these constraints, whilst otherwise proceeding with decoding under their grammar and model. By evaluating performance with and without constraints, we can directly observe the cascading impact of each error class on each the parser. We implement our procedure for the graphbased MSTparser (McDonald and Pereira, 2006) and the transition-based ZPar (Zhang and Clark, 2011) using basic Stanford dependencies over the OntoNotes 4.0 release of the WSJ Penn Treebank data. Our results show that erroneously attaching NPs, PPs, modifiers, and punctuation have the largest overall impact on UAS. Of those, NPs and punctuation have the most substantial cascading impact, indicating that these errors have the most effect on the remainder of the parse. Enforcing correct punctuation arcs has a particularly large impact on accuracy, even though most evaluation scripts ignore punctuation. We find that punctuation arcs are commonly misplaced by large distances in the final parse,</context>
<context position="11973" citStr="Zhang and Clark, 2011" startWordPosition="1911" endWordPosition="1914">e process. We enforce constraints by allowing complete spans to be formed only from constrained tokens 1As the variant of Stanford dependencies we use are projective, we did not use non-projective decoding. 1150 to their correct heads with the correct labels. Any complete span between an incorrect head and the constrained token is forbidden. The algorithm is forced to choose the constrained spans as it builds the parse; these constraints have no impact on the parser’s coverage as all possible head selections are considered. 3.2 ZPar implementation ZPar is an arc-eager transition-based parser (Zhang and Clark, 2011) that uses an incremental process with a stack storing partial parse states (Nivre et al., 2004). Each state represents tokens that may accept further arcs. The tokens of a sentence are initially stored in a buffer, and at each point during parsing, the parser decides whether or not to create an arc between the front token of the buffer and the top token on the stack. We apply constraints in a similar way to Nivre et al. (2014). Arc creation actions are factored on the dependency label to be assigned to the arc. ZPar scores each possible action using a perceptron model over features from the f</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic Processing Using the Generalized Perceptron and Beam Search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>