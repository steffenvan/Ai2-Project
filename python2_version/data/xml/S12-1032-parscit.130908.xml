<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001397">
<title confidence="0.996792">
A Probabilistic Lexical Model for Ranking Textual Inferences
</title>
<author confidence="0.967429">
Eyal Shnarch and Ido Dagan
</author>
<affiliation confidence="0.9805025">
Computer Science Department
Bar-Ilan University
</affiliation>
<address confidence="0.686605">
Ramat-Gan 52900, Israel
</address>
<email confidence="0.996221">
{shey,dagan}@cs.biu.ac.il
</email>
<author confidence="0.991387">
Jacob Goldberger
</author>
<affiliation confidence="0.985257">
Faculty of Engineering
Bar-Ilan University
</affiliation>
<address confidence="0.513297">
Ramat-Gan 52900, Israel
</address>
<email confidence="0.997086">
goldbej@eng.biu.ac.il
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999649769230769">
Identifying textual inferences, where the
meaning of one text follows from another, is
a general underlying task within many natu-
ral language applications. Commonly, it is ap-
proached either by generative syntactic-based
methods or by “lightweight” heuristic lexical
models. We suggest a model which is confined
to simple lexical information, but is formu-
lated as a principled generative probabilistic
model. We focus our attention on the task of
ranking textual inferences and show substan-
tially improved results on a recently investi-
gated question answering data set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998368078431373">
The task of identifying texts which share semantic
content arises as a general need in many natural lan-
guage processing applications. For instance, a para-
phrasing application has to recognize texts which
convey roughly the same content, and a summariza-
tion application needs to single out texts which con-
tain the content stated by other texts. We refer to this
general task as textual inference similar to prior use
of this term (Raina et al., 2005; Schoenmackers et
al., 2008; Haghighi et al., 2005).
In many textual inference scenarios the setting re-
quires a classification decision of whether the infer-
ence relation holds or not. But in other scenarios
ranking according to inference likelihood would be
the natural task. In this work we focus on ranking
textual inferences; given a sentence and a corpus,
the task is to rank the corpus passages by their plau-
sibility to imply as much of the sentence meaning as
possible. Most naturally, this is the case in question
answering (QA), where systems search for passages
that cover the semantic components of the question.
A recent line of research was dedicated to this task
(Wang et al., 2007; Heilman and Smith, 2010; Wang
and Manning, 2010).
A related scenario is the task of Recognizing Tex-
tual Entailment (RTE) within a corpus (Bentivogli
et al., 2010)1. In this task, inference systems should
identify, for a given hypothesis, the sentences which
entail it in a given corpus. Even though RTE was
presented as a classification task, it has an appeal-
ing potential as a ranking task as well. For instance,
one may want to find texts that validate a claim such
as cellular radiation is dangerous for children, or to
learn more about it from a newswire corpus. To that
end, one should look for additional mentions of this
claim such as extensive usage of cell phones may be
harmful for youngsters. This can be done by rank-
ing the corpus passages by their likelihood to entail
the claim, where the top ranked passages are likely
to contain additional relevant information.
Two main approaches have been used to address
textual inference (for either ranking or classifica-
tion). One is based on transformations over syntac-
tic parse trees (Echihabi and Marcu, 2003; Heilman
and Smith, 2010). Some works in this line describe
a probabilistic generative process in which the parse
tree of the question is generated from the passage
(Wang et al., 2007; Wang and Manning, 2010).
In the second approach, lexical models have been
employed for textual inference (MacKinlay and
Baldwin, 2009; Clark and Harrison, 2010). Typi-
</bodyText>
<footnote confidence="0.979163">
1http://www.nist.gov/tac/2010/RTE/index.html
</footnote>
<page confidence="0.891089">
237
</page>
<note confidence="0.9735385">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999949717391304">
cally, lexical models consider a text fragment as a
bag of terms and split the inference decision into
two steps. The first is a term-level estimation of the
inference likelihood for each term independently,
based on direct lexical match and on lexical knowl-
edge resources. Some commonly used resources are
WordNet (Fellbaum, 1998), distributional-similarity
thesauri (Lin, 1998), and web knowledge resources
such as (Suchanek et al., 2007). The second step
is making a final sentence-level decision based on
these estimations for the component terms. Lex-
ical models have the advantage of being fast and
easy to utilize (e.g. no dependency on parsing tools)
while being highly competitive with top performing
systems, e.g. the system of Majumdar and Bhat-
tacharyya (2010).
In this work, we investigate how well such lexi-
cal models can perform in textual inference ranking
scenarios. However, while lexical models usually
apply heuristic methods, we would like to pursue a
principled learning-based generative framework, in
analogy to the approaches for syntactic-based infer-
ence. An attractive work in this spirit is presented in
(Shnarch et al., 2011a), that propose a model which
is both lexical and probabilistic. Later, Shnarch et
al. (2011b) improved this model and reported re-
sults that outperformed previous lexical models and
were on par with state-of-the-art RTE models.
Whereas their term-level model provides means
to integrate lexical knowledge in a probabilistic
manner, their sentence-level model depends to a
great extent on heuristic normalizations which were
introduced to incorporate prominent aspects of the
sentence-level decision. This deviates their model
from a pure probabilistic methodology.
Our work aims at amending this deficiency and
proposes a new probabilistic sentence-level model
based on a Markovian process. In that model, all
parameters are estimated by an EM algorithm. We
evaluate this model on the tasks of ranking passages
for QA and ranking textual entailments within a cor-
pus, and show that eliminating the need for heuris-
tic normalizations greatly improves state-of-the-art
performance. The full implementation of our model
is available for download2 and can be used as an
easy-to-install and highly competitive inference en-
</bodyText>
<footnote confidence="0.686374">
2http://www.cs.biu.ac.il/�nlp/downloads/probLexModel.html
</footnote>
<bodyText confidence="0.987285333333333">
gine that operates only on lexical knowledge, or as a
lexical component integrated within a more complex
inference system.
</bodyText>
<sectionHeader confidence="0.943264" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999970833333333">
Wang et al. (2007) provided an annotated data set,
based on the Text REtrieval Conference (TREC) QA
tracks3, specifically for the task of ranking candidate
answer passages. We adopt their experimental setup
and next review the line of syntactic-based works
which reported results on this data set.
</bodyText>
<subsectionHeader confidence="0.996097">
2.1 Syntactic generative models
</subsectionHeader>
<bodyText confidence="0.999894333333333">
Wang et al. (2007) propose a quasi-synchronous
grammar formulation which specifies the generation
of the question parse tree, loosely conditioned on the
parse tree of the candidate answer passage. Their
model showed improvement over previous syntac-
tic models for QA: Punyakanok et al. (2004), who
computed similarity between question-answer pairs
with a generalized tree-edit distance, and Cui et al.
(2005), who developed an information measure for
sentence similarity based on dependency paths of
aligned words. Wang et al. (2007) reproduced these
methods and extended them to utilize WordNet.
More recently, Heilman and Smith (2010) im-
proved Wang et al. (2007) results with a classifica-
tion based approach. Feature for the classifier were
extracted from a greedy algorithm which searches
for tree-edit sequences which transform the parse
tree of the candidate answer into the one of the ques-
tion. Unlike other works reviewed here, this one
does not utilize lexical knowledge resources.
Similarly, Wang and Manning (2010) present an
extended tree-edit operations set and search for edit
sequences to generate the question from the answer
candidate. Their CRF-based classifier models these
sequences as latent variables.
An important merit of these methods is that they
offer principled, often probabilistic, generative mod-
els for the task of ranking candidate answers. Their
drawback is the need for syntactic analysis which
makes them slower to run, dependent on parsing per-
formance, which is often mediocre in many text gen-
res, and inadequate for languages which lack proper
parsing tools.
</bodyText>
<footnote confidence="0.956037">
3http://trec.nist.gov/data/qamain.html
</footnote>
<page confidence="0.990248">
238
</page>
<subsectionHeader confidence="0.997752">
2.2 Lexical models
</subsectionHeader>
<bodyText confidence="0.993286632911392">
Lexical models, on the other hand, are faster, eas-
ier to implement and are more practical for vari-
ous genres and languages. Such models derive from
knowledge resources lexical inference rules which
indicate that the meaning of a lexical term can be
inferred from the meaning of another term (e.g.
youngsters —* children and harmful —* dangerous).
They are common in the Recognizing Textual En-
tailment (RTE) systems and we present some rep-
resentative methods for that task. We adopt textual
entailment terminology and henceforth use Hypoth-
esis (denoted H) for the inferred text fragment and
Text (denoted T) for the text from which it is being
inferred4.
Majumdar and Bhattacharyya (2010) utilized a
simple union of lexical rules derived from vari-
ous lexical resources for the term-level step. They
derived their sentence-level decision based on the
number of matched hypothesis terms. The results
of this simple model were only slightly worse than
the best results of the RTE-6 challenge which were
achieved by a syntactic-based system (Jia et al.,
2010). Clark and Harrison (2010), on the other hand,
considered the number of mismatched terms in es-
tablishing their sentence-level decision. MacKinlay
and Baldwin (2009) represented text and hypothe-
sis as word vectors augmented with lexical knowl-
edge. For sentence-level similarity they used a vari-
ant of the cosine similarity score. Common to most
of these lexical models is the application of heuris-
tic methods in both the term and the sentence level
steps.
Targeted to replace heuristic methods with princi-
pled ones, Shnarch et al. (2011a) present a model
which aims at combining the advantages of a proba-
bilistic generative model with the simplicity of lex-
ical methods. In some analogy to generative parse-
tree based models, they propose a generative process
for the creation of the hypothesis from the text.
At the term-level, their model combines knowl-
edge from various input resources and has the ad-
vantages of considering the effect of transitive rule
application (e.g. mobile phone —* cell phone —* cel-
lular) as well as the integration of multiple pieces
4In the task of passage ranking for QA, the hypothesis is the
question and the text is the candidate passage.
of evidence for the inference of a term (e.g. both
the appearance of harmful and risky in T provide
evidence for the inference of dangerous in H). We
denote this term-level Probabilistic Lexical Model
as PLMT L, and have reproduced it in our work as
presented in Section 4.1. For the sentence-level de-
cision they describe an AND gate mechanism, i.e.
deducing a positive inference decision for H as a
whole only if all its terms were inferred from T.
In an extension to that work, Shnarch et al.
(2011b) modified PLMTL to improve the sentence-
level step. They pointed out some prominent aspects
for the sentence-level decision. First, they suggest
that a hypothesis as a whole can be inferred from
the text even if some of its terms are not inferred.
To model this, they introduced a noisy-AND mech-
anism (Pearl, 1988). Additionally, they emphasized
the effect of hypothesis length and the dependency
between terms on the sentence-level decision. How-
ever, they did not fully achieve their target of pre-
senting a fully coherent probabilistic model, as their
model included heuristic normalization formulae.
On the contrary, the model we present is the first
along this line to be fully specified in terms of a
generative setting and formulated in pure probabilis-
tic terms. We introduce a Markovian-style proba-
bilistic model for the sentence-level decision. This
model receives as input term-level probabilistic es-
timates, which may be provided by any term-level
model. In our implementation we embed PLMT L as
the term-level model and present a complete coher-
ent Markovian-based Probabilistic Lexical Model,
which we term M-PLM.
</bodyText>
<sectionHeader confidence="0.969271" genericHeader="method">
3 Markovian sentence-level model
</sectionHeader>
<bodyText confidence="0.9999455">
The goal of a sentence-level model is to integrate
term-level inputs into an inference decision for the
hypothesis as a whole. For a hypothesis H =
hi, ... , h, and a text T, term-level models first esti-
mate independently for each term ht its probability
to be inferred from T. Let xt be a binary random
variable representing the event that ht is indeed in-
ferred from T (i.e., xt = 1 if ht is inferred and 0
otherwise).
Given these term-level probabilities, a sentence-
level model is employed to estimate the probability
that H as a whole is inferred from T. This step is
</bodyText>
<page confidence="0.988191">
239
</page>
<figure confidence="0.858937">
Text:
</figure>
<figureCaption confidence="0.999462">
Figure 1: A probabilistic lexical model: the upper part is the
</figureCaption>
<bodyText confidence="0.9953003125">
term-level input to the sentence-level Markovian process, de-
picted in the lower part. xi is a binary variable representing the
inference of hi and yj is a variable for the accumulative infer-
ence decision for the first j terms of Hypo. The final sentence-
level decision is given by yn.
the focus of our work. We assume that the term-
level probabilities are given as input. Section 4.1
describes PLMTL, as a concrete method for deriving
these probabilities.
Our sentence-level model is based on a Marko-
vian process and is described in Section 3.1. In par-
ticular, it takes into account, in probabilistic terms,
the prominent factors in lexical entailment, men-
tioned in Section 2. An efficient inference algorithm
for our model is given in Section 3.2 and EM-based
learning is specified in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.999382">
3.1 Markovian sentence-level decision
</subsectionHeader>
<bodyText confidence="0.99982494117647">
The motivation for proposing a Markovian process
for the sentence-level is to establish an intermedi-
ate model, lying between two extremes: assuming
full independence between hypothesis terms versus
assuming that every term is dependent on all other
terms. The former alternative is too weak, while
the latter alternative is computationally hard and
not very informative, and thus hard to capture in
a model. Our model specifies a Markovian depen-
dence structure, which limits the dependence scope
to adjacent terms, as follows.
We define a binary variable yt to be the accumu-
lated sentence-level inference decision up to ht. In
other words, yt =1 if the subset {h1, ... , ht} of H’s
terms is inferred as a whole from T.
Note that this means that yt can be 1 even if some
terms amongst h1, ... , ht are not inferred. As yn is
the decision for the complete hypothesis, our model
addresses this way the prominent aspect that the hy-
pothesis as a whole may be inferred even if some of
its terms are not inferred. The reason for allowing
this is that such un-inferred terms may be inferred
from the global context of T, or alternatively, are ac-
tually inferred from T but the knowledge resources
in use do not contain the proper lexical rule to make
such inference.
Figure 1 describes both steps of a full lexical in-
ference model. Its lower part depicts our Markovian
process. In the proposed model the inference deci-
sion at each position t is a combination of xt, the
variable for the event of ht being inferred, and yt−1,
the accumulated decision at the previous position.
Therefore, the transition parameters of M-PLM can
be modeled as:
</bodyText>
<equation confidence="0.91218">
qij(k)=P(yt=k|yt−1=i,xt=j) bk,i,jE{0,1}
</equation>
<bodyText confidence="0.999457333333333">
where y1=x1. For instance, q01(1) is the probability
that yt =1, given that yt−1=0 and xt =1.
Applying the Markovian process on the entire
hypothesis we get yn, which represents the final
sentence-level decision, where a soft decision is ob-
tained by computing the probability of yn =1:
</bodyText>
<equation confidence="0.99572">
P(xt)P(yt|yt−1, xt)
x1, ..., xn
y2, ..., yn−1, yn =1
</equation>
<bodyText confidence="0.999810235294118">
The summation is done over all possible binary
values of the term-level variables x1, ..., xn and the
accumulated sentence-level variables y2,..., yn−1
where yn =1. Note that for clarity, in this formula xt
and yt denote the binary values at the corresponding
variable positions. A tractable form for computing
P(yn =1) is presented in Section 3.2.
Overall, the prominent factors in lexical entail-
ment, raised by prior works, are incorporated within
the core structure of this probabilistic model, with-
out the need to resort to heuristic normalizations.
Reducing the negative affect of hypothesis length on
the entailment probability is achieved by having yt,
at each position, being directly dependent only on xt
and yt−1 as opposed to being affected by all hypoth-
esis terms. The second factor, modeling the depen-
dency between hypothesis terms, is addressed by the
</bodyText>
<equation confidence="0.763157857142857">
Hypo:
term-level
sentence-lev
P(yn =1) = � P(x1)
n
ri
t=2
</equation>
<page confidence="0.96808">
240
</page>
<bodyText confidence="0.999916461538462">
indirect dependency of yn on all preceding hypothe-
sis terms. This dependency arises from the recursive
nature of the Markovian model, as can be seen in the
next section.
Our proposed Markovian process presents a linear
dependency between terms which, to some extent,
poses an anomaly with respect to the structure of the
entailment phenomenon. Yet, as we do want to limit
the dependence structure, following the natural or-
der of the sentence words seems the most reasonable
choice, as common in many other types of sequential
models. We also tried randomizing the word order
which, on average, did not improve performance.
</bodyText>
<subsectionHeader confidence="0.869731">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.9959198">
The accumulated sentence-level inference can be
efficiently computed using a typical forward algo-
rithm. We denote the probability of xt =j, j ∈{0,1}
by ht(j) = P(xt = j). The forward step is given in
Eq. (1) and its initialization is defined in Eq. (2).
</bodyText>
<equation confidence="0.999921666666667">
αt(k) = P(yt=k)= X αt−1(i)ht(j)qij(k) (1)
i,j∈{0,1}
α1(k) = P(x1 =k) (2)
</equation>
<bodyText confidence="0.994808125">
where k ∈{0,1} and t = 2,..., n.
αt(k) is the probability that the accumulated de-
cision at position t is k. It is calculated by sum-
ming over the probabilities of all four combinations
of αt−1(i) and ht(j), multiplied by the correspond-
ing transition probability, qij(k).
The soft sentence-level decision can be efficiently
calculated by:
</bodyText>
<equation confidence="0.981772">
P(yn =a) = αn(a) a∈{0, 1} (3)
</equation>
<subsectionHeader confidence="0.997256">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.999976769230769">
Typically, natural language applications work at the
sentence-level. The training data for such applica-
tions is, therefore, available as annotations at the
sentence-level. Term-level alignments between pas-
sage terms and question terms are rarely available.
Hence, we learn our term-level parameters from
available sentence-level annotations, using the gen-
erative process described above to bridge the gap be-
tween these two levels.
For learning we use the typical backwards algo-
rithm which is described by Eq. (4) and Eq. (5),
where Qt(a|i) is the probability that the full hypoth-
esis inference value is a given that yt =i.
</bodyText>
<equation confidence="0.99518775">
Qn(a|i) = P(yn =a|yn=i) = 1{a=i} (4)
Qt(a|i) = P(yn =a|yt =i) =
X= ht+1(j)qij(k)Qt+1(a|k) (5)
j,k∈{0,1}
</equation>
<bodyText confidence="0.9948043">
where t = n−1, ..,1, a ∈ {0,1} and 1{condition} is
the indicator function which returns 1 if condition
holds and 0 otherwise.
To estimate qij(k), the parameters of the Marko-
vian process, we employ the EM algorithm:
E-step: For each (T, H) pair in the training
data set, annotated with a ∈ {0,1} as its sentence-
level inference value, we evaluate the expected
probability of every transition given the annotation
value a:
</bodyText>
<equation confidence="0.97856">
wtijk(T,H) = P(yt−1=i,xt=j,yt=k|yn=a)
αt−1(i)ht(j)qij(k)Qt(a|k)
= (6)
P(yn =a)
∀i, j, k ∈{0,1} and t = 2,..., |H|.
</equation>
<bodyText confidence="0.9644854">
M-step: Given the values of wtijk(T, H) we
can estimate each qij(1), i, j ∈{0, 1}, by taking the
proportion of transitions in which yt−1 = i, xt = j
and yt = 1, out of the total transitions in which
yt−1=i and xt =j:
</bodyText>
<equation confidence="0.999207">
P P|H|
t=2 wtij1(T, H)
(T,H)
qij(1) ← (7)
H|)
P(T,H) Pt=2 Pk∈{0,1} wtijk(T, H
qij(0) = 1−qij(1)
</equation>
<sectionHeader confidence="0.99111" genericHeader="method">
4 Complete model implementation
</sectionHeader>
<bodyText confidence="0.9999959">
We next describe the end-to-end probabilistic lexical
inference model we used in our evaluations. We im-
plemented PLMT L as our term-level model to pro-
vide us with ht(j), the term-level probabilities. We
chose this model since it is fully lexical, has the ad-
vantages of lexical knowledge integration described
in Section 2 and achieved top results on RTE data
sets. Next, we summarize PLMTL, and in Appendix
A we show how to adjust the learning schema to fit
into our sentence-level model.
</bodyText>
<page confidence="0.994855">
241
</page>
<subsectionHeader confidence="0.475465">
4.1 PLMTL
</subsectionHeader>
<bodyText confidence="0.999989282051282">
Shnarch et al. (2011a) provide a term-level model
which integrates lexical rules from various knowl-
edge resources. As described below it also consid-
ers transitive chains of rule applications as well as
the impact of parallel chains which provide multiple
evidence that hEH is inferred from T.
Their model assumes a parameter OR for each
knowledge resource R in use. OR specifies the re-
source’s reliability, i.e. the prior probability that ap-
plying a rule from R to an arbitrary text-hypothesis
pair would yield a valid inference.
Next, transitive chains may connect a text term to
a hypothesis term via intermediate term(s). For in-
stance, starting from the text term T-Mobile, a chain
that utilizes the lexical rules T-Mobile —* telecom
and telecom —* cell phone enables the inference of
the term cell phone from T. They compute, for each
step in a chain, the probability that this step is valid
based on the OR values. Denoting the resource which
provided a rule r by R(r), Eq. (8) specifies that the
validity probability of the inference step correspond-
ing to the application of the rule r within the chain c
pointing at ht (as represented by xtcr) is OR(r).
Next, for a chain c pointing at ht (represented by
xtc) to be valid, all its rule steps should be valid for
this pair. Eq. (9) estimates this probability by the
joint probability that the applications of all rules r E
c are valid, assuming independence of rules.
Several chains may connect terms in T to ht, thus
providing multiple pieces of evidence that ht is in-
ferred from T. For instance, both youngsters and
kids in T may indicate the inference of children in
H. For a term ht to be inferred from the entire sen-
tence T it is enough that at least one of the chains
from T to ht is valid. This is the complement event
of ht not being inferred from T which happens when
all chains which suggest the inference of ht, denoted
by Qht), are invalid. Eq. (10) specifies this proba-
bility (again assuming independence of chains).
</bodyText>
<equation confidence="0.998410833333333">
P(xtcr = 1) = OR(r) (8)
�P(xtc = 1) = P(xtcr = 1) (9)
rEc
ht(1) = P(xt = 1) = 1−P(xt = 0) (10)
= 1− � P(xtc = 0)
cEC(ht)
</equation>
<bodyText confidence="0.99998965">
With respect to the contributions of our work, we
note that previous works resorted to applying some
heuristic amendments on these equations to achieve
valuable results. In contrast, our work is the first
to present a purely generative model. This achieve-
ment shows that it is possible to shift from ad-hoc
heuristic methods, which are common practice, to
more solid mathematically-based methods.
Finally, for ranking text passages from a corpus
for a given hypothesis (question in the QA scenario),
our Markovian sentence-level model takes as its in-
put the outcome of Eq. (10) for each ht E H. For
PLMTL we need to estimate the model parameters,
that is the various OR values. In our Markovian
model this is done by the scheme detailed in Ap-
pendix A. Given these term-level probabilities, our
model computes for each hypothesis its probabil-
ity to be inferred from each of the corpus passages,
namely P(y,,, = 1) in Eq (3). Passages are then
ranked according to this probability.
</bodyText>
<sectionHeader confidence="0.970244" genericHeader="evaluation">
5 Evaluations and Results
</sectionHeader>
<bodyText confidence="0.999983571428572">
To evaluate the performance of M-PLM for ranking
textual inferences we focused on the task of ranking
candidate answer passages for question answering
(QA) as presented in Section 5.1. Additionally, we
demonstrate the added value of our sentence-level
model in another ranking experiment based on RTE
data sets, described in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.994425">
5.1 Answer ranking for question answering
</subsectionHeader>
<bodyText confidence="0.99986225">
Data set We adopted the experimental setup of
Wang et al. (2007) who also provided an annotated
data set for answer passage ranking in QA5.
In their data set an instance is a pair of a factoid
question and a candidate answer passage (a single
sentence in this data set). It was constructed from the
data of the QA tracks at TREC 8–13. The question-
candidate pairs were manually judged and a pair was
annotated as positive if the candidate passage indi-
cates the correct answer for the question. The train-
ing and test sets roughly contain 5700 and 1500 pairs
correspondingly.
</bodyText>
<footnote confidence="0.99565325">
5The data set was kindly provided to us by
Mengqiu Wang and is available for download at
http://www.cs.stanford.edu/˜mengqiu/data/qg-emnlp07-
data.tgz.
</footnote>
<page confidence="0.995574">
242
</page>
<bodyText confidence="0.999736139534884">
Method PLMTL utilizes WordNet and the Catvar
(Categorial Variation) derivations database (Habash
and Dorr, 2003) as generic and publicly available
lexical knowledge resources, when question and
answer terms are restricted to the first WordNet
sense. In order to be consistent with (Shnarch et al.,
2011b), the best performing model of prior work,
we restricted our model to utilize only these two re-
sources which they used. However, additional lexi-
cal resources can be provided as input to our model
(e.g. a distributional similarity-base thesaurus).
We report Mean Average Precision (MAP) and
Mean Reciprocal Rank (MRR), the standard mea-
sures for ranked lists. In the cases of tie we took
a conservative approach and ranked positive anno-
tated instances below the negative instances scored
with the same probability. Hence, the reported fig-
ures are lower-bounds for any tie-breaking method
that could have been applied.
Results We compared our model to all 5 mod-
els evaluated for this data set, described in Sec-
tion 2, and to our own implementation of (Shnarch
et al., 2011b). We term this model Heuristically-
Normalized Probabilistic Lexical Model, HN-PLM,
since it modifies PLMTL by introducing heuristic
normalization formulae. As explained earlier, both
M-PLM and HN-PLM embed PLMT L in their im-
plementation but they differ in their sentence-level
model. In our implementation of both models,
PLMTL applies chains of transitive rule applications
whose maximal length is 3.
As seen in Table 1, M-PLM outperforms all prior
models by a large margin. A comparison of M-PLM
and HN-PLM reveals the major positive effect of
choosing the Markovian process for the sentence-
level decision. By avoiding heuristically-normalized
formulae and having all our parameters being part of
the Markovian model, we managed to increases both
MAP and MRR by nearly 2.5%6.
Ablation Test As an additional examination of
the impact of the Markovian process components,
we evaluated the contribution of having 4 transition
parameters. The AND-logic applied by (Shnarch et
</bodyText>
<footnote confidence="0.9518338">
6The difference is not significant according to the Wilcoxon
test, however we note that given the data set size it is hard to get
a significant difference and that both Heilman and Smith (2010)
and Wang and Manning (2010) improvements over the results
of Wang et al. (2007) were not statistically significant.
</footnote>
<table confidence="0.98291225">
System MAP MRR
Punyakanok et al. 41.89 49.39
Cui et al. 43.50 55.69
Wang &amp; Manning 59.51 69.51
Wang et al. 60.29 68.52
Heilman &amp; Smith 60.91 69.17
Shnarch et al. HN-PLM 61.89 70.24
M-PLM 64.38 72.69
</table>
<tableCaption confidence="0.993791">
Table 1: Results (in %) for the task of answer ranking for
question answering (sorted by MAP).
</tableCaption>
<bodyText confidence="0.99913095">
al., 2011a) to their sentence-level decision roughly
corresponds to 2 of the Markovian parameters. A
binary AND outputs 1 if both its inputs are 1. This
corresponds to q11(1) which is indeed estimate to be
near 1. In any other case an AND gate outputs 0.
This corresponds to q00(1) which was estimated to
be near zero.
The two parameters q01 and q10 are novel to the
Markovian process and do not have counterparts in
(Shnarch et al., 2011a). These parameters are the
cases in which the sentence-level decision accumu-
lated so far and the term-level decision do not agree.
Introducing these 2 parameters enables our model to
provide a positive decision for the hypothesis as a
whole (or for a part of it) even if some of its terms
were not inferred. We performed an ablation test on
each of these two parameters by forcing the value of
the ablated parameter to be zero. The notable perfor-
mance drop presented in Table 2 indicates the crucial
contribution of these parameters to our model.
</bodyText>
<table confidence="0.875404666666667">
Ablated parameter A MAP A MRR
q01(1) = 0 -2.61 -4.91
q10(1) = 0 -2.12 -2.86
</table>
<tableCaption confidence="0.928956">
Table 2: Ablation test for the novel parameters of the Marko-
vian process. Results (in %) indicate performance drop when
forcing a parameter to be zero.
</tableCaption>
<subsectionHeader confidence="0.985423">
5.2 RTE evaluations
</subsectionHeader>
<bodyText confidence="0.998733">
To assess the added value of our model on an addi-
tional ranking evaluation, we utilize the search task
data sets of the recent Recognizing Textual Entail-
ment (RTE) benchmarks (Bentivogli et al., 2009;
Bentivogli et al., 2010), which were originally con-
</bodyText>
<page confidence="0.995011">
243
</page>
<bodyText confidence="0.989847875">
structed for the task of entailment classification. In
that task a hypothesis is given with a corpus and the
goal is to identify which sentences of the corpus en-
tail the hypothesis. This setting naturally lends itself
to a ranking scenario, in which the desired output is
a list of the corpus sentences ranked by their proba-
bility to entail the given hypothesis.
To that end, we employed the same method-
ology as described in the previous section. Ta-
ble 3 presents the improvement of our model over
HN-PLM, whose classification performance was re-
ported to be on par with best-performing systems on
these data sets7. As can be seen, the improvement
is substantial for both measures on both data sets.
These results further assess the contribution of our
Markovian sentence-level model.
</bodyText>
<table confidence="0.997375">
RTE-5 RTE-6
MAP MRR MAP MRR
HN-PLM 58.0 82.9 54.0 71.9
M-PLM 61.6 84.8 60.0 79.2
A +3.6 +1.9 +6.0 +7.3
</table>
<tableCaption confidence="0.944596">
Table 3: Improvements of our sentence-level model over
HN-PLM. Results (in %) are shown for the last RTE and
for the search task in RTE-5.
</tableCaption>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.960908526315789">
This paper investigated probabilistic lexical mod-
els for ranking textual inferences focusing on pas-
sage ranking for QA. We showed that our coher-
ent probabilistic model, whose sentence-level model
is based on a Markovian process, considerably im-
proves five prior syntactic-based models as well as
a heuristically-normalized lexical model. Therefore,
it raises the baseline for future methods.
In future work we would like to further explore
a broader range of related probabilistic models. Es-
pecially, as our Markovian process is dependent on
term order, it would be interesting to investigate
models which are not order dependent.
Initial experiments on the classification task show
that M-PLM performs well above the average sys-
tem but below HN-PLM, since it does not normalize
7RTE data sets were only used for the classification task
so far, therefore there are no state-of-the-art results to compare
with, when utilizing them for the ranking task.
the estimated probability well across hypothesis. We
therefore suggest a future work on better classifica-
tion models.
Finally, we view this work as joining a line of re-
search which develops principled probabilistic mod-
els for the task of textual inference and demonstrates
their superiority over heuristic methods.
A Appendix: Adaptation of PLMTL
learning
M-PLM embeds PLMTL as its term-level model.
PLMTL introduces BR values as additional parame-
ters for the complete model. We show how we mod-
ify (Shnarch et al., 2011a) E-step formula to fit our
Markovian modeling, described in Section 3.1. The
M-step formula remains exactly the same.
Eq. (11) estimates the a-posteriori validity prob-
ability of a single application of the rule r in the
transitive chain c pointing at ht, given that the an-
notation of the pair is a.
</bodyText>
<equation confidence="0.995975">
wtcr(T, H) = P(xtcr = 1|yn = a) =
P(yn = a)
</equation>
<bodyText confidence="0.9998854">
where t=2 ... n and P(xt =j|xtcr =1) is the prob-
ability that the inference value of xt is j, given that
the application of r provides a valid inference step.
As appeared in (Shnarch et al., 2011b) this probabil-
ity can be evaluated as follows:
</bodyText>
<equation confidence="0.995956666666667">
� P(xt = 0) 1− P(xtc = 1)
P(xt =1|xtcr =1)=1−
P(xtc = 0) BR(r)
</equation>
<bodyText confidence="0.998886666666667">
For t = 1 there is no accumulated sentence-level
decision at the previous position (i.e. no αt−1) there-
fore Eq. (11) becomes:
</bodyText>
<equation confidence="0.81716">
w1cr(T, H) = EjE10,1}P(x1=j |x1cr = 1)BR(r)01(a |j)
</equation>
<sectionHeader confidence="0.998152" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995338857142857">
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community’s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
</bodyText>
<equation confidence="0.968597666666667">
(11)
Ei,j,kE10,1} αt−1(i)P(xt =j|xtcr =1)BR(r)qij(k)Qt(a|k)
P(yn = a)
</equation>
<page confidence="0.998636">
244
</page>
<sectionHeader confidence="0.995866" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775963414634">
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The sixth
PASCAL recognizing textual entailment challenge. In
Proceedings of the Text Analysis Conference.
Peter Clark and Phil Harrison. 2010. BLUE-Lite: a
knowledge-based lexical entailment system for RTE6.
In Proceedings of the Text Analysis Conference.
Hang Cui, Renxu Sun, Keya Li, Min yen Kan, and Tat
seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings of
SIGIR.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
Nizar Habash and Bonnie Dorr. 2003. A categorial vari-
ation database for english. In Proceedings of the Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings of
the Conference of the North American Chapter of the
Association for Computational Linguistics.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM participation
at the Text Analysis Conference 2010 RTE and sum-
marization track. In Proceedings of the Text Analysis
Conference.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING.
Andrew MacKinlay and Timothy Baldwin. 2009. A
baseline approach to the RTE5 search pilot. In Pro-
ceedings of the Text Analysis Conference.
Debarghya Majumdar and Pushpak Bhattacharyya.
2010. Lexical based text entailment system for main
task of RTE6. In Proceedings of the Text Analysis
Conference.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks ofplausible inference. Morgan
Kaufmann.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004.
Mapping dependencies trees: An application to ques-
tion answering. In Proceedings of the International
Symposium on Artificial Intelligence and Mathemat-
ics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Stefan Schoenmackers, Oren Etzioni, and Daniel Weld.
2008. Scaling textual inference to the web. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011a.
A probabilistic modeling framework for lexical entail-
ment. In Proceedings ofACL.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011b.
Towards a probabilistic model for lexical entailment.
In Proceedings of the TextInfer Workshop on Textual
Entailment.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of WWW.
Mengqiu Wang and Christopher Manning. 2010. Proba-
bilistic tree-edit models with structured latent variables
for textual entailment and question answering. In Pro-
ceedings of Coling.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
</reference>
<page confidence="0.998707">
245
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.050956">
<title confidence="0.997389">A Probabilistic Lexical Model for Ranking Textual Inferences</title>
<author confidence="0.795796">Shnarch</author>
<affiliation confidence="0.7872895">Computer Science Bar-Ilan</affiliation>
<address confidence="0.594958">Ramat-Gan 52900,</address>
<title confidence="0.448545">Jacob Faculty of Bar-Ilan</title>
<author confidence="0.640708">Ramat-Gan</author>
<email confidence="0.99539">goldbej@eng.biu.ac.il</email>
<abstract confidence="0.993473">Identifying textual inferences, where the meaning of one text follows from another, is a general underlying task within many natural language applications. Commonly, it is approached either by generative syntactic-based methods or by “lightweight” heuristic lexical models. We suggest a model which is confined to simple lexical information, but is formulated as a principled generative probabilistic model. We focus our attention on the task of textual inferences show substantially improved results on a recently investigated question answering data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
</authors>
<title>The fifth PASCAL recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<contexts>
<context position="28173" citStr="Bentivogli et al., 2009" startWordPosition="4623" endWordPosition="4626">y forcing the value of the ablated parameter to be zero. The notable performance drop presented in Table 2 indicates the crucial contribution of these parameters to our model. Ablated parameter A MAP A MRR q01(1) = 0 -2.61 -4.91 q10(1) = 0 -2.12 -2.86 Table 2: Ablation test for the novel parameters of the Markovian process. Results (in %) indicate performance drop when forcing a parameter to be zero. 5.2 RTE evaluations To assess the added value of our model on an additional ranking evaluation, we utilize the search task data sets of the recent Recognizing Textual Entailment (RTE) benchmarks (Bentivogli et al., 2009; Bentivogli et al., 2010), which were originally con243 structed for the task of entailment classification. In that task a hypothesis is given with a corpus and the goal is to identify which sentences of the corpus entail the hypothesis. This setting naturally lends itself to a ranking scenario, in which the desired output is a list of the corpus sentences ranked by their probability to entail the given hypothesis. To that end, we employed the same methodology as described in the previous section. Table 3 presents the improvement of our model over HN-PLM, whose classification performance was </context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth PASCAL recognizing textual entailment challenge. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The sixth PASCAL recognizing textual entailment challenge.</title>
<date>2010</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<contexts>
<context position="2192" citStr="Bentivogli et al., 2010" startWordPosition="338" endWordPosition="341">nce likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1. In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of this claim such as extensive usage of cell phones may be harmful for youngsters. This can be done by ranking the corpus passages by their l</context>
<context position="28199" citStr="Bentivogli et al., 2010" startWordPosition="4627" endWordPosition="4630">e ablated parameter to be zero. The notable performance drop presented in Table 2 indicates the crucial contribution of these parameters to our model. Ablated parameter A MAP A MRR q01(1) = 0 -2.61 -4.91 q10(1) = 0 -2.12 -2.86 Table 2: Ablation test for the novel parameters of the Markovian process. Results (in %) indicate performance drop when forcing a parameter to be zero. 5.2 RTE evaluations To assess the added value of our model on an additional ranking evaluation, we utilize the search task data sets of the recent Recognizing Textual Entailment (RTE) benchmarks (Bentivogli et al., 2009; Bentivogli et al., 2010), which were originally con243 structed for the task of entailment classification. In that task a hypothesis is given with a corpus and the goal is to identify which sentences of the corpus entail the hypothesis. This setting naturally lends itself to a ranking scenario, in which the desired output is a list of the corpus sentences ranked by their probability to entail the given hypothesis. To that end, we employed the same methodology as described in the previous section. Table 3 presents the improvement of our model over HN-PLM, whose classification performance was reported to be on par with</context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2010</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2010. The sixth PASCAL recognizing textual entailment challenge. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Phil Harrison</author>
</authors>
<title>BLUE-Lite: a knowledge-based lexical entailment system for RTE6.</title>
<date>2010</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<contexts>
<context position="3442" citStr="Clark and Harrison, 2010" startWordPosition="547" endWordPosition="550">laim, where the top ranked passages are likely to contain additional relevant information. Two main approaches have been used to address textual inference (for either ranking or classification). One is based on transformations over syntactic parse trees (Echihabi and Marcu, 2003; Heilman and Smith, 2010). Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (Wang et al., 2007; Wang and Manning, 2010). In the second approach, lexical models have been employed for textual inference (MacKinlay and Baldwin, 2009; Clark and Harrison, 2010). Typi1http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources. Some commonly used resources are WordNet (Fellbaum, 1998), distributional-similarity thesauri (Lin, 19</context>
<context position="9193" citStr="Clark and Harrison (2010)" startWordPosition="1412" endWordPosition="1415">s for that task. We adopt textual entailment terminology and henceforth use Hypothesis (denoted H) for the inferred text fragment and Text (denoted T) for the text from which it is being inferred4. Majumdar and Bhattacharyya (2010) utilized a simple union of lexical rules derived from various lexical resources for the term-level step. They derived their sentence-level decision based on the number of matched hypothesis terms. The results of this simple model were only slightly worse than the best results of the RTE-6 challenge which were achieved by a syntactic-based system (Jia et al., 2010). Clark and Harrison (2010), on the other hand, considered the number of mismatched terms in establishing their sentence-level decision. MacKinlay and Baldwin (2009) represented text and hypothesis as word vectors augmented with lexical knowledge. For sentence-level similarity they used a variant of the cosine similarity score. Common to most of these lexical models is the application of heuristic methods in both the term and the sentence level steps. Targeted to replace heuristic methods with principled ones, Shnarch et al. (2011a) present a model which aims at combining the advantages of a probabilistic generative mod</context>
</contexts>
<marker>Clark, Harrison, 2010</marker>
<rawString>Peter Clark and Phil Harrison. 2010. BLUE-Lite: a knowledge-based lexical entailment system for RTE6. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min yen Kan</author>
<author>Tat seng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="6863" citStr="Cui et al. (2005)" startWordPosition="1048" endWordPosition="1051">y for the task of ranking candidate answer passages. We adopt their experimental setup and next review the line of syntactic-based works which reported results on this data set. 2.1 Syntactic generative models Wang et al. (2007) propose a quasi-synchronous grammar formulation which specifies the generation of the question parse tree, loosely conditioned on the parse tree of the candidate answer passage. Their model showed improvement over previous syntactic models for QA: Punyakanok et al. (2004), who computed similarity between question-answer pairs with a generalized tree-edit distance, and Cui et al. (2005), who developed an information measure for sentence similarity based on dependency paths of aligned words. Wang et al. (2007) reproduced these methods and extended them to utilize WordNet. More recently, Heilman and Smith (2010) improved Wang et al. (2007) results with a classification based approach. Feature for the classifier were extracted from a greedy algorithm which searches for tree-edit sequences which transform the parse tree of the candidate answer into the one of the question. Unlike other works reviewed here, this one does not utilize lexical knowledge resources. Similarly, Wang an</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min yen Kan, and Tat seng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3096" citStr="Echihabi and Marcu, 2003" startWordPosition="492" endWordPosition="495">hat validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of this claim such as extensive usage of cell phones may be harmful for youngsters. This can be done by ranking the corpus passages by their likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information. Two main approaches have been used to address textual inference (for either ranking or classification). One is based on transformations over syntactic parse trees (Echihabi and Marcu, 2003; Heilman and Smith, 2010). Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (Wang et al., 2007; Wang and Manning, 2010). In the second approach, lexical models have been employed for textual inference (MacKinlay and Baldwin, 2009; Clark and Harrison, 2010). Typi1http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Bonnie Dorr</author>
</authors>
<title>A categorial variation database for english.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24199" citStr="Habash and Dorr, 2003" startWordPosition="3958" endWordPosition="3961">nswer passage (a single sentence in this data set). It was constructed from the data of the QA tracks at TREC 8–13. The questioncandidate pairs were manually judged and a pair was annotated as positive if the candidate passage indicates the correct answer for the question. The training and test sets roughly contain 5700 and 1500 pairs correspondingly. 5The data set was kindly provided to us by Mengqiu Wang and is available for download at http://www.cs.stanford.edu/˜mengqiu/data/qg-emnlp07- data.tgz. 242 Method PLMTL utilizes WordNet and the Catvar (Categorial Variation) derivations database (Habash and Dorr, 2003) as generic and publicly available lexical knowledge resources, when question and answer terms are restricted to the first WordNet sense. In order to be consistent with (Shnarch et al., 2011b), the best performing model of prior work, we restricted our model to utilize only these two resources which they used. However, additional lexical resources can be provided as input to our model (e.g. a distributional similarity-base thesaurus). We report Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), the standard measures for ranked lists. In the cases of tie we took a conservative approac</context>
</contexts>
<marker>Habash, Dorr, 2003</marker>
<rawString>Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for english. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1385" citStr="Haghighi et al., 2005" startWordPosition="202" endWordPosition="205">inferences and show substantially improved results on a recently investigated question answering data set. 1 Introduction The task of identifying texts which share semantic content arises as a general need in many natural language processing applications. For instance, a paraphrasing application has to recognize texts which convey roughly the same content, and a summarization application needs to single out texts which contain the content stated by other texts. We refer to this general task as textual inference similar to prior use of this term (Raina et al., 2005; Schoenmackers et al., 2008; Haghighi et al., 2005). In many textual inference scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research wa</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2053" citStr="Heilman and Smith, 2010" startWordPosition="315" endWordPosition="318">ting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1. In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of </context>
<context position="7091" citStr="Heilman and Smith (2010)" startWordPosition="1082" endWordPosition="1085"> (2007) propose a quasi-synchronous grammar formulation which specifies the generation of the question parse tree, loosely conditioned on the parse tree of the candidate answer passage. Their model showed improvement over previous syntactic models for QA: Punyakanok et al. (2004), who computed similarity between question-answer pairs with a generalized tree-edit distance, and Cui et al. (2005), who developed an information measure for sentence similarity based on dependency paths of aligned words. Wang et al. (2007) reproduced these methods and extended them to utilize WordNet. More recently, Heilman and Smith (2010) improved Wang et al. (2007) results with a classification based approach. Feature for the classifier were extracted from a greedy algorithm which searches for tree-edit sequences which transform the parse tree of the candidate answer into the one of the question. Unlike other works reviewed here, this one does not utilize lexical knowledge resources. Similarly, Wang and Manning (2010) present an extended tree-edit operations set and search for edit sequences to generate the question from the answer candidate. Their CRF-based classifier models these sequences as latent variables. An important </context>
<context position="26328" citStr="Heilman and Smith (2010)" startWordPosition="4298" endWordPosition="4301">of choosing the Markovian process for the sentencelevel decision. By avoiding heuristically-normalized formulae and having all our parameters being part of the Markovian model, we managed to increases both MAP and MRR by nearly 2.5%6. Ablation Test As an additional examination of the impact of the Markovian process components, we evaluated the contribution of having 4 transition parameters. The AND-logic applied by (Shnarch et 6The difference is not significant according to the Wilcoxon test, however we note that given the data set size it is hard to get a significant difference and that both Heilman and Smith (2010) and Wang and Manning (2010) improvements over the results of Wang et al. (2007) were not statistically significant. System MAP MRR Punyakanok et al. 41.89 49.39 Cui et al. 43.50 55.69 Wang &amp; Manning 59.51 69.51 Wang et al. 60.29 68.52 Heilman &amp; Smith 60.91 69.17 Shnarch et al. HN-PLM 61.89 70.24 M-PLM 64.38 72.69 Table 1: Results (in %) for the task of answer ranking for question answering (sorted by MAP). al., 2011a) to their sentence-level decision roughly corresponds to 2 of the Markovian parameters. A binary AND outputs 1 if both its inputs are 1. This corresponds to q11(1) which is indee</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Houping Jia</author>
<author>Xiaojiang Huang</author>
<author>Tengfei Ma</author>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>RTE and summarization track.</title>
<date>2010</date>
<booktitle>PKUTM participation at the Text Analysis Conference</booktitle>
<contexts>
<context position="9166" citStr="Jia et al., 2010" startWordPosition="1408" endWordPosition="1411">presentative methods for that task. We adopt textual entailment terminology and henceforth use Hypothesis (denoted H) for the inferred text fragment and Text (denoted T) for the text from which it is being inferred4. Majumdar and Bhattacharyya (2010) utilized a simple union of lexical rules derived from various lexical resources for the term-level step. They derived their sentence-level decision based on the number of matched hypothesis terms. The results of this simple model were only slightly worse than the best results of the RTE-6 challenge which were achieved by a syntactic-based system (Jia et al., 2010). Clark and Harrison (2010), on the other hand, considered the number of mismatched terms in establishing their sentence-level decision. MacKinlay and Baldwin (2009) represented text and hypothesis as word vectors augmented with lexical knowledge. For sentence-level similarity they used a variant of the cosine similarity score. Common to most of these lexical models is the application of heuristic methods in both the term and the sentence level steps. Targeted to replace heuristic methods with principled ones, Shnarch et al. (2011a) present a model which aims at combining the advantages of a p</context>
</contexts>
<marker>Jia, Huang, Ma, Wan, Xiao, 2010</marker>
<rawString>Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun Wan, and Jianguo Xiao. 2010. PKUTM participation at the Text Analysis Conference 2010 RTE and summarization track. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="4045" citStr="Lin, 1998" startWordPosition="630" endWordPosition="631">, 2010). Typi1http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources. Some commonly used resources are WordNet (Fellbaum, 1998), distributional-similarity thesauri (Lin, 1998), and web knowledge resources such as (Suchanek et al., 2007). The second step is making a final sentence-level decision based on these estimations for the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principl</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew MacKinlay</author>
<author>Timothy Baldwin</author>
</authors>
<title>A baseline approach to the RTE5 search pilot.</title>
<date>2009</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<contexts>
<context position="3415" citStr="MacKinlay and Baldwin, 2009" startWordPosition="543" endWordPosition="546">ir likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information. Two main approaches have been used to address textual inference (for either ranking or classification). One is based on transformations over syntactic parse trees (Echihabi and Marcu, 2003; Heilman and Smith, 2010). Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (Wang et al., 2007; Wang and Manning, 2010). In the second approach, lexical models have been employed for textual inference (MacKinlay and Baldwin, 2009; Clark and Harrison, 2010). Typi1http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources. Some commonly used resources are WordNet (Fellbaum, 1998), distributional-s</context>
<context position="9331" citStr="MacKinlay and Baldwin (2009)" startWordPosition="1432" endWordPosition="1435">ext (denoted T) for the text from which it is being inferred4. Majumdar and Bhattacharyya (2010) utilized a simple union of lexical rules derived from various lexical resources for the term-level step. They derived their sentence-level decision based on the number of matched hypothesis terms. The results of this simple model were only slightly worse than the best results of the RTE-6 challenge which were achieved by a syntactic-based system (Jia et al., 2010). Clark and Harrison (2010), on the other hand, considered the number of mismatched terms in establishing their sentence-level decision. MacKinlay and Baldwin (2009) represented text and hypothesis as word vectors augmented with lexical knowledge. For sentence-level similarity they used a variant of the cosine similarity score. Common to most of these lexical models is the application of heuristic methods in both the term and the sentence level steps. Targeted to replace heuristic methods with principled ones, Shnarch et al. (2011a) present a model which aims at combining the advantages of a probabilistic generative model with the simplicity of lexical methods. In some analogy to generative parsetree based models, they propose a generative process for the</context>
</contexts>
<marker>MacKinlay, Baldwin, 2009</marker>
<rawString>Andrew MacKinlay and Timothy Baldwin. 2009. A baseline approach to the RTE5 search pilot. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Debarghya Majumdar</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Lexical based text entailment system for main task of RTE6.</title>
<date>2010</date>
<booktitle>In Proceedings of the Text Analysis Conference.</booktitle>
<contexts>
<context position="4436" citStr="Majumdar and Bhattacharyya (2010)" startWordPosition="690" endWordPosition="694">el estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources. Some commonly used resources are WordNet (Fellbaum, 1998), distributional-similarity thesauri (Lin, 1998), and web knowledge resources such as (Suchanek et al., 2007). The second step is making a final sentence-level decision based on these estimations for the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the approaches for syntactic-based inference. An attractive work in this spirit is presented in (Shnarch et al., 2011a), that propose a model which is both lexical and probabilistic. Later, Shnarch et al. (2011b) improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art RTE</context>
<context position="8799" citStr="Majumdar and Bhattacharyya (2010)" startWordPosition="1349" endWordPosition="1352">to implement and are more practical for various genres and languages. Such models derive from knowledge resources lexical inference rules which indicate that the meaning of a lexical term can be inferred from the meaning of another term (e.g. youngsters —* children and harmful —* dangerous). They are common in the Recognizing Textual Entailment (RTE) systems and we present some representative methods for that task. We adopt textual entailment terminology and henceforth use Hypothesis (denoted H) for the inferred text fragment and Text (denoted T) for the text from which it is being inferred4. Majumdar and Bhattacharyya (2010) utilized a simple union of lexical rules derived from various lexical resources for the term-level step. They derived their sentence-level decision based on the number of matched hypothesis terms. The results of this simple model were only slightly worse than the best results of the RTE-6 challenge which were achieved by a syntactic-based system (Jia et al., 2010). Clark and Harrison (2010), on the other hand, considered the number of mismatched terms in establishing their sentence-level decision. MacKinlay and Baldwin (2009) represented text and hypothesis as word vectors augmented with lexi</context>
</contexts>
<marker>Majumdar, Bhattacharyya, 2010</marker>
<rawString>Debarghya Majumdar and Pushpak Bhattacharyya. 2010. Lexical based text entailment system for main task of RTE6. In Proceedings of the Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic reasoning in intelligent systems: networks ofplausible inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11150" citStr="Pearl, 1988" startWordPosition="1746" endWordPosition="1747"> PLMT L, and have reproduced it in our work as presented in Section 4.1. For the sentence-level decision they describe an AND gate mechanism, i.e. deducing a positive inference decision for H as a whole only if all its terms were inferred from T. In an extension to that work, Shnarch et al. (2011b) modified PLMTL to improve the sentencelevel step. They pointed out some prominent aspects for the sentence-level decision. First, they suggest that a hypothesis as a whole can be inferred from the text even if some of its terms are not inferred. To model this, they introduced a noisy-AND mechanism (Pearl, 1988). Additionally, they emphasized the effect of hypothesis length and the dependency between terms on the sentence-level decision. However, they did not fully achieve their target of presenting a fully coherent probabilistic model, as their model included heuristic normalization formulae. On the contrary, the model we present is the first along this line to be fully specified in terms of a generative setting and formulated in pure probabilistic terms. We introduce a Markovian-style probabilistic model for the sentence-level decision. This model receives as input term-level probabilistic estimate</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic reasoning in intelligent systems: networks ofplausible inference. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Mapping dependencies trees: An application to question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Symposium on Artificial Intelligence and Mathematics.</booktitle>
<contexts>
<context position="6747" citStr="Punyakanok et al. (2004)" startWordPosition="1032" endWordPosition="1035">nd Wang et al. (2007) provided an annotated data set, based on the Text REtrieval Conference (TREC) QA tracks3, specifically for the task of ranking candidate answer passages. We adopt their experimental setup and next review the line of syntactic-based works which reported results on this data set. 2.1 Syntactic generative models Wang et al. (2007) propose a quasi-synchronous grammar formulation which specifies the generation of the question parse tree, loosely conditioned on the parse tree of the candidate answer passage. Their model showed improvement over previous syntactic models for QA: Punyakanok et al. (2004), who computed similarity between question-answer pairs with a generalized tree-edit distance, and Cui et al. (2005), who developed an information measure for sentence similarity based on dependency paths of aligned words. Wang et al. (2007) reproduced these methods and extended them to utilize WordNet. More recently, Heilman and Smith (2010) improved Wang et al. (2007) results with a classification based approach. Feature for the classifier were extracted from a greedy algorithm which searches for tree-edit sequences which transform the parse tree of the candidate answer into the one of the q</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004. Mapping dependencies trees: An application to question answering. In Proceedings of the International Symposium on Artificial Intelligence and Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1333" citStr="Raina et al., 2005" startWordPosition="194" endWordPosition="197">us our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set. 1 Introduction The task of identifying texts which share semantic content arises as a general need in many natural language processing applications. For instance, a paraphrasing application has to recognize texts which convey roughly the same content, and a summarization application needs to single out texts which contain the content stated by other texts. We refer to this general task as textual inference similar to prior use of this term (Raina et al., 2005; Schoenmackers et al., 2008; Haghighi et al., 2005). In many textual inference scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic comp</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel Weld</author>
</authors>
<title>Scaling textual inference to the web.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1361" citStr="Schoenmackers et al., 2008" startWordPosition="198" endWordPosition="201">the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set. 1 Introduction The task of identifying texts which share semantic content arises as a general need in many natural language processing applications. For instance, a paraphrasing application has to recognize texts which convey roughly the same content, and a summarization application needs to single out texts which contain the content stated by other texts. We refer to this general task as textual inference similar to prior use of this term (Raina et al., 2005; Schoenmackers et al., 2008; Haghighi et al., 2005). In many textual inference scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A re</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Daniel Weld. 2008. Scaling textual inference to the web. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Jacob Goldberger</author>
<author>Ido Dagan</author>
</authors>
<title>A probabilistic modeling framework for lexical entailment.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4816" citStr="Shnarch et al., 2011" startWordPosition="749" endWordPosition="752"> the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the approaches for syntactic-based inference. An attractive work in this spirit is presented in (Shnarch et al., 2011a), that propose a model which is both lexical and probabilistic. Later, Shnarch et al. (2011b) improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art RTE models. Whereas their term-level model provides means to integrate lexical knowledge in a probabilistic manner, their sentence-level model depends to a great extent on heuristic normalizations which were introduced to incorporate prominent aspects of the sentence-level decision. This deviates their model from a pure probabilistic methodology. Our work aims at amending this def</context>
<context position="9702" citStr="Shnarch et al. (2011" startWordPosition="1494" endWordPosition="1497">e RTE-6 challenge which were achieved by a syntactic-based system (Jia et al., 2010). Clark and Harrison (2010), on the other hand, considered the number of mismatched terms in establishing their sentence-level decision. MacKinlay and Baldwin (2009) represented text and hypothesis as word vectors augmented with lexical knowledge. For sentence-level similarity they used a variant of the cosine similarity score. Common to most of these lexical models is the application of heuristic methods in both the term and the sentence level steps. Targeted to replace heuristic methods with principled ones, Shnarch et al. (2011a) present a model which aims at combining the advantages of a probabilistic generative model with the simplicity of lexical methods. In some analogy to generative parsetree based models, they propose a generative process for the creation of the hypothesis from the text. At the term-level, their model combines knowledge from various input resources and has the advantages of considering the effect of transitive rule application (e.g. mobile phone —* cell phone —* cellular) as well as the integration of multiple pieces 4In the task of passage ranking for QA, the hypothesis is the question and th</context>
<context position="19885" citStr="Shnarch et al. (2011" startWordPosition="3213" endWordPosition="3216">j(1) ← (7) H|) P(T,H) Pt=2 Pk∈{0,1} wtijk(T, H qij(0) = 1−qij(1) 4 Complete model implementation We next describe the end-to-end probabilistic lexical inference model we used in our evaluations. We implemented PLMT L as our term-level model to provide us with ht(j), the term-level probabilities. We chose this model since it is fully lexical, has the advantages of lexical knowledge integration described in Section 2 and achieved top results on RTE data sets. Next, we summarize PLMTL, and in Appendix A we show how to adjust the learning schema to fit into our sentence-level model. 241 4.1 PLMTL Shnarch et al. (2011a) provide a term-level model which integrates lexical rules from various knowledge resources. As described below it also considers transitive chains of rule applications as well as the impact of parallel chains which provide multiple evidence that hEH is inferred from T. Their model assumes a parameter OR for each knowledge resource R in use. OR specifies the resource’s reliability, i.e. the prior probability that applying a rule from R to an arbitrary text-hypothesis pair would yield a valid inference. Next, transitive chains may connect a text term to a hypothesis term via intermediate term</context>
<context position="24389" citStr="Shnarch et al., 2011" startWordPosition="3988" endWordPosition="3991"> positive if the candidate passage indicates the correct answer for the question. The training and test sets roughly contain 5700 and 1500 pairs correspondingly. 5The data set was kindly provided to us by Mengqiu Wang and is available for download at http://www.cs.stanford.edu/˜mengqiu/data/qg-emnlp07- data.tgz. 242 Method PLMTL utilizes WordNet and the Catvar (Categorial Variation) derivations database (Habash and Dorr, 2003) as generic and publicly available lexical knowledge resources, when question and answer terms are restricted to the first WordNet sense. In order to be consistent with (Shnarch et al., 2011b), the best performing model of prior work, we restricted our model to utilize only these two resources which they used. However, additional lexical resources can be provided as input to our model (e.g. a distributional similarity-base thesaurus). We report Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), the standard measures for ranked lists. In the cases of tie we took a conservative approach and ranked positive annotated instances below the negative instances scored with the same probability. Hence, the reported figures are lower-bounds for any tie-breaking method that could h</context>
<context position="27177" citStr="Shnarch et al., 2011" startWordPosition="4449" endWordPosition="4452"> 68.52 Heilman &amp; Smith 60.91 69.17 Shnarch et al. HN-PLM 61.89 70.24 M-PLM 64.38 72.69 Table 1: Results (in %) for the task of answer ranking for question answering (sorted by MAP). al., 2011a) to their sentence-level decision roughly corresponds to 2 of the Markovian parameters. A binary AND outputs 1 if both its inputs are 1. This corresponds to q11(1) which is indeed estimate to be near 1. In any other case an AND gate outputs 0. This corresponds to q00(1) which was estimated to be near zero. The two parameters q01 and q10 are novel to the Markovian process and do not have counterparts in (Shnarch et al., 2011a). These parameters are the cases in which the sentence-level decision accumulated so far and the term-level decision do not agree. Introducing these 2 parameters enables our model to provide a positive decision for the hypothesis as a whole (or for a part of it) even if some of its terms were not inferred. We performed an ablation test on each of these two parameters by forcing the value of the ablated parameter to be zero. The notable performance drop presented in Table 2 indicates the crucial contribution of these parameters to our model. Ablated parameter A MAP A MRR q01(1) = 0 -2.61 -4.9</context>
<context position="30740" citStr="Shnarch et al., 2011" startWordPosition="5043" endWordPosition="5046">ore there are no state-of-the-art results to compare with, when utilizing them for the ranking task. the estimated probability well across hypothesis. We therefore suggest a future work on better classification models. Finally, we view this work as joining a line of research which develops principled probabilistic models for the task of textual inference and demonstrates their superiority over heuristic methods. A Appendix: Adaptation of PLMTL learning M-PLM embeds PLMTL as its term-level model. PLMTL introduces BR values as additional parameters for the complete model. We show how we modify (Shnarch et al., 2011a) E-step formula to fit our Markovian modeling, described in Section 3.1. The M-step formula remains exactly the same. Eq. (11) estimates the a-posteriori validity probability of a single application of the rule r in the transitive chain c pointing at ht, given that the annotation of the pair is a. wtcr(T, H) = P(xtcr = 1|yn = a) = P(yn = a) where t=2 ... n and P(xt =j|xtcr =1) is the probability that the inference value of xt is j, given that the application of r provides a valid inference step. As appeared in (Shnarch et al., 2011b) this probability can be evaluated as follows: � P(xt = 0) </context>
</contexts>
<marker>Shnarch, Goldberger, Dagan, 2011</marker>
<rawString>Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011a. A probabilistic modeling framework for lexical entailment. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Jacob Goldberger</author>
<author>Ido Dagan</author>
</authors>
<title>Towards a probabilistic model for lexical entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of the TextInfer Workshop on Textual Entailment.</booktitle>
<contexts>
<context position="4816" citStr="Shnarch et al., 2011" startWordPosition="749" endWordPosition="752"> the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the approaches for syntactic-based inference. An attractive work in this spirit is presented in (Shnarch et al., 2011a), that propose a model which is both lexical and probabilistic. Later, Shnarch et al. (2011b) improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art RTE models. Whereas their term-level model provides means to integrate lexical knowledge in a probabilistic manner, their sentence-level model depends to a great extent on heuristic normalizations which were introduced to incorporate prominent aspects of the sentence-level decision. This deviates their model from a pure probabilistic methodology. Our work aims at amending this def</context>
<context position="9702" citStr="Shnarch et al. (2011" startWordPosition="1494" endWordPosition="1497">e RTE-6 challenge which were achieved by a syntactic-based system (Jia et al., 2010). Clark and Harrison (2010), on the other hand, considered the number of mismatched terms in establishing their sentence-level decision. MacKinlay and Baldwin (2009) represented text and hypothesis as word vectors augmented with lexical knowledge. For sentence-level similarity they used a variant of the cosine similarity score. Common to most of these lexical models is the application of heuristic methods in both the term and the sentence level steps. Targeted to replace heuristic methods with principled ones, Shnarch et al. (2011a) present a model which aims at combining the advantages of a probabilistic generative model with the simplicity of lexical methods. In some analogy to generative parsetree based models, they propose a generative process for the creation of the hypothesis from the text. At the term-level, their model combines knowledge from various input resources and has the advantages of considering the effect of transitive rule application (e.g. mobile phone —* cell phone —* cellular) as well as the integration of multiple pieces 4In the task of passage ranking for QA, the hypothesis is the question and th</context>
<context position="19885" citStr="Shnarch et al. (2011" startWordPosition="3213" endWordPosition="3216">j(1) ← (7) H|) P(T,H) Pt=2 Pk∈{0,1} wtijk(T, H qij(0) = 1−qij(1) 4 Complete model implementation We next describe the end-to-end probabilistic lexical inference model we used in our evaluations. We implemented PLMT L as our term-level model to provide us with ht(j), the term-level probabilities. We chose this model since it is fully lexical, has the advantages of lexical knowledge integration described in Section 2 and achieved top results on RTE data sets. Next, we summarize PLMTL, and in Appendix A we show how to adjust the learning schema to fit into our sentence-level model. 241 4.1 PLMTL Shnarch et al. (2011a) provide a term-level model which integrates lexical rules from various knowledge resources. As described below it also considers transitive chains of rule applications as well as the impact of parallel chains which provide multiple evidence that hEH is inferred from T. Their model assumes a parameter OR for each knowledge resource R in use. OR specifies the resource’s reliability, i.e. the prior probability that applying a rule from R to an arbitrary text-hypothesis pair would yield a valid inference. Next, transitive chains may connect a text term to a hypothesis term via intermediate term</context>
<context position="24389" citStr="Shnarch et al., 2011" startWordPosition="3988" endWordPosition="3991"> positive if the candidate passage indicates the correct answer for the question. The training and test sets roughly contain 5700 and 1500 pairs correspondingly. 5The data set was kindly provided to us by Mengqiu Wang and is available for download at http://www.cs.stanford.edu/˜mengqiu/data/qg-emnlp07- data.tgz. 242 Method PLMTL utilizes WordNet and the Catvar (Categorial Variation) derivations database (Habash and Dorr, 2003) as generic and publicly available lexical knowledge resources, when question and answer terms are restricted to the first WordNet sense. In order to be consistent with (Shnarch et al., 2011b), the best performing model of prior work, we restricted our model to utilize only these two resources which they used. However, additional lexical resources can be provided as input to our model (e.g. a distributional similarity-base thesaurus). We report Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), the standard measures for ranked lists. In the cases of tie we took a conservative approach and ranked positive annotated instances below the negative instances scored with the same probability. Hence, the reported figures are lower-bounds for any tie-breaking method that could h</context>
<context position="27177" citStr="Shnarch et al., 2011" startWordPosition="4449" endWordPosition="4452"> 68.52 Heilman &amp; Smith 60.91 69.17 Shnarch et al. HN-PLM 61.89 70.24 M-PLM 64.38 72.69 Table 1: Results (in %) for the task of answer ranking for question answering (sorted by MAP). al., 2011a) to their sentence-level decision roughly corresponds to 2 of the Markovian parameters. A binary AND outputs 1 if both its inputs are 1. This corresponds to q11(1) which is indeed estimate to be near 1. In any other case an AND gate outputs 0. This corresponds to q00(1) which was estimated to be near zero. The two parameters q01 and q10 are novel to the Markovian process and do not have counterparts in (Shnarch et al., 2011a). These parameters are the cases in which the sentence-level decision accumulated so far and the term-level decision do not agree. Introducing these 2 parameters enables our model to provide a positive decision for the hypothesis as a whole (or for a part of it) even if some of its terms were not inferred. We performed an ablation test on each of these two parameters by forcing the value of the ablated parameter to be zero. The notable performance drop presented in Table 2 indicates the crucial contribution of these parameters to our model. Ablated parameter A MAP A MRR q01(1) = 0 -2.61 -4.9</context>
<context position="30740" citStr="Shnarch et al., 2011" startWordPosition="5043" endWordPosition="5046">ore there are no state-of-the-art results to compare with, when utilizing them for the ranking task. the estimated probability well across hypothesis. We therefore suggest a future work on better classification models. Finally, we view this work as joining a line of research which develops principled probabilistic models for the task of textual inference and demonstrates their superiority over heuristic methods. A Appendix: Adaptation of PLMTL learning M-PLM embeds PLMTL as its term-level model. PLMTL introduces BR values as additional parameters for the complete model. We show how we modify (Shnarch et al., 2011a) E-step formula to fit our Markovian modeling, described in Section 3.1. The M-step formula remains exactly the same. Eq. (11) estimates the a-posteriori validity probability of a single application of the rule r in the transitive chain c pointing at ht, given that the annotation of the pair is a. wtcr(T, H) = P(xtcr = 1|yn = a) = P(yn = a) where t=2 ... n and P(xt =j|xtcr =1) is the probability that the inference value of xt is j, given that the application of r provides a valid inference step. As appeared in (Shnarch et al., 2011b) this probability can be evaluated as follows: � P(xt = 0) </context>
</contexts>
<marker>Shnarch, Goldberger, Dagan, 2011</marker>
<rawString>Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011b. Towards a probabilistic model for lexical entailment. In Proceedings of the TextInfer Workshop on Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A Core of Semantic Knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="4106" citStr="Suchanek et al., 2007" startWordPosition="638" endWordPosition="641">dex.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources. Some commonly used resources are WordNet (Fellbaum, 1998), distributional-similarity thesauri (Lin, 1998), and web knowledge resources such as (Suchanek et al., 2007). The second step is making a final sentence-level decision based on these estimations for the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the app</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling.</booktitle>
<contexts>
<context position="2078" citStr="Wang and Manning, 2010" startWordPosition="319" endWordPosition="322">ation decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1. In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of this claim such as extens</context>
<context position="3305" citStr="Wang and Manning, 2010" startWordPosition="527" endWordPosition="530">usage of cell phones may be harmful for youngsters. This can be done by ranking the corpus passages by their likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information. Two main approaches have been used to address textual inference (for either ranking or classification). One is based on transformations over syntactic parse trees (Echihabi and Marcu, 2003; Heilman and Smith, 2010). Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (Wang et al., 2007; Wang and Manning, 2010). In the second approach, lexical models have been employed for textual inference (MacKinlay and Baldwin, 2009; Clark and Harrison, 2010). Typi1http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match a</context>
<context position="7479" citStr="Wang and Manning (2010)" startWordPosition="1144" endWordPosition="1147"> (2005), who developed an information measure for sentence similarity based on dependency paths of aligned words. Wang et al. (2007) reproduced these methods and extended them to utilize WordNet. More recently, Heilman and Smith (2010) improved Wang et al. (2007) results with a classification based approach. Feature for the classifier were extracted from a greedy algorithm which searches for tree-edit sequences which transform the parse tree of the candidate answer into the one of the question. Unlike other works reviewed here, this one does not utilize lexical knowledge resources. Similarly, Wang and Manning (2010) present an extended tree-edit operations set and search for edit sequences to generate the question from the answer candidate. Their CRF-based classifier models these sequences as latent variables. An important merit of these methods is that they offer principled, often probabilistic, generative models for the task of ranking candidate answers. Their drawback is the need for syntactic analysis which makes them slower to run, dependent on parsing performance, which is often mediocre in many text genres, and inadequate for languages which lack proper parsing tools. 3http://trec.nist.gov/data/qa</context>
<context position="26356" citStr="Wang and Manning (2010)" startWordPosition="4303" endWordPosition="4306">cess for the sentencelevel decision. By avoiding heuristically-normalized formulae and having all our parameters being part of the Markovian model, we managed to increases both MAP and MRR by nearly 2.5%6. Ablation Test As an additional examination of the impact of the Markovian process components, we evaluated the contribution of having 4 transition parameters. The AND-logic applied by (Shnarch et 6The difference is not significant according to the Wilcoxon test, however we note that given the data set size it is hard to get a significant difference and that both Heilman and Smith (2010) and Wang and Manning (2010) improvements over the results of Wang et al. (2007) were not statistically significant. System MAP MRR Punyakanok et al. 41.89 49.39 Cui et al. 43.50 55.69 Wang &amp; Manning 59.51 69.51 Wang et al. 60.29 68.52 Heilman &amp; Smith 60.91 69.17 Shnarch et al. HN-PLM 61.89 70.24 M-PLM 64.38 72.69 Table 1: Results (in %) for the task of answer ranking for question answering (sorted by MAP). al., 2011a) to their sentence-level decision roughly corresponds to 2 of the Markovian parameters. A binary AND outputs 1 if both its inputs are 1. This corresponds to q11(1) which is indeed estimate to be near 1. In </context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasisynchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2028" citStr="Wang et al., 2007" startWordPosition="311" endWordPosition="314">e scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1. In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look fo</context>
<context position="3280" citStr="Wang et al., 2007" startWordPosition="523" endWordPosition="526"> such as extensive usage of cell phones may be harmful for youngsters. This can be done by ranking the corpus passages by their likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information. Two main approaches have been used to address textual inference (for either ranking or classification). One is based on transformations over syntactic parse trees (Echihabi and Marcu, 2003; Heilman and Smith, 2010). Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (Wang et al., 2007; Wang and Manning, 2010). In the second approach, lexical models have been employed for textual inference (MacKinlay and Baldwin, 2009; Clark and Harrison, 2010). Typi1http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based </context>
<context position="6144" citStr="Wang et al. (2007)" startWordPosition="941" endWordPosition="944"> parameters are estimated by an EM algorithm. We evaluate this model on the tasks of ranking passages for QA and ranking textual entailments within a corpus, and show that eliminating the need for heuristic normalizations greatly improves state-of-the-art performance. The full implementation of our model is available for download2 and can be used as an easy-to-install and highly competitive inference en2http://www.cs.biu.ac.il/�nlp/downloads/probLexModel.html gine that operates only on lexical knowledge, or as a lexical component integrated within a more complex inference system. 2 Background Wang et al. (2007) provided an annotated data set, based on the Text REtrieval Conference (TREC) QA tracks3, specifically for the task of ranking candidate answer passages. We adopt their experimental setup and next review the line of syntactic-based works which reported results on this data set. 2.1 Syntactic generative models Wang et al. (2007) propose a quasi-synchronous grammar formulation which specifies the generation of the question parse tree, loosely conditioned on the parse tree of the candidate answer passage. Their model showed improvement over previous syntactic models for QA: Punyakanok et al. (20</context>
<context position="23422" citStr="Wang et al. (2007)" startWordPosition="3833" endWordPosition="3836"> probability to be inferred from each of the corpus passages, namely P(y,,, = 1) in Eq (3). Passages are then ranked according to this probability. 5 Evaluations and Results To evaluate the performance of M-PLM for ranking textual inferences we focused on the task of ranking candidate answer passages for question answering (QA) as presented in Section 5.1. Additionally, we demonstrate the added value of our sentence-level model in another ranking experiment based on RTE data sets, described in Section 5.2. 5.1 Answer ranking for question answering Data set We adopted the experimental setup of Wang et al. (2007) who also provided an annotated data set for answer passage ranking in QA5. In their data set an instance is a pair of a factoid question and a candidate answer passage (a single sentence in this data set). It was constructed from the data of the QA tracks at TREC 8–13. The questioncandidate pairs were manually judged and a pair was annotated as positive if the candidate passage indicates the correct answer for the question. The training and test sets roughly contain 5700 and 1500 pairs correspondingly. 5The data set was kindly provided to us by Mengqiu Wang and is available for download at ht</context>
<context position="26408" citStr="Wang et al. (2007)" startWordPosition="4312" endWordPosition="4315">cally-normalized formulae and having all our parameters being part of the Markovian model, we managed to increases both MAP and MRR by nearly 2.5%6. Ablation Test As an additional examination of the impact of the Markovian process components, we evaluated the contribution of having 4 transition parameters. The AND-logic applied by (Shnarch et 6The difference is not significant according to the Wilcoxon test, however we note that given the data set size it is hard to get a significant difference and that both Heilman and Smith (2010) and Wang and Manning (2010) improvements over the results of Wang et al. (2007) were not statistically significant. System MAP MRR Punyakanok et al. 41.89 49.39 Cui et al. 43.50 55.69 Wang &amp; Manning 59.51 69.51 Wang et al. 60.29 68.52 Heilman &amp; Smith 60.91 69.17 Shnarch et al. HN-PLM 61.89 70.24 M-PLM 64.38 72.69 Table 1: Results (in %) for the task of answer ranking for question answering (sorted by MAP). al., 2011a) to their sentence-level decision roughly corresponds to 2 of the Markovian parameters. A binary AND outputs 1 if both its inputs are 1. This corresponds to q11(1) which is indeed estimate to be near 1. In any other case an AND gate outputs 0. This correspon</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? a quasisynchronous grammar for QA. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>