<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.981404">
Sentiment Retrieval using Generative Models
</title>
<author confidence="0.982485">
Koji Eguchi
</author>
<affiliation confidence="0.7376035">
National Institute of Informatics
Tokyo 101-8430, Japan
</affiliation>
<email confidence="0.99791">
eguchi@nii.ac.jp
</email>
<author confidence="0.997407">
Victor Lavrenko
</author>
<affiliation confidence="0.893603333333333">
Department of Computer Science
University of Massachusetts
Amherst, MA 01003, USA
</affiliation>
<email confidence="0.998429">
lavrenko@cs.umass.edu
</email>
<sectionHeader confidence="0.993867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999601388888889">
Ranking documents or sentences accord-
ing to both topic and sentiment relevance
should serve a critical function in helping
users when topics and sentiment polari-
ties of the targeted text are not explicitly
given, as is often the case on the web. In
this paper, we propose several sentiment
information retrieval models in the frame-
work of probabilistic language models, as-
suming that a user both inputs query terms
expressing a certain topic and also speci-
fies a sentiment polarity of interest in some
manner. We combine sentiment relevance
models and topic relevance models with
model parameters estimated from training
data, considering the topic dependence of
the sentiment. Our experiments prove that
our models are effective.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990127272727">
The recent rapid expansion of access to informa-
tion has significantly increased the demands on re-
trieval or classification of sentiment information
from a large amount of textual data. The field of
sentiment classification has recently received con-
siderable attention, where the polarities of senti-
ment, such as positive or negative, were identified
from unstructured text (Shanahan et al., 2005).
A number of studies have investigated sentiment
classification at document level, e.g., (Pang et al.,
2002; Dave et al., 2003), and at sentence level,
e.g., (Hu and Liu, 2004; Kim and Hovy, 2004;
Nigam and Hurst, 2005); however, the accuracy
is still less than desirable. Therefore, ranking ac-
cording to the likelihood of containing sentiment
information is expected to serve a crucial func-
tion in helping users. We believe that our work
is the first attempt at sentiment retrieval that aims
at finding sentences containing information with a
specific sentiment polarity on a certain topic.
Intuitively, the expression of sentiment in text
is dependent on the topic. For example, a nega-
tive view for some voting event may be expressed
using ‘flaw’, while a negative view for some politi-
cian may be expressed using ‘reckless’. Moreover,
sentiment polarities are also dependent on topics
or domains. For example, the adjective ‘unpre-
dictable’ may have a negative orientation in an au-
tomotive review, in a phrase such as ‘unpredictable
steering’, but it could have a positive orientation in
a movie review, in a phrase such as ‘unpredictable
plot’, as mentioned in (Turney, 2002) in the con-
text of his sentiment word detection.
We propose sentiment retrieval models in the
framework of generative language modeling, not
only assuming query terms expressing a certain
topic, but also assuming that the polarity of sen-
timent interest is specified by the user in some
manner, where the topic dependence of the sen-
timent is considered. To the best of our knowl-
edge, there have been no other studies on a re-
trieval model unifying both topic and sentiment,
and further, there have been no other studies on
sentiment retrieval. The sentiment information of-
ten appears as local in a document, and therefore
focusing on finer levels, i.e., sentence or passage
levels rather than document level, is crucial. We
thus experiment on sentiment retrieval at the sen-
tence level in this paper.
The rest of this paper is structured as follows.
Section 2 introduces the work related to this study.
Section 3 describes a generative model of sen-
timent, which is proposed here as a theoretical
framework for our work. Section 4 describes the
task definition and our sentiment retrieval model.
</bodyText>
<page confidence="0.984971">
345
</page>
<note confidence="0.8468515">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 345–354,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998341666666667">
Section 5 explains the data we used for our experi-
ments, and gives our experimental results. Section
6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999292" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999950435897436">
Some efforts for the TREC Novelty Track were
related to our work. Although some of the topics
used in the Novelty Track in 2003 and 2004 (Sobo-
roff and Harman, 2003; Soboroff, 2004) were re-
lated to opinions, most of the efforts were fo-
cused on topic, such as studies using term dis-
tribution within each sentence, e.g., (Allan et al.,
2003; Losada, 2005; Murdock and Croft, 2005).
Amongst the participants in the TREC Novelty
Track, only (Kim et al., 2004) proposed a method
specialized to opinion-bearing sentence retrieval,
by making use of lists of words with positive or
negative polarities. They aimed to find opinions
on a given topic but did not distinguish or did not
care about sentiment polarities that should be rep-
resented in some sentences (hereafter, opinion re-
trieval). We focus on finding positive views or
negative views according to a given topic and sen-
timent of interest (hereafter, sentiment retrieval).
Our work is the first work on sentiment retrieval,
to the best of our knowledge.
In the context of sentiment classification, some
researchers have conducted studies on the topic
dependence of sentiment polarities. (Nasukawa
and Yi, 2003) and (Yi et al., 2003) extracted pos-
itive or negative expressions on a given product
name using handmade lexicons. (Engstr¨om, 2004)
studied how the topic dependence influences the
accuracy of sentiment classification and attempted
to reduce the influence to improve the accuracy.
(Wilson et al., 2005) investigated how context in-
fluences sentiment polarity at the phrase level in a
corpus, beginning with a predefined list of words
with polarities. Their focus on the phenomena of
topic dependence of sentiment can be shared with
our work; however, their work is not directly re-
lated to ours, because we focus on a different task,
sentiment retrieval, where different approaches are
required.
</bodyText>
<sectionHeader confidence="0.992332" genericHeader="method">
3 A Generative Model of Sentiment
</sectionHeader>
<bodyText confidence="0.99996648">
In this section we will provide a formal underpin-
ning for our approach to sentiment retrieval. The
approach is based on the generative paradigm: we
describe a statistical process that could be viewed,
hypothetically, as a source of every statement of
interest to our system. We stress that this genera-
tive process is to be treated as purely hypothetical;
the process is only intended to reflect those aspects
of human discourse that are pertinent to the prob-
lem of retrieving affectively appropriate and topic-
relevant texts in response to a query posed by our
user.
Before giving a formal specification of our
model, we will provide a high-level overview of
the main ideas. We are trying to model a col-
lection of natural-language statements, some of
which are relevant to a user’s query. In our ex-
periments, these statements are individual sen-
tences, but the model can be applied to textual
chunks of any length. We assume that the con-
tent of an individual statement can be modeled
independently of all other statements in the col-
lection. Each statement consists of some topic-
bearing and some sentiment-bearing words. We
assume that the topic-bearing words represent ex-
changeable samples from some underlying topic
language model. Exchangeability means that the
relative order of the words is irrelevant, but the
words are not independent of each other—the idea
often stated as a bag-of-words assumption. Sim-
ilarly, sentiment-bearing words are viewed as an
order-invariant ‘bag’, sampled from the underly-
ing sentiment language model. We will explicitly
model dependency between the topic and senti-
ment language models, and will demonstrate that
treating them independently leads to sub-optimal
retrieval performance. When a sentiment polarity
value is observed for a given statement, we will
treat it as a ternary variable influencing the topic
and sentiment language models.
We represent a user’s query as just another state-
ment, consisting of topic and sentiment parts, sub-
ject to all the independence assumptions stated
above. We will use the query to estimate the topic
and sentiment language models that are represen-
tative of the user’s interests. Following (Lavrenko
and Croft, 2001), we will use the term relevance
models to describe these models, and will use them
to rank statements in order of their relevance to the
query.
</bodyText>
<subsectionHeader confidence="0.994089">
3.1 Definitions
</subsectionHeader>
<bodyText confidence="0.99904375">
We start by providing a set of definitions that will
be used in the remainder of this section. The task
of our model is to generate a collection of state-
mentsw1...w,,. A statementwiis a string of
</bodyText>
<page confidence="0.998405">
346
</page>
<bodyText confidence="0.999962290322581">
wordswi1:::wini, drawn from a common vocabu-
laryV. We introduce a binary variablebij2fS;Tg
as an indicator of whether the word in thejth po-
sition of the ith statement will be a topic word or
a sentiment word. For our purposes, bij is either
provided by a human annotator (manual annota-
tion), or determined heuristically (automatic an-
notation).
The sentiment polarityxifor a given statement
is a discrete random variable with three outcomes:
f —1; 0; +1g, representing negative, neutral and
positive polarity values, respectively. As a matter
of convenience we will often denote a statement as
a triple fwsi; wti; xig, where wsicontains the sen-
timent words and wticontains the topic words. As
we mentioned above, the user’s query is treated
as just another statement. It will be denoted as
a triple fqs qt; qxg, corresponding to sentiment
words, topic keywords, and the desired polarity
value. We will use p to denote a unigram lan-
guage model, i.e., a function that assigns a number
p(v)2[0;1℄to every wordvin our vocabularyV,
such that�vp(v)=1. The set of all possible un-
igram language models is the probability simplex
IP. Similarly, px will denote a distribution over
the three possible polarity values, and IPx is the
corresponding ternary probability simplex. We de-
fine 7r : IP x IP x IPx ![0;1℄ to be a measure func-
tion that assigns a probability 7(p1; p2; px) to a
pair of language models p1 and p2 together with a
polarity model px .
</bodyText>
<subsectionHeader confidence="0.973589">
3.2 Generative model
</subsectionHeader>
<bodyText confidence="0.9998164">
Using the definitions presented above, and assum-
ing that7()is given, we hypothesize that a new
statementwicontaining wordswi1:::wimwith
sentiment polarityxican be generated according
to the following mechanism.
</bodyText>
<listItem confidence="0.999744">
2. Samplexifrom a polarity distributionpx(�).
1. Drawpt;psandpxfrom�(�;�;�).
3. For each position j = 1::: m:
</listItem>
<equation confidence="0.92264">
a) if bi j =T : draw wi j from pt
(b) if bi j = S: draw wi j from ps
</equation>
<bodyText confidence="0.90949">
The probability of observing the new statement
</bodyText>
<equation confidence="0.9412145">
wi1 :::wim under this mechanism is given by:
m
pt(wij) ifbij=T
ps(wij) otherwise
j=1
(1)
</equation>
<bodyText confidence="0.995944571428572">
The summation in equation (1) goes over all pos-
sible pairs of language modelspt;ps, but we can
avoid integration by specifying a mass function
7() that assigns nonzero probabilities to a finite
subset of points in IP x IP x IPx. We accomplish
this by using a nonparametric estimate for 7r (), the
details of which are provided below.
</bodyText>
<subsectionHeader confidence="0.687569">
3.2.1 A nonparametric generative mass
function
</subsectionHeader>
<bodyText confidence="0.907539804878049">
We use a nonparametric estimate for7r(•;�;�),
which makes our generative model similar to
kernel-based density estimators orParzen-window
classifiers (Silverman, 1986). The primary dif-
ference is that our model operates over discrete
events (strings of words), and accordingly the
mass function is defined over the space of distribu-
tions, rather than directly over the data points. Our
estimate relies on a collection of paired observa-
tionsC=fwti;wsi;xi:i=1::ng, which represent
statements for which we know which words are
topic words(wti), and which are sentiment words
(wsi). Each of these observations corresponds to
a unique point pti ; psi ; px i in the space of paired
distributions IP x IP x IPx , defined by the follow-
ing coordinates:
pxi(x)=Ax1x=xi+(1—Ax): psi(v)=�s#(v;wsi)=#(wsi)+(1��s)�sv
pti(v)=�t#(v;wti)=#(wti)+(1��t)�tv
Here,#(v;wti)represents the number of times the
wordvwas observed in the topic part of statement
i, the length of which is denoted by#(wti).ctv
stands for the relative frequency of v in the topic
part of the collection. The same definitions ap-
ply to the sentiment parameters # (v; wsi), # (wsi)
and csv. The Boolean indicator function 1y returns
one when the predicate y is true and zero other-
wise. Metaparameters At, As and Ax specify the
amount of Dirichlet smoothing (Zhai and Lafferty,
2001) applied to the topic, sentiment and polarity
estimates respectively; values for these parameters
are determined empirically.
We define 7r(pt; ps px) to have mass 1n when
its argument pt; ps px corresponds to some ob-
servation pti ; psi ; px i, and zero otherwise:
7(pt; ps; px) = 1 n 1pt=ptix1ps=psix1px=pxi:
=1
Equation (3) maintains empirical dependencies
between the topic language modelptand the sen-
timent modelps, because we assign nonzero prob-
7(pt;ps;px)px(xi)
pt;ps;px
</bodyText>
<page confidence="0.957351">
347
</page>
<bodyText confidence="0.998041">
ability mass only to pairs of models that actually
co-occur in our observations.
</bodyText>
<subsectionHeader confidence="0.927463">
3.2.2 Limitations of the model
</subsectionHeader>
<bodyText confidence="0.9999965">
Our model represents each statementwias a
bag of words, or more formally an order-invariant
sequence. This representation is often confused
with word independence, which is a much stronger
assumption. The generative model defined by
equation (1) ignores the relative ordering of the
words, but it does allow arbitrarily strong un-
ordered dependencies among them. To illustrate,
consider the probability of observing the words
‘unpredictable’ and ‘plot’ in the same statement.
Suppose we set At, As=1 in equation (2), reduc-
ing the effects of smoothing. It should be evi-
dent that P (unpredictable,plot) will be non-zero
only when the two words actually co-occur in the
training data. By carefully selecting the smoothing
parameters, the model can preserve dependencies
between topic and sentiment words, and is quite
capable of distinguishing the positive sentiment of
‘unpredictable plot’ from the negative sentiment
of ‘unpredictable steering’. On the other hand, the
model does ignore the ordering of the words, so it
will not be able to differentiate the negative phrase
‘gone from good to bad’ from its exact opposite.
Furthermore, our model is not well suited for mod-
eling adjacency effects: the phrase ‘unpredictable
plot’ is treated in the same way as two separate
words, ‘unpredictable’ and ‘plot’, co-occurring in
the same sentence.
</bodyText>
<subsectionHeader confidence="0.999351">
3.3 Using the model for retrieval
</subsectionHeader>
<bodyText confidence="0.953148225806452">
The generative model presented above can be ap-
plied to sentiment retrieval in the following fash-
ion. We start with a collection of statementsCand
a queryfqs,qt,qxgsupplied by the user. We use
the machinery outlined in Section 3.2 to estimate
the topic and sentiment relevance models corre-
sponding to the user’s information need, and then
determine which statements in our collection most
closely correspond to these models of relevance.
The topic relevance model Rt and sentiment rele-
vance model Rs are estimated as follows. We as-
sume that our query qs , qt , qx is a random sample
from a distribution defined by equation (1), and
then for each word v we estimate the likelihood
that v would be observed if we sampled one more
topic or sentiment word:
Rt (v)=P(qs ,qtÆv,qx), Rs(v)=P(qsÆv,qt qx)
P(qs qt qx)P(qs,qt,qx).
(4)
Both the numerator and denominator are com-
puted according to equation (1), with the mass
function 7 () given by equations (3) and (2). We
use the notation qÆv to denote appending word v
to the string q. Estimation is done over the train-
ing corpus, which may or may not include numeric
values of sentiment polarity.1 Once we have esti-
mates for the topic and sentiment relevance mod-
els, we can rank testing statementswby their sim-
ilarity toRtandRs. We rank statements using
a variation of cross-entropy, which was proposed
by (Zhai, 2002):
</bodyText>
<equation confidence="0.95057">
Rs(v) log ps(v).
</equation>
<bodyText confidence="0.98638475">
(5)
Here the summations extend over all words v in
the vocabulary, Rt and Rs are given by equa-
tion (4), whileptandpsare computed according
to equation (2). A weighting parameteroallows
us to change the balance of topic and sentiment
in the final ranking formula; its value is selected
empirically.
</bodyText>
<sectionHeader confidence="0.918741" genericHeader="method">
4 Sentiment Retrieval Task
</sectionHeader>
<subsectionHeader confidence="0.992428">
4.1 Task definition
</subsectionHeader>
<bodyText confidence="0.999997142857143">
We define two variations of the sentiment retrieval
task. In one, the user supplies us with a numeric
value for the desired polarityqx. In the other,
the user supplies a set of seed wordsqs, reflect-
ing the desired sentiment. The first task requires
us to have polarity observationsxiin our training
data, while the second does not.
</bodyText>
<subsectionHeader confidence="0.729901">
Task with training data:
</subsectionHeader>
<bodyText confidence="0.999267571428571">
Input: (1) a set of topic keywordsqtand (2)
a sentiment specification qx 2 f —1,1g. In
this case we assume qs to be the empty
string.
Output: a ranked list of topic-relevant and
sentiment-relevant sentences from the test
data.
</bodyText>
<subsectionHeader confidence="0.543527">
Task with seed words:
</subsectionHeader>
<bodyText confidence="0.950512333333333">
Input: (1) a set of topic keywordsqtand (2)
a set of sentiment seed wordsqs. In this
case our model ignoresqxandxi.
</bodyText>
<footnote confidence="0.981013">
1When the training corpus does not contain numeric po-
larity valuesxi, we assume7F(pt,ps,px)=7F(pt,ps)and
forcepx(xi)to be a constant.
</footnote>
<figure confidence="0.620622">
Rt(v)logpt(v)+(1—o)E
v
o�v
</figure>
<page confidence="0.982044">
348
</page>
<bodyText confidence="0.999635722222222">
Output: a ranked list of topic-relevant and
sentiment-relevant sentences from the test
data.
In the first task, we split our corpus into three
parts: (i) the training set, which was used for es-
timating the relevance modelsRsandRt; (ii) the
development set, which was used for tuning the
model parametersAt,Asanda; and (iii) the testing
set, from which we retrieved sentences in response
to the query. In the second task, we split the corpus
into two parts: (i) the training set, which was used
for tuning the model parameters; and (ii) the test-
ing set, which was used for constructing Rs and
Rt and from which we retrieved sentences in re-
sponse to queries. The testing set was identical
in both tasks. Note that the sentiment relevance
model Rs can be constructed in a topic-dependent
fashion for both tasks.
</bodyText>
<subsectionHeader confidence="0.996803">
4.2 Variations of the retrieval model
</subsectionHeader>
<bodyText confidence="0.993468391891892">
slm: the retrieval model as described in Sec-
tion 3.3.
lmt: the standard language modeling ap-
proach (Ponte and Croft, 1998; Song and
Croft, 1999) on the topic keywords qt for the
topic part of the text wt.
lms: the standard language modeling approach
on the sentiment keywords qs for the senti-
ment part of the text ws.
base: the weighted linear combination of lmt
and lms.
rmt: only the topic relevance model was used
for ranking using qt and for wt 3
rms: only the sentiment relevance model was
used for ranking using qs and for ws.
rmt-base: the slm model with a = 1, ignoring
the sentiment relevance model.
rms-base: the slm model witha=0, ignoring
the topic relevance model.
2Because the training set was used for tuning the model
parameters, no development set was required for this task.
3When we use the automatic annotation that is described
in Section 5.2.2, we use the whole text instead of the topic
part of the text, for the reasons given in that section. This
treatment is applied to the base, rmt-base, rms-base, rmt-rms,
rmt-slm and slm models that are described in this section for
using the automatic annotation. However, we distinguish the
lmt and rmt models using the topic part of the text and the
lmtf and rmtf models, as baselines, using the whole text, re-
spectively, even in the experiments using the automatic anno-
tation.
rmt-rms: the rmt and rms models are treated
independently.
rmt-slm: the rmt and rms-base models are
combined.
lmtf: the standard language modeling ap-
proach usingqtfor the nonsplit text, as base-
line.
rmtf: the conventional relevance model was
used for ranking usingqtfor the nonsplit text,
as baseline.
lmtsf: the standard language modeling ap-
proach using bothqtandqsfor the nonsplit
text, for reference.
rmtsf: the conventional relevance model was
used for ranking using bothqtandqsfor the
nonsplit text, for reference.
Note that the relevance models are constructed
using training data for the training-based task, but
are constructed using test data for the seed-based
task, as mentioned in Section 4.1. Therefore, the
base model is only used for the training data, not
for the test data, in the training-based task, while
it can be performed for the test data in the case of
the seed-based task. Moreover, the lms, lmtsf and
rmtsf models are based on the premise of using
seed words to specify sentiments, and so they are
only applicable to the seed-based task.
In the models described in this subsec-
tion,AtandAsin equation (2) were set to
Dirichlet estimates (Zhai and Lafferty, 2001),
(wti)=(#(wti) + ltt) and #(wsi)=(#(wsi) +Its)
for the relevance models Rt and Rs, respectively,
in equation (4), and were fixed at 0.9 for ranking
as in equation (5) for our experiments in Section 5.
Here, fit and Its were selected empirically accord-
ing to the tasks described in Section 4.1. The
model parameterain equation (5) was also se-
lected empirically in the same manner. The num-
ber of ranked documents used in the relevance
modelsRtandRs, in equation (4), was selected
empirically in the same manner as above; how-
ever, we fixed the number of terms used in the rel-
evance models as 1000.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999549">
5.1 Data set and evaluation measure
</subsectionHeader>
<bodyText confidence="0.967111666666667">
We used the MPQA Opinion Corpus version
1.2 (Wilson et al., 2005; Wiebe et al., 2005) to
measure the effectiveness of our sentiment re-
</bodyText>
<page confidence="0.996367">
349
</page>
<bodyText confidence="0.997676272727273">
trieval models. We summarize this data set as fol-
lows.
•This corpus contains news articles collected
from 187 different foreign and U.S. news
sources from June 2001 to May 2002. The cor-
pus contains 535 documents, a total of 11,114
sentences.
•The majority of the articles are on 10 differ-
ent topics, which are labeled at document level,
but, in addition to these, a number of additional
articles were randomly selected from a larger
corpus of 270,000 documents.
•Each article was manually annotated using an
annotation scheme for opinions and other pri-
vate states at phrase level. We only used the
annotations for sentiments that included some
attributes such as polarity and strength.
In this data set, the topic relevance for the 10
topics is known at the document level, but un-
known at the sentence level. We assumed that all
the sentences in a relevant document could be con-
sidered relevant to the topic.4
This data set was annotated with sentiment po-
larities at the phrase level, but not explicitly an-
notated at the sentence level. Therefore, we pro-
vided sentiment polarities at the sentence level to
prepare training data and data for evaluation. We
set the sentence-level sentiment polarity equal to
the polarity with the highest strength in each sen-
tence.5
Queries were expressed using the title of one of
the 10 topics and specified as positive or negative.
Thus, we had 20 types of queries for our experi-
ments. Because the supposed relevance judgments
in this setting are imperfect at sentence level, we
used bpref (Buckley and Voorhees, 2004), in both
the training and testing phases, as it is known to
be tolerant of imperfect judgments. Bpref uses bi-
nary relevance judgments to define the preference
relation (i.e., any relevant document is preferred
over any nonrelevant document for a given topic),
while other measures, such as mean average pre-
cision, depend only on the ranks of the relevant
documents.
</bodyText>
<footnote confidence="0.983756428571429">
4This is a strong assumption to make and may not be true
in all cases. A larger, more complete data set is required to
perform a more detailed analysis, which is left as future work.
5We disregarded ‘neutral’ and ‘both’ if other polarities ap-
peared. We can also set the sentence-level sentiment polarity
according to the presence of polarity in each sentence, but we
did not consider this setting here.
</footnote>
<subsectionHeader confidence="0.99699">
5.2 Extracting sentiment expressions
5.2.1 Using manual annotation
</subsectionHeader>
<bodyText confidence="0.999940555555556">
Because the MPQA corpus was annotated with
phrase-level sentiments, we can use these anno-
tations to split a sentence into a topic partwt
and a sentiment partw&apos;S. The Krovetz stem-
mer (Krovetz, 1993) was applied to the topic part,
the sentiment part and to the query terms6 and, for
the retrieval experiments in Sections 5.3 and 5.4,
a total of 418 stopwords from a standard stopword
list were removed when they appeared.
</bodyText>
<subsectionHeader confidence="0.941993">
5.2.2 Using automatic annotation
</subsectionHeader>
<bodyText confidence="0.99987">
In automatic extraction of sentiment expres-
sions in this study, we detected sentiment-bearing
words using lists of words with established polar-
ities. At this stage, topic dependence was not con-
sidered; however, at the stage of sentiment model-
ing, the topic dependence can be reflected, as de-
scribed in Sections 3 and 4.
We first prepared a list of words indicating sen-
timents. We used Hatzivassiloglou and McKe-
own’s sentiment word list (Hatzivassiloglou and
McKeown, 1997), which consists of 657 positive
and 679 negative adjectives, and The General In-
quirer (Stone et al., 1966), which contains 1621
positive and 1989 negative words.7 By merging
these lists, we obtained 1947 positive and 2348
negative words. After stemming these words in the
same manner as in Section 5.2.1, we were left with
1667 positive and 2129 negative words, which we
will use hereafter in this paper.
The sentiment polarities are sometimes sensi-
tive to the structural information, for instance,
a negation expression reverses the following
sentiment polarity. To handle negation, ev-
ery sentiment-bearing word was rewritten with a
‘NEG’ suffix, such as ‘good NEG’, if an odd num-
ber of negation expressions was found within the
five preceding words in the sentence. To detect
negation expressions, we used a predefined nega-
tion expression list. This negation handling is sim-
ilar to that used in (Das and Chen, 2001; Pang et
al., 2002). We extracted sentiment-bearing expres-
sions using the list of words with established po-
</bodyText>
<footnote confidence="0.999710625">
6We used the topic labels attached to the MPQA corpus as
the topic query termsq&apos;in all the experiments in Sections 5.3
and 5.4.
7We extracted positive and negative words from the Gen-
eral Inquirer basically in the same manner as in (Turney and
Littman, 2003); however, we did not exclude any words, un-
like (Turney and Littman, 2003), where some seed words
were excluded for the evaluation of their work.
</footnote>
<page confidence="0.998103">
350
</page>
<tableCaption confidence="0.999548">
Table 1: Sample probabilities from the sentiment relevance models
</tableCaption>
<table confidence="0.984998708333333">
Reaction to President Bush’s 2002 presidential election Israeli settlements in
Topic-independent Topic-independent 2002 State of the Union Address in Zimbabwe Gaza and West Bank
w/ manual annot. w/ automatic annot. w/ manual annot. w/ automatic annot. w/ manual annot. w/ automatic annot. w/ manual annot. w/ automatic annot.
P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w
0.047 demand 0.029 state 0.030 support 0.067 state 0.042 support 0.039 support 0.041 ask 0.097 settle
0.031 expect 0.026 support 0.016 promise 0.034 support 0.033 legitimate 0.033 legitimate 0.036 agreed 0.032 peace
0.031 defend 0.014 lead 0.014 call 0.024 call 0.031 free 0.033 lead 0.036 call 0.025 state
0.031 invite 0.013 call 0.014 excellent 0.019 meet 0.029 congratulate 0.025 free 0.033 aim 0.022 secure
0.031 humane 0.013 minister 0.013 goal 0.017 minister 0.028 fair 0.025 fair 0.028 immediate 0.015 call
0.031 safeguard 0.011 right 0.013 express 0.015 promise 0.023 please 0.018 state 0.025 aware 0.014 conflict
0.031 nutritious 0.010 foreign 0.013 best 0.014 white 0.017 confident 0.017 congratulate 0.024 key 0.013 support
0.031 helpful 0.009 hope 0.012 count 0.013 foreign 0.017 call 0.015 call 0.022 expect 0.012 right
0.016 time 0.009 meet 0.012 cooperate 0.012 success 0.012 hopeful 0.015 meet 0.018 justify 0.011 attack
0.016 say 0.008 interest 0.011 proposal 0.011 defense 0.012 express 0.013 unity 0.018 honoure 0.011 minister
0.091 evil 0.037 state 0.065 evil 0.098 state 0.029 flaw 0.028 flaw 0.018 palestinian 0.100 settle
0.080 axis 0.022 evil 0.049 axis 0.051 evil 0.018 condemn 0.026 critic 0.013 protest 0.031 state
0.045 threat 0.015 right 0.022 critic 0.028 critic 0.015 true 0.023 state 0.012 decide 0.019 peace
0.033 qualify 0.015 prison 0.011 prepare 0.017 call 0.014 critic 0.022 opposition 0.011 peace 0.014 secure NEG
0.030 wrote 0.013 critic 0.010 recognize 0.012 interest 0.012 expect 0.019 reject 0.011 fatten 0.013 critic
0.020 particular 0.010 human 0.010 reckless 0.011 move 0.011 reject 0.017 condemn 0.011 believe 0.012 force
0.020 word 0.008 support 0.010 country 0.011 reject 0.011 s 0.016 legal 0.009 plan 0.012 attack
0.018 harsh 0.008 protest 0.009 upset 0.010 slam 0.011 fair 0.015 move 0.009 fear 0.012 war
0.015 reject 0.008 war 0.009 pick 0.010 right 0.011 free 0.015 democratic 0.009 mistake 0.011 believe
0.015 dangerous 0.008 force 0.009 eyesore 0.010 attack 0.010 angry 0.014 support 0.009 continue 0.011 minister
</table>
<bodyText confidence="0.994539166666667">
The upper and lower tables correspond to positive and negative sentiments, respectively. The topic-independent
sentiment relevance models (in the left two columns) correspond to rms, and the topic-dependent models (in the
rest of the columns) correspond to rms-base, which is used for slm.
larities, considering negation, as described above.
Note that we used the list of words with sentiments
to extract sentiment expressions, but we did not
use the predefined sentiments to model sentiment
relevance.
Some expressions are sometimes used to ex-
press a certain topic, such as settlements in “Is-
raeli settlements in Gaza and West Bank”; but at
other times are used to express a certain sentiment,
such as the same word in “All parties signed court-
mediated compromise settlements”. Therefore, we
will use whole sentences to model topic relevance,
while we will use the automatically extracted sen-
timent expressions to model sentiment relevance,
in Sections 5.3 and 5.4.
</bodyText>
<subsectionHeader confidence="0.997047">
5.3 Experiments on training-based task
</subsectionHeader>
<bodyText confidence="0.999687979591837">
We conducted experiments on the training-based
task described in Section 4.1, using either man-
ual annotation as described in Section 5.2.1 or au-
tomatic annotation as described in Section 5.2.2.
Table 1 contrasts sample probabilities from topic-
independent sentiment relevance models and those
from topic-dependent sentiment relevance models.
In the left two columns of this table, two sets of
sample probabilities using the topic-independent
model are presented. One was computed from the
manual annotation and the other was computed
from the automatic annotation. In the remain-
ing columns, samples using the topic-dependent
model are shown according to the three topics:
(1) “reaction to President Bush’s 2002 State of
the Union Address”, (2) “2002 presidential elec-
tion in Zimbabwe”, and (3) “Israeli settlements
in Gaza and West Bank”. A number of posi-
tive expressions appeared topic dependent, such
as ‘promise’ (stemmed from ‘promising’ or not)
and ‘support’ for Topic (1), ‘legitimate’ and ‘con-
gratulate’ for Topic (2) and ‘justify’ and ‘se-
cure’ for Topic (3); while negative expressions ap-
peared topic-dependent, such as ‘critic’ (stemmed
from ‘criticism’) and ‘eyesore’ for Topic (1),
‘flaw’ and ‘condemn’ for Topic (2) and ‘mistake’
and ‘secure NEG’ (i.e., ‘secure’ was negated) for
Topic (3).
Some expressions were unexpectedly generated
regardless of the types of annotation, e.g., ‘pales-
tinian’ for Topic (3); however, we found some
characteristics in the results using automatic anno-
tation. Some expressions on opinions that did not
convey sentiments, such as ‘state’, frequently ap-
peared regardless of topic. This sort of expression
may effectively function as degrading sentences
only conveying facts, but may function harmfully
by catching sentences conveying opinions without
sentiments in the task of sentiment retrieval. Some
topic expressions, such as ‘settle’ (stemmed from
‘settlement’ or not) for Topic (3), were generated,
because such words convey positive sentiments in
some other contexts and thus they were contained
in the list of sentiment-bearing words that we used
for automatic annotation. This will not cause a
topic relevance model to drift, because we mod-
eled the topic relevance using whole sentences, as
described in Section 5.2.2; however, it may harm
the sentiment relevance model to some extent.
</bodyText>
<page confidence="0.998978">
351
</page>
<tableCaption confidence="0.960731">
Table 2: Experimental results of training-based
task using manually annotated data
Table 3: Experimental results of training-based
task using automatically annotated data
</tableCaption>
<table confidence="0.999844">
10% 25% 40%
Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)
lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145)
lmt 0.1499 (0.1164) 0.1499 (0.1164) 0.1444 (0.1148)
rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691)
rmt 0.1712 (0.1619) 0.1712 (0.1619) 0.1922 (0.1705)
rmt-base 0.1922 (0.1723) 0.2005 (0.1812) 0.2100* (0.1951)
rms 0.0464 (0.0384) 0.0452 (0.0394) 0.0375 (0.0320)
rms-base 0.0772 (0.0640) 0.0869 (0.0704) 0.0865 (0.0724)
rmt-rms 0.2025 (0.1413) 0.2210 (0.1925) 0.2117 (0.2003)
rmt-slm 0.2278* (0.1715) 0.2249 (0.1676) 0.1999 (0.1819)
slm 0.2006 (0.1914) 0.2247 (0.1824) 0.2441* (0.2427)
</table>
<tableCaption confidence="0.962131">
‘*’ indicates statistically significant improve-
ment over rmtf wherep&lt;0.05with the two-
sided Wilcoxon signed-rank test.
</tableCaption>
<bodyText confidence="0.999995642857143">
We performed retrieval experiments in the steps
described in Section 4.1. For this purpose, we split
the data into three parts: (i)x% as the training
data, (ii)(50-x)% as the evaluation data, and
(iii)50% as the test data.
The test results of training-based task using
manually annotated data and automatically anno-
tated data are shown in Tables 2 and 3, respec-
tively. The scores were computed according to the
bpref evaluation measure (Buckley and Voorhees,
2004), as mentioned in Section 5.1. In addition
to the bpref, mean average precision values are
presented as ‘AvgP’ in the tables, for reference.8
In these tables, the top row indicates the percent-
ages of the training datax. It turned out that
in all our experiments the appropriate fraction of
training data was 40%. In this setting, our slm
model worked 76.1% better than the query like-
lihood model and 32.6% better than the conven-
tional relevance model, when using manual anno-
tation, and both improvements were statistically
significant according to the Wilcoxon signed-rank
test.9 When using automatic annotation, the slm
model worked 67.2% better than the query like-
lihood model and 25.9% better than the conven-
tional relevance model, where both improvements
were statistically significant. The rmt-base model
also worked well with automatic annotation.
</bodyText>
<subsectionHeader confidence="0.994191">
5.4 Experiments on seed-based task
</subsectionHeader>
<bodyText confidence="0.991626">
For experiments on the seed-based task that was
described in Section 4.1, we used three groups of
</bodyText>
<footnote confidence="0.9970492">
8As mentioned in Section 5.1, the bpref is more appro-
priate for the evaluation of our experiments than the mean
average precision.
9Significance tests involved only 20 queries, which makes
it difficult to achieve statistical significance.
</footnote>
<table confidence="0.999699166666667">
10% 25% 40%
Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)
lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145)
lmt 0.1325 (0.0972) 0.1315 (0.0976) 0.1325 (0.0972)
rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691)
rmt 0.1490 (0.1418) 0.1762 (0.1584) 0.1695 (0.1485)
rmt-base 0.2076* (0.1936) 0.2252* (0.2139) 0.2302* (0.2196)
rms 0.0347 (0.0287) 0.0501 (0.0408) 0.0501 (0.0408)
rms-base 0.0943 (0.0733) 0.1196 (0.0896) 0.1241 (0.0979)
rmt-rms 0.1690 (0.1182) 0.2063 (0.1938) 0.1603 (0.1591)
rmt-slm 0.1980 (0.1426) 0.2013 (0.1835) 0.2148 (0.1882)
slm 0.2011 (0.1537) 0.2261* (0.1716) 0.2318* (0.1802)
</table>
<tableCaption confidence="0.962027">
‘*’ indicates statistically significant improve-
ment over rmtf wherep&lt;0.05with the two-
sided Wilcoxon signed-rank test.
</tableCaption>
<bodyText confidence="0.9984144">
seed words: KAM, TUR and ORG. Each group
consists of a positive word set q&apos;+) and a negative
word set q&apos;_), as follows:
KAM:q&apos;+)_{good}, andq&apos;_)_{bad}.
TUR:q�(+)_{good, nice, excellent, positive,
fortunate, correct, superior}, and q&apos;_) _ {bad,
nasty, poor, negative, unfortunate, wrong, infe-
rior}.
ORG: q&apos;+) _ {support, demand, promise,
want, hope}, and q&apos;_) _ {refuse, accuse, crit-
icism, fear, reject}.
KAM and TUR were used in (Kamps and
Marx, 2002) and (Turney and Littman, 2003),
respectively. We constructed ORG considering
sentiment-bearing words that may frequently ap-
pear in newspaper articles.
We experimented with the seed-based task,
making use of each of these seed word groups, in
the steps described in Section 4.1. For this pur-
pose, we split the data into two parts: (i) 50% as
the estimation data and (ii) 50% as the test data.
The test results using manually annotated data
and automatically annotated data are shown in Ta-
bles 4 and 5, respectively, where the scores were
computed according to the bpref evaluation mea-
sure. Mean average precision values are also pre-
sented as ‘AvgP’ in the tables, for reference.
When using the manually annotated approach,
our slm model worked well, especially with the
seed word group ORG, as shown in Table 4. Us-
ing ORG, the slm model worked 61.2% better
than the query likelihood model and 15.2% bet-
ter than the conventional relevance model, where
both improvements were statistically significant
according to the Wilcoxon signed-rank test. Even
</bodyText>
<page confidence="0.997168">
352
</page>
<tableCaption confidence="0.918726">
Table 4: Experimental results of seed-based task
using manually annotated data
Table 5: Experimental results of seed-based task
using automatically annotated data
</tableCaption>
<table confidence="0.998932866666667">
ORG TUR KAM
Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)
lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119)
lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062)
lmt 0.1501 (0.1171) 0.1501 (0.1171) 0.1501 (0.1171)
base 0.1615 (0.1319) 0.1531 (0.1217) 0.1514 (0.1180)
rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776)
rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754)
rmt 0.1974 (0.1826) 0.1974 (0.1826) 0.1974 (0.1826)
rmt-base 0.1960 (0.1918) 0.1931 (0.1703) 0.1837 (0.1721)
rms 0.0434 (0.0262) 0.0295 (0.0205) 0.0280 (0.0170)
rms-base 0.1142 (0.1022) 0.1144 (0.0841) 0.1226 (0.0973)
rmt-rms 0.1705 (0.1117) 0.1403 (0.1424) 0.1405 (0.0842)
rmt-slm 0.2266* (0.2034) 0.2272* (0.2012) 0.2264* (0.2016)
slm 0.2233* (0.2048) 0.2160 (0.1945) 0.2072 (0.1929)
</table>
<tableCaption confidence="0.902527333333333">
‘*’ indicates statistically significant improve-
ment over rmtf wherep&lt;0.05with the two-
sided Wilcoxon signed-rank test.
</tableCaption>
<bodyText confidence="0.999728071428572">
using the other seed word groups, the slm model
worked 49–56% better than the query likelihood
model and 6–12% better than the conventional
relevance model; however, the latter improve-
ment was not statistically significant. The rmt-slm
model also worked well with manual annotation.
When using automatic annotation, the slm
model worked 46–48% better than the query like-
lihood model and 4–6% better than the conven-
tional relevance model, as shown in Table 5. The
improvements over the conventional relevance
model were statistically significant only when us-
ingTURorKAM; however, the score when us-
ingORGis almost comparable with the others.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999850470588235">
We propose sentiment retrieval models in the
framework of probabilistic generative models, not
only assuming that a user inputs query terms ex-
pressing a certain topic, but also assuming that the
user specifies a sentiment polarity of interest ei-
ther as a sentiment specificationq&apos;E{-1,11or
as a set of sentiment seed wordsqs. For this pur-
pose, we combine sentiment relevance models and
topic relevance models, considering the topic de-
pendence of the sentiment. In our experiments,
our model worked significantly better than stan-
dard language modeling approaches, both when
usingq&apos;andqs, and with both manual and auto-
matic annotation of the fragments expressing sen-
timents in text. Withqsand automatic annota-
tion, our model still worked significantly better
than the standard approaches; however, the per-
</bodyText>
<table confidence="0.998576133333333">
ORG TUR KAM
Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP)
lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119)
lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062)
lmt 0.1325 (0.0972) 0.1325 (0.0972) 0.1325 (0.0972)
basef 0.1550 (0.1369) 0.1451 (0.1188) 0.1416 (0.1142)
rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776)
rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754)
rmt 0.1757 (0.1578) 0.1757 (0.1578) 0.1757 (0.1578)
rmt-base 0.1957 (0.1862) 0.1976 (0.1882) 0.1825 (0.1704)
rms 0.0421 (0.0236) 0.0364 (0.0205) 0.0217 (0.0147)
rms-base 0.1268 (0.1096) 0.1301 (0.1148) 0.1326 (0.1158)
rmt-rms 0.1465 (0.1514) 0.1390 (0.1393) 0.1252 (0.0757)
rmt-slm 0.1977 (0.1811) 0.2008 (0.1649) 0.1959 (0.1677)
slm 0.2031 (0.1714) 0.2055* (0.1668) 0.2044* (0.1698)
</table>
<tableCaption confidence="0.903884666666667">
‘*’ indicates statistically significant improve-
ment over rmtf wherep&lt;0.05with the two-
sided Wilcoxon signed-rank test.
</tableCaption>
<bodyText confidence="0.9995125625">
formance did not reach that achieved with other
settings. We believe the performance can be im-
proved with larger-scale data.
We experimented to find sentences that were
relevant to a given topic and were appropriate to
a given sentiment; however, our models can also
be applied to textual chunks of any length, such as
at document level or passage level. Our model can
be easily extended to opinion retrieval, if the opin-
ion retrieval is defined as retrieving sentences or
documents that contain either positive or negative
sentiments. This issue is worth pursuing in future
work. Approaches considering polarity strength
or continuous values for the polarity specification,
rather than using{-1�11, can also be considered
in future work.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999689">
We thank James Allan, W. Bruce Croft and the anony-
mous reviewers for valuable discussions and comments. This
work was supported in part by the Overseas Research Schol-
ars Program and the Grant-in-Aid for Scientific Research
(#17680011) from the Ministry of Education, Culture, Sports,
Science and Technology, Japan, in part by the Telecommu-
nications Advancement Foundation, Japan, in part by the
Center for Intelligent Information Retrieval, and in part by
the Defense Advanced Research Projects Agency (DARPA),
USA under contract number HR0011-06-C-0023. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessar-
ily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998435">
James Allan, Courtney Wade, and Alvaro Bolivar. 2003. Re-
trieval and novelty detection at the sentence level. In Proc.
of the 26th Annual International ACM SIGIR Conference,
pages 314–321, Toronto, Canada.
</reference>
<page confidence="0.989876">
353
</page>
<reference confidence="0.999932947368421">
Chris Buckley and Ellen M. Voorhees. 2004. Retrieval eval-
uation with incomplete information. In Proc. of the 27th
Annual International ACM SIGIR Conference, pages 25–
32, Sheffield, United Kingdom.
Sanjiv R. Das and Mike Y. Chen. 2001. Yahoo! for Ama-
zon: Sentiment parsing from small talk on the Web. In
Proc. of the 2001 European Finance Association Annual
Conference, Barcelona, Spain.
Kushal Dave, Steve Lawrence, and David M. Pennock. 2003.
Mining the peanut gallery: Opinion extraction and seman-
tic classification of product reviews. In Proc. of the 12th
International Conference on the World Wide Web, pages
519–528, Budapest, Hungary.
Charlotta Engstr¨om. 2004. Topic dependence in sentiment
classification. Master’s thesis, University of Cambridge.
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997.
Predicting the semantic orientation of adjectives. In Proc.
of the 35th Annual Meeting of the Association for Compu-
tational Linguistics, pages 174–181, Madrid, Spain.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proc. of the 10th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 168–177, Seattle, USA.
Jaap Kamps and Maarten Marx. 2002. Words with attitude.
In Proc. of the 1st International Conference on Global
WordNet, pages 332–341, Mysore, India.
Soo-Min Kim and Eduard Hovy. 2004. Determining the sen-
timent of opinions. In Proc. ofthe 20th International Con-
ference on Computational Linguistics, Geneva, Czech Re-
public.
Soo-Min Kim, Deepak Ravichandran, and Eduard Hovy.
2004. ISI Novelty Track system for TREC 2004. In Proc.
of the 13th Text Retrieval Conference. NIST Special Pub-
lication 500-261.
Robert Krovetz. 1993. Viewing morphology as an inference
process. In Proc. of the 16th Annual International ACM
SIGIR Conference, pages 191–202, Pittsburgh, Pennsylva-
nia, USA.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance-based
language models. In Proc. of the 24th Annual Interna-
tional ACM-SIGIR Conference, pages 120–127, New Or-
leans, Louisiana, USA.
David E. Losada. 2005. Language modeling for sentence
retrieval: A comparison between multiple-Bernoulli and
multinomial models. In Information Retrieval and Theory
Workshop, Glasgow, United Kingdom.
Vanessa Murdock and W. Bruce Croft. 2005. A translation
model for sentence retrieval. In Proc. of HLT/EMNLP
2005, pages 684–691, Vancouver, Canada.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment anal-
ysis: Capturing favorability using natural language pro-
cessing. In Proc. of the 2nd International Conference on
Knowledge Capture, pages 70–77, Sanibel Island, Florida,
USA.
Kamal Nigam and Matthew Hurst, 2005. Computing Atti-
tude and Affect in Text: Theory and Applications, chapter
Towards a Robust Metric of Opinion. Springer.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine
learning techniques. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing,
pages 79–86, Philadelphia, Pennsylvania, USA.
Jay M. Ponte and W. Bruce Croft. 1998. A language mod-
eling approach to information retrieval. In Proc. of the
21st Annual International ACM-SIGIR Conference, pages
275–281, Melbourne, Australia.
James Shanahan, Yan Qu, and Janyce Wiebe, editors. 2005.
Computing attitude and affect in text. Springer.
B. W. Silverman, 1986. Density Estimation for Statistics and
Data Analysis, pages 75–94. CRC Press.
Ian Soboroff and Donna Harman. 2003. Overview of the
TREC 2003 Novelty Track. In Proc. of the 12th Text Re-
trieval Conference, pages 38–53. NIST Special Publica-
tion 500-255.
Ian Soboroff. 2004. Overview of the TREC 2004 Novelty
Track. In Proc. of the 13th Text Retrieval Conference.
NIST Special Publication 500-261.
Fei Song and W. Bruce Croft. 1999. A general language
model for information retrieval. In Proc. of the 8th Inter-
national Conference on Information and Knowledge Man-
agement, pages 316–321, Kansas City, Missouri, USA.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Sys-
tems, 21(4):315–346.
Peter D. Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classification
of reviews. In Proc. of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 417–424,
Philadelphia, Pennsylvania, USA.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 1(2):0–0.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proc. of HLT/EMNLP 2005, Vancouver,
Canada.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extracting
sentiments about a given topic using natural language pro-
cessing techniques. In Proc. of the 3rd IEEE International
Conference on Data Mining, pages 427– 434, Melbourne,
Florida, USA.
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to ad hoc
information retrieval. In Proc. of the 24th AnnualInterna-
tional ACM-SIGIR Conference, pages 334–342, New Or-
leans, Louisiana, USA.
Chengxiang Zhai. 2002. Risk Minimization and Language
Modeling in Text Retrieval. PhD dissertation, Carnegie
Mellon University.
</reference>
<page confidence="0.999146">
354
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.844466">
<title confidence="0.998584">Sentiment Retrieval using Generative Models</title>
<author confidence="0.917869">Koji</author>
<affiliation confidence="0.99871">National Institute of</affiliation>
<address confidence="0.961812">Tokyo 101-8430,</address>
<email confidence="0.967114">eguchi@nii.ac.jp</email>
<author confidence="0.986784">Victor</author>
<affiliation confidence="0.999831">Department of Computer University of</affiliation>
<address confidence="0.998069">Amherst, MA 01003,</address>
<email confidence="0.999932">lavrenko@cs.umass.edu</email>
<abstract confidence="0.99993247368421">Ranking documents or sentences according to both topic and sentiment relevance should serve a critical function in helping users when topics and sentiment polarities of the targeted text are not explicitly given, as is often the case on the web. In this paper, we propose several sentiment information retrieval models in the framework of probabilistic language models, assuming that a user both inputs query terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner. We combine sentiment relevance models and topic relevance models with model parameters estimated from training data, considering the topic dependence of the sentiment. Our experiments prove that our models are effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Courtney Wade</author>
<author>Alvaro Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In Proc. of the 26th Annual International ACM SIGIR Conference,</booktitle>
<pages>314--321</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="4334" citStr="Allan et al., 2003" startWordPosition="689" endWordPosition="692">rical Methods in Natural Language Processing (EMNLP 2006), pages 345–354, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 5 explains the data we used for our experiments, and gives our experimental results. Section 6 concludes the paper. 2 Related Work Some efforts for the TREC Novelty Track were related to our work. Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al., 2003; Losada, 2005; Murdock and Croft, 2005). Amongst the participants in the TREC Novelty Track, only (Kim et al., 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval). We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is th</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>James Allan, Courtney Wade, and Alvaro Bolivar. 2003. Retrieval and novelty detection at the sentence level. In Proc. of the 26th Annual International ACM SIGIR Conference, pages 314–321, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Retrieval evaluation with incomplete information.</title>
<date>2004</date>
<booktitle>In Proc. of the 27th Annual International ACM SIGIR Conference,</booktitle>
<pages>25--32</pages>
<location>Sheffield, United Kingdom.</location>
<contexts>
<context position="22427" citStr="Buckley and Voorhees, 2004" startWordPosition="3655" endWordPosition="3658">s annotated with sentiment polarities at the phrase level, but not explicitly annotated at the sentence level. Therefore, we provided sentiment polarities at the sentence level to prepare training data and data for evaluation. We set the sentence-level sentiment polarity equal to the polarity with the highest strength in each sentence.5 Queries were expressed using the title of one of the 10 topics and specified as positive or negative. Thus, we had 20 types of queries for our experiments. Because the supposed relevance judgments in this setting are imperfect at sentence level, we used bpref (Buckley and Voorhees, 2004), in both the training and testing phases, as it is known to be tolerant of imperfect judgments. Bpref uses binary relevance judgments to define the preference relation (i.e., any relevant document is preferred over any nonrelevant document for a given topic), while other measures, such as mean average precision, depend only on the ranks of the relevant documents. 4This is a strong assumption to make and may not be true in all cases. A larger, more complete data set is required to perform a more detailed analysis, which is left as future work. 5We disregarded ‘neutral’ and ‘both’ if other pola</context>
<context position="32846" citStr="Buckley and Voorhees, 2004" startWordPosition="5273" endWordPosition="5276">06 (0.1914) 0.2247 (0.1824) 0.2441* (0.2427) ‘*’ indicates statistically significant improvement over rmtf wherep&lt;0.05with the twosided Wilcoxon signed-rank test. We performed retrieval experiments in the steps described in Section 4.1. For this purpose, we split the data into three parts: (i)x% as the training data, (ii)(50-x)% as the evaluation data, and (iii)50% as the test data. The test results of training-based task using manually annotated data and automatically annotated data are shown in Tables 2 and 3, respectively. The scores were computed according to the bpref evaluation measure (Buckley and Voorhees, 2004), as mentioned in Section 5.1. In addition to the bpref, mean average precision values are presented as ‘AvgP’ in the tables, for reference.8 In these tables, the top row indicates the percentages of the training datax. It turned out that in all our experiments the appropriate fraction of training data was 40%. In this setting, our slm model worked 76.1% better than the query likelihood model and 32.6% better than the conventional relevance model, when using manual annotation, and both improvements were statistically significant according to the Wilcoxon signed-rank test.9 When using automatic</context>
</contexts>
<marker>Buckley, Voorhees, 2004</marker>
<rawString>Chris Buckley and Ellen M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proc. of the 27th Annual International ACM SIGIR Conference, pages 25– 32, Sheffield, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv R Das</author>
<author>Mike Y Chen</author>
</authors>
<title>Yahoo! for Amazon: Sentiment parsing from small talk on the Web.</title>
<date>2001</date>
<booktitle>In Proc. of the 2001 European Finance Association Annual Conference,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="25106" citStr="Das and Chen, 2001" startWordPosition="4095" endWordPosition="4098">5.2.1, we were left with 1667 positive and 2129 negative words, which we will use hereafter in this paper. The sentiment polarities are sometimes sensitive to the structural information, for instance, a negation expression reverses the following sentiment polarity. To handle negation, every sentiment-bearing word was rewritten with a ‘NEG’ suffix, such as ‘good NEG’, if an odd number of negation expressions was found within the five preceding words in the sentence. To detect negation expressions, we used a predefined negation expression list. This negation handling is similar to that used in (Das and Chen, 2001; Pang et al., 2002). We extracted sentiment-bearing expressions using the list of words with established po6We used the topic labels attached to the MPQA corpus as the topic query termsq&apos;in all the experiments in Sections 5.3 and 5.4. 7We extracted positive and negative words from the General Inquirer basically in the same manner as in (Turney and Littman, 2003); however, we did not exclude any words, unlike (Turney and Littman, 2003), where some seed words were excluded for the evaluation of their work. 350 Table 1: Sample probabilities from the sentiment relevance models Reaction to Preside</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>Sanjiv R. Das and Mike Y. Chen. 2001. Yahoo! for Amazon: Sentiment parsing from small talk on the Web. In Proc. of the 2001 European Finance Association Annual Conference, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proc. of the 12th International Conference on the World Wide Web,</booktitle>
<pages>519--528</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1528" citStr="Dave et al., 2003" startWordPosition="224" endWordPosition="227">endence of the sentiment. Our experiments prove that our models are effective. 1 Introduction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data. The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al., 2005). A number of studies have investigated sentiment classification at document level, e.g., (Pang et al., 2002; Dave et al., 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable. Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users. We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic. Intuitively, the expression of sentiment in text is dependent on the topic. For example, a negative view for some voting event may be e</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proc. of the 12th International Conference on the World Wide Web, pages 519–528, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotta Engstr¨om</author>
</authors>
<title>Topic dependence in sentiment classification. Master’s thesis,</title>
<date>2004</date>
<institution>University of Cambridge.</institution>
<marker>Engstr¨om, 2004</marker>
<rawString>Charlotta Engstr¨om. 2004. Topic dependence in sentiment classification. Master’s thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<location>Madrid,</location>
<contexts>
<context position="24189" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3945" endWordPosition="3948">eval experiments in Sections 5.3 and 5.4, a total of 418 stopwords from a standard stopword list were removed when they appeared. 5.2.2 Using automatic annotation In automatic extraction of sentiment expressions in this study, we detected sentiment-bearing words using lists of words with established polarities. At this stage, topic dependence was not considered; however, at the stage of sentiment modeling, the topic dependence can be reflected, as described in Sections 3 and 4. We first prepared a list of words indicating sentiments. We used Hatzivassiloglou and McKeown’s sentiment word list (Hatzivassiloglou and McKeown, 1997), which consists of 657 positive and 679 negative adjectives, and The General Inquirer (Stone et al., 1966), which contains 1621 positive and 1989 negative words.7 By merging these lists, we obtained 1947 positive and 2348 negative words. After stemming these words in the same manner as in Section 5.2.1, we were left with 1667 positive and 2129 negative words, which we will use hereafter in this paper. The sentiment polarities are sometimes sensitive to the structural information, for instance, a negation expression reverses the following sentiment polarity. To handle negation, every sentiment</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics, pages 174–181, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proc. of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, USA.</location>
<contexts>
<context position="1576" citStr="Hu and Liu, 2004" startWordPosition="233" endWordPosition="236">at our models are effective. 1 Introduction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data. The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al., 2005). A number of studies have investigated sentiment classification at document level, e.g., (Pang et al., 2002; Dave et al., 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable. Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users. We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic. Intuitively, the expression of sentiment in text is dependent on the topic. For example, a negative view for some voting event may be expressed using ‘flaw’, while a negative view for</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proc. of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
</authors>
<title>Words with attitude.</title>
<date>2002</date>
<booktitle>In Proc. of the 1st International Conference on Global WordNet,</booktitle>
<pages>332--341</pages>
<location>Mysore, India.</location>
<contexts>
<context position="35244" citStr="Kamps and Marx, 2002" startWordPosition="5630" endWordPosition="5633">m 0.2011 (0.1537) 0.2261* (0.1716) 0.2318* (0.1802) ‘*’ indicates statistically significant improvement over rmtf wherep&lt;0.05with the twosided Wilcoxon signed-rank test. seed words: KAM, TUR and ORG. Each group consists of a positive word set q&apos;+) and a negative word set q&apos;_), as follows: KAM:q&apos;+)_{good}, andq&apos;_)_{bad}. TUR:q�(+)_{good, nice, excellent, positive, fortunate, correct, superior}, and q&apos;_) _ {bad, nasty, poor, negative, unfortunate, wrong, inferior}. ORG: q&apos;+) _ {support, demand, promise, want, hope}, and q&apos;_) _ {refuse, accuse, criticism, fear, reject}. KAM and TUR were used in (Kamps and Marx, 2002) and (Turney and Littman, 2003), respectively. We constructed ORG considering sentiment-bearing words that may frequently appear in newspaper articles. We experimented with the seed-based task, making use of each of these seed word groups, in the steps described in Section 4.1. For this purpose, we split the data into two parts: (i) 50% as the estimation data and (ii) 50% as the test data. The test results using manually annotated data and automatically annotated data are shown in Tables 4 and 5, respectively, where the scores were computed according to the bpref evaluation measure. Mean avera</context>
</contexts>
<marker>Kamps, Marx, 2002</marker>
<rawString>Jaap Kamps and Maarten Marx. 2002. Words with attitude. In Proc. of the 1st International Conference on Global WordNet, pages 332–341, Mysore, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proc. ofthe 20th International Conference on Computational Linguistics,</booktitle>
<location>Geneva, Czech Republic.</location>
<contexts>
<context position="1596" citStr="Kim and Hovy, 2004" startWordPosition="237" endWordPosition="240">effective. 1 Introduction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data. The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al., 2005). A number of studies have investigated sentiment classification at document level, e.g., (Pang et al., 2002; Dave et al., 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable. Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users. We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic. Intuitively, the expression of sentiment in text is dependent on the topic. For example, a negative view for some voting event may be expressed using ‘flaw’, while a negative view for some politician may</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proc. ofthe 20th International Conference on Computational Linguistics, Geneva, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>ISI Novelty Track system for TREC</title>
<date>2004</date>
<booktitle>In Proc. of the 13th Text Retrieval Conference. NIST Special Publication</booktitle>
<pages>500--261</pages>
<contexts>
<context position="4451" citStr="Kim et al., 2004" startWordPosition="708" endWordPosition="711">mputational Linguistics Section 5 explains the data we used for our experiments, and gives our experimental results. Section 6 concludes the paper. 2 Related Work Some efforts for the TREC Novelty Track were related to our work. Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al., 2003; Losada, 2005; Murdock and Croft, 2005). Amongst the participants in the TREC Novelty Track, only (Kim et al., 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval). We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is the first work on sentiment retrieval, to the best of our knowledge. In the context of sentiment classification, some r</context>
</contexts>
<marker>Kim, Ravichandran, Hovy, 2004</marker>
<rawString>Soo-Min Kim, Deepak Ravichandran, and Eduard Hovy. 2004. ISI Novelty Track system for TREC 2004. In Proc. of the 13th Text Retrieval Conference. NIST Special Publication 500-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Krovetz</author>
</authors>
<title>Viewing morphology as an inference process.</title>
<date>1993</date>
<booktitle>In Proc. of the 16th Annual International ACM SIGIR Conference,</booktitle>
<pages>191--202</pages>
<location>Pittsburgh, Pennsylvania, USA.</location>
<contexts>
<context position="23461" citStr="Krovetz, 1993" startWordPosition="3828" endWordPosition="3829">rue in all cases. A larger, more complete data set is required to perform a more detailed analysis, which is left as future work. 5We disregarded ‘neutral’ and ‘both’ if other polarities appeared. We can also set the sentence-level sentiment polarity according to the presence of polarity in each sentence, but we did not consider this setting here. 5.2 Extracting sentiment expressions 5.2.1 Using manual annotation Because the MPQA corpus was annotated with phrase-level sentiments, we can use these annotations to split a sentence into a topic partwt and a sentiment partw&apos;S. The Krovetz stemmer (Krovetz, 1993) was applied to the topic part, the sentiment part and to the query terms6 and, for the retrieval experiments in Sections 5.3 and 5.4, a total of 418 stopwords from a standard stopword list were removed when they appeared. 5.2.2 Using automatic annotation In automatic extraction of sentiment expressions in this study, we detected sentiment-bearing words using lists of words with established polarities. At this stage, topic dependence was not considered; however, at the stage of sentiment modeling, the topic dependence can be reflected, as described in Sections 3 and 4. We first prepared a list</context>
</contexts>
<marker>Krovetz, 1993</marker>
<rawString>Robert Krovetz. 1993. Viewing morphology as an inference process. In Proc. of the 16th Annual International ACM SIGIR Conference, pages 191–202, Pittsburgh, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance-based language models.</title>
<date>2001</date>
<booktitle>In Proc. of the 24th Annual International ACM-SIGIR Conference,</booktitle>
<pages>120--127</pages>
<location>New Orleans, Louisiana, USA.</location>
<contexts>
<context position="8074" citStr="Lavrenko and Croft, 2001" startWordPosition="1285" endWordPosition="1288">dency between the topic and sentiment language models, and will demonstrate that treating them independently leads to sub-optimal retrieval performance. When a sentiment polarity value is observed for a given statement, we will treat it as a ternary variable influencing the topic and sentiment language models. We represent a user’s query as just another statement, consisting of topic and sentiment parts, subject to all the independence assumptions stated above. We will use the query to estimate the topic and sentiment language models that are representative of the user’s interests. Following (Lavrenko and Croft, 2001), we will use the term relevance models to describe these models, and will use them to rank statements in order of their relevance to the query. 3.1 Definitions We start by providing a set of definitions that will be used in the remainder of this section. The task of our model is to generate a collection of statementsw1...w,,. A statementwiis a string of 346 wordswi1:::wini, drawn from a common vocabularyV. We introduce a binary variablebij2fS;Tg as an indicator of whether the word in thejth position of the ith statement will be a topic word or a sentiment word. For our purposes, bij is either</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance-based language models. In Proc. of the 24th Annual International ACM-SIGIR Conference, pages 120–127, New Orleans, Louisiana, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Losada</author>
</authors>
<title>Language modeling for sentence retrieval: A comparison between multiple-Bernoulli and multinomial models.</title>
<date>2005</date>
<booktitle>In Information Retrieval and Theory Workshop,</booktitle>
<location>Glasgow, United Kingdom.</location>
<contexts>
<context position="4348" citStr="Losada, 2005" startWordPosition="693" endWordPosition="694">ural Language Processing (EMNLP 2006), pages 345–354, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 5 explains the data we used for our experiments, and gives our experimental results. Section 6 concludes the paper. 2 Related Work Some efforts for the TREC Novelty Track were related to our work. Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al., 2003; Losada, 2005; Murdock and Croft, 2005). Amongst the participants in the TREC Novelty Track, only (Kim et al., 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval). We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is the first work o</context>
</contexts>
<marker>Losada, 2005</marker>
<rawString>David E. Losada. 2005. Language modeling for sentence retrieval: A comparison between multiple-Bernoulli and multinomial models. In Information Retrieval and Theory Workshop, Glasgow, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Murdock</author>
<author>W Bruce Croft</author>
</authors>
<title>A translation model for sentence retrieval.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<pages>684--691</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4374" citStr="Murdock and Croft, 2005" startWordPosition="695" endWordPosition="698">Processing (EMNLP 2006), pages 345–354, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 5 explains the data we used for our experiments, and gives our experimental results. Section 6 concludes the paper. 2 Related Work Some efforts for the TREC Novelty Track were related to our work. Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al., 2003; Losada, 2005; Murdock and Croft, 2005). Amongst the participants in the TREC Novelty Track, only (Kim et al., 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval). We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is the first work on sentiment retrieval, to </context>
</contexts>
<marker>Murdock, Croft, 2005</marker>
<rawString>Vanessa Murdock and W. Bruce Croft. 2005. A translation model for sentence retrieval. In Proc. of HLT/EMNLP 2005, pages 684–691, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: Capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proc. of the 2nd International Conference on Knowledge Capture,</booktitle>
<pages>70--77</pages>
<location>Sanibel Island, Florida, USA.</location>
<contexts>
<context position="5157" citStr="Nasukawa and Yi, 2003" startWordPosition="819" endWordPosition="822">of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval). We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is the first work on sentiment retrieval, to the best of our knowledge. In the context of sentiment classification, some researchers have conducted studies on the topic dependence of sentiment polarities. (Nasukawa and Yi, 2003) and (Yi et al., 2003) extracted positive or negative expressions on a given product name using handmade lexicons. (Engstr¨om, 2004) studied how the topic dependence influences the accuracy of sentiment classification and attempted to reduce the influence to improve the accuracy. (Wilson et al., 2005) investigated how context influences sentiment polarity at the phrase level in a corpus, beginning with a predefined list of words with polarities. Their focus on the phenomena of topic dependence of sentiment can be shared with our work; however, their work is not directly related to ours, becaus</context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: Capturing favorability using natural language processing. In Proc. of the 2nd International Conference on Knowledge Capture, pages 70–77, Sanibel Island, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Matthew Hurst</author>
</authors>
<title>Computing Attitude and Affect in Text: Theory and Applications, chapter Towards a Robust Metric of Opinion.</title>
<date>2005</date>
<publisher>Springer.</publisher>
<contexts>
<context position="1620" citStr="Nigam and Hurst, 2005" startWordPosition="241" endWordPosition="244">ction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data. The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al., 2005). A number of studies have investigated sentiment classification at document level, e.g., (Pang et al., 2002; Dave et al., 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable. Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users. We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic. Intuitively, the expression of sentiment in text is dependent on the topic. For example, a negative view for some voting event may be expressed using ‘flaw’, while a negative view for some politician may be expressed using ‘rec</context>
</contexts>
<marker>Nigam, Hurst, 2005</marker>
<rawString>Kamal Nigam and Matthew Hurst, 2005. Computing Attitude and Affect in Text: Theory and Applications, chapter Towards a Robust Metric of Opinion. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1508" citStr="Pang et al., 2002" startWordPosition="220" endWordPosition="223">ering the topic dependence of the sentiment. Our experiments prove that our models are effective. 1 Introduction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data. The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al., 2005). A number of studies have investigated sentiment classification at document level, e.g., (Pang et al., 2002; Dave et al., 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable. Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users. We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic. Intuitively, the expression of sentiment in text is dependent on the topic. For example, a negative view for some v</context>
<context position="25126" citStr="Pang et al., 2002" startWordPosition="4099" endWordPosition="4102">with 1667 positive and 2129 negative words, which we will use hereafter in this paper. The sentiment polarities are sometimes sensitive to the structural information, for instance, a negation expression reverses the following sentiment polarity. To handle negation, every sentiment-bearing word was rewritten with a ‘NEG’ suffix, such as ‘good NEG’, if an odd number of negation expressions was found within the five preceding words in the sentence. To detect negation expressions, we used a predefined negation expression list. This negation handling is similar to that used in (Das and Chen, 2001; Pang et al., 2002). We extracted sentiment-bearing expressions using the list of words with established po6We used the topic labels attached to the MPQA corpus as the topic query termsq&apos;in all the experiments in Sections 5.3 and 5.4. 7We extracted positive and negative words from the General Inquirer basically in the same manner as in (Turney and Littman, 2003); however, we did not exclude any words, unlike (Turney and Littman, 2003), where some seed words were excluded for the evaluation of their work. 350 Table 1: Sample probabilities from the sentiment relevance models Reaction to President Bush’s 2002 presi</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79–86, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of the 21st Annual International ACM-SIGIR Conference,</booktitle>
<pages>275--281</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="17678" citStr="Ponte and Croft, 1998" startWordPosition="2854" endWordPosition="2857">hich we retrieved sentences in response to the query. In the second task, we split the corpus into two parts: (i) the training set, which was used for tuning the model parameters; and (ii) the testing set, which was used for constructing Rs and Rt and from which we retrieved sentences in response to queries. The testing set was identical in both tasks. Note that the sentiment relevance model Rs can be constructed in a topic-dependent fashion for both tasks. 4.2 Variations of the retrieval model slm: the retrieval model as described in Section 3.3. lmt: the standard language modeling approach (Ponte and Croft, 1998; Song and Croft, 1999) on the topic keywords qt for the topic part of the text wt. lms: the standard language modeling approach on the sentiment keywords qs for the sentiment part of the text ws. base: the weighted linear combination of lmt and lms. rmt: only the topic relevance model was used for ranking using qt and for wt 3 rms: only the sentiment relevance model was used for ranking using qs and for ws. rmt-base: the slm model with a = 1, ignoring the sentiment relevance model. rms-base: the slm model witha=0, ignoring the topic relevance model. 2Because the training set was used for tuni</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proc. of the 21st Annual International ACM-SIGIR Conference, pages 275–281, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<title>Computing attitude and affect in text.</title>
<date>2005</date>
<editor>James Shanahan, Yan Qu, and Janyce Wiebe, editors.</editor>
<publisher>Springer.</publisher>
<marker>2005</marker>
<rawString>James Shanahan, Yan Qu, and Janyce Wiebe, editors. 2005. Computing attitude and affect in text. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Silverman</author>
</authors>
<title>Density Estimation for Statistics and Data Analysis,</title>
<date>1986</date>
<pages>75--94</pages>
<publisher>CRC Press.</publisher>
<contexts>
<context position="10970" citStr="Silverman, 1986" startWordPosition="1768" endWordPosition="1769"> mechanism is given by: m pt(wij) ifbij=T ps(wij) otherwise j=1 (1) The summation in equation (1) goes over all possible pairs of language modelspt;ps, but we can avoid integration by specifying a mass function 7() that assigns nonzero probabilities to a finite subset of points in IP x IP x IPx. We accomplish this by using a nonparametric estimate for 7r (), the details of which are provided below. 3.2.1 A nonparametric generative mass function We use a nonparametric estimate for7r(•;�;�), which makes our generative model similar to kernel-based density estimators orParzen-window classifiers (Silverman, 1986). The primary difference is that our model operates over discrete events (strings of words), and accordingly the mass function is defined over the space of distributions, rather than directly over the data points. Our estimate relies on a collection of paired observationsC=fwti;wsi;xi:i=1::ng, which represent statements for which we know which words are topic words(wti), and which are sentiment words (wsi). Each of these observations corresponds to a unique point pti ; psi ; px i in the space of paired distributions IP x IP x IPx , defined by the following coordinates: pxi(x)=Ax1x=xi+(1—Ax): p</context>
</contexts>
<marker>Silverman, 1986</marker>
<rawString>B. W. Silverman, 1986. Density Estimation for Statistics and Data Analysis, pages 75–94. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
<author>Donna Harman</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<journal>NIST Special Publication</journal>
<booktitle>In Proc. of the 12th Text Retrieval Conference,</booktitle>
<pages>38--53</pages>
<contexts>
<context position="4160" citStr="Soboroff and Harman, 2003" startWordPosition="657" endWordPosition="661">h is proposed here as a theoretical framework for our work. Section 4 describes the task definition and our sentiment retrieval model. 345 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 345–354, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 5 explains the data we used for our experiments, and gives our experimental results. Section 6 concludes the paper. 2 Related Work Some efforts for the TREC Novelty Track were related to our work. Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al., 2003; Losada, 2005; Murdock and Croft, 2005). Amongst the participants in the TREC Novelty Track, only (Kim et al., 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, </context>
</contexts>
<marker>Soboroff, Harman, 2003</marker>
<rawString>Ian Soboroff and Donna Harman. 2003. Overview of the TREC 2003 Novelty Track. In Proc. of the 12th Text Retrieval Conference, pages 38–53. NIST Special Publication 500-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
</authors>
<title>Overview of the TREC</title>
<date>2004</date>
<booktitle>In Proc. of the 13th Text Retrieval Conference. NIST Special Publication</booktitle>
<pages>500--261</pages>
<contexts>
<context position="4177" citStr="Soboroff, 2004" startWordPosition="662" endWordPosition="663">oretical framework for our work. Section 4 describes the task definition and our sentiment retrieval model. 345 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 345–354, Sydney, July 2006. c�2006 Association for Computational Linguistics Section 5 explains the data we used for our experiments, and gives our experimental results. Section 6 concludes the paper. 2 Related Work Some efforts for the TREC Novelty Track were related to our work. Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al., 2003; Losada, 2005; Murdock and Croft, 2005). Amongst the participants in the TREC Novelty Track, only (Kim et al., 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval</context>
</contexts>
<marker>Soboroff, 2004</marker>
<rawString>Ian Soboroff. 2004. Overview of the TREC 2004 Novelty Track. In Proc. of the 13th Text Retrieval Conference. NIST Special Publication 500-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Song</author>
<author>W Bruce Croft</author>
</authors>
<title>A general language model for information retrieval.</title>
<date>1999</date>
<booktitle>In Proc. of the 8th International Conference on Information and Knowledge Management,</booktitle>
<pages>316--321</pages>
<location>Kansas City, Missouri, USA.</location>
<contexts>
<context position="17701" citStr="Song and Croft, 1999" startWordPosition="2858" endWordPosition="2861">nces in response to the query. In the second task, we split the corpus into two parts: (i) the training set, which was used for tuning the model parameters; and (ii) the testing set, which was used for constructing Rs and Rt and from which we retrieved sentences in response to queries. The testing set was identical in both tasks. Note that the sentiment relevance model Rs can be constructed in a topic-dependent fashion for both tasks. 4.2 Variations of the retrieval model slm: the retrieval model as described in Section 3.3. lmt: the standard language modeling approach (Ponte and Croft, 1998; Song and Croft, 1999) on the topic keywords qt for the topic part of the text wt. lms: the standard language modeling approach on the sentiment keywords qs for the sentiment part of the text ws. base: the weighted linear combination of lmt and lms. rmt: only the topic relevance model was used for ranking using qt and for wt 3 rms: only the sentiment relevance model was used for ranking using qs and for ws. rmt-base: the slm model with a = 1, ignoring the sentiment relevance model. rms-base: the slm model witha=0, ignoring the topic relevance model. 2Because the training set was used for tuning the model parameters</context>
</contexts>
<marker>Song, Croft, 1999</marker>
<rawString>Fei Song and W. Bruce Croft. 1999. A general language model for information retrieval. In Proc. of the 8th International Conference on Information and Knowledge Management, pages 316–321, Kansas City, Missouri, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24296" citStr="Stone et al., 1966" startWordPosition="3963" endWordPosition="3966">eared. 5.2.2 Using automatic annotation In automatic extraction of sentiment expressions in this study, we detected sentiment-bearing words using lists of words with established polarities. At this stage, topic dependence was not considered; however, at the stage of sentiment modeling, the topic dependence can be reflected, as described in Sections 3 and 4. We first prepared a list of words indicating sentiments. We used Hatzivassiloglou and McKeown’s sentiment word list (Hatzivassiloglou and McKeown, 1997), which consists of 657 positive and 679 negative adjectives, and The General Inquirer (Stone et al., 1966), which contains 1621 positive and 1989 negative words.7 By merging these lists, we obtained 1947 positive and 2348 negative words. After stemming these words in the same manner as in Section 5.2.1, we were left with 1667 positive and 2129 negative words, which we will use hereafter in this paper. The sentiment polarities are sometimes sensitive to the structural information, for instance, a negation expression reverses the following sentiment polarity. To handle negation, every sentiment-bearing word was rewritten with a ‘NEG’ suffix, such as ‘good NEG’, if an odd number of negation expressio</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="25471" citStr="Turney and Littman, 2003" startWordPosition="4157" endWordPosition="4160">‘good NEG’, if an odd number of negation expressions was found within the five preceding words in the sentence. To detect negation expressions, we used a predefined negation expression list. This negation handling is similar to that used in (Das and Chen, 2001; Pang et al., 2002). We extracted sentiment-bearing expressions using the list of words with established po6We used the topic labels attached to the MPQA corpus as the topic query termsq&apos;in all the experiments in Sections 5.3 and 5.4. 7We extracted positive and negative words from the General Inquirer basically in the same manner as in (Turney and Littman, 2003); however, we did not exclude any words, unlike (Turney and Littman, 2003), where some seed words were excluded for the evaluation of their work. 350 Table 1: Sample probabilities from the sentiment relevance models Reaction to President Bush’s 2002 presidential election Israeli settlements in Topic-independent Topic-independent 2002 State of the Union Address in Zimbabwe Gaza and West Bank w/ manual annot. w/ automatic annot. w/ manual annot. w/ automatic annot. w/ manual annot. w/ automatic annot. w/ manual annot. w/ automatic annot. P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(w1Q)w P(</context>
<context position="35275" citStr="Turney and Littman, 2003" startWordPosition="5635" endWordPosition="5638">0.1716) 0.2318* (0.1802) ‘*’ indicates statistically significant improvement over rmtf wherep&lt;0.05with the twosided Wilcoxon signed-rank test. seed words: KAM, TUR and ORG. Each group consists of a positive word set q&apos;+) and a negative word set q&apos;_), as follows: KAM:q&apos;+)_{good}, andq&apos;_)_{bad}. TUR:q�(+)_{good, nice, excellent, positive, fortunate, correct, superior}, and q&apos;_) _ {bad, nasty, poor, negative, unfortunate, wrong, inferior}. ORG: q&apos;+) _ {support, demand, promise, want, hope}, and q&apos;_) _ {refuse, accuse, criticism, fear, reject}. KAM and TUR were used in (Kamps and Marx, 2002) and (Turney and Littman, 2003), respectively. We constructed ORG considering sentiment-bearing words that may frequently appear in newspaper articles. We experimented with the seed-based task, making use of each of these seed word groups, in the steps described in Section 4.1. For this purpose, we split the data into two parts: (i) 50% as the estimation data and (ii) 50% as the test data. The test results using manually annotated data and automatically annotated data are shown in Tables 4 and 5, respectively, where the scores were computed according to the bpref evaluation measure. Mean average precision values are also pr</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="2578" citStr="Turney, 2002" startWordPosition="397" endWordPosition="398">olarity on a certain topic. Intuitively, the expression of sentiment in text is dependent on the topic. For example, a negative view for some voting event may be expressed using ‘flaw’, while a negative view for some politician may be expressed using ‘reckless’. Moreover, sentiment polarities are also dependent on topics or domains. For example, the adjective ‘unpredictable’ may have a negative orientation in an automotive review, in a phrase such as ‘unpredictable steering’, but it could have a positive orientation in a movie review, in a phrase such as ‘unpredictable plot’, as mentioned in (Turney, 2002) in the context of his sentiment word detection. We propose sentiment retrieval models in the framework of generative language modeling, not only assuming query terms expressing a certain topic, but also assuming that the polarity of sentiment interest is specified by the user in some manner, where the topic dependence of the sentiment is considered. To the best of our knowledge, there have been no other studies on a retrieval model unifying both topic and sentiment, and further, there have been no other studies on sentiment retrieval. The sentiment information often appears as local in a docu</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="20821" citStr="Wiebe et al., 2005" startWordPosition="3384" endWordPosition="3387">), and were fixed at 0.9 for ranking as in equation (5) for our experiments in Section 5. Here, fit and Its were selected empirically according to the tasks described in Section 4.1. The model parameterain equation (5) was also selected empirically in the same manner. The number of ranked documents used in the relevance modelsRtandRs, in equation (4), was selected empirically in the same manner as above; however, we fixed the number of terms used in the relevance models as 1000. 5 Experiments 5.1 Data set and evaluation measure We used the MPQA Opinion Corpus version 1.2 (Wilson et al., 2005; Wiebe et al., 2005) to measure the effectiveness of our sentiment re349 trieval models. We summarize this data set as follows. •This corpus contains news articles collected from 187 different foreign and U.S. news sources from June 2001 to May 2002. The corpus contains 535 documents, a total of 11,114 sentences. •The majority of the articles are on 10 different topics, which are labeled at document level, but, in addition to these, a number of additional articles were randomly selected from a larger corpus of 270,000 documents. •Each article was manually annotated using an annotation scheme for opinions and othe</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2):0–0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5459" citStr="Wilson et al., 2005" startWordPosition="865" endWordPosition="868">ing to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is the first work on sentiment retrieval, to the best of our knowledge. In the context of sentiment classification, some researchers have conducted studies on the topic dependence of sentiment polarities. (Nasukawa and Yi, 2003) and (Yi et al., 2003) extracted positive or negative expressions on a given product name using handmade lexicons. (Engstr¨om, 2004) studied how the topic dependence influences the accuracy of sentiment classification and attempted to reduce the influence to improve the accuracy. (Wilson et al., 2005) investigated how context influences sentiment polarity at the phrase level in a corpus, beginning with a predefined list of words with polarities. Their focus on the phenomena of topic dependence of sentiment can be shared with our work; however, their work is not directly related to ours, because we focus on a different task, sentiment retrieval, where different approaches are required. 3 A Generative Model of Sentiment In this section we will provide a formal underpinning for our approach to sentiment retrieval. The approach is based on the generative paradigm: we describe a statistical pro</context>
<context position="20800" citStr="Wilson et al., 2005" startWordPosition="3380" endWordPosition="3383">ively, in equation (4), and were fixed at 0.9 for ranking as in equation (5) for our experiments in Section 5. Here, fit and Its were selected empirically according to the tasks described in Section 4.1. The model parameterain equation (5) was also selected empirically in the same manner. The number of ranked documents used in the relevance modelsRtandRs, in equation (4), was selected empirically in the same manner as above; however, we fixed the number of terms used in the relevance models as 1000. 5 Experiments 5.1 Data set and evaluation measure We used the MPQA Opinion Corpus version 1.2 (Wilson et al., 2005; Wiebe et al., 2005) to measure the effectiveness of our sentiment re349 trieval models. We summarize this data set as follows. •This corpus contains news articles collected from 187 different foreign and U.S. news sources from June 2001 to May 2002. The corpus contains 535 documents, a total of 11,114 sentences. •The majority of the articles are on 10 different topics, which are labeled at document level, but, in addition to these, a number of additional articles were randomly selected from a larger corpus of 270,000 documents. •Each article was manually annotated using an annotation scheme </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proc. of HLT/EMNLP 2005, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In Proc. of the 3rd IEEE International Conference on Data Mining,</booktitle>
<pages>427--434</pages>
<location>Melbourne, Florida, USA.</location>
<contexts>
<context position="5179" citStr="Yi et al., 2003" startWordPosition="824" endWordPosition="827">ive or negative polarities. They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval). We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval). Our work is the first work on sentiment retrieval, to the best of our knowledge. In the context of sentiment classification, some researchers have conducted studies on the topic dependence of sentiment polarities. (Nasukawa and Yi, 2003) and (Yi et al., 2003) extracted positive or negative expressions on a given product name using handmade lexicons. (Engstr¨om, 2004) studied how the topic dependence influences the accuracy of sentiment classification and attempted to reduce the influence to improve the accuracy. (Wilson et al., 2005) investigated how context influences sentiment polarity at the phrase level in a corpus, beginning with a predefined list of words with polarities. Their focus on the phenomena of topic dependence of sentiment can be shared with our work; however, their work is not directly related to ours, because we focus on a differ</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proc. of the 3rd IEEE International Conference on Data Mining, pages 427– 434, Melbourne, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. of the 24th AnnualInternational ACM-SIGIR Conference,</booktitle>
<pages>334--342</pages>
<location>New Orleans, Louisiana, USA.</location>
<contexts>
<context position="12134" citStr="Zhai and Lafferty, 2001" startWordPosition="1949" endWordPosition="1952">fined by the following coordinates: pxi(x)=Ax1x=xi+(1—Ax): psi(v)=�s#(v;wsi)=#(wsi)+(1��s)�sv pti(v)=�t#(v;wti)=#(wti)+(1��t)�tv Here,#(v;wti)represents the number of times the wordvwas observed in the topic part of statement i, the length of which is denoted by#(wti).ctv stands for the relative frequency of v in the topic part of the collection. The same definitions apply to the sentiment parameters # (v; wsi), # (wsi) and csv. The Boolean indicator function 1y returns one when the predicate y is true and zero otherwise. Metaparameters At, As and Ax specify the amount of Dirichlet smoothing (Zhai and Lafferty, 2001) applied to the topic, sentiment and polarity estimates respectively; values for these parameters are determined empirically. We define 7r(pt; ps px) to have mass 1n when its argument pt; ps px corresponds to some observation pti ; psi ; px i, and zero otherwise: 7(pt; ps; px) = 1 n 1pt=ptix1ps=psix1px=pxi: =1 Equation (3) maintains empirical dependencies between the topic language modelptand the sentiment modelps, because we assign nonzero prob7(pt;ps;px)px(xi) pt;ps;px 347 ability mass only to pairs of models that actually co-occur in our observations. 3.2.2 Limitations of the model Our mode</context>
<context position="20090" citStr="Zhai and Lafferty, 2001" startWordPosition="3257" endWordPosition="3260">ed using training data for the training-based task, but are constructed using test data for the seed-based task, as mentioned in Section 4.1. Therefore, the base model is only used for the training data, not for the test data, in the training-based task, while it can be performed for the test data in the case of the seed-based task. Moreover, the lms, lmtsf and rmtsf models are based on the premise of using seed words to specify sentiments, and so they are only applicable to the seed-based task. In the models described in this subsection,AtandAsin equation (2) were set to Dirichlet estimates (Zhai and Lafferty, 2001), (wti)=(#(wti) + ltt) and #(wsi)=(#(wsi) +Its) for the relevance models Rt and Rs, respectively, in equation (4), and were fixed at 0.9 for ranking as in equation (5) for our experiments in Section 5. Here, fit and Its were selected empirically according to the tasks described in Section 4.1. The model parameterain equation (5) was also selected empirically in the same manner. The number of ranked documents used in the relevance modelsRtandRs, in equation (4), was selected empirically in the same manner as above; however, we fixed the number of terms used in the relevance models as 1000. 5 Ex</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of the 24th AnnualInternational ACM-SIGIR Conference, pages 334–342, New Orleans, Louisiana, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
</authors>
<title>Risk Minimization and Language Modeling in Text Retrieval.</title>
<date>2002</date>
<institution>Carnegie Mellon University.</institution>
<note>PhD dissertation,</note>
<contexts>
<context position="15459" citStr="Zhai, 2002" startWordPosition="2489" endWordPosition="2490">rd: Rt (v)=P(qs ,qtÆv,qx), Rs(v)=P(qsÆv,qt qx) P(qs qt qx)P(qs,qt,qx). (4) Both the numerator and denominator are computed according to equation (1), with the mass function 7 () given by equations (3) and (2). We use the notation qÆv to denote appending word v to the string q. Estimation is done over the training corpus, which may or may not include numeric values of sentiment polarity.1 Once we have estimates for the topic and sentiment relevance models, we can rank testing statementswby their similarity toRtandRs. We rank statements using a variation of cross-entropy, which was proposed by (Zhai, 2002): Rs(v) log ps(v). (5) Here the summations extend over all words v in the vocabulary, Rt and Rs are given by equation (4), whileptandpsare computed according to equation (2). A weighting parameteroallows us to change the balance of topic and sentiment in the final ranking formula; its value is selected empirically. 4 Sentiment Retrieval Task 4.1 Task definition We define two variations of the sentiment retrieval task. In one, the user supplies us with a numeric value for the desired polarityqx. In the other, the user supplies a set of seed wordsqs, reflecting the desired sentiment. The first t</context>
</contexts>
<marker>Zhai, 2002</marker>
<rawString>Chengxiang Zhai. 2002. Risk Minimization and Language Modeling in Text Retrieval. PhD dissertation, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>