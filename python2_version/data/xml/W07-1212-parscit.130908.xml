<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000427">
<title confidence="0.991827">
Creating a Systemic Functional Grammar Corpus from the Penn Treebank
</title>
<author confidence="0.998268">
Matthew Honnibal and James R. Curran
</author>
<affiliation confidence="0.9983305">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.459222">
NSW 2006, Australia
</address>
<email confidence="0.994866">
{mhonn, james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.998536" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954833333333">
The lack of a large annotated systemic func-
tional grammar (SFG) corpus has posed a
significant challenge for the development of
the theory. Automating SFG annotation is
challenging because the theory uses a mini-
mal constituency model, allocating as much
of the work as possible to a set of hierarchi-
cally organised features.
In this paper we show that despite the un-
orthodox organisation of SFG, adapting ex-
isting resources remains the most practical
way to create an annotated corpus. We
present and analyse SFGBank, an automated
conversion of the Penn Treebank into sys-
temic functional grammar. The corpus is
comparable to those available for other lin-
guistic theories, offering many opportunities
for new research.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999837538461538">
Systemic functional grammar (Halliday and
Matthiessen, 2004) aims to describe the set of
meaningful choices a speaker makes when putting a
thought into words. Each of these choices is seen as
a resource for shaping the meaning in a particular
way, and the selection will have a distinct grammat-
ical outcome as well as a semantic implication. The
choices are presented hierarchically, so that early
selections restrict other choices. For instance, if a
speaker chooses imperative mood for a clause, they
cannot choose a tense. Each selection is linked to a
syntactic expression rule. When imperative mood
is selected, the subject of the clause is suppressed;
</bodyText>
<page confidence="0.996859">
89
</page>
<bodyText confidence="0.999583914285714">
when interrogative mood is selected, the order of
the subject and first auxiliary are reversed.
Systemic grammars are very different from gram-
mars influenced by the formalist tradition. Systemic
analysis locates a constituent within a typology, and
yields a set of features that describe its salient prop-
erties. These features have proven useful for re-
search in applied linguistics, on topics such as stylis-
tics, discourse analysis and translation. As a gener-
ative theory, systemic grammars are less effective.
There have been a few attempts, such as those dis-
cussed by O’Donnell and Bateman (2005), but as yet
a wide coverage systemic grammar that can be used
for tractable parsing has not been developed.
The lack of a corpus and parser has limited re-
search on systemic grammars, as corpus studies have
been restricted to small samples of manually coded
examples, or imprecise queries of unannotated data.
The corpus we present, obtained by converting the
Penn Treebank, addresses this issue. It also suggests
a way to automatically code novel text, by convert-
ing the output of a parser for a different formalism.
This would also allow the use of SFG features for
NLP applications to be explored, and support current
research using SFG for applied linguistics.
The conversion process relies on a set of manually
coded rules. The first step of the process is to col-
lect SFG clauses and their constituents from parses in
the Penn Treebank. Each clause constituent is then
assigned up to three function labels, for the three si-
multaneous semantic and pragmatic structures Hal-
liday (1970) describes. Finally, the system features
are calculated, using rules referring to the function
labels assigned in the previous step. This paper ex-
tends the work described in Honnibal (2004).
</bodyText>
<note confidence="0.924524">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 89–96,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999109" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999907590909091">
Converting the Penn Treebank is the standard ap-
proach to creating a corpus annotated according to a
specific linguistic theory. This has been the method
used to create LTAG (Frank, 2001), LFG (Frank
et al., 2003) and CCG (Hockenmaier and Steedman,
2005) corpora, among others. We employ a similar
methodology, converting the corpus using manually
specified rules.
Since the SFG annotation is semantically oriented,
the work also bears some resemblance to Prop-
bank (Palmer et al., 2005). However, Propbank is
concerned with manually adding information to the
Penn Treebank, rather than automatically reinter-
preting the same information through the lens of a
different linguistic theory.
We chose not to base our conversion on the Prop-
bank annotation, as it does not currently cover the
Brown or Switchboard sections of the Treebank.
The wider variety of genres provided by these sec-
tions makes the corpus much more useful for SFG,
since the theory devotes significant attention to prag-
matic phenomena and stylistic variation.
</bodyText>
<sectionHeader confidence="0.985763" genericHeader="method">
3 Systemic Functional Grammar
</sectionHeader>
<bodyText confidence="0.999798222222222">
Generating a constituent using a systemic func-
tional grammar involves traversing a decision-tree-
like structure referred to as a system network. The
nodes of this tree are referred to as systems, and the
options from the systems are referred to as features.
At each system, the feature selected may add con-
straints on the type, number or order of the internal
structure of the constituent. When the entire net-
work has been traversed, the constraints are unified,
and the required constituents generated.
In order to annotate a sentence according to a sys-
temic functional grammar, we must specify the set
of features encountered as the system network is tra-
versed, and apply function labels to each constituent.
The function labeling is required because the con-
straints are always specified according to the child
constituents’ function, rather than their form.
Constituents may have more than one function
label, as SFG describes three metafunctions, fol-
lowing Halliday’s (1969) argument that a clause is
structured simultaneously as a communicative act, a
piece of information, and a representation of reality.
Interpersonal function labels are assigned to clause
constituents in determining the clause’s communica-
tive status. The most important interpersonal func-
tions are Subject and Finite, since the relative posi-
tion of the constituents bearing these labels largely
determines whether the clause will be a question,
statement or command.
The textual structure of the clause includes the
functions Theme and Rheme, following Halliday’s
(1970) theory of information structure.
Finally, the experiential function of a constituent
is its semantic role, described in terms of a small
set of labels that are only minimally sensitive to the
semantics of the predicate.
</bodyText>
<sectionHeader confidence="0.988368" genericHeader="method">
4 Annotation Implemented
</sectionHeader>
<bodyText confidence="0.999991045454545">
We base our annotation on the clause network in
the Nigel grammar (Mann and Matthiessen, 1983),
as it is freely available and discussed at length in
Matthiessen (1995). It is difficult to include annota-
tion from the group and phrase networks, because of
the flat bracketing of constituents in the Penn Tree-
bank. The converted corpus has full coverage over
all sections of the Penn Treebank 3 corpus.
We implement features from 41 systems from the
clause network, out of a possible 62. The most
prominent missing features relate to process type.
The process type system classifies clauses as one of
four broad semantic types: material, mental, verbal
or relational, with subsequent systems making finer
grained distinctions. This is mostly determined by
the argument structure of the verb, but also depends
on its lexical semantics. Process type assignment
therefore suffers from word sense ambiguity, so we
are unable to select from this system or others which
depend on its result. Figure 1 gives an example of
a clause with interpersonal, textual and experiential
function labels applied to its constituents.
</bodyText>
<sectionHeader confidence="0.954992" genericHeader="method">
5 Creating the Corpus
</sectionHeader>
<bodyText confidence="0.999916857142857">
SFG specifies the structure of a clause from ‘above’,
by setting constraints that are imposed by the set of
features selected from the system network. These
constraints describe the structure in terms of inter-
personal, textual and experiential function labels.
These functions then determine the boundaries of
the clause, by specifying its constituents.
</bodyText>
<page confidence="0.996058">
90
</page>
<table confidence="0.999837857142857">
Constituent Interpersonal Textual Ideational
and – Txt. Theme –
last year Adjunct Top. Theme Circumstance
prices Subject Rheme Participant
were Finite Rheme –
quickly Adjunct Rheme Circumstance
plummeting Predicator Rheme Process
</table>
<tableCaption confidence="0.999613">
Table 1: SFG function labels assigned to clause constituents.
</tableCaption>
<figure confidence="0.843324">
preprocess(parse)
clauses = []
for word in parse.words():
if isPredicate(word):
constituents = getConstituents(word)
clauses.append(constituents)
</figure>
<figureCaption confidence="0.999445">
Figure 2: Conversion algorithm.
</figureCaption>
<bodyText confidence="0.999940666666667">
The Penn Treebank provides rich syntactic trees,
specifying the structure of the sentence. We there-
fore proceed from ‘below’, using the Penn Treebank
to find clauses and their constituents, then applying
function labels to them, and using the function labels
as the basis for rules to traverse the system network.
</bodyText>
<subsectionHeader confidence="0.997844">
5.1 Finding Constituents
</subsectionHeader>
<bodyText confidence="0.999993769230769">
In this stage, we search the Treebank parse for
SFG clauses, and collect their constituents. Clauses
are identified by searching for predicates that head
them, and constituents are collecting by traversing
upwards from the predicate, collecting the nodes’
siblings until we hit an S node.
There are a few common constructions which
present problems for one or both of these pro-
cedures. These exceptions are handled by pre-
processing the Treebank tree, changing its structure
to be compatible with the predicate and constituent
extraction algorithms. Figure 2 describes the con-
version process more formally.
</bodyText>
<subsectionHeader confidence="0.632705">
5.1.1 Finding predicates
</subsectionHeader>
<bodyText confidence="0.9313001">
A predicate is the main verb in the clause. In the
Treebank annotation, the predicate will be the word
attached to the lowest node in a VP chain, because
auxiliaries attach higher up. Figure 3 describes the
function to decide whether a word is a predicate. Es-
sentially, we want words that are the last word at-
tached to a VP, that do not have a VP sibling.
Figure 1 marks the predicates and constituents in
a Treebank parse. The predicates are underlined, and
the constituents numbered to match the predicate.
</bodyText>
<construct confidence="0.895015875">
if verb.parent.label == ’VP’:
for sibling in verb.parent.children:
if sibling.isWord():
if sibling.offset &gt; verb.offset:
return False
if sibling.label == ’VP’:
return False
return True
</construct>
<figureCaption confidence="0.974835">
Figure 3: Determining whether a word is a predicate.
</figureCaption>
<construct confidence="0.8942197">
node = predicate
constituents = [predicate]
while node.label not in clauseLabels:
for sibling in node.parent.children:
if sibling != node:
constituents.append(sibling)
for sibling in node.parent.children:
if sibling != node
and sibling.label in conjOrWHLabels:
constituents.append(sibling)
</construct>
<figureCaption confidence="0.99734">
Figure 4: Finding constituents.
</figureCaption>
<subsubsectionHeader confidence="0.683664">
5.1.2 Getting Constituents
</subsubsectionHeader>
<bodyText confidence="0.999990875">
Once we have a predicate, we can traverse the tree
around it to collect the constituents in the clause it
heads. We do this by collecting its siblings and mov-
ing up the tree, collecting the ‘uncle’ nodes, until we
hit the top of the clause. Figure 4 describes the pro-
cess more formally. The final loop collects conjunc-
tions and WH constituents that attach alongside the
clause node, such as the ‘which’ in Figure 1.
</bodyText>
<subsectionHeader confidence="0.887035">
5.1.3 Pre-processing Ellipsis and Gapping
</subsectionHeader>
<bodyText confidence="0.999971909090909">
Ellipsis and gapping involve two or more pred-
icates sharing some constituents. When the shar-
ing can be denoted using the tree structure, by plac-
ing the shared items above the point where the VPs
fork, we refer to the construction as ellipsis. Figure
5 shows a sentence with a subject and an auxiliary
shared between two predicates. 3.4% of predicates
share at least one constituent with another clause via
ellipsis. We pre-process ellipsis constructions by in-
serting an S node above each VP after the first, and
adding traces for the shared constituents.
</bodyText>
<page confidence="0.980182">
91
</page>
<figure confidence="0.8488241875">
S
������� ��� �� ������������
����� �����
S
NP
� ��
�
The plant
NP 1
SBAR ����
�� ��
WHNP 2
VP
���� ����
was 1 VP ����
�� � �
</figure>
<table confidence="0.96426325">
employed 1 S-PRP 1
which VP PPP VP
� � � �� � ���
is 2 VP to 3 VP
�� � ��� ��
owned 2 PP 2 � �
�� � ��� make 3 NP 3
by Vose Co them
</table>
<figureCaption confidence="0.998988">
Figure 1: A parse tree with predicates underlined and constituents numbered.
</figureCaption>
<bodyText confidence="0.9998628">
In gapping constructions, the shared constituent
is the predicate itself, and what differs between the
two clauses are the arguments. The Treebank uses
special trace rules to describe which arguments must
be copied across to the gapped clause. We create
traces to the shared constituents and add them to
each gapped clause, so that the trace of the verb will
be picked up as a predicate later on. Gapping is a
very rare phenomenon – only 0.02% clauses have
gapped predicates.
</bodyText>
<subsubsectionHeader confidence="0.854238">
5.1.4 Pre-processing Semi-auxiliaries
</subsubsectionHeader>
<bodyText confidence="0.999471368421052">
In Figure 6 the verb ‘continue’ will match our
rules for predicate extraction, described in Section
5.1. SFG analyses this and other ‘semi-auxiliaries’
(Quirk et al., 1991) as a serial verb construction,
rather than a matrix clause and a complement clause.
Since we want to treat the finite verb as though it
were an auxiliary, we pre-process these cases by
simply deleting the S node, and attaching its chil-
dren directly to the semi-auxiliary’s VP.
Defining the semi-auxiliary constructions is not
so simple, however. Quirk et al. note that some
of these verbs are more like auxiliaries than others,
and organise them into a rough gradient according
to their formal properties. The problem is that there
is not clear agreement in the SFG literature about
where the line should be drawn. Matthiessen (1995)
describes all non-finite sentential complements as
serial-verb constructions. Martin et al. (1997) argue
that verbs such as ‘want’ impose selectional restric-
</bodyText>
<figure confidence="0.9760625">
S
�� � PPP
NP VP
1�
Prices continue S
VP
/1\1
to rise
</figure>
<figureCaption confidence="0.9925905">
Figure 6: Treebank representation of a sentence with
a semi-auxiliary.
</figureCaption>
<bodyText confidence="0.997507823529412">
tions on the subject, and therefore should be treated
as full verbs with a clause complement. Other com-
promises are possible as well.
Using Matthiessen’s definition, we collect 5.3%
fewer predicates than if we treated all semi-
auxiliaries as main verbs. If the complement clause
has a different subject from the parent clause, when
the two are merged the new verb will seem to have
extra arguments. 58% of these mergings introduce
an extra argument in this way. For example,
Investors want the market to boom
will be analysed as though boom has two argu-
ments, investors and market. We prevent this from
occurring by adding an extra condition for merg-
ing clauses, stating that the subject of the embedded
clause should be a trace co-indexed with the subject
of the parent clause.
</bodyText>
<page confidence="0.853688">
92
</page>
<figure confidence="0.853522777777778">
S
✘ ✘✘✘✘ ❳❳❳❳❳
NP ✭✭✭✭ ✭✭✭VP ❤❤❤❤❤❤❤
Asbestos was VP
✭✭✭✭ ✭✭✭✭ ❤❤❤❤❤❤❤❤
❅
VP
✦ ❛❛❛
✦ ✦
used ✏PP PPPP
✏✏ ✏
in the early 1950s
and VP
✟ ✟ ❍❍
replaced PP
❩❩
✚✚
in 1956
</figure>
<figureCaption confidence="0.999592">
Figure 5: Treebank representation of ellipsis. Predicates are underlined, shared items are in bold.
</figureCaption>
<subsectionHeader confidence="0.997842">
5.2 Constituent functions
</subsectionHeader>
<bodyText confidence="0.999958857142857">
As discussed above, we attach up to three function
labels to each clause constituent, one for each meta-
function. The rules to do this rely on the order of
constituents and the function dash-tags in the Penn
Treebank. Some experiential function rules also re-
fer to interpersonal labels, and some textual function
rules refer to experiential labels.
</bodyText>
<subsectionHeader confidence="0.555601">
5.2.1 Interpersonal Function Labels
</subsectionHeader>
<bodyText confidence="0.999981117647059">
The possible interpersonal function labels we as-
sign are Subject, Complement, Adjunct, Finite, and
Predicator. The Finite and Predicator are the first
tensed verb, and the predicate respectively. If there
are no auxiliary verbs, Halliday and Matthiessen
(2004) describes the predicate as functioning both
as Finite and Predicator. Since this is the only case
in which a constituent would receive multiple labels
from a single metafunction, we instead assign the
single label Finite/Predicator.
For NPs, Subjects, Complements and Adjuncts
are distinguished using the Penn Treebank’s dash-
tag function labels. SFG always assigns preposi-
tional phrases the label Adjunct. All NP constituents
that are not marked with an adverbial function tag in
the Treebank are labeled Complement. Conjunctions
are not assigned interpersonal functions.
</bodyText>
<subsectionHeader confidence="0.537406">
5.2.2 Experiential Function Labels
</subsectionHeader>
<bodyText confidence="0.999550473684211">
The experiential function labels we assign are
Participant, Process and Circumstance. This is a
simplification of the function labels described by
Halliday and Matthiessen (2004), as Participants are
usually subdivided into what other linguistic theo-
ries refer to as semantic roles. SFG has its own se-
mantic role description, which relies on process type
features. For instance, Participants in a verbal pro-
cess like ‘say’ have the role options Sayer, Target,
Receiver and Verbiage.
Distinguishing process types requires a word
sense disambiguated corpus and a word sense sen-
sitive process type lexicon. While there is a signifi-
cant intersection between the Penn Treebank and the
Semcor word sense disambiguated corpus, there is
currently no suitable process type lexicon. Conse-
quently, Participants have not been subtyped. The
Process is simply the verb phrase, while the Subject
and Complements are Participants.
</bodyText>
<subsubsectionHeader confidence="0.486672">
5.2.3 Textual Function labels
</subsubsectionHeader>
<bodyText confidence="0.999935294117647">
The textual metafunction describes the informa-
tion structure of the clause. Halliday’s textual func-
tion labels are Textual Theme, Interpersonal Theme,
Topical Theme and Rheme. Theme and Rheme are
often referred to as Topic and Comment in other the-
ories of information structure (Vallduvi, 1993). The-
ories also disagree about exactly where to draw the
boundary between the two.
In Halliday’s theory, the Rheme begins after
the first full constituent with an experiential func-
tion label, and extends to the end of the clause.
The first constituent with an experiential function
is labeled Topical Theme. Constituents before it
are labeled either Interpersonal Theme or Textual
Theme. Auxiliaries and vocatives are labeled In-
terpersonal Theme, while conjunctions are labeled
Textual Theme.
</bodyText>
<page confidence="0.996806">
93
</page>
<table confidence="0.999940375">
System Null % Feature 1 Feature 2
clause class 0% major (86%) minor (13%)
agency 13% effective (52%) middle (34%)
conjunction 13% non-conjuncted (64%) conjuncted (21%)
finiteness 13% finite (67%) non-finite (19%)
polarity 13% positive (81%) negative (4%)
rank 13% ranking (66%) shifted (19%)
secondary/beta clause 13% false (58%) true (28%)
status 13% bound (45%) free (41%)
deicticity 32% temporal (60%) modal (7%)
person 32% non-interactant (54%) interactant (13%)
theme selection 32% unmarked (58%) marked (9%)
voice 47% active (45%) passive (6%)
embed type 80% nominal qualifier (15%) other qualifier (3%)
theme role 90% as adjunct (7%) as process (1%)
passive agency 93% non-agentive (5%) agentive (1%)
</table>
<tableCaption confidence="0.998642">
Table 2: Selected systems and how often their features are selected.
</tableCaption>
<subsectionHeader confidence="0.997307">
5.3 System Selections
</subsectionHeader>
<bodyText confidence="0.99997188">
As discussed above, the system features are organ-
ised into hierarchies, with every feature assuming a
null value unless its system’s entry condition is met.
We therefore approach the system network much
like a decision tree, using rules to control how the
network is traversed.
The rules used to traverse the network cannot be
explained here in full, as there are 41 such decision
functions currently implemented. Table 2 lists a few
of the systems we implement, along with how of-
ten their features are selected. Because the system
network is organised hierarchically, a selection will
not always be made from a given system, since the
‘entry condition’ may not be met. For instance, the
feature agency=effective is an entry condition for the
voice system, so if a clause is middle, no voice will
be selected. The Null % column describes how of-
ten the entry condition of the clause is not met. Sys-
tems further down the heirarchy will obviously be
relevant less often, as will systems which describe a
finer grained distinction for an already rare feature.
The following sections describe the system net-
work in terms of four general regions. The systems
within each region largely sub-categorise each other,
or relate to the same grammatical phenomenon.
</bodyText>
<subsectionHeader confidence="0.998688">
5.4 Mood systems
</subsectionHeader>
<bodyText confidence="0.999982838709677">
Assuming the clause is independent, the major mood
options are declarative, interrogative and imperative.
Deciding between these is quite simple: in interrog-
ative clauses, the Subject occurs after the first auxil-
iary. Imperative clauses have no Subject.
There are a few more granular systems for in-
terrogative clauses, recording whether the question
is polar or WH. If the clause is WH interrogative,
there are two further features recording whether the
requested constituent functions as Subject, Adjunct
or Complement. The values of these features are
quite simple to calculate, by finding the WH element
among the constituents and retrieving its interper-
sonal function.
If the clause is not imperative, there are systems
recording the person (first, second or third) of the
subject, and whether the first auxiliary is modal,
present tense, past tense, or future tense. SFG de-
scribes three tenses in English, regarding ‘will’ and
‘shall’ auxiliaries as future tense markers, rather
than modals.
If the clause is imperative, there is a further sys-
tem recording whether the clause is the ‘jussive’ im-
perative with ‘let’s’, an ‘oblative’ imperative with
‘let me’, or a second person imperative. If the im-
perative is second person, a further feature records
whether the ‘you’ is explicit or implied.
There are also features recording the ‘polarity’ of
the clause: whether it is positive or negative, and, if
negative, whether the negative marker is full-formed
or cliticised as -n’t.
</bodyText>
<subsectionHeader confidence="0.997287">
5.5 Voice systems
</subsectionHeader>
<bodyText confidence="0.9999412">
In the Nigel grammar, the first voice distinction
drawn is not between active and passive, but be-
tween transitive and intransitive clauses. Intransitive
clauses cannot be passivised, as there is no Comple-
ment to shift to Subject. It therefore makes sense to
</bodyText>
<page confidence="0.998526">
94
</page>
<bodyText confidence="0.9999625">
carve these off first. If the clause is transitive, an-
other system records whether it is active or passive.
The rules to draw this distinction simply look at the
verb phrase, checking whether the last auxiliary is a
form of the verb ‘be’ and the lexical verb has a past
participle part-of-speech tag. Finally, a further sys-
tem records whether passive clauses have an agent
introduced by ‘by’.
</bodyText>
<subsectionHeader confidence="0.997588">
5.6 Theme systems
</subsectionHeader>
<bodyText confidence="0.99999">
Theme systems record what occurs at the start of the
clause. Typically in English, the first major con-
stituent will be the logical subject, and hence also
the Topical Theme. A system records whether this is
or is not the case. If the clause is finite and the log-
ical subject is not the Topical Theme, the clause is
said to have a ‘marked’ theme. Verb phrase Topical
Themes are considered unmarked if the clause is im-
perative. A further system records whether the Top-
ical Theme is the logical object (as in passivisation),
or whether it is a fronted Adjunct. Passive clauses
may have a fronted Adjunct, so does not necessar-
ily have a logical object as Topical Theme. There
are two further systems recording whether the clause
has a Textual Theme and/or an Interpersonal Theme.
</bodyText>
<subsectionHeader confidence="0.985613">
5.7 Taxis systems
</subsectionHeader>
<bodyText confidence="0.998907681818182">
Taxis systems record dependency relationships be-
tween clauses. There are two types of information:
whether the attachment is made through coordina-
tion or subordination, and the semantic type of the
attachment. Broadly, semantic type is between ‘ex-
pansion’ and ‘projection’, projection being reported
(or quoted) speech or thought. A further system
records the subtype of expansion clauses, which is
quite a subtle distinction. Unfortunately Halliday
chose thoroughly unhelpful terminology for this dis-
tinction: his subtypes of expansion are elaboration,
enhancement and extension. Enhancing clauses are
essentially adverbial, and are almost always subor-
dinate. Extending clauses, by contrast, are approxi-
mately the ‘and’ relationship, and are almost always
coordinate. Elaborating clauses qualify or further
define the information in the clause they are attached
to. Elaborating clauses can be either subordinate
or coordinate. Subordinate elaborating clauses are
non-defining relative clauses, while coordinate elab-
orating clauses are usually introduced by a conjunc-
tive adjunct, like ‘particularly’.
</bodyText>
<sectionHeader confidence="0.998217" genericHeader="method">
6 Accuracy
</sectionHeader>
<bodyText confidence="0.999995730769231">
In order to evaluate the accuracy of the conversion
process, we manually evaluated the constituency
structure of a randomly selected sample of 200
clauses. The conversion heuristics were developed
on section 00 of the Wall Street Journal and section 2
of Switchboard, while the evaluation sentences were
sampled from the rest of the Penn Treebank.
We limited evaluation to the constituency conver-
sion process, in order to examine more clauses. The
function labels are calculated from the constituency
conversion, while the system features are calculated
from the function labels and other system features.
Since the system network is like a decision tree,
whether a feature is null-valued depends on prior
feature decisions. These dependencies in the anno-
tation mean that evaluating all of it involves some re-
dundancy. We therefore evaluated the constituency
structure, since it did not depend on any of the other
annotation, and the conversion heuristics involved in
calculating it were more complicated than those for
the function labels and system features.
In the 200 clause sample, we found three clauses
with faulty constituency structures. One of these
was the result of a part-of-speech tag error in the
Treebank. The other two errors were conjunctions
that were incorrectly replicated in ellipsis clauses.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999158">
The Penn Treebank was designed as a largely the-
ory neutral corpus. In deciding on an annotation
scheme, it emphasised the need to have its annota-
tors work quickly and consistent, rather than fidelity
to any particular linguistic theory (Marcus et al.,
1994).
The fact that it has been successfully converted to
so many other annotation schemes suggests that its
annotation is indeed consistent and fine grained. It
is therefore unsurprising that it is possible to con-
vert it to SFG as well. Despite historically different
concerns, SFG still fundamentally agrees with other
theories about which constructions are syntactically
distinct — it simply has an unorthodox strategy for
representing that variation, delegating more work to
</bodyText>
<page confidence="0.996131">
95
</page>
<bodyText confidence="0.999996028571428">
feature structures and less work to the syntactic rep-
resentation.
Now that a sizable SFG corpus has been created,
it can be put to use for linguistic and NLP research.
Linguistically, we suggest that it would be interest-
ing to use the corpus to explore some of the as-
sertions in the literature that have until now been
untestable. For instance, Halliday and Matthiessen
(2004) suggests that the motivation for passivisation
is largely structural — what comes first in a clause is
an important choice in English. This implies that the
combination of passive voice and a fronted adjunct
should be unlikely. There should be many such sim-
ple queries that can shed interesting light on abstract
claims in the literature.
Large annotated corpora are currently very impor-
tant for parsing research, making this work a vital
first step towards exploring whether SFG annotation
can be automated. The fact that Treebank parses can
be converted into SFG annotation suggests it can be,
although most parsers do not replicate the dash-tags
attached to Treebank labels, which are necessary to
distinguish SFG categories in our conversion system.
Even without automating annotation, the corpus
offers some potential for investigating how useful
SFG features are for NLP tasks. The Penn Treebank
includes texts from a variety of genres, including
newspaper text, literature and spoken dialogue. The
Switchboard section of the corpus also comes with
various demographic properties about the speakers,
and is over a million words. We therefore suggest
that gold standard SFG features could be used in
some simple document classification experiments,
such as predicting the gender or education level of
speakers in the Switchboard corpus.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991975">
We would like to thanks the anonymous review-
ers for their helpful comments. James Curran was
funded under ARC Discovery grants DP0453131
and DP0665973.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99954724">
Anette Frank. 2001. Treebank conversion: Converting the NE-
GRA treebank to an LTAG grammar. In Proceedings of the
EUROLAN Workshop on Multi-layer Corpus-based Analy-
sis. Iasi, Romania.
Anette Frank, Louisa Sadler, Josef van Genabith, and Andy
Way. 2003. From Treebank Resources To LFG F-Structures
- Automatic F-Structure Annotation of Treebank Trees and
CFGs extractedfrom Treebanks. Kluwer, Dordrecht.
Michael A. K. Halliday. 1969. Options and functions in the
English clause. Brno Studies in English, 8:82–88. Reprinted
in Halliday and Martin (eds.)(1981) Readings in Systemic
Linguistics, Batsford, London.
Michael A. K. Halliday. 1970. Language structure and language
function. In John Lyons, editor, New Horizons in Linguistics.
Penguin, Harmondsworth.
Michael A. K. Halliday and Christian M. I. M. Matthiessen.
2004. An Introduction to Functional Grammar. Edward
Arnold, London, third edition.
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank man-
ual. Technical Report MS-CIS-05-09, University of Penn-
sylvania.
Matthew Honnibal. 2004. Converting the Penn Treebank to
Systemic Sunctional Grammar. In Proceedings of the Aus-
tralasian Language Technology Workshop (ALTW04).
William C. Mann and Christian M. I. M. Matthiessen. 1983. An
overview of the Nigel text generation grammar. Technical
Report RR-83-113, USC/Information Sciences Institute.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus of
English: The Penn Treebank. Computational Linguistics,
19(2):313–330.
James R. Martin, Christian M. I. M. Matthiessen, and Clare
Painter. 1997. Working with Functional Grammar. Arnold,
London.
Christian Matthiessen. 1995. Lexicogrammatical Cartography.
International Language Sciences Publishers, Tokyo, Taipei
and Dallas.
Michael O’Donnell and John A. Bateman. 2005. SFL in com-
putational contexts: a contemporary history. In J. Webster,
R. Hasan, and C. M. I. M. Matthiessen, editors, Continu-
ing Discourse on Language: A functional perspective, pages
343–382. Equinox, London.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71–106.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan
Svartvik. 1991. A Grammar of Contemporary English.
Longman, London.
Enric Vallduvi. 1993. Information packing: A survey. Technical
Report HCRC/RP-44, Universiy of Edinburgh.
</reference>
<page confidence="0.998444">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.492484">
<title confidence="0.997528">Creating a Systemic Functional Grammar Corpus from the Penn Treebank</title>
<author confidence="0.985676">R Honnibal</author>
<affiliation confidence="0.986826">School of Information University of</affiliation>
<address confidence="0.514461">NSW 2006,</address>
<abstract confidence="0.998482789473684">The lack of a large annotated systemic funcgrammar corpus has posed a significant challenge for the development of theory. Automating is challenging because the theory uses a minimal constituency model, allocating as much of the work as possible to a set of hierarchically organised features. In this paper we show that despite the unorganisation of adapting existing resources remains the most practical way to create an annotated corpus. We present and analyse SFGBank, an automated conversion of the Penn Treebank into systemic functional grammar. The corpus is comparable to those available for other linguistic theories, offering many opportunities for new research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anette Frank</author>
</authors>
<title>Treebank conversion: Converting the NEGRA treebank to an LTAG grammar.</title>
<date>2001</date>
<booktitle>In Proceedings of the EUROLAN Workshop on Multi-layer Corpus-based Analysis. Iasi,</booktitle>
<location>Romania.</location>
<contexts>
<context position="3759" citStr="Frank, 2001" startWordPosition="597" endWordPosition="598">eous semantic and pragmatic structures Halliday (1970) describes. Finally, the system features are calculated, using rules referring to the function labels assigned in the previous step. This paper extends the work described in Honnibal (2004). Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 89–96, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hockenmaier and Steedman, 2005) corpora, among others. We employ a similar methodology, converting the corpus using manually specified rules. Since the SFG annotation is semantically oriented, the work also bears some resemblance to Propbank (Palmer et al., 2005). However, Propbank is concerned with manually adding information to the Penn Treebank, rather than automatically reinterpreting the same information through the lens of a different linguistic theory. We chose not to base our conversion on the Propbank annotation, as it does not currently cover the B</context>
</contexts>
<marker>Frank, 2001</marker>
<rawString>Anette Frank. 2001. Treebank conversion: Converting the NEGRA treebank to an LTAG grammar. In Proceedings of the EUROLAN Workshop on Multi-layer Corpus-based Analysis. Iasi, Romania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Louisa Sadler</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>From Treebank Resources To LFG F-Structures - Automatic F-Structure Annotation of Treebank Trees and CFGs extractedfrom Treebanks.</title>
<date>2003</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Frank, Sadler, van Genabith, Way, 2003</marker>
<rawString>Anette Frank, Louisa Sadler, Josef van Genabith, and Andy Way. 2003. From Treebank Resources To LFG F-Structures - Automatic F-Structure Annotation of Treebank Trees and CFGs extractedfrom Treebanks. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
</authors>
<date>1969</date>
<booktitle>Options and functions in the English clause. Brno Studies in English, 8:82–88. Reprinted in Halliday and Martin (eds.)(1981) Readings in Systemic Linguistics,</booktitle>
<location>Batsford, London.</location>
<marker>Halliday, 1969</marker>
<rawString>Michael A. K. Halliday. 1969. Options and functions in the English clause. Brno Studies in English, 8:82–88. Reprinted in Halliday and Martin (eds.)(1981) Readings in Systemic Linguistics, Batsford, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
</authors>
<title>Language structure and language function. In</title>
<date>1970</date>
<booktitle>New Horizons in Linguistics.</booktitle>
<editor>John Lyons, editor,</editor>
<publisher>Penguin, Harmondsworth.</publisher>
<contexts>
<context position="3201" citStr="Halliday (1970)" startWordPosition="511" endWordPosition="513">, addresses this issue. It also suggests a way to automatically code novel text, by converting the output of a parser for a different formalism. This would also allow the use of SFG features for NLP applications to be explored, and support current research using SFG for applied linguistics. The conversion process relies on a set of manually coded rules. The first step of the process is to collect SFG clauses and their constituents from parses in the Penn Treebank. Each clause constituent is then assigned up to three function labels, for the three simultaneous semantic and pragmatic structures Halliday (1970) describes. Finally, the system features are calculated, using rules referring to the function labels assigned in the previous step. This paper extends the work described in Honnibal (2004). Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 89–96, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hocken</context>
</contexts>
<marker>Halliday, 1970</marker>
<rawString>Michael A. K. Halliday. 1970. Language structure and language function. In John Lyons, editor, New Horizons in Linguistics. Penguin, Harmondsworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Christian M I M Matthiessen</author>
</authors>
<title>An Introduction to Functional Grammar. Edward Arnold, London, third edition.</title>
<date>2004</date>
<contexts>
<context position="1014" citStr="Halliday and Matthiessen, 2004" startWordPosition="149" endWordPosition="152">ion is challenging because the theory uses a minimal constituency model, allocating as much of the work as possible to a set of hierarchically organised features. In this paper we show that despite the unorthodox organisation of SFG, adapting existing resources remains the most practical way to create an annotated corpus. We present and analyse SFGBank, an automated conversion of the Penn Treebank into systemic functional grammar. The corpus is comparable to those available for other linguistic theories, offering many opportunities for new research. 1 Introduction Systemic functional grammar (Halliday and Matthiessen, 2004) aims to describe the set of meaningful choices a speaker makes when putting a thought into words. Each of these choices is seen as a resource for shaping the meaning in a particular way, and the selection will have a distinct grammatical outcome as well as a semantic implication. The choices are presented hierarchically, so that early selections restrict other choices. For instance, if a speaker chooses imperative mood for a clause, they cannot choose a tense. Each selection is linked to a syntactic expression rule. When imperative mood is selected, the subject of the clause is suppressed; 89</context>
<context position="15116" citStr="Halliday and Matthiessen (2004)" startWordPosition="2428" endWordPosition="2431">above, we attach up to three function labels to each clause constituent, one for each metafunction. The rules to do this rely on the order of constituents and the function dash-tags in the Penn Treebank. Some experiential function rules also refer to interpersonal labels, and some textual function rules refer to experiential labels. 5.2.1 Interpersonal Function Labels The possible interpersonal function labels we assign are Subject, Complement, Adjunct, Finite, and Predicator. The Finite and Predicator are the first tensed verb, and the predicate respectively. If there are no auxiliary verbs, Halliday and Matthiessen (2004) describes the predicate as functioning both as Finite and Predicator. Since this is the only case in which a constituent would receive multiple labels from a single metafunction, we instead assign the single label Finite/Predicator. For NPs, Subjects, Complements and Adjuncts are distinguished using the Penn Treebank’s dashtag function labels. SFG always assigns prepositional phrases the label Adjunct. All NP constituents that are not marked with an adverbial function tag in the Treebank are labeled Complement. Conjunctions are not assigned interpersonal functions. 5.2.2 Experiential Function</context>
<context position="26046" citStr="Halliday and Matthiessen (2004)" startWordPosition="4147" endWordPosition="4150">G as well. Despite historically different concerns, SFG still fundamentally agrees with other theories about which constructions are syntactically distinct — it simply has an unorthodox strategy for representing that variation, delegating more work to 95 feature structures and less work to the syntactic representation. Now that a sizable SFG corpus has been created, it can be put to use for linguistic and NLP research. Linguistically, we suggest that it would be interesting to use the corpus to explore some of the assertions in the literature that have until now been untestable. For instance, Halliday and Matthiessen (2004) suggests that the motivation for passivisation is largely structural — what comes first in a clause is an important choice in English. This implies that the combination of passive voice and a fronted adjunct should be unlikely. There should be many such simple queries that can shed interesting light on abstract claims in the literature. Large annotated corpora are currently very important for parsing research, making this work a vital first step towards exploring whether SFG annotation can be automated. The fact that Treebank parses can be converted into SFG annotation suggests it can be, alt</context>
</contexts>
<marker>Halliday, Matthiessen, 2004</marker>
<rawString>Michael A. K. Halliday and Christian M. I. M. Matthiessen. 2004. An Introduction to Functional Grammar. Edward Arnold, London, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Ccgbank manual.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-09,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3826" citStr="Hockenmaier and Steedman, 2005" startWordPosition="606" endWordPosition="609">(1970) describes. Finally, the system features are calculated, using rules referring to the function labels assigned in the previous step. This paper extends the work described in Honnibal (2004). Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 89–96, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hockenmaier and Steedman, 2005) corpora, among others. We employ a similar methodology, converting the corpus using manually specified rules. Since the SFG annotation is semantically oriented, the work also bears some resemblance to Propbank (Palmer et al., 2005). However, Propbank is concerned with manually adding information to the Penn Treebank, rather than automatically reinterpreting the same information through the lens of a different linguistic theory. We chose not to base our conversion on the Propbank annotation, as it does not currently cover the Brown or Switchboard sections of the Treebank. The wider variety of </context>
</contexts>
<marker>Hockenmaier, Steedman, 2005</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2005. Ccgbank manual. Technical Report MS-CIS-05-09, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
</authors>
<title>Converting the Penn Treebank to Systemic Sunctional Grammar.</title>
<date>2004</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop (ALTW04).</booktitle>
<contexts>
<context position="3390" citStr="Honnibal (2004)" startWordPosition="541" endWordPosition="542">s for NLP applications to be explored, and support current research using SFG for applied linguistics. The conversion process relies on a set of manually coded rules. The first step of the process is to collect SFG clauses and their constituents from parses in the Penn Treebank. Each clause constituent is then assigned up to three function labels, for the three simultaneous semantic and pragmatic structures Halliday (1970) describes. Finally, the system features are calculated, using rules referring to the function labels assigned in the previous step. This paper extends the work described in Honnibal (2004). Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 89–96, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hockenmaier and Steedman, 2005) corpora, among others. We employ a similar methodology, converting the corpus using manually specified rules. Since the SFG annotation is semantically oriented, th</context>
</contexts>
<marker>Honnibal, 2004</marker>
<rawString>Matthew Honnibal. 2004. Converting the Penn Treebank to Systemic Sunctional Grammar. In Proceedings of the Australasian Language Technology Workshop (ALTW04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Christian M I M Matthiessen</author>
</authors>
<title>An overview of the Nigel text generation grammar.</title>
<date>1983</date>
<tech>Technical Report RR-83-113,</tech>
<institution>USC/Information Sciences Institute.</institution>
<contexts>
<context position="6514" citStr="Mann and Matthiessen, 1983" startWordPosition="1023" endWordPosition="1026"> functions are Subject and Finite, since the relative position of the constituents bearing these labels largely determines whether the clause will be a question, statement or command. The textual structure of the clause includes the functions Theme and Rheme, following Halliday’s (1970) theory of information structure. Finally, the experiential function of a constituent is its semantic role, described in terms of a small set of labels that are only minimally sensitive to the semantics of the predicate. 4 Annotation Implemented We base our annotation on the clause network in the Nigel grammar (Mann and Matthiessen, 1983), as it is freely available and discussed at length in Matthiessen (1995). It is difficult to include annotation from the group and phrase networks, because of the flat bracketing of constituents in the Penn Treebank. The converted corpus has full coverage over all sections of the Penn Treebank 3 corpus. We implement features from 41 systems from the clause network, out of a possible 62. The most prominent missing features relate to process type. The process type system classifies clauses as one of four broad semantic types: material, mental, verbal or relational, with subsequent systems makin</context>
</contexts>
<marker>Mann, Matthiessen, 1983</marker>
<rawString>William C. Mann and Christian M. I. M. Matthiessen. 1983. An overview of the Nigel text generation grammar. Technical Report RR-83-113, USC/Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="25192" citStr="Marcus et al., 1994" startWordPosition="4009" endWordPosition="4012">in calculating it were more complicated than those for the function labels and system features. In the 200 clause sample, we found three clauses with faulty constituency structures. One of these was the result of a part-of-speech tag error in the Treebank. The other two errors were conjunctions that were incorrectly replicated in ellipsis clauses. 7 Conclusion The Penn Treebank was designed as a largely theory neutral corpus. In deciding on an annotation scheme, it emphasised the need to have its annotators work quickly and consistent, rather than fidelity to any particular linguistic theory (Marcus et al., 1994). The fact that it has been successfully converted to so many other annotation schemes suggests that its annotation is indeed consistent and fine grained. It is therefore unsurprising that it is possible to convert it to SFG as well. Despite historically different concerns, SFG still fundamentally agrees with other theories about which constructions are syntactically distinct — it simply has an unorthodox strategy for representing that variation, delegating more work to 95 feature structures and less work to the syntactic representation. Now that a sizable SFG corpus has been created, it can b</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Martin</author>
<author>Christian M I M Matthiessen</author>
<author>Clare Painter</author>
</authors>
<title>Working with Functional Grammar.</title>
<date>1997</date>
<location>Arnold, London.</location>
<contexts>
<context position="13216" citStr="Martin et al. (1997)" startWordPosition="2105" endWordPosition="2108">inite verb as though it were an auxiliary, we pre-process these cases by simply deleting the S node, and attaching its children directly to the semi-auxiliary’s VP. Defining the semi-auxiliary constructions is not so simple, however. Quirk et al. note that some of these verbs are more like auxiliaries than others, and organise them into a rough gradient according to their formal properties. The problem is that there is not clear agreement in the SFG literature about where the line should be drawn. Matthiessen (1995) describes all non-finite sentential complements as serial-verb constructions. Martin et al. (1997) argue that verbs such as ‘want’ impose selectional restricS �� � PPP NP VP 1� Prices continue S VP /1\1 to rise Figure 6: Treebank representation of a sentence with a semi-auxiliary. tions on the subject, and therefore should be treated as full verbs with a clause complement. Other compromises are possible as well. Using Matthiessen’s definition, we collect 5.3% fewer predicates than if we treated all semiauxiliaries as main verbs. If the complement clause has a different subject from the parent clause, when the two are merged the new verb will seem to have extra arguments. 58% of these mergi</context>
</contexts>
<marker>Martin, Matthiessen, Painter, 1997</marker>
<rawString>James R. Martin, Christian M. I. M. Matthiessen, and Clare Painter. 1997. Working with Functional Grammar. Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Matthiessen</author>
</authors>
<date>1995</date>
<booktitle>Lexicogrammatical Cartography. International Language Sciences Publishers,</booktitle>
<location>Tokyo, Taipei and Dallas.</location>
<contexts>
<context position="6587" citStr="Matthiessen (1995)" startWordPosition="1037" endWordPosition="1038"> bearing these labels largely determines whether the clause will be a question, statement or command. The textual structure of the clause includes the functions Theme and Rheme, following Halliday’s (1970) theory of information structure. Finally, the experiential function of a constituent is its semantic role, described in terms of a small set of labels that are only minimally sensitive to the semantics of the predicate. 4 Annotation Implemented We base our annotation on the clause network in the Nigel grammar (Mann and Matthiessen, 1983), as it is freely available and discussed at length in Matthiessen (1995). It is difficult to include annotation from the group and phrase networks, because of the flat bracketing of constituents in the Penn Treebank. The converted corpus has full coverage over all sections of the Penn Treebank 3 corpus. We implement features from 41 systems from the clause network, out of a possible 62. The most prominent missing features relate to process type. The process type system classifies clauses as one of four broad semantic types: material, mental, verbal or relational, with subsequent systems making finer grained distinctions. This is mostly determined by the argument s</context>
<context position="13117" citStr="Matthiessen (1995)" startWordPosition="2095" endWordPosition="2096">b construction, rather than a matrix clause and a complement clause. Since we want to treat the finite verb as though it were an auxiliary, we pre-process these cases by simply deleting the S node, and attaching its children directly to the semi-auxiliary’s VP. Defining the semi-auxiliary constructions is not so simple, however. Quirk et al. note that some of these verbs are more like auxiliaries than others, and organise them into a rough gradient according to their formal properties. The problem is that there is not clear agreement in the SFG literature about where the line should be drawn. Matthiessen (1995) describes all non-finite sentential complements as serial-verb constructions. Martin et al. (1997) argue that verbs such as ‘want’ impose selectional restricS �� � PPP NP VP 1� Prices continue S VP /1\1 to rise Figure 6: Treebank representation of a sentence with a semi-auxiliary. tions on the subject, and therefore should be treated as full verbs with a clause complement. Other compromises are possible as well. Using Matthiessen’s definition, we collect 5.3% fewer predicates than if we treated all semiauxiliaries as main verbs. If the complement clause has a different subject from the parent</context>
</contexts>
<marker>Matthiessen, 1995</marker>
<rawString>Christian Matthiessen. 1995. Lexicogrammatical Cartography. International Language Sciences Publishers, Tokyo, Taipei and Dallas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael O’Donnell</author>
<author>John A Bateman</author>
</authors>
<title>SFL in computational contexts: a contemporary history. In</title>
<date>2005</date>
<booktitle>Continuing Discourse on Language: A functional perspective,</booktitle>
<pages>343--382</pages>
<editor>J. Webster, R. Hasan, and C. M. I. M. Matthiessen, editors,</editor>
<publisher>Equinox,</publisher>
<location>London.</location>
<marker>O’Donnell, Bateman, 2005</marker>
<rawString>Michael O’Donnell and John A. Bateman. 2005. SFL in computational contexts: a contemporary history. In J. Webster, R. Hasan, and C. M. I. M. Matthiessen, editors, Continuing Discourse on Language: A functional perspective, pages 343–382. Equinox, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4058" citStr="Palmer et al., 2005" startWordPosition="641" endWordPosition="644">inguistic Processing, pages 89–96, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics 2 Related Work Converting the Penn Treebank is the standard approach to creating a corpus annotated according to a specific linguistic theory. This has been the method used to create LTAG (Frank, 2001), LFG (Frank et al., 2003) and CCG (Hockenmaier and Steedman, 2005) corpora, among others. We employ a similar methodology, converting the corpus using manually specified rules. Since the SFG annotation is semantically oriented, the work also bears some resemblance to Propbank (Palmer et al., 2005). However, Propbank is concerned with manually adding information to the Penn Treebank, rather than automatically reinterpreting the same information through the lens of a different linguistic theory. We chose not to base our conversion on the Propbank annotation, as it does not currently cover the Brown or Switchboard sections of the Treebank. The wider variety of genres provided by these sections makes the corpus much more useful for SFG, since the theory devotes significant attention to pragmatic phenomena and stylistic variation. 3 Systemic Functional Grammar Generating a constituent using</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Grammar of Contemporary English.</title>
<date>1991</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="12483" citStr="Quirk et al., 1991" startWordPosition="1987" endWordPosition="1990">tself, and what differs between the two clauses are the arguments. The Treebank uses special trace rules to describe which arguments must be copied across to the gapped clause. We create traces to the shared constituents and add them to each gapped clause, so that the trace of the verb will be picked up as a predicate later on. Gapping is a very rare phenomenon – only 0.02% clauses have gapped predicates. 5.1.4 Pre-processing Semi-auxiliaries In Figure 6 the verb ‘continue’ will match our rules for predicate extraction, described in Section 5.1. SFG analyses this and other ‘semi-auxiliaries’ (Quirk et al., 1991) as a serial verb construction, rather than a matrix clause and a complement clause. Since we want to treat the finite verb as though it were an auxiliary, we pre-process these cases by simply deleting the S node, and attaching its children directly to the semi-auxiliary’s VP. Defining the semi-auxiliary constructions is not so simple, however. Quirk et al. note that some of these verbs are more like auxiliaries than others, and organise them into a rough gradient according to their formal properties. The problem is that there is not clear agreement in the SFG literature about where the line s</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1991</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1991. A Grammar of Contemporary English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enric Vallduvi</author>
</authors>
<title>Information packing: A survey.</title>
<date>1993</date>
<tech>Technical Report HCRC/RP-44,</tech>
<institution>Universiy of Edinburgh.</institution>
<contexts>
<context position="16963" citStr="Vallduvi, 1993" startWordPosition="2706" endWordPosition="2707">nificant intersection between the Penn Treebank and the Semcor word sense disambiguated corpus, there is currently no suitable process type lexicon. Consequently, Participants have not been subtyped. The Process is simply the verb phrase, while the Subject and Complements are Participants. 5.2.3 Textual Function labels The textual metafunction describes the information structure of the clause. Halliday’s textual function labels are Textual Theme, Interpersonal Theme, Topical Theme and Rheme. Theme and Rheme are often referred to as Topic and Comment in other theories of information structure (Vallduvi, 1993). Theories also disagree about exactly where to draw the boundary between the two. In Halliday’s theory, the Rheme begins after the first full constituent with an experiential function label, and extends to the end of the clause. The first constituent with an experiential function is labeled Topical Theme. Constituents before it are labeled either Interpersonal Theme or Textual Theme. Auxiliaries and vocatives are labeled Interpersonal Theme, while conjunctions are labeled Textual Theme. 93 System Null % Feature 1 Feature 2 clause class 0% major (86%) minor (13%) agency 13% effective (52%) mid</context>
</contexts>
<marker>Vallduvi, 1993</marker>
<rawString>Enric Vallduvi. 1993. Information packing: A survey. Technical Report HCRC/RP-44, Universiy of Edinburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>