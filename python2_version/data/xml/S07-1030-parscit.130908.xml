<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.098255">
<title confidence="0.995732">
FICO: Web Person Disambiguation Via Weighted Similarity
of Entity Contexts
</title>
<author confidence="0.925814">
Paul Kalmar
</author>
<affiliation confidence="0.832694">
Fair Isaac Corporation
</affiliation>
<address confidence="0.889616">
3661 Valley Centre Dr.
San Diego, CA 92130 USA
</address>
<email confidence="0.998271">
PaulKalmar@FairIsaac.com
</email>
<author confidence="0.808923">
Matthias Blume
</author>
<affiliation confidence="0.730544">
Fair Isaac Corporation
</affiliation>
<address confidence="0.8762395">
3661 Valley Centre Dr.
San Diego, CA 92130 USA
</address>
<email confidence="0.999254">
MatthiasBlume@FairIsaac.com
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998953545454545">
Entity disambiguation resolves the many-
to-many correspondence between mentions
of entities in text and unique real-world en-
tities. Fair Isaac’s entity disambiguation
uses language-independent entity context
to agglomeratively resolve mentions with
similar names to unique entities. This pa-
per describes Fair Isaac’s automatic entity
disambiguation capability and assesses its
performance on the SemEval 2007 Web
People Search task.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985166666667">
We use the term entity to mean a specific person or
object. A mention is a reference to an entity such
as a word or phrase in a document. Taken to-
gether, all mentions that refer to the same real-
world object model that entity (Mitchell et al.
2004). Entity disambiguation inherently involves
resolving many-to-many relationships. Multiple
distinct strings may refer to the same entity. Si-
multaneously, multiple identical mentions refer to
distinct entities (Bagga and Baldwin, 1998).
Fair Isaac’s entity disambiguation software is
based largely on language-independent algorithms
that resolve mentions in the context of the entire
corpus. The system utilizes multiple types of con-
text as evidence for determining whether two men-
tions correspond to the same entity and it auto-
matically learns the weight of evidence of each
context item via corpus statistics.
The goal of the Web People Search task (Artiles
et al. 2007) is to assign Web pages to groups,
where each group contains all (and only those)
pages that refer to one unique entity. A page is
assigned to multiple groups if it mentions multiple
entities, for example “John F. Kennedy” and the
“John F. Kennedy Library”. The pages were se-
lected via a set of keyword queries, and the disam-
biguation is evaluated only on those query entities.
This differs from Fair Isaac’s system in a few key
ways: our system deals with mentions rather than
documents, our system does not require a filter on
mentions, and our system is generally used for
large collections of documents containing very
many names rather than small sets of highly am-
biguous documents dealing with one specific
name. Nevertheless, it was possible to run the Fair
Isaac entity disambiguation system on the Web
People Search task data with almost no modifica-
tions and achieve accurate results.
The remaining sections of this paper describe
Fair Isaac’s automatic entity disambiguation meth-
odology and report on the performance of the sys-
tem on the WePS data.
</bodyText>
<sectionHeader confidence="0.993692" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999901625">
In unstructured text, each document provides a
natural context for entity disambiguation. After
cleaning up extraneous markup we carry out
within-document co-reference resolution, aggregat-
ing information about each entity mentioned in
each document. We then use these entity attributes
as features in determining which documents deal
with the same entity.
</bodyText>
<subsectionHeader confidence="0.99961">
2.1 Dealing with Raw Web Data
</subsectionHeader>
<bodyText confidence="0.9946395">
The first challenge in dealing with data from the
Web is to decide which documents are useful and
</bodyText>
<page confidence="0.986511">
149
</page>
<bodyText confidence="0.976792">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 149–152,
Prague, June 2007. c�2007 Association for Computational Linguistics
what text from those documents contains relevant
information. As a first pass, the first HTML file in
a folder which contained the query name was used
as the main page. In retrospect, it might have been
better to combine all portions of the page, or
choose the longest page. We copied the title ele-
ment and converted all text chunks to paragraphs,
eliminating all other HTML and script. If no
HTML was found in the directory for a page, the
first text file which contained the query was used
instead.
</bodyText>
<subsectionHeader confidence="0.996094">
2.2 Within-Document Disambiguation
</subsectionHeader>
<bodyText confidence="0.9999851">
When dealing with unstructured text, a named en-
tity recognition (NER) system provides the input to
the entity disambiguation. Due to time constraints
and that Persons are the entity type of primary in-
terest, any mention that matches one of the query
strings is automatically labeled as a Person, regard-
less of its actual type.
As described in Blume (2005), the system next
carries out entity type-specific parsing in order to
extract entity attributes such as titles, generate
standardized names (e.g. p_abdul_khan_p for “Dr.
Abdul Q. Khan”), and populate the data structures
(token hashes) that are used to perform the within-
document entity disambiguation.
We err on the side of not merging entities rather
than incorrectly merging entities. Looking at mul-
tiple documents provides additional statistics.
Thus, the cross-document disambiguation process
described in the next section will still merge some
entities even within individual documents.
</bodyText>
<subsectionHeader confidence="0.995076">
2.3 Cross-Document Disambiguation
</subsectionHeader>
<bodyText confidence="0.999927895522388">
Our cross-document entity disambiguation relies
on one key insight: an entity can be distinguished
by the company it keeps. If Abdul Khan 1 associ-
ates with different people and organizations at dif-
ferent locations than Abdul Khan 2, then he is
probably a different person. Furthermore, if it is
possible to compare two entities based on one type
of context, it is possible to compare them based on
every type of context.
Within each domain, we require a finite set of
context items. In the domains of co-occurring lo-
cations, organizations, and persons, these are the
standardized names derived in the entity informa-
tion extraction phase of within-document disam-
biguation. We use the logarithm of the inverse
name frequency (the number of standard person
names with which this context item appears), INF,
as a weight indicating the salience of each context
item. Co-occurrence with a common name pro-
vides less indication that two mentions correspond
to the same entity than co-occurrence with an un-
common name. To reduce noise, only entities that
occur within a given window of entities are in-
cluded in this vector. In all test runs, this window
is set to 10 entities on either side. Because of the
effects that small corpora have on statistics, we
added a large amount of newswire text to improve
frequency counts. Many of the query names would
have low frequency in a text corpus that is not
about them specifically, but have high frequency in
this task because each document contains at least
one mention of them. This would cause the INF
weight to incorrectly estimate the importance of
any token; adding additional documents to the dis-
ambiguation run reduces this effect and brings fre-
quency counts to more realistic levels.
We similarly count title tokens that occur with
the entity and compute INF weights for the title
tokens. Topic context, as described in Blume
(2005), was used in some post-submission runs.
We define a separate distance measure per con-
text domain. We are able to discount the co-
occurrence with multiple items as well as quantify
an unexpected lack of shared co-occurrence by
engineering each distance measure for each spe-
cific domain. The score produced by each distance
measure may be loosely interpreted as the log of
the likelihood of two randomly generated contexts
sharing the observed degree of similarity.
In addition to the context-based distance meas-
ures, we utilize a lexical (string) distance measure
based on exactly the same transformations as used
to compare strings for intra-document entity dis-
ambiguation plus the Soundex algorithm (Knuth
1998) to measure whether two name tokens sound
the same. A large negative score indicates a great
deal of similarity (log likelihood).
The process of cross-document entity disam-
biguation now boils down to repeatedly finding a
pair of entities, comparing them (computing the
sum of the above distance measures), and merging
them if the score exceeds some threshold. We
compute sets of keys based on lexical similarity
and compare only entities that are likely to match.
The WePS evaluation only deals with entities that
match a query. Thus, we added a new step of key
generation based on the query.
</bodyText>
<page confidence="0.996379">
150
</page>
<sectionHeader confidence="0.998051" genericHeader="method">
3 Performance
</sectionHeader>
<bodyText confidence="0.999831671641791">
We have tested our entity disambiguation system
on several semi-structured and unstructured text
data sets. Here, we report the performance on the
training data provided for the Web People Search
task. This corpus consists of raw Web pages with
substantial variation in capitalization, punctuation,
grammar, and spelling – characteristics that make
NER challenging. A few other issues also nega-
tively impact our performance, including extrane-
ous text, long lists of entities, and the issue of find-
ing the correct document to parse.
The NER process identified a ratio of approxi-
mately 220 mentions per document across 3,359
documents. Within-document entity disambigua-
tion reduced this to approximately 113 entities per
document, which we refer to as document-level
entities. Of these, 3,383 Persons (including those
Organizations and Locations which were relabeled
as Persons) contained a query name. Cross-
document entity disambiguation reduced this to
976 distinct persons with 721 distinct standardized
names. Thus, 2,407 merge operations were per-
formed in this step. On average, there are 48 men-
tions per query name. Our system found an aver-
age of 14 unique entities per query name. In the
gold standard, the average is 9 unique entities per
query name.
Looking at the names that matched in the out-
put, it is clear that NER is very important to the
process. Post submission of our initial run, we
used proper tokenization of punctuation and an
additional NER system, which corrected many
mistakes in the grouping of names. Also, many of
the names that were incorrectly merged would not
have been compared if not for the introduction of
the additional key that compares all mentions that
match a query name.
For the WePS evaluation submission, we con-
verted our results to document-level entities by
mapping each mention to the document that it was
part of and removing duplicates. If we did not find
a mention in a document, we labeled the document
as a singleton entity.
We also used a number of standard metrics for
our internal evaluation. Most of these operate on
document-level entities rather than on documents.
To convert the ground truth provided for the task
to a form usable for these metrics, we assume that
each entity contains all mentions in the corre-
sponding document group. These metrics test the
cross-document disambiguation rather than the
NER and within-document disambiguation. These
metrics should not be used to compare between
different versions of NER and within-document
disambiguation, since the ground truth used in the
evaluation is generated by these processes.
In Table 1, we compare a run with the additional
newswire data and the comparison key (our WePS
submission), leaving out the additional newswire
data and the additional comparison key, and leav-
ing out only the additional comparison key.
In Table 2, we compare runs based on the im-
proved NER (available only after the WePS sub-
mission deadline). The first uses the same parame-
ters as our submission, the second uses an in-
creased threshold, and the third utilizes the word
vector-based clustering (document topics).
</bodyText>
<table confidence="0.96927875">
Acc. Prec. Recall Harm. Purity
WithExtraKey 0.670 0.545 0.906 0.818
NoAddedData 0.743 0.752 0.584 0.841
NoExtraKey 0.770 0.767 0.624 0.861
</table>
<tableCaption confidence="0.6590066">
Table 1. Results of pairwise comparisons and clus-
terwise harmonic mean of purity and inverse purity
on various disambiguation runs. Each metric is
averaged across the individual results for every
query name.
</tableCaption>
<table confidence="0.9963915">
Acc. Prec. Recall Harm. Purity
WithExtraKey 0.690 0.618 0.552 0.815
1.25 Thresh 0.720 0.733 0.500 0.812
Topic Info 0.719 0.645 0.545 0.818
</table>
<tableCaption confidence="0.9992">
Table 2. Results based on improved named entity
</tableCaption>
<bodyText confidence="0.964934294117647">
recognition. These should not be directly com-
pared against those in Table 1, since the different
NER yields different ground truth for these evalua-
tion metrics.
Most of our metrics are based on pairwise com-
parisons – all document-level entities are compared
against all other document-level entities that match
the same query name, noting whether the pair was
coreferent in the results and in the ground truth.
With such comparison, we obtain measures includ-
ing precision, recall, and accuracy. In this training
data, depending on which NER is used, 35,000-
50,000 pairwise comparisons are possible.
We also define a clusterwise measure of the
harmonic mean between purity and inverse purity
with respect to mentions. This is different from the
metric provided by WePS, purity and inverse pu-
</bodyText>
<page confidence="0.995258">
151
</page>
<bodyText confidence="0.999959444444444">
rity at the document level. Since some documents
contain multiple entities, the latter metric does not
perform correctly. Mentions, on the other hand,
are always unique in our disambiguation. How-
ever, because the ground truth was specified at the
document level, documents containing multiple
entities that match a query yield ambiguous men-
tions. These decrease all purity-related scores
equally and do not vary between runs.
The addition of the newswire data improved re-
sults. Inclusion of an extra comparison based on
query name matches allowed for comparison of
entities with names that do not match the format of
person names, and only slightly reduced overall
performance. The new NER run can only be com-
pared on the last three runs. to the system per-
forms better with topic context than without it.
In comparison, in the 2005 Knowledge Discov-
ery and Dissemination (KD-D) Challenge Task
ER-1a (the main entity disambiguation task), we
achieved an accuracy of 94.5%. The margin of
error in the evaluation was estimated at 3% due to
errors in the “ground truth”. This was a pure dis-
ambiguation task with no NER or name standardi-
zation required. The evaluation set contained 100
names, 9027 documents, and 583,152 pair-wise
assertions.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="method">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999266">
Although the primary purposes of Fair Isaac’s en-
tity disambiguation system differ from the goal of
the Web People Search task, we found that with
little modification it was possible to fairly accu-
rately cluster Web pages with a given query name
according to the real-world entities mentioned on
the page. Most of the errors that we encountered
are related to information extraction from unstruc-
tured data as opposed to the cross-document entity
disambiguation itself.
</bodyText>
<sectionHeader confidence="0.938392" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.98716975">
This material is based upon work supported by the
Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or rec-
ommendations expressed in this material are those
of the author(s) and do not necessarily reflect the
views of the Defense Advanced Research Projects
Agency (DARPA).
</bodyText>
<sectionHeader confidence="0.996383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938951219512">
Artiles, J., Gonzalo, J. and Sekine, S. (2007). The Se-
mEval-2007 WePS Evaluation: Establishing a bench-
mark for the Web People Search Task. In Proceed-
ings of Semeval 2007, Association for Computational
Linguistics.
Bagga, A. and Baldwin, B. (1998). Entity-based Cross-
document Coreferencing Using the Vector Space
Model. 17th International Conference on Computa-
tional Linguistics (CoLing-ACL). Montreal, Canada.
10-14 August, 1998, 79-85.
Blume, M. (2005). Automatic Entity Disambiguation:
Benefits to NER, Relation Extraction, Link Analysis,
and Inference. 1st International Conference on Intel-
ligence Analysis. McLean, Virginia. 2-5 May, 2005.
Gooi, C. H. and Allan, J. (2004). Cross-Document
Coreference on a Large Scale Corpus. Human Lan-
guage Technology Conference (HLT-NAACL).
Boston, Massachusetts. 2-7 May, 2004, 9-16.
Kalashnikov, D. V. and Mehrotra, S. (2005). A Prob-
abilistic Model for Entity Disambiguation Using Re-
lationships. SIAM International Conference on Data
Mining (SDM). Newport Beach, California. 21-23
April, 2005.
Knuth, D. E. (1998). The Art of Computer Program-
ming, Volume 3: Sorting and Searching. Addison-
Wesley Professional.
Mann, G. S. and Yarowsky, D. (2003). Unsupervised
Personal Name Disambiguation. Conference on
Computational Natural Language Learning (CoNLL).
Edmonton, Canada. 31 May - 1 June, 2003, 33-40.
Mitchell, A.; Strassel, S.; Przybocki, P.; Davis, J. K.;
Doddington, G.; Grishman, R.; Meyers, A.; Brun-
stein, A.; Ferro, L. and Sundheim, B. (2004). Anno-
tation Guidelines for Entity Detection and Tracking
(EDT), Version 4.2.6.
http://www.ldc.upenn.edu/Projects/ACE/.
Ravin, Y. and Kazi, Z. (1999). Is Hillary Rodham Clin-
ton the President? Disambiguating Names across
Documents. ACL 1999 Workshop on Coreference
and Its Applications. College Park, Maryland. 22
June, 1999, 9-16.
</reference>
<page confidence="0.998132">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.149270">
<title confidence="0.998782">FICO: Web Person Disambiguation Via Weighted Similarity of Entity Contexts</title>
<author confidence="0.996759">Paul Kalmar</author>
<affiliation confidence="0.677562">Fair Isaac Corporation</affiliation>
<address confidence="0.9917525">3661 Valley Centre Dr. San Diego, CA 92130 USA</address>
<email confidence="0.97427">PaulKalmar@FairIsaac.com</email>
<author confidence="0.996204">Matthias Blume</author>
<affiliation confidence="0.551466">Fair Isaac Corporation</affiliation>
<address confidence="0.9888585">3661 Valley Centre Dr. San Diego, CA 92130 USA</address>
<email confidence="0.998774">MatthiasBlume@FairIsaac.com</email>
<abstract confidence="0.945413583333333">Entity disambiguation resolves the manyto-many correspondence between mentions of entities in text and unique real-world entities. Fair Isaac’s entity disambiguation uses language-independent entity context to agglomeratively resolve mentions with similar names to unique entities. This paper describes Fair Isaac’s automatic entity disambiguation capability and assesses its performance on the SemEval 2007 Web People Search task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task.</title>
<date>2007</date>
<booktitle>In Proceedings of Semeval 2007, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1669" citStr="Artiles et al. 2007" startWordPosition="245" endWordPosition="248">ng many-to-many relationships. Multiple distinct strings may refer to the same entity. Simultaneously, multiple identical mentions refer to distinct entities (Bagga and Baldwin, 1998). Fair Isaac’s entity disambiguation software is based largely on language-independent algorithms that resolve mentions in the context of the entire corpus. The system utilizes multiple types of context as evidence for determining whether two mentions correspond to the same entity and it automatically learns the weight of evidence of each context item via corpus statistics. The goal of the Web People Search task (Artiles et al. 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. A page is assigned to multiple groups if it mentions multiple entities, for example “John F. Kennedy” and the “John F. Kennedy Library”. The pages were selected via a set of keyword queries, and the disambiguation is evaluated only on those query entities. This differs from Fair Isaac’s system in a few key ways: our system deals with mentions rather than documents, our system does not require a filter on mentions, and our system is generally used for large collections of doc</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Artiles, J., Gonzalo, J. and Sekine, S. (2007). The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task. In Proceedings of Semeval 2007, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based Crossdocument Coreferencing Using the Vector Space Model.</title>
<date>1998</date>
<booktitle>17th International Conference on Computational Linguistics (CoLing-ACL).</booktitle>
<pages>10--14</pages>
<location>Montreal,</location>
<contexts>
<context position="1232" citStr="Bagga and Baldwin, 1998" startWordPosition="175" endWordPosition="178">air Isaac’s automatic entity disambiguation capability and assesses its performance on the SemEval 2007 Web People Search task. 1 Introduction We use the term entity to mean a specific person or object. A mention is a reference to an entity such as a word or phrase in a document. Taken together, all mentions that refer to the same realworld object model that entity (Mitchell et al. 2004). Entity disambiguation inherently involves resolving many-to-many relationships. Multiple distinct strings may refer to the same entity. Simultaneously, multiple identical mentions refer to distinct entities (Bagga and Baldwin, 1998). Fair Isaac’s entity disambiguation software is based largely on language-independent algorithms that resolve mentions in the context of the entire corpus. The system utilizes multiple types of context as evidence for determining whether two mentions correspond to the same entity and it automatically learns the weight of evidence of each context item via corpus statistics. The goal of the Web People Search task (Artiles et al. 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. A page is assigned to multiple groups if i</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, A. and Baldwin, B. (1998). Entity-based Crossdocument Coreferencing Using the Vector Space Model. 17th International Conference on Computational Linguistics (CoLing-ACL). Montreal, Canada. 10-14 August, 1998, 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Blume</author>
</authors>
<title>Automatic Entity Disambiguation: Benefits to NER, Relation Extraction, Link Analysis, and Inference.</title>
<date>2005</date>
<booktitle>1st International Conference on Intelligence Analysis.</booktitle>
<pages>2--5</pages>
<location>McLean, Virginia.</location>
<contexts>
<context position="4290" citStr="Blume (2005)" startWordPosition="676" endWordPosition="677">opied the title element and converted all text chunks to paragraphs, eliminating all other HTML and script. If no HTML was found in the directory for a page, the first text file which contained the query was used instead. 2.2 Within-Document Disambiguation When dealing with unstructured text, a named entity recognition (NER) system provides the input to the entity disambiguation. Due to time constraints and that Persons are the entity type of primary interest, any mention that matches one of the query strings is automatically labeled as a Person, regardless of its actual type. As described in Blume (2005), the system next carries out entity type-specific parsing in order to extract entity attributes such as titles, generate standardized names (e.g. p_abdul_khan_p for “Dr. Abdul Q. Khan”), and populate the data structures (token hashes) that are used to perform the withindocument entity disambiguation. We err on the side of not merging entities rather than incorrectly merging entities. Looking at multiple documents provides additional statistics. Thus, the cross-document disambiguation process described in the next section will still merge some entities even within individual documents. 2.3 Cro</context>
<context position="6794" citStr="Blume (2005)" startWordPosition="1080" endWordPosition="1081">unt of newswire text to improve frequency counts. Many of the query names would have low frequency in a text corpus that is not about them specifically, but have high frequency in this task because each document contains at least one mention of them. This would cause the INF weight to incorrectly estimate the importance of any token; adding additional documents to the disambiguation run reduces this effect and brings frequency counts to more realistic levels. We similarly count title tokens that occur with the entity and compute INF weights for the title tokens. Topic context, as described in Blume (2005), was used in some post-submission runs. We define a separate distance measure per context domain. We are able to discount the cooccurrence with multiple items as well as quantify an unexpected lack of shared co-occurrence by engineering each distance measure for each specific domain. The score produced by each distance measure may be loosely interpreted as the log of the likelihood of two randomly generated contexts sharing the observed degree of similarity. In addition to the context-based distance measures, we utilize a lexical (string) distance measure based on exactly the same transformat</context>
</contexts>
<marker>Blume, 2005</marker>
<rawString>Blume, M. (2005). Automatic Entity Disambiguation: Benefits to NER, Relation Extraction, Link Analysis, and Inference. 1st International Conference on Intelligence Analysis. McLean, Virginia. 2-5 May, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Gooi</author>
<author>J Allan</author>
</authors>
<title>Cross-Document Coreference on a Large Scale Corpus.</title>
<date>2004</date>
<booktitle>Human Language Technology Conference (HLT-NAACL).</booktitle>
<pages>9--16</pages>
<location>Boston, Massachusetts.</location>
<marker>Gooi, Allan, 2004</marker>
<rawString>Gooi, C. H. and Allan, J. (2004). Cross-Document Coreference on a Large Scale Corpus. Human Language Technology Conference (HLT-NAACL). Boston, Massachusetts. 2-7 May, 2004, 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Kalashnikov</author>
<author>S Mehrotra</author>
</authors>
<title>A Probabilistic Model for Entity Disambiguation Using Relationships.</title>
<date>2005</date>
<booktitle>SIAM International Conference on Data Mining (SDM).</booktitle>
<pages>21--23</pages>
<location>Newport Beach, California.</location>
<marker>Kalashnikov, Mehrotra, 2005</marker>
<rawString>Kalashnikov, D. V. and Mehrotra, S. (2005). A Probabilistic Model for Entity Disambiguation Using Relationships. SIAM International Conference on Data Mining (SDM). Newport Beach, California. 21-23 April, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>The Art of Computer Programming, Volume 3: Sorting and Searching.</title>
<date>1998</date>
<publisher>AddisonWesley Professional.</publisher>
<contexts>
<context position="7506" citStr="Knuth 1998" startWordPosition="1192" endWordPosition="1193"> are able to discount the cooccurrence with multiple items as well as quantify an unexpected lack of shared co-occurrence by engineering each distance measure for each specific domain. The score produced by each distance measure may be loosely interpreted as the log of the likelihood of two randomly generated contexts sharing the observed degree of similarity. In addition to the context-based distance measures, we utilize a lexical (string) distance measure based on exactly the same transformations as used to compare strings for intra-document entity disambiguation plus the Soundex algorithm (Knuth 1998) to measure whether two name tokens sound the same. A large negative score indicates a great deal of similarity (log likelihood). The process of cross-document entity disambiguation now boils down to repeatedly finding a pair of entities, comparing them (computing the sum of the above distance measures), and merging them if the score exceeds some threshold. We compute sets of keys based on lexical similarity and compare only entities that are likely to match. The WePS evaluation only deals with entities that match a query. Thus, we added a new step of key generation based on the query. 150 3 P</context>
</contexts>
<marker>Knuth, 1998</marker>
<rawString>Knuth, D. E. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching. AddisonWesley Professional.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<date>2003</date>
<booktitle>Unsupervised Personal Name Disambiguation. Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<volume>31</volume>
<pages>33--40</pages>
<location>Edmonton,</location>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Mann, G. S. and Yarowsky, D. (2003). Unsupervised Personal Name Disambiguation. Conference on Computational Natural Language Learning (CoNLL). Edmonton, Canada. 31 May - 1 June, 2003, 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mitchell</author>
<author>S Strassel</author>
<author>P Przybocki</author>
<author>J K Davis</author>
<author>G Doddington</author>
<author>R Grishman</author>
<author>A Meyers</author>
<author>A Brunstein</author>
<author>L Ferro</author>
<author>B Sundheim</author>
</authors>
<title>Annotation Guidelines for Entity Detection and Tracking (EDT),</title>
<date>2004</date>
<journal>Version</journal>
<volume>4</volume>
<pages>http://www.ldc.upenn.edu/Projects/ACE/.</pages>
<contexts>
<context position="998" citStr="Mitchell et al. 2004" startWordPosition="146" endWordPosition="149">tions of entities in text and unique real-world entities. Fair Isaac’s entity disambiguation uses language-independent entity context to agglomeratively resolve mentions with similar names to unique entities. This paper describes Fair Isaac’s automatic entity disambiguation capability and assesses its performance on the SemEval 2007 Web People Search task. 1 Introduction We use the term entity to mean a specific person or object. A mention is a reference to an entity such as a word or phrase in a document. Taken together, all mentions that refer to the same realworld object model that entity (Mitchell et al. 2004). Entity disambiguation inherently involves resolving many-to-many relationships. Multiple distinct strings may refer to the same entity. Simultaneously, multiple identical mentions refer to distinct entities (Bagga and Baldwin, 1998). Fair Isaac’s entity disambiguation software is based largely on language-independent algorithms that resolve mentions in the context of the entire corpus. The system utilizes multiple types of context as evidence for determining whether two mentions correspond to the same entity and it automatically learns the weight of evidence of each context item via corpus s</context>
</contexts>
<marker>Mitchell, Strassel, Przybocki, Davis, Doddington, Grishman, Meyers, Brunstein, Ferro, Sundheim, 2004</marker>
<rawString>Mitchell, A.; Strassel, S.; Przybocki, P.; Davis, J. K.; Doddington, G.; Grishman, R.; Meyers, A.; Brunstein, A.; Ferro, L. and Sundheim, B. (2004). Annotation Guidelines for Entity Detection and Tracking (EDT), Version 4.2.6. http://www.ldc.upenn.edu/Projects/ACE/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ravin</author>
<author>Z Kazi</author>
</authors>
<title>Is Hillary Rodham Clinton the President? Disambiguating Names across Documents. ACL 1999 Workshop on Coreference and Its Applications. College Park,</title>
<date>1999</date>
<volume>22</volume>
<pages>9--16</pages>
<location>Maryland.</location>
<marker>Ravin, Kazi, 1999</marker>
<rawString>Ravin, Y. and Kazi, Z. (1999). Is Hillary Rodham Clinton the President? Disambiguating Names across Documents. ACL 1999 Workshop on Coreference and Its Applications. College Park, Maryland. 22 June, 1999, 9-16.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>