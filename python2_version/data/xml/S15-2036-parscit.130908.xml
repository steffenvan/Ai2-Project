<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014482">
<title confidence="0.997789">
QCRI: Answer Selection for Community Question Answering –
Experiments for Arabic and English
</title>
<author confidence="0.979273">
Massimo Nicosia1, Simone Filice2, Alberto Barr´on-Cede˜no2,
Iman Saleh3, Hamdy Mubarak2, Wei Gao2, Preslav Nakov2,
Giovanni Da San Martino2, Alessandro Moschitti2, Kareem Darwish2,
Lluis M`arquez2, Shafiq Joty2 and Walid Magdy2
</author>
<affiliation confidence="0.996604">
1 University of Trento 2 Qatar Computing Research Institute 3 Cairo University
</affiliation>
<email confidence="0.81956525">
massimo.nicosia@unitn.it
{sfilice,albarron,hmubarak,wgao,pnakov,gmartino}@qf.org.qa
{amoschitti,kdarwish,lmarquez,sjoty,wmagdy}@qf.org.qa
iman.saleh@fci-cu.edu.eg
</email>
<sectionHeader confidence="0.982535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997587666666667">
This paper describes QCRI’s participation in
SemEval-2015 Task 3 “Answer Selection in
Community Question Answering”, which tar-
geted real-life Web forums, and was offered
in both Arabic and English. We apply a super-
vised machine learning approach considering
a manifold of features including among others
word n-grams, text similarity, sentiment anal-
ysis, the presence of specific words, and the
context of a comment. Our approach was the
best performing one in the Arabic subtask and
the third best in the two English subtasks.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999020833333333">
SemEval-2015 Task 3 “Answer Selection in Com-
munity Question Answering” challenged the partici-
pants to automatically predict the appropriateness of
the answers in a community question answering set-
ting (M`arquez et al., 2015). Given a question q E Q
asked by user uQ and a set of comments C, the main
task was to determine whether a comment c E C
offered a suitable answer to q or not.
In the case of Arabic, the questions were ex-
tracted from Fatwa, a community question an-
swering website about Islam.1 Each question in-
cludes five comments, provided by scholars on the
topic, each of which has to be automatically la-
beled as (i) DIRECT: a direct answer to the ques-
tion; (ii) RELATED: not a direct answer to the ques-
tion but with information related to the topic; and
(iii) IRRELEVANT: an answer to another question,
not related to the topic. This is subtask A, Arabic.
</bodyText>
<footnote confidence="0.884652">
1http://fatwa.islamweb.net
</footnote>
<bodyText confidence="0.999885636363637">
In the case of English, the dataset was extracted
from Qatar Living, a forum for people to pose ques-
tions on multiple aspects of daily life in Qatar.2
Unlike Fatwa, the questions and comments in this
dataset come from regular users, making them sig-
nificantly more varied, informal, open, and noisy. In
this case, the input to the system consists of a ques-
tion and a variable number of comments, each of
which is to be labeled as (i) GOOD: the comment
is definitively relevant; (ii) POTENTIAL: the com-
ment is potentially useful; and (iii) BAD: the com-
ment is irrelevant (e.g., it is part of a dialogue, unre-
lated to the topic, or it is written in a language other
than English). This is subtask A, English.
Additionally, a subset of the questions required a
YES /NO answer, and there was another subtask for
them, which asked to determine whether the over-
all answer to the question, according to the evidence
provided by the comments, is (i) YES, (ii) NO, or
(iii) UNSURE. This is subtask B, English.
Details about the subtasks and the experimental
settings can be found in (M`arquez et al., 2015).
Below we describe the supervised learning ap-
proach of QCRI, which considers different kinds of
features: lexical, syntactic and semantic similarities;
the context in which a comment appears; n-grams
occurrence; and some heuristics. We ranked first in
the Arabic, and third in the two English subtasks.
The rest of the paper is organized as follows: Sec-
tion 2 describes the features used, Section 3 dis-
cusses our models and our official results, and Sec-
tion 4 presents post-competition experiments and of-
fers some final remarks.
</bodyText>
<footnote confidence="0.961865">
2http://www.qatarliving.com/forum
</footnote>
<page confidence="0.967089">
203
</page>
<note confidence="0.9665635">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 203–209,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.992891" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.999649">
In this section, we describe the different features
we considered including similarity measures (Sec-
tion 2.1), the context in which a comment appears
(Section 2.2), and the occurrence of certain vocabu-
lary and phrase triggers (Sections 2.3 and 2.4). How
and where we apply them is discussed in Section 3.
Note that while our general approach is based on su-
pervised machine learning, some of our contrastive
submissions are rule-based.
</bodyText>
<subsectionHeader confidence="0.995267">
2.1 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9999244">
The similarity features measure the similarity
sim(q, c) between the question and a target com-
ment, assuming that high similarity signals a
GOOD answer. We consider three kinds of similar-
ity measures, which we describe below.
</bodyText>
<subsectionHeader confidence="0.887982">
2.1.1 Lexical Similarity
</subsectionHeader>
<bodyText confidence="0.99998765">
We compute the similarity between word n-gram
representations (n = [1,... , 4]) of q and c, using
the following lexical similarity measures (after stop-
word removal): greedy string tiling (Wise, 1996),
longest common subsequences (Allison and Dix,
1986), Jaccard coefficient (Jaccard, 1901), word
containment (Lyon et al., 2001), and cosine similar-
ity. We further compute cosine on lemmata and POS
tags, either including stopwords or not.
We also use similarity measures, which weigh the
terms using the following three formulæ:
where idf(t) is the inverse document fre-
quency (Sparck Jones, 1972) of term t in the
entire Qatar Living dataset, C is the number of
comments in this collection, and tf(t) is the term
frequency of the term in the comment. Equations 2
and 3 are variations of idf; cf. Nallapati (2004).
For subtask B, we further considered the cosine
similarity between the tf-idf-weighted intersection
of the words in q and c.
</bodyText>
<subsectionHeader confidence="0.858596">
2.1.2 Syntactic Similarity
</subsectionHeader>
<bodyText confidence="0.9999819">
We further use a partial tree kernel (Moschitti,
2006) to calculate the similarity between the ques-
tion and the comment based on their corresponding
shallow syntactic trees. These trees have word lem-
mata as leaves, then there is a POS tag node par-
ent for each lemma leaf, and POS tag nodes are in
turn grouped under shallow parsing chunks, which
are linked to a root sentence node; finally, all root
sentence nodes are linked to a super root for all sen-
tences in the question/comment.
</bodyText>
<subsectionHeader confidence="0.940628">
2.1.3 Semantic Similarity
</subsectionHeader>
<bodyText confidence="0.999623238095238">
We apply three approaches to build word-
embedding vector representations, using (i) la-
tent semantic analysis (Croce and Previtali, 2010),
trained on the Qatar Living corpus with a word
co-occurrence window of size ±3 and producing
a vector of 250 dimensions with SVD (we pro-
duced a vector for each noun in the vocabulary);
(ii) GloVe (Pennington et al., 2014), using a model
pre-trained on Common Crawl (42B tokens), with
300 dimensions; and (iii) COMPOSES (Baroni et
al., 2014), using previously-estimated predict vec-
tors of 400 dimensions.3 We represent both q and c
as a sum of the vectors corresponding to the words
within them (neglecting the subject of c). We com-
pute the cosine similarity to estimate sim(q, c).
We also experimented with word2vec (Mikolov et
al., 2013) vectors pre-trained with both cbow and
skipgram on news data, and also with both word2vec
and GloVe vectors trained on Qatar Living data, but
we discarded them as they did not help us on top of
all other features we had.
</bodyText>
<subsectionHeader confidence="0.998314">
2.2 Context
</subsectionHeader>
<bodyText confidence="0.999947">
Comments are organized sequentially according to
the time line of the comment thread. Whether a
question includes further comments by the person
who asked the original question or just several com-
ments by the same user, or whether it belongs to
a category in which a given kind of answer is ex-
pected, are all important factors. Therefore, we con-
sider a set of features that try to describe a comment
in the context of the entire comment thread.
</bodyText>
<footnote confidence="0.947617">
3They are available at http://nlp.stanford.edu/
projects/glove/ and http://clic.cimec.unitn.
it/composes/semantic-vectors.html
</footnote>
<equation confidence="0.951806111111111">
E idf(t) (1)
sim(q, c) = log(idf(t)) (2)
tEqnc log I 1 + tf (t) / (3)
E
sim(q, c) =
tEqnc
E
sim(q, c) =
tEqnc
</equation>
<page confidence="0.991577">
204
</page>
<bodyText confidence="0.998602">
We have boolean context features that explore the
following situations:
</bodyText>
<listItem confidence="0.941464">
• c is written by uq (i.e., the same user behind q),
• c is written by uq and contains an acknowledg-
ment (e.g., thank*, appreciat*),
• c is written by uq and includes further ques-
tion(s), and
• c is written by uq and includes no acknowledg-
ments nor further questions.
</listItem>
<bodyText confidence="0.9191">
We further have numerical features exploring
whether comment c appears in the proximity of a
comment by uq; the assumption is that an acknowl-
edgment or further questions by uq could signal a
bad answer:
</bodyText>
<listItem confidence="0.995200125">
• among the comments following c there is one
by uq containing an acknowledgment,
• among the comments following c there is one
by uq not containing an acknowledgment,
• among the comments following c there is one
by uq containing a question, and
• among the comments preceding c there is one
by uq containing a question.
</listItem>
<bodyText confidence="0.97233175">
The numerical value of these last four features is
determined by the distance k, in number of com-
ments, between c and the closest comment by uq
(k = oc if no comments by uq exist):
</bodyText>
<equation confidence="0.951099">
f(c) = max (0, 1.1 − (k · 0.1)) (4)
</equation>
<bodyText confidence="0.99679384375">
We also tried to model potential dialogues
by identifying interlacing comments between two
users. Our dialogue features rely on identifying con-
versation chains between two users:
ui → ... → uj → ... → ui → ... → [uj]
Note that comments by other users can appear in
between the nodes of this “pseudo-conversation”
chain. We consider three features: whether a com-
ment is at the beginning, in the middle, or at the end
of such a chain. We have copies of these three fea-
tures for the special case when uq = uj.
We are also interested in modeling whether a user
ui has been particularly active in a question thread.
Thus, we add one boolean feature: whether ui wrote
more than one comment in the current thread.
Three more features identify the first, the mid-
dle and the last comments by ui. One extra feature
counts the total number of comments written by ui.
Moreover, we empirically observed that the likeli-
hood of a comment being GOOD decreases with its
position in the thread. Therefore, we also include
another real-valued feature: max(20, i)/20, where i
represents the position of the comment in the thread.
Finally, Qatar Living includes twenty-six differ-
ent categories in which one could request informa-
tion and advice. Some of them tend to include more
open-ended questions and even invite discussion on
ambiguous topics, e.g., Socialising, Life in Qatar,
Qatari Culture. Some other require more precise an-
swers and allow for less discussion, e.g., Visas and
Permits. Therefore, we include one boolean feature
per category to consider this information.
</bodyText>
<subsectionHeader confidence="0.997863">
2.3 Word n-Grams
</subsectionHeader>
<bodyText confidence="0.999561363636364">
Our features include n-grams, independently ob-
tained from both the question and the comment:
[1, 2]-grams for Arabic, and stopworded [1, 2,3]-
grams
,3]-
grams for English. That is, each n-gram appearing
in the texts becomes a member of the feature vector.
The value for such features is tf-idf, with idf com-
puted on the entire Qatar Living dataset.
Our aim is to capture the words that are as-
sociated with questions and comments in the dif-
ferent classes. We assume that objective and
clear questions would tend to produce objective
and GOOD comments. On the other hand, subjec-
tive or badly formulated questions would call for
BAD comments or discussion, i.e., dialogues, among
the users. This can be reflected by the vocabulary
used, regardless of the topic of the formulated ques-
tion. This is also true for comments: the occurrence
of particular words could make a comment more
likely to be GOOD or BAD, regardless of what ques-
tion was asked.
</bodyText>
<subsectionHeader confidence="0.973772">
2.4 Heuristics
</subsectionHeader>
<bodyText confidence="0.999920142857143">
Exploring the training data, we noticed that many
GOOD comments suggested visiting a Web site or
contained an email address. Therefore, we included
two boolean features to verify the presence of URLs
or emails in c. Another feature captures the length
of c, as longer (GOOD) comments usually contain
detailed information to answer a question.
</bodyText>
<page confidence="0.993174">
205
</page>
<subsectionHeader confidence="0.900037">
2.5 Polarity
</subsectionHeader>
<bodyText confidence="0.99970175">
These features, which we used for subtask B only,
try to determine whether a comment is positive or
negative, which could be associated with YES or
NO answers. The polarity of a comment c is
</bodyText>
<equation confidence="0.9626085">
pol(c) = � pol(w) (5)
w∈c
</equation>
<bodyText confidence="0.999949">
where pol(w) is the polarity of word w in the NRC
Hashtag Sentiment Lexicon v0.1 (Mohammad et al.,
2013). We disregarded pol(w) if its absolute value
was less than 1.
We further use boolean features that check the ex-
istence of some keywords in the comment. Their
values are set to true if c contains words like (i) yes,
can, sure, wish, would, or (ii) no, not, neither.
</bodyText>
<subsectionHeader confidence="0.962095">
2.6 User Profile
</subsectionHeader>
<bodyText confidence="0.9999986">
With this set of features, we aim to model the
behavior of the different participants in previous
queries. Given comment c by user u, we con-
sider the number of GOOD, BAD, POTENTIAL, and
DIALOGUE comments u has produced before.4 We
also consider the average word length of GOOD,
BAD, POTENTIAL, and DIALOGUE comments.
These features are computed both considering all
questions and taking into account only those from
the target category. 5
</bodyText>
<sectionHeader confidence="0.98926" genericHeader="method">
3 Submissions and Results
</sectionHeader>
<bodyText confidence="0.999965666666667">
Below we describe our primary submissions for the
three subtasks; then we discuss our contrastive sub-
missions. Our classifications for subtask A, for both
Arabic and English, are at the comment level. Ta-
ble 1 shows our official results at the competition;
all reported F1 values are macro-averaged.
</bodyText>
<subsectionHeader confidence="0.997333">
3.1 Primary Submissions
</subsectionHeader>
<bodyText confidence="0.9994406">
Arabic. We used logistic regression. The features
are lexical similarities (Section 2.1) and n-grams
(Section 2.3). In a sort of stacking, the output of
our cont1 submission is included as another feature
(cf. Section 3.2).
</bodyText>
<footnote confidence="0.99412025">
4About 72% of the comments in the test set were written by
users who had been seen in the training/development set.
5In Section 4.3, we will observe that computing these
category-level features was not a good idea.
</footnote>
<bodyText confidence="0.999729146341464">
This submission achieved the first position in the
competition (F1 = 78.55, compared to 70.99 for the
second one). It showed a particularly high perfor-
mance when labeling RELATED comments.
English, subtask A. Here we used a linear SVM,
and a one-vs.-rest approach as we have a multiclass
problem. The features for this submission consist
of lexical, syntactic, and semantic similarities (Sec-
tion 2.1), context information (Section 2.2), n-grams
(Section 2.3), and heuristics (Section 2.4). Similarly
to Arabic, the output of our rule-based system from
the cont2 submission is another feature.
This submission achieved the third position in
the competition (F1 = 53.74, compared to 57.19
for the top one). POTENTIAL comments proved
to be the hardest, as the border with respect to
the rest of the comments is very fuzzy. Indeed,
a manual inspection on some random comments
has shown that distinguishing between GOOD and
POTENTIAL comments is often impossible.
English, subtask B. Following the or-
ganizers’ manual labeling strategy for the
YES /NO questions (M`arquez et al., 2015), we
used three steps: (i) identifying the GOOD comments
for q; (ii) classifying each of them as YES, NO, or
UNSURE; and (iii) aggregating these predictions to
the question level (majority). In case of a draw, we
labeled the question as UNSURE .6
Step (i) is subtask A. For step (ii) , we train a clas-
sifier as for subtask A, including the polarity and the
user profile features (cf. Sections 2.5 and 2.6).7
This submission achieved the third position in the
competition: F1 = 53.60, compared to 63.70 for the
top one. Unlike the other subtasks, for which we
trained on both the training and the testing datasets,
here we used the training data only, which was due
to instability of the results when adding the devel-
opment data. Post-submission experiments revealed
this was due to some bugs as well as to unreliability
of some of the statistics. Further discussion on this
can be found in Section 4.3.
</bodyText>
<footnote confidence="0.9936494">
6The majority class in the training and dev. sets (YES ) could
be the default answer. Still, we opted for a conservative deci-
sion: choosing UNSURE if no enough evidence was found.
7Even if the user profile information seems to fit for subtask
A rather than B, at development time it was effective for B only.
</footnote>
<page confidence="0.994461">
206
</page>
<table confidence="0.999707416666667">
ar DIRECT IRREL RELATED F1
primary 77.31 91.21 67.13 78.55
cont1 74.89 91.23 63.68 76.60
cont2 76.63 90.30 63.98 76.97
en A GOOD BAD POT F1
primary 78.45 72.39 10.40 53.74
cont1 76.08 75.68 17.44 56.40
cont2 75.46 72.48 7.97 51.97
en B YES NO UNSURE F1
primary 80.00 44.44 36.36 53.60
cont1 75.68 0.00 0.00 25.23
cont2 66.67 33.33 47.06 49.02
</table>
<tableCaption confidence="0.91402825">
Table 1: Per-class and macro-averaged F, scores for our
official primary and contrastive submissions to SemEval-
2015 Task 3 for Arabic (ar) and English (en), subtasks A
and B.
</tableCaption>
<subsectionHeader confidence="0.997055">
3.2 Contrastive Submissions
</subsectionHeader>
<bodyText confidence="0.999951848484849">
Arabic. We approach our contrastive submis-
sion 1 as a ranking problem. After stopword removal
and stemming, we compute sim(q, c) as follows:
where we empirically set w(t) = 1 if t is a 1-gram,
and w(t) = 4 if t is a 2-gram. Given the 5 com-
ments c1, ... , c5 ∈ C associated with q, we map the
maximum similarity maxC sim(q, c) to a maximum
100% similarity and we map the rest of the scores
proportionally. Each comment is assigned a class
according to the following ranges: [80, 100]% for
DIRECT, (20,80)% for RELATED, and [0,20]% for
IRRELEVANT. We manually tuned these threshold
values on the training data.
As for the contrastive submission 2, we built a bi-
nary classifier DIRECTvs. NO-DIRECTusing lo-
gistic regression. We then sorted the comments
according to the classifier’s prediction confidence
and we assigned labels as follows: DIRECT for the
top ranked, RELATED for the second ranked, and
IRRELEVANT for the rest. We only included lexical
similarities as features, discarding those weighted
with idf variants.
The performance of these two contrastive submis-
sions was below but close to that of our primary sub-
mission (F1 of 76.60 and 76.97, vs. 78.55 for pri-
mary), particularly for IRRELEVANT comments.
English, subtask A. Our contrastive submis-
sion 1, uses the same features and schema as our
primary submission, but with SVMlight (Joachims,
1999), which allows us to deal with the class im-
balance by tuning the j parameter, i.e., the cost
of making mistakes on positive examples. This
time, we set the C hyper-parameter to the default
value. As we focused on improving the performance
on POTENTIAL instances, we obtained better re-
sults for this category (F1 of 17.44 vs. 10.40 for
POTENTIAL), surpassing the overall performance
for our primary submission (F1 of 56.40 vs. 53.74).
Our contrastive submission 2 is similar to our
Arabic contrastive submission 1, using the same
ranges, but now for GOOD, POTENTIAL, and BAD.
We also have post-processing heuristics: c is clas-
sified as GOOD if it includes a URL, starts with an
imperative verb (e.g., try, view, contact, check), or
contains yes words (e.g., yes, yep, yup) or no words
(e.g., no, nooo, nope). Moreover, comments written
by the author of the question or including acknowl-
edgments are considered dialogues, and thus classi-
fied as BAD. The result of this submission is slightly
lower than for primary and contrastive 1: F1=51.97.
English, subtask B. Our contrastive submission 1
is like our primary, but is trained on both the training
and the development data. The reason for the low
results (an F1 of 25.23, compared to 53.60 for the
primary) were bugs in the polarity features (cf. Sec-
tion 2.5) and lack of statistics for properly estimating
the category-level user profiles (cf. Section 2.6).
The contrastive submission 2 is a rule-based sys-
tem. A question is answered as YES if it starts with
affirmative words: yes, yep, yeah, etc. It is labeled
as NO if it starts with negative words: no, nop, nope,
etc. The answer to q becomes that of the majority
of the comments: UNSURE in case of tie. It is worth
noting the comparably high performance when deal-
ing with UNSURE questions: F1=47.06, compared to
36.36 for our primary submission.
</bodyText>
<sectionHeader confidence="0.996338" genericHeader="method">
4 Post-Submission Experiments
</sectionHeader>
<bodyText confidence="0.999954">
We carried out post-submission experiments in or-
der to understand how different feature families con-
tributed to the performance of our classifiers; the re-
sults are shown in Table 2. We also managed to im-
prove our performance for all three subtasks.
</bodyText>
<figure confidence="0.54876">
1
sim(q, c) =
|
q|tEycw(t)
(6)
</figure>
<page confidence="0.989696">
207
</page>
<table confidence="0.999854870967742">
ar (only) DIR IRREL REL F1
n-grams 30.40 41.07 72.27 47.91
cont1 74.89 63.68 91.23 76.60
similarities 61.83 25.63 82.55 56.67
ar (without) DIR REL IRREL F1
n-grams 75.51 91.31 63.85 76.89
cont1 69.50 82.85 50.87 67.74
similarities 77.24 91.07 67.76 78.69
en A (only) GOOD BAD POT F1
context 67.65 45.03 11.51 47.90
n-grams 71.22 40.12 5.99 44.86
heuristics 76.46 41.94 7.11 52.57
similarities 62.93 44.58 9.62 46.16
lexical 62.25 41.46 8.66 44.82
syntactic 59.18 36.20 0.00 36.47
semantic 55.56 40.42 9.92 42.16
en A (without) GOOD BAD POT F1
context 76.05 41.53 8.98 51.50
n-grams 77.25 45.56 12.23 55.17
heuristics 73.84 65.33 6.81 48.66
similarities 78.02 71.82 9.88 53.24
lexical 78.23 72.81 9.91 53.65
syntactic 78.81 43.89 9.91 53.73
semantic 78.41 71.82 10.30 53.51
en B YES NO UNS F1
post1 78.79 57.14 20.00 51.98
post2 85.71 57.14 25.00 55.95
primary D/G/Y I/B/N R/P/U F1
ar 77.31 91.21 67.13 78.55
en A 78.45 72.39 10.40 53.74
en B 80.00 44.44 36.36 53.60
</table>
<tableCaption confidence="0.988355">
Table 2: Post-submission results for Arabic (ar) and En-
glish (en), for subtasks A and B. The lines marked with
only show results using a particular type of features only,
while those marked as without show results when using
all features but those of a particular type. The best results
for each subtask are marked in bold; the results for our of-
ficial primary submissions are included for comparison.
</tableCaption>
<subsectionHeader confidence="0.971252">
4.1 Arabic
</subsectionHeader>
<bodyText confidence="0.999892333333333">
We ran experiments with the same framework as in
our primary submission by considering the subsets
of features in isolation (only) or all features except
for a subset (without). The n-gram features together
with our cont1 submission (recall that we also use
cont1 as a feature in our primary submission) allow
for a slightly better performance than our —already
winning— primary submission (F1 = 78.69, com-
pared to F1 = 78.55). The cont1 feature turns out
to be the most important one, and, as it already con-
tains similarity, combining it with other similarity
features does not yield any further improvements.
</bodyText>
<subsectionHeader confidence="0.935002">
4.2 English, Subtask A
</subsectionHeader>
<bodyText confidence="0.999990846153846">
We performed experiments similar to those we did
for Arabic. According to the only figures, the heuris-
tic features seem to be the most useful ones, fol-
lowed by the context-based ones. The latter explore
a dimension ignored by the rest: these features are
completely uncorrelated and provide a good perfor-
mance boost (as the without experiments show). On
the other hand, using all features but the n-grams
improves over the performance of our primary run
(F1 = 55.17 compared to F1 = 53.74). This is
an interesting but not very significant result as these
features had already boosted our performance at de-
velopment time. Further research is necessary.
</bodyText>
<subsectionHeader confidence="0.977008">
4.3 English, Subtask B
</subsectionHeader>
<bodyText confidence="0.999960166666667">
Our post-submission efforts focused on investigat-
ing why learning from the training data only was
considerably better than learning from training+dev.
The output labels on the test set in the two learning
scenarios showed considerable differences: when
learning from training+dev, the predicted labels
were YES for all but three cases. After correcting
a bug in our implementation of the polarity-related
features, the result when learning on training+dev
became F1=51.98 (Table 2, post1). Further anal-
ysis showed that the features counting the number
of GOOD, BAD, and POTENTIAL comments within
categories by the same user (cf. Section 2.6) var-
ied greatly when computed on training and on train-
ing+dev, as the number of comments by a user in a
category was, in most cases, too small to yield very
reliable statistics. After discarding these three fea-
tures, the F1 raised to 55.95 (Table 2, post2), which
is higher than what we obtained at submission time.
Note that, once again, the UNSURE class is by far the
hardest to identify properly.
Surprisingly, learning with the bug-free imple-
mentation from the training set yielded a much
higher F1 of 69.35 on the test dataset (not shown
in the table). Analysis revealed that the difference
in performance was due to misclassifying just four
questions. Indeed, the differences seem to occur due
to the natural randomness of the classifier on a small
test dataset and they cannot be considered statisti-
cally significant (M`arquez et al., 2015).
</bodyText>
<page confidence="0.99756">
208
</page>
<sectionHeader confidence="0.996561" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979277777778">
We have presented the system developed by the team
of the Qatar Computing Research Institute (QCRI)
for participating in SemEval-2015 Task 3 on An-
swer Selection in Community Question Answering.
We used a supervised machine learning approach
and a manifold of features including word n-grams,
text similarity, sentiment dictionaries, the presence
of specific words, the context of a comment, some
heuristics, etc. Our approach was the best perform-
ing one in the Arabic task, and the third best in the
two English tasks.
We further presented a detailed study of which
kinds of features helped most for each language and
for each subtask, which should help researchers fo-
cus their efforts in the future.
In future work, we plan to use richer linguistic an-
notations, more complex kernels, and large semantic
resources.
</bodyText>
<sectionHeader confidence="0.99748" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99583325">
This research is developed by the Arabic Language Technolo-
gies (ALT) group at the Qatar Computing Research Institute
(QCRI), Qatar Foundation in collaboration with MIT. It is part
of the Interactive sYstems for Answer Search (Iyas) project.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999921960526316">
Lloyd Allison and Trevor Dix. 1986. A bit-string
longest-common-subsequence algorithm. Inf. Pro-
cess. Lett., 23(6):305–310, December.
Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.
2014. Don’t count, predict! A systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL ’14, pages 238–247,
Baltimore, MD, USA.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold Learning for the Semi-Supervised Induction of
FrameNet Predicates: An Empirical Investigation. In
Proceedings of the 2010 Workshop on GEometrical
Models of Natural Language Semantics, GEMS ’10,
pages 7–16, Uppsala, Sweden.
Paul Jaccard. 1901. ´Etude comparative de la distribution
florale dans une portion des Alpes et des Jura. Bul-
letin del la Soci´et´e Vaudoise des Sciences Naturelles,
37:547–579.
Thorsten Joachims. 1999. Making Large-scale Sup-
port Vector Machine Learning Practical. In Bernhard
Sch¨olkopf, Christopher J. C. Burges, and Alexander J.
Smola, editors, Advances in Kernel Methods, pages
169–184. MIT Press, Cambridge, MA, USA.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’01, pages 118–125, Pitts-
burgh, PA, USA.
Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro
Moschitti, Preslav Nakov, and Bilal Randeree. 2015.
SemEval-2015 Task 3: Answer Selection in Commu-
nity Question Answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, Denver, CO, USA.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT ’13, pages 746–
751, Atlanta, GA, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceedings of
the Seventh International Workshop on Semantic Eval-
uation, SemEval ’13, pages 321–327, Atlanta, GA,
USA.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Johannes F¨urnkranz, Tobias Scheffer, and Myra
Spiliopoulou, editors, Machine Learning: ECML
2006, volume 4212 of Lecture Notes in Computer Sci-
ence, pages 318–329. Springer Berlin Heidelberg.
Ramesh Nallapati. 2004. Discriminative models for in-
formation retrieval. In Proceedings of the 27th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ’04,
pages 64–71, Sheffield, United Kingdom.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’14, pages 1532–1543, Doha, Qatar.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28:11–21.
Michael Wise. 1996. Yap3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the Twenty-seventh SIGCSE Technical
Symposium on Computer Science Education, SIGCSE
’96, pages 130–134, New York, NY, USA.
</reference>
<page confidence="0.998961">
209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867838">
<title confidence="0.9883645">QCRI: Answer Selection for Community Question Answering Experiments for Arabic and English</title>
<author confidence="0.9953465">Simone Alberto Da San</author>
<affiliation confidence="0.987376">of Trento Computing Research Institute</affiliation>
<email confidence="0.940125">iman.saleh@fci-cu.edu.eg</email>
<abstract confidence="0.997139461538462">This paper describes QCRI’s participation in SemEval-2015 Task 3 “Answer Selection in Community Question Answering”, which targeted real-life Web forums, and was offered in both Arabic and English. We apply a supervised machine learning approach considering a manifold of features including among others text similarity, sentiment analysis, the presence of specific words, and the context of a comment. Our approach was the best performing one in the Arabic subtask and the third best in the two English subtasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Inf. Process. Lett.,</journal>
<volume>23</volume>
<issue>6</issue>
<contexts>
<context position="4830" citStr="Allison and Dix, 1986" startWordPosition="754" endWordPosition="757">h is based on supervised machine learning, some of our contrastive submissions are rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1,... , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (2004). For subtask B, we further considered th</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor Dix. 1986. A bit-string longest-common-subsequence algorithm. Inf. Process. Lett., 23(6):305–310, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL ’14,</booktitle>
<pages>238--247</pages>
<location>Baltimore, MD, USA.</location>
<contexts>
<context position="6531" citStr="Baroni et al., 2014" startWordPosition="1039" endWordPosition="1042">entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL ’14, pages 238–247, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Daniele Previtali</author>
</authors>
<title>Manifold Learning for the Semi-Supervised Induction of FrameNet Predicates: An Empirical Investigation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, GEMS ’10,</booktitle>
<pages>7--16</pages>
<location>Uppsala,</location>
<contexts>
<context position="6189" citStr="Croce and Previtali, 2010" startWordPosition="980" endWordPosition="983">tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen</context>
</contexts>
<marker>Croce, Previtali, 2010</marker>
<rawString>Danilo Croce and Daniele Previtali. 2010. Manifold Learning for the Semi-Supervised Induction of FrameNet Predicates: An Empirical Investigation. In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, GEMS ’10, pages 7–16, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>Etude comparative de la distribution florale dans une portion des Alpes et des Jura.</title>
<date>1901</date>
<booktitle>Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<pages>37--547</pages>
<contexts>
<context position="4867" citStr="Jaccard, 1901" startWordPosition="760" endWordPosition="761">me of our contrastive submissions are rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1,... , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (2004). For subtask B, we further considered the cosine similarity between the tf-id</context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. ´Etude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles, 37:547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-scale Support Vector Machine Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods,</booktitle>
<pages>169--184</pages>
<editor>In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="17755" citStr="Joachims, 1999" startWordPosition="2962" endWordPosition="2963">mments according to the classifier’s prediction confidence and we assigned labels as follows: DIRECT for the top ranked, RELATED for the second ranked, and IRRELEVANT for the rest. We only included lexical similarities as features, discarding those weighted with idf variants. The performance of these two contrastive submissions was below but close to that of our primary submission (F1 of 76.60 and 76.97, vs. 78.55 for primary), particularly for IRRELEVANT comments. English, subtask A. Our contrastive submission 1, uses the same features and schema as our primary submission, but with SVMlight (Joachims, 1999), which allows us to deal with the class imbalance by tuning the j parameter, i.e., the cost of making mistakes on positive examples. This time, we set the C hyper-parameter to the default value. As we focused on improving the performance on POTENTIAL instances, we obtained better results for this category (F1 of 17.44 vs. 10.40 for POTENTIAL), surpassing the overall performance for our primary submission (F1 of 56.40 vs. 53.74). Our contrastive submission 2 is similar to our Arabic contrastive submission 1, using the same ranges, but now for GOOD, POTENTIAL, and BAD. We also have post-process</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making Large-scale Support Vector Machine Learning Practical. In Bernhard Sch¨olkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01,</booktitle>
<pages>118--125</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="4905" citStr="Lyon et al., 2001" startWordPosition="764" endWordPosition="767">are rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1,... , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (2004). For subtask B, we further considered the cosine similarity between the tf-idf-weighted intersection of the words i</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01, pages 118–125, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>James Glass</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Bilal Randeree</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, CO, USA.</location>
<marker>M`arquez, Glass, Magdy, Moschitti, Nakov, Randeree, 2015</marker>
<rawString>Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. 2015. SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’13,</booktitle>
<pages>746--751</pages>
<location>Atlanta, GA, USA.</location>
<contexts>
<context position="6829" citStr="Mikolov et al., 2013" startWordPosition="1089" endWordPosition="1092">tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’13, pages 746– 751, Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<pages>321--327</pages>
<location>Atlanta, GA, USA.</location>
<contexts>
<context position="12047" citStr="Mohammad et al., 2013" startWordPosition="1998" endWordPosition="2001">ents suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD) comments usually contain detailed information to answer a question. 205 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is pol(c) = � pol(w) (5) w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD, BAD, POTENTIAL, and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD, BAD, POTENTIAL, and DIALOGUE comments. </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13, pages 321–327, Atlanta, GA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>Machine Learning: ECML 2006,</booktitle>
<volume>4212</volume>
<pages>318--329</pages>
<editor>In Johannes F¨urnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="5597" citStr="Moschitti, 2006" startWordPosition="882" endWordPosition="883">either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (2004). For subtask B, we further considered the cosine similarity between the tf-idf-weighted intersection of the words in q and c. 2.1.2 Syntactic Similarity We further use a partial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), traine</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Johannes F¨urnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors, Machine Learning: ECML 2006, volume 4212 of Lecture Notes in Computer Science, pages 318–329. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Nallapati</author>
</authors>
<title>Discriminative models for information retrieval.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’04,</booktitle>
<pages>64--71</pages>
<location>Sheffield, United Kingdom.</location>
<contexts>
<context position="5389" citStr="Nallapati (2004)" startWordPosition="850" endWordPosition="851">996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (2004). For subtask B, we further considered the cosine similarity between the tf-idf-weighted intersection of the words in q and c. 2.1.2 Syntactic Similarity We further use a partial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for </context>
</contexts>
<marker>Nallapati, 2004</marker>
<rawString>Ramesh Nallapati. 2004. Discriminative models for information retrieval. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’04, pages 64–71, Sheffield, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP ’14,</booktitle>
<pages>1532--1543</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="6412" citStr="Pennington et al., 2014" startWordPosition="1020" endWordPosition="1023">arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP ’14, pages 1532–1543, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--11</pages>
<contexts>
<context position="5173" citStr="Jones, 1972" startWordPosition="809" endWordPosition="810">xical Similarity We compute the similarity between word n-gram representations (n = [1,... , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (2004). For subtask B, we further considered the cosine similarity between the tf-idf-weighted intersection of the words in q and c. 2.1.2 Syntactic Similarity We further use a partial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28:11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wise</author>
</authors>
<title>Yap3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96,</booktitle>
<pages>130--134</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="4777" citStr="Wise, 1996" startWordPosition="749" endWordPosition="750">ion 3. Note that while our general approach is based on supervised machine learning, some of our contrastive submissions are rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1,... , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: where idf(t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf(t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nall</context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael Wise. 1996. Yap3: Improved detection of similarities in computer program and other texts. In Proceedings of the Twenty-seventh SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’96, pages 130–134, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>