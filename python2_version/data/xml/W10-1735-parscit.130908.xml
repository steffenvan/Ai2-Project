<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001627">
<title confidence="0.9979095">
Chunk-based Verb Reordering in VSO Sentences for
Arabic-English Statistical Machine Translation
</title>
<author confidence="0.727438">
Arianna Bisazza and Marcello Federico
</author>
<affiliation confidence="0.729274">
Fondazione Bruno Kessler
Human Language Technologies
</affiliation>
<address confidence="0.612781">
Trento, Italy
</address>
<email confidence="0.999008">
{bisazza,federico}@fbk.eu
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997558">
In Arabic-to-English phrase-based statis-
tical machine translation, a large number
of syntactic disfluencies are due to wrong
long-range reordering of the verb in VSO
sentences, where the verb is anticipated
with respect to the English word order.
In this paper, we propose a chunk-based
reordering technique to automatically de-
tect and displace clause-initial verbs in the
Arabic side of a word-aligned parallel cor-
pus. This method is applied to preprocess
the training data, and to collect statistics
about verb movements. From this anal-
ysis, specific verb reordering lattices are
then built on the test sentences before de-
coding them. The application of our re-
ordering methods on the training and test
sets results in consistent BLEU score im-
provements on the NIST-MT 2009 Arabic-
English benchmark.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9980435">
Shortcomings of phrase-based statistical machine
translation (PSMT) with respect to word reorder-
ing have been recently shown on the Arabic-
English pair by Birch et al. (2009). An empiri-
cal investigation of the output of a strong baseline
we developed with the Moses toolkit (Koehn et
al., 2007) for the NIST 2009 evaluation, revealed
that an evident cause of syntactic disfluency is the
anticipation of the verb in Arabic Verb-Subject-
Object (VSO) sentences – a class that is highly
represented in the news genre1.
Fig. 1 shows two examples where the Arabic
main verb phrase comes before the subject. In
such sentences, the subject can be followed by
adjectives, adverbs, coordinations, or appositions
that further increase the distance between the verb
</bodyText>
<footnote confidence="0.813137">
1In fact, Arabic syntax admits both SVO and VSO orders.
</footnote>
<bodyText confidence="0.999539219512195">
and its object. When translating into English – a
primarily SVO language – the resulting long verb
reorderings are often missed by the PSMT decoder
either because of pure modeling errors or because
of search errors (Germann et al., 2001): i.e. their
span is longer than the maximum allowed distor-
tion distance, or the correct reordering hypothesis
does not emerge from the explored search space
because of a low score. In the two examples, the
missed verb reorderings result in different transla-
tion errors by the decoder, respectively, the intro-
duction of a subject pronoun before the verb and,
even worse, a verbless sentence.
In Arabic-English machine translation, other
kinds of reordering are of course very frequent: for
instance, adjectival modifiers following their noun
and head-initial genitive constructions (Idafa).
These, however, appear to be mostly local, there-
fore more likely to be modeled through phrase in-
ternal alignments, or to be captured by the reorder-
ing capabilities of the decoder. In general there is a
quite uneven distribution of word-reordering phe-
nomena in Arabic-English, and long-range move-
ments concentrate on few patterns.
Reordering in PSMT is typically performed
by (i) constraining the maximum allowed word
movement and exponentially penalizing long re-
orderings (distortion limit and penalty), and (ii)
through so-called lexicalized orientation models
(Och et al., 2004; Koehn et al., 2007; Galley
and Manning, 2008). While the former is mainly
aimed at reducing the computational complexity
of the decoding algorithm, the latter assigns at
each decoding step a score to the next source
phrase to cover, according to its orientation with
respect to the last translated phrase. In fact, neither
method discriminates among different reordering
distances for a specific word or syntactic class. To
our view, this could be a reason for their inade-
quacy to properly deal with the reordering pecu-
liarities of the Arabic-English language pair. In
</bodyText>
<page confidence="0.975631">
235
</page>
<note confidence="0.847463875">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 235–243,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
src: w AstdEt kl mn AlsEwdyp w lybyA w swryASubj sfrA’ hAObj fy AldnmArk .
ref: Each of Saudi Arabia, Libya and SyriaSubj recalled their ambassadorsObj from Denmark.
MT: He recalled all from Saudi Arabia, Libya and Syria ambassadors in Denmark.
src: jdd AlEAhl Almgrby Almlk mHmd AlsAdsSubj dEm hObj l m$rwE Alr}ys Alfrnsy
ref: The Moroccan monarch King Mohamed VISubj renewed his supportObj to the project of French President
MT: The Moroccan monarch King Mohamed VI his support to the French President
</note>
<figureCaption confidence="0.999942">
Figure 1: Examples of problematic SMT outputs due to verb anticipation in the Arabic source.
</figureCaption>
<bodyText confidence="0.997565357142857">
this work, we introduce a reordering technique
that addresses this limitation.
The remainder of the paper is organized as fol-
lows. In Sect. 2 we describe our verb reordering
technique and in Sect. 3 we present statistics about
verb movement collected through this technique.
We then discuss the results of preliminary MT ex-
periments involving verb reordering of the training
based on these findings (Sect. 4). Afterwards, we
explain our lattice approach to verb reordering in
the test and provide evaluation on a well-known
MT benchmark (Sect. 5). In the last two sections
we review some related work and draw the final
conclusions.
</bodyText>
<sectionHeader confidence="0.952984" genericHeader="method">
2 Chunk-based Verb Reordering
</sectionHeader>
<bodyText confidence="0.998159954545454">
The goal of our work is to displace Arabic verbs
from their clause-initial position to a position that
minimizes the amount of word reordering needed
to produce a correct translation. In order to re-
strict the set of possible movements of a verb and
to abstract from the usual token-based movement
length measure, we decided to use shallow syn-
tax chunking of the source language. Full syntac-
tic parsing is another option which we have not
tried so far mainly because popular parsers that are
available for Arabic do not mark grammatical re-
lations such as the ones we are interested in.
We assume that Arabic verb reordering only
occurs between shallow syntax chunks, and not
within them. For this purpose we annotated our
Arabic data with the AMIRA chunker by Diab et
al. (2004)2. The resulting chunks are generally
short (1.6 words on average). We then consider
a specific type of reordering by defining a produc-
tion rule of the kind: “move a chunk of type T
along with its L left neighbours and R right neigh-
bours by a shift of S chunks”. A basic set of rules
</bodyText>
<footnote confidence="0.931871333333333">
2This tool implies morphological segmentation of the
Arabic text. All word statistics in this paper refer to AMIRA-
segmented text.
</footnote>
<bodyText confidence="0.9997065">
that displaces the verbal chunk to the right by at
most 10 positions corresponds to the setting:
T=’VP’, L=0, R=0, S=1..10
In order to address cases where the verb is moved
along with its adverbial, we also add a set of rules
that include a one-chunk right context in the move-
ment:
T=’VP’, L=0, R=1, S=1..10
To prevent verb reordering from overlapping
with the scope of the following clause, we always
limit the maximum movement to the position of
the next verb. Thus, for each verb occurrence, the
number of allowed movements for our setting is at
most 2 x 10 = 20.
Assuming that a word-aligned translation of the
sentence is available, the best movement, if any,
will be the one that reduces the amount of distor-
tion in the alignment, that is: (i) it reduces the
number of swaps by 1 or more, and (ii) it mini-
mizes the sum of distances between source posi-
tions aligned to consecutive target positions, i.e.
Ei |ai − (ai_1 + 1) |where ai is the index of the
foreign word aligned to the ith English word. In
case several movements are optimal according to
these two criteria, e.g. because of missing word-
alignment links, only the shortest good movement
is retained.
The proposed reordering method has been ap-
plied to various parallel data sets in order to per-
form a quantitative analysis of verb anticipation,
and to train a PSMT system on more monotonic
alignments.
</bodyText>
<sectionHeader confidence="0.649861" genericHeader="method">
3 Analysis of Verb Reordering
</sectionHeader>
<bodyText confidence="0.9991368">
We applied the above technique to two parallel
corpora3 provided by the organizers of the NIST-
MT09 Evaluation. The first corpus (Gale-NW)
contains human-made alignments. As these re-
fer to non-segmented text, they were adjusted to
</bodyText>
<footnote confidence="0.980301">
3Newswire sections of LDC2006E93 and LDC2009E08,
respectively 4337 and 777 sentence pairs.
</footnote>
<page confidence="0.995739">
236
</page>
<figureCaption confidence="0.9983964">
Figure 2: Percentage of verb reorderings by maxi-
mum shift (0 stands for no movement).
Figure 3: Distortion reduction in the GALE-NW
corpus: jump occurrences grouped by length range
(in nb. of words).
</figureCaption>
<bodyText confidence="0.9875298">
agree with AMIRA-style segmentation. For the
second corpus (Eval08-NW), we filtered out sen-
tences longer than 80 tokens in order to make
word alignment feasible with GIZA++ (Och and
Ney, 2003). We then used the Intersection of
the direct and inverse alignments, as computed by
Moses. The choice of such a high-precision, low-
recall alignment set is supported by the findings of
Habash (2007) on syntactic rule extraction from
parallel corpora.
</bodyText>
<subsectionHeader confidence="0.998961">
3.1 The Verb’s Dance
</subsectionHeader>
<bodyText confidence="0.999985190476191">
There are 1,955 verb phrases in Gale-NW and
11,833 in Eval08-NW. Respectively 86% and 84%
of these do not need to be moved according to the
alignments. The remaining 14% and 16% are dis-
tributed by movement length as shown in Fig. 2:
most verb reorderings consist in a 1-chunk long
jump to the right (8.3% in Gale-NW and 11.6% in
Eval08-NW). The rest of the distribution is simi-
lar in the two corpora, which indicates a good cor-
respondence between verb reordering observed in
automatic and manual alignments. By increasing
the maximum movement length from 1 to 2, we
can cover an additional 3% of verb reorderings,
and around 1% when passing from 2 to 3. We
recall that the length measured in chunks doesn’t
necessarily correspond to the number of jumped
tokens. These figures are useful to determine an
optimal set of reordering rules. From now on we
will focus on verb movements of at most 6 chunks,
as these account for about 99.5% of the verb oc-
currences.
</bodyText>
<subsectionHeader confidence="0.999788">
3.2 Impact on Corpus Global Distortion
</subsectionHeader>
<bodyText confidence="0.999988787878788">
We tried to measure the impact of chunk-based
verb reordering on the total word distortion found
in parallel data. For the sake of reliability, this
investigation was carried out on the manually
aligned corpus (Gale-NW) only. Fig. 3 shows the
positive effect of verb reordering on the total dis-
tortion, which is measured as the number of words
that have to be jumped on the source side in or-
der to cover the sentence in the target order (that
is |ai − (ai−1 + 1)|). Jumps have been grouped
by length and the relative decrease of jumps per
length is shown on top of each double column.
These figures do not prove as we hoped that
verb reordering resolves most of the long range re-
orderings. Thus we manually inspected a sample
of verb-reordered sentences that still contain long
jumps, and found out that many of these were due
to what we could call “unnecessary” reordering. In
fact, human translations that are free to some ex-
tent, often display a global sentence restructuring
that makes distortion dramatically increase. We
believe this phenomenon introduces noise in our
analysis since these are not reorderings that an MT
system needs to capture to produce an accurate
and fluent translation.
Nevertheless, we can see from the relative de-
crease percentages shown in the plot, that although
short jumps are by far the most frequent, verb
reordering affects especially medium and long
range distortion. More precisely, our selective
reordering technique solves 21.8% of the 5-to-6-
words jumps, 25.9% of the 7-to-9-words jumps
and 24.2% of the 10-to-14-words jumps, against
</bodyText>
<page confidence="0.977345">
237
</page>
<bodyText confidence="0.99985475">
only 9.5% of the 2-words jumps, for example.
Since our primary goal is to improve the handling
of long reorderings, this makes us think that we
are advancing in a promising direction.
</bodyText>
<sectionHeader confidence="0.98558" genericHeader="method">
4 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.999904454545455">
In this section we investigate how verb reordering
on the source language can affect translation qual-
ity. We apply verb reordering both on the training
and the test data. However, while the parallel cor-
pus used for training can be reordered by exploit-
ing word alignments, for the test corpus we need
a verb reordering ”prediction model”. For these
preliminary experiments, we assumed that optimal
verb-reordering of the test data is provided by an
oracle that has access to the word alignments with
the reference translations.
</bodyText>
<subsectionHeader confidence="0.971977">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999881172413793">
We trained a Moses-based system on a subset of
the NIST-MT09 Evaluation data4 for a total of
981K sentences, 30M words. We first aligned the
data with GIZA++ and use the resulting Intersec-
tion set to apply the technique explained in Sect. 2.
We then retrained the whole system – from word
alignment to phrase scoring – on the reordered
data and evaluated it on two different versions of
Eval08-NW: plain and oracle verb-reordered, ob-
tained by exploiting word alignments with the first
of the four available English references. The first
experiment is meant to measure the impact of the
verb reordering procedure on training only. The
latter will provide an estimate of the maximum im-
provement we can expect from the application to
the test of an optimal verb reordering prediction
technique. Given our experimental setting, one
could argue that our BLEU score is biased because
one of the references was also used to generate the
verb reordering. However, in a series of exper-
iments not reported here, we evaluated the same
systems using only the remaining three references
and observed similar trends as when all four refer-
ences are used.
Feature weights were optimized through MERT
(Och, 2003) on the newswire section of the NIST-
MT06 evaluation set (Dev06-NW), in the origi-
nal version for the baseline system, in the verb-
reordered version for the reordered system.
</bodyText>
<footnote confidence="0.924255666666667">
4LDC2007T08, 2003T07, 2004E72, 2004T17, 2004T18,
2005E46, 2006E25, 2006E44 and LDC2006E39 – the two
last with first reference only.
</footnote>
<figureCaption confidence="0.985033">
Figure 4: BLEU scores of baseline and reordered
system on plain and oracle reordered Eval08-NW.
</figureCaption>
<bodyText confidence="0.970737090909091">
Fig. 4 shows the results in terms of BLEU score
for (i) the baseline system, (ii) the reordered sys-
tem on a plain version of Eval08-NW and (iii) the
reordered system on the reordered test. The scores
are plotted against the distortion limit (DL) used
in decoding. Because high DL values (8-10) im-
ply a larger search space and because we want to
give Moses the best possible conditions to prop-
erly handle long reordering, we relaxed for these
conditions the default pruning parameter to the
point that led the highest BLEU score5.
</bodyText>
<subsectionHeader confidence="0.865134">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999991736842105">
The first observation is that the reordered system
always performs better (0.5�0.6 points) than the
baseline on the plain test, despite the mismatch
between training and test ordering. This may be
due to the fact that automatic word alignments
are more accurate when less reordering is present
in the data, although previous work (Lopez and
Resnik, 2006) showed that even large gains in
alignment accuracy seldom lead to better trans-
lation performances. Moreover phrase extraction
may benefit from a distortion reduction, since its
heuristics rely on word order in order to expand
the context of alignment links.
The results on the oracle reordered test are also
interesting: a gain of at least 1.2 point absolute
over the baseline is reported in all tested DL condi-
tions. These improvements are remarkable, keep-
ing in mind that only 31% of the train and 33% of
the test sentences get modified by verb reordering.
</bodyText>
<footnote confidence="0.9939415">
5That is, the histogram pruning maximum stack size was
set to 1000 instead of the default 200.
</footnote>
<page confidence="0.984663">
238
</page>
<figureCaption confidence="0.999308">
Figure 5: Reordering lattices for Arabic VSO sentences: word-based and chunk-based.
</figureCaption>
<bodyText confidence="0.999949">
Concerning distortion, although long verb
movements are often observed in parallel corpora,
relaxing the DL to high values does not bene-
fit the translation, even with our ‘generous’ set-
ting (wider beam search). This is probably due to
the fact that, with weakly constrained distortion,
the risk of search errors increases as the reorder-
ing model fails to properly rank an exponentially
growing set of permutations. Therefore many cor-
rect reordering hypotheses receive low scores and
get lost in pruning or recombination.
</bodyText>
<sectionHeader confidence="0.984356" genericHeader="method">
5 Verb Reordering Lattices
</sectionHeader>
<bodyText confidence="0.999971285714286">
Having assessed the negative impact of VSO sen-
tences on Arabic-English translation performance,
we now propose a method to improve the handling
of this phenomenon at decoding time. As in real
working conditions word alignments of the input
text are not available, we explore a reordering lat-
tice approach.
</bodyText>
<subsectionHeader confidence="0.992881">
5.1 Lattice Construction
</subsectionHeader>
<bodyText confidence="0.999747684210526">
Firstly conceived to optimally encode multiple
transcription hypothesis produced by a speech rec-
ognizer, word lattices have later been used to rep-
resent various forms of input ambiguity, mainly at
the level of token boundaries (e.g. word segmenta-
tion, morphological decomposition, word decom-
pounding (Dyer et al., 2008)).
A main problem when dealing with permuta-
tions is that the lattice size can grow very quickly
when medium to long reorderings are represented.
We are particularly concerned with this issue be-
cause our decoding will perform additional re-
ordering on the lattice input. Thanks to the re-
strictions we set on our verb movement reordering
rules described in Sect. 2 – i.e. only reordering be-
tween chunks and no overlap between consecutive
verb chunks movement – we are able to produce
quite compact word lattices.
Fig. 5 illustrates how a chunk-based reordering
lattice is generated. Suppose we want to translate
the Arabic sentence “w &gt;kdt mSAdr rsmyp wjwd
rAbT byn AlAEtdA’At”, whose English meaning is
“Official sources confirmed that there was a link
between the attacks”. The Arabic main verb &gt;kdt
(confirmed) is in pre-subject position. If we ap-
plied word-based rather than chunk-based rules to
move the verb to the right, we would produce the
first lattice of the figure, containing 7 paths (the
original plus 6 verb movements). With the chunk-
based rules, we treat instead chunks as units and
get the second lattice. Then, by expanding each
chunk, we obtain the final, less dense lattice, that
compared to the first does not contain 3 (unlikely)
reordering edges.
To be consistent with the reordering applied to
the training data, we use a set of rules that move
each verb phrase alone or with its following chunk
by 1 to 6 chunks to the right. With this settings,
</bodyText>
<page confidence="0.998457">
239
</page>
<figureCaption confidence="0.9732475">
Figure 6: Structure of a chunk-based reordering lattice for verb reordering, before word expansion. Edges
in boldface represent the verbal chunk.
</figureCaption>
<bodyText confidence="0.999647636363636">
our lattice generation algorithm computes a com-
pact lattice (Fig. 6) that introduces at most 5xAS
chunk edges for each verb chunk, where AS is the
permitted movement range (6 in this case).
Before translation, each edge has to be associ-
ated with a weight that the decoder will use as ad-
ditional feature. To differentiate between the orig-
inal word order and verb reordering we assign a
fixed weight of 1 to the edges of the plain path, and
0.25 to the other edges. As future work we will de-
vise more discriminative weighting schemes.
</bodyText>
<subsectionHeader confidence="0.994043">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999908857142857">
For the experiments, we relied on the existing
Moses-implementation of non-monotonic decod-
ing for word lattices (Dyer et al., 2008) with
some fixes concerning the computation of reorder-
ing distance. The translation system is the same
as the one presented in Sect. 4, to which we
added an additional feature function evaluating
the lattice weights (weight-i). Instead of rerun-
ning MERT, we directly estimated the additional
feature-function weight over a suitable interval
(0.002 to 0.5), by running the decoder several
times on the development set. The resulting op-
timal weight was 0.05.
Table 1 presents results on three test sets:
Eval08-NW which was used to calibrate the re-
ordering rules, Reo08-NW a specific test set con-
sisting of the 33% of Eval08-NW sentences that
actually require verb reordering, and Eval09-NW
a yet unseen dataset (newswire section of the
NIST-MT09 evaluation set, 586 sentences). Best
results with lattice decoding were obtained with a
distortion limit (DL) of 4, while best performance
of text decoding was obtained with a DL of 6.
As we hoped, translating a verb reordering lat-
tice yields an additional improvement to the re-
ordering of the training corpus: from 43.67%
to 44.04% on Eval08-NW and from 48.53% to
48.96% on Eval09-NW. The gap between the
baseline and the score obtainable by oracle verb
reordering, as estimated in the preliminary exper-
iments on Eval08-NW (44.36%), has been largely
filled.
On the specific test set – Reo08-NW – we ob-
serve a performance drop when reordered models
are applied to non-reordered (plain) input: from
46.90% to 46.64%. Hence it seems that the mis-
match between training and test data is signifi-
cantly impacting on the reordering capabilities of
the system with respect to verbs. We speculate
that such negative effect is diluted in the full test
set (Eval08-NW) and compensated by the positive
influence of verb reordering on phrase extraction.
Indeed, when the lattice technique is applied we
get an improvement of about 0.6 point over the
baseline, which is still a fair result, but not as good
as the one obtained on the general test sets.
Finally, our approach led to an overall gain of
0.8 point BLEU over the baseline, on Eval09-NW.
We believe this is a satisfactory result, given the
fairly good starting performance, and given that
the BLEU metric is known not to be very sensi-
tive to word order variations (Callison-Burch et
al., 2006). For the future, we plan to also use spe-
cific evaluation metrics that will allow us to isolate
the impact of our approach on reordering, like the
ones by Birch et al. (2010).
</bodyText>
<table confidence="0.981605166666667">
System DL eval08nw reo08nw eval09nw
baseline 6 43.10 46.90 48.13
reord. training +
plain input 6 43.67 46.64 48.53
lattice 4 44.04 47.51 48.96
oracle reord. 4 44.36 48.25 na
</table>
<tableCaption confidence="0.9993415">
Table 1: BLEU scores of baseline and reordered
system on plain test and on reordering lattices.
</tableCaption>
<page confidence="0.997014">
240
</page>
<sectionHeader confidence="0.99987" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999991036363637">
Linguistically motivated word reordering for
Arabic-English has been proposed in several re-
cent works. Habash (2007) extracts syntactic re-
ordering rules from a word-aligned parallel cor-
pus whose Arabic side has been fully parsed. The
rules involve reordering of syntactic constituents
and are applied in a deterministic way (always
the most probable) as preprocessing of training
and test data. The technique achieves consistent
improvements only in very restrictive conditions:
maximum phrase size of 1 and monotonic decod-
ing, thus failing to enhance the existing reorder-
ing capabilities of PSMT. In (Crego and Habash,
2008; Elming and Habash, 2009) possible in-
put permutations are represented through a word
graph, which is then processed by a monotonic
phrase- or n-gram-based decoder. Thus, these ap-
proaches are conceived as alternatives, rather than
integrations, to PSMT reordering. On the contrary,
we focused on a single type of significant long re-
orderings, in order to integrate class-specific re-
ordering methods into a standard PSMT system.
To our knowledge, the work by Niehues and
Kolss (2009) on German-English is the only ex-
ample of a lattice-based reordering approach be-
ing coupled with reordering at decoding time. In
their paper, discontinuous non-deterministic POS-
based rules learned from a word-aligned corpus
are applied to German sentences in the form of
weighted edges in a word lattice. Their phrase-
based decoder admits local reordering within a
fixed window of 2 words, while, in our work, we
performed experiments up to a distortion limit of
10. Another major difference is that we used shal-
low syntax annotation to effectively reduce the
number of possible permutations. A first attempt
to adapt our technique to the German language is
described in Hardmeier et al. (2010).
Our work is also tightly related to the prob-
lem of noun-phrase subject detection, recently ad-
dressed by Green et al. (2009). In fact, detect-
ing the extension of the subject can be a suffi-
cient condition to guess the optimal reordering of
the verb. In their paper, a discriminative classi-
fier was trained on a rich variety of linguistic fea-
tures to detect the full scope of Arabic NP subjects
in verb-initial clauses. The authors reported an F-
score of 61.3%, showing that the problem is hard
to solve even when more linguistic information is
available. In order to integrate the output of the
classifier into a PSMT decoder, a specific trans-
lation feature was designed to reward hypotheses
in which the subject is translated as a contiguous
block. Unfortunately, no improvement in transla-
tion quality was obtained.
</bodyText>
<sectionHeader confidence="0.99907" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999975405405406">
Word reordering remains one of the hardest prob-
lems in statistical machine translation. Based on
the intuition that few reordering patterns would
suffice to handle the most significant cases of long
reorderings in Arabic-English, we decided to fo-
cus on the problem of VSO sentences.
Thanks to simple linguistic assumptions on verb
movement, we developed an efficient, low-cost
technique to reorder the training data, on the one
hand, and to better handle verb reordering at de-
coding time, on the other. In particular, translation
is performed on a compact word lattice that repre-
sents likely verb movements. The resulting system
outperforms a strong baseline in terms of BLEU,
and produces globally more readable translations.
However, the problem is not totally solved because
many verb reorderings are still missed, despite
the suggestions provided by the lattice. Different
factors can explain these errors: poor interaction
between lattice and distortion/orientation models
used by the decoder; poor discriminative power of
the target language model with respect to different
reorderings of the source.
As a first step to improvement, we will devise
a discriminative weighting scheme based on the
length of the reorderings represented in the lat-
tice. For the longer term we are working towards
bringing linguistically informed reordering con-
straints inside decoding, as an alternative to the
lattice solution. In addition, we plan to couple
our reordering technique with more informative
language models, including for instance syntac-
tic analysis of the hypothesis under construction.
Finally we would like to compare the proposed
chunk-based technique with one that exploits full
syntactic parsing of the Arabic sentence to further
decrease the reordering possibilities of the verb.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9983756">
This work was supported by the EuroMatrixPlus
project (IST-231720) which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
</bodyText>
<page confidence="0.992137">
241
</page>
<note confidence="0.735916">
src: w A$Ar AlsnAtwr AlY dEm h m$rwEA ErD ElY mjls Al$ywx
ref: The Senator referred to his support for a project proposed to the Senate
base MT: The Senator to support projects presented to the Senate
new MT: Senator noted his support projects presented to the Senate
</note>
<figure confidence="0.351065125">
src: mn jAnb h hdd &gt;bw mSEb EbdAlwdwd Amyr AlqAEdp b blAd Almgrb AlAslAmy fy nfs Al$ryT b AlqyAm
b slslp AEtdA’At w &gt;EmAl &lt;rhAbyp Dd AlmSAlH w Alm&amp;ssAt AljzA}ryp fy AlEdyd mn AlmnATq
AljzA}ryp
ref: For his part , Abu Musab Abd al-Wadud , the commander of al-Qaeda in the Islamic Maghreb Countries,
threatened in the same tape to carry out a series of attacks and terrorist actions against Algerian interests and
organizations in many parts of Algeria
base MT: For his part threatened Abu Musab EbdAlwdwd Amir al-Qaeda Islamic Morocco country in the same tape to
carry out a series of attacks and terrorist acts against the interests and the Algerian institutions in many areas of
Algiers
new MT: For his part, Abu Musab EbdAlwdwd Amir al Qaida threatened to Morocco Islamic country in the same tape
to carry out a series of attacks and terrorist acts against the interests of the Algerian and institutions in many
areas of Algiers
src: w ymtd Alm$rwE 500 km mtr w yrbT Almdyntyn Almqdstyn b mdynp jdp ElY sAHl AlbHr Al&gt;Hmr .
ref: The project is 500 kilometers long and connects the two holy cities with the city of Jeddah on the Red Sea coast.
base MT: It extends the project 500 km and linking the two holy cities in the city of Jeddah on the Red Sea coast.
new MT: The project extends 500 km , linking the two holy cities in the city of Jeddah on the Red Sea coast.
</figure>
<figureCaption confidence="0.99796">
Figure 7: Examples showing MT improvements coming from chunk-based verb-reordering.
</figureCaption>
<sectionHeader confidence="0.997506" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998187301587302">
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phe-
nomena. In StatMT ’09: Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
197–205, Morristown, NJ, USA. Association for
Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, Published online.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of BLEU
in machine translation research. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Trento,
Italy, April.
Josep M. Crego and Nizar Habash. 2008. Using shal-
low syntax information to improve word alignment
and reordering for smt. In StatMT ’08: Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 53–61, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic Tagging of Arabic Text: From
Raw Text to Base Phrase Chunks. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 149–152,
Boston, Massachusetts, USA, May 2 - May 7. As-
sociation for Computational Linguistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012–
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jakob Elming and Nizar Habash. 2009. Syntactic re-
ordering for English-Arabic phrase-based machine
translation. In Proceedings of the EACL 2009 Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 69–77, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP ’08: Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 848–856, Morristown, NJ, USA.
Association for Computational Linguistics.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding
and optimal decoding for machine translation. In
Proceedings of the 39th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
228–335, Toulouse, France.
Spence Green, Conal Sathi, and Christopher D. Man-
ning. 2009. NP subject detection in verb-initial Ara-
bic clauses. In Proceedings of the Third Workshop
on Computational Approaches to Arabic Script-
based Languages (CAASL3), Ottawa, Canada.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Bente Maegaard, ed-
itor, Proceedings of the Machine Translation Summit
XI, pages 215–222, Copenhagen, Denmark.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. FBK at WMT 2010: Word lat-
tices for morphological reduction and chunk-based
</reference>
<page confidence="0.962599">
242
</page>
<reference confidence="0.99983">
reordering. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
rics MATR, Uppsala, Sweden, July. Association for
Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What’s the
link? In 5th Conference of the Association for Ma-
chine Translation in the Americas (AMTA), Boston,
Massachusetts, August.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206–214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technologies and the Annual Meet-
ing of the North American Chapter of the Associ-
ation of Computational Linguistics (HLT-NAACL),
Boston, MA.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Erhard Hinrichs
and Dan Roth, editors, Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 160–167.
</reference>
<page confidence="0.999051">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.090148">
<title confidence="0.9965375">Chunk-based Verb Reordering in VSO Sentences Arabic-English Statistical Machine Translation</title>
<author confidence="0.6698455">Arianna Bisazza</author>
<author confidence="0.6698455">Marcello Fondazione Bruno</author>
<affiliation confidence="0.695724">Human Language</affiliation>
<address confidence="0.514912">Trento,</address>
<abstract confidence="0.966449285714286">In Arabic-to-English phrase-based statistical machine translation, a large number of syntactic disfluencies are due to wrong long-range reordering of the verb in VSO sentences, where the verb is anticipated with respect to the English word order. In this paper, we propose a chunk-based reordering technique to automatically detect and displace clause-initial verbs in the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 Arabic- English benchmark.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>A quantitative analysis of reordering phenomena.</title>
<date>2009</date>
<booktitle>In StatMT ’09: Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>197--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1224" citStr="Birch et al. (2009)" startWordPosition="177" endWordPosition="180">the Arabic side of a word-aligned parallel corpus. This method is applied to preprocess the training data, and to collect statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 ArabicEnglish benchmark. 1 Introduction Shortcomings of phrase-based statistical machine translation (PSMT) with respect to word reordering have been recently shown on the ArabicEnglish pair by Birch et al. (2009). An empirical investigation of the output of a strong baseline we developed with the Moses toolkit (Koehn et al., 2007) for the NIST 2009 evaluation, revealed that an evident cause of syntactic disfluency is the anticipation of the verb in Arabic Verb-SubjectObject (VSO) sentences – a class that is highly represented in the news genre1. Fig. 1 shows two examples where the Arabic main verb phrase comes before the subject. In such sentences, the subject can be followed by adjectives, adverbs, coordinations, or appositions that further increase the distance between the verb 1In fact, Arabic synt</context>
</contexts>
<marker>Birch, Blunsom, Osborne, 2009</marker>
<rawString>Alexandra Birch, Phil Blunsom, and Miles Osborne. 2009. A quantitative analysis of reordering phenomena. In StatMT ’09: Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 197–205, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT evaluation: evaluating reordering. Machine Translation,</title>
<date>2010</date>
<note>Published online.</note>
<contexts>
<context position="21383" citStr="Birch et al. (2010)" startWordPosition="3527" endWordPosition="3530">n improvement of about 0.6 point over the baseline, which is still a fair result, but not as good as the one obtained on the general test sets. Finally, our approach led to an overall gain of 0.8 point BLEU over the baseline, on Eval09-NW. We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burch et al., 2006). For the future, we plan to also use specific evaluation metrics that will allow us to isolate the impact of our approach on reordering, like the ones by Birch et al. (2010). System DL eval08nw reo08nw eval09nw baseline 6 43.10 46.90 48.13 reord. training + plain input 6 43.67 46.64 48.53 lattice 4 44.04 47.51 48.96 oracle reord. 4 44.36 48.25 na Table 1: BLEU scores of baseline and reordered system on plain test and on reordering lattices. 240 6 Related Work Linguistically motivated word reordering for Arabic-English has been proposed in several recent works. Habash (2007) extracts syntactic reordering rules from a word-aligned parallel corpus whose Arabic side has been fully parsed. The rules involve reordering of syntactic constituents and are applied in a det</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, Published online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluation the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="21209" citStr="Callison-Burch et al., 2006" startWordPosition="3494" endWordPosition="3497">t is diluted in the full test set (Eval08-NW) and compensated by the positive influence of verb reordering on phrase extraction. Indeed, when the lattice technique is applied we get an improvement of about 0.6 point over the baseline, which is still a fair result, but not as good as the one obtained on the general test sets. Finally, our approach led to an overall gain of 0.8 point BLEU over the baseline, on Eval09-NW. We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burch et al., 2006). For the future, we plan to also use specific evaluation metrics that will allow us to isolate the impact of our approach on reordering, like the ones by Birch et al. (2010). System DL eval08nw reo08nw eval09nw baseline 6 43.10 46.90 48.13 reord. training + plain input 6 43.67 46.64 48.53 lattice 4 44.04 47.51 48.96 oracle reord. 4 44.36 48.25 na Table 1: BLEU scores of baseline and reordered system on plain test and on reordering lattices. 240 6 Related Work Linguistically motivated word reordering for Arabic-English has been proposed in several recent works. Habash (2007) extracts syntactic</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of BLEU in machine translation research. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Nizar Habash</author>
</authors>
<title>Using shallow syntax information to improve word alignment and reordering for smt.</title>
<date>2008</date>
<booktitle>In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22298" citStr="Crego and Habash, 2008" startWordPosition="3671" endWordPosition="3674">nguistically motivated word reordering for Arabic-English has been proposed in several recent works. Habash (2007) extracts syntactic reordering rules from a word-aligned parallel corpus whose Arabic side has been fully parsed. The rules involve reordering of syntactic constituents and are applied in a deterministic way (always the most probable) as preprocessing of training and test data. The technique achieves consistent improvements only in very restrictive conditions: maximum phrase size of 1 and monotonic decoding, thus failing to enhance the existing reordering capabilities of PSMT. In (Crego and Habash, 2008; Elming and Habash, 2009) possible input permutations are represented through a word graph, which is then processed by a monotonic phrase- or n-gram-based decoder. Thus, these approaches are conceived as alternatives, rather than integrations, to PSMT reordering. On the contrary, we focused on a single type of significant long reorderings, in order to integrate class-specific reordering methods into a standard PSMT system. To our knowledge, the work by Niehues and Kolss (2009) on German-English is the only example of a lattice-based reordering approach being coupled with reordering at decodin</context>
</contexts>
<marker>Crego, Habash, 2008</marker>
<rawString>Josep M. Crego and Nizar Habash. 2008. Using shallow syntax information to improve word alignment and reordering for smt. In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation, pages 53–61, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Kadri Hacioglu</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Short Papers,</booktitle>
<volume>2</volume>
<pages>149--152</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="6066" citStr="Diab et al. (2004)" startWordPosition="955" endWordPosition="958">e a correct translation. In order to restrict the set of possible movements of a verb and to abstract from the usual token-based movement length measure, we decided to use shallow syntax chunking of the source language. Full syntactic parsing is another option which we have not tried so far mainly because popular parsers that are available for Arabic do not mark grammatical relations such as the ones we are interested in. We assume that Arabic verb reordering only occurs between shallow syntax chunks, and not within them. For this purpose we annotated our Arabic data with the AMIRA chunker by Diab et al. (2004)2. The resulting chunks are generally short (1.6 words on average). We then consider a specific type of reordering by defining a production rule of the kind: “move a chunk of type T along with its L left neighbours and R right neighbours by a shift of S chunks”. A basic set of rules 2This tool implies morphological segmentation of the Arabic text. All word statistics in this paper refer to AMIRAsegmented text. that displaces the verbal chunk to the right by at most 10 positions corresponds to the setting: T=’VP’, L=0, R=0, S=1..10 In order to address cases where the verb is moved along with it</context>
</contexts>
<marker>Diab, Hacioglu, Jurafsky, 2004</marker>
<rawString>Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Short Papers, pages 149–152, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1012</pages>
<contexts>
<context position="16627" citStr="Dyer et al., 2008" startWordPosition="2724" endWordPosition="2727">act of VSO sentences on Arabic-English translation performance, we now propose a method to improve the handling of this phenomenon at decoding time. As in real working conditions word alignments of the input text are not available, we explore a reordering lattice approach. 5.1 Lattice Construction Firstly conceived to optimally encode multiple transcription hypothesis produced by a speech recognizer, word lattices have later been used to represent various forms of input ambiguity, mainly at the level of token boundaries (e.g. word segmentation, morphological decomposition, word decompounding (Dyer et al., 2008)). A main problem when dealing with permutations is that the lattice size can grow very quickly when medium to long reorderings are represented. We are particularly concerned with this issue because our decoding will perform additional reordering on the lattice input. Thanks to the restrictions we set on our verb movement reordering rules described in Sect. 2 – i.e. only reordering between chunks and no overlap between consecutive verb chunks movement – we are able to produce quite compact word lattices. Fig. 5 illustrates how a chunk-based reordering lattice is generated. Suppose we want to t</context>
<context position="18924" citStr="Dyer et al., 2008" startWordPosition="3114" endWordPosition="3117">Fig. 6) that introduces at most 5xAS chunk edges for each verb chunk, where AS is the permitted movement range (6 in this case). Before translation, each edge has to be associated with a weight that the decoder will use as additional feature. To differentiate between the original word order and verb reordering we assign a fixed weight of 1 to the edges of the plain path, and 0.25 to the other edges. As future work we will devise more discriminative weighting schemes. 5.2 Evaluation For the experiments, we relied on the existing Moses-implementation of non-monotonic decoding for word lattices (Dyer et al., 2008) with some fixes concerning the computation of reordering distance. The translation system is the same as the one presented in Sect. 4, to which we added an additional feature function evaluating the lattice weights (weight-i). Instead of rerunning MERT, we directly estimated the additional feature-function weight over a suitable interval (0.002 to 0.5), by running the decoder several times on the development set. The resulting optimal weight was 0.05. Table 1 presents results on three test sets: Eval08-NW which was used to calibrate the reordering rules, Reo08-NW a specific test set consistin</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012–</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date></date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ohio,</location>
<marker>Columbus, </marker>
<rawString>1020, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Nizar Habash</author>
</authors>
<title>Syntactic reordering for English-Arabic phrase-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>69--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="22324" citStr="Elming and Habash, 2009" startWordPosition="3675" endWordPosition="3678">ord reordering for Arabic-English has been proposed in several recent works. Habash (2007) extracts syntactic reordering rules from a word-aligned parallel corpus whose Arabic side has been fully parsed. The rules involve reordering of syntactic constituents and are applied in a deterministic way (always the most probable) as preprocessing of training and test data. The technique achieves consistent improvements only in very restrictive conditions: maximum phrase size of 1 and monotonic decoding, thus failing to enhance the existing reordering capabilities of PSMT. In (Crego and Habash, 2008; Elming and Habash, 2009) possible input permutations are represented through a word graph, which is then processed by a monotonic phrase- or n-gram-based decoder. Thus, these approaches are conceived as alternatives, rather than integrations, to PSMT reordering. On the contrary, we focused on a single type of significant long reorderings, in order to integrate class-specific reordering methods into a standard PSMT system. To our knowledge, the work by Niehues and Kolss (2009) on German-English is the only example of a lattice-based reordering approach being coupled with reordering at decoding time. In their paper, di</context>
</contexts>
<marker>Elming, Habash, 2009</marker>
<rawString>Jakob Elming and Nizar Habash. 2009. Syntactic reordering for English-Arabic phrase-based machine translation. In Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3313" citStr="Galley and Manning, 2008" startWordPosition="506" endWordPosition="509">hese, however, appear to be mostly local, therefore more likely to be modeled through phrase internal alignments, or to be captured by the reordering capabilities of the decoder. In general there is a quite uneven distribution of word-reordering phenomena in Arabic-English, and long-range movements concentrate on few patterns. Reordering in PSMT is typically performed by (i) constraining the maximum allowed word movement and exponentially penalizing long reorderings (distortion limit and penalty), and (ii) through so-called lexicalized orientation models (Och et al., 2004; Koehn et al., 2007; Galley and Manning, 2008). While the former is mainly aimed at reducing the computational complexity of the decoding algorithm, the latter assigns at each decoding step a score to the next source phrase to cover, according to its orientation with respect to the last translated phrase. In fact, neither method discriminates among different reordering distances for a specific word or syntactic class. To our view, this could be a reason for their inadequacy to properly deal with the reordering peculiarities of the Arabic-English language pair. In 235 Proceedings of the Joint 5th Workshop on Statistical Machine Translation</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 848–856, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>228--335</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="2096" citStr="Germann et al., 2001" startWordPosition="321" endWordPosition="324">b-SubjectObject (VSO) sentences – a class that is highly represented in the news genre1. Fig. 1 shows two examples where the Arabic main verb phrase comes before the subject. In such sentences, the subject can be followed by adjectives, adverbs, coordinations, or appositions that further increase the distance between the verb 1In fact, Arabic syntax admits both SVO and VSO orders. and its object. When translating into English – a primarily SVO language – the resulting long verb reorderings are often missed by the PSMT decoder either because of pure modeling errors or because of search errors (Germann et al., 2001): i.e. their span is longer than the maximum allowed distortion distance, or the correct reordering hypothesis does not emerge from the explored search space because of a low score. In the two examples, the missed verb reorderings result in different translation errors by the decoder, respectively, the introduction of a subject pronoun before the verb and, even worse, a verbless sentence. In Arabic-English machine translation, other kinds of reordering are of course very frequent: for instance, adjectival modifiers following their noun and head-initial genitive constructions (Idafa). These, ho</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 228–335, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Conal Sathi</author>
<author>Christopher D Manning</author>
</authors>
<title>NP subject detection in verb-initial Arabic clauses.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Computational Approaches to Arabic Scriptbased Languages (CAASL3),</booktitle>
<location>Ottawa, Canada.</location>
<contexts>
<context position="23598" citStr="Green et al. (2009)" startWordPosition="3882" endWordPosition="3885">om a word-aligned corpus are applied to German sentences in the form of weighted edges in a word lattice. Their phrasebased decoder admits local reordering within a fixed window of 2 words, while, in our work, we performed experiments up to a distortion limit of 10. Another major difference is that we used shallow syntax annotation to effectively reduce the number of possible permutations. A first attempt to adapt our technique to the German language is described in Hardmeier et al. (2010). Our work is also tightly related to the problem of noun-phrase subject detection, recently addressed by Green et al. (2009). In fact, detecting the extension of the subject can be a sufficient condition to guess the optimal reordering of the verb. In their paper, a discriminative classifier was trained on a rich variety of linguistic features to detect the full scope of Arabic NP subjects in verb-initial clauses. The authors reported an Fscore of 61.3%, showing that the problem is hard to solve even when more linguistic information is available. In order to integrate the output of the classifier into a PSMT decoder, a specific translation feature was designed to reward hypotheses in which the subject is translated</context>
</contexts>
<marker>Green, Sathi, Manning, 2009</marker>
<rawString>Spence Green, Conal Sathi, and Christopher D. Manning. 2009. NP subject detection in verb-initial Arabic clauses. In Proceedings of the Third Workshop on Computational Approaches to Arabic Scriptbased Languages (CAASL3), Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Syntactic preprocessing for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Bente Maegaard, editor, Proceedings of the Machine Translation Summit XI,</booktitle>
<pages>215--222</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="8792" citStr="Habash (2007)" startWordPosition="1427" endWordPosition="1428">77 sentence pairs. 236 Figure 2: Percentage of verb reorderings by maximum shift (0 stands for no movement). Figure 3: Distortion reduction in the GALE-NW corpus: jump occurrences grouped by length range (in nb. of words). agree with AMIRA-style segmentation. For the second corpus (Eval08-NW), we filtered out sentences longer than 80 tokens in order to make word alignment feasible with GIZA++ (Och and Ney, 2003). We then used the Intersection of the direct and inverse alignments, as computed by Moses. The choice of such a high-precision, lowrecall alignment set is supported by the findings of Habash (2007) on syntactic rule extraction from parallel corpora. 3.1 The Verb’s Dance There are 1,955 verb phrases in Gale-NW and 11,833 in Eval08-NW. Respectively 86% and 84% of these do not need to be moved according to the alignments. The remaining 14% and 16% are distributed by movement length as shown in Fig. 2: most verb reorderings consist in a 1-chunk long jump to the right (8.3% in Gale-NW and 11.6% in Eval08-NW). The rest of the distribution is similar in the two corpora, which indicates a good correspondence between verb reordering observed in automatic and manual alignments. By increasing the </context>
<context position="21790" citStr="Habash (2007)" startWordPosition="3595" endWordPosition="3596">ns (Callison-Burch et al., 2006). For the future, we plan to also use specific evaluation metrics that will allow us to isolate the impact of our approach on reordering, like the ones by Birch et al. (2010). System DL eval08nw reo08nw eval09nw baseline 6 43.10 46.90 48.13 reord. training + plain input 6 43.67 46.64 48.53 lattice 4 44.04 47.51 48.96 oracle reord. 4 44.36 48.25 na Table 1: BLEU scores of baseline and reordered system on plain test and on reordering lattices. 240 6 Related Work Linguistically motivated word reordering for Arabic-English has been proposed in several recent works. Habash (2007) extracts syntactic reordering rules from a word-aligned parallel corpus whose Arabic side has been fully parsed. The rules involve reordering of syntactic constituents and are applied in a deterministic way (always the most probable) as preprocessing of training and test data. The technique achieves consistent improvements only in very restrictive conditions: maximum phrase size of 1 and monotonic decoding, thus failing to enhance the existing reordering capabilities of PSMT. In (Crego and Habash, 2008; Elming and Habash, 2009) possible input permutations are represented through a word graph,</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In Bente Maegaard, editor, Proceedings of the Machine Translation Summit XI, pages 215–222, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>FBK at WMT 2010: Word lattices for morphological reduction and chunk-based reordering.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="23473" citStr="Hardmeier et al. (2010)" startWordPosition="3860" endWordPosition="3863">pproach being coupled with reordering at decoding time. In their paper, discontinuous non-deterministic POSbased rules learned from a word-aligned corpus are applied to German sentences in the form of weighted edges in a word lattice. Their phrasebased decoder admits local reordering within a fixed window of 2 words, while, in our work, we performed experiments up to a distortion limit of 10. Another major difference is that we used shallow syntax annotation to effectively reduce the number of possible permutations. A first attempt to adapt our technique to the German language is described in Hardmeier et al. (2010). Our work is also tightly related to the problem of noun-phrase subject detection, recently addressed by Green et al. (2009). In fact, detecting the extension of the subject can be a sufficient condition to guess the optimal reordering of the verb. In their paper, a discriminative classifier was trained on a rich variety of linguistic features to detect the full scope of Arabic NP subjects in verb-initial clauses. The authors reported an Fscore of 61.3%, showing that the problem is hard to solve even when more linguistic information is available. In order to integrate the output of the classi</context>
</contexts>
<marker>Hardmeier, Bisazza, Federico, 2010</marker>
<rawString>Christian Hardmeier, Arianna Bisazza, and Marcello Federico. 2010. FBK at WMT 2010: Word lattices for morphological reduction and chunk-based reordering. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1344" citStr="Koehn et al., 2007" startWordPosition="198" endWordPosition="201">t statistics about verb movements. From this analysis, specific verb reordering lattices are then built on the test sentences before decoding them. The application of our reordering methods on the training and test sets results in consistent BLEU score improvements on the NIST-MT 2009 ArabicEnglish benchmark. 1 Introduction Shortcomings of phrase-based statistical machine translation (PSMT) with respect to word reordering have been recently shown on the ArabicEnglish pair by Birch et al. (2009). An empirical investigation of the output of a strong baseline we developed with the Moses toolkit (Koehn et al., 2007) for the NIST 2009 evaluation, revealed that an evident cause of syntactic disfluency is the anticipation of the verb in Arabic Verb-SubjectObject (VSO) sentences – a class that is highly represented in the news genre1. Fig. 1 shows two examples where the Arabic main verb phrase comes before the subject. In such sentences, the subject can be followed by adjectives, adverbs, coordinations, or appositions that further increase the distance between the verb 1In fact, Arabic syntax admits both SVO and VSO orders. and its object. When translating into English – a primarily SVO language – the result</context>
<context position="3286" citStr="Koehn et al., 2007" startWordPosition="502" endWordPosition="505">tructions (Idafa). These, however, appear to be mostly local, therefore more likely to be modeled through phrase internal alignments, or to be captured by the reordering capabilities of the decoder. In general there is a quite uneven distribution of word-reordering phenomena in Arabic-English, and long-range movements concentrate on few patterns. Reordering in PSMT is typically performed by (i) constraining the maximum allowed word movement and exponentially penalizing long reorderings (distortion limit and penalty), and (ii) through so-called lexicalized orientation models (Och et al., 2004; Koehn et al., 2007; Galley and Manning, 2008). While the former is mainly aimed at reducing the computational complexity of the decoding algorithm, the latter assigns at each decoding step a score to the next source phrase to cover, according to its orientation with respect to the last translated phrase. In fact, neither method discriminates among different reordering distances for a specific word or syntactic class. To our view, this could be a reason for their inadequacy to properly deal with the reordering peculiarities of the Arabic-English language pair. In 235 Proceedings of the Joint 5th Workshop on Stat</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Word-based alignment, phrase-based translation: What’s the link?</title>
<date>2006</date>
<booktitle>In 5th Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="14686" citStr="Lopez and Resnik, 2006" startWordPosition="2415" endWordPosition="2418"> DL values (8-10) imply a larger search space and because we want to give Moses the best possible conditions to properly handle long reordering, we relaxed for these conditions the default pruning parameter to the point that led the highest BLEU score5. 4.2 Discussion The first observation is that the reordered system always performs better (0.5�0.6 points) than the baseline on the plain test, despite the mismatch between training and test ordering. This may be due to the fact that automatic word alignments are more accurate when less reordering is present in the data, although previous work (Lopez and Resnik, 2006) showed that even large gains in alignment accuracy seldom lead to better translation performances. Moreover phrase extraction may benefit from a distortion reduction, since its heuristics rely on word order in order to expand the context of alignment links. The results on the oracle reordered test are also interesting: a gain of at least 1.2 point absolute over the baseline is reported in all tested DL conditions. These improvements are remarkable, keeping in mind that only 31% of the train and 33% of the test sentences get modified by verb reordering. 5That is, the histogram pruning maximum </context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>Adam Lopez and Philip Resnik. 2006. Word-based alignment, phrase-based translation: What’s the link? In 5th Conference of the Association for Machine Translation in the Americas (AMTA), Boston, Massachusetts, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-based model for long-range reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>206--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="22780" citStr="Niehues and Kolss (2009)" startWordPosition="3747" endWordPosition="3750">imum phrase size of 1 and monotonic decoding, thus failing to enhance the existing reordering capabilities of PSMT. In (Crego and Habash, 2008; Elming and Habash, 2009) possible input permutations are represented through a word graph, which is then processed by a monotonic phrase- or n-gram-based decoder. Thus, these approaches are conceived as alternatives, rather than integrations, to PSMT reordering. On the contrary, we focused on a single type of significant long reorderings, in order to integrate class-specific reordering methods into a standard PSMT system. To our knowledge, the work by Niehues and Kolss (2009) on German-English is the only example of a lattice-based reordering approach being coupled with reordering at decoding time. In their paper, discontinuous non-deterministic POSbased rules learned from a word-aligned corpus are applied to German sentences in the form of weighted edges in a word lattice. Their phrasebased decoder admits local reordering within a fixed window of 2 words, while, in our work, we performed experiments up to a distortion limit of 10. Another major difference is that we used shallow syntax annotation to effectively reduce the number of possible permutations. A first </context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-based model for long-range reorderings in SMT. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 206–214, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8594" citStr="Och and Ney, 2003" startWordPosition="1392" endWordPosition="1395">Evaluation. The first corpus (Gale-NW) contains human-made alignments. As these refer to non-segmented text, they were adjusted to 3Newswire sections of LDC2006E93 and LDC2009E08, respectively 4337 and 777 sentence pairs. 236 Figure 2: Percentage of verb reorderings by maximum shift (0 stands for no movement). Figure 3: Distortion reduction in the GALE-NW corpus: jump occurrences grouped by length range (in nb. of words). agree with AMIRA-style segmentation. For the second corpus (Eval08-NW), we filtered out sentences longer than 80 tokens in order to make word alignment feasible with GIZA++ (Och and Ney, 2003). We then used the Intersection of the direct and inverse alignments, as computed by Moses. The choice of such a high-precision, lowrecall alignment set is supported by the findings of Habash (2007) on syntactic rule extraction from parallel corpora. 3.1 The Verb’s Dance There are 1,955 verb phrases in Gale-NW and 11,833 in Eval08-NW. Respectively 86% and 84% of these do not need to be moved according to the alignments. The remaining 14% and 16% are distributed by movement length as shown in Fig. 2: most verb reorderings consist in a 1-chunk long jump to the right (8.3% in Gale-NW and 11.6% in</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="3266" citStr="Och et al., 2004" startWordPosition="498" endWordPosition="501">tial genitive constructions (Idafa). These, however, appear to be mostly local, therefore more likely to be modeled through phrase internal alignments, or to be captured by the reordering capabilities of the decoder. In general there is a quite uneven distribution of word-reordering phenomena in Arabic-English, and long-range movements concentrate on few patterns. Reordering in PSMT is typically performed by (i) constraining the maximum allowed word movement and exponentially penalizing long reorderings (distortion limit and penalty), and (ii) through so-called lexicalized orientation models (Och et al., 2004; Koehn et al., 2007; Galley and Manning, 2008). While the former is mainly aimed at reducing the computational complexity of the decoding algorithm, the latter assigns at each decoding step a score to the next source phrase to cover, according to its orientation with respect to the last translated phrase. In fact, neither method discriminates among different reordering distances for a specific word or syntactic class. To our view, this could be a reason for their inadequacy to properly deal with the reordering peculiarities of the Arabic-English language pair. In 235 Proceedings of the Joint </context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="13387" citStr="Och, 2003" startWordPosition="2203" endWordPosition="2204"> verb reordering procedure on training only. The latter will provide an estimate of the maximum improvement we can expect from the application to the test of an optimal verb reordering prediction technique. Given our experimental setting, one could argue that our BLEU score is biased because one of the references was also used to generate the verb reordering. However, in a series of experiments not reported here, we evaluated the same systems using only the remaining three references and observed similar trends as when all four references are used. Feature weights were optimized through MERT (Och, 2003) on the newswire section of the NISTMT06 evaluation set (Dev06-NW), in the original version for the baseline system, in the verbreordered version for the reordered system. 4LDC2007T08, 2003T07, 2004E72, 2004T17, 2004T18, 2005E46, 2006E25, 2006E44 and LDC2006E39 – the two last with first reference only. Figure 4: BLEU scores of baseline and reordered system on plain and oracle reordered Eval08-NW. Fig. 4 shows the results in terms of BLEU score for (i) the baseline system, (ii) the reordered system on a plain version of Eval08-NW and (iii) the reordered system on the reordered test. The scores </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>