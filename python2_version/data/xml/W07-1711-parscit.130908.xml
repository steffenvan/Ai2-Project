<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030723">
<title confidence="0.945588">
Multilingual Word Sense Discrimination: A Comparative Cross-Linguistic
Study
</title>
<author confidence="0.992286">
Alla Rozovskaya
</author>
<affiliation confidence="0.9971375">
Department of Linguistics
Univ. of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.814219">
Urbana, IL 61801
</address>
<email confidence="0.999087">
rozovska@uiuc.edu
</email>
<author confidence="0.994804">
Richard Sproat
</author>
<affiliation confidence="0.997374">
Department of Linguistics
Univ. of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.814283">
Urbana, IL 61801
</address>
<email confidence="0.99919">
rws@uiuc.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947423076923">
We describe a study that evaluates an ap-
proach to Word Sense Discrimination on
three languages with different linguistic
structures, English, Hebrew, and Russian.
The goal of the study is to determine
whether there are significant performance
differences for the languages and to iden-
tify language-specific problems. The algo-
rithm is tested on semantically ambiguous
words using data from Wikipedia, an online
encyclopedia. We evaluate the induced clus-
ters against sense clusters created manually.
The results suggest a correlation between the
algorithm’s performance and morphological
complexity of the language. In particular,
we obtain FScores of 0.68 , 0.66 and 0.61 for
English, Hebrew, and Russian, respectively.
Moreover, we perform an experiment on
Russian, in which the context terms are lem-
matized. The lemma-based approach signif-
icantly improves the results over the word-
based approach, by increasing the FScore by
16%. This result demonstrates the impor-
tance of morphological analysis for the task
for morphologically rich languages like Rus-
sian.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992355">
Ambiguity is pervasive in natural languages and cre-
ates an additional challenge for Natural Language
applications. Determining the sense of an ambigu-
ous word in a given context may benefit many NLP
</bodyText>
<page confidence="0.98075">
82
</page>
<bodyText confidence="0.996728735294118">
tasks, such as Machine Translation, Question An-
swering, or Text-to-Speech synthesis.
The Word Sense Discrimination (WSD) or Word
Sense Induction task consists of grouping together
the occurrences of a semantically ambiguous term
according to its senses. Word Sense Discrimination
is similar to Word Sense Disambiguation, but allows
for a more unsupervised approach to the problem,
since it does not require a pre-defined set of senses.
This is important, given the number of potentially
ambiguous words in a language. Moreover, labeling
an occurrence with its sense is not always necessary.
For example, in Information Retrieval WSD would
be useful for the identification of documents relevant
to a query containing an ambiguous term.
Different approaches to WSD have been pro-
posed, but the evaluation is often conducted using
a single language, so it is difficult to predict per-
formance on another language. To the best of our
knowledge, there has not been a systematic com-
parative analysis of WSD systems on different lan-
guages. Yet, it is interesting to see whether there
are significant differences in performance when a
method is applied to several languages that have dif-
ferent linguistic structures. Identifying the reasons
for performance differences might suggest what fea-
tures are useful for the task.
The present project adopts an approach to WSD
that is based on similarity measure between context
terms of an ambiguous word. We compare the per-
formance of an algorithm for WSD on English, He-
brew, and Russian, using lexically ambiguous words
and corpora of similar sizes.
We believe that testing on the above languages
</bodyText>
<subsubsectionHeader confidence="0.755404">
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 82–87,
</subsubsectionHeader>
<bodyText confidence="0.998955586206897">
Prague, June 2007. c�2007 Association for Computational Linguistics
might give an idea about how accuracy of an algo-
rithm for WSD is affected by language choice. Rus-
sian is a member of the Slavic language group and is
morphologically rich. Verbs, nouns, and adjectives
are characterized by a developed inflectional system,
which results in a large number of wordforms. He-
brew is a Semitic language, and is complex in a dif-
ferent way. In addition to the root-pattern morphol-
ogy that affects the word stem, it also has a com-
plex verb declination system. Moreover, function
words, such as prepositions and determiners, cliti-
cize, thereby increasing the number of wordforms.
Lastly, cliticization, coupled with the absence of
short vowels in text, introduces an additional level
of ambiguity for Hebrew.
There are two main findings to this study. First,
we show that the morphological complexity of the
language affects the performance of the algorithm
for WSD. Second, the lemma-based approach to
Russian WSD significantly improves the results over
the word-based approach.
The rest of the paper is structured as follows:
first, we describe previous work that is related to the
project. Section 3 provides details about the algo-
rithm for WSD that we use. We then describe the
experiments and the evaluation methodology in Sec-
tions 4 and 5, respectively. We conclude with a dis-
cussion of the results and directions for future work.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999869">
First, we describe several approaches to WSD that
are most relevant to the present project: Since we
are dealing with languages that do not have many
linguistic resources available, we chose a most unsu-
pervised, knowledge-poor approach to the task that
relies on words occurring in the context of an am-
biguous word. Next, we consider two papers on
WSD that provide evaluation for two languages. Fi-
nally, we describe work that is concerned with the
role of morphology for the task.
</bodyText>
<subsectionHeader confidence="0.996287">
2.1 Approaches to Word Sense Discrimination
</subsectionHeader>
<bodyText confidence="0.99998096875">
Pantel and Lin (2002) learn word sense induction
from an untagged corpus by finding the set of the
most similar words to the target and by clustering
the words. Each word cluster corresponds to a sense.
Thus, senses are viewed as clusters of words.
Another approach is based on clustering the oc-
currences of an ambiguous word in a corpus into
clusters that correspond to distinct senses of the
word. Based on this approach, a sense is defined
as a cluster of contexts of an ambiguous word. Each
occurrence of an ambiguous word is represented as a
vector of features, where features are based on terms
occurring in the context of the target word. For ex-
ample, Pedersen and Bruce (1997) cluster the oc-
currences of an ambiguous word by constructing a
vector of terms occurring in the context of the tar-
get. Sch¨utze (1992) presents a method that explores
the similarity between the context terms occurring
around the target. This is accomplished by consider-
ing feature vectors of context terms of the ambigu-
ous word. The algorithm is evaluated on natural and
artificially-constructed ambiguous English words.
Sproat and van Santen (1998) introduce a tech-
nique for automatic detection of ambiguous words
in a corpus and measuring their degree of polysemy.
This technique employs a similarity measure be-
tween the context terms similar in spirit to the one
in (Sch¨utze, 1992) and singular value decomposi-
tion in order to detect context terms that are impor-
tant for disambiguating the target. They show that
the method is capable of identifying polysemous En-
glish words.
</bodyText>
<subsectionHeader confidence="0.999356">
2.2 Cross-Linguistic Study of WSD
</subsectionHeader>
<bodyText confidence="0.999967315789474">
Levinson (1999) presents an approach to WSD that
is evaluated on English and Hebrew. He finds 50
most similar words to the target and clusters them
into groups, the number of groups being the num-
ber of senses. He reports comparable results for
the two languages, but he uses both morphologi-
cally and lexically ambiguous words. Moreover, the
evaluation methodology focuses on the success of
disambiguation for an ambiguous word, and reports
the number of ambiguous words that were disam-
biguated successfully.
Davidov and Rappoport (2006) describe an al-
gorithm for unsupervised discovery of word cate-
gories and evaluate it on Russian and English cor-
pora. However, the focus of their work is on the dis-
covery of semantic categories and from the results
they report for the two languages it is difficult to in-
fer how the languages compare against each other.
We conduct a more thorough evaluation. We also
</bodyText>
<page confidence="0.993292">
83
</page>
<bodyText confidence="0.999790333333333">
control cross-linguistically for number of training
examples and level of ambiguity of selected words,
as described in Section 4.
</bodyText>
<subsectionHeader confidence="0.995713">
2.3 Morphology and WSD
</subsectionHeader>
<bodyText confidence="0.9999412">
McRoy (1992) describes a study of different sources
useful for word sense disambiguation, including
morphological information. She reports that mor-
phology is useful, but the focus is on derivational
morphology of the English language. In the present
context, we are interested in the effect of inflectional
morphology on WSD, especially for languages, such
as Russian and Hebrew.
Gaustad (2004) proposes a lemma-based ap-
proach to a Maximum Entropy Word Sense Disam-
biguation System for Dutch. She shows that collaps-
ing wordforms of an ambiguous word yields a more
robust classifier due to the availability of more train-
ing data. The results indicate an improvement of this
approach over classification based on wordforms.
</bodyText>
<sectionHeader confidence="0.994644" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.984912514285714">
Our algorithm relies on the method for selection of
relevant contextual terms and on distance measure
between them introduced in (Sproat and van Santen,
1998) and on the approach described in (Sch¨utze,
1998), though the details of clustering differ slightly.
The intuition behind the algorithm can be summa-
rized as follows: (1) words that occur in the context
of the ambiguous word are useful for determining
its sense; and (2) contextual terms of an ambiguous
word belong to topics corresponding to the senses
of the ambiguous word. Before describing the algo-
rithm in detail, we give an overview of the system.
The algorithm starts by collecting all the occur-
rences of an ambiguous word in the corpus together
with the surrounding context. Next, we build a sym-
metric distance matrix D, where rows and columns
correspond to context terms, and D[i][j] is the dis-
tance value of term i and term j. The distance mea-
sure is supposed to reflect how the two terms are
close semantically (whether they are related to the
same topic). For example, we would expect the dis-
tance between the words financial and money to be
smaller than the distance between the words finan-
cial and river: The first pair is more likely to occur
in the same context, than the second one. Using the
distance measure, the context terms are partitioned
into sense clusters. Finally, we group the sentences
containing the ambiguous word into sentence clus-
ters using the context term clusters.
We now describe each step in detail:
1. We collect contextual terms of an ambigu-
ous word w in a context window of 50 words
around the target. Each context term t is as-
signed a weight (Sproat and J. van Santen,
1998):
</bodyText>
<equation confidence="0.994929">
wt = CO(t|w) (1)
FREQ(t)
</equation>
<bodyText confidence="0.998920875">
CO(tjw) is the frequency of the term in the
context of w, and FREQ(t) is the frequency
of the term in the corpus. Term weights are
used to select context terms that will be help-
ful in determining the sense of the ambiguous
word in a particular context. Furthermore, term
weights are employed in (4) in sentence clus-
tering.
</bodyText>
<listItem confidence="0.586926666666667">
2. For each pair ti and tj of context terms, we
compute the distance between them (Sproat
and J. van Santen,1998):
</listItem>
<equation confidence="0.99486375">
�COw(ti|tj) COw(tj|ti)
FREG(ti) + FREQ(tj) �
�������� = 1 �(2)
2
</equation>
<bodyText confidence="0.993187666666667">
COw(ti�tj) is the frequency of ti in the con-
text of tj, and FREQ(ti) is the frequency of
ti in the training corpus. We assume that the
distance between ti and tj is inversely propor-
tional to the semantic similarity between ti and
tj.
</bodyText>
<listItem confidence="0.992335857142857">
3. Using the distance matrix from (2), the con-
text terms are clustered using an agglomerative
clustering technique:
• Start by assigning each context term to a separate
cluster
• While stopping criterion is false: merge two clus-
ters whose distance 1 is the smallest.2
</listItem>
<footnote confidence="0.837264714285714">
1There are several ways to define the distance between clus-
ters. Having experimented with three - Single Link, Complete
Link and Group Average, it was found that Complete Link def-
inition works best for the present task. (Complete Link distance
between clusters i and j is defined as the maximum distance be-
tween a term from cluster i and a term from cluster j).
2In the present study, the clusters are merged as long as the
</footnote>
<page confidence="0.999361">
84
</page>
<tableCaption confidence="0.860114333333333">
The output of step (3) is a set of context term
clusters for the target word. Below are shown
select members for term clusters for the English
word bass:
Cluster 1: songwriter singer joined keyboardist
Cluster 2: waters fishing trout feet largemouth
</tableCaption>
<bodyText confidence="0.9884744">
4. Finally, the sentences containing the ambigu-
ous word are grouped using the context term
clusters from (3). Specifically, given a sen-
tence with the ambiguous word, we compute
the score of the sentence with respect to each
context word cluster in (3) and assign the sen-
tence to the cluster with the highest score. The
score of the sentence with respect to cluster c
is the sum of weights of sentence context terms
that are in c.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999826263157895">
The algorithm is evaluated on 9 ambiguous words
with two-sense distinctions. We select words that
(i) have the same two-sense distinction in all three
languages or (ii) are ambiguous in one of the lan-
guages, but each of their senses corresponds to an
unambiguous translation in the other two languages.
In the latter case, the translations are merged to-
gether to create an artificially ambiguous word. We
believe that this selection approach allows for a col-
lection of a comparable set of ambiguous words for
the three languages. An example of an ambiguous
word is the English word table, that corresponds to
two gross sense distinctions (tabular array, and a
piece offurniture). This word has two translations
into Russian and Hebrew, that correspond to the two
senses. The selected words are presented in Table 1.
The words display different types of ambigu-
ity. In particular, disambiguating the Hebrew word
gishah (access; approach) or the Russian word mir
(peace; world) would be useful in Machine Transla-
tion, while determining the sense of a word like lan-
guage would benefit an Information Retrieval sys-
tem. It should also be noted that several words pos-
sess additional senses, which were ignored because
they rarely occurred in the corpus. For example, the
Russian word yazyk (language) also has the meaning
of tongue (body part).
number of clusters exceeds the number of senses of the ambigu-
ous word in the test data.
The corpus for each language consists of 15M
word tokens, and for the same ambiguous word the
same number of training examples is selected from
each language. For each ambiguous word, a set of
100-150 examples together with 50 words of con-
text is selected from the section of the corpus not
used for training. These examples are manually an-
notated for senses and used as the test set for each
language.
</bodyText>
<sectionHeader confidence="0.994594" genericHeader="method">
5 Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.99886425">
The evaluation is conducted by comparing the in-
duced sentence clusters to clusters created manually.
We use three evaluation measures : cluster purity,
entropy, and FScore. 3
For a cluster Cr of size qr, where the size is the
number of examples in that cluster, the dominating
sense Si in that cluster is selected and cluster purity
is computed as follows:
</bodyText>
<equation confidence="0.957121">
� (3)
</equation>
<bodyText confidence="0.999978631578948">
where nir is the number of examples in cluster Cr
with sense Si.
For an ambiguous word w, cluster purity P(w) is
the weighted average of purities of the clusters for
that word. 4. Higher cluster purity score corresponds
to a better clustering outcome.
Entropy and FScore measures are described in de-
tail in Zhao and Karypis (2005). Entropy indicates
how distinct senses are distributed between the two
clusters. The perfect distribution is the assignment
of all examples with sense 1 to one cluster and all
examples with sense 2 to the other cluster. In such
case, the entropy is 0. In general, a lower value in-
dicates a better cluster quality. Entropy is computed
for each cluster. Entropy for word w is the weighted
average of the entropies of the clusters for that word.
Finally, FScore considers both the coverage of the
algorithm and its ability to discriminate between the
two senses. FScore is computed as the harmonic
</bodyText>
<footnote confidence="0.967657">
3Examples whose scores with respect to all clusters are zero
(examples that do not contain any terms found in the distance
matrix) are not assigned to any cluster, and thus do not affect
cluster purity and cluster entropy. This is captured by the FS-
core measure described below.
4In the present study, the number of clusters and the number
of senses for a word is always 2
</footnote>
<equation confidence="0.970869">
Z
P(C*) = n*
q*
</equation>
<page confidence="0.997597">
85
</page>
<table confidence="0.9991152">
Senses English Hebrew Russian
access;approach access;approach gishah dostup;podxod
actor;player actor;player saxqan akter;igrok
evidence; quarrel argument vikuax;nimuq argument
body part; chief head rosh golova;glava
world;peace world; peace shalom;olam mir
furniture; tabular array table shulxan;tavlah stol;tablitza
allow;resolve allow;resolve hershah;patar razreshat’
ambiance; air atmosphere avira;atmosfera atmosfera
human lang.;program. lang. language safah yazyk
</table>
<tableCaption confidence="0.840740166666667">
Table 1: Ambiguous words for testing: The first column indicates the senses; unambiguous translations that
were merged to create an ambiguous word are indicated by a semicolon
mean of Precision and Recall, where recall and pre-
cision for sense Si with respect to cluster Cr are
computed by treating cluster Cr as the output of a
retrieval system for sense Si .
</tableCaption>
<sectionHeader confidence="0.99854" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999992476923077">
We show results for two experiments. Experi-
ment 1 compares the algorithm’s performance cross-
linguistically without morphological analysis ap-
plied to any of the languages. Experiment 2 com-
pares the performance for Russian in two settings:
with and without morphological processing per-
formed on the context terms.
Table 2 presents experimental results. Baseline is
computed by assigning the most common sense to
all occurrences of the ambiguous word. We observe
that English achieves the highest performance both
in terms of cluster purity and FScore, while Russian
performs most poorly among the three languages.
This behavior may be correlated with the average
frequency of the context terms that are used to con-
struct the distance matrix in the corpus (cf. 7 for
English and 4.2 for Russian). In particular, the dif-
ference in the frequencies can be attributed to the
morphological complexity of Russian, as compared
to English and Hebrew. Hebrew is more complex
than English morphologically, which would account
for a drop in performance for the Hebrew words vs.
the English words. Furthermore, one would expect
a higher degree of ambiguity for Hebrew due to the
absence of short vowels in text.
It is worth noting that while both Hebrew and
Russian possess features that might negatively af-
fect the performance, Hebrew performs better than
Russian. We hypothesize that cliticization and the
lack of vowels in text are not as significant factors
for the performance as the high inflectional nature
of a language, such as Russian. We observe that the
majotity of the context terms selected by the algo-
rithm for disambiguation belong to the noun cate-
gory. This seems intuitive, since nouns generally
provide more information content than other parts
of speech and thus should be useful for resolving
lexical ambiguity. While an English or a Hebrew
noun only has several wordforms, a Russian noun
may have up to 12 different forms due to various in-
flections.
The morphological complexity of Russian affects
the performance in two ways. First, cluster purity
is affected, since the counts of context terms are not
sufficiently reliable to accurately estimate term dis-
tances. Incorrect term distances subsequently affect
the quality of the term clusters. Second, the percent-
age of default occurrences (examples that have no
context terms occurring in the distance matrix) is the
least for English (0.22) and the highest for Russian
(0.27). The default occurrences affect the recall.
The results of experiment 2 support the fact that
morphological complexity of a language negatively
affects the performance. In that experiment, the in-
flections are removed from all the context terms. We
apply a morphological analyzer 5 to the corpus and
replace each word with its lemma. In 10% of the
word tokens, the analyzer gives more than one pos-
sible analysis, in which case the first analysis is se-
lected. As can be seen in Table 2 (last row), remov-
ing inflections produces a significant improvement
both in recall and precision, while preserving the
cluster purity and slightly reducing cluster entropy.
Moreover, the performance in terms of recall, pre-
cision, and coverage is better than for English and
</bodyText>
<footnote confidence="0.998939">
5Available at http://www.aot.ru/
</footnote>
<page confidence="0.98386">
86
</page>
<table confidence="0.9999068">
Language Baseline Coverage Precision Recall FScore Purity Entropy
English 0.73 0.78 0.77 0.61 0.68 0.79 0.61
Hebrew 0.72 0.79 0.76 0.58 0.66 0.82 0.59
Russian 0.72 0.73 0.70 0.54 0.61 0.81 0.62
Russian(lemma) 0.72 0.80 0.77 0.66 0.71 0.82 0.61
</table>
<tableCaption confidence="0.932796">
Table 2: Results: Baseline is the most frequent sense; coverage is the number of occurrences on which the
decision was made by the algorithm
</tableCaption>
<bodyText confidence="0.613485">
Hebrew.
</bodyText>
<sectionHeader confidence="0.995507" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99998588">
We have described a cross-linguistic study of a
Word Sense Discrimination technique. An algo-
rithm based on context term clustering was applied
to ambiguous words from English, Hebrew, and
Russian, and a comparative analysis of the results
was presented. Several observations can be made.
First, the results suggest that the performance can
be affected by morphological complexity in the case
of a language, such as Russian, specifically, both in
terms of precision and recall. Second, removing in-
flectional morphology not only boosts the recall, but
significantly improves the precision. These results
support the view that morphological processing is
beneficial for WSD.
For future work, we plan to investigate more
thoroughly the role of morphological analysis
for WSD in Russian and Hebrew. In particular,
we will focus on the inflectional morphology of
Russian in order to determine whether removing
inflections consistently improves results for Russian
ambiguous words across different parts of speech.
Further, considering the complex structure of the
Hebrew language, we would like to determine what
kind of linguistic processing is useful for Hebrew in
the WSD context.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997922">
We are grateful to Roxana Girju and the anony-
mous reviewers for very useful suggestions and
comments. This work is funded in part by grants
from the National Security Agency and the National
Science Foundation.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999647210526316">
Dmitry Davidov and Ari Rappoport 2006. Efficient Un-
supervised Discovery of Word Categories Using Sym-
metric Patterns and High Frequency Words. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, 297–304.
Sydney, Australia.
Richard O. Duda and Peter E. Hart. 1973. Pattern Classi-
fication and Scene Analysis. John Wiley &amp; Sons, New
York.
Tanja Gaustad. 2004. A Lemma-Based Approach to a
Maximum Entropy Word Sense Disambiguation Sys-
tem for Dutch. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics (Col-
ing 2004), 778-784. Geneva.
Dmitry Levinson. 1999. Corpus-Based Method
for Unsupervised Word Sense Disambiguation.
www.stanford.edu/ dmitryle/acai99w1.ps.
Susan Weber McRoy. 1992. Using Multiple Knowledge
Sources for Word Sense Discrimination. Computa-
tional Linguistics, 18(1): 1–30.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text In In Proceedings of ACM
SIGKDD, pages 613-619. Edmonton.
Ted Pedersen and Rebecca Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of the
Second Conference on Empirical Methods in Natural
Language Processing, 197-207. Providence, RI, Au-
gust.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
Richard Sproat and Jan van Santen. 1998. Automatic
ambiguity detection. In Proceedings of International
Conference on Spoken Language Processing. Sydney,
Australia, 1998.
Ying Zhao and George Karypis. 2005. Hierarchical
Clustering Algorithms for Document Datasets Data
Mining and Knowledge Discovery, 10(2):141–168.
</reference>
<page confidence="0.99947">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296321">
<title confidence="0.9388345">Multilingual Word Sense Discrimination: A Comparative Cross-Linguistic Study</title>
<author confidence="0.967901">Alla</author>
<affiliation confidence="0.9989225">Department of Univ. of Illinois at</affiliation>
<address confidence="0.711471">Urbana, IL</address>
<email confidence="0.999319">rozovska@uiuc.edu</email>
<author confidence="0.994821">Richard</author>
<affiliation confidence="0.999217">Department of Univ. of Illinois at</affiliation>
<address confidence="0.705475">Urbana, IL</address>
<email confidence="0.999394">rws@uiuc.edu</email>
<abstract confidence="0.987743148148148">We describe a study that evaluates an approach to Word Sense Discrimination on three languages with different linguistic structures, English, Hebrew, and Russian. The goal of the study is to determine whether there are significant performance differences for the languages and to identify language-specific problems. The algorithm is tested on semantically ambiguous words using data from Wikipedia, an online encyclopedia. We evaluate the induced clusters against sense clusters created manually. The results suggest a correlation between the algorithm’s performance and morphological complexity of the language. In particular, we obtain FScores of 0.68 , 0.66 and 0.61 for English, Hebrew, and Russian, respectively. Moreover, we perform an experiment on Russian, in which the context terms are lemmatized. The lemma-based approach significantly improves the results over the wordbased approach, by increasing the FScore by 16%. This result demonstrates the importance of morphological analysis for the task for morphologically rich languages like Russian.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient Unsupervised Discovery of Word Categories Using Symmetric Patterns and High Frequency Words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>297--304</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="7396" citStr="Davidov and Rappoport (2006)" startWordPosition="1174" endWordPosition="1177">ethod is capable of identifying polysemous English words. 2.2 Cross-Linguistic Study of WSD Levinson (1999) presents an approach to WSD that is evaluated on English and Hebrew. He finds 50 most similar words to the target and clusters them into groups, the number of groups being the number of senses. He reports comparable results for the two languages, but he uses both morphologically and lexically ambiguous words. Moreover, the evaluation methodology focuses on the success of disambiguation for an ambiguous word, and reports the number of ambiguous words that were disambiguated successfully. Davidov and Rappoport (2006) describe an algorithm for unsupervised discovery of word categories and evaluate it on Russian and English corpora. However, the focus of their work is on the discovery of semantic categories and from the results they report for the two languages it is difficult to infer how the languages compare against each other. We conduct a more thorough evaluation. We also 83 control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disa</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov and Ari Rappoport 2006. Efficient Unsupervised Discovery of Word Categories Using Symmetric Patterns and High Frequency Words. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 297–304. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanja Gaustad</author>
</authors>
<title>A Lemma-Based Approach to a Maximum Entropy Word Sense Disambiguation System for Dutch.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (Coling</booktitle>
<pages>778--784</pages>
<location>Geneva.</location>
<contexts>
<context position="8312" citStr="Gaustad (2004)" startWordPosition="1324" endWordPosition="1325">inst each other. We conduct a more thorough evaluation. We also 83 control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. She reports that morphology is useful, but the focus is on derivational morphology of the English language. In the present context, we are interested in the effect of inflectional morphology on WSD, especially for languages, such as Russian and Hebrew. Gaustad (2004) proposes a lemma-based approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. She shows that collapsing wordforms of an ambiguous word yields a more robust classifier due to the availability of more training data. The results indicate an improvement of this approach over classification based on wordforms. 3 Approach Our algorithm relies on the method for selection of relevant contextual terms and on distance measure between them introduced in (Sproat and van Santen, 1998) and on the approach described in (Sch¨utze, 1998), though the details of clustering differ slightly. Th</context>
</contexts>
<marker>Gaustad, 2004</marker>
<rawString>Tanja Gaustad. 2004. A Lemma-Based Approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. In Proceedings of the 20th International Conference on Computational Linguistics (Coling 2004), 778-784. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Levinson</author>
</authors>
<title>Corpus-Based Method for Unsupervised Word Sense Disambiguation.</title>
<date>1999</date>
<note>www.stanford.edu/ dmitryle/acai99w1.ps.</note>
<contexts>
<context position="6875" citStr="Levinson (1999)" startWordPosition="1092" endWordPosition="1093">mbiguous word. The algorithm is evaluated on natural and artificially-constructed ambiguous English words. Sproat and van Santen (1998) introduce a technique for automatic detection of ambiguous words in a corpus and measuring their degree of polysemy. This technique employs a similarity measure between the context terms similar in spirit to the one in (Sch¨utze, 1992) and singular value decomposition in order to detect context terms that are important for disambiguating the target. They show that the method is capable of identifying polysemous English words. 2.2 Cross-Linguistic Study of WSD Levinson (1999) presents an approach to WSD that is evaluated on English and Hebrew. He finds 50 most similar words to the target and clusters them into groups, the number of groups being the number of senses. He reports comparable results for the two languages, but he uses both morphologically and lexically ambiguous words. Moreover, the evaluation methodology focuses on the success of disambiguation for an ambiguous word, and reports the number of ambiguous words that were disambiguated successfully. Davidov and Rappoport (2006) describe an algorithm for unsupervised discovery of word categories and evalua</context>
</contexts>
<marker>Levinson, 1999</marker>
<rawString>Dmitry Levinson. 1999. Corpus-Based Method for Unsupervised Word Sense Disambiguation. www.stanford.edu/ dmitryle/acai99w1.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Weber McRoy</author>
</authors>
<title>Using Multiple Knowledge Sources for Word Sense Discrimination.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>1--30</pages>
<contexts>
<context position="7930" citStr="McRoy (1992)" startWordPosition="1267" endWordPosition="1268">guous words that were disambiguated successfully. Davidov and Rappoport (2006) describe an algorithm for unsupervised discovery of word categories and evaluate it on Russian and English corpora. However, the focus of their work is on the discovery of semantic categories and from the results they report for the two languages it is difficult to infer how the languages compare against each other. We conduct a more thorough evaluation. We also 83 control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. She reports that morphology is useful, but the focus is on derivational morphology of the English language. In the present context, we are interested in the effect of inflectional morphology on WSD, especially for languages, such as Russian and Hebrew. Gaustad (2004) proposes a lemma-based approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. She shows that collapsing wordforms of an ambiguous word yields a more robust classifier due to the availability of more</context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>Susan Weber McRoy. 1992. Using Multiple Knowledge Sources for Word Sense Discrimination. Computational Linguistics, 18(1): 1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering Word Senses from Text In</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD,</booktitle>
<pages>613--619</pages>
<location>Edmonton.</location>
<contexts>
<context position="5282" citStr="Pantel and Lin (2002)" startWordPosition="824" endWordPosition="827">a discussion of the results and directions for future work. 2 Related Work First, we describe several approaches to WSD that are most relevant to the present project: Since we are dealing with languages that do not have many linguistic resources available, we chose a most unsupervised, knowledge-poor approach to the task that relies on words occurring in the context of an ambiguous word. Next, we consider two papers on WSD that provide evaluation for two languages. Finally, we describe work that is concerned with the role of morphology for the task. 2.1 Approaches to Word Sense Discrimination Pantel and Lin (2002) learn word sense induction from an untagged corpus by finding the set of the most similar words to the target and by clustering the words. Each word cluster corresponds to a sense. Thus, senses are viewed as clusters of words. Another approach is based on clustering the occurrences of an ambiguous word in a corpus into clusters that correspond to distinct senses of the word. Based on this approach, a sense is defined as a cluster of contexts of an ambiguous word. Each occurrence of an ambiguous word is represented as a vector of features, where features are based on terms occurring in the con</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering Word Senses from Text In In Proceedings of ACM SIGKDD, pages 613-619. Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Rebecca Bruce</author>
</authors>
<title>Distinguishing word senses in untagged text.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>197--207</pages>
<location>Providence, RI,</location>
<contexts>
<context position="5945" citStr="Pedersen and Bruce (1997)" startWordPosition="941" endWordPosition="944">tagged corpus by finding the set of the most similar words to the target and by clustering the words. Each word cluster corresponds to a sense. Thus, senses are viewed as clusters of words. Another approach is based on clustering the occurrences of an ambiguous word in a corpus into clusters that correspond to distinct senses of the word. Based on this approach, a sense is defined as a cluster of contexts of an ambiguous word. Each occurrence of an ambiguous word is represented as a vector of features, where features are based on terms occurring in the context of the target word. For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. Sch¨utze (1992) presents a method that explores the similarity between the context terms occurring around the target. This is accomplished by considering feature vectors of context terms of the ambiguous word. The algorithm is evaluated on natural and artificially-constructed ambiguous English words. Sproat and van Santen (1998) introduce a technique for automatic detection of ambiguous words in a corpus and measuring their degree of polysemy. This technique employs a similar</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>Ted Pedersen and Rebecca Bruce. 1997. Distinguishing word senses in untagged text. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 197-207. Providence, RI, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Jan van Santen</author>
</authors>
<title>Automatic ambiguity detection.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<location>Sydney, Australia,</location>
<marker>Sproat, van Santen, 1998</marker>
<rawString>Richard Sproat and Jan van Santen. 1998. Automatic ambiguity detection. In Proceedings of International Conference on Spoken Language Processing. Sydney, Australia, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Hierarchical Clustering Algorithms for Document Datasets Data Mining and Knowledge Discovery,</title>
<date>2005</date>
<contexts>
<context position="15025" citStr="Zhao and Karypis (2005)" startWordPosition="2485" endWordPosition="2488"> to clusters created manually. We use three evaluation measures : cluster purity, entropy, and FScore. 3 For a cluster Cr of size qr, where the size is the number of examples in that cluster, the dominating sense Si in that cluster is selected and cluster purity is computed as follows: � (3) where nir is the number of examples in cluster Cr with sense Si. For an ambiguous word w, cluster purity P(w) is the weighted average of purities of the clusters for that word. 4. Higher cluster purity score corresponds to a better clustering outcome. Entropy and FScore measures are described in detail in Zhao and Karypis (2005). Entropy indicates how distinct senses are distributed between the two clusters. The perfect distribution is the assignment of all examples with sense 1 to one cluster and all examples with sense 2 to the other cluster. In such case, the entropy is 0. In general, a lower value indicates a better cluster quality. Entropy is computed for each cluster. Entropy for word w is the weighted average of the entropies of the clusters for that word. Finally, FScore considers both the coverage of the algorithm and its ability to discriminate between the two senses. FScore is computed as the harmonic 3Exa</context>
</contexts>
<marker>Zhao, Karypis, 2005</marker>
<rawString>Ying Zhao and George Karypis. 2005. Hierarchical Clustering Algorithms for Document Datasets Data Mining and Knowledge Discovery, 10(2):141–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>