<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000098">
<title confidence="0.990471">
Character-Level Chinese Dependency Parsing
</title>
<author confidence="0.992087">
Meishan Zhang†, Yue Zhang$ , Wanxiang Che†, Ting Liu†�
</author>
<affiliation confidence="0.991175">
†Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.616738">
{mszhang, car, tliu}@ir.hit.edu.cn
</email>
<affiliation confidence="0.985405">
$Singapore University of Technology and Design
</affiliation>
<email confidence="0.97642">
yue zhang@sutd.edu.sg
</email>
<sectionHeader confidence="0.99684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998280722222222">
Recent work on Chinese analysis has led
to large-scale annotations of the internal
structures of words, enabling character-
level analysis of Chinese syntactic struc-
tures. In this paper, we investigate the
problem of character-level Chinese depen-
dency parsing, building dependency trees
over characters. Character-level infor-
mation can benefit downstream applica-
tions by offering flexible granularities for
word segmentation while improving word-
level dependency parsing accuracies. We
present novel adaptations of two ma-
jor shift-reduce dependency parsing algo-
rithms to character-level parsing. Exper-
imental results on the Chinese Treebank
demonstrate improved performances over
word-based parsing methods.
</bodyText>
<sectionHeader confidence="0.999526" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972736842106">
As a light-weight formalism offering syntactic
information to downstream applications such as
SMT, the dependency grammar has received in-
creasing interest in the syntax parsing commu-
nity (McDonald et al., 2005; Nivre and Nilsson,
2005; Carreras et al., 2006; Duan et al., 2007; Koo
and Collins, 2010; Zhang and Clark, 2008; Nivre,
2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi
and McCallum, 2013). Chinese dependency trees
were conventionally defined over words (Chang et
al., 2009; Li et al., 2012), requiring word segmen-
tation and POS-tagging as pre-processing steps.
Recent work on Chinese analysis has embarked
on investigating the syntactic roles of characters,
leading to large-scale annotations of word internal
structures (Li, 2011; Zhang et al., 2013). Such an-
notations enable dependency parsing on the char-
acter level, building dependency trees over Chi-
nese characters. Figure 1(c) shows an example of
</bodyText>
<note confidence="0.583176">
∗Corresponding author.
</note>
<figure confidence="0.647938">
林业局 Di局K 会 上 发-
forestry administration deputy director meeting in make a speech
(a) a word-based dependency tree
(b) a character-level dependency tree by Zhao (2009) with
real intra-word and pseudo inter-word dependencies
(c) a character-level dependency tree investigated in this pa-
per with both real intra- and inter-word dependencies
</figure>
<figureCaption confidence="0.9875385">
Figure 1: An example character-level dependency
tree. “林业局Di局K在大会上发- (The deputy
</figureCaption>
<bodyText confidence="0.935226368421053">
director of forestry administration make a speech
in the meeting)”.
a character-level dependency tree, where the leaf
nodes are Chinese characters.
Character-level dependency parsing is interest-
ing in at least two aspects. First, character-level
trees circumvent the issue that no universal stan-
dard exists for Chinese word segmentation. In the
well-known Chinese word segmentation bakeoff
tasks, for example, different segmentation stan-
dards have been used by different data sets (Emer-
son, 2005). On the other hand, most disagreement
on segmentation standards boils down to disagree-
ment on segmentation granularity. As demon-
strated by Zhao (2009), one can extract both fine-
grained and coarse-grained words from character-
level dependency trees, and hence can adapt to
flexible segmentation standards using this formal-
ism. In Figure 1(c), for example, “Di局K (deputy
</bodyText>
<figure confidence="0.98469525">
woods industry office deputy office manager
林 业 局
Di 局 K
meeting in
会
上
发 -
make speech
woods industry office deputy office manager
林 业 局
Di 局 K
meeting in
会
上
发 -
make speech
</figure>
<page confidence="0.94066">
1326
</page>
<note confidence="0.8532265">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.992563044444444">
director)” can be segmented as both “ACJ (deputy)
 |局长 (director)” and “ACJ局 长 (deputy direc-
tor)”, but not “ACJ (deputy) 局 (office) I 长 (man-
ager)”, by dependency coherence. Chinese lan-
guage processing tasks, such as machine transla-
tion, can benefit from flexible segmentation stan-
dards (Zhang et al., 2008; Chang et al., 2008).
Second, word internal structures can also be
useful for syntactic parsing. Zhang et al. (2013)
have shown the usefulness of word structures in
Chinese constituent parsing. Their results on the
Chinese Treebank (CTB) showed that character-
level constituent parsing can bring increased per-
formances even with the pseudo word structures.
They further showed that better performances can
be achieved when manually annotated word struc-
tures are used instead of pseudo structures.
In this paper, we make an investigation of
character-level Chinese dependency parsing using
Zhang et al. (2013)’s annotations and based on
a transition-based parsing framework (Zhang and
Clark, 2011). There are two dominant transition-
based dependency parsing systems, namely the
arc-standard and the arc-eager parsers (Nivre,
2008). We study both algorithms for character-
level dependency parsing in order to make a com-
prehensive investigation. For direct comparison
with word-based parsers, we incorporate the tra-
ditional word segmentation, POS-tagging and de-
pendency parsing stages in our joint parsing mod-
els. We make changes to the original transition
systems, and arrive at two novel transition-based
character-level parsers.
We conduct experiments on three data sets, in-
cluding CTB 5.0, CTB 6.0 and CTB 7.0. Exper-
imental results show that the character-level de-
pendency parsing models outperform the word-
based methods on all the data sets. Moreover,
manually annotated intra-word dependencies can
give improved word-level dependency accuracies
than pseudo intra-word dependencies. These re-
sults confirm the usefulness of character-level
syntax for Chinese analysis. The source codes
are freely available at http://sourceforge.
net/projects/zpar/, version 0.7.
</bodyText>
<sectionHeader confidence="0.923193" genericHeader="method">
2 Character-Level Dependency Tree
</sectionHeader>
<bodyText confidence="0.999968072727273">
Character-level dependencies were first proposed
by Zhao (2009). They show that by annotat-
ing character dependencies within words, one can
adapt to different segmentation standards. The
dependencies they study are restricted to intra-
word characters, as illustrated in Figure 1(b). For
inter-word dependencies, they use a pseudo right-
headed representation.
In this study, we integrate inter-word syntactic
dependencies and intra-word dependencies using
large-scale annotations of word internal structures
by Zhang et al. (2013), and study their interac-
tions. We extract unlabeled dependencies from
bracketed word structures according to Zhang et
al.’s head annotations. In Figure 1(c), the depen-
dencies shown by dashed arcs are intra-word de-
pendencies, which reflect the internal word struc-
tures, while the dependencies with solid arcs are
inter-word dependencies, which reflect the syntac-
tic structures between words.
In this formulation, a character-level depen-
dency tree satisfies the same constraints as the
traditional word-based dependency tree for Chi-
nese, including projectivity. We differentiate intra-
word dependencies and inter-word dependencies
by the arc type, so that our work can be com-
pared with conventional word segmentation, POS-
tagging and dependency parsing pipelines under a
canonical segmentation standard.
The character-level dependency trees hold to a
specific word segmentation standard, but are not
limited to it. We can extract finer-grained words
of different granulities from a coarse-grained word
by taking projective subtrees of different sizes. For
example, taking all the intra-word modifier nodes
of “长 (manager)” in Figure 1(c) results in the
word “ACJ局长 (deputy director)”, while taking the
first modifier node of “长 (manager)” results in the
word “局长 (director)”. Note that “ACJ局 (deputy
office)” cannot be a word because it does not form
a projective span without “长 (manager)”.
Inner-word dependencies can also bring bene-
fits to parsing word-level dependencies. The head
character can be a less sparse feature compared
to a word. As intra-word dependencies lead to
fine-grained subwords, we can also use these sub-
words for better parsing. In this work, we use
the innermost left/right subwords as atomic fea-
tures. To extract the subwords, we find the inner-
most left/right modifiers of the head character, re-
spectively, and then conjoin them with all their de-
scendant characters to form the smallest left/right
subwords. Figure 2 shows an example, where the
smallest left subword of “大法官 (chief lawyer)”
is “法官 (lawyer)”, and the smallest right subword
</bodyText>
<page confidence="0.996924">
1327
</page>
<figureCaption confidence="0.993387">
Figure 2: An example to illustrate the innermost
left/right subwords.
</figureCaption>
<bodyText confidence="0.949166">
of “合M化 (legalize)” is “合M (legal)”.
</bodyText>
<sectionHeader confidence="0.931054" genericHeader="method">
3 Character-Level Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999990735294118">
A transition-based framework with global learn-
ing and beam search decoding (Zhang and Clark,
2011) has been applied to a number of natural lan-
guage processing tasks, including word segmen-
tation, POS-tagging and syntactic parsing (Zhang
and Clark, 2010; Huang and Sagae, 2010; Bohnet
and Nivre, 2012; Zhang et al., 2013). It models
a task incrementally from a start state to an end
state, where each intermediate state during decod-
ing can be regarded as a partial output. A num-
ber of actions are defined so that the state ad-
vances step by step. To learn the model param-
eters, it usually uses the online perceptron algo-
rithm with early-update under the inexact decod-
ing condition (Collins, 2002; Collins and Roark,
2004). Transition-based dependency parsing can
be modeled under this framework, where the state
consists of a stack and a queue, and the set of ac-
tions can be either the arc-eager (Zhang and Clark,
2008) or the arc-standard (Huang et al., 2009)
transition systems.
When the internal structures of words are an-
notated, character-level dependency parsing can
be treated as a special case of word-level depen-
dency parsing, with “words” being “characters”.
A big weakness of this approach is that full words
and POS-tags cannot be used for feature engineer-
ing. Both are crucial to well-established features
for word segmentation, POS-tagging and syntactic
parsing. In this section, we introduce novel exten-
sions to the arc-standard and the arc-eager tran-
sition systems, so that word-based and character-
based features can be used simultaneously for
character-level dependency parsing.
</bodyText>
<subsectionHeader confidence="0.99841">
3.1 The Arc-Standard Model
</subsectionHeader>
<bodyText confidence="0.999805388888889">
The arc-standard model has been applied to joint
segmentation, POS-tagging and dependency pars-
ing (Hatori et al., 2012), but with pseudo word
structures. For unified processing of annotated
word structures and fair comparison between
character-level arc-eager and arc-standard sys-
tems, we define a different arc-standard transition
system, consistent with our character-level arc-
eager system.
In the word-based arc-standard model, the tran-
sition state includes a stack and a queue, where
the stack contains a sequence of partially-parsed
dependency trees, and the queue consists of un-
processed input words. Four actions are defined
for state transition, including arc-left (AL, which
creates a left arc between the top element so and
the second top element sl on the stack), arc-right
(AR, which creates a right arc between so and sl),
pop-root (PR, which defines the root node of a de-
pendency tree when there is only one element on
the stack and no element in the queue), and the last
shift (SH, which shifts the first element qo of the
queue onto the stack).
For character-level dependency parsing, there
are two types of dependencies: inter-word depen-
dencies and intra-word dependencies. To parse
them with both character and word features, we
extend the original transition actions into two cat-
egories, for inter-word dependencies and intra-
word dependencies, respectively. The actions for
inter-word dependencies include inter-word arc-
left (ALw), inter-word arc-right (ARw), pop-root
(PR) and inter-word shift (SHw). Their definitions
are the same as the word-based model, with one
exception that the inter-word shift operation has
a parameter denoting the POS-tag of the incoming
word, so that POS disambiguation is performed by
the SHw action.
The actions for intra-word dependencies in-
clude intra-word arc-left (ALc), intra-word arc-
right (ARc), pop-word (PW) and inter-word shift
(SHc). The definitions of ALc, ARc and SHc are
the same as the word-based arc-standard model,
while PW changes the top element on the stack
into a full-word node, which can only take inter-
word dependencies. One thing to note is that, due
to variable word sizes in character-level parsing,
the number of actions can vary between differ-
ent sequences of actions corresponding to differ-
ent analyses. We use the padding method (Zhu
et al., 2013), adding an IDLE action to finished
transition action sequences, for better alignments
between states in the beam.
In the character-level arc-standard transition
</bodyText>
<figure confidence="0.994592375">
大 M 官 合 M 化
big law officer agree with law ize
(a) smallest left subword (b) smallest right subword
1328
step action stack queue dependencies
0 - 0 林 业 ··· 0
1 SHw(NR) 林/NR 业 局 · · · 0
2 SHc 林/NR 业/NR 局 副 ··· 0
3 ALc 业/NR 局 副··· A1 = {林x业}
4 SHc 业/NR 局/NR 副 局 · · · A1
5 ALc 局/NR 副 局 · · · A2 = A1 U{业x局}
6 PW 林业局/NR 副 局 · · · A2
7 SHw(NN) 林业局/NR 副/NN 局 长 ··· A2
··· ··· ··· ··· ···
12 PW 林业局/NR 副局长/NN 会 上 · · · Ai
13 ALw 副局长/NN 会 上 · · · Ai+1 = Ai U{林业局/NRx副局长/NN}
··· ··· ··· ··· ···
(a) character-level dependency parsing using the arc-standard algorithm
step action stack deque queue dependencies
0 - 0 林 业 ···
1 SHc(NR) 0 林/NR 业 局 · · · 0
2 ALc 0 0 业/NR 局 ··· A1 = {林x业}
3 SHc 0 业/NR 局 副 · · · A1
4 ALc 0 0 局/NR 副 · · · A2 = A1 U{业x局}
5 SHc 0 局/NR 副 局 · · · A2
6 PW 0 林业局/NR 副 局 · · · A2
7 SHw 林业局/NR 0 副 局 · · · A2
··· ··· ··· ··· ··· ···
13 PW 林业局/NR 副局长/NN 会 上 · · · Ai
14 ALw 0 副局长/NN 会 上 · · · Ai+1 = Ai U{林业局/NRx副局长/NN}
··· ··· ··· ··· ··· ···
(b) character-level dependency parsing using the arc-eager algorithm, t = 1
</figure>
<figureCaption confidence="0.99997">
Figure 3: Character-level dependency parsing of the sentence in Figure 1(c).
</figureCaption>
<bodyText confidence="0.999973294117647">
system, each word is initialized by the action SHw
with a POS tag, before being incrementally mod-
ified by a sequence of intra-word actions, and fi-
nally being completed by the action PW. The inter-
word actions can be applied when all the elements
on the stack are full-word nodes, while the intra-
word actions can be applied when at least the top
element on the stack is a partial-word node. For
the actions ALc and ARc to be valid, the top two
elements on the stack are both partial-word nodes.
For the action PW to be valid, only the top ele-
ment on the stack is a partial-word node. Figure
3(a) gives an example action sequence.
There are three types of features. The first two
types are traditionally established features for the
dependency parsing and joint word segmentation
and POS-tagging tasks. We use the features pro-
posed by Hatori et al. (2012). The word-level
dependency parsing features are added when the
inter-word actions are applied, and the features
for joint word segmentation and POS-tagging are
added when the actions PW, SHw and SHc are ap-
plied. Following the work of Hatori et al. (2012),
we have a parameter α to adjust the weights for
joint word segmentation and POS-tagging fea-
tures. We apply word-based dependency pars-
ing features to intra-word dependency parsing as
well, by using subwords (the conjunction of char-
acters spanning the head node) to replace words in
word features. The third type of features is word-
structure features. We extract the head charac-
ter and the smallest subwords containing the head
character from the intra-word dependencies (Sec-
tion 2). Table 1 summarizes the features.
</bodyText>
<subsectionHeader confidence="0.999887">
3.2 The Arc-Eager Model
</subsectionHeader>
<bodyText confidence="0.999940357142857">
Similar to the arc-standard case, the state of a
word-based arc-eager model consists of a stack
and a queue, where the stack contains a sequence
of partial dependency trees, and the queue con-
sists of unprocessed input words. Unlike the arc-
standard model, which builds dependencies on the
top two elements on the stack, the arc-eager model
builds dependencies between the top element of
the stack and the first element of the queue. Five
actions are defined for state transformation: arc-
left (AL, which creates a left arc between the top
element of the stack so and the first element in
the queue qo, while popping so off the stack),
arc-right (AR, which creates a right arc between
</bodyText>
<page confidence="0.987283">
1329
</page>
<tableCaption confidence="0.6482635">
Feature templates
Table 1: Feature templates encoding intra-word
</tableCaption>
<bodyText confidence="0.999151897727273">
dependencies. L and R denote the two elements
over which the dependencies are built; the sub-
scripts lc1 and rc1 denote the left-most and right-
most children, respectively; the subscripts lc2 and
rc2 denote the second left-most and second right-
most children, respectively; w denotes the word;
t denotes the POS tag; c denotes the head charac-
ter; lsw and rsw denote the smallest left and right
subwords respectively, as shown in Figure 2.
so and qo, while shifting qo from the queue onto
the stack), pop-root (PR, which defines the ROOT
node of the dependency tree when there is only
one element on the stack and no element in the
queue), reduce (RD, which pops so off the stack),
and shift (SH, which shifts qo onto the stack).
There is no previous work that exploits the
arc-eager algorithm for jointly performing POS-
tagging and dependency parsing. Since the first
element of the queue can be shifted onto the stack
by either SH or AR, it is more difficult to assign
a POS tag to each word by using a single action.
In this work, we make a change to the configu-
ration state, adding a deque between the stack and
the queue to save partial words with intra-word de-
pendencies. We divide the transition actions into
two categories, one for inter-word dependencies
(ARw, ALw, SHw, RDw and PR) and the other
for intra-word dependencies (ARc, ALc, SHc, RDc
and PW), requiring that the intra-word actions be
operated between the deque and the queue, while
the inter-word actions be operated between the
stack and the deque.
For character-level arc-eager dependency pars-
ing, the inter-word actions are the same as the
word-based methods. The actions ALc and ARc
are the same as ALw and ARw, except that they
operate on characters, but the SHc operation has a
parameter to denote the POS tag of a word. The
PW action recognizes a full-word. We also have
an IDLE action, for the same reason as the arc-
standard model.
In the character-level arc-eager transition sys-
tem, a word is formed in a similar way with that
of character-level arc-standard algorithm. Each
word is initialized by the action SHc with a POS
tag, and then incrementally changed a sequence of
intra-word actions, before being finalized by the
action PW. All these actions operate between the
queue and deque. For the action PW, only the
first element in the deque (close to the queue) is
a partial-word node. For the actions ARc and ALc
to be valid, the first element in the deque must be
a partial-word node. The action SHc have a POS
tag when shifting the first character of a word,but
does not have such a parameter when shifting the
next characters of a word. For the action SHc with
a POS tag to be valid, the first element in the deque
must be a full-word node. Different from the arc-
standard model, at any stage we can choose either
the action SHc with a POS tag to initialize a new
word on the deque, or the inter-word actions on
the stack. In order to eliminate the ambiguity, we
define a new parameter t to limit the max size of
the deque. If the deque is full with t words, inter-
word actions are performed; otherwise intra-word
actions are performed. All the inter-word actions
must be applied on full-word nodes between the
stack an the deque. Figure 3(b) gives an example
action sequence.
Similar to the arc-standard case, there are three
types of features, with the first two types being
traditionally established features for dependency
parsing and joint word segmentation and POS-
tagging. The dependency parsing features are
taken from the work of Zhang and Nivre (2011),
and the features for joint word segmentation and
POS-tagging are taken from Zhang and Clark
(2010)1. The word-level dependency parsing fea-
tures are triggered when the inter-word actions are
applied, while the features of joint word segmenta-
tion and POS-tagging are added when the actions
SHc, ARc and PW are applied. Again we use a pa-
rameter α to adjust the weights for joint word seg-
mentation and POS-tagging features. The word-
level features for dependency parsing are applied
to intra-word dependency parsing as well, by us-
ing subwords to replace words. The third type of
features is word-structure features, which are the
</bodyText>
<footnote confidence="0.96977475">
1Since Hatori et al. (2012) also use Zhang and Clark
(2010)’s features, the arc-standard and arc-eager character-
level dependency parsing models have the same features for
joint word segmentation and POS-tagging.
</footnote>
<table confidence="0.99755635">
Lc, Lct, Rc, Rct, Llc1c, Lrc1c, Rlc1c,
Lc · Rc, Llc1ct, Lrc1ct, Rlc1ct,
Lc · Rw, Lw · Rc, Lct · Rw,
Lwt · Rc, Lw · Rct, Lc · Rwt,
Lc · Rc · Llc1c, Lc · Rc · Lrc1c,
Lc · Rc · Llc2c, Lc · Rc · Lrc2c,
Lc · Rc · Rlc1c, Lc · Rc · Rlc2c,
Llsw, Lrsw, Rlsw, Rrsw, Llswt,
Lrswt, Rlswt, Rrswt, Llsw · Rw,
Lrsw · Rw, Lw · Rlsw, Lw · Rrsw
1330
CTB50 CTB60 CTB70
Training #sent 18k 23k 31k
#word 494k 641k 718k
#sent 350 2.1k 10k
Development #word 6.8k 60k 237k
#oov 553 3.3k 13k
#sent 348 2.8k 10k
Test #word 8.0k 82k 245k
#oov 278 4.6k 13k
</table>
<tableCaption confidence="0.997763">
Table 2: Statistics of datasets.
</tableCaption>
<bodyText confidence="0.9837315">
same as those of the character-level arc-standard
model, shown in Table 1.
</bodyText>
<sectionHeader confidence="0.999938" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993495">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999983904761905">
We use the Chinese Penn Treebank 5.0, 6.0 and 7.0
to conduct the experiments, splitting the corpora
into training, development and test sets according
to previous work. Three different splitting meth-
ods are used, namely CTB50 by Zhang and Clark
(2010), CTB60 by the official documentation of
CTB 6.0, and CTB70 by Wang et al. (2011). The
dataset statistics are shown in Table 2. We use
the head rules of Zhang and Clark (2008) to con-
vert phrase structures into dependency structures.
The intra-word dependencies are extracted from
the annotations of Zhang et al. (2013)2.
The standard measures of word-level precision,
recall and F1 score are used to evaluate word seg-
mentation, POS-tagging and dependency parsing,
following Hatori et al. (2012). In addition, we use
the same measures to evaluate intra-word depen-
dencies, which indicate the performance of pre-
dicting word structures. A word’s structure is cor-
rect only if all the intra-word dependencies are all
correctly recognized.
</bodyText>
<subsectionHeader confidence="0.987926">
4.2 Baseline and Proposed Models
</subsectionHeader>
<bodyText confidence="0.999960888888889">
For the baseline, we have two different pipeline
models. The first consists of a joint segmentation
and POS-tagging model (Zhang and Clark, 2010)
and a word-based dependency parsing model us-
ing the arc-standard algorithm (Huang et al.,
2009). We name this model STD (pipe). The
second consists of the same joint segmentation
and POS-tagging model and a word-based depen-
dency parsing model using the arc-eager algorithm
</bodyText>
<footnote confidence="0.7602336">
2https://github.com/zhangmeishan/
wordstructures; their annotation was conducted
on CTB 5.0, while we made annotations of the remainder of
the CTB 7.0 words. We also make the annotations publicly
available at the same site.
</footnote>
<bodyText confidence="0.998946714285714">
(Zhang and Nivre, 2011). We name this model
EAG (pipe). For the pipeline models, we use a
beam of size 16 for joint segmentation and POS-
tagging, and a beam of size 64 for dependency
parsing, according to previous work.
We study the following character-level depen-
dency parsing models:
</bodyText>
<listItem confidence="0.985639055555555">
• STD (real, pseudo): the arc-standard model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
• STD (pseudo, real): the arc-standard model
with pseudo intra-word dependencies and
real inter-word dependencies;
• STD (real, real): the arc-standard model with
annotated intra-word dependencies and real
inter-word dependencies;
• EAG (real, pseudo): the arc-eager model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
• EAG (pseudo, real): the arc-eager model
with pseudo intra-word dependencies and
real inter-word dependencies;
• EAG (real, real): the arc-eager model with
annotated intra-word dependencies and real
inter-word dependencies.
</listItem>
<bodyText confidence="0.9998463">
The annotated intra-word dependencies refer to
the dependencies extracted from annotated word
structures, while the pseudo intra-word depen-
dencies used in the above models are similar
to those of Hatori et al. (2012). For a given
word w = c1c2 · · · cm, the intra-word depen-
dency structure is cx1 cx2 · · ·x cm3. The real inter-
word dependencies refer to the syntactic word-
level dependencies by head-finding rules from
CTB, while the pseudo inter-word dependencies
refer to the word-level dependencies used by Zhao
(2009) (wx1 wx2 · · ·x wn). The character-level
models with annotated intra-word dependencies
and pseudo inter-word dependencies are compared
with the pipelines on word segmentation and POS-
tagging accuracies, and are compared with the
character-level models with annotated intra-word
dependencies and real inter-word dependencies
on word segmentation, POS-tagging and word-
structure predicating accuracies. All the proposed
</bodyText>
<footnote confidence="0.9921345">
3We also tried similar structures with right arcs, which
gave lower accuracies.
</footnote>
<page confidence="0.950074">
1331
</page>
<table confidence="0.9920975">
STD (real, real) SEG POS DEP WS
α = 1 95.85 91.60 76.96 95.14
α = 2 96.09 91.89 77.28 95.29
α = 3 96.02 91.84 77.22 95.23
α = 4 96.10 91.96 77.49 95.29
α = 5 96.07 91.90 77.31 95.21
</table>
<tableCaption confidence="0.9448845">
Table 3: Development test results of the character-
level arc-standard model on CTB60.
</tableCaption>
<table confidence="0.991893363636364">
EAG (real, real) SEG POS DEP WS
1 96.00 91.66 74.63 95.49
2 95.93 91.75 76.60 95.37
3 95.93 91.74 76.94 95.36
4 95.91 91.71 76.82 95.33
5 95.95 91.73 76.84 95.40
1 95.93 91.74 76.94 95.36
2 96.11 91.99 77.17 95.56
3 96.16 92.01 77.48 95.62
4 96.11 91.93 77.40 95.53
5 96.00 91.84 77.10 95.43
</table>
<tableCaption confidence="0.997683">
Table 4: Development test results of the character-
</tableCaption>
<bodyText confidence="0.981047333333333">
level arc-eager model on CTB60.
models use a beam of size 64 after considering
both speeds and accuracies.
</bodyText>
<subsectionHeader confidence="0.999643">
4.3 Development Results
</subsectionHeader>
<bodyText confidence="0.999937428571429">
Our development tests are designed for two pur-
poses: adjusting the parameters for the two pro-
posed character-level models and testing the effec-
tiveness of the novel word-structure features. Tun-
ing is conducted by maximizing word-level depen-
dency accuracies. All the tests are conducted on
the CTB60 data set.
</bodyText>
<subsubsectionHeader confidence="0.678273">
4.3.1 Parameter Tuning
</subsubsectionHeader>
<bodyText confidence="0.9998471875">
For the arc-standard model, there is only one pa-
rameter α that needs tuning. It adjusts the weights
of segmentation and POS-tagging features, be-
cause the number of feature templates is much less
for the two tasks than for parsing. We set the value
of α to 1 · · · 5, respectively. Table 3 shows the
accuracies on the CTB60 development set. Ac-
cording to the results, we use α = 4 for our final
character-level arc-standard model.
For the arc-eager model, there are two parame-
ters t and α. t denotes the deque size of the arc-
eager model, while α shares the same meaning as
the arc-standard model. We take two steps for pa-
rameter tuning, first adjusting the more crucial pa-
rameter t and then adjusting α on the best t. Both
parameters are assigned the values of 1 to 5. Ta-
</bodyText>
<table confidence="0.999014428571429">
SEG POS DEP WS
STD (real, real) 96.10 91.96 77.49 95.29
STD (real, real)/wo 95.99 91.79 77.19 95.35
A -0.11 -0.17 -0.30 +0.06
EAG (real, real) 96.16 92.01 77.48 95.62
EAG (real, real)/wo 96.09 91.82 77.12 95.56
A -0.07 -0.19 -0.36 -0.06
</table>
<tableCaption confidence="0.986071">
Table 5: Feature ablation tests for the novel word-
</tableCaption>
<bodyText confidence="0.977023166666667">
structure features, where “/wo” denotes the corre-
sponding models without the novel intra-word de-
pendency features.
ble 4 shows the results. According to results, we
set t = 3 and α = 3 for the final character-level
arc-eager model, respectively.
</bodyText>
<subsectionHeader confidence="0.9972915">
4.3.2 Effectiveness of Word-Structure
Features
</subsectionHeader>
<bodyText confidence="0.999971666666667">
To test the effectiveness of our novel word-
structure features, we conduct feature ablation ex-
periments on the CTB60 development data set for
the proposed arc-standard and arc-eager models,
respectively. Table 5 shows the results. We can
see that both the two models achieve better accu-
racies on word-level dependencies with the novel
word-structure features, while the features do not
affect word-structure predication significantly.
</bodyText>
<subsectionHeader confidence="0.999374">
4.4 Final Results
</subsectionHeader>
<bodyText confidence="0.9999478">
Table 6 shows the final results on the CTB50,
CTB60 and CTB70 data sets, respectively. The
results demonstrate that the character-level depen-
dency parsing models are significantly better than
the corresponding word-based pipeline models,
for both the arc-standard and arc-eager systems.
Similar to the findings of Zhang et al. (2013), we
find that the annotated word structures can give
better accuracies than pseudo word structures. An-
other interesting finding is that, although the arc-
eager algorithm achieves lower accuracies in the
word-based pipeline models, it obtains compara-
tive accuracies in the character-level models.
We also compare our results to those of Hatori
et al. (2012), which is comparable to STD (pseudo,
real) since similar arc-standard algorithms and
features are used. The major difference is the
set of transition actions. We rerun their system
on the three datasets4. As shown in Table 6, our
arc-standard system with pseudo word structures
</bodyText>
<footnote confidence="0.844162333333333">
4http://triplet.cc/. We use a different
constituent-to-dependency conversion scheme in com-
parison with Hatori et al. (2012)’s work.
</footnote>
<equation confidence="0.9978629">
α = 1 t =
t = 3 t =
t =
t =
t =
α =
α =
α =
α =
α =
</equation>
<page confidence="0.978666">
1332
</page>
<table confidence="0.998044769230769">
Model CTB50 CTB60 CTB70
SEG POS DEP WS SEG POS DEP WS SEG POS DEP WS
The arc-standard models
STD (pipe) 97.53 93.28 79.72 – 95.32 90.65 75.35 – 95.23 89.92 73.93 –
STD (real, pseudo) 97.78 93.74 – 97.40 95.77$ 91.24$ – 95.08 95.59$ 90.49$ – 94.97
STD (pseudo, real) 97.67 94.28$ 81.63$ – 95.63$ 91.40$ 76.75$ – 95.53$ 90.75$ 75.63$ –
STD (real, real) 97.84 94.62$ 82.14$ 97.30 95.56$ 91.39$ 77.09$ 94.80 95.51$ 90.76$ 75.70$ 94.78
Hatori+ ’12 97.75 94.33 81.56 – 95.26 91.06 75.93 – 95.27 90.53 74.73 –
The arc-eager models
EAG (pipe) 97.53 93.28 79.59 – 95.32 90.65 74.98 – 95.23 89.92 73.46 –
EAG (real, pseudo) 97.75 93.88 – 97.45 95.63$ 91.07$ – 95.06 95.50$ 90.36$ – 95.00
EAG (pseudo, real) 97.76 94.36$ 81.70$ – 95.63$ 91.34$ 76.87$ – 95.39$ 90.56$ 75.56$ –
EAG (real, real) 97.84 94.36$ 82.07$ 97.49 95.71$ 91.51$ 76.99$ 95.16 95.47$ 90.72$ 75.76$ 94.94
</table>
<tableCaption confidence="0.9602985">
Table 6: Main results, where the results marked with ‡ denote that the p-value is less than 0.001 compared
with the pipeline word-based models using pairwise t-test.
</tableCaption>
<bodyText confidence="0.999210076923077">
brings consistent better accuracies than their work
on all the three data sets.
Both the pipelines and character-level mod-
els with pseudo inter-word dependencies perform
word segmentation and POS-tagging jointly, with-
out using real word-level syntactic information. A
comparison between them (STD/EAG (pipe) vs.
STD/EAG (real, pseudo)) reflects the effectiveness
of annotated intra-word dependencies on segmen-
tation and POS-tagging. We can see that both the
arc-standard and arc-eager models with annotated
intra-word dependencies can improve the segmen-
tation accuracies by 0.3% and the POS-tagging ac-
curacies by 0.5% on average on the three datasets.
Similarly, a comparison between the character-
level models with pseudo inter-word dependen-
cies and the character-level models with real inter-
word dependencies (STD/EAG (real, pseudo) vs.
STD/EAG (real, real)) can reflect the effectiveness
of annotated inter-word structures on morphology
analysis. We can see that improved POS-tagging
accuracies are achieved using the real inter-word
dependencies when jointly performing inner- and
inter-word dependencies. However, we find that
the inter-word dependencies do not help the word-
structure accuracies.
</bodyText>
<subsectionHeader confidence="0.998271">
4.5 Analysis
</subsectionHeader>
<bodyText confidence="0.9999413">
To better understand the character-level parsing
models, we conduct error analysis in this section.
All the experiments are conducted on the CTB60
test data sets. The new advantage of the character-
level models is that one can parse the internal
word structures of intra-word dependencies. Thus
we are interested in their capabilities of predict-
ing word structures. We study the word-structure
accuracies in two aspects, including OOV, word
length, POS tags and the parsing model.
</bodyText>
<subsectionHeader confidence="0.538896">
4.5.1 OOV
</subsectionHeader>
<bodyText confidence="0.999972214285714">
The word-structure accuracy of OOV words re-
flects a model’s ability of handling unknown
words. The overall recalls of OOV word structures
are 67.98% by STD (real, real) and 69.01% by
EAG (real, real), respectively. We find that most
errors are caused by failures of word segmenta-
tion. We further investigate the accuracies when
words are correctly segmented, where the accura-
cies of OOV word structures are 87.64% by STD
(real, real) and 89.07% by EAG (real, real). The
results demonstrate that the structures of Chinese
words are not difficult to predict, and confirm the
fact that Chinese word structures have some com-
mon syntactic patterns.
</bodyText>
<subsectionHeader confidence="0.544211">
4.5.2 Parsing Model
</subsectionHeader>
<bodyText confidence="0.9999785">
From the above analysis in terms of OOV, word
lengths and POS tags, we can see that the EAG
(real, real) model and the STD (real, real) mod-
els behave similarly on word-structure accuracies.
Here we study the two models more carefully,
comparing their word accuracies sentence by sen-
tence. Figure 4 shows the results, where each
point denotes a sentential comparison between
STD (real, real) and EAG (real, real), the x-axis
denotes the sentential word-structure accuracy of
STD (real, real), and the y-axis denotes that of
EAG (real, real). The points at the diagonal show
the same accuracies by the two models, while oth-
ers show that the two models perform differently
on the corresponding sentences. We can see that
most points are beyond the diagonal line, indicat-
</bodyText>
<page confidence="0.957794">
1333
</page>
<figure confidence="0.989152142857143">
1
0.9
0.8
0.7
0.6
0.6 0.7 0.8 0.9 1
STD (real, real)
</figure>
<figureCaption confidence="0.9691825">
Figure 4: Sentential word-structure accuracies of
STD (real, real) and EAG (real, real).
</figureCaption>
<bodyText confidence="0.986607">
ing that the two parsing models can be comple-
mentary in parsing intra-word dependencies.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999977035714285">
Zhao (2009) was the first to study character-level
dependencies; they argue that since no consistent
word boundaries exist over Chinese word segmen-
tation, dependency-based representations of word
structures serve as a good alternative for Chinese
word segmentation. Thus their main concern is
to parse intra-word dependencies. In this work,
we extend their formulation, making use of large-
scale annotations of Zhang et al. (2013), so that the
syntactic word-level dependencies can be parsed
together with intra-word dependencies.
Hatori et al. (2012) proposed a joint model
for Chinese word segmentation, POS-tagging and
dependency parsing, studying the influence of
joint model and character features for parsing,
Their model is extended from the arc-standard
transition-based model, and can be regarded as
an alternative to the arc-standard model of our
work when pseudo intra-word dependencies are
used. Similar work is done by Li and Zhou (2012).
Our proposed arc-standard model is more concise
while obtaining better performance than Hatori et
al. (2012)’s work. With respect to word structures,
real intra-word dependencies are often more com-
plicated, while pseudo word structures cannot be
used to correctly guide segmentation.
Zhao (2009), Hatori et al. (2012) and our
work all study character-level dependency pars-
ing. While Zhao (2009) focus on word internal
structures using pseudo inter-word dependencies,
Hatori et al. (2012) investigate a joint model using
pseudo intra-word dependencies. We use manual
dependencies for both inner- and inter-word struc-
tures, studying their influences on each other.
Zhang et al. (2013) was the first to perform Chi-
nese syntactic parsing over characters. They ex-
tended word-level constituent trees by annotated
word structures, and proposed a transition-based
approach to parse intra-word structures and word-
level constituent structures jointly. For Hebrew,
Tsarfaty and Goldberg (2008) investigated joint
segmentation and parsing over characters using a
graph-based method. Our work is similar in ex-
ploiting character-level syntax. We study the de-
pendency grammar, another popular syntactic rep-
resentation, and propose two novel transition sys-
tems for character-level dependency parsing.
Nivre (2008) gave a systematic description of
the arc-standard and arc-eager algorithms, cur-
rently two popular transition-based parsing meth-
ods for word-level dependency parsing. We extend
both algorithms to character-level joint word seg-
mentation, POS-tagging and dependency parsing.
To our knowledge, we are the first to apply the arc-
eager system to joint models and achieve compar-
ative performances to the arc-standard model.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999958466666667">
We studied the character-level Chinese depen-
dency parsing, by making novel extensions to
two commonly-used transition-based dependency
parsing algorithms for word-based dependency
parsing. With both pseudo and annotated word
structures, our character-level models obtained
better accuracies than previous work on seg-
mentation, POS-tagging and word-level depen-
dency parsing. We further analyzed some im-
portant factors for intra-word dependencies, and
found that two proposed character-level pars-
ing models are complementary in parsing intra-
word dependencies. We make the source code
publicly available at http://sourceforge.
net/projects/zpar/, version 0.7.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998481">
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61370164, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design.
</bodyText>
<note confidence="0.542697">
EAG (real, real)
</note>
<page confidence="0.980148">
1334
</page>
<sectionHeader confidence="0.995719" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996480407407408">
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the EMNLP-CONLL, pages 1455–1465,
Jeju Island, Korea, July.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd COLING, number August, pages
89–97.
Xavier Carreras, Mihai Surdeanu, and Llu´ıs M`arquez.
2006. Projective dependency parsing with per-
ceptron. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 181–185, New York City, June.
Pi-Chuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In ACL Workshop
on Statistical Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, , and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of ACL, pages
1052–1062, August.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the 7th EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, vol-
ume 4701 of Lecture Notes in Computer Science,
pages 559–566.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 123–133.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th ACL,
pages 1045–1053, Jeju Island, Korea, July.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th ACL, pages 1077–1086, Up-
psala, Sweden, July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1222–1231. Asso-
ciation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the ACL, pages 1–11.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445–1454, Jeju Island, Ko-
rea, July.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th
ACL, pages 675–684, Jeju Island, Korea, July.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of the 49th ACL, pages 1405–
1414, Portland, Oregon, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, number
June, pages 91–98, Morristown, NJ, USA.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
ACL.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-based
or morpheme-based? annotation strategies for mod-
ern hebrew clitics. In LREC. European Language
Resources Association.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmenta-
tion and pos tagging with semi-supervised methods
using large auto-analyzed data. In Proceedings of
5th IJCNLP, pages 309–317, Chiang Mai, Thailand,
November.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
Proceedings of EMNLP, pages 562–571, Honolulu,
Hawaii, October.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
EMNLP, pages 843–852, Cambridge, MA, October.
</reference>
<page confidence="0.829208">
1335
</page>
<reference confidence="0.998378590909091">
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th ACL, pages 188–193, Port-
land, Oregon, USA, June.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Chinese word segmentation and statistical
machine translation. IEEE Transactions on Signal
Processing, 5(2).
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st ACL, pages 125–134,
Sofia, Bulgaria, August.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the EACL, pages 879–887, Athens, Greece, March.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51stACL, pages 434–443, Sofia, Bulgaria, August.
</reference>
<page confidence="0.992789">
1336
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.251200">
<title confidence="0.998896">Character-Level Chinese Dependency Parsing</title>
<author confidence="0.939019">Yue Wanxiang Ting</author>
<affiliation confidence="0.893806">Center for Social Computing and Information Harbin Institute of Technology, car, University of Technology and</affiliation>
<abstract confidence="0.9728023">yue zhang@sutd.edu.sg Abstract Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures. In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the EMNLP-CONLL,</booktitle>
<pages>1455--1465</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="8769" citStr="Bohnet and Nivre, 2012" startWordPosition="1295" endWordPosition="1298"> to form the smallest left/right subwords. Figure 2 shows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 Figure 2: An example to illustrate the innermost left/right subwords. of “合M化 (legalize)” is “合M (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhan</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the EMNLP-CONLL, pages 1455–1465, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd COLING, number August,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="1382" citStr="Bohnet, 2010" startWordPosition="188" endWordPosition="189">cy parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd COLING, number August, pages 89–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Mihai Surdeanu</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Projective dependency parsing with perceptron.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>181--185</pages>
<location>New York City,</location>
<marker>Carreras, Surdeanu, M`arquez, 2006</marker>
<rawString>Xavier Carreras, Mihai Surdeanu, and Llu´ıs M`arquez. 2006. Projective dependency parsing with perceptron. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 181–185, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Chris Manning</author>
</authors>
<title>Optimizing chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="3958" citStr="Chang et al., 2008" startWordPosition="582" endWordPosition="585">ch woods industry office deputy office manager 林 业 局 Di 局 K meeting in 会 上 发 - make speech 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics director)” can be segmented as both “ACJ (deputy) |局长 (director)” and “ACJ局 长 (deputy director)”, but not “ACJ (deputy) 局 (office) I 长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotation</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Chris Manning. 2008. Optimizing chinese word segmentation for machine translation performance. In ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
</authors>
<title>Discriminative reordering with chinese grammatical relations features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="1516" citStr="Chang et al., 2009" startWordPosition="206" endWordPosition="209">arsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administration deputy director meeting in make a speech (a) a word-based dependency tree (b) a character-level dependency</context>
</contexts>
<marker>Chang, Tseng, Jurafsky, 2009</marker>
<rawString>Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, , and Christopher D. Manning. 2009. Discriminative reordering with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Transition-based dependency parsing with selectional branching.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1052--1062</pages>
<contexts>
<context position="1431" citStr="Choi and McCallum, 2013" startWordPosition="194" endWordPosition="197">el adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administration deputy director meeti</context>
</contexts>
<marker>Choi, McCallum, 2013</marker>
<rawString>Jinho D. Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In Proceedings of ACL, pages 1052–1062, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9189" citStr="Collins and Roark, 2004" startWordPosition="1370" endWordPosition="1373">011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features fo</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th EMNLP.</booktitle>
<contexts>
<context position="9163" citStr="Collins, 2002" startWordPosition="1368" endWordPosition="1369">ng and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to we</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 7th EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic models for action-based chinese dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ECML/ECPPKDD,</booktitle>
<volume>4701</volume>
<pages>559--566</pages>
<contexts>
<context position="1309" citStr="Duan et al., 2007" startWordPosition="174" endWordPosition="177">lexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese charac</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based chinese dependency parsing. In Proceedings of ECML/ECPPKDD, volume 4701 of Lecture Notes in Computer Science, pages 559–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<contexts>
<context position="2885" citStr="Emerson, 2005" startWordPosition="407" endWordPosition="409">l intra- and inter-word dependencies Figure 1: An example character-level dependency tree. “林业局Di局K在大会上发- (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation. In the well-known Chinese word segmentation bakeoff tasks, for example, different segmentation standards have been used by different data sets (Emerson, 2005). On the other hand, most disagreement on segmentation standards boils down to disagreement on segmentation granularity. As demonstrated by Zhao (2009), one can extract both finegrained and coarse-grained words from characterlevel dependency trees, and hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “Di局K (deputy woods industry office deputy office manager 林 业 局 Di 局 K meeting in 会 上 发 - make speech woods industry office deputy office manager 林 业 局 Di 局 K meeting in 会 上 发 - make speech 1326 Proceedings of the 52nd Annual Meeting of the Asso</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 123–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th ACL,</booktitle>
<pages>1045--1053</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="10210" citStr="Hatori et al., 2012" startWordPosition="1527" endWordPosition="1530">ency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased features can be used simultaneously for character-level dependency parsing. 3.1 The Arc-Standard Model The arc-standard model has been applied to joint segmentation, POS-tagging and dependency parsing (Hatori et al., 2012), but with pseudo word structures. For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system. In the word-based arc-standard model, the transition state includes a stack and a queue, where the stack contains a sequence of partially-parsed dependency trees, and the queue consists of unprocessed input words. Four actions are defined for state transition, including arc-left (AL, which creates a left arc between the top </context>
<context position="14541" citStr="Hatori et al. (2012)" startWordPosition="2330" endWordPosition="2333">the elements on the stack are full-word nodes, while the intraword actions can be applied when at least the top element on the stack is a partial-word node. For the actions ALc and ARc to be valid, the top two elements on the stack are both partial-word nodes. For the action PW to be valid, only the top element on the stack is a partial-word node. Figure 3(a) gives an example action sequence. There are three types of features. The first two types are traditionally established features for the dependency parsing and joint word segmentation and POS-tagging tasks. We use the features proposed by Hatori et al. (2012). The word-level dependency parsing features are added when the inter-word actions are applied, and the features for joint word segmentation and POS-tagging are added when the actions PW, SHw and SHc are applied. Following the work of Hatori et al. (2012), we have a parameter α to adjust the weights for joint word segmentation and POS-tagging features. We apply word-based dependency parsing features to intra-word dependency parsing as well, by using subwords (the conjunction of characters spanning the head node) to replace words in word features. The third type of features is wordstructure fea</context>
<context position="20283" citStr="Hatori et al. (2012)" startWordPosition="3318" endWordPosition="3321">segmentation and POS-tagging are taken from Zhang and Clark (2010)1. The word-level dependency parsing features are triggered when the inter-word actions are applied, while the features of joint word segmentation and POS-tagging are added when the actions SHc, ARc and PW are applied. Again we use a parameter α to adjust the weights for joint word segmentation and POS-tagging features. The wordlevel features for dependency parsing are applied to intra-word dependency parsing as well, by using subwords to replace words. The third type of features is word-structure features, which are the 1Since Hatori et al. (2012) also use Zhang and Clark (2010)’s features, the arc-standard and arc-eager characterlevel dependency parsing models have the same features for joint word segmentation and POS-tagging. Lc, Lct, Rc, Rct, Llc1c, Lrc1c, Rlc1c, Lc · Rc, Llc1ct, Lrc1ct, Rlc1ct, Lc · Rw, Lw · Rc, Lct · Rw, Lwt · Rc, Lw · Rct, Lc · Rwt, Lc · Rc · Llc1c, Lc · Rc · Lrc1c, Lc · Rc · Llc2c, Lc · Rc · Lrc2c, Lc · Rc · Rlc1c, Lc · Rc · Rlc2c, Llsw, Lrsw, Rlsw, Rrsw, Llswt, Lrswt, Rlswt, Rrswt, Llsw · Rw, Lrsw · Rw, Lw · Rlsw, Lw · Rrsw 1330 CTB50 CTB60 CTB70 Training #sent 18k 23k 31k #word 494k 641k 718k #sent 350 2.1k 10</context>
<context position="21890" citStr="Hatori et al. (2012)" startWordPosition="3607" endWordPosition="3610">t and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2. The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the intra-word dependencies are all correctly recognized. 4.2 Baseline and Proposed Models For the baseline, we have two different pipeline models. The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al., 2009). We name this model STD (pipe). The second consists of the same joint segmentation and</context>
<context position="23999" citStr="Hatori et al. (2012)" startWordPosition="3919" endWordPosition="3922">ted intra-word dependencies and real inter-word dependencies; • EAG (real, pseudo): the arc-eager model with annotated intra-word dependencies and pseudo inter-word dependencies; • EAG (pseudo, real): the arc-eager model with pseudo intra-word dependencies and real inter-word dependencies; • EAG (real, real): the arc-eager model with annotated intra-word dependencies and real inter-word dependencies. The annotated intra-word dependencies refer to the dependencies extracted from annotated word structures, while the pseudo intra-word dependencies used in the above models are similar to those of Hatori et al. (2012). For a given word w = c1c2 · · · cm, the intra-word dependency structure is cx1 cx2 · · ·x cm3. The real interword dependencies refer to the syntactic wordlevel dependencies by head-finding rules from CTB, while the pseudo inter-word dependencies refer to the word-level dependencies used by Zhao (2009) (wx1 wx2 · · ·x wn). The character-level models with annotated intra-word dependencies and pseudo inter-word dependencies are compared with the pipelines on word segmentation and POStagging accuracies, and are compared with the character-level models with annotated intra-word dependencies and r</context>
<context position="28367" citStr="Hatori et al. (2012)" startWordPosition="4649" endWordPosition="4652">ts, respectively. The results demonstrate that the character-level dependency parsing models are significantly better than the corresponding word-based pipeline models, for both the arc-standard and arc-eager systems. Similar to the findings of Zhang et al. (2013), we find that the annotated word structures can give better accuracies than pseudo word structures. Another interesting finding is that, although the arceager algorithm achieves lower accuracies in the word-based pipeline models, it obtains comparative accuracies in the character-level models. We also compare our results to those of Hatori et al. (2012), which is comparable to STD (pseudo, real) since similar arc-standard algorithms and features are used. The major difference is the set of transition actions. We rerun their system on the three datasets4. As shown in Table 6, our arc-standard system with pseudo word structures 4http://triplet.cc/. We use a different constituent-to-dependency conversion scheme in comparison with Hatori et al. (2012)’s work. α = 1 t = t = 3 t = t = t = t = α = α = α = α = α = 1332 Model CTB50 CTB60 CTB70 SEG POS DEP WS SEG POS DEP WS SEG POS DEP WS The arc-standard models STD (pipe) 97.53 93.28 79.72 – 95.32 90</context>
<context position="33799" citStr="Hatori et al. (2012)" startWordPosition="5529" endWordPosition="5532">n be complementary in parsing intra-word dependencies. 5 Related Work Zhao (2009) was the first to study character-level dependencies; they argue that since no consistent word boundaries exist over Chinese word segmentation, dependency-based representations of word structures serve as a good alternative for Chinese word segmentation. Thus their main concern is to parse intra-word dependencies. In this work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more compl</context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2012</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2012. Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese. In Proceedings of the 50th ACL, pages 1045–1053, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th ACL,</booktitle>
<pages>1077--1086</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8745" citStr="Huang and Sagae, 2010" startWordPosition="1291" endWordPosition="1294">r descendant characters to form the smallest left/right subwords. Figure 2 shows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 Figure 2: An example to illustrate the innermost left/right subwords. of “合M化 (legalize)” is “合M (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be ei</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th ACL, pages 1077–1086, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1222--1231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9428" citStr="Huang et al., 2009" startWordPosition="1411" endWordPosition="1414">incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased features can be used simultaneously for </context>
<context position="22403" citStr="Huang et al., 2009" startWordPosition="3687" endWordPosition="3690">are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the intra-word dependencies are all correctly recognized. 4.2 Baseline and Proposed Models For the baseline, we have two different pipeline models. The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al., 2009). We name this model STD (pipe). The second consists of the same joint segmentation and POS-tagging model and a word-based dependency parsing model using the arc-eager algorithm 2https://github.com/zhangmeishan/ wordstructures; their annotation was conducted on CTB 5.0, while we made annotations of the remainder of the CTB 7.0 words. We also make the annotations publicly available at the same site. (Zhang and Nivre, 2011). We name this model EAG (pipe). For the pipeline models, we use a beam of size 16 for joint segmentation and POStagging, and a beam of size 64 for dependency parsing, accordi</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1222–1231. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1332" citStr="Koo and Collins, 2010" startWordPosition="178" endWordPosition="181">es for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the ACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Guodong Zhou</author>
</authors>
<title>Unified dependency parsing of chinese morphological and syntactic structures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1445--1454</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="34198" citStr="Li and Zhou (2012)" startWordPosition="5590" endWordPosition="5593">his work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012) and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and</context>
</contexts>
<marker>Li, Zhou, 2012</marker>
<rawString>Zhongguo Li and Guodong Zhou. 2012. Unified dependency parsing of chinese morphological and syntactic structures. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1445–1454, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
</authors>
<title>Exploiting multiple treebanks for parsing with quasisynchronous grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th ACL,</booktitle>
<pages>675--684</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="1534" citStr="Li et al., 2012" startWordPosition="210" endWordPosition="213"> results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administration deputy director meeting in make a speech (a) a word-based dependency tree (b) a character-level dependency tree by Zhao (200</context>
</contexts>
<marker>Li, Liu, Che, 2012</marker>
<rawString>Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Exploiting multiple treebanks for parsing with quasisynchronous grammars. In Proceedings of the 50th ACL, pages 675–684, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
</authors>
<title>Parsing the internal structure of words: A new paradigm for chinese word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL,</booktitle>
<pages>1405--1414</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1774" citStr="Li, 2011" startWordPosition="245" endWordPosition="246">d increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administration deputy director meeting in make a speech (a) a word-based dependency tree (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies Figure 1: An example character-level dependency tree. “林业局Di局K在大会上</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Zhongguo Li. 2011. Parsing the internal structure of words: A new paradigm for chinese word segmentation. In Proceedings of the 49th ACL, pages 1405– 1414, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL, number June,</booktitle>
<pages>91--98</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1242" citStr="McDonald et al., 2005" startWordPosition="162" endWordPosition="165">ter-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL, number June, pages 91–98, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1267" citStr="Nivre and Nilsson, 2005" startWordPosition="166" endWordPosition="169">an benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, bui</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1368" citStr="Nivre, 2008" startWordPosition="186" endWordPosition="187">evel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author</context>
<context position="4765" citStr="Nivre, 2008" startWordPosition="702" endWordPosition="703">Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two dominant transitionbased dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). We study both algorithms for characterlevel dependency parsing in order to make a comprehensive investigation. For direct comparison with word-based parsers, we incorporate the traditional word segmentation, POS-tagging and dependency parsing stages in our joint parsing models. We make changes to the original transition systems, and arrive at two novel transition-based character-level parsers. We conduct experiments on three data sets, including CTB 5.0, CTB 6.0 and CTB 7.0. Experimental results show that the character-level dependency parsing models outperform the wordbased methods on all t</context>
<context position="35497" citStr="Nivre (2008)" startWordPosition="5779" endWordPosition="5780">s the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing. Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. We extend both algorithms to character-level joint word segmentation, POS-tagging and dependency parsing. To our knowledge, we are the first to apply the arceager system to joint models and achieve comparative performances to the arc-standard model. 6 Conclusions We studied the character-level Chinese dependency parsing, by making novel extensions to two commonly-used transition-based dependency parsing algorithms for word-based </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Yoav Goldberg</author>
</authors>
<title>Word-based or morpheme-based? annotation strategies for modern hebrew clitics. In</title>
<date>2008</date>
<booktitle>LREC. European Language Resources Association.</booktitle>
<contexts>
<context position="35182" citStr="Tsarfaty and Goldberg (2008)" startWordPosition="5733" endWordPosition="5736">el dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and inter-word structures, studying their influences on each other. Zhang et al. (2013) was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing. Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. We extend both algorithms to character-level joint word segmentation, POS-tagging and dependency parsing. To our knowl</context>
</contexts>
<marker>Tsarfaty, Goldberg, 2008</marker>
<rawString>Reut Tsarfaty and Yoav Goldberg. 2008. Word-based or morpheme-based? annotation strategies for modern hebrew clitics. In LREC. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th IJCNLP,</booktitle>
<pages>309--317</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="21477" citStr="Wang et al. (2011)" startWordPosition="3542" endWordPosition="3545">k 718k #sent 350 2.1k 10k Development #word 6.8k 60k 237k #oov 553 3.3k 13k #sent 348 2.8k 10k Test #word 8.0k 82k 245k #oov 278 4.6k 13k Table 2: Statistics of datasets. same as those of the character-level arc-standard model, shown in Table 1. 4 Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2. The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the in</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of 5th IJCNLP, pages 309–317, Chiang Mai, Thailand, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>562--571</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1355" citStr="Zhang and Clark, 2008" startWordPosition="182" endWordPosition="185">n while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresp</context>
<context position="9387" citStr="Zhang and Clark, 2008" startWordPosition="1404" endWordPosition="1407">2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased</context>
<context position="21571" citStr="Zhang and Clark (2008)" startWordPosition="3560" endWordPosition="3563"> 10k Test #word 8.0k 82k 245k #oov 278 4.6k 13k Table 2: Statistics of datasets. same as those of the character-level arc-standard model, shown in Table 1. 4 Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2. The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the intra-word dependencies are all correctly recognized. 4.2 Baseline and Proposed Models For the b</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing. In Proceedings of EMNLP, pages 562–571, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>843--852</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="8722" citStr="Zhang and Clark, 2010" startWordPosition="1287" endWordPosition="1290">join them with all their descendant characters to form the smallest left/right subwords. Figure 2 shows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 Figure 2: An example to illustrate the innermost left/right subwords. of “合M化 (legalize)” is “合M (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the s</context>
<context position="19729" citStr="Zhang and Clark (2010)" startWordPosition="3226" endWordPosition="3229">is full with t words, interword actions are performed; otherwise intra-word actions are performed. All the inter-word actions must be applied on full-word nodes between the stack an the deque. Figure 3(b) gives an example action sequence. Similar to the arc-standard case, there are three types of features, with the first two types being traditionally established features for dependency parsing and joint word segmentation and POStagging. The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010)1. The word-level dependency parsing features are triggered when the inter-word actions are applied, while the features of joint word segmentation and POS-tagging are added when the actions SHc, ARc and PW are applied. Again we use a parameter α to adjust the weights for joint word segmentation and POS-tagging features. The wordlevel features for dependency parsing are applied to intra-word dependency parsing as well, by using subwords to replace words. The third type of features is word-structure features, which are the 1Since Hatori et al. (2012) also use Zhang and Clark (2010)’s features, t</context>
<context position="21396" citStr="Zhang and Clark (2010)" startWordPosition="3527" endWordPosition="3530">Lw · Rlsw, Lw · Rrsw 1330 CTB50 CTB60 CTB70 Training #sent 18k 23k 31k #word 494k 641k 718k #sent 350 2.1k 10k Development #word 6.8k 60k 237k #oov 553 3.3k 13k #sent 348 2.8k 10k Test #word 8.0k 82k 245k #oov 278 4.6k 13k Table 2: Statistics of datasets. same as those of the character-level arc-standard model, shown in Table 1. 4 Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2. The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performanc</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the EMNLP, pages 843–852, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="4633" citStr="Zhang and Clark, 2011" startWordPosition="682" endWordPosition="685"> for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two dominant transitionbased dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). We study both algorithms for characterlevel dependency parsing in order to make a comprehensive investigation. For direct comparison with word-based parsers, we incorporate the traditional word segmentation, POS-tagging and dependency parsing stages in our joint parsing models. We make changes to the original transition systems, and arrive at two novel transition-based character-level parsers. We conduct experiments on three data sets, including CTB 5.0, CTB 6.0</context>
<context position="8569" citStr="Zhang and Clark, 2011" startWordPosition="1263" endWordPosition="1266">t/right subwords as atomic features. To extract the subwords, we find the innermost left/right modifiers of the head character, respectively, and then conjoin them with all their descendant characters to form the smallest left/right subwords. Figure 2 shows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 Figure 2: An example to illustrate the innermost left/right subwords. of “合M化 (legalize)” is “合M (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Coll</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL,</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1405" citStr="Zhang and Nivre, 2011" startWordPosition="190" endWordPosition="193">uracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 Introduction As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administra</context>
<context position="19629" citStr="Zhang and Nivre (2011)" startWordPosition="3210" endWordPosition="3213">iminate the ambiguity, we define a new parameter t to limit the max size of the deque. If the deque is full with t words, interword actions are performed; otherwise intra-word actions are performed. All the inter-word actions must be applied on full-word nodes between the stack an the deque. Figure 3(b) gives an example action sequence. Similar to the arc-standard case, there are three types of features, with the first two types being traditionally established features for dependency parsing and joint word segmentation and POStagging. The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010)1. The word-level dependency parsing features are triggered when the inter-word actions are applied, while the features of joint word segmentation and POS-tagging are added when the actions SHc, ARc and PW are applied. Again we use a parameter α to adjust the weights for joint word segmentation and POS-tagging features. The wordlevel features for dependency parsing are applied to intra-word dependency parsing as well, by using subwords to replace words. The third type of features is word-structu</context>
<context position="22828" citStr="Zhang and Nivre, 2011" startWordPosition="3751" endWordPosition="3754">line models. The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al., 2009). We name this model STD (pipe). The second consists of the same joint segmentation and POS-tagging model and a word-based dependency parsing model using the arc-eager algorithm 2https://github.com/zhangmeishan/ wordstructures; their annotation was conducted on CTB 5.0, while we made annotations of the remainder of the CTB 7.0 words. We also make the annotations publicly available at the same site. (Zhang and Nivre, 2011). We name this model EAG (pipe). For the pipeline models, we use a beam of size 16 for joint segmentation and POStagging, and a beam of size 64 for dependency parsing, according to previous work. We study the following character-level dependency parsing models: • STD (real, pseudo): the arc-standard model with annotated intra-word dependencies and pseudo inter-word dependencies; • STD (pseudo, real): the arc-standard model with pseudo intra-word dependencies and real inter-word dependencies; • STD (real, real): the arc-standard model with annotated intra-word dependencies and real inter-word d</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th ACL, pages 188–193, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Chinese word segmentation and statistical machine translation.</title>
<date>2008</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="3937" citStr="Zhang et al., 2008" startWordPosition="578" endWordPosition="581">in 会 上 发 - make speech woods industry office deputy office manager 林 业 局 Di 局 K meeting in 会 上 发 - make speech 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics director)” can be segmented as both “ACJ (deputy) |局长 (director)” and “ACJ局 长 (deputy director)”, but not “ACJ (deputy) 局 (office) I 长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Chinese word segmentation and statistical machine translation. IEEE Transactions on Signal Processing, 5(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Chinese parsing exploiting characters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st ACL,</booktitle>
<pages>125--134</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1795" citStr="Zhang et al., 2013" startWordPosition="247" endWordPosition="250">ng interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administration deputy director meeting in make a speech (a) a word-based dependency tree (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies Figure 1: An example character-level dependency tree. “林业局Di局K在大会上发- (The deputy direct</context>
<context position="4054" citStr="Zhang et al. (2013)" startWordPosition="597" endWordPosition="600">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics director)” can be segmented as both “ACJ (deputy) |局长 (director)” and “ACJ局 长 (deputy director)”, but not “ACJ (deputy) 局 (office) I 长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two domin</context>
<context position="6259" citStr="Zhang et al. (2013)" startWordPosition="910" endWordPosition="913">le at http://sourceforge. net/projects/zpar/, version 0.7. 2 Character-Level Dependency Tree Character-level dependencies were first proposed by Zhao (2009). They show that by annotating character dependencies within words, one can adapt to different segmentation standards. The dependencies they study are restricted to intraword characters, as illustrated in Figure 1(b). For inter-word dependencies, they use a pseudo rightheaded representation. In this study, we integrate inter-word syntactic dependencies and intra-word dependencies using large-scale annotations of word internal structures by Zhang et al. (2013), and study their interactions. We extract unlabeled dependencies from bracketed word structures according to Zhang et al.’s head annotations. In Figure 1(c), the dependencies shown by dashed arcs are intra-word dependencies, which reflect the internal word structures, while the dependencies with solid arcs are inter-word dependencies, which reflect the syntactic structures between words. In this formulation, a character-level dependency tree satisfies the same constraints as the traditional word-based dependency tree for Chinese, including projectivity. We differentiate intraword dependencies</context>
<context position="8790" citStr="Zhang et al., 2013" startWordPosition="1299" endWordPosition="1302">ft/right subwords. Figure 2 shows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 Figure 2: An example to illustrate the innermost left/right subwords. of “合M化 (legalize)” is “合M (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or</context>
<context position="21714" citStr="Zhang et al. (2013)" startWordPosition="3581" endWordPosition="3584"> Table 1. 4 Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2. The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the intra-word dependencies are all correctly recognized. 4.2 Baseline and Proposed Models For the baseline, we have two different pipeline models. The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a </context>
<context position="28011" citStr="Zhang et al. (2013)" startWordPosition="4594" endWordPosition="4597"> and arc-eager models, respectively. Table 5 shows the results. We can see that both the two models achieve better accuracies on word-level dependencies with the novel word-structure features, while the features do not affect word-structure predication significantly. 4.4 Final Results Table 6 shows the final results on the CTB50, CTB60 and CTB70 data sets, respectively. The results demonstrate that the character-level dependency parsing models are significantly better than the corresponding word-based pipeline models, for both the arc-standard and arc-eager systems. Similar to the findings of Zhang et al. (2013), we find that the annotated word structures can give better accuracies than pseudo word structures. Another interesting finding is that, although the arceager algorithm achieves lower accuracies in the word-based pipeline models, it obtains comparative accuracies in the character-level models. We also compare our results to those of Hatori et al. (2012), which is comparable to STD (pseudo, real) since similar arc-standard algorithms and features are used. The major difference is the set of transition actions. We rerun their system on the three datasets4. As shown in Table 6, our arc-standard </context>
<context position="33678" citStr="Zhang et al. (2013)" startWordPosition="5512" endWordPosition="5515">ure 4: Sentential word-structure accuracies of STD (real, real) and EAG (real, real). ing that the two parsing models can be complementary in parsing intra-word dependencies. 5 Related Work Zhao (2009) was the first to study character-level dependencies; they argue that since no consistent word boundaries exist over Chinese word segmentation, dependency-based representations of word structures serve as a good alternative for Chinese word segmentation. Thus their main concern is to parse intra-word dependencies. In this work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better perform</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. In Proceedings of the 51st ACL, pages 125–134, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Character-level dependencies in chinese: Usefulness and learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>879--887</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="2136" citStr="Zhao (2009)" startWordPosition="302" endWordPosition="303">l., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of ∗Corresponding author. 林业局 Di局K 会 上 发- forestry administration deputy director meeting in make a speech (a) a word-based dependency tree (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies Figure 1: An example character-level dependency tree. “林业局Di局K在大会上发- (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation. In the we</context>
<context position="5796" citStr="Zhao (2009)" startWordPosition="847" endWordPosition="848">s on three data sets, including CTB 5.0, CTB 6.0 and CTB 7.0. Experimental results show that the character-level dependency parsing models outperform the wordbased methods on all the data sets. Moreover, manually annotated intra-word dependencies can give improved word-level dependency accuracies than pseudo intra-word dependencies. These results confirm the usefulness of character-level syntax for Chinese analysis. The source codes are freely available at http://sourceforge. net/projects/zpar/, version 0.7. 2 Character-Level Dependency Tree Character-level dependencies were first proposed by Zhao (2009). They show that by annotating character dependencies within words, one can adapt to different segmentation standards. The dependencies they study are restricted to intraword characters, as illustrated in Figure 1(b). For inter-word dependencies, they use a pseudo rightheaded representation. In this study, we integrate inter-word syntactic dependencies and intra-word dependencies using large-scale annotations of word internal structures by Zhang et al. (2013), and study their interactions. We extract unlabeled dependencies from bracketed word structures according to Zhang et al.’s head annotat</context>
<context position="24303" citStr="Zhao (2009)" startWordPosition="3975" endWordPosition="3976">e arc-eager model with annotated intra-word dependencies and real inter-word dependencies. The annotated intra-word dependencies refer to the dependencies extracted from annotated word structures, while the pseudo intra-word dependencies used in the above models are similar to those of Hatori et al. (2012). For a given word w = c1c2 · · · cm, the intra-word dependency structure is cx1 cx2 · · ·x cm3. The real interword dependencies refer to the syntactic wordlevel dependencies by head-finding rules from CTB, while the pseudo inter-word dependencies refer to the word-level dependencies used by Zhao (2009) (wx1 wx2 · · ·x wn). The character-level models with annotated intra-word dependencies and pseudo inter-word dependencies are compared with the pipelines on word segmentation and POStagging accuracies, and are compared with the character-level models with annotated intra-word dependencies and real inter-word dependencies on word segmentation, POS-tagging and wordstructure predicating accuracies. All the proposed 3We also tried similar structures with right arcs, which gave lower accuracies. 1331 STD (real, real) SEG POS DEP WS α = 1 95.85 91.60 76.96 95.14 α = 2 96.09 91.89 77.28 95.29 α = 3 </context>
<context position="33260" citStr="Zhao (2009)" startWordPosition="5452" endWordPosition="5453">-axis denotes the sentential word-structure accuracy of STD (real, real), and the y-axis denotes that of EAG (real, real). The points at the diagonal show the same accuracies by the two models, while others show that the two models perform differently on the corresponding sentences. We can see that most points are beyond the diagonal line, indicat1333 1 0.9 0.8 0.7 0.6 0.6 0.7 0.8 0.9 1 STD (real, real) Figure 4: Sentential word-structure accuracies of STD (real, real) and EAG (real, real). ing that the two parsing models can be complementary in parsing intra-word dependencies. 5 Related Work Zhao (2009) was the first to study character-level dependencies; they argue that since no consistent word boundaries exist over Chinese word segmentation, dependency-based representations of word structures serve as a good alternative for Chinese word segmentation. Thus their main concern is to parse intra-word dependencies. In this work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-ta</context>
<context position="34495" citStr="Zhao (2009)" startWordPosition="5635" endWordPosition="5636">sing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012) and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and inter-word structures, studying their influences on each other. Zhang et al. (2013) was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures</context>
</contexts>
<marker>Zhao, 2009</marker>
<rawString>Hai Zhao. 2009. Character-level dependencies in chinese: Usefulness and learning. In Proceedings of the EACL, pages 879–887, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51stACL,</booktitle>
<pages>434--443</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="12418" citStr="Zhu et al., 2013" startWordPosition="1875" endWordPosition="1878">guation is performed by the SHw action. The actions for intra-word dependencies include intra-word arc-left (ALc), intra-word arcright (ARc), pop-word (PW) and inter-word shift (SHc). The definitions of ALc, ARc and SHc are the same as the word-based arc-standard model, while PW changes the top element on the stack into a full-word node, which can only take interword dependencies. One thing to note is that, due to variable word sizes in character-level parsing, the number of actions can vary between different sequences of actions corresponding to different analyses. We use the padding method (Zhu et al., 2013), adding an IDLE action to finished transition action sequences, for better alignments between states in the beam. In the character-level arc-standard transition 大 M 官 合 M 化 big law officer agree with law ize (a) smallest left subword (b) smallest right subword 1328 step action stack queue dependencies 0 - 0 林 业 ··· 0 1 SHw(NR) 林/NR 业 局 · · · 0 2 SHc 林/NR 业/NR 局 副 ··· 0 3 ALc 业/NR 局 副··· A1 = {林x业} 4 SHc 业/NR 局/NR 副 局 · · · A1 5 ALc 局/NR 副 局 · · · A2 = A1 U{业x局} 6 PW 林业局/NR 副 局 · · · A2 7 SHw(NN) 林业局/NR 副/NN 局 长 ··· A2 ··· ··· ··· ··· ··· 12 PW 林业局/NR 副局长/NN 会 上 · · · Ai 13 ALw 副局长/NN 会 上 · · </context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In Proceedings of the 51stACL, pages 434–443, Sofia, Bulgaria, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>