<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.450537">
<title confidence="0.997794">
USP-EACH Frequency-based Greedy Attribute Selection for
Referring Expressions Generation
</title>
<author confidence="0.998515">
Diego Jesus de Lucena Ivandré Paraboni
</author>
<affiliation confidence="0.9953085">
Escola de Artes, Ciências e Humanidades Escola de Artes, Ciências e Humanidades
University of São Paulo – USP University of São Paulo – USP
</affiliation>
<address confidence="0.759453">
Av. Arlindo Bettio, 1000 - São Paulo, Brazil Av. Arlindo Bettio, 1000 - São Paulo, Brazil
</address>
<email confidence="0.994255">
diego.si@usp.br ivandre@usp.br
</email>
<sectionHeader confidence="0.99555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99968">
Both greedy and domain-oriented REG algo-
rithms have significant strengths but tend to
perform poorly according to humanlikeness
criteria as measured by, e.g., Dice scores. In
this work we describe an attempt to combine
both perspectives into a single attribute selec-
tion strategy to be used as part of the Dale &amp;
Reiter Incremental algorithm in the REG
Challenge 2008, and the results in both Furni-
ture and People domains.
</bodyText>
<sectionHeader confidence="0.998036" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999674285714286">
Minimality and Humanlikeness in REG are often
conflicting goals. Greedy algorithms tend to favour
shorter descriptions, but in doing so their output
may look unnatural. On the other hand, domain-
oriented algorithms that arguably favour more
“human-like” strategies (e.g., selecting the most
typical attributes first) pay little or no attention to
minimality, and as a result the generated descrip-
tions may become overly long or clumsy.
Which strategy might a human speaker favour?
In this work we describe an algorithm that disre-
gards minimality entirely and attempts to select
‘typical’ attributes based on two simple assump-
tions: first, when facing a complex context with a
large number of objects, an attempt to compute the
precise attribute capable of ruling out the largest
possible number of distractors is not only hard
(from the computational point of view), but also
less natural than simply using typical (e.g., fre-
quent) attributes. On the other hand, as the number
of distractors decreases, it may become gradually
clearer for the speaker which attributes are most
helpful to achieve uniqueness, up to the point in
which she may naturally switch to a ‘greedy’ strat-
egy and finalize the description. These assump-
tions are implemented as an attribute selection
strategy to be used with the Incremental algorithm
(Dale &amp; Reiter, 1995) described below.
</bodyText>
<sectionHeader confidence="0.967082" genericHeader="introduction">
2 System Description
</sectionHeader>
<bodyText confidence="0.9993076">
We take a simple view of humanlikeness in which
the list of preferred attributes is sorted by relative
frequency1 as seen in the training data. The result-
ing list P is the centre piece of the following attrib-
ute selection strategy:
</bodyText>
<listItem confidence="0.9891984">
(1) select all attributes whose relative frequency
falls above a trainable threshold value t (in our
experiments t is estimated to be 0.8 for both
Furniture and People domains.)
(2) if the resulting description uniquely describes
the target object, then finalizes.
(3) if not, starting from the most frequent attribute
in P, search exhaustively for an attribute g
such that g, if selected, would rule out all re-
maining distractors in the context.
</listItem>
<footnote confidence="0.9235865">
1 This contrasts the work in Kelleher (2007), which takes into
account absolute counts seen in the training data.
</footnote>
<page confidence="0.989169">
219
</page>
<listItem confidence="0.8957212">
(4) if such attribute g exists, then g is selected and
the algorithm finalizes.
(5) if not, select the most frequent attribute f that
can rule out at least one distractor, and repeat
steps (3-5).
</listItem>
<bodyText confidence="0.999924096774193">
The selection of attribute g stands for the greedy
component of our approach, whilst the initial at-
tributes in step 1 and the attribute f account for our
‘humanlikeness as frequency’ assumption. The
overall effect attempted is the following:
- Highly frequent attributes are always selected.
In our tests this means that the attributes type
and colour were always included in Furniture
descriptions, and type was always included in
People descriptions (in both cases this is so re-
gardless of discriminatory power.) As a result,
we can only produce minimal descriptions by
chance.
- In a complex situation of reference (in which
many attributes may rule out many distractors,
but more than one will be required to achieve
uniqueness) the algorithm simply selects the
most frequent attributes, perhaps not unlike a
human speaker who has to single out the target
object but who does not have the time or re-
sources to come up with the ‘best’ attribute
straight away.
- As the number of distractors decreases, a sin-
gle attribute capable of ruling out all distrac-
tors will eventually emerge, forcing the
algorithm to switch to a greedy strategy and fi-
nalize. Once again, this might be just what
humans do when a suitable (i.e., economical)
attribute becomes sufficiently salient and all
distractors in the context can be ruled out at
once.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.9996968">
Below we summarize our results for Task 1 (At-
tribute Selection) and also for Task 3 (Attribute
Selection and Surface Realisation combined) for
the REG 2008 development data set (80 instances
for Furniture and 68 instances for People.) As ex-
pected, our algorithm is heavily penalized in the
Minimality criteria but performs reasonably well in
Humanlikeness (Dice and MASI.) if compared to
the systems presented in the previous GRE Chal-
lenge.
</bodyText>
<table confidence="0.998383444444444">
Overall Furniture People
Mean SD Mean SD Mean SD
Dice 0.75 0.25 0.82 0.22 0.66 0.26
MASI 0.53 0.39 0.62 0.39 0.42 0.35
Accuracy 0.37 0.48 0.49 0.50 0.24 0.43
Uniqueness 1.00 - 1.00 - 1.00 -
Minimality - - - - - -
String-edit distance 6.70 3.09 6.13 3.28 7.38 2.72
String-accuracy 0.02 0.14 0.04 0.19 - -
</table>
<figureCaption confidence="0.993841">
Figure 1. Attribute Selection and Surface Realisation results
</figureCaption>
<sectionHeader confidence="0.998024" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992735">
This work has been supported by CNPq-Brazil
(484015/2007-9) and FAPESP (2006/03941-7).
</bodyText>
<sectionHeader confidence="0.99929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9985635">
Dale, Robert and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science (19).
Kelleher, J.D. (2007) DIT - Frequency Based Incre-
mental Attribute Selection for GRE. MT Summit XI
Workshop Using Corpora for Natural Language
Generation: Language Generation and Machine
Translation, pp. 90-91.
</reference>
<page confidence="0.99759">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270483">
<title confidence="0.9993895">USP-EACH Frequency-based Greedy Attribute Selection for Referring Expressions Generation</title>
<author confidence="0.997414">Diego Jesus de_Lucena Ivandré Paraboni</author>
<affiliation confidence="0.9870385">Escola de Artes, Ciências e Humanidades Escola de Artes, Ciências e Humanidades University of São Paulo – USP University of São Paulo – USP</affiliation>
<address confidence="0.695163">Av. Arlindo Bettio, 1000 - São Paulo, Brazil Av. Arlindo Bettio, 1000 - São Paulo, Brazil</address>
<email confidence="0.830963">diego.si@usp.brivandre@usp.br</email>
<abstract confidence="0.873292363636364">Both greedy and domain-oriented REG algorithms have significant strengths but tend to perform poorly according to humanlikeness criteria as measured by, e.g., Dice scores. In this work we describe an attempt to combine both perspectives into a single attribute selection strategy to be used as part of the Dale &amp; Reiter Incremental algorithm in the REG Challenge 2008, and the results in both Furniture and People domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science</journal>
<volume>19</volume>
<contexts>
<context position="2178" citStr="Dale &amp; Reiter, 1995" startWordPosition="340" endWordPosition="343">the precise attribute capable of ruling out the largest possible number of distractors is not only hard (from the computational point of view), but also less natural than simply using typical (e.g., frequent) attributes. On the other hand, as the number of distractors decreases, it may become gradually clearer for the speaker which attributes are most helpful to achieve uniqueness, up to the point in which she may naturally switch to a ‘greedy’ strategy and finalize the description. These assumptions are implemented as an attribute selection strategy to be used with the Incremental algorithm (Dale &amp; Reiter, 1995) described below. 2 System Description We take a simple view of humanlikeness in which the list of preferred attributes is sorted by relative frequency1 as seen in the training data. The resulting list P is the centre piece of the following attribute selection strategy: (1) select all attributes whose relative frequency falls above a trainable threshold value t (in our experiments t is estimated to be 0.8 for both Furniture and People domains.) (2) if the resulting description uniquely describes the target object, then finalizes. (3) if not, starting from the most frequent attribute in P, sear</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, Robert and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science (19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Kelleher</author>
</authors>
<title>DIT - Frequency Based Incremental Attribute Selection for GRE. MT Summit XI Workshop Using Corpora for Natural Language Generation: Language Generation and Machine Translation,</title>
<date>2007</date>
<pages>90--91</pages>
<contexts>
<context position="2940" citStr="Kelleher (2007)" startWordPosition="469" endWordPosition="470">ency1 as seen in the training data. The resulting list P is the centre piece of the following attribute selection strategy: (1) select all attributes whose relative frequency falls above a trainable threshold value t (in our experiments t is estimated to be 0.8 for both Furniture and People domains.) (2) if the resulting description uniquely describes the target object, then finalizes. (3) if not, starting from the most frequent attribute in P, search exhaustively for an attribute g such that g, if selected, would rule out all remaining distractors in the context. 1 This contrasts the work in Kelleher (2007), which takes into account absolute counts seen in the training data. 219 (4) if such attribute g exists, then g is selected and the algorithm finalizes. (5) if not, select the most frequent attribute f that can rule out at least one distractor, and repeat steps (3-5). The selection of attribute g stands for the greedy component of our approach, whilst the initial attributes in step 1 and the attribute f account for our ‘humanlikeness as frequency’ assumption. The overall effect attempted is the following: - Highly frequent attributes are always selected. In our tests this means that the attri</context>
</contexts>
<marker>Kelleher, 2007</marker>
<rawString>Kelleher, J.D. (2007) DIT - Frequency Based Incremental Attribute Selection for GRE. MT Summit XI Workshop Using Corpora for Natural Language Generation: Language Generation and Machine Translation, pp. 90-91.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>