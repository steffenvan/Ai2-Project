<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000052">
<title confidence="0.99">
IOA: Improving SVM Based Sentiment Classification Through Post
Processing
</title>
<author confidence="0.997221">
Peijia Li, Weiqun Xu, Chenglong Ma, Jia Sun, Yonghong Yan
</author>
<affiliation confidence="0.9954415">
The Key Laboratory of Speech Acoustics and Content Understanding
Institute of Acoustics, Chinese Academy of Sciences
</affiliation>
<address confidence="0.898077">
No. 21 North 4th Ring West Road, Haidian District, 100190 Beijing, China
</address>
<email confidence="0.998771">
{lipeijia,xuweiqun,machenglong,sunjia,yanyonghong}@hccl.ioa.ac.cn
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890923076923">
This paper describes our systems for
expression-level and message-level sentiment
analysis – two subtasks of SemEval-2015
Task 10 on sentiment analysis in Twitter. First
we built two baseline systems for the two sub-
tasks using SVM with a variety of features.
Then we improved the systems through model
iteration and probability-output weighting
respectively. Our submissions are ranked the
3rd and 2nd among eleven teams on the 2015
test set and progress test set in subtask A and
the 7th and 4th among 40 teams on the two
test sets respectively in subtask B.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999815392156863">
Recently sentiment analysis has become one of the
most popular research topics in the natural language
processing community, mainly due to the exponen-
tial growth of social media data replete with sub-
jective information. The once neglected topic has
spurred immense interests from both academia and
industry. Many approaches have been proposed for
sentiment analysis in customer reviews, blogs and
microblogs (for good reviews, see (Pang and Lee,
2008; Liu, 2012; Kiritchenko et al., 2014)). These
approaches can be roughly divided into two cate-
gories. One is knowledge intensive or rule-based
approaches, e.g., (Taboada et al., 2011; Reckman et
al., 2013). Such approaches can achieve reasonably
good results when tailored for a specific domain but
their maintainability and cross domain portability is
usually weak. The other is data intensive or machine
learning-based, which learns to analyse sentiment
from data. It is currently the most predominant ap-
proach, including supervised learning, deep learning
etc. Sentiment analysis is often taken as a classifica-
tion task. Widely used classifiers include Support
Vector Machines (SVM), Maximum Entropy Mod-
els (MaxEnt), and naive Bayes classifiers. Common
features include word/character n-grams and senti-
ment lexicons, among others. Key research issues
for learning approaches include feature engineering,
model selection, ensemble learning, etc.
SemEval 2015 task10 (Rosenthal et al., 2015) is
a sequel to the two tasks on sentiment analysis in
Twitter in the past two years (Nakov et al., 2013;
Rosenthal et al., 2014). They have provided freely
available, annotated corpus as a common testbed and
significantly promoted sentiment analysis in tweet-
like short and informal texts. The same metric,
i.e., the average F1 score of positive and negative
classes, is used for measuring performances. But
this year there are some changes. Besides the classi-
cal expression-level (A) and message-level (B) sub-
tasks, another three subtasks are added, i.e., subtask
C – topic-based message polarity classification, sub-
task D – detecting trends towards a topic, and sub-
task E – determining strength of association of twit-
ter terms with positive sentiment. The organisers
make no distinction between constrained and uncon-
strained systems, which means participants could
utilise any other data. But it has to be described in
the submission form.
We submitted systems only for the expression-
level and message-level subtasks. In this paper, we
provide some details behind the systems.
</bodyText>
<page confidence="0.978533">
545
</page>
<note confidence="0.687675">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 545–550,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.9997979">
Data TaskA TaskB
Twittter2013-train 7,639 7,972
Twittter2013-dev 929 1,372
Twittter2013-test 3,625 3,198
SMS2013-test 2,334 2,093
Twittter2014-test 2,028 1,561
LiveJournal2014-test 1,315 1,142
Sarcasm2014-test 124 86
Twitter2015-test 3,092 2,390
Progress2015-test 10,681 8,987
</table>
<tableCaption confidence="0.921353333333333">
Table 1: Statistics of all the datasets. The last row
of Progress2015-test data is composed of all the pre-
vious test data sets.
</tableCaption>
<sectionHeader confidence="0.954316" genericHeader="method">
2 Our System
</sectionHeader>
<bodyText confidence="0.9996846">
Our systems are built with an SVM classifier us-
ing various features and resources, including sen-
timent lexicons and word vectors. To further im-
prove the performance, we use model iteration and
probability-output weighting.
</bodyText>
<subsectionHeader confidence="0.913686">
2.1 Resources
</subsectionHeader>
<bodyText confidence="0.999725941176471">
The resources used in our system are as follows:
Labeled training and test data: Although the
organisers make no difference between constrained
and unconstrained systems, it is not easy to make
additional data effective (Rosenthal et al., 2014). So
we just use the provided labeled data. However,
since we did not participate in the past two evalu-
ations, we are unable to get the full labeled data be-
cause some tweets are unavailable. But we crawled
as much data as possible using the provided script.
Table 1 shows the size of the labeled data and test
data we get. The 2015 test data is released directly
and the results are required to be submitted in one
week. We take the training data and development
data as our training data. The test data from the pre-
vious years can be used for tuning parameters (but
NOT for training).
</bodyText>
<subsectionHeader confidence="0.534304">
Sentiment Lexicons and Word Embedding: As
</subsectionHeader>
<bodyText confidence="0.99981">
many researchers have showed, e.g., (Mohammad
et al., 2013), sentiment lexicons play an important
role in sentiment analysis. In our system, seven
sentiment lexicons are used: the Hashtag Sentiment
lexicon, the Sentiment140 lexicon (Mohammad and
Turney, 2010), the MPQA lexicon (Wilson et al.,
</bodyText>
<table confidence="0.996711">
Feature subtask A subtask B
word ngrams ✓ ✓
POS ✓
clusters ✓
word vector ✓ ✓
negation ✓ ✓
lexicons ✓ ✓
characters ✓
</table>
<tableCaption confidence="0.998323">
Table 2: Features extracted for each subtask.
</tableCaption>
<bodyText confidence="0.998928785714286">
2005), the Bing Liu lexicon (Hu and Liu, 2004),
the AFINN-111 (Nielsen, 2011), the SentiWordNet
(Baccianella et al., 2010) and the Hedonometer lex-
icon1. In addition, as word embeddings have been
utilised to produce promising results in various NLP
applications, we use sentiment-specific word em-
bedding (Tang et al., 2014) in our system.
LibSVM: We used the package LibSVM (Chang
and Lin, 2011) to construct the classification model
for both subtasks.
CMU Tweet NLP: It is an open resource
(Owoputi et al., 2013) for analysing tweets and was
used to extract features for tokenising, POS tagging
and clustering.
</bodyText>
<subsectionHeader confidence="0.998476">
2.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.998285">
The main preprocessing steps are the following:
</bodyText>
<listItem confidence="0.999824285714286">
• All upper case letters are converted to lower
case ones
• URLs and user names are replaced with strings
‘http://someurl’ and ‘@someuser’ respectively
• Tokenise and label the tweets with part-
of-speech using Carnegie Mellon University
(CMU) tool (Owoputi et al., 2013)
</listItem>
<subsectionHeader confidence="0.976905">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.9382135">
After preprocessing, each tweet is represented as a
feature vector made up of part of the following fea-
tures, the features used in each subtask are shown in
Table 2.
</bodyText>
<listItem confidence="0.762561666666667">
• Word N-grams: A binary value of contigu-
ous n-grams of 1, 2, 3, and 4 tokens and non-
contiguous n-grams (n=3, 4). Non-contiguous
</listItem>
<footnote confidence="0.976421">
1http://hedonometer.org/words.html
</footnote>
<page confidence="0.997132">
546
</page>
<bodyText confidence="0.9996042">
n-grams are those intermediate grams that are
replaced with a special symbol like ‘*’. For ex-
ample, a 4-gram “I * * guys” is the correspond-
ing non-contiguous gram of contiguous gram “I
love you guys”.
</bodyText>
<listItem confidence="0.937407193548387">
• Character N-grams: Although character n-
grams have been used in sentiment analysis by
many researchers, we find that the features are
not effective for subtask B, so they are only
used for subtask A. This feature is the binary
value of the two and three prefix and suffix let-
ters.
• POS: Ten features are added by pos tagging.
They are respectively the count of interjec-
tion, adverb, preposition, article, verb, punctu-
ation, noun, pronoun, adjective and hashtag in
a tweet.
• Clusters: Every token in a tweet is mapped
to one of Twitter Word Clusters by CMU tool
(Owoputi et al., 2013). The features extracted
are a boolean vector showing the presence or
absence of the tweet in the 1000 clusters which
are generated from about 56 million tweets.
• Word Vector: Words are represented as a vec-
tor of 50 dimensions. Then we use min, aver-
age and max functions to convert the embed-
dings into fixed-length features, in a way simi-
lar to the pooling technique used in CNN to get
a tweet vector representation. So another three
features are added.
• Negation: A binary value indicating the
negated contexts. The “_NEG” suffix is ap-
pended to grams if they are in a negation scope
which starts with a negation word and ends
with certain punctuation marks2.
• Lexicons: For each token in one tweet, if it
</listItem>
<bodyText confidence="0.767694833333333">
appears in sentiment lexicons in section 2.1, it
is mapped to the corresponding score. In the
lexicons which have no sentiment score we set
the positive +1 and the negative -1. Other to-
kens are set to zero. Then a tweet would be
represented with its total score, maximal score,
</bodyText>
<footnote confidence="0.671354">
2http://sentiment.christopherpotts.net/lingstruc.html#negation
</footnote>
<bodyText confidence="0.987245333333333">
minimal score, negative score, last word score
which does not equal zero, and the count of to-
kens with non-negative score.
</bodyText>
<subsectionHeader confidence="0.987687">
2.4 Training
</subsectionHeader>
<bodyText confidence="0.999961727272727">
SVM is used as the classifier in our systems with
the features described in section 2.3. We trained
SVM on the labeled tweets with the RBF kernel
and tuned the parameters on the dev dataset. For
both subtasks, we tuned the parameters for Twit-
ter2015 test data using the Twitter2013, Twitter2014
test data as dev dataset and tuned the parameters for
the progress2015 test data using all the previous test
data as dev dataset. The parameters were tuned to
maximise the average F1 score of positive and neg-
ative classes using brute-force grid search.
</bodyText>
<subsectionHeader confidence="0.998873">
2.5 Post-processing
</subsectionHeader>
<bodyText confidence="0.999051833333333">
We tried different strategies for the different sub-
tasks. For subtask A, we adopted a model iteration
approach described in Algorithm 1. For subtask B,
we used probability-output weighting to adapt SVM
model with RBF kernel to the data set, similar to
(Miura et al., 2014).
</bodyText>
<sectionHeader confidence="0.6005475" genericHeader="method">
2.5.1 Model iteration for expression-level
subtask
</sectionHeader>
<bodyText confidence="0.904123166666667">
It was found that utilising more external data did
not improve the performance as expected because
of the different data resource and annotation method
(Rosenthal et al., 2014). So we tried a model itera-
tion approach.3 We added the test data labeled with
high confidence into the training data and then re-
trained a new model. The algorithm for subtask A is
given in Algorithm 1 and the experiment results are
given in section 3.1.
3NB: Our approach is different from the semi-supervised
learning in that we use limited test data while semi-supervised
learning usually uses a large number of external data.
</bodyText>
<page confidence="0.960142">
547
</page>
<figure confidence="0.480006">
Data c g I p wpos wneg
A-Twitter15 1100 0.00287 2 0.8 - -
A-Progress15 1100 0.00287 2 0.8 - -
B-Twitter15 1200 0.00267 - - 3.2 2.2
B-Progress15 1200 0.00267 - - 2.1 1.4
</figure>
<tableCaption confidence="0.991745">
Table 3: The parameters for different test data. I is the maximum number of iteration. wpos and wneg are
weight parameters.
</tableCaption>
<table confidence="0.933375666666667">
Data subtask A subtask B
baseline submitted baseline submitted
Twtitter15 82.31 82.76 60.02 62.62
Twitter13 83.86 83.90 68.79 71.32
SMS 84.38 84.18 68.03 68.14
Twitter14 85.09 85.37 68.70 71.86
LiveJounal 85.47 85.62 71.68 74.52
Sarcasm 71.81 71.81 53.70 51.48
Algorithm 1: Model iteration for subtask A.
</table>
<subsubsectionHeader confidence="0.188977">
2.5.2 Probability output weighting for
</subsubsectionHeader>
<bodyText confidence="0.917922375">
message-level subtask
We applied probability-output weighting (Miura
et al., 2014) into SVM and adapted it to subtask
B. For a tweet x, the base model output probabil-
ity p(c|x) for each polarity c (c E {pos, neg, neu}).
A weighting factor wc that adjusted the probability-
output p(c|x) was introduced. The system labeled
the tweet with polarity c which maximises the prod-
</bodyText>
<tableCaption confidence="0.978367">
Table 4: The overall results.
</tableCaption>
<bodyText confidence="0.8962285">
uct of wc and p(c|x), namely arg max
c
The weighting parameters wc for each polarity was
tuned by maximising the accuracy using grid-search
in the corresponding dev data. The results can be
seen in section 3.2.
</bodyText>
<sectionHeader confidence="0.997975" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999925">
The official evaluation metric of the task is the aver-
age F1 score of the positive and the negative classes.
After the base training (Section 2.4), we got the base
results in Table 4, “baseline” columns. Then we fo-
cused on improving systems for both subtasks. And
the improved (or not) results are shown in the “sub-
mitted” columns.
</bodyText>
<subsectionHeader confidence="0.987347">
3.1 Subtask A: expression-level sentiment
analysis
</subsectionHeader>
<bodyText confidence="0.999630375">
We built the system using 8,568 tweets, including
7,639 training tweets and 929 development tweets
described in section 2.1 using the features in section
2.3. After the release of the labeled test data, we
compared the performance using the same model to
rerun the test data. We set different threshold param-
eters p referred in section 2.5 to compare the results.
The experiment results are given in Table 5.
</bodyText>
<figure confidence="0.920908428571428">
Data: Train data D; Test data T; Polarity
C = {pos, neg, neu}; Threshold p;
The maximum number of iteration I;
Result: The probability-output p(c|x) for
each instance x E T; The label l�x�
for each instance x E T, lW E C
1 begin
2 i := 0;
3 do
4 Train a sentiment model M with D;
5 Compute p(c|x) for each instance
xET;
6 ΔD := 0;
7 for x in T do
8 pWx := maCxp(c|x);
9 lW := arg maxp(c|x);
c∈C
10 if pW
max &gt; p then
11 remove x from T;
12 add (x, lW) to ΔD ;
13 end
14 end
15 D +- D UΔD;
16 i++
17 while (ΔD =� 0 and i &lt; I);
18 end
wc x p(c|x).
</figure>
<page confidence="0.981528">
548
</page>
<table confidence="0.999855142857143">
Threshold p 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Twitter2015 82.42 82.56 82.76 82.76 82.70 82.53 82.31
Twitter2013 83.95 84.00 83.90 84.62 84.49 84.38 83.86
SMS 84.02 84.09 84.18 84.41 84.43 84.48 84.38
Twitter2014 84.96 85.44 85.37 85.13 84.81 85.17 85.09
LiveJounal 85.58 85.31 85.62 85.61 85.58 85.58 85.47
Sarcasm 71.81 71.58 71.81 73.07 71.81 71.58 71.81
</table>
<tableCaption confidence="0.999683">
Table 5: The results for subtask A under different threshold p . Numbers in bold are the submitted results.
</tableCaption>
<subsectionHeader confidence="0.9993335">
3.2 Subtask B: message-level sentiment
analysis
</subsectionHeader>
<bodyText confidence="0.999991111111111">
We adapted the probability-output weighting ap-
proach to subtask B. The experiment result shows
that weighting is effective for this subtask. The im-
provement using the parameters in Table 3 can be
seen from Table 4.
The approach is effective for improving the twit-
ter F1 score but degrades the performance on the
Sarcasm data, maybe because it depends too much
on the data.
</bodyText>
<subsectionHeader confidence="0.999972">
3.3 Experiment analysis
</subsectionHeader>
<bodyText confidence="0.999984157894737">
For subtask A, we made iteration stop at i = 2. The
reason why there is little improvement is: (1) Af-
ter each iteration, the number of new data added to
the training data for retraining a new model is rather
small. (2) Once the classifier puts a high confidence
on a label, this instance is very likely to be similar to
existing instances, which means the added instances
would not contribute very much to classification.
In the experiments after submission, we tried to
interchange the improvement method between the
subtasks, but they showed a little decrease on both
subtasks. When the model iteration approach was
used in subtask B, we did not receive expected im-
provement. This may be because that the perfor-
mance for subtask B is lower than that for subtask
A, which may result in the wrong samples added
into the training data. When the probability-output
weighting approach was used on subtask A, we only
got limited improvement in the F1 score.
</bodyText>
<sectionHeader confidence="0.997176" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999935916666667">
We described our system for two subtasks of Se-
mEval 2015 task 10 – Sentiment Analysis in Twit-
ter. Our systems are built by integrating a variety of
features into SVM as baselines and then improved
by model iteration and probability-output weighting
for expression-level and message-level subtasks re-
spectively. We compared the results and analyse the
reason of the improvement. Our submissions are
ranked the 3rd and 2nd among eleven teams on the
2015 test set and progress test set in subtask A and
the 7th and 4th among 40 teams on the two test sets
respectively in subtask B.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999625">
We would like to thank the shared task organis-
ers for their support throughout this work. This
work is partially supported by the National Natural
Science Foundation of China (Nos. 11161140319,
91120001, 61271426), the Strategic Priority Re-
search Program of the Chinese Academy of Sciences
(Grant Nos. XDA06030100, XDA06030500), the
National 863 Program (No. 2012AA012503) and
the CAS Priority Deployment Project (No. KGZD-
EW-103-2).
</bodyText>
<sectionHeader confidence="0.997217" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977430071428572">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC, volume 10, pages 2200–2204.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
</reference>
<page confidence="0.994978">
549
</page>
<reference confidence="0.988667725">
mal texts. Journal of Artificial Intelligence Research
(JAIR), 50:723–762.
Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1–167.
Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. Teamx: A sentiment ana-
lyzer with enhanced lexicon mapping and weighting
scheme for unbalanced data. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 628–632, Dublin, Ireland, August.
Saif M Mohammad and Peter D Turney. 2010. Emo-
tions evoked by common words and phrases: Using
mechanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26–34.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (SemEval
2013), pages 321–327, Atlanta, Georgia, USA, June.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 312–320,
Atlanta, Georgia, USA, June.
Finn Årup Nielsen. 2011. A new ANEW: evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ’Making
Sense of Microposts’: Big things come in small pack-
ages, Heraklion, Crete, Greece, May 30, 2011, pages
93–98.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In HLT-NAACL, pages
380–390.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Hilke Reckman, Cheyanne Baird, Jean Crawford,
Richard Crowell, Linnea Micciulla, Saratendu Sethi,
and Fruzsina Veress. 2013. teragram: Rule-based de-
tection of sentiment phrases using sas sentiment anal-
ysis. In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 513–519, Atlanta,
Georgia, USA, June.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in Twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 73–80, Dublin, Ireland, August.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in Twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, June.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267–307.
Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting
Liu. 2014. Building large-scale Twitter-specific sen-
timent lexicon : A representation learning approach.
In COLING 2014, 25th International Conference on
Computational Linguistics, Proceedings of the Confer-
ence: Technical Papers, August 23-29, 2014, Dublin,
Ireland, pages 172–182.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
</reference>
<bodyText confidence="0.896938">
on human language technology and empirical methods
in natural language processing, pages 347–354.
</bodyText>
<page confidence="0.994843">
550
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.719446">
<title confidence="0.997231">IOA: Improving SVM Based Sentiment Classification Through Post Processing</title>
<author confidence="0.974903">Peijia Li</author>
<author confidence="0.974903">Weiqun Xu</author>
<author confidence="0.974903">Chenglong Ma</author>
<author confidence="0.974903">Jia Sun</author>
<author confidence="0.974903">Yonghong</author>
<affiliation confidence="0.8858885">The Key Laboratory of Speech Acoustics and Content Institute of Acoustics, Chinese Academy of</affiliation>
<address confidence="0.810763">No. 21 North 4th Ring West Road, Haidian District, 100190 Beijing,</address>
<email confidence="0.979461">lipeijia@hccl.ioa.ac.cn</email>
<email confidence="0.979461">xuweiqun@hccl.ioa.ac.cn</email>
<email confidence="0.979461">machenglong@hccl.ioa.ac.cn</email>
<email confidence="0.979461">sunjia@hccl.ioa.ac.cn</email>
<email confidence="0.979461">yanyonghong@hccl.ioa.ac.cn</email>
<abstract confidence="0.998243928571429">This paper describes our systems for expression-level and message-level sentiment analysis – two subtasks of SemEval-2015 10 on analysis in First we built two baseline systems for the two subtasks using SVM with a variety of features. Then we improved the systems through model iteration and probability-output weighting respectively. Our submissions are ranked the 3rd and 2nd among eleven teams on the 2015 test set and progress test set in subtask A and the 7th and 4th among 40 teams on the two test sets respectively in subtask B.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>2200--2204</pages>
<contexts>
<context position="5785" citStr="Baccianella et al., 2010" startWordPosition="887" endWordPosition="890">ng). Sentiment Lexicons and Word Embedding: As many researchers have showed, e.g., (Mohammad et al., 2013), sentiment lexicons play an important role in sentiment analysis. In our system, seven sentiment lexicons are used: the Hashtag Sentiment lexicon, the Sentiment140 lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised to produce promising results in various NLP applications, we use sentiment-specific word embedding (Tang et al., 2014) in our system. LibSVM: We used the package LibSVM (Chang and Lin, 2011) to construct the classification model for both subtasks. CMU Tweet NLP: It is an open resource (Owoputi et al., 2013) for analysing tweets and was used to extract features for tokenising, POS tagging and clustering. 2.2 Preprocessing The main preprocessing steps are the following: • All upper case letters are converted to low</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="6057" citStr="Chang and Lin, 2011" startWordPosition="931" endWordPosition="934">n (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised to produce promising results in various NLP applications, we use sentiment-specific word embedding (Tang et al., 2014) in our system. LibSVM: We used the package LibSVM (Chang and Lin, 2011) to construct the classification model for both subtasks. CMU Tweet NLP: It is an open resource (Owoputi et al., 2013) for analysing tweets and was used to extract features for tokenising, POS tagging and clustering. 2.2 Preprocessing The main preprocessing steps are the following: • All upper case letters are converted to lower case ones • URLs and user names are replaced with strings ‘http://someurl’ and ‘@someuser’ respectively • Tokenise and label the tweets with partof-speech using Carnegie Mellon University (CMU) tool (Owoputi et al., 2013) 2.3 Features After preprocessing, each tweet is</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="5709" citStr="Hu and Liu, 2004" startWordPosition="877" endWordPosition="880">previous years can be used for tuning parameters (but NOT for training). Sentiment Lexicons and Word Embedding: As many researchers have showed, e.g., (Mohammad et al., 2013), sentiment lexicons play an important role in sentiment analysis. In our system, seven sentiment lexicons are used: the Hashtag Sentiment lexicon, the Sentiment140 lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised to produce promising results in various NLP applications, we use sentiment-specific word embedding (Tang et al., 2014) in our system. LibSVM: We used the package LibSVM (Chang and Lin, 2011) to construct the classification model for both subtasks. CMU Tweet NLP: It is an open resource (Owoputi et al., 2013) for analysing tweets and was used to extract features for tokenising, POS tagging and clustering. 2.2 Preprocessing The main preproce</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>50--723</pages>
<contexts>
<context position="1460" citStr="Kiritchenko et al., 2014" startWordPosition="215" endWordPosition="218">set and progress test set in subtask A and the 7th and 4th among 40 teams on the two test sets respectively in subtask B. 1 Introduction Recently sentiment analysis has become one of the most popular research topics in the natural language processing community, mainly due to the exponential growth of social media data replete with subjective information. The once neglected topic has spurred immense interests from both academia and industry. Many approaches have been proposed for sentiment analysis in customer reviews, blogs and microblogs (for good reviews, see (Pang and Lee, 2008; Liu, 2012; Kiritchenko et al., 2014)). These approaches can be roughly divided into two categories. One is knowledge intensive or rule-based approaches, e.g., (Taboada et al., 2011; Reckman et al., 2013). Such approaches can achieve reasonably good results when tailored for a specific domain but their maintainability and cross domain portability is usually weak. The other is data intensive or machine learning-based, which learns to analyse sentiment from data. It is currently the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classification task. Widely used cl</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mohammad. 2014. Sentiment analysis of short informal texts. Journal of Artificial Intelligence Research (JAIR), 50:723–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1433" citStr="Liu, 2012" startWordPosition="213" endWordPosition="214"> 2015 test set and progress test set in subtask A and the 7th and 4th among 40 teams on the two test sets respectively in subtask B. 1 Introduction Recently sentiment analysis has become one of the most popular research topics in the natural language processing community, mainly due to the exponential growth of social media data replete with subjective information. The once neglected topic has spurred immense interests from both academia and industry. Many approaches have been proposed for sentiment analysis in customer reviews, blogs and microblogs (for good reviews, see (Pang and Lee, 2008; Liu, 2012; Kiritchenko et al., 2014)). These approaches can be roughly divided into two categories. One is knowledge intensive or rule-based approaches, e.g., (Taboada et al., 2011; Reckman et al., 2013). Such approaches can achieve reasonably good results when tailored for a specific domain but their maintainability and cross domain portability is usually weak. The other is data intensive or machine learning-based, which learns to analyse sentiment from data. It is currently the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classifi</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Miura</author>
<author>Shigeyuki Sakaki</author>
<author>Keigo Hattori</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>628--632</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="9770" citStr="Miura et al., 2014" startWordPosition="1559" endWordPosition="1562">e parameters for Twitter2015 test data using the Twitter2013, Twitter2014 test data as dev dataset and tuned the parameters for the progress2015 test data using all the previous test data as dev dataset. The parameters were tuned to maximise the average F1 score of positive and negative classes using brute-force grid search. 2.5 Post-processing We tried different strategies for the different subtasks. For subtask A, we adopted a model iteration approach described in Algorithm 1. For subtask B, we used probability-output weighting to adapt SVM model with RBF kernel to the data set, similar to (Miura et al., 2014). 2.5.1 Model iteration for expression-level subtask It was found that utilising more external data did not improve the performance as expected because of the different data resource and annotation method (Rosenthal et al., 2014). So we tried a model iteration approach.3 We added the test data labeled with high confidence into the training data and then retrained a new model. The algorithm for subtask A is given in Algorithm 1 and the experiment results are given in section 3.1. 3NB: Our approach is different from the semi-supervised learning in that we use limited test data while semi-supervi</context>
<context position="11152" citStr="Miura et al., 2014" startWordPosition="1785" endWordPosition="1788">ter15 1200 0.00267 - - 3.2 2.2 B-Progress15 1200 0.00267 - - 2.1 1.4 Table 3: The parameters for different test data. I is the maximum number of iteration. wpos and wneg are weight parameters. Data subtask A subtask B baseline submitted baseline submitted Twtitter15 82.31 82.76 60.02 62.62 Twitter13 83.86 83.90 68.79 71.32 SMS 84.38 84.18 68.03 68.14 Twitter14 85.09 85.37 68.70 71.86 LiveJounal 85.47 85.62 71.68 74.52 Sarcasm 71.81 71.81 53.70 51.48 Algorithm 1: Model iteration for subtask A. 2.5.2 Probability output weighting for message-level subtask We applied probability-output weighting (Miura et al., 2014) into SVM and adapted it to subtask B. For a tweet x, the base model output probability p(c|x) for each polarity c (c E {pos, neg, neu}). A weighting factor wc that adjusted the probabilityoutput p(c|x) was introduced. The system labeled the tweet with polarity c which maximises the prodTable 4: The overall results. uct of wc and p(c|x), namely arg max c The weighting parameters wc for each polarity was tuned by maximising the accuracy using grid-search in the corresponding dev data. The results can be seen in section 3.2. 3 Experiments and Results The official evaluation metric of the task is</context>
</contexts>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and Tomoko Ohkuma. 2014. Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628–632, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>26--34</pages>
<contexts>
<context position="5466" citStr="Mohammad and Turney, 2010" startWordPosition="830" endWordPosition="833">le 1 shows the size of the labeled data and test data we get. The 2015 test data is released directly and the results are required to be submitted in one week. We take the training data and development data as our training data. The test data from the previous years can be used for tuning parameters (but NOT for training). Sentiment Lexicons and Word Embedding: As many researchers have showed, e.g., (Mohammad et al., 2013), sentiment lexicons play an important role in sentiment analysis. In our system, seven sentiment lexicons are used: the Hashtag Sentiment lexicon, the Sentiment140 lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised to produce promising results in various NLP applications, we use sentiment-specific word embedding (Tang et al., 2014) in our system. LibSVM: We used the package LibSVM (Chang and Lin, 2011) to const</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M Mohammad and Peter D Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>321--327</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5266" citStr="Mohammad et al., 2013" startWordPosition="802" endWordPosition="805"> not participate in the past two evaluations, we are unable to get the full labeled data because some tweets are unavailable. But we crawled as much data as possible using the provided script. Table 1 shows the size of the labeled data and test data we get. The 2015 test data is released directly and the results are required to be submitted in one week. We take the training data and development data as our training data. The test data from the previous years can be used for tuning parameters (but NOT for training). Sentiment Lexicons and Word Embedding: As many researchers have showed, e.g., (Mohammad et al., 2013), sentiment lexicons play an important role in sentiment analysis. In our system, seven sentiment lexicons are used: the Hashtag Sentiment lexicon, the Sentiment140 lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 321–327, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2518" citStr="Nakov et al., 2013" startWordPosition="373" endWordPosition="376">ntly the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classification task. Widely used classifiers include Support Vector Machines (SVM), Maximum Entropy Models (MaxEnt), and naive Bayes classifiers. Common features include word/character n-grams and sentiment lexicons, among others. Key research issues for learning approaches include feature engineering, model selection, ensemble learning, etc. SemEval 2015 task10 (Rosenthal et al., 2015) is a sequel to the two tasks on sentiment analysis in Twitter in the past two years (Nakov et al., 2013; Rosenthal et al., 2014). They have provided freely available, annotated corpus as a common testbed and significantly promoted sentiment analysis in tweetlike short and informal texts. The same metric, i.e., the average F1 score of positive and negative classes, is used for measuring performances. But this year there are some changes. Besides the classical expression-level (A) and message-level (B) subtasks, another three subtasks are added, i.e., subtask C – topic-based message polarity classification, subtask D – detecting trends towards a topic, and subtask E – determining strength of asso</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn Årup Nielsen</author>
</authors>
<title>A new ANEW: evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, Heraklion,</booktitle>
<pages>93--98</pages>
<location>Crete, Greece,</location>
<contexts>
<context position="5740" citStr="Nielsen, 2011" startWordPosition="883" endWordPosition="884">ing parameters (but NOT for training). Sentiment Lexicons and Word Embedding: As many researchers have showed, e.g., (Mohammad et al., 2013), sentiment lexicons play an important role in sentiment analysis. In our system, seven sentiment lexicons are used: the Hashtag Sentiment lexicon, the Sentiment140 lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised to produce promising results in various NLP applications, we use sentiment-specific word embedding (Tang et al., 2014) in our system. LibSVM: We used the package LibSVM (Chang and Lin, 2011) to construct the classification model for both subtasks. CMU Tweet NLP: It is an open resource (Owoputi et al., 2013) for analysing tweets and was used to extract features for tokenising, POS tagging and clustering. 2.2 Preprocessing The main preprocessing steps are the following: </context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn Årup Nielsen. 2011. A new ANEW: evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, Heraklion, Crete, Greece, May 30, 2011, pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1422" citStr="Pang and Lee, 2008" startWordPosition="209" endWordPosition="212"> eleven teams on the 2015 test set and progress test set in subtask A and the 7th and 4th among 40 teams on the two test sets respectively in subtask B. 1 Introduction Recently sentiment analysis has become one of the most popular research topics in the natural language processing community, mainly due to the exponential growth of social media data replete with subjective information. The once neglected topic has spurred immense interests from both academia and industry. Many approaches have been proposed for sentiment analysis in customer reviews, blogs and microblogs (for good reviews, see (Pang and Lee, 2008; Liu, 2012; Kiritchenko et al., 2014)). These approaches can be roughly divided into two categories. One is knowledge intensive or rule-based approaches, e.g., (Taboada et al., 2011; Reckman et al., 2013). Such approaches can achieve reasonably good results when tailored for a specific domain but their maintainability and cross domain portability is usually weak. The other is data intensive or machine learning-based, which learns to analyse sentiment from data. It is currently the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hilke Reckman</author>
<author>Cheyanne Baird</author>
<author>Jean Crawford</author>
<author>Richard Crowell</author>
<author>Linnea Micciulla</author>
<author>Saratendu Sethi</author>
<author>Fruzsina Veress</author>
</authors>
<title>teragram: Rule-based detection of sentiment phrases using sas sentiment analysis.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>513--519</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1627" citStr="Reckman et al., 2013" startWordPosition="241" endWordPosition="244">come one of the most popular research topics in the natural language processing community, mainly due to the exponential growth of social media data replete with subjective information. The once neglected topic has spurred immense interests from both academia and industry. Many approaches have been proposed for sentiment analysis in customer reviews, blogs and microblogs (for good reviews, see (Pang and Lee, 2008; Liu, 2012; Kiritchenko et al., 2014)). These approaches can be roughly divided into two categories. One is knowledge intensive or rule-based approaches, e.g., (Taboada et al., 2011; Reckman et al., 2013). Such approaches can achieve reasonably good results when tailored for a specific domain but their maintainability and cross domain portability is usually weak. The other is data intensive or machine learning-based, which learns to analyse sentiment from data. It is currently the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classification task. Widely used classifiers include Support Vector Machines (SVM), Maximum Entropy Models (MaxEnt), and naive Bayes classifiers. Common features include word/character n-grams and senti</context>
</contexts>
<marker>Reckman, Baird, Crawford, Crowell, Micciulla, Sethi, Veress, 2013</marker>
<rawString>Hilke Reckman, Cheyanne Baird, Jean Crawford, Richard Crowell, Linnea Micciulla, Saratendu Sethi, and Fruzsina Veress. 2013. teragram: Rule-based detection of sentiment phrases using sas sentiment analysis. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 513–519, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--80</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2543" citStr="Rosenthal et al., 2014" startWordPosition="377" endWordPosition="380">inant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classification task. Widely used classifiers include Support Vector Machines (SVM), Maximum Entropy Models (MaxEnt), and naive Bayes classifiers. Common features include word/character n-grams and sentiment lexicons, among others. Key research issues for learning approaches include feature engineering, model selection, ensemble learning, etc. SemEval 2015 task10 (Rosenthal et al., 2015) is a sequel to the two tasks on sentiment analysis in Twitter in the past two years (Nakov et al., 2013; Rosenthal et al., 2014). They have provided freely available, annotated corpus as a common testbed and significantly promoted sentiment analysis in tweetlike short and informal texts. The same metric, i.e., the average F1 score of positive and negative classes, is used for measuring performances. But this year there are some changes. Besides the classical expression-level (A) and message-level (B) subtasks, another three subtasks are added, i.e., subtask C – topic-based message polarity classification, subtask D – detecting trends towards a topic, and subtask E – determining strength of association of twitter terms </context>
<context position="4579" citStr="Rosenthal et al., 2014" startWordPosition="678" endWordPosition="681"> Table 1: Statistics of all the datasets. The last row of Progress2015-test data is composed of all the previous test data sets. 2 Our System Our systems are built with an SVM classifier using various features and resources, including sentiment lexicons and word vectors. To further improve the performance, we use model iteration and probability-output weighting. 2.1 Resources The resources used in our system are as follows: Labeled training and test data: Although the organisers make no difference between constrained and unconstrained systems, it is not easy to make additional data effective (Rosenthal et al., 2014). So we just use the provided labeled data. However, since we did not participate in the past two evaluations, we are unable to get the full labeled data because some tweets are unavailable. But we crawled as much data as possible using the provided script. Table 1 shows the size of the labeled data and test data we get. The 2015 test data is released directly and the results are required to be submitted in one week. We take the training data and development data as our training data. The test data from the previous years can be used for tuning parameters (but NOT for training). Sentiment Lexi</context>
<context position="9999" citStr="Rosenthal et al., 2014" startWordPosition="1593" endWordPosition="1596">uned to maximise the average F1 score of positive and negative classes using brute-force grid search. 2.5 Post-processing We tried different strategies for the different subtasks. For subtask A, we adopted a model iteration approach described in Algorithm 1. For subtask B, we used probability-output weighting to adapt SVM model with RBF kernel to the data set, similar to (Miura et al., 2014). 2.5.1 Model iteration for expression-level subtask It was found that utilising more external data did not improve the performance as expected because of the different data resource and annotation method (Rosenthal et al., 2014). So we tried a model iteration approach.3 We added the test data labeled with high confidence into the training data and then retrained a new model. The algorithm for subtask A is given in Algorithm 1 and the experiment results are given in section 3.1. 3NB: Our approach is different from the semi-supervised learning in that we use limited test data while semi-supervised learning usually uses a large number of external data. 547 Data c g I p wpos wneg A-Twitter15 1100 0.00287 2 0.8 - - A-Progress15 1100 0.00287 2 0.8 - - B-Twitter15 1200 0.00267 - - 3.2 2.2 B-Progress15 1200 0.00267 - - 2.1 1</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in Twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73–80, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="2414" citStr="Rosenthal et al., 2015" startWordPosition="352" endWordPosition="355">e other is data intensive or machine learning-based, which learns to analyse sentiment from data. It is currently the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classification task. Widely used classifiers include Support Vector Machines (SVM), Maximum Entropy Models (MaxEnt), and naive Bayes classifiers. Common features include word/character n-grams and sentiment lexicons, among others. Key research issues for learning approaches include feature engineering, model selection, ensemble learning, etc. SemEval 2015 task10 (Rosenthal et al., 2015) is a sequel to the two tasks on sentiment analysis in Twitter in the past two years (Nakov et al., 2013; Rosenthal et al., 2014). They have provided freely available, annotated corpus as a common testbed and significantly promoted sentiment analysis in tweetlike short and informal texts. The same metric, i.e., the average F1 score of positive and negative classes, is used for measuring performances. But this year there are some changes. Besides the classical expression-level (A) and message-level (B) subtasks, another three subtasks are added, i.e., subtask C – topic-based message polarity cl</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in Twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly D Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="1604" citStr="Taboada et al., 2011" startWordPosition="237" endWordPosition="240">timent analysis has become one of the most popular research topics in the natural language processing community, mainly due to the exponential growth of social media data replete with subjective information. The once neglected topic has spurred immense interests from both academia and industry. Many approaches have been proposed for sentiment analysis in customer reviews, blogs and microblogs (for good reviews, see (Pang and Lee, 2008; Liu, 2012; Kiritchenko et al., 2014)). These approaches can be roughly divided into two categories. One is knowledge intensive or rule-based approaches, e.g., (Taboada et al., 2011; Reckman et al., 2013). Such approaches can achieve reasonably good results when tailored for a specific domain but their maintainability and cross domain portability is usually weak. The other is data intensive or machine learning-based, which learns to analyse sentiment from data. It is currently the most predominant approach, including supervised learning, deep learning etc. Sentiment analysis is often taken as a classification task. Widely used classifiers include Support Vector Machines (SVM), Maximum Entropy Models (MaxEnt), and naive Bayes classifiers. Common features include word/char</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly D. Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
</authors>
<title>Building large-scale Twitter-specific sentiment lexicon : A representation learning approach.</title>
<date>2014</date>
<booktitle>In COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers,</booktitle>
<pages>172--182</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="5985" citStr="Tang et al., 2014" startWordPosition="918" endWordPosition="921">icons are used: the Hashtag Sentiment lexicon, the Sentiment140 lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., Feature subtask A subtask B word ngrams ✓ ✓ POS ✓ clusters ✓ word vector ✓ ✓ negation ✓ ✓ lexicons ✓ ✓ characters ✓ Table 2: Features extracted for each subtask. 2005), the Bing Liu lexicon (Hu and Liu, 2004), the AFINN-111 (Nielsen, 2011), the SentiWordNet (Baccianella et al., 2010) and the Hedonometer lexicon1. In addition, as word embeddings have been utilised to produce promising results in various NLP applications, we use sentiment-specific word embedding (Tang et al., 2014) in our system. LibSVM: We used the package LibSVM (Chang and Lin, 2011) to construct the classification model for both subtasks. CMU Tweet NLP: It is an open resource (Owoputi et al., 2013) for analysing tweets and was used to extract features for tokenising, POS tagging and clustering. 2.2 Preprocessing The main preprocessing steps are the following: • All upper case letters are converted to lower case ones • URLs and user names are replaced with strings ‘http://someurl’ and ‘@someuser’ respectively • Tokenise and label the tweets with partof-speech using Carnegie Mellon University (CMU) too</context>
</contexts>
<marker>Tang, Wei, Qin, Zhou, Liu, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting Liu. 2014. Building large-scale Twitter-specific sentiment lexicon : A representation learning approach. In COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, August 23-29, 2014, Dublin, Ireland, pages 172–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference</booktitle>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>