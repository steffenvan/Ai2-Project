<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.066527">
<title confidence="0.984701">
Spoken Arabic Dialect Identification Using Phonotactic Modeling
</title>
<author confidence="0.993234">
Fadi Biadsy and Julia Hirschberg Nizar Habash
</author>
<affiliation confidence="0.978232">
Department of Computer Science Center for Computational Learning Systems
Columbia University, New York, USA Columbia University, New York, USA
</affiliation>
<email confidence="0.996295">
Ifadi,julial@cs.columbia.edu habash@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975736842105">
The Arabic language is a collection of
multiple variants, among which Modern
Standard Arabic (MSA) has a special sta-
tus as the formal written standard language
of the media, culture and education across
the Arab world. The other variants are in-
formal spoken dialects that are the media
of communication for daily life. Arabic di-
alects differ substantially from MSA and
each other in terms of phonology, mor-
phology, lexical choice and syntax. In this
paper, we describe a system that automat-
ically identifies the Arabic dialect (Gulf,
Iraqi, Levantine, Egyptian and MSA) of a
speaker given a sample of his/her speech.
The phonotactic approach we use proves
to be effective in identifying these di-
alects with considerable overall accuracy
— 81.60% using 30s test utterances.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956338983051">
For the past three decades, there has been a great
deal of work on the automatic identification (ID)
of languages from the speech signal alone. Re-
cently, accent and dialect identification have be-
gun to receive attention from the speech science
and technology communities. The task of dialect
identification is the recognition of a speaker’s re-
gional dialect, within a predetermined language,
given a sample of his/her speech. The dialect-
identification problem has been viewed as more
challenging than that of language ID due to the
greater similarity between dialects of the same lan-
guage. Our goal in this paper is to analyze the ef-
fectiveness of a phonotactic approach, i.e. making
use primarily of the rules that govern phonemes
and their sequences in a language — a techniques
which has often been employed by the language
ID community — for the identification of Arabic
dialects.
The Arabic language has multiple variants, in-
cluding Modern Standard Arabic (MSA), the for-
mal written standard language of the media, cul-
ture and education, and the informal spoken di-
alects that are the preferred method of communi-
cation in daily life. While there are commercially
available Automatic Speech Recognition (ASR)
systems for recognizing MSA with low error rates
(typically trained on Broadcast News), these rec-
ognizers fail when a native Arabic speaker speaks
in his/her regional dialect. Even in news broad-
casts, speakers often code switch between MSA
and dialect, especially in conversational speech,
such as that found in interviews and talk shows.
Being able to identify dialect vs. MSA as well as to
identify which dialect is spoken during the recog-
nition process will enable ASR engines to adapt
their acoustic, pronunciation, morphological, and
language models appropriately and thus improve
recognition accuracy.
Identifying the regional dialect of a speaker will
also provide important benefits for speech tech-
nology beyond improving speech recognition. It
will allow us to infer the speaker’s regional origin
and ethnicity and to adapt features used in speaker
identification to regional original. It should also
prove useful in adapting the output of text-to-
speech synthesis to produce regional speech as
well as MSA – important for spoken dialogue sys-
tems’ development.
In Section 2, we describe related work. In Sec-
tion 3, we discuss some linguistic aspects of Ara-
bic dialects which are important to dialect iden-
tification. In Section 4, we describe the Arabic
dialect corpora employed in our experiments. In
Section 5, we explain our approach to the identifi-
cation of Arabic dialects. We present our experi-
mental results in Section 6. Finally, we conclude
in Section 7 and identify directions for future re-
search.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997675">
A variety of cues by which humans and machines
distinguish one language from another have been
explored in previous research on language identi-
</bodyText>
<note confidence="0.952048">
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 53–61,
Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998847">
53
</page>
<bodyText confidence="0.999886927710843">
fication. Examples of such cues include phone in-
ventory and phonotactics, prosody, lexicon, mor-
phology, and syntax. Some of the most suc-
cessful approaches to language ID have made
use of phonotactic variation. For example, the
Phone Recognition followed by Language Model-
ing (PRLM) approach uses phonotactic informa-
tion to identify languages from the acoustic sig-
nal alone (Zissman, 1996). In this approach, a
phone recognizer (not necessarily trained on a re-
lated language) is used to tokenize training data for
each language to be classified. Phonotactic lan-
guage models generated from this tokenized train-
ing speech are used during testing to compute lan-
guage ID likelihoods for unknown utterances.
Similar cues have successfully been used for
the identification of regional dialects. Zisssman
et al. (1996) show that the PRLM approach yields
good results classifying Cuban and Peruvian di-
alects of Spanish, using an English phone recog-
nizer trained on TIMIT (Garofolo et al., 1993).
The recognition accuracy of this system on these
two dialects is 84%, using up to 3 minutes of test
utterances. Torres-Carrasquillo et al. (2004) devel-
oped an alternate system that identifies these two
Spanish dialects using Gaussian Mixture Models
(GMM) with shifted-delta-cepstral features. This
system performs less accurately (accuracy of 70%)
than that of (Zissman et al., 1996). Alorfi (2008)
uses an ergodic HMM to model phonetic dif-
ferences between two Arabic dialects (Gulf and
Egyptian Arabic) employing standard MFCC (Mel
Frequency Cepstral Coefficients) and delta fea-
tures. With the best parameter settings, this system
achieves high accuracy of 96.67% on these two
dialects. Ma et al. (2006) use multi-dimensional
pitch flux features and MFCC features to distin-
guish three Chinese dialects. In this system the
pitch flux features reduce the error rate by more
than 30% when added to a GMM based MFCC
system. Given 15s of test-utterances, the system
achieves an accuracy of 90% on the three dialects.
Intonational cues have been shown to be good
indicators to human subjects identifying regional
dialects. Peters et al. (2002) show that human sub-
jects rely on intonational cues to identify two Ger-
man dialects (Hamburg urban dialects vs. North-
ern Standard German). Similarly, Barakat et
al. (1999) show that subjects distinguish between
Western vs. Eastern Arabic dialects significantly
above chance based on intonation alone.
Hamdi et al. (2004) show that rhythmic dif-
ferences exist between Western and Eastern Ara-
bic. The analysis of these differences is done by
comparing percentages of vocalic intervals (%V)
and the standard deviation of intervocalic inter-
vals (AC) across the two groups. These features
have been shown to capture the complexity of the
syllabic structure of a language/dialect in addition
to the existence of vowel reduction. The com-
plexity of syllabic structure of a language/dialect
and the existence of vowel reduction in a language
are good correlates with the rhythmic structure of
the language/dialect, hence the importance of such
a cue for language/dialect identification (Ramus,
2002).
As far as we could determine, there is no
previous work that analyzes the effectiveness of
a phonotactic approach, particularly the parallel
PRLM, for identifying Arabic dialects. In this pa-
per, we build a system based on this approach and
evaluate its performance on five Arabic dialects
(four regional dialects and MSA). In addition, we
experiment with six phone recognizers trained on
six languages as well as three MSA phone recog-
nizers and analyze their contribution to this classi-
fication task. Moreover, we make use of a discrim-
inative classifier that takes all the perplexities of
the language models on the phone sequences and
outputs the hypothesized dialect. This classifier
turns out to be an important component, although
it has not been a standard component in previous
work.
</bodyText>
<sectionHeader confidence="0.983142" genericHeader="method">
3 Linguistic Aspects of Arabic Dialects
</sectionHeader>
<subsectionHeader confidence="0.999196">
3.1 Arabic and its Dialects
</subsectionHeader>
<bodyText confidence="0.999890058823529">
MSA is the official language of the Arab world.
It is the primary language of the media and cul-
ture. MSA is syntactically, morphologically and
phonologically based on Classical Arabic, the lan-
guage of the Qur’an (Islam’s Holy Book). Lexi-
cally, however, it is much more modern. It is not
a native language of any Arabs but is the language
of education across the Arab world. MSA is pri-
marily written not spoken.
The Arabic dialects, in contrast, are the true na-
tive language forms. They are generally restricted
in use to informal daily communication. They
are not taught in schools or even standardized, al-
though there is a rich popular dialect culture of
folktales, songs, movies, and TV shows. Dialects
are primarily spoken, not written. However, this
is changing as more Arabs gain access to elec-
</bodyText>
<page confidence="0.996975">
54
</page>
<bodyText confidence="0.998582538461538">
tronic media such as emails and newsgroups. Ara-
bic dialects are loosely related to Classical Ara-
bic. They are the result of the interaction between
different ancient dialects of Classical Arabic and
other languages that existed in, neighbored and/or
colonized what is today the Arab world. For ex-
ample, Algerian Arabic has many influences from
Berber as well as French.
Arabic dialects vary on many dimensions –
primarily, geography and social class. Geo-
linguistically, the Arab world can be divided in
many different ways. The following is only one
of many that covers the main Arabic dialects:
</bodyText>
<listItem confidence="0.977277428571428">
• Gulf Arabic (GLF) includes the dialects of
Kuwait, Saudi Arabia, Bahrain, Qatar, United
Arab Emirates, and Oman.
• Iraqi Arabic (IRQ) is the dialect of Iraq. In
some dialect classifications, Iraqi Arabic is
considered a sub-dialect of Gulf Arabic.
• Levantine Arabic (LEV) includes the di-
alects of Lebanon, Syria, Jordan, Palestine
and Israel.
• Egyptian Arabic (EGY) covers the dialects
of the Nile valley: Egypt and Sudan.
• Maghrebi Arabic covers the dialects of
Morocco, Algeria, Tunisia and Mauritania.
Libya is sometimes included.
</listItem>
<bodyText confidence="0.990348148148148">
Yemenite Arabic is often considered its own
class. Maltese Arabic is not always consid-
ered an Arabic dialect. It is the only Arabic
variant that is considered a separate language
and is written with Latin script.
Socially, it is common to distinguish three sub-
dialects within each dialect region: city dwellers,
peasants/farmers and Bedouins. The three degrees
are often associated with a class hierarchy from
rich, settled city-dwellers down to Bedouins. Dif-
ferent social associations exist as is common in
many other languages around the world.
The relationship between MSA and the dialect
in a specific region is complex. Arabs do not think
of these two as separate languages. This particular
perception leads to a special kind of coexistence
between the two forms of language that serve dif-
ferent purposes. This kind of situation is what lin-
guists term diglossia. Although the two variants
have clear domains of prevalence: formal written
(MSA) versus informal spoken (dialect), there is
a large gray area in between and it is often filled
with a mixing of the two forms.
In this paper, we focus on classifying the di-
alect of audio recordings into one of five varieties:
MSA, GLF, IRQ, LEV, and EGY. We do not ad-
dress other dialects or diglossia.
</bodyText>
<subsectionHeader confidence="0.989368">
3.2 Phonological Variations among Arabic
Dialects
</subsectionHeader>
<bodyText confidence="0.999963394736842">
Although Arabic dialects and MSA vary on many
different levels — phonology, orthography, mor-
phology, lexical choice and syntax — we will
focus on phonological difference in this paper.1
MSA’s phonological profile includes 28 conso-
nants, three short vowels, three long vowels and
two diphthongs (/ay/ and /aw/). Arabic dialects
vary phonologically from standard Arabic and
each other. Some of the common variations in-
clude the following (Holes, 2004; Habash, 2006):
The MSA consonant (/q/) is realized as a glot-
tal stop /’/ in EGY and LEV and as /g/ in GLF and
IRQ. For example, the MSA word /t¯ari:q/ ‘road’
appears as /t¯ari:’/ (EGY and LEV) and /t¯ari:g/ (GLF
and IRQ). Other variants also are found in sub di-
alects such as /k/ in rural Palestinian (LEV) and
/dj/ in some GLF dialects. These changes do not
apply to modern and religious borrowings from
MSA. For instance, the word for ‘Qur’an’ is never
pronounced as anything but /qur’a:n/.
The MSA alveolar affricate (/dj/) is realized as
/g/ in EGY, as /j/ in LEV and as /y/ in GLF. IRQ
preserves the MSA pronunciation. For example,
the word for ‘handsome’ is /djami:l/ (MSA, IRQ),
/gami:l/ (EGY), /jami:l/ (LEV) and /yami:l/ (GLF).
The MSA consonant (/k/) is generally realized
as /k/ in Arabic dialects with the exception of GLF,
IRQ and the Palestinian rural sub-dialect of LEV,
which allow a /ˇc/ pronunciation in certain con-
texts. For example, the word for ‘fish’ is /samak/
in MSA, EGY and most of LEV but /simaˇc/ in IRQ
and GLF.
The MSA consonant /B/ is pronounced as /t/ in
LEV and EGY (or /s/ in more recent borrowings
from MSA), e.g., the MSA word /Bala:Ba/ ‘three’
is pronounced /tala:ta/ in EGY and /tla:te/ in LEV.
IRQ and GLF generally preserve the MSA pronun-
ciation.
</bodyText>
<footnote confidence="0.8651522">
1It is important to point out that since Arabic dialects are
not standardized, their orthography may not always be con-
sistent. However, this is not a relevant point to this paper
since we are interested in dialect identification using audio
recordings and without using the dialectal transcripts at all.
</footnote>
<page confidence="0.997653">
55
</page>
<bodyText confidence="0.999987454545455">
The MSA consonant /δ/ is pronounced as /d/
in LEV and EGY (or /z/ in more recent borrow-
ings from MSA), e.g., the word for ‘this’ is pro-
nounced /ha:δa/ in MSA versus /ha:da/ (LEV) and
/da/ EGY. IRQ and GLF generally preserve the
MSA pronunciation.
The MSA consonants /d¯/ (emphatic/velarized
d) and /δ¯/ (emphatic /δ/) are both normalized to
/d¯/ in EGY and LEV and to /δ¯/ in GLF and IRQ.
For example, the MSA sentence /δ¯alla yad¯rubu/
‘he continued to hit’ is pronounced /d¯all yud¯rub/
(LEV) and /δ¯all yuδ¯rub/ (GLF). In modern bor-
rowings from MSA, /δ¯/ is pronounced as /z¯/ (em-
phatic z) in EGY and LEV. For instance, the word
for ‘police officer’ is /δ¯a:bit¯/ in MSA but /z¯a:bit¯/
in EGY and LEV.
In some dialects, a loss of the emphatic feature
of some MSA consonants occurs, e.g., the MSA
word /lat¯i:f/ ‘pleasant’ is pronounced as /lati:f/ in
the Lebanese city sub-dialect of LEV. Empha-
sis typically spreads to neighboring vowels: if a
vowel is preceded or succeeded directly by an em-
phatic consonant (/d¯/, /s¯/, /t¯/, /δ¯/) then the vowel
becomes an emphatic vowel. As a result, the loss
of the emphatic feature does not affect the conso-
nants only, but also their neighboring vowels.
Other vocalic differences among MSA and the
dialects include the following: First, short vow-
els change or are completely dropped, e.g., the
MSA word /yaktubu/ ‘he writes’ is pronounced
/yiktib/ (EGY and IRQ) or /yoktob/ (LEV). Sec-
ond, final and unstressed long vowels are short-
ened, e.g., the word /mat¯a:ra:t/ ‘airports’ in MSA
becomes /mat¯ara:t/ in many dialects. Third, the
MSA diphthongs /aw/ and /ay/ have mostly be-
come /o:/ and /e:/, respectively. These vocalic
changes, particularly vowel drop lead to different
syllabic structures. MSA syllables are primarily
light (CV, CV:, CVC) but can also be (CV:C and
CVCC) in utterance-final positions. EGY sylla-
bles are the same as MSA’s although without the
utterance-final restriction. LEV, IRQ and GLF al-
low heavier syllables including word initial clus-
ters such as CCV:C and CCVCC.
</bodyText>
<sectionHeader confidence="0.999367" genericHeader="method">
4 Corpora
</sectionHeader>
<bodyText confidence="0.999914693877551">
When training a system intended to classify lan-
guages or dialects, it is of course important to use
training and testing corpora recorded under simi-
lar acoustic conditions. We are able to obtain cor-
pora from the Linguistic Data Consortium (LDC)
with similar recording conditions for four Arabic
dialects: Gulf Arabic, Iraqi Arabic, Egyptian Ara-
bic, and Levantine Arabic. These are corpora of
spontaneous telephone conversations produced by
native speakers of the dialects, speaking with fam-
ily members, friends, and unrelated individuals,
sometimes about predetermined topics. Although,
the data have been annotated phonetically and/or
orthographically by LDC, in this paper, we do not
make use of any of annotations.
We use the speech files of 965 speakers (about
41.02 hours of speech) from the Gulf Arabic
conversational telephone Speech database for our
Gulf Arabic data (Appen Pty Ltd, 2006a).2 From
these speakers we hold out 150 speakers for test-
ing (about 6.06 hours of speech).3 We use the Iraqi
Arabic Conversational Telephone Speech database
(Appen Pty Ltd, 2006b) for the Iraqi dialect, se-
lecting 475 Iraqi Arabic speakers with a total du-
ration of about 25.73 hours of speech. From
these speakers we hold out 150 speakers4 for test-
ing (about 7.33 hours of speech). Our Levan-
tine data consists of 1258 speakers from the Ara-
bic CTS Levantine Fisher Training Data Set 1-3
(Maamouri, 2006). This set contains about 78.79
hours of speech in total. We hold out 150 speakers
for testing (about 10 hours of speech) from Set 1.5
For our Egyptian data, we use CallHome Egyp-
tian and its Supplement (Canavan et al., 1997)
and CallFriend Egyptian (Canavan and Zipperlen,
1996). We use 398 speakers from these corpora
(75.7 hours of speech), holding out 150 speakers
for testing.6 (about 28.7 hours of speech.)
Unfortunately, as far as we can determine, there
is no data with similar recording conditions for
MSA. Therefore, we obtain our MSA training data
from TDT4 Arabic broadcast news. We use about
47.6 hours of speech. The acoustic signal was pro-
cessed using forced-alignment with the transcript
to remove non-speech data, such as music. For
testing we again use 150 speakers, this time iden-
tified automatically from the GALE Year 2 Dis-
tillation evaluation corpus (about 12.06 hours of
speech). Non-speech data (e.g., music) in the test
</bodyText>
<footnote confidence="0.996287727272727">
2We excluded very short speech files from the corpora.
3The 24 speakers in devtest folder and the last 63 files,
after sorting by file name, in train2c folder (126 speakers).
The sorting is done to make our experiments reproducible by
other researchers.
4Similar to the Gulf corpus, the 24 speakers in devtest
folder and the last 63 files (after sorting by filename) in
train2c folder (126 speakers)
5We use the last 75 files in Set 1, after sorting by name.
6The test speakers were from evaltest and devtest folders
in CallHome and CallFriend.
</footnote>
<page confidence="0.996801">
56
</page>
<bodyText confidence="0.99993875">
corpus was removed manually. It should be noted
that the data includes read speech by anchors and
reporters as well as spontaneous speech spoken in
interviews in studios and though the phone.
</bodyText>
<sectionHeader confidence="0.991509" genericHeader="method">
5 Our Dialect ID Approach
</sectionHeader>
<bodyText confidence="0.999881857142857">
Since, as described in Section 3, Arabic dialects
differ in many respects, such as phonology, lex-
icon, and morphology, it is highly likely that
they differ in terms of phone-sequence distribu-
tion and phonotactic constraints. Thus, we adopt
the phonotactic approach to distinguishing among
Arabic dialects.
</bodyText>
<sectionHeader confidence="0.785765" genericHeader="method">
5.1 PRLM for dialect ID
</sectionHeader>
<bodyText confidence="0.995317982758621">
As mentioned in Section 2, the PRLM approach to
language identification (Zissman, 1996) has had
considerable success. Recall that, in the PRLM
approach, the phones of the training utterances of
a dialect are first identified using a single phone
recognizer.7 Then an n-gram language model is
trained on the resulting phone sequences for this
dialect. This process results in an n-gram lan-
guage model for each dialect to model the dialect
distribution of phone sequence occurrences. Dur-
ing recognition, given a test speech segment, we
run the phone recognizer to obtain the phone se-
quence for this segment and then compute the per-
plexity of each dialect n-gram model on the se-
quence. The dialect with the n-gram model that
minimizes the perplexity is hypothesized to be the
dialect from which the segment comes.
Parallel PRLM is an extension to the PRLM ap-
proach, in which multiple (k) parallel phone rec-
ognizers, each trained on a different language, are
used instead of a single phone recognizer (Ziss-
man, 1996). For training, we run all phone recog-
nizers in parallel on the set of training utterances
of each dialect. An n-gram model on the outputs of
each phone recognizer is trained for each dialect.
Thus if we have m dialects, k x m n-gram models
are trained. During testing, given a test utterance,
we run all phone recognizers on this utterance and
compute the perplexity of each n-gram model on
the corresponding output phone sequence. Finally,
the perplexities are fed to a combiner to determine
the hypothesized dialect. In our implementation,
7The phone recognizer is typically trained on one of the
languages being identified. Nonetheless, a phone recognize
trained on any language might be a good approximation,
since languages typically share many phones in their phonetic
inventory.
we employ a logistic regression classifier as our
back-end combiner. We have experimented with
different classifiers such as SVM, and neural net-
works, but logistic regression classifier was supe-
rior. The system is illustrated in Figure 1.
We hypothesize that using multiple phone rec-
ognizers as opposed to only one allows the system
to capture subtle phonetic differences that might
be crucial to distinguish dialects. Particularly,
since the phone recognizers are trained on differ-
ent languages, they may be able to model different
vocalic and consonantal systems, hence a different
phonetic inventory. For example, an MSA phone
recognizer typically does not model the phoneme
/g/; however, an English phone recognizer does.
As described in Section 3, this phoneme is an
important cue to distinguishing Egyptian Arabic
from other Arabic dialects. Moreover, phone rec-
ognizers are prone to many errors; relying upon
multiple phone streams rather than one may lead
to a more robust model overall.
</bodyText>
<subsectionHeader confidence="0.997567">
5.2 Phone Recognizers
</subsectionHeader>
<bodyText confidence="0.999954178571428">
In our experiments, we have used phone recogniz-
ers for English, German, Japanese, Hindi, Man-
darin, and Spanish, from a toolkit developed by
Brno University of Technology.8 These phone rec-
ognizers were trained on the OGI multilanguage
database (Muthusamy et al., 1992) using a hybrid
approach based on Neural Networks and Viterbi
decoding without language models (open-loop)
(Matejka et al., 2005).
Since Arabic dialect identification is our goal,
we hypothesize that an Arabic phone recognizer
would also be useful, particularly since other
phone recognizers do not cover all Arabic con-
sonants, such as pharyngeals and emphatic alveo-
lars. Therefore, we have built our own MSA phone
recognizer using the HMM toolkit (HTK) (Young
et al., 2006). The monophone acoustic models
are built using 3-state continuous HMMs without
state-skipping, with a mixture of 12 Gaussians per
state. We extract standard Mel Frequency Cepstral
Coefficients (MFCC) features from 25 ms frames,
with a frame shift of 10 ms. Each feature vec-
tor is 39D: 13 features (12 cepstral features plus
energy), 13 deltas, and 13 double-deltas. The fea-
tures are normalized using cepstral mean normal-
ization. We use the Broadcast News TDT4 corpus
(Arabic Set 1; 47.61 hours of speech; downsam-
pled to 8Khz) to train our acoustic models. The
</bodyText>
<footnote confidence="0.95499">
8www.fit.vutbr.cz/research/groups/speech/sw/phnrec
</footnote>
<page confidence="0.9956">
57
</page>
<figure confidence="0.999884">
Acoustic
Preprocessing
Japanese Phone
Recogni�er
English Phone
Recogni�er
Arabic Phone
Recogni�er
Eng/ish phones
Arabic phones
Japanese phones
&apos;34#21%23 &apos;(
&apos;34#21%23 &apos;(
-./01%#2 &apos;(
-./01%#2 &apos;(
&apos;34#21%23 &apos;(
-./01%#2 &apos;(
(56 &apos;(
!&amp;quot;#$% &apos;(
!&amp;quot;#$% &apos;(
(56 &apos;(
)*+, &apos;(
)*+, &apos;(
(56 &apos;(
!&amp;quot;#$% &apos;(
)*+, &apos;(
Perp/exities
Back-End
Classifier
Hypothesized
Dia/ect
</figure>
<figureCaption confidence="0.999993">
Figure 1: Parallel Phone Recognition Followed by Language Modeling (PRLM) for Arabic Dialect Identification.
</figureCaption>
<bodyText confidence="0.999600066666667">
pronunciation dictionary is generated as described
in (Biadsy et al., 2009). Using these settings we
build three MSA phone recognizers: (1) an open-
loop phone recognizer which does not distinguish
emphatic vowels from non-emphatic (ArbO), (2)
an open-loop with emphatic vowels (ArbOE), and
(3) a phone recognizer with emphatic vowels and
with a bi-gram phone language model (ArbLME).
We add a new pronunciation rule to the set of
rules described in (Biadsy et al., 2009) to distin-
guish emphatic vowels from non-emphatic ones
(see Section 3) when generating our pronunciation
dictionary for training the acoustic models for the
the phone recognizers. In total we build 9 (Arabic
and non-Arabic) phone recognizers.
</bodyText>
<sectionHeader confidence="0.997821" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999952615384615">
In this section, we evaluate the effectiveness of the
parallel PRLM approach on distinguishing Ara-
bic dialects. We first run the nine phone recog-
nizers described in Section 5 on the training data
described in Section 4, for each dialect. This pro-
cess produces nine sets of phone sequences for
each dialect. In our implementation, we train a
tri-gram language model on each phone set using
the SRILM toolkit (Stolcke, 2002). Thus, in total,
we have 9 x (number of dialects) tri-grams.
In all our experiments, the 150 test speakers of
each dialect are first decoded using the phone rec-
ognizers. Then the perplexities of the correspond-
ing tri-gram models on these sequences are com-
puted, and are given to the logistic regression clas-
sifier. Instead of splitting our held-out data into
test and training sets, we report our results with
10-fold cross validation.
We have conducted three experiments to eval-
uate our system. The first is to compare the per-
formance of our system to Alorfi’s (2008) on the
same two dialects (Gulf and Egyptian Arabic).
The second is to attempt to classify four collo-
quial Arabic dialects. In the third experiment, we
include MSA as well in a five-way classification
task.
</bodyText>
<subsectionHeader confidence="0.998025">
6.1 Gulf vs. Egyptian Dialect ID
</subsectionHeader>
<bodyText confidence="0.999981586206896">
To our knowledge, Alorfi’s (2008) work is the
only work dealing with the automatic identifica-
tion of Arabic dialects. In this work, an Ergodic
HMM is used to model phonetic differences be-
tween Gulf and Egyptian Arabic using MFCC and
delta features. The test and training data used in
this work was collected from TV soap operas con-
taining both the Egyptian and Gulf dialects and
from twenty speakers from CallHome Egyptian
database. The best accuracy reported by Alorfi
(2008) on identifying the dialect of 40 utterances
of duration of 30 seconds each of 40 male speakers
(20 Egyptians and 20 Gulf speakers) is 96.67%.
Since we do not have access to the test collec-
tion used in (Alorfi, 2008), we test a version of our
system which identifies these two dialects only on
our 150 Gulf and 150 Egyptian speakers, as de-
scribed in Section 4. Our best result is 97.00%
(Egyptian and Gulf F-Measure = 0.97) when us-
ing only the features from the ArbOE, English,
Japanese, and Mandarin phone recognizers. While
our accuracy might not be significantly higher than
that of Alorfi’s, we note a few advantages of our
experiments. First, the test sets of both dialects
are from telephone conversations, with the same
recording conditions, as opposed to a mix of dif-
ferent genres. Second, in our system we test 300
speakers as oppose to 40, so our results may be
more reliable. Third, our test data includes female
</bodyText>
<page confidence="0.998638">
58
</page>
<figureCaption confidence="0.997682">
Figure 2: The accuracies and F-Measures of the four-way
classification task with different test-utterance durations
</figureCaption>
<bodyText confidence="0.9983805">
speakers as well as male, so our results are more
general.
</bodyText>
<subsectionHeader confidence="0.998373">
6.2 Four Colloquial Arabic Dialect ID
</subsectionHeader>
<bodyText confidence="0.999893870967742">
In our second experiment, we test our system on
four colloquial Arabic dialects (Gulf, Iraqi, Levan-
tine, and Egyptian). As mentioned above, we use
the phone recognizers to decode the training data
to train the 9 tri-gram models per dialect (9x4=36
tri-gram models). We report our 10-fold cross val-
idation results on the test data in Figure 2. To
analyze how dependent our system is on the du-
ration of the test utterance, we report the system
accuracy and the F-measure of each class for dif-
ferent durations (5s – 2m). The longer the ut-
terance, the better we expect the system to per-
form. We can observe from these results that re-
gardless of the test-utterance duration, the best dis-
tinguished dialect among the four dialects is Egyp-
tian (F-Measure of 94% with 30s test utterances),
followed by Levantine (F-Measure of 84% with
30s), and the most confusable dialects, according
to the classification confusion matrix, are those of
the Gulf and Iraqi Arabic (F-Measure of 68.7%,
67.3%, respectively with 30s). This confusion is
consistent with dialect classifications that consider
Iraqi a sub-dialect of Gulf Arabic, as mentioned in
Section 3.
We were also interested in testing which phone
recognizers contribute the most to the classifica-
tion task. We observe that employing a subset of
the phone recognizers as opposed to all of them
provides us with better results. Table 1 shows
which phone recognizers are selected empirically,
for each test-utterance duration condition.9
</bodyText>
<footnote confidence="0.989412">
9Starting from all phone recognizers, we remove one rec-
ognizer at a time; if the cross-validation accuracy decreases,
</footnote>
<table confidence="0.993269571428572">
Dur. Acc. (%) Phone Recognizers
5s 60.83 ArbOE+ArbLME+G+H+M+S
15s 72.83 ArbOE+ArbLME+G+H+M
30s 78.50 ArbO+H+S
45s 81.5 ArbE+ArbLME+H+G+S
60s 83.33 ArbOE+ArbLME+E+G+H+M
120s 84.00 ArbOE+ArbLME+G+M
</table>
<tableCaption confidence="0.987973625">
Table 1: Accuracy of the four-way classification (four col-
loquial Arabic dialects) and the best combination of phone
recognizers used per test-utterances duration; The phone
recognizers used are: E=English, G=German, H=Hindi,
M=Mandarin, S=Spanish, ArbO=open-loop MSA without
emphatic vowels, ArbOE=open-loop MSA with emphatic
vowels, ArbLME=MSA with emphatic vowels and bi-gram
phone LM
</tableCaption>
<bodyText confidence="0.99997625">
We observe that the MSA phone recognizers are
the most important phone recognizers for this task,
usually when emphatic vowels are modeled. In all
scenarios, removing all MSA phone recognizers
leads to a significant drop in accuracy. German,
Mandarin, Hindi, and Spanish typically contribute
to the classification task, but English, and Japanese
phone recognizers are less helpful. It is possible
that the more useful recognizers are able to cap-
ture more of the distinctions among the Arabic di-
alects; however, it might also be that the overall
quality of the recognizers also varies.
</bodyText>
<subsectionHeader confidence="0.987946">
6.3 Dialect ID with MSA
</subsectionHeader>
<bodyText confidence="0.993209333333333">
Considering MSA as a dialectal variant of Ara-
bic, we are also interested in analyzing the perfor-
mance of our system when including it in our clas-
sification task. In this experiment, we add MSA as
the fifth dialect. We perform the same steps de-
scribed above for training, using the MSA corpus
described in Section 4. For testing, we use also
our 150 hypothesized MSA speakers as our test
set. Interestingly, in this five-way classification,
we observe that the F-Measure for the MSA class
in the cross-validation task is always above 98%
regardless of the test-utterance duration, as shown
in Figure 3.
It would seem that MSA is rarely confused with
any of the colloquial dialects: it appears to have a
distinct phonotactic distribution. This explanation
is supported by linguists, who note that MSA dif-
fers from Arabic dialects in terms of its phonology,
lexicon, syntax and morphology, which appears to
lead to a profound impact on its phonotactic distri-
bution. Similar to the four-way classification task,
we add it back. We have experimented with an automatic
feature selection methods, but with the empirical (‘greedy’)
selection we typically obtain higher accuracy.
</bodyText>
<figure confidence="0.9971881">
70
9s
100
95
90
85
80
45
75
65
60
55
50
5 15 30 45 60 120
Test-utterance duraion in seconds
Accuracy
Gulf F-Measure
Iraqi F-Measure
Levantine F-Measure
Egyptian F-Measure
</figure>
<page confidence="0.855167">
59
</page>
<figureCaption confidence="0.9486645">
Figure 3: The accuracies and F-Measures of the five-way
classification task with different test-utterance durations
</figureCaption>
<table confidence="0.998688428571429">
Dur. Acc. (%) Phone Recognizers
5s 68.67 ArbO+ArbLME+H+M
15s 76.67 ArbLME+G+H+J+M
30s 81.60 ArbO+ArbOE+E+G+H+J+M+S
45s 84.80 ArbOE+ArbLME+E+G+H+J+M+S
60s 86.93 ArbOE+ArbLME+G+J+M+S
120s 87.86 ArbO+ArbLME+E+S
</table>
<tableCaption confidence="0.721200375">
Table 2: Accuracy of the five-way classification (4 colloquial
Arabic dialects + MSA) and the best combination of phone
recognizers used per test-utterances duration; The phone
recognizers used are: E=English, G=German, H=Hindi,
J=Japanese, M=Mandarin, S=Spanish, ArbO=open-loop
MSA without emphatic vowels, ArbOE=open-loop MSA
with emphatic vowels, ArbLME=MSA with emphatic vow-
els and bi-gram phone LM
</tableCaption>
<bodyText confidence="0.99998775">
Egyptian was the most easily distinguished dialect
(F-Measure=90.2%, with 30s test utterance) fol-
lowed by Levantine (79.4%), and then Iraqi and
Gulf (71.7% and 68.3%, respectively). Due to the
high MSA F-Measure, the five-way classifier can
also be used as a binary classifier to distinguish
MSA from colloquial Arabic (Gulf, Iraqi, Levan-
tine, and Egyption) reliably.
It should be noted that our classification results
for MSA might be inflated for several reasons: (1)
The MSA test data were collected from Broad-
cast News, which includes read (anchor and re-
porter) speech, as well as telephone speech (for in-
terviews). (2) The identities of the test speakers in
the MSA corpus were determined automatically,
and so might not be as accurate.
As a result of the high identification rate of
MSA, the overall accuracy in the five-way clas-
sification task is higher than that of the four-way
classification. Table 2 presents the phone recog-
nizers selected the accuracy for each test utterance
duration. We observe here that the most impor-
tant phone recognizers are those trained on MSA
(ArbO, ArbOE, and/or ArbLME). Removing them
completely leads to a significant drop in accu-
racy. In this classification task, we observe that all
phone recognizers play a role in the classification
task in some of the conditions.
</bodyText>
<sectionHeader confidence="0.991295" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999964864864865">
In this paper, we have shown that four Arabic
colloquial dialects (Gulf, Iraqi, Levantine, and
Egyptian) plus MSA can be distinguished using
a phonotactic approach with good accuracy. The
parallel PRLM approach we employ thus appears
to be effective not only for language identification
but also for Arabic dialect ID.
We have found that the most distinguishable
dialect among the five variants we consider here
is MSA, independent of the duration of the test-
utterance (F-Measure is always above 98.00%).
Egyptian Arabic is second (F-Measure of 90.2%
with 30s test-utterances), followed by Levantine
(F-Measure of 79.4%, with 30s test). The most
confusable dialects are Iraqi and Gulf (F-Measure
of 71.7% and 68.3%, respectively, with 30s test-
utterances). This high degree of Iraqi-Gulf confu-
sion is consistent with some classifications of Iraqi
Arabic as a sub-dialect of Gulf Arabic. We have
obtained a total accuracy of 81.60% in this five-
way classification task when given 30s-duration
utterances. We have also observed that the most
useful phone streams for classification are those
of our Arabic phone recognizers — typically those
with emphatic vowels.
As mentioned above, the high F-measure for
MSA may be due to the MSA corpora we have
used, which differs in genre from the dialect cor-
pora. Therefore, one focus of our future research
will be to collect MSA data with similar record-
ing conditions to the other dialects to validate
our results. We are also interested in including
prosodic features, such as intonational, durational,
and rhythmic features in our classification. A more
long-term and general goal is to use our results to
improve ASR for cases in which code-switching
occurs between MSA and other dialects.
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997598875">
We thank Dan Ellis, Michael Mandel, and Andrew Rosenberg
for useful discussions. This material is based upon work sup-
ported by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023 (approved
for public release, distribution unlimited). Any opinions,
findings and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reflect the views of DARPA.
</bodyText>
<figure confidence="0.998329190476191">
�
100
95
90
85
80
45
75
70
65
60
55
50
5 15 30 45 60 120
Test-utterance duraion in seconds
Accuracy
Gulf F-Measure
Iraqi F-Measure
Levantine F-Measure
Egyptian F-Measure
MSA F-Measure
</figure>
<page confidence="0.964037">
60
</page>
<bodyText confidence="0.81409825">
P. Torres-Carrasquillo, T. P. Gleason, and D. A. Reynolds.
2004. Dialect identification using Gaussian Mixture Mod-
els. In Proceedings of the Speaker and Language Recog-
nition Workshop, Spain.
</bodyText>
<sectionHeader confidence="0.858584" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999235193548387">
F. S. Alorfi. 2008. PhD Dissertation: Automatic Identifica-
tion Of Arabic Dialects Using Hidden Markov Models. In
University of Pittsburgh.
Appen Pty Ltd. 2006a. Gulf Arabic Conversational Tele-
phone Speech Linguistic Data Consortium, Philadelphia.
Appen Pty Ltd. 2006b. Iraqi Arabic Conversational Tele-
phone Speech Linguistic Data Consortium, Philadelphia.
M. Barkat, J. Ohala, and F. Pellegrino. 1999. Prosody as a
Distinctive Feature for the Discrimination of Arabic Di-
alects. In Proceedings of Eurospeech’99.
F. Biadsy, N. Habash, and J. Hirschberg. 2009. Improv-
ing the Arabic Pronunciation Dictionary for Phone and
Word Recognition with Linguistically-Based Pronuncia-
tion Rules. In Proceedings of NAACL/HLT 2009, Col-
orado, USA.
A. Canavan and G. Zipperlen. 1996. CALLFRIEND Egyp-
tian Arabic Speech Linguistic Data Consortium, Philadel-
phia.
A. Canavan, G. Zipperlen, and D. Graff. 1997. CALL-
HOME Egyptian Arabic Speech Linguistic Data Consor-
tium, Philadelphia.
J. S. Garofolo et al. 1993. TIMIT Acoustic-Phonetic
Continuous Speech Corpus Linguistic Data Consortium,
Philadelphia.
N. Habash. 2006. On Arabic and its Dialects. Multilingual
Magazine, 17(81).
R. Hamdi, M. Barkat-Defradas, E. Ferragne, and F. Pelle-
grino. 2004. Speech Timing and Rhythmic Structure in
Arabic Dialects: A Comparison of Two Approaches. In
Proceedings of Interspeech’04.
C. Holes. 2004. Modern Arabic: Structures, Functions, and
Varieties. Georgetown University Press. Revised Edition.
B. Ma, D. Zhu, and R. Tong. 2006. Chinese Dialect Iden-
tification Using Tone Features Based On Pitch Flux. In
Proceedings of ICASP’06.
M. Maamouri. 2006. Levantine Arabic QT Training Data
Set 5, Speech Linguistic Data Consortium, Philadelphia.
P. Matejka, P. Schwarz, J. Cernocky, and P. Chytil. 2005.
Phonotactic Language Identification using High Quality
Phoneme Recognition. In Proceedings of Eurospeech’05.
Y. K. Muthusamy, R.A. Cole, and B.T. Oshika. 1992. The
OGI Multi-Language Telephone Speech Corpus. In Pro-
ceedings of ICSLP’92.
J. Peters, P. Gilles, P. Auer, and M. Selting. 2002. Iden-
tification of Regional Varieties by Intonational Cues. An
Experimental Study on Hamburg and Berlin German.
45(2):115–139.
F. Ramus. 2002. Acoustic Correlates of Linguistic Rhythm:
Perspectives. In Speech Prosody.
A. Stolcke. 2002. SRILM - an Extensible Language Model-
ing Toolkit. In ICASP’02, pages 901–904.
S. Young, G. Evermann, M. Gales, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Wood-
land. 2006. The HTK Book, version 3.4.
M. A. Zissman, T. Gleason, D. Rekart, and B. Losiewicz.
1996. Automatic Dialect Identification of Extempora-
neous Conversational, Latin American Spanish Speech.
In Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, Atlanta, USA.
M. A. Zissman. 1996. Comparison of Four Approaches to
Automatic Language Identification of Telephone Speech.
IEEE Transactions of Speech and Audio Processing, 4(1).
</reference>
<page confidence="0.999233">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.790643">
<title confidence="0.999601">Spoken Arabic Dialect Identification Using Phonotactic Modeling</title>
<author confidence="0.998365">Biadsy Hirschberg Nizar Habash</author>
<affiliation confidence="0.919409">Department of Computer Science Center for Computational Learning Systems Columbia University, New York, USA Columbia University, New York, USA</affiliation>
<email confidence="0.999613">habash@ccls.columbia.edu</email>
<abstract confidence="0.99655865">The Arabic language is a collection of multiple variants, among which Modern Standard Arabic (MSA) has a special status as the formal written standard language of the media, culture and education across the Arab world. The other variants are informal spoken dialects that are the media of communication for daily life. Arabic dialects differ substantially from MSA and each other in terms of phonology, morphology, lexical choice and syntax. In this paper, we describe a system that automatically identifies the Arabic dialect (Gulf, Iraqi, Levantine, Egyptian and MSA) of a speaker given a sample of his/her speech. The phonotactic approach we use proves to be effective in identifying these dialects with considerable overall accuracy — 81.60% using 30s test utterances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F S Alorfi</author>
</authors>
<title>PhD Dissertation: Automatic Identification Of Arabic Dialects Using Hidden Markov Models. In</title>
<date>2008</date>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="5558" citStr="Alorfi (2008)" startWordPosition="866" endWordPosition="867">ional dialects. Zisssman et al. (1996) show that the PRLM approach yields good results classifying Cuban and Peruvian dialects of Spanish, using an English phone recognizer trained on TIMIT (Garofolo et al., 1993). The recognition accuracy of this system on these two dialects is 84%, using up to 3 minutes of test utterances. Torres-Carrasquillo et al. (2004) developed an alternate system that identifies these two Spanish dialects using Gaussian Mixture Models (GMM) with shifted-delta-cepstral features. This system performs less accurately (accuracy of 70%) than that of (Zissman et al., 1996). Alorfi (2008) uses an ergodic HMM to model phonetic differences between two Arabic dialects (Gulf and Egyptian Arabic) employing standard MFCC (Mel Frequency Cepstral Coefficients) and delta features. With the best parameter settings, this system achieves high accuracy of 96.67% on these two dialects. Ma et al. (2006) use multi-dimensional pitch flux features and MFCC features to distinguish three Chinese dialects. In this system the pitch flux features reduce the error rate by more than 30% when added to a GMM based MFCC system. Given 15s of test-utterances, the system achieves an accuracy of 90% on the t</context>
<context position="25979" citStr="Alorfi (2008)" startWordPosition="4205" endWordPosition="4206">ial Arabic dialects. In the third experiment, we include MSA as well in a five-way classification task. 6.1 Gulf vs. Egyptian Dialect ID To our knowledge, Alorfi’s (2008) work is the only work dealing with the automatic identification of Arabic dialects. In this work, an Ergodic HMM is used to model phonetic differences between Gulf and Egyptian Arabic using MFCC and delta features. The test and training data used in this work was collected from TV soap operas containing both the Egyptian and Gulf dialects and from twenty speakers from CallHome Egyptian database. The best accuracy reported by Alorfi (2008) on identifying the dialect of 40 utterances of duration of 30 seconds each of 40 male speakers (20 Egyptians and 20 Gulf speakers) is 96.67%. Since we do not have access to the test collection used in (Alorfi, 2008), we test a version of our system which identifies these two dialects only on our 150 Gulf and 150 Egyptian speakers, as described in Section 4. Our best result is 97.00% (Egyptian and Gulf F-Measure = 0.97) when using only the features from the ArbOE, English, Japanese, and Mandarin phone recognizers. While our accuracy might not be significantly higher than that of Alorfi’s, we n</context>
</contexts>
<marker>Alorfi, 2008</marker>
<rawString>F. S. Alorfi. 2008. PhD Dissertation: Automatic Identification Of Arabic Dialects Using Hidden Markov Models. In University of Pittsburgh.</rawString>
</citation>
<citation valid="false">
<booktitle>Ltd. 2006a. Gulf Arabic Conversational Telephone Speech Linguistic Data Consortium,</booktitle>
<institution>Appen Pty</institution>
<location>Philadelphia.</location>
<marker></marker>
<rawString>Appen Pty Ltd. 2006a. Gulf Arabic Conversational Telephone Speech Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="false">
<booktitle>Ltd. 2006b. Iraqi Arabic Conversational Telephone Speech Linguistic Data Consortium,</booktitle>
<institution>Appen Pty</institution>
<location>Philadelphia.</location>
<marker></marker>
<rawString>Appen Pty Ltd. 2006b. Iraqi Arabic Conversational Telephone Speech Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Barkat</author>
<author>J Ohala</author>
<author>F Pellegrino</author>
</authors>
<title>Prosody as a Distinctive Feature for the Discrimination of Arabic Dialects.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech’99.</booktitle>
<marker>Barkat, Ohala, Pellegrino, 1999</marker>
<rawString>M. Barkat, J. Ohala, and F. Pellegrino. 1999. Prosody as a Distinctive Feature for the Discrimination of Arabic Dialects. In Proceedings of Eurospeech’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Biadsy</author>
<author>N Habash</author>
<author>J Hirschberg</author>
</authors>
<title>Improving the Arabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL/HLT 2009,</booktitle>
<location>Colorado, USA.</location>
<contexts>
<context position="23609" citStr="Biadsy et al., 2009" startWordPosition="3810" endWordPosition="3813">hz) to train our acoustic models. The 8www.fit.vutbr.cz/research/groups/speech/sw/phnrec 57 Acoustic Preprocessing Japanese Phone Recogni�er English Phone Recogni�er Arabic Phone Recogni�er Eng/ish phones Arabic phones Japanese phones &apos;34#21%23 &apos;( &apos;34#21%23 &apos;( -./01%#2 &apos;( -./01%#2 &apos;( &apos;34#21%23 &apos;( -./01%#2 &apos;( (56 &apos;( !&amp;quot;#$% &apos;( !&amp;quot;#$% &apos;( (56 &apos;( )*+, &apos;( )*+, &apos;( (56 &apos;( !&amp;quot;#$% &apos;( )*+, &apos;( Perp/exities Back-End Classifier Hypothesized Dia/ect Figure 1: Parallel Phone Recognition Followed by Language Modeling (PRLM) for Arabic Dialect Identification. pronunciation dictionary is generated as described in (Biadsy et al., 2009). Using these settings we build three MSA phone recognizers: (1) an openloop phone recognizer which does not distinguish emphatic vowels from non-emphatic (ArbO), (2) an open-loop with emphatic vowels (ArbOE), and (3) a phone recognizer with emphatic vowels and with a bi-gram phone language model (ArbLME). We add a new pronunciation rule to the set of rules described in (Biadsy et al., 2009) to distinguish emphatic vowels from non-emphatic ones (see Section 3) when generating our pronunciation dictionary for training the acoustic models for the the phone recognizers. In total we build 9 (Arabi</context>
</contexts>
<marker>Biadsy, Habash, Hirschberg, 2009</marker>
<rawString>F. Biadsy, N. Habash, and J. Hirschberg. 2009. Improving the Arabic Pronunciation Dictionary for Phone and Word Recognition with Linguistically-Based Pronunciation Rules. In Proceedings of NAACL/HLT 2009, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Canavan</author>
<author>G Zipperlen</author>
</authors>
<title>CALLFRIEND Egyptian Arabic Speech Linguistic Data Consortium,</title>
<date>1996</date>
<location>Philadelphia.</location>
<contexts>
<context position="17147" citStr="Canavan and Zipperlen, 1996" startWordPosition="2778" endWordPosition="2781">abase (Appen Pty Ltd, 2006b) for the Iraqi dialect, selecting 475 Iraqi Arabic speakers with a total duration of about 25.73 hours of speech. From these speakers we hold out 150 speakers4 for testing (about 7.33 hours of speech). Our Levantine data consists of 1258 speakers from the Arabic CTS Levantine Fisher Training Data Set 1-3 (Maamouri, 2006). This set contains about 78.79 hours of speech in total. We hold out 150 speakers for testing (about 10 hours of speech) from Set 1.5 For our Egyptian data, we use CallHome Egyptian and its Supplement (Canavan et al., 1997) and CallFriend Egyptian (Canavan and Zipperlen, 1996). We use 398 speakers from these corpora (75.7 hours of speech), holding out 150 speakers for testing.6 (about 28.7 hours of speech.) Unfortunately, as far as we can determine, there is no data with similar recording conditions for MSA. Therefore, we obtain our MSA training data from TDT4 Arabic broadcast news. We use about 47.6 hours of speech. The acoustic signal was processed using forced-alignment with the transcript to remove non-speech data, such as music. For testing we again use 150 speakers, this time identified automatically from the GALE Year 2 Distillation evaluation corpus (about </context>
</contexts>
<marker>Canavan, Zipperlen, 1996</marker>
<rawString>A. Canavan and G. Zipperlen. 1996. CALLFRIEND Egyptian Arabic Speech Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Canavan</author>
<author>G Zipperlen</author>
<author>D Graff</author>
</authors>
<title>CALLHOME Egyptian Arabic Speech Linguistic Data Consortium,</title>
<date>1997</date>
<location>Philadelphia.</location>
<contexts>
<context position="17093" citStr="Canavan et al., 1997" startWordPosition="2771" endWordPosition="2774">raqi Arabic Conversational Telephone Speech database (Appen Pty Ltd, 2006b) for the Iraqi dialect, selecting 475 Iraqi Arabic speakers with a total duration of about 25.73 hours of speech. From these speakers we hold out 150 speakers4 for testing (about 7.33 hours of speech). Our Levantine data consists of 1258 speakers from the Arabic CTS Levantine Fisher Training Data Set 1-3 (Maamouri, 2006). This set contains about 78.79 hours of speech in total. We hold out 150 speakers for testing (about 10 hours of speech) from Set 1.5 For our Egyptian data, we use CallHome Egyptian and its Supplement (Canavan et al., 1997) and CallFriend Egyptian (Canavan and Zipperlen, 1996). We use 398 speakers from these corpora (75.7 hours of speech), holding out 150 speakers for testing.6 (about 28.7 hours of speech.) Unfortunately, as far as we can determine, there is no data with similar recording conditions for MSA. Therefore, we obtain our MSA training data from TDT4 Arabic broadcast news. We use about 47.6 hours of speech. The acoustic signal was processed using forced-alignment with the transcript to remove non-speech data, such as music. For testing we again use 150 speakers, this time identified automatically from </context>
</contexts>
<marker>Canavan, Zipperlen, Graff, 1997</marker>
<rawString>A. Canavan, G. Zipperlen, and D. Graff. 1997. CALLHOME Egyptian Arabic Speech Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Garofolo</author>
</authors>
<title>TIMIT Acoustic-Phonetic Continuous Speech Corpus Linguistic Data Consortium,</title>
<date>1993</date>
<location>Philadelphia.</location>
<marker>Garofolo, 1993</marker>
<rawString>J. S. Garofolo et al. 1993. TIMIT Acoustic-Phonetic Continuous Speech Corpus Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>On Arabic and its Dialects.</title>
<date>2006</date>
<journal>Multilingual Magazine,</journal>
<volume>17</volume>
<issue>81</issue>
<contexts>
<context position="11848" citStr="Habash, 2006" startWordPosition="1882" endWordPosition="1883"> one of five varieties: MSA, GLF, IRQ, LEV, and EGY. We do not address other dialects or diglossia. 3.2 Phonological Variations among Arabic Dialects Although Arabic dialects and MSA vary on many different levels — phonology, orthography, morphology, lexical choice and syntax — we will focus on phonological difference in this paper.1 MSA’s phonological profile includes 28 consonants, three short vowels, three long vowels and two diphthongs (/ay/ and /aw/). Arabic dialects vary phonologically from standard Arabic and each other. Some of the common variations include the following (Holes, 2004; Habash, 2006): The MSA consonant (/q/) is realized as a glottal stop /’/ in EGY and LEV and as /g/ in GLF and IRQ. For example, the MSA word /t¯ari:q/ ‘road’ appears as /t¯ari:’/ (EGY and LEV) and /t¯ari:g/ (GLF and IRQ). Other variants also are found in sub dialects such as /k/ in rural Palestinian (LEV) and /dj/ in some GLF dialects. These changes do not apply to modern and religious borrowings from MSA. For instance, the word for ‘Qur’an’ is never pronounced as anything but /qur’a:n/. The MSA alveolar affricate (/dj/) is realized as /g/ in EGY, as /j/ in LEV and as /y/ in GLF. IRQ preserves the MSA pron</context>
</contexts>
<marker>Habash, 2006</marker>
<rawString>N. Habash. 2006. On Arabic and its Dialects. Multilingual Magazine, 17(81).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hamdi</author>
<author>M Barkat-Defradas</author>
<author>E Ferragne</author>
<author>F Pellegrino</author>
</authors>
<title>Speech Timing and Rhythmic Structure in Arabic Dialects: A Comparison of Two Approaches.</title>
<date>2004</date>
<booktitle>In Proceedings of Interspeech’04.</booktitle>
<contexts>
<context position="6618" citStr="Hamdi et al. (2004)" startWordPosition="1032" endWordPosition="1035">res reduce the error rate by more than 30% when added to a GMM based MFCC system. Given 15s of test-utterances, the system achieves an accuracy of 90% on the three dialects. Intonational cues have been shown to be good indicators to human subjects identifying regional dialects. Peters et al. (2002) show that human subjects rely on intonational cues to identify two German dialects (Hamburg urban dialects vs. Northern Standard German). Similarly, Barakat et al. (1999) show that subjects distinguish between Western vs. Eastern Arabic dialects significantly above chance based on intonation alone. Hamdi et al. (2004) show that rhythmic differences exist between Western and Eastern Arabic. The analysis of these differences is done by comparing percentages of vocalic intervals (%V) and the standard deviation of intervocalic intervals (AC) across the two groups. These features have been shown to capture the complexity of the syllabic structure of a language/dialect in addition to the existence of vowel reduction. The complexity of syllabic structure of a language/dialect and the existence of vowel reduction in a language are good correlates with the rhythmic structure of the language/dialect, hence the impor</context>
</contexts>
<marker>Hamdi, Barkat-Defradas, Ferragne, Pellegrino, 2004</marker>
<rawString>R. Hamdi, M. Barkat-Defradas, E. Ferragne, and F. Pellegrino. 2004. Speech Timing and Rhythmic Structure in Arabic Dialects: A Comparison of Two Approaches. In Proceedings of Interspeech’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Holes</author>
</authors>
<title>Modern Arabic: Structures, Functions, and Varieties. Georgetown University Press. Revised Edition.</title>
<date>2004</date>
<contexts>
<context position="11833" citStr="Holes, 2004" startWordPosition="1880" endWordPosition="1881">cordings into one of five varieties: MSA, GLF, IRQ, LEV, and EGY. We do not address other dialects or diglossia. 3.2 Phonological Variations among Arabic Dialects Although Arabic dialects and MSA vary on many different levels — phonology, orthography, morphology, lexical choice and syntax — we will focus on phonological difference in this paper.1 MSA’s phonological profile includes 28 consonants, three short vowels, three long vowels and two diphthongs (/ay/ and /aw/). Arabic dialects vary phonologically from standard Arabic and each other. Some of the common variations include the following (Holes, 2004; Habash, 2006): The MSA consonant (/q/) is realized as a glottal stop /’/ in EGY and LEV and as /g/ in GLF and IRQ. For example, the MSA word /t¯ari:q/ ‘road’ appears as /t¯ari:’/ (EGY and LEV) and /t¯ari:g/ (GLF and IRQ). Other variants also are found in sub dialects such as /k/ in rural Palestinian (LEV) and /dj/ in some GLF dialects. These changes do not apply to modern and religious borrowings from MSA. For instance, the word for ‘Qur’an’ is never pronounced as anything but /qur’a:n/. The MSA alveolar affricate (/dj/) is realized as /g/ in EGY, as /j/ in LEV and as /y/ in GLF. IRQ preserv</context>
</contexts>
<marker>Holes, 2004</marker>
<rawString>C. Holes. 2004. Modern Arabic: Structures, Functions, and Varieties. Georgetown University Press. Revised Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ma</author>
<author>D Zhu</author>
<author>R Tong</author>
</authors>
<title>Chinese Dialect Identification Using Tone Features Based On Pitch Flux.</title>
<date>2006</date>
<booktitle>In Proceedings of ICASP’06.</booktitle>
<contexts>
<context position="5864" citStr="Ma et al. (2006)" startWordPosition="912" endWordPosition="915">tes of test utterances. Torres-Carrasquillo et al. (2004) developed an alternate system that identifies these two Spanish dialects using Gaussian Mixture Models (GMM) with shifted-delta-cepstral features. This system performs less accurately (accuracy of 70%) than that of (Zissman et al., 1996). Alorfi (2008) uses an ergodic HMM to model phonetic differences between two Arabic dialects (Gulf and Egyptian Arabic) employing standard MFCC (Mel Frequency Cepstral Coefficients) and delta features. With the best parameter settings, this system achieves high accuracy of 96.67% on these two dialects. Ma et al. (2006) use multi-dimensional pitch flux features and MFCC features to distinguish three Chinese dialects. In this system the pitch flux features reduce the error rate by more than 30% when added to a GMM based MFCC system. Given 15s of test-utterances, the system achieves an accuracy of 90% on the three dialects. Intonational cues have been shown to be good indicators to human subjects identifying regional dialects. Peters et al. (2002) show that human subjects rely on intonational cues to identify two German dialects (Hamburg urban dialects vs. Northern Standard German). Similarly, Barakat et al. (</context>
</contexts>
<marker>Ma, Zhu, Tong, 2006</marker>
<rawString>B. Ma, D. Zhu, and R. Tong. 2006. Chinese Dialect Identification Using Tone Features Based On Pitch Flux. In Proceedings of ICASP’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
</authors>
<date>2006</date>
<booktitle>Levantine Arabic QT Training Data Set 5, Speech Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="16869" citStr="Maamouri, 2006" startWordPosition="2732" endWordPosition="2733">eech) from the Gulf Arabic conversational telephone Speech database for our Gulf Arabic data (Appen Pty Ltd, 2006a).2 From these speakers we hold out 150 speakers for testing (about 6.06 hours of speech).3 We use the Iraqi Arabic Conversational Telephone Speech database (Appen Pty Ltd, 2006b) for the Iraqi dialect, selecting 475 Iraqi Arabic speakers with a total duration of about 25.73 hours of speech. From these speakers we hold out 150 speakers4 for testing (about 7.33 hours of speech). Our Levantine data consists of 1258 speakers from the Arabic CTS Levantine Fisher Training Data Set 1-3 (Maamouri, 2006). This set contains about 78.79 hours of speech in total. We hold out 150 speakers for testing (about 10 hours of speech) from Set 1.5 For our Egyptian data, we use CallHome Egyptian and its Supplement (Canavan et al., 1997) and CallFriend Egyptian (Canavan and Zipperlen, 1996). We use 398 speakers from these corpora (75.7 hours of speech), holding out 150 speakers for testing.6 (about 28.7 hours of speech.) Unfortunately, as far as we can determine, there is no data with similar recording conditions for MSA. Therefore, we obtain our MSA training data from TDT4 Arabic broadcast news. We use ab</context>
</contexts>
<marker>Maamouri, 2006</marker>
<rawString>M. Maamouri. 2006. Levantine Arabic QT Training Data Set 5, Speech Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Matejka</author>
<author>P Schwarz</author>
<author>J Cernocky</author>
<author>P Chytil</author>
</authors>
<title>Phonotactic Language Identification using High Quality Phoneme Recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of Eurospeech’05.</booktitle>
<contexts>
<context position="22121" citStr="Matejka et al., 2005" startWordPosition="3584" endWordPosition="3587">tinguishing Egyptian Arabic from other Arabic dialects. Moreover, phone recognizers are prone to many errors; relying upon multiple phone streams rather than one may lead to a more robust model overall. 5.2 Phone Recognizers In our experiments, we have used phone recognizers for English, German, Japanese, Hindi, Mandarin, and Spanish, from a toolkit developed by Brno University of Technology.8 These phone recognizers were trained on the OGI multilanguage database (Muthusamy et al., 1992) using a hybrid approach based on Neural Networks and Viterbi decoding without language models (open-loop) (Matejka et al., 2005). Since Arabic dialect identification is our goal, we hypothesize that an Arabic phone recognizer would also be useful, particularly since other phone recognizers do not cover all Arabic consonants, such as pharyngeals and emphatic alveolars. Therefore, we have built our own MSA phone recognizer using the HMM toolkit (HTK) (Young et al., 2006). The monophone acoustic models are built using 3-state continuous HMMs without state-skipping, with a mixture of 12 Gaussians per state. We extract standard Mel Frequency Cepstral Coefficients (MFCC) features from 25 ms frames, with a frame shift of 10 m</context>
</contexts>
<marker>Matejka, Schwarz, Cernocky, Chytil, 2005</marker>
<rawString>P. Matejka, P. Schwarz, J. Cernocky, and P. Chytil. 2005. Phonotactic Language Identification using High Quality Phoneme Recognition. In Proceedings of Eurospeech’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y K Muthusamy</author>
<author>R A Cole</author>
<author>B T Oshika</author>
</authors>
<title>The OGI Multi-Language Telephone Speech Corpus.</title>
<date>1992</date>
<booktitle>In Proceedings of ICSLP’92.</booktitle>
<contexts>
<context position="21992" citStr="Muthusamy et al., 1992" startWordPosition="3565" endWordPosition="3568">odel the phoneme /g/; however, an English phone recognizer does. As described in Section 3, this phoneme is an important cue to distinguishing Egyptian Arabic from other Arabic dialects. Moreover, phone recognizers are prone to many errors; relying upon multiple phone streams rather than one may lead to a more robust model overall. 5.2 Phone Recognizers In our experiments, we have used phone recognizers for English, German, Japanese, Hindi, Mandarin, and Spanish, from a toolkit developed by Brno University of Technology.8 These phone recognizers were trained on the OGI multilanguage database (Muthusamy et al., 1992) using a hybrid approach based on Neural Networks and Viterbi decoding without language models (open-loop) (Matejka et al., 2005). Since Arabic dialect identification is our goal, we hypothesize that an Arabic phone recognizer would also be useful, particularly since other phone recognizers do not cover all Arabic consonants, such as pharyngeals and emphatic alveolars. Therefore, we have built our own MSA phone recognizer using the HMM toolkit (HTK) (Young et al., 2006). The monophone acoustic models are built using 3-state continuous HMMs without state-skipping, with a mixture of 12 Gaussians</context>
</contexts>
<marker>Muthusamy, Cole, Oshika, 1992</marker>
<rawString>Y. K. Muthusamy, R.A. Cole, and B.T. Oshika. 1992. The OGI Multi-Language Telephone Speech Corpus. In Proceedings of ICSLP’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peters</author>
<author>P Gilles</author>
<author>P Auer</author>
<author>M Selting</author>
</authors>
<title>Identification of Regional Varieties by Intonational Cues. An Experimental Study on Hamburg and</title>
<date>2002</date>
<volume>45</volume>
<issue>2</issue>
<contexts>
<context position="6298" citStr="Peters et al. (2002)" startWordPosition="983" endWordPosition="986">dard MFCC (Mel Frequency Cepstral Coefficients) and delta features. With the best parameter settings, this system achieves high accuracy of 96.67% on these two dialects. Ma et al. (2006) use multi-dimensional pitch flux features and MFCC features to distinguish three Chinese dialects. In this system the pitch flux features reduce the error rate by more than 30% when added to a GMM based MFCC system. Given 15s of test-utterances, the system achieves an accuracy of 90% on the three dialects. Intonational cues have been shown to be good indicators to human subjects identifying regional dialects. Peters et al. (2002) show that human subjects rely on intonational cues to identify two German dialects (Hamburg urban dialects vs. Northern Standard German). Similarly, Barakat et al. (1999) show that subjects distinguish between Western vs. Eastern Arabic dialects significantly above chance based on intonation alone. Hamdi et al. (2004) show that rhythmic differences exist between Western and Eastern Arabic. The analysis of these differences is done by comparing percentages of vocalic intervals (%V) and the standard deviation of intervocalic intervals (AC) across the two groups. These features have been shown t</context>
</contexts>
<marker>Peters, Gilles, Auer, Selting, 2002</marker>
<rawString>J. Peters, P. Gilles, P. Auer, and M. Selting. 2002. Identification of Regional Varieties by Intonational Cues. An Experimental Study on Hamburg and Berlin German. 45(2):115–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ramus</author>
</authors>
<title>Acoustic Correlates of Linguistic Rhythm: Perspectives. In Speech Prosody.</title>
<date>2002</date>
<contexts>
<context position="7287" citStr="Ramus, 2002" startWordPosition="1137" endWordPosition="1138"> Eastern Arabic. The analysis of these differences is done by comparing percentages of vocalic intervals (%V) and the standard deviation of intervocalic intervals (AC) across the two groups. These features have been shown to capture the complexity of the syllabic structure of a language/dialect in addition to the existence of vowel reduction. The complexity of syllabic structure of a language/dialect and the existence of vowel reduction in a language are good correlates with the rhythmic structure of the language/dialect, hence the importance of such a cue for language/dialect identification (Ramus, 2002). As far as we could determine, there is no previous work that analyzes the effectiveness of a phonotactic approach, particularly the parallel PRLM, for identifying Arabic dialects. In this paper, we build a system based on this approach and evaluate its performance on five Arabic dialects (four regional dialects and MSA). In addition, we experiment with six phone recognizers trained on six languages as well as three MSA phone recognizers and analyze their contribution to this classification task. Moreover, we make use of a discriminative classifier that takes all the perplexities of the langu</context>
</contexts>
<marker>Ramus, 2002</marker>
<rawString>F. Ramus. 2002. Acoustic Correlates of Linguistic Rhythm: Perspectives. In Speech Prosody.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit. In</title>
<date>2002</date>
<booktitle>ICASP’02,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="24694" citStr="Stolcke, 2002" startWordPosition="3987" endWordPosition="3988">enerating our pronunciation dictionary for training the acoustic models for the the phone recognizers. In total we build 9 (Arabic and non-Arabic) phone recognizers. 6 Experiments and Results In this section, we evaluate the effectiveness of the parallel PRLM approach on distinguishing Arabic dialects. We first run the nine phone recognizers described in Section 5 on the training data described in Section 4, for each dialect. This process produces nine sets of phone sequences for each dialect. In our implementation, we train a tri-gram language model on each phone set using the SRILM toolkit (Stolcke, 2002). Thus, in total, we have 9 x (number of dialects) tri-grams. In all our experiments, the 150 test speakers of each dialect are first decoded using the phone recognizers. Then the perplexities of the corresponding tri-gram models on these sequences are computed, and are given to the logistic regression classifier. Instead of splitting our held-out data into test and training sets, we report our results with 10-fold cross validation. We have conducted three experiments to evaluate our system. The first is to compare the performance of our system to Alorfi’s (2008) on the same two dialects (Gulf</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In ICASP’02, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>G Evermann</author>
<author>M Gales</author>
<author>D Kershaw</author>
<author>G Moore</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>D Povey</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<title>The HTK Book, version 3.4.</title>
<date>2006</date>
<contexts>
<context position="22466" citStr="Young et al., 2006" startWordPosition="3639" endWordPosition="3642">lkit developed by Brno University of Technology.8 These phone recognizers were trained on the OGI multilanguage database (Muthusamy et al., 1992) using a hybrid approach based on Neural Networks and Viterbi decoding without language models (open-loop) (Matejka et al., 2005). Since Arabic dialect identification is our goal, we hypothesize that an Arabic phone recognizer would also be useful, particularly since other phone recognizers do not cover all Arabic consonants, such as pharyngeals and emphatic alveolars. Therefore, we have built our own MSA phone recognizer using the HMM toolkit (HTK) (Young et al., 2006). The monophone acoustic models are built using 3-state continuous HMMs without state-skipping, with a mixture of 12 Gaussians per state. We extract standard Mel Frequency Cepstral Coefficients (MFCC) features from 25 ms frames, with a frame shift of 10 ms. Each feature vector is 39D: 13 features (12 cepstral features plus energy), 13 deltas, and 13 double-deltas. The features are normalized using cepstral mean normalization. We use the Broadcast News TDT4 corpus (Arabic Set 1; 47.61 hours of speech; downsampled to 8Khz) to train our acoustic models. The 8www.fit.vutbr.cz/research/groups/speec</context>
</contexts>
<marker>Young, Evermann, Gales, Kershaw, Moore, Odell, Ollason, Povey, Valtchev, Woodland, 2006</marker>
<rawString>S. Young, G. Evermann, M. Gales, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland. 2006. The HTK Book, version 3.4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Zissman</author>
<author>T Gleason</author>
<author>D Rekart</author>
<author>B Losiewicz</author>
</authors>
<title>Automatic Dialect Identification of Extemporaneous Conversational, Latin American Spanish Speech.</title>
<date>1996</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Atlanta, USA.</location>
<contexts>
<context position="5543" citStr="Zissman et al., 1996" startWordPosition="862" endWordPosition="865">e identification of regional dialects. Zisssman et al. (1996) show that the PRLM approach yields good results classifying Cuban and Peruvian dialects of Spanish, using an English phone recognizer trained on TIMIT (Garofolo et al., 1993). The recognition accuracy of this system on these two dialects is 84%, using up to 3 minutes of test utterances. Torres-Carrasquillo et al. (2004) developed an alternate system that identifies these two Spanish dialects using Gaussian Mixture Models (GMM) with shifted-delta-cepstral features. This system performs less accurately (accuracy of 70%) than that of (Zissman et al., 1996). Alorfi (2008) uses an ergodic HMM to model phonetic differences between two Arabic dialects (Gulf and Egyptian Arabic) employing standard MFCC (Mel Frequency Cepstral Coefficients) and delta features. With the best parameter settings, this system achieves high accuracy of 96.67% on these two dialects. Ma et al. (2006) use multi-dimensional pitch flux features and MFCC features to distinguish three Chinese dialects. In this system the pitch flux features reduce the error rate by more than 30% when added to a GMM based MFCC system. Given 15s of test-utterances, the system achieves an accuracy </context>
</contexts>
<marker>Zissman, Gleason, Rekart, Losiewicz, 1996</marker>
<rawString>M. A. Zissman, T. Gleason, D. Rekart, and B. Losiewicz. 1996. Automatic Dialect Identification of Extemporaneous Conversational, Latin American Spanish Speech. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Zissman</author>
</authors>
<title>Comparison of Four Approaches to Automatic Language Identification of Telephone Speech.</title>
<date>1996</date>
<journal>IEEE Transactions of Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4561" citStr="Zissman, 1996" startWordPosition="711" endWordPosition="712">d in previous research on language identiProceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 53–61, Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics 53 fication. Examples of such cues include phone inventory and phonotactics, prosody, lexicon, morphology, and syntax. Some of the most successful approaches to language ID have made use of phonotactic variation. For example, the Phone Recognition followed by Language Modeling (PRLM) approach uses phonotactic information to identify languages from the acoustic signal alone (Zissman, 1996). In this approach, a phone recognizer (not necessarily trained on a related language) is used to tokenize training data for each language to be classified. Phonotactic language models generated from this tokenized training speech are used during testing to compute language ID likelihoods for unknown utterances. Similar cues have successfully been used for the identification of regional dialects. Zisssman et al. (1996) show that the PRLM approach yields good results classifying Cuban and Peruvian dialects of Spanish, using an English phone recognizer trained on TIMIT (Garofolo et al., 1993). T</context>
<context position="18996" citStr="Zissman, 1996" startWordPosition="3084" endWordPosition="3085">nually. It should be noted that the data includes read speech by anchors and reporters as well as spontaneous speech spoken in interviews in studios and though the phone. 5 Our Dialect ID Approach Since, as described in Section 3, Arabic dialects differ in many respects, such as phonology, lexicon, and morphology, it is highly likely that they differ in terms of phone-sequence distribution and phonotactic constraints. Thus, we adopt the phonotactic approach to distinguishing among Arabic dialects. 5.1 PRLM for dialect ID As mentioned in Section 2, the PRLM approach to language identification (Zissman, 1996) has had considerable success. Recall that, in the PRLM approach, the phones of the training utterances of a dialect are first identified using a single phone recognizer.7 Then an n-gram language model is trained on the resulting phone sequences for this dialect. This process results in an n-gram language model for each dialect to model the dialect distribution of phone sequence occurrences. During recognition, given a test speech segment, we run the phone recognizer to obtain the phone sequence for this segment and then compute the perplexity of each dialect n-gram model on the sequence. The </context>
</contexts>
<marker>Zissman, 1996</marker>
<rawString>M. A. Zissman. 1996. Comparison of Four Approaches to Automatic Language Identification of Telephone Speech. IEEE Transactions of Speech and Audio Processing, 4(1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>