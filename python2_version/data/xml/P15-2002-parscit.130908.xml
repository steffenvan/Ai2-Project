<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001401">
<title confidence="0.970469">
On metric embedding for boosting semantic similarity computations
</title>
<author confidence="0.924608">
Julien Subercaze, Christophe Gravier, Frederique Laforest
</author>
<affiliation confidence="0.459143333333333">
Universit´e de Lyon, F-42023, Saint-Etienne, France,
CNRS, UMR5516, Laboratoire Hubert Curien, F-42000, Saint-Etienne, France,
Universit´e de Saint-Etienne, Jean Monnet, F-42000, Saint-Etienne, France.
</affiliation>
<email confidence="0.675034">
firstname.lastname@univ-st-etienne.fr
</email>
<sectionHeader confidence="0.984482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996286">
Computing pairwise word semantic simi-
larity is widely used and serves as a build-
ing block in many tasks in NLP. In this
paper, we explore the embedding of the
shortest-path metrics from a knowledge
base (Wordnet) into the Hamming hyper-
cube, in order to enhance the computa-
tion performance. We show that, although
an isometric embedding is untractable, it
is possible to achieve good non-isometric
embeddings. We report a speedup of
three orders of magnitude for the task of
computing Leacock and Chodorow (LCH)
similarity while keeping strong correla-
tions (r = .819, p = .826).
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99915853968254">
Among semantic relatedness measures, seman-
tic similarity encodes the conceptual distance be-
tween two units of language – this goes beyond
lexical ressemblance. When words are the speech
units, semantic similarity is at the very core of
many NLP problems. It has proven to be essen-
tial for word sense disambiguation (Mavroeidis et
al., 2005; Basile et al., 2014), open domain ques-
tion answering (Yih et al., 2014), and informa-
tion retrieval on the Web (Varelas et al., 2005),
to name a few. Two established strategies to es-
timate pairwise word semantic similarity includes
knowledge-based and distributional semantics.
Knowledge-based approaches exploit the struc-
ture of the taxonomy ((Leacock and Chodorow,
1998; Hirst and St-Onge, 1998; Wu and Palmer,
1994)), its content ((Banerjee and Pedersen,
2002)), or both (Resnik, 1995; Lin, 1998). In
the earliest applications, Wordnet-based semantic
similarity played a predominant role so that se-
mantic similarity measures reckon with informa-
tion from the lexical hierarchy. It therefore ignores
contextual information on word occurrences and
relies on humans to encode such hierarchies – a
tedious task in practice. In contrast, well-known
distributional semantics strategies encode seman-
tic similarity using the correlation of statistical ob-
servations on the occurrences of words in a textual
corpora (Lin, 1998).
While providing a significant impact on a
broad range of applications, (Herbelot and Gane-
salingam, 2013; Lazaridou et al., 2013; Beltagy
et al., 2014; Bernardi et al., 2013; Goyal et al.,
2013; Lebret et al., 2013), distributional semantics
– similarly to knowledge-based strategies – strug-
gle to process the ever-increasing size of textual
corpora in a reasonable amount of time. As an an-
swer, embedding high-dimensional distributional
semantics models for words into low-dimensional
spaces (henceforth word embedding (Collobert
and Weston, 2008)) has emerged as a popular
method. Word embedding utilizes deep learn-
ing to learn a real-valued vector representation of
words so that any vector distance – usually the
cosine similarity – encodes the word-to-word se-
mantic similarity. Although word embedding was
successfully applied for several NLP tasks (Her-
mann et al., 2014; Andreas and Klein, 2014; Clin-
chant and Perronnin, 2013; Xu et al., 2014; Li
and Liu, 2014; Goyal et al., 2013), it implies a
slow training phase – measured in days (Collobert
and Weston, 2008; Mnih and Kavukcuoglu, 2013;
Mikolov et al., 2013), though re-embedding words
seems promising (Labutov and Lipson, 2013).
There is another usually under-considered issue:
the tractability of the pairwise similarity computa-
tion in the vector space for large volume of data.
Despite these limitations, the current enthusiasm
for word embedding certainly echoes the need
for lightning fast word-to-word semantic similar-
ity computation.
In this context, it is surprising that embedding
semantic similarity of words in low dimensional
</bodyText>
<page confidence="0.914789">
8
</page>
<bodyText confidence="0.972301489361702">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 8–14,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
spaces for knowledge-based approaches is under-
studied. This oversight may well condemn the
word-to-word semantic similarity task to remain
corpus-dependant – i.e. ignoring the background
knowledge provided by a lexical hierarchy.
In this paper, we propose an embedding of
knowledge base semantic similarity based on the
shortest path metric (Leacock and Chodorow,
1998), into the Hamming hypercube of size n (the
size of targeted binary codes). The Leacock and
Chodorow semantic similarity is one of the most
meaningful measure. It yields the second rank
for highest correlation with the data collected by
(Miller and Charles, 1991), and the first one within
edge centric approaches, as shown by (Seco et al.,
2004). This method is only surpassed by the infor-
mation theoretic based similarity from (Jiang and
Conrath, 1997). A second study present similar
result (Budanitsky and Hirst, 2006), while a third
one ranks this similarity measure at the first rank
for precision in paraphrase identification (Mihal-
cea et al., 2006).
The hypercube embedding technique benefits
from the execution of Hamming distance within a
few cycles on modern CPUs. This allows the com-
putation of several millions distances per second.
Multi-index techniques allows the very fast com-
putation of top-k queries (Norouzi et al., 2012) on
the Hamming space. However, the dimension of
the hypercube (i.e. the number of bits used to
represent an element) should obey the threshold
of few CPU words (64, 128 ..., bits) to maintain
such efficiency (Heo et al., 2012).
An isometric embedding requires a excessively
high number of dimensions to be feasible. How-
ever, in this paper we show that practical em-
beddings exist and present a method to construct
them. The best embedding presents very strong
correlations (r = .819, p = .829) with the Lea-
cock &amp; Chodorow similarity measure (LCH in the
rest of this paper). Our experiments against the
state-of-the art implementation including caching
techniques show that performance is increased by
up to three orders of magnitude.
</bodyText>
<sectionHeader confidence="0.78643" genericHeader="method">
2 Shortest path metric embedding
</sectionHeader>
<bodyText confidence="0.999581418604651">
Let us first introduce few notations. We denote Hn2
as an n-dimensional hypercube whose nodes are
labeled by the 2n binary n-tuples. The nodes are
adjacent if and only if their corresponding n-tuples
differ in exactly one position, i.e. their Hamming
distance (Ei) is equal to one. In what follows, Qn
denotes the metric space composed of Hn2 with E1.
We tackle the following problem: We aim
at defining a function f that maps every node
w of the taxonomy (Wordnet for Leacock &amp;
Chodorow) into Qn so that for every pair of nodes:
∀(wi, wj), d(wi, wj) = A · E1(f(wi), f(wj)),
where A is a scalar. For practical purposes, the
construction of the mapping should also be rea-
sonable in terms of time complexity.
Theoretical limitations Wordnet with its hyper-
nym relation forms a partially ordered set (poset).
The first approach is to perform an isometric em-
bedding from the poset with shortest path distance
into the Hamming hypercube. Such a mapping
would exactly preserve the original distance in the
embedding. As proven by (Deza and Laurent,
1997), poset lattices, with their shortest path met-
ric, can be isometrically embedded into the hyper-
cube, but the embedding requires 2n dimensions.
The resulting embedding would not fit in the mem-
ory of any existing computer, for a lattice having
more than 60 nodes. Using Wordnet, with tens of
thousands synsets, this embedding is untractable.
The bound given by Deza et al. is not tight, how-
ever it would require a more than severe improve-
ment to be of any practical interest.
Tree embedding To reduce the dimensionality,
we weaken the lattice into a tree. We build a
tree from the Wordnet’s Hyponyms/Hypernyms
poset by cutting 1,300 links, which correspond to
roughly one percent of the edges in the original lat-
tice. The nature of the cut to be performed can be
subject to discussion. In this preliminary research,
we used a simple approach. Since hypernyms are
ordered, we decided to preserve only the first hy-
pernym – semantically more relevant, or at least
statistically – and to cut edges to other hypernyms.
</bodyText>
<figureCaption confidence="0.986663">
Figure 1: Construction of isometric embedding on
a sample tree. For this six nodes tree, the embed-
ding requires five bits.
</figureCaption>
<figure confidence="0.999366142857143">
000
00000
A
B C F
D E
100 010 001
B C F
D E
A
10000 01000 00100
B C F
D E
A
01010 01001
</figure>
<page confidence="0.97493">
9
</page>
<bodyText confidence="0.99989527027027">
Our experiments in Table 1 shows that using the
obtained tree instead of the lattice keeps a high
correlation (r = .919, p = .931) with the origi-
nal LCH distance, thus validating the approach.
(Wilkeit, 1990) showed that any k-ary tree of
size n can be embedded into Qn−1. We give an
isometric embedding algorithm, which is linear
in time and space, exhibiting a much better time
complexity than Winkler’s generic approach for
graphs, running in O(n5) (Winkler, 1984). Start-
ing with an empty binary signature, the algorithm
is the following: at each step of a depth-first pre-
order traversal: if the node has k children, we set
the signature for the i-th child by appending k ze-
roes to the parent’s signature and by setting the i-th
of the k bits to one. An example is given in Figure
1. However, when using real-world datasets such
as Wordnet, the embedding still requires several
thousands of bits to represent a node. This dimen-
sion reduction to tens of kilobits per node remains
far from our goal of several CPU words, and calls
for a task-specific approach.
Looking at the construction of the isometric em-
bedding, the large dimension results from the ap-
pending of bits to all nodes in the tree. This results
in a large number of bits that are rarely set to one.
At the opposite, the optimal embedding in terms
of dimension is given by the approach of (Chen
and Stallmann, 1995) that assigns gray codes to
each node. However, the embedding is not isomet-
ric and introduces a very large error. As shown in
Table 1, this approach gives the most compact em-
bedding with Flo92(87,000)] = 17 bits, but leads
to poor correlations (r = .235 and p = .186).
An exhaustive search is also out of reach: for
a fixed dimension n and r nodes in the tree, the
number of combinations C is given by:
</bodyText>
<equation confidence="0.9189235">
C = (2n)!
(n − r)!
</equation>
<bodyText confidence="0.999691333333333">
Even with the smallest value of n = 17 and r =
87,000, we have C &gt; 1010,000. With n = 64, to
align to a CPU word, C &gt; 10
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="method">
3 Non-isometric Embedding
</sectionHeader>
<bodyText confidence="0.999963714285714">
Our approach is a trade-off between the isomet-
ric embedding and the pre-order gray code solu-
tion. When designing our algorithm, we had to
decide which tree distance we will preserve, either
between parent and children, or among siblings.
Therefore, we take into account the nature of the
tree that we aim to embed into the hypercube. Let
</bodyText>
<figure confidence="0.630327">
Isometric Pre-order Gray Code
RMSE=.66, r=-0.07, ρ=-0.12
</figure>
<figureCaption confidence="0.989591">
Figure 2: Approaches to reduce the tree embed-
ding dimensions.
</figureCaption>
<bodyText confidence="0.999576277777778">
first analyse the characteristics of the tree obtained
from the cut. The tree has an average branching
factor of 4.9, with a standard deviation of 14 and
96% of the nodes have a branching factor lesser
than 20. At the opposite, the depth is very stable
with an average of 8.5, a standard deviation of 2,
and a maximum of 18. Consequently, we decide
to preserve the parent-children distance over the
very unstable siblings distance. To lower the di-
mensions, we aim at allocating less than k bits for
a node with k children, thus avoiding the signature
extension taking place for every node in the iso-
metric approach. Our approach uses the following
principles.
Branch inheritance: each node inherits the
signature from its father, but contrary to isometric
embedding, the signature extension does not ap-
ply to all the nodes in the tree. This guarantees the
compactness of the structure.
Parentship preservation: when allocating less
bits than required for the isometric embedding,
we introduce an error. Our allocation favours as
much as possible the parentship distance at the
expense of the sibling distance. As a first allo-
cation, for a node with k children, we allocate
Flo92(k + 1)] bits for the signatures, in order to
guarantee the unicity of the signature. Each child
node is assigned a signature extension using a
gray code generation on the Flo92(k + 1)] bits.
The parent node simply extends its signature with
Flo92(k + 1)] zeroes, which is much more com-
pact than the k bits from the isometric embedding
algorithm.
Word alignment: The two previous techniques
give a compact embedding for low-depth trees,
which is the case of Wordnet. The dimension D
</bodyText>
<figure confidence="0.998090266666667">
00000 000
00001 00010 00100 01000 10000 001 011 010 110 101
B C D E F
A
B C D E F
A
000 0000
001 010 100 101 011 0001 0010 0100 1000 1001
Value sorting Additional bit and sorting
RMSE=.6, r=.19, ρ=.16 RMSE=.33, r=.55, ρ=-.57
B C D E F
A
B C D E F
A
100,000.
</figure>
<page confidence="0.953221">
10
</page>
<bodyText confidence="0.999098393939394">
of the embedding is not necessarily aligned to
a CPU word size W: kW &lt; D &lt; (k + 1)W.
We want to exploit the potential (k + 1)W − D
bits that are unused but still processed by the
CPU. For this purpose we rank the nodes along
a value v(i), i E N to decide which nodes are
allowed to use extra bits. Since our approach
favours parent/child distance, we want to allow
additional bits for nodes that are both close to
the root and the head of a large branch. To bal-
ance the two values, we use the following formula:
v(i) = (maxdepth − depth(i)) · log(sizebranch(i))
We therefore enable our approach to take full
advantage of the otherwise unused bits.
In order to enhance the quality of the embed-
ding, we also introduce two potential optimiza-
tions:
The first is called Children-sorting: we allocate
a better preserving signature to children having
larger descents. A better signature is among the
available the 2Flo92(k+1)1 available, the one that re-
duces the error with the parent node. We rank the
children by the size of their descent and assign the
signatures accordingly.
The second optimization is named Value-
sorting and is depicted in Figure 2. Among the
2Flo92(k+1)1 available signatures, only k + 1 will
be assigned (one for the parent and k for the chil-
dren). For instance in the case of 5 children as
depicted in Figure 2, we allocate 3 bits for 6 signa-
tures. We favor the parentship distance by select-
ing first the signatures where one bit differs from
the parent’s one.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999883266666667">
In this section, we run two experiments to eval-
uate both the soundness and the performance of
our approach. In the first experiment, we test the
quality of our embedding against the tree distance
and the LCH similarity. The goal is to assess the
soundness of our approach and to measure the cor-
relation between the approximate embedding and
the original LCH similarity.
In the second experiment we compare the com-
putational performance of our approach against an
optimized in-memory library that implements the
LCH similarity.
Our algorithm called FSE for Fast Similarity
Embedding, is implemented in Java and avail-
able publicly1. Our testbed is an Intel Xeon E3
</bodyText>
<footnote confidence="0.793319">
1Source code, binaries and instructions to reproduce
</footnote>
<figure confidence="0.965586">
80 90 100 110 120 130
Embedding dimension
</figure>
<figureCaption confidence="0.996040166666667">
Figure 3: FSE: influence of optimizations and di-
mensions on the correlation over the tree distance
on Wordnet.
1246v3 with 16GB of memory, a 256Go PCI Ex-
press SSD. The system runs a 64-bit Linux 3.13.0
kernel with Oracle’s JDK 7u67.
</figureCaption>
<bodyText confidence="0.999958666666667">
The FSE algorithm is implemented in various
flavours. FSE-Base denotes the basic algorithm,
containing none of the optimizations detailed in
the previous section. FSE-Base can be aug-
mented with either or both of the optimizations.
This latter version is denoted FSE-Best.
</bodyText>
<subsectionHeader confidence="0.985277">
4.1 Embedding
</subsectionHeader>
<bodyText confidence="0.999960263157895">
We first measure the correlation of the embedded
distance with the original tree distance, to validate
the approach and to determine the gain induced by
the optimizations. Figure 3 shows the influence
of dimensions and optimizations on the Pearson’s
product moment correlation r. The base version
reaches r = .77 for an embedding of dimension
128. Regarding the optimizations, children sort-
ing is more efficient than value sorting, excepted
for dimensions under 90. Finally, combined opti-
mizations (FSE-Best) exhibit a higher correlation
(r = .89) than the other versions.
We then measure the correlation with the Lea-
cock &amp; Chodorow similarity measure. We com-
pare our approach to the gray codes embedding
from (Chen and Stallmann, 1995) as well as the
isometric embedding. We compute the correlation
on 5 millions distances from the Wordnet-Core
noun pairs2 (Table 1). As expected, the embed-
</bodyText>
<footnote confidence="0.95336675">
the experiments are available at http://demo-satin.
telecom-st-etienne.fr/FSE/
2https://wordnet.princeton.edu/
wordnet/download/standoff/
</footnote>
<figure confidence="0.99155325">
Combined
Base + value sorting
Base + children sorting
FSE-Base
Pearson’s r 0.9
0.85
0.8
0.75
</figure>
<page confidence="0.99267">
11
</page>
<table confidence="0.9997118">
Embedding Bits Pearson’s r Spearman’s ρ
Chen et al. 17 .235 .186
FSE-Base 84 .699 .707
FSE-Best 128 .819 .829
Isometric 84K .919 .931
</table>
<tableCaption confidence="0.706978666666667">
Table 1: Correlations between LCH, isometric em-
bedding, and FSE for all distances on all Wordnet-
Core noun pairs (p-values &lt; 10−14).
</tableCaption>
<table confidence="0.9999174">
Algorithm Measure Amount of pairs (n)
103 104 105 106 107
WS4J 103· ms 0.156 1.196 11.32 123.89 1,129.3
FSE-Best ms 0.04 0.59 14.15 150.58 1,482
speedup x3900 x2027 x800 x822 x762
</table>
<tableCaption confidence="0.9309065">
Table 2: Running time in milliseconds for pairwise
similarity computations.
</tableCaption>
<bodyText confidence="0.9888756">
ding obtained using gray codes present a very low
correlation with the original distance.
Similarly to the results obtained on the tree dis-
tance correlation, FSE-Best exhibits the highest
scores with r = .819 and ρ = .829, not far from
the theoretical bound of r = .919 and ρ = .931
for the isometric embedding of the same tree. Our
approach requires 650 times less bits than the iso-
metric one, while keeping strong guarantees on the
correlation with the original LCH distance.
</bodyText>
<subsectionHeader confidence="0.990177">
4.2 Speedup
</subsectionHeader>
<bodyText confidence="0.999935055555556">
Table 4.2 presents the computation time of the
LCH similarity. This is computed using WS4J3, an
efficient library that enables in-memory caching.
Because of the respective computational com-
plexities of the Hamming distance and the shortest
path algorithms, FSE unsurprisingly boosts LCH
similarity computation by orders of magnitudes.
When the similarity is computed on a small num-
ber of pairs (a situation of the utmost practical in-
terest), the factor of improvement is three orders
of magnitude. This factor decreases to an amount
of 800 times for very large scale applications. The
reason of the decrease is that WS4J caching mech-
anism becomes more efficient for larger numbers
of comparisons. As the caching system stores
shortest path between nodes, these computed val-
ues are more likely to be a subpath of another
query when the number of queries grows.
</bodyText>
<footnote confidence="0.857664">
3https://code.google.com/p/ws4j/
</footnote>
<sectionHeader confidence="0.97852" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991375">
We proposed in this paper a novel approach based
on metric embedding to boost the computation of
shortest-path based similarity measures such as
the one of Leacock &amp; Chodorow. We showed that
an isometric embedding of the Wordnet’s hyper-
nym/hyponym lattice does not lead to a practical
solution. To tackle this issue, we weaken the lat-
tice structure into a tree by cutting less relevant
edges. We then devised an algorithm and several
optimizations to embed the tree shortest-path dis-
tance in a word-aligned number of bits. Such an
embedding can be used to boost NLP core algo-
rithms – this was demonstrated here on the com-
putation of LCH for which our approach offers a
factor of improvement of three orders of magni-
tude, with a very strong correlation.
</bodyText>
<sectionHeader confidence="0.984456" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996602">
This work is supported by the OpenCloudware
project. OpenCloudware is funded by the French
FSN (Fond national pour la Soci´et´e Num´erique),
and is supported by Pˆoles Minalogic, Systematic
and SCS.
</bodyText>
<sectionHeader confidence="0.988121" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.964248269230769">
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax. In Associa-
tion for Computational Linguistics (ACL).
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Computational linguis-
tics and intelligent text processing, pages 136–145.
Springer.
Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An enhanced lesk word sense dis-
ambiguation algorithm through a distributional se-
mantic model. In Proceedings of COLING, pages
1591–1600.
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Semantic parsing using distributional seman-
tics and probabilistic logic. Association for Compu-
tational Linguistics (ACL), page 7.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Association for Computa-
tional Linguistics (ACL), pages 53–57.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
</reference>
<page confidence="0.994851">
12
</page>
<reference confidence="0.928346648148148">
Woei-Kae Chen and Matthias FM Stallmann. 1995.
On embedding binary trees into hypercubes.
Journal of Parallel and Distributed Computing,
24(2):132–138.
St´ephane Clinchant and Florent Perronnin. 2013. Ag-
gregating continuous word embeddings for informa-
tion retrieval. Association for Computational Lin-
guistics (ACL).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
M. Deza and M. Laurent. 1997. Geometry of Cuts and
Metrics. Springer, 588 pages.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard H
Hovy. 2013. A structured distributional semantic
model for event co-reference. In Association for
Computational Linguistics (ACL), pages 467–473.
Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu
Chang, and Sung-Eui Yoon. 2012. Spherical hash-
ing. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 2957–
2964. IEEE.
Aur´elie Herbelot and Mohan Ganesalingam. 2013.
Measuring semantic content in distributional vec-
tors. In Association for Computational Linguistics
(ACL), pages 440–445.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Association for Computational Linguistics (ACL).
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. WordNet: An
electronic lexical database, 305:305–332.
Jay J Jiang and David W Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. Proceedings of the 10th Research on Com-
putational Linguistics International Conference.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In ACL (2), pages 489–493.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In Association
for Computational Linguistics (ACL), pages 1517–
1526.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
R´emi Lebret, Jo¨el Legrand, and Ronan Collobert.
2013. Is Deep Learning Really Necessary for Word
Embeddings? Technical report, Idiap.
Chen Li and Yang Liu. 2014. Improving text normal-
ization via unsupervised model and discriminative
reranking. Association for Computational Linguis-
tics (ACL), page 86.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In ICML, volume 98, pages 296–
304.
Dimitrios Mavroeidis, George Tsatsaronis, Michalis
Vazirgiannis, Martin Theobald, and Gerhard
Weikum. 2005. Word sense disambiguation for
exploiting hierarchical thesauri in text classification.
In Knowledge Discovery in Databases: PKDD
2005, pages 181–192. Springer.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775–780.
Tomas Mikolov, Kai Chenand, Greg Corradoand, and
Jeffrey Dean. 2013. Efficient estimation of word
representations in vector space. In Proceedings of
the International Conference on Learning Represen-
tations.
George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
cognitive processes, 6(1):1–28.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 2265–2273. Curran Associates, Inc.
Mohammad Norouzi, Ali Punjani, and David J Fleet.
2012. Fast search in hamming space with multi-
index hashing. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on,
pages 3108–3115. IEEE.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Confer-
ence on Artificial Intelligence - Volume 1, IJCAI’95,
pages 448–453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Nuno Seco, Tony Veale, and Jer Hayes. 2004. An in-
trinsic information content metric for semantic sim-
ilarity in wordnet. In ECAI, volume 16, page 1089.
Giannis Varelas, Epimenidis Voutsakis, Paraskevi
Raftopoulou, Euripides GM Petrakis, and Evange-
los E Milios. 2005. Semantic similarity methods
in wordnet and their application to information re-
trieval on the web. In Proceedings of the 7th an-
nual ACM international workshop on Web informa-
tion and data management, pages 10–16. ACM.
</reference>
<page confidence="0.988161">
13
</page>
<reference confidence="0.999649388888889">
Elke Wilkeit. 1990. Isometric embeddings in ham-
ming graphs. Journal of Combinatorial Theory, Se-
ries B, 50(2):179–197.
Peter M Winkler. 1984. Isometric embedding in prod-
ucts of complete graphs. Discrete Applied Mathe-
matics, 7(2):221–225.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133–138. Association for Com-
putational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao. 2014.
Product feature mining: Semantic clues versus syn-
tactic constituents. In Association for Computa-
tional Linguistics (ACL), pages 336–346.
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.
</reference>
<page confidence="0.999272">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.433360">
<title confidence="0.996948">On metric embedding for boosting semantic similarity computations</title>
<author confidence="0.966788">Julien Subercaze</author>
<author confidence="0.966788">Christophe Gravier</author>
<author confidence="0.966788">Frederique</author>
<affiliation confidence="0.81613">Universit´e de Lyon, F-42023, Saint-Etienne,</affiliation>
<address confidence="0.849323">CNRS, UMR5516, Laboratoire Hubert Curien, F-42000, Saint-Etienne, Universit´e de Saint-Etienne, Jean Monnet, F-42000, Saint-Etienne,</address>
<email confidence="0.995934">firstname.lastname@univ-st-etienne.fr</email>
<abstract confidence="0.999676266666667">Computing pairwise word semantic similarity is widely used and serves as a building block in many tasks in NLP. In this paper, we explore the embedding of the shortest-path metrics from a knowledge base (Wordnet) into the Hamming hypercube, in order to enhance the computation performance. We show that, although an isometric embedding is untractable, it is possible to achieve good non-isometric embeddings. We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correla-</abstract>
<intro confidence="0.616806">p</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3226" citStr="Andreas and Klein, 2014" startWordPosition="476" endWordPosition="479">tegies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Computational linguistics and intelligent text processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1769" citStr="Banerjee and Pedersen, 2002" startWordPosition="258" endWordPosition="261">the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a </context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Computational linguistics and intelligent text processing, pages 136–145. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Annalina Caputo</author>
<author>Giovanni Semeraro</author>
</authors>
<title>An enhanced lesk word sense disambiguation algorithm through a distributional semantic model.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1591--1600</pages>
<contexts>
<context position="1327" citStr="Basile et al., 2014" startWordPosition="192" endWordPosition="195">g is untractable, it is possible to achieve good non-isometric embeddings. We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correlations (r = .819, p = .826). 1 Introduction Among semantic relatedness measures, semantic similarity encodes the conceptual distance between two units of language – this goes beyond lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity me</context>
</contexts>
<marker>Basile, Caputo, Semeraro, 2014</marker>
<rawString>Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2014. An enhanced lesk word sense disambiguation algorithm through a distributional semantic model. In Proceedings of COLING, pages 1591–1600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Semantic parsing using distributional semantics and probabilistic logic. Association for Computational Linguistics (ACL),</title>
<date>2014</date>
<pages>7</pages>
<contexts>
<context position="2476" citStr="Beltagy et al., 2014" startWordPosition="363" endWordPosition="366">tic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbelot and Ganesalingam, 2013; Lazaridou et al., 2013; Beltagy et al., 2014; Bernardi et al., 2013; Goyal et al., 2013; Lebret et al., 2013), distributional semantics – similarly to knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the w</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond Mooney. 2014. Semantic parsing using distributional semantics and probabilistic logic. Association for Computational Linguistics (ACL), page 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raffaella Bernardi</author>
<author>Georgiana Dinu</author>
<author>Marco Marelli</author>
<author>Marco Baroni</author>
</authors>
<title>A relatedness benchmark to test the role of determiners in compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>53--57</pages>
<contexts>
<context position="2499" citStr="Bernardi et al., 2013" startWordPosition="367" endWordPosition="370">a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbelot and Ganesalingam, 2013; Lazaridou et al., 2013; Beltagy et al., 2014; Bernardi et al., 2013; Goyal et al., 2013; Lebret et al., 2013), distributional semantics – similarly to knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic si</context>
</contexts>
<marker>Bernardi, Dinu, Marelli, Baroni, 2013</marker>
<rawString>Raffaella Bernardi, Georgiana Dinu, Marco Marelli, and Marco Baroni. 2013. A relatedness benchmark to test the role of determiners in compositional distributional semantics. In Association for Computational Linguistics (ACL), pages 53–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="5091" citStr="Budanitsky and Hirst, 2006" startWordPosition="758" endWordPosition="761">ding of knowledge base semantic similarity based on the shortest path metric (Leacock and Chodorow, 1998), into the Hamming hypercube of size n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by (Miller and Charles, 1991), and the first one within edge centric approaches, as shown by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techniques allows the very fast computation of top-k queries (Norouzi et al., 2012) on the Hamming space. However, the dimension of the hypercube (i.e. the number of bits used to represent an element) should obey the threshold of few CPU words (64, 128 ..., bits) </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Woei-Kae Chen</author>
<author>Matthias FM Stallmann</author>
</authors>
<title>On embedding binary trees into hypercubes.</title>
<date>1995</date>
<journal>Journal of Parallel and Distributed Computing,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="9930" citStr="Chen and Stallmann, 1995" startWordPosition="1600" endWordPosition="1603"> example is given in Figure 1. However, when using real-world datasets such as Wordnet, the embedding still requires several thousands of bits to represent a node. This dimension reduction to tens of kilobits per node remains far from our goal of several CPU words, and calls for a task-specific approach. Looking at the construction of the isometric embedding, the large dimension results from the appending of bits to all nodes in the tree. This results in a large number of bits that are rarely set to one. At the opposite, the optimal embedding in terms of dimension is given by the approach of (Chen and Stallmann, 1995) that assigns gray codes to each node. However, the embedding is not isometric and introduces a very large error. As shown in Table 1, this approach gives the most compact embedding with Flo92(87,000)] = 17 bits, but leads to poor correlations (r = .235 and p = .186). An exhaustive search is also out of reach: for a fixed dimension n and r nodes in the tree, the number of combinations C is given by: C = (2n)! (n − r)! Even with the smallest value of n = 17 and r = 87,000, we have C &gt; 1010,000. With n = 64, to align to a CPU word, C &gt; 10 3 Non-isometric Embedding Our approach is a trade-off bet</context>
<context position="16384" citStr="Chen and Stallmann, 1995" startWordPosition="2731" endWordPosition="2734">oach and to determine the gain induced by the optimizations. Figure 3 shows the influence of dimensions and optimizations on the Pearson’s product moment correlation r. The base version reaches r = .77 for an embedding of dimension 128. Regarding the optimizations, children sorting is more efficient than value sorting, excepted for dimensions under 90. Finally, combined optimizations (FSE-Best) exhibit a higher correlation (r = .89) than the other versions. We then measure the correlation with the Leacock &amp; Chodorow similarity measure. We compare our approach to the gray codes embedding from (Chen and Stallmann, 1995) as well as the isometric embedding. We compute the correlation on 5 millions distances from the Wordnet-Core noun pairs2 (Table 1). As expected, the embedthe experiments are available at http://demo-satin. telecom-st-etienne.fr/FSE/ 2https://wordnet.princeton.edu/ wordnet/download/standoff/ Combined Base + value sorting Base + children sorting FSE-Base Pearson’s r 0.9 0.85 0.8 0.75 11 Embedding Bits Pearson’s r Spearman’s ρ Chen et al. 17 .235 .186 FSE-Base 84 .699 .707 FSE-Best 128 .819 .829 Isometric 84K .919 .931 Table 1: Correlations between LCH, isometric embedding, and FSE for all dista</context>
</contexts>
<marker>Chen, Stallmann, 1995</marker>
<rawString>Woei-Kae Chen and Matthias FM Stallmann. 1995. On embedding binary trees into hypercubes. Journal of Parallel and Distributed Computing, 24(2):132–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Clinchant</author>
<author>Florent Perronnin</author>
</authors>
<title>Aggregating continuous word embeddings for information retrieval. Association for Computational Linguistics (ACL).</title>
<date>2013</date>
<contexts>
<context position="3257" citStr="Clinchant and Perronnin, 2013" startWordPosition="480" endWordPosition="484">ess the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation. In this context, it is surpri</context>
</contexts>
<marker>Clinchant, Perronnin, 2013</marker>
<rawString>St´ephane Clinchant and Florent Perronnin. 2013. Aggregating continuous word embeddings for information retrieval. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2874" citStr="Collobert and Weston, 2008" startWordPosition="420" endWordPosition="423">ical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbelot and Ganesalingam, 2013; Lazaridou et al., 2013; Beltagy et al., 2014; Bernardi et al., 2013; Goyal et al., 2013; Lebret et al., 2013), distributional semantics – similarly to knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words s</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Deza</author>
<author>M Laurent</author>
</authors>
<date>1997</date>
<journal>Geometry of Cuts and Metrics. Springer,</journal>
<volume>588</volume>
<pages>pages.</pages>
<contexts>
<context position="7324" citStr="Deza and Laurent, 1997" startWordPosition="1131" endWordPosition="1134">ps every node w of the taxonomy (Wordnet for Leacock &amp; Chodorow) into Qn so that for every pair of nodes: ∀(wi, wj), d(wi, wj) = A · E1(f(wi), f(wj)), where A is a scalar. For practical purposes, the construction of the mapping should also be reasonable in terms of time complexity. Theoretical limitations Wordnet with its hypernym relation forms a partially ordered set (poset). The first approach is to perform an isometric embedding from the poset with shortest path distance into the Hamming hypercube. Such a mapping would exactly preserve the original distance in the embedding. As proven by (Deza and Laurent, 1997), poset lattices, with their shortest path metric, can be isometrically embedded into the hypercube, but the embedding requires 2n dimensions. The resulting embedding would not fit in the memory of any existing computer, for a lattice having more than 60 nodes. Using Wordnet, with tens of thousands synsets, this embedding is untractable. The bound given by Deza et al. is not tight, however it would require a more than severe improvement to be of any practical interest. Tree embedding To reduce the dimensionality, we weaken the lattice into a tree. We build a tree from the Wordnet’s Hyponyms/Hy</context>
</contexts>
<marker>Deza, Laurent, 1997</marker>
<rawString>M. Deza and M. Laurent. 1997. Geometry of Cuts and Metrics. Springer, 588 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kartik Goyal</author>
<author>Sujay Kumar Jauhar</author>
<author>Huiying Li</author>
<author>Mrinmaya Sachan</author>
<author>Shashank Srivastava</author>
<author>Eduard H Hovy</author>
</authors>
<title>A structured distributional semantic model for event co-reference.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>467--473</pages>
<contexts>
<context position="2519" citStr="Goyal et al., 2013" startWordPosition="371" endWordPosition="374">hat semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbelot and Ganesalingam, 2013; Lazaridou et al., 2013; Beltagy et al., 2014; Bernardi et al., 2013; Goyal et al., 2013; Lebret et al., 2013), distributional semantics – similarly to knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although w</context>
</contexts>
<marker>Goyal, Jauhar, Li, Sachan, Srivastava, Hovy, 2013</marker>
<rawString>Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard H Hovy. 2013. A structured distributional semantic model for event co-reference. In Association for Computational Linguistics (ACL), pages 467–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jae-Pil Heo</author>
<author>Youngwoon Lee</author>
<author>Junfeng He</author>
<author>Shih-Fu Chang</author>
<author>Sung-Eui Yoon</author>
</authors>
<title>Spherical hashing.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,</booktitle>
<pages>2957--2964</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5737" citStr="Heo et al., 2012" startWordPosition="864" endWordPosition="867">s similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techniques allows the very fast computation of top-k queries (Norouzi et al., 2012) on the Hamming space. However, the dimension of the hypercube (i.e. the number of bits used to represent an element) should obey the threshold of few CPU words (64, 128 ..., bits) to maintain such efficiency (Heo et al., 2012). An isometric embedding requires a excessively high number of dimensions to be feasible. However, in this paper we show that practical embeddings exist and present a method to construct them. The best embedding presents very strong correlations (r = .819, p = .829) with the Leacock &amp; Chodorow similarity measure (LCH in the rest of this paper). Our experiments against the state-of-the art implementation including caching techniques show that performance is increased by up to three orders of magnitude. 2 Shortest path metric embedding Let us first introduce few notations. We denote Hn2 as an n-</context>
</contexts>
<marker>Heo, Lee, He, Chang, Yoon, 2012</marker>
<rawString>Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu Chang, and Sung-Eui Yoon. 2012. Spherical hashing. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2957– 2964. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elie Herbelot</author>
<author>Mohan Ganesalingam</author>
</authors>
<title>Measuring semantic content in distributional vectors.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>440--445</pages>
<contexts>
<context position="2430" citStr="Herbelot and Ganesalingam, 2013" startWordPosition="354" endWordPosition="358"> 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbelot and Ganesalingam, 2013; Lazaridou et al., 2013; Beltagy et al., 2014; Bernardi et al., 2013; Goyal et al., 2013; Lebret et al., 2013), distributional semantics – similarly to knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance –</context>
</contexts>
<marker>Herbelot, Ganesalingam, 2013</marker>
<rawString>Aur´elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In Association for Computational Linguistics (ACL), pages 440–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic frame identification with distributed word representations.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3201" citStr="Hermann et al., 2014" startWordPosition="471" endWordPosition="475">o knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semant</context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic frame identification with distributed word representations. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>305--305</pages>
<contexts>
<context position="1702" citStr="Hirst and St-Onge, 1998" startWordPosition="248" endWordPosition="251">guage – this goes beyond lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a text</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database, 305:305–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>Proceedings of the 10th Research on Computational Linguistics International Conference.</booktitle>
<contexts>
<context position="5023" citStr="Jiang and Conrath, 1997" startWordPosition="748" endWordPosition="751">ovided by a lexical hierarchy. In this paper, we propose an embedding of knowledge base semantic similarity based on the shortest path metric (Leacock and Chodorow, 1998), into the Hamming hypercube of size n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by (Miller and Charles, 1991), and the first one within edge centric approaches, as shown by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techniques allows the very fast computation of top-k queries (Norouzi et al., 2012) on the Hamming space. However, the dimension of the hypercube (i.e. the number of bits used to represent an elem</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J Jiang and David W Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of the 10th Research on Computational Linguistics International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Re-embedding words.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>489--493</pages>
<contexts>
<context position="3515" citStr="Labutov and Lipson, 2013" startWordPosition="524" endWordPosition="527"> a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation. In this context, it is surprising that embedding semantic similarity of words in low dimensional 8 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 8–14</context>
</contexts>
<marker>Labutov, Lipson, 2013</marker>
<rawString>Igor Labutov and Hod Lipson. 2013. Re-embedding words. In ACL (2), pages 489–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Marco Marelli</author>
<author>Roberto Zamparelli</author>
<author>Marco Baroni</author>
</authors>
<title>Compositional-ly derived representations of morphologically complex words in distributional semantics.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1517--1526</pages>
<contexts>
<context position="2454" citStr="Lazaridou et al., 2013" startWordPosition="359" endWordPosition="362">ons, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbelot and Ganesalingam, 2013; Lazaridou et al., 2013; Beltagy et al., 2014; Bernardi et al., 2013; Goyal et al., 2013; Lebret et al., 2013), distributional semantics – similarly to knowledge-based strategies – struggle to process the ever-increasing size of textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine simi</context>
</contexts>
<marker>Lazaridou, Marelli, Zamparelli, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositional-ly derived representations of morphologically complex words in distributional semantics. In Association for Computational Linguistics (ACL), pages 1517– 1526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="1677" citStr="Leacock and Chodorow, 1998" startWordPosition="244" endWordPosition="247">nce between two units of language – this goes beyond lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occur</context>
<context position="4569" citStr="Leacock and Chodorow, 1998" startWordPosition="673" endWordPosition="676">the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 8–14, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics spaces for knowledge-based approaches is understudied. This oversight may well condemn the word-to-word semantic similarity task to remain corpus-dependant – i.e. ignoring the background knowledge provided by a lexical hierarchy. In this paper, we propose an embedding of knowledge base semantic similarity based on the shortest path metric (Leacock and Chodorow, 1998), into the Hamming hypercube of size n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by (Miller and Charles, 1991), and the first one within edge centric approaches, as shown by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precis</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Jo¨el Legrand</author>
<author>Ronan Collobert</author>
</authors>
<title>Is Deep Learning Really Necessary for Word Embeddings?</title>
<date>2013</date>
<tech>Technical report, Idiap.</tech>
<marker>Lebret, Jo¨el Legrand, Collobert, 2013</marker>
<rawString>R´emi Lebret, Jo¨el Legrand, and Ronan Collobert. 2013. Is Deep Learning Really Necessary for Word Embeddings? Technical report, Idiap.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Yang Liu</author>
</authors>
<title>Improving text normalization via unsupervised model and discriminative reranking. Association for Computational Linguistics (ACL),</title>
<date>2014</date>
<pages>86</pages>
<contexts>
<context position="3292" citStr="Li and Liu, 2014" startWordPosition="489" endWordPosition="492">in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation. In this context, it is surprising that embedding semantic simila</context>
</contexts>
<marker>Li, Liu, 2014</marker>
<rawString>Chen Li and Yang Liu. 2014. Improving text normalization via unsupervised model and discriminative reranking. Association for Computational Linguistics (ACL), page 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In ICML,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="1805" citStr="Lin, 1998" startWordPosition="266" endWordPosition="267">ore of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applications, (Herbel</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML, volume 98, pages 296– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Mavroeidis</author>
<author>George Tsatsaronis</author>
<author>Michalis Vazirgiannis</author>
<author>Martin Theobald</author>
<author>Gerhard Weikum</author>
</authors>
<title>Word sense disambiguation for exploiting hierarchical thesauri in text classification.</title>
<date>2005</date>
<booktitle>In Knowledge Discovery in Databases: PKDD</booktitle>
<pages>181--192</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1305" citStr="Mavroeidis et al., 2005" startWordPosition="188" endWordPosition="191">ugh an isometric embedding is untractable, it is possible to achieve good non-isometric embeddings. We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correlations (r = .819, p = .826). 1 Introduction Among semantic relatedness measures, semantic similarity encodes the conceptual distance between two units of language – this goes beyond lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that </context>
</contexts>
<marker>Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald, Weikum, 2005</marker>
<rawString>Dimitrios Mavroeidis, George Tsatsaronis, Michalis Vazirgiannis, Martin Theobald, and Gerhard Weikum. 2005. Word sense disambiguation for exploiting hierarchical thesauri in text classification. In Knowledge Discovery in Databases: PKDD 2005, pages 181–192. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="5225" citStr="Mihalcea et al., 2006" startWordPosition="779" endWordPosition="783">ze n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by (Miller and Charles, 1991), and the first one within edge centric approaches, as shown by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techniques allows the very fast computation of top-k queries (Norouzi et al., 2012) on the Hamming space. However, the dimension of the hypercube (i.e. the number of bits used to represent an element) should obey the threshold of few CPU words (64, 128 ..., bits) to maintain such efficiency (Heo et al., 2012). An isometric embedding requires a excessively high number of dimensions to be feasible</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI, volume 6, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chenand</author>
<author>Greg Corradoand</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Learning Representations.</booktitle>
<contexts>
<context position="3445" citStr="Mikolov et al., 2013" startWordPosition="515" endWordPosition="518">eforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation. In this context, it is surprising that embedding semantic similarity of words in low dimensional 8 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Join</context>
</contexts>
<marker>Mikolov, Chenand, Corradoand, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chenand, Greg Corradoand, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of the International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity. Language and cognitive processes,</title>
<date>1991</date>
<pages>6--1</pages>
<contexts>
<context position="4832" citStr="Miller and Charles, 1991" startWordPosition="716" endWordPosition="719">for knowledge-based approaches is understudied. This oversight may well condemn the word-to-word semantic similarity task to remain corpus-dependant – i.e. ignoring the background knowledge provided by a lexical hierarchy. In this paper, we propose an embedding of knowledge base semantic similarity based on the shortest path metric (Leacock and Chodorow, 1998), into the Hamming hypercube of size n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by (Miller and Charles, 1991), and the first one within edge centric approaches, as shown by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techn</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>2265--2273</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="3422" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="511" endWordPosition="514">low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation. In this context, it is surprising that embedding semantic similarity of words in low dimensional 8 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2265–2273. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Norouzi</author>
<author>Ali Punjani</author>
<author>David J Fleet</author>
</authors>
<title>Fast search in hamming space with multiindex hashing.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,</booktitle>
<pages>3108--3115</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5510" citStr="Norouzi et al., 2012" startWordPosition="824" endWordPosition="827">own by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techniques allows the very fast computation of top-k queries (Norouzi et al., 2012) on the Hamming space. However, the dimension of the hypercube (i.e. the number of bits used to represent an element) should obey the threshold of few CPU words (64, 128 ..., bits) to maintain such efficiency (Heo et al., 2012). An isometric embedding requires a excessively high number of dimensions to be feasible. However, in this paper we show that practical embeddings exist and present a method to construct them. The best embedding presents very strong correlations (r = .819, p = .829) with the Leacock &amp; Chodorow similarity measure (LCH in the rest of this paper). Our experiments against th</context>
</contexts>
<marker>Norouzi, Punjani, Fleet, 2012</marker>
<rawString>Mohammad Norouzi, Ali Punjani, and David J Fleet. 2012. Fast search in hamming space with multiindex hashing. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3108–3115. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1793" citStr="Resnik, 1995" startWordPosition="264" endWordPosition="265"> at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998). While providing a significant impact on a broad range of applicati</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Seco</author>
<author>Tony Veale</author>
<author>Jer Hayes</author>
</authors>
<title>An intrinsic information content metric for semantic similarity in wordnet.</title>
<date>2004</date>
<booktitle>In ECAI,</booktitle>
<volume>16</volume>
<pages>1089</pages>
<contexts>
<context position="4915" citStr="Seco et al., 2004" startWordPosition="731" endWordPosition="734">o-word semantic similarity task to remain corpus-dependant – i.e. ignoring the background knowledge provided by a lexical hierarchy. In this paper, we propose an embedding of knowledge base semantic similarity based on the shortest path metric (Leacock and Chodorow, 1998), into the Hamming hypercube of size n (the size of targeted binary codes). The Leacock and Chodorow semantic similarity is one of the most meaningful measure. It yields the second rank for highest correlation with the data collected by (Miller and Charles, 1991), and the first one within edge centric approaches, as shown by (Seco et al., 2004). This method is only surpassed by the information theoretic based similarity from (Jiang and Conrath, 1997). A second study present similar result (Budanitsky and Hirst, 2006), while a third one ranks this similarity measure at the first rank for precision in paraphrase identification (Mihalcea et al., 2006). The hypercube embedding technique benefits from the execution of Hamming distance within a few cycles on modern CPUs. This allows the computation of several millions distances per second. Multi-index techniques allows the very fast computation of top-k queries (Norouzi et al., 2012) on t</context>
</contexts>
<marker>Seco, Veale, Hayes, 2004</marker>
<rawString>Nuno Seco, Tony Veale, and Jer Hayes. 2004. An intrinsic information content metric for semantic similarity in wordnet. In ECAI, volume 16, page 1089.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giannis Varelas</author>
</authors>
<title>Epimenidis Voutsakis, Paraskevi Raftopoulou, Euripides GM Petrakis, and Evangelos E Milios.</title>
<date>2005</date>
<booktitle>In Proceedings of the 7th annual ACM international workshop on Web information and data management,</booktitle>
<pages>10--16</pages>
<publisher>ACM.</publisher>
<marker>Varelas, 2005</marker>
<rawString>Giannis Varelas, Epimenidis Voutsakis, Paraskevi Raftopoulou, Euripides GM Petrakis, and Evangelos E Milios. 2005. Semantic similarity methods in wordnet and their application to information retrieval on the web. In Proceedings of the 7th annual ACM international workshop on Web information and data management, pages 10–16. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elke Wilkeit</author>
</authors>
<title>Isometric embeddings in hamming graphs.</title>
<date>1990</date>
<journal>Journal of Combinatorial Theory, Series B,</journal>
<volume>50</volume>
<issue>2</issue>
<contexts>
<context position="8757" citStr="Wilkeit, 1990" startWordPosition="1393" endWordPosition="1394">ed a simple approach. Since hypernyms are ordered, we decided to preserve only the first hypernym – semantically more relevant, or at least statistically – and to cut edges to other hypernyms. Figure 1: Construction of isometric embedding on a sample tree. For this six nodes tree, the embedding requires five bits. 000 00000 A B C F D E 100 010 001 B C F D E A 10000 01000 00100 B C F D E A 01010 01001 9 Our experiments in Table 1 shows that using the obtained tree instead of the lattice keeps a high correlation (r = .919, p = .931) with the original LCH distance, thus validating the approach. (Wilkeit, 1990) showed that any k-ary tree of size n can be embedded into Qn−1. We give an isometric embedding algorithm, which is linear in time and space, exhibiting a much better time complexity than Winkler’s generic approach for graphs, running in O(n5) (Winkler, 1984). Starting with an empty binary signature, the algorithm is the following: at each step of a depth-first preorder traversal: if the node has k children, we set the signature for the i-th child by appending k zeroes to the parent’s signature and by setting the i-th of the k bits to one. An example is given in Figure 1. However, when using r</context>
</contexts>
<marker>Wilkeit, 1990</marker>
<rawString>Elke Wilkeit. 1990. Isometric embeddings in hamming graphs. Journal of Combinatorial Theory, Series B, 50(2):179–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter M Winkler</author>
</authors>
<title>Isometric embedding in products of complete graphs.</title>
<date>1984</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="9016" citStr="Winkler, 1984" startWordPosition="1436" endWordPosition="1437">r this six nodes tree, the embedding requires five bits. 000 00000 A B C F D E 100 010 001 B C F D E A 10000 01000 00100 B C F D E A 01010 01001 9 Our experiments in Table 1 shows that using the obtained tree instead of the lattice keeps a high correlation (r = .919, p = .931) with the original LCH distance, thus validating the approach. (Wilkeit, 1990) showed that any k-ary tree of size n can be embedded into Qn−1. We give an isometric embedding algorithm, which is linear in time and space, exhibiting a much better time complexity than Winkler’s generic approach for graphs, running in O(n5) (Winkler, 1984). Starting with an empty binary signature, the algorithm is the following: at each step of a depth-first preorder traversal: if the node has k children, we set the signature for the i-th child by appending k zeroes to the parent’s signature and by setting the i-th of the k bits to one. An example is given in Figure 1. However, when using real-world datasets such as Wordnet, the embedding still requires several thousands of bits to represent a node. This dimension reduction to tens of kilobits per node remains far from our goal of several CPU words, and calls for a task-specific approach. Looki</context>
</contexts>
<marker>Winkler, 1984</marker>
<rawString>Peter M Winkler. 1984. Isometric embedding in products of complete graphs. Discrete Applied Mathematics, 7(2):221–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1724" citStr="Wu and Palmer, 1994" startWordPosition="252" endWordPosition="255">lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hierarchy. It therefore ignores contextual information on word occurrences and relies on humans to encode such hierarchies – a tedious task in practice. In contrast, well-known distributional semantics strategies encode semantic similarity using the correlation of statistical observations on the occurrences of words in a textual corpora (Lin, 1998</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liheng Xu</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Jun Zhao</author>
</authors>
<title>Product feature mining: Semantic clues versus syntactic constituents.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>336--346</pages>
<contexts>
<context position="3274" citStr="Xu et al., 2014" startWordPosition="485" endWordPosition="488"> textual corpora in a reasonable amount of time. As an answer, embedding high-dimensional distributional semantics models for words into low-dimensional spaces (henceforth word embedding (Collobert and Weston, 2008)) has emerged as a popular method. Word embedding utilizes deep learning to learn a real-valued vector representation of words so that any vector distance – usually the cosine similarity – encodes the word-to-word semantic similarity. Although word embedding was successfully applied for several NLP tasks (Hermann et al., 2014; Andreas and Klein, 2014; Clinchant and Perronnin, 2013; Xu et al., 2014; Li and Liu, 2014; Goyal et al., 2013), it implies a slow training phase – measured in days (Collobert and Weston, 2008; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013), though re-embedding words seems promising (Labutov and Lipson, 2013). There is another usually under-considered issue: the tractability of the pairwise similarity computation in the vector space for large volume of data. Despite these limitations, the current enthusiasm for word embedding certainly echoes the need for lightning fast word-to-word semantic similarity computation. In this context, it is surprising that embeddi</context>
</contexts>
<marker>Xu, Liu, Lai, Zhao, 2014</marker>
<rawString>Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao. 2014. Product feature mining: Semantic clues versus syntactic constituents. In Association for Computational Linguistics (ACL), pages 336–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1378" citStr="Yih et al., 2014" startWordPosition="201" endWordPosition="204">sometric embeddings. We report a speedup of three orders of magnitude for the task of computing Leacock and Chodorow (LCH) similarity while keeping strong correlations (r = .819, p = .826). 1 Introduction Among semantic relatedness measures, semantic similarity encodes the conceptual distance between two units of language – this goes beyond lexical ressemblance. When words are the speech units, semantic similarity is at the very core of many NLP problems. It has proven to be essential for word sense disambiguation (Mavroeidis et al., 2005; Basile et al., 2014), open domain question answering (Yih et al., 2014), and information retrieval on the Web (Varelas et al., 2005), to name a few. Two established strategies to estimate pairwise word semantic similarity includes knowledge-based and distributional semantics. Knowledge-based approaches exploit the structure of the taxonomy ((Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Wu and Palmer, 1994)), its content ((Banerjee and Pedersen, 2002)), or both (Resnik, 1995; Lin, 1998). In the earliest applications, Wordnet-based semantic similarity played a predominant role so that semantic similarity measures reckon with information from the lexical hie</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>