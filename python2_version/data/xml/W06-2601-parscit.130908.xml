<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000632">
<title confidence="0.990759">
Maximum Entropy Tagging with Binary and Real-Valued Features
</title>
<author confidence="0.955452">
Vanessa Sandrini Marcello Federico Mauro Cettolo
</author>
<affiliation confidence="0.751706">
ITC-irst - Centro per la Ricerca Scientifica e Tecnologica
</affiliation>
<address confidence="0.666974">
38050 Povo (Trento) - ITALY
</address>
<email confidence="0.996713">
{surname}@itc.it
</email>
<sectionHeader confidence="0.99858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559">
Recent literature on text-tagging reported
successful results by applying Maximum
Entropy (ME) models. In general, ME
taggers rely on carefully selected binary
features, which try to capture discrimi-
nant information from the training data.
This paper introduces a standard setting
of binary features, inspired by the litera-
ture on named-entity recognition and text
chunking, and derives corresponding real-
valued features based on smoothed log-
probabilities. The resulting ME models
have orders of magnitude fewer parame-
ters. Effective use of training data to esti-
mate features and parameters is achieved
by integrating a leaving-one-out method
into the standard ME training algorithm.
Experimental results on two tagging tasks
show statistically significant performance
gains after augmenting standard binary-
feature models with real-valued features.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99116254">
The Maximum Entropy (ME) statistical frame-
work (Darroch and Ratcliff, 1972; Berger et al.,
1996) has been successfully deployed in several
NLP tasks. In recent evaluation campaigns, e.g.
DARPA IE and CoNLL 2000-2003, ME models
reached state-of-the-art performance on a range of
text-tagging tasks.
With few exceptions, best ME taggers rely on
carefully designed sets of features. Features cor-
respond to binary functions, which model events,
observed in the (annotated) training data and sup-
posed to be meaningful or discriminative for the
task at hand. Hence, ME models result in a log-
linear combination of a large set of features, whose
weights can be estimated by the well known Gen-
eralized Iterative Scaling (GIS) algorithm by Dar-
roch and Ratcliff (1972).
Despite ME theory and its related training algo-
rithm (Darroch and Ratcliff, 1972) do not set re-
strictions on the range of feature functionsl, pop-
ular NLP text books (Manning and Schutze, 1999)
and research papers (Berger et al., 1996) seem
to limit them to binary features. In fact, only
recently, log-probability features have been de-
ployed in ME models for statistical machine trans-
lation (Och and Ney, 2002).
This paper focuses on ME models for two text-
tagging tasks: Named Entity Recognition (NER)
and Text Chuncking (TC). By taking inspiration
from the literature (Bender et al., 2003; Borth-
wick, 1999; Koeling, 2000), a set of standard bi-
nary features is introduced. Hence, for each fea-
ture type, a corresponding real-valued feature is
developed in terms of smoothed probability distri-
butions estimated on the training data. A direct
comparison of ME models based on binary, real-
valued, and mixed features is presented. Besides,
performance on the tagging tasks, complexity and
training time by each model are reported. ME es-
timation with real-valued features is accomplished
by combining GIS with the leave-one-out method
(Manning and Schutze, 1999).
Experiments were conducted on two publicly
available benchmarks for which performance lev-
els of many systems are published on the Web. Re-
sults show that better ME models for NER and TC
can be developed by integrating binary and real-
valued features.
&apos;Darroch and Ratcliff (1972) show how any set of real-
valued feature functions can be properly handled.
</bodyText>
<page confidence="0.98083">
1
</page>
<sectionHeader confidence="0.95322" genericHeader="introduction">
2 ME Models for Text Tagging
</sectionHeader>
<bodyText confidence="0.992151">
Given a sequence of words wT1 = w1, ... , wT and
a set of tags C, the goal of text-tagging is to find
a sequence of tags cT1 = c1, ... , cT which maxi-
mizes the posterior probability, i.e.:
</bodyText>
<equation confidence="0.853131666666667">
cT ˆ 1 = arg max p(cT1  |wT1 ). (1)
cT
1
</equation>
<bodyText confidence="0.994479">
By assuming a discriminative model, Eq. (1) can
be rewritten as follows:
</bodyText>
<equation confidence="0.967541166666667">
p(ct |ct−1
1 , wT1 ), (2)
where p(ct|ct−1
1 , wT1 ) is the target conditional
probability of tag ct given the context (ct−1
1 , wT1 ),
</equation>
<bodyText confidence="0.994348375">
i.e. the entire sequence of words and the full se-
quence of previous tags. Typically, independence
assumptions are introduced in order to reduce the
context size. While this introduces some approxi-
mations in the probability distribution, it consid-
erably reduces data sparseness in the sampling
space. For this reason, the context is limited here
to the two previous tags (ct−1
t−2) and to four words
around the current word (wt+2
t−2). Moreover, limit-
ing the context to the two previous tags permits to
apply dynamic programming (Bender et al., 2003)
to efficiently solve the maximization (2).
Let y = ct denote the class to be guessed (y E Y)
at time t and x = ct−1
</bodyText>
<equation confidence="0.971174833333333">
t−2, wt+2
t−2 its context (x E X).
The generic ME model results:
exp(En i=1 λifi(x, y))
pA(y  |x) = (3)
Ey, exp(Eni= 1 λifi(x, y0))
</equation>
<bodyText confidence="0.999929157894737">
The n feature functions fi(x, y) represent any kind
of information about the event (x, y) which can be
useful for the classification task. Typically, binary
features are employed which model the verifica-
tion of simple events within the target class and
the context.
In Mikheev (1998), binary features for text tagging
are classified into two broad classes: atomic and
complex. Atomic features tell information about
the current tag and one single item (word or tag) of
the context. Complex features result as a combina-
tion of two or more atomic features. In this way, if
the grouped events are not independent, complex
features should capture higher correlations or de-
pendencies, possibly useful to discriminate.
In the following, a standard set of binary fea-
tures is presented, which is generally employed
for text-tagging tasks. The reader familiar with the
topic can directly check this set in Table 1.
</bodyText>
<sectionHeader confidence="0.949735" genericHeader="method">
3 Standard Binary Features
</sectionHeader>
<bodyText confidence="0.9996725">
Binary features are indicator functions of specified
events of the sample space X x Y. Hence, they
take value 1 if the event occurs or 0 otherwise. For
the sake of notation, the feature name denotes the
type of event, while the index specifies its param-
eters. For example:
</bodyText>
<equation confidence="0.86444">
Orthperson,Cap,−1(x, y)
</equation>
<bodyText confidence="0.99902525">
corresponds to an Orthographic feature which is
active if and only if the class at time t is person
and the word at time t−1 in the context starts with
capitalized letter.
</bodyText>
<subsectionHeader confidence="0.998243">
3.1 Atomic Features
</subsectionHeader>
<bodyText confidence="0.956756166666667">
Lexical features These features model co-
occurrences of classes and single words of the con-
text. Lexical features are defined on a window
of f2 positions around the current word. Lexical
features are denoted by the name Lex and indexed
with the triple c, w, d which fixes the current class,
i.e. ct = c, the identity and offset of the word in
the context, i.e. wt+d = w. Formally, the feature
is computed by:
Lex c,w,d(x, y) =ˆ δ(ct = c) · δ(wt+d = w).
For example, the lexical feature for word
Verona, at position t with tag loc (location) is:
</bodyText>
<equation confidence="0.997375">
Lexloc,Verona,0(x, y) = δ(ct = loc) ·
·δ(wt = Verona).
</equation>
<bodyText confidence="0.999732461538462">
Lexical features might introduce data sparseness
in the model, given that in real texts an impor-
tant fraction of words occur only once. In other
words, many words in the test set will have no
corresponding features-parameter pairs estimated
on the training data. To cope with this problem,
all words observed only once in the training data
were mapped into the special symbol oov.
Syntactic features They model co-occurrences
of the current class with part-of-speech or chunk
tags of a specific position in the context. Syntactic
features are denoted by the name Syn and indexed
with a 4-tuple (c, Pos, p, d) or (c, Chnk, p, d),
</bodyText>
<equation confidence="0.98441">
cTˆ1 = arg max
cT
1
T
H
t=1
</equation>
<page confidence="0.985626">
2
</page>
<table confidence="0.998552">
Name Index Definition
Lex c, w, d δ(ct = c) · δ(wt+d = w), d ∈ Z
Syn c, T, p, d δ(ct = c) · δ(T(wt+d) = p) , T ∈ {Pos, Chnk}, d ∈ Z
Orth c, F, d δ(ct = c) · F(wt+d) , F ∈ {IsCap, IsCAP}, d ∈ Z
Dict c, L, d δ(ct = c) · InList(L, wt+d), d ∈ Z
Tran c, c0, d δ(ct = c) · δ(ct−d = c0) d ∈ N+
Lex+ c, s, k, ws+k−1 Qs+k−1
Syn+ s d=s Lexc,wd,d(x, y), k ∈ N+, s ∈ Z
Orth+ c, T, s, k,ps+k−1 Qs+k−1
Dict+ s d=s Sync,T,pd,d(x, y), k ∈ N+, s ∈ Z
Tran+ c, F, k, b+k δ(ct = c) · Qk d=−k δ(Orthc,F,d(x, y) = bd) , bd ∈ {0, 1}, k ∈ N+
−k δ(ct = c) · Qkd=−k δ(Dictc,L,d(x, y) = bd) , bd ∈ {0, 1}, k ∈ N+
c, L, k, b+k Qd=1, Tranc cd d(x, y) k ∈ N+
−k
c, k, ci
</table>
<tableCaption confidence="0.999958">
Table 1: Standard set of binary features for text tagging.
</tableCaption>
<bodyText confidence="0.999915333333333">
which fixes the class ct, the considered syntactic
information, and the tag and offset within the con-
text. Formally, these features are computed by:
</bodyText>
<equation confidence="0.999139">
Sync,Pos,p,d(x,y)ˆ=δ(ct = c) · δ(Pos(wt+d) = p)
Sync,Chnk,p,d(x,y)ˆ=δ(ct = c)·δ(Chnk(wt+d) = p).
</equation>
<bodyText confidence="0.999976111111111">
Orthographic features These features model
co-occurrences of the current class with surface
characteristics of words of the context, e.g. check
if a specific word in the context starts with cap-
italized letter (IsCap) or is fully capitalized
(IsCAP). In this framework, only capitalization
information is considered. Analogously to syntac-
tic features, orthographic features are defined as
follows:
</bodyText>
<equation confidence="0.5692125">
Orthc,IsCap,d(x, y)ˆ=δ(ct = c) · IsCap(wt+d)
Orthc,IsCAP,d(x, y)ˆ=δ(ct = c) · IsCAP(wt+d).
</equation>
<bodyText confidence="0.969177066666667">
Dictionary features These features check if
specific positions in the context contain words oc-
curring in some prepared list. This type of feature
results relevant for tasks such as NER, in which
gazetteers of proper names can be used to improve
coverage of the training data. Atomic dictionary
features are defined as follows:
Dictc,L,d(x, y)ˆ=δ(ct = c) · InList(L, wt+d)
where L is a specific pre-compiled list, and
InList is a function which returns 1 if the spec-
ified word matches one of the multi-word entries
of list L, and 0 otherwise.
Transition features Transition features model
Markov dependencies between the current tag and
a previous tag. They are defined as follows:
</bodyText>
<equation confidence="0.916783">
Tranc,c&apos;,d(x,y)ˆ=δ(ct = c) · δ(ct−d = c0).
</equation>
<subsectionHeader confidence="0.997809">
3.2 Complex Features
</subsectionHeader>
<bodyText confidence="0.997692888888889">
More complex events are defined by combining
two or more atomic features in one of two ways.
Product features take the intersection of the cor-
responding atomic events. Vector features con-
sider all possible outcomes of the component fea-
tures.
For instance, the product of 3 atomic Lexical
features, with class c, offsets −2, −1, 0, and words
v−2, v−1, v0, is:
</bodyText>
<equation confidence="0.971190333333333">
0
Lex+c,−2,3,v_2,v_1,v0(x, y)=ˆ Y Lexc,vd,d(x, y).
d=−2
</equation>
<bodyText confidence="0.993148">
Vector features obtained from three Dictionary
features with the same class c, list L, and offsets,
respectively, -1,0,+1, are indexed over all possible
binary outcomes b−1, b0, b1 of the single atomic
features, i.e.:
</bodyText>
<equation confidence="0.983781">
Dict+c,L,1,b_1,b0,b+1(x, y)ˆ=δ(ct = c)×
Y1 δ(Dictc,L,d(x,y) = bd).
d=−1
</equation>
<bodyText confidence="0.999709272727273">
Complex features used in the experiments are de-
scribed in Table 1.
The use of complex features significantly in-
creases the model complexity. Assuming that
there are 10, 000 words occurring more than once
in the training corpus, the above lexical feature po-
tentially adds O(|C|1012) parameters!
As complex binary features might result pro-
hibitive from a computational point of view, real-
valued features should be considered as an alter-
native.
</bodyText>
<page confidence="0.996366">
3
</page>
<table confidence="0.999898818181818">
Feature Index Probability Distribution
Lex d p(ct  |wt+d)
Syn T, d p(ct  |T(wt+d))
Orth F, d p(ct  |F(wt+d))
Dict List, d p(ct  |IsIn(List, wt+d))
Tran d p(ct  |ct−d)
Lex+ s, k p(ct  |wt+s, .., wt+s+k−1
Syn+ T, s, k p(ct  |T(wt+s, ... , wt+s+k−1))
Orth+ k, F p(ct  |F(wt−k), . . . , F(wt+k))
Dict+ k, L p(ct  |InList(L, wt−k), ... , InList(L, wt+k))
Tran+ k p(ct  |ct−k, . . . , ct+k))
</table>
<tableCaption confidence="0.999797">
Table 2: Corresponding standard set of real-values features.
</tableCaption>
<sectionHeader confidence="0.999322" genericHeader="method">
4 Real-valued Features
</sectionHeader>
<bodyText confidence="0.995890578947369">
A binary feature can be seen as a probability mea-
sure with support set made of a single event. Ac-
cording to this point of view, we might easily ex-
tend binary features to probability measures de-
fined over larger event spaces. In fact, it results
convenient to introduce features which are log-
arithms of conditional probabilities. It can be
shown that in this way linear constraints of the
ME model can be interpreted in terms of Kullback-
Leibler distances between the target model and the
conditional distributions (Klakow, 1998).
Let p1(y|x),p2(y|x),...,pn(y|x) be n different
conditional probability distributions estimated on
the training corpus. In our framework, each con-
ditional probability pi is associated to a feature fi
which is defined over a subspace [X]i x Y of the
sample space X x Y. Hence, pi(y|x) should be
read as a shorthand of p(y |[x]i).
The corresponding real-valued feature is:
</bodyText>
<equation confidence="0.962193">
fi(x, y) = logpi(y  |x). (4)
</equation>
<bodyText confidence="0.576647">
In this way, the ME in Eq. (3) can be rewritten as:
</bodyText>
<equation confidence="0.9803805">
�n i pi(y|x)λi
pλ(y|x) =
. (5)
Ey&apos; Hi pi(y  |x)λ z
</equation>
<bodyText confidence="0.939105333333333">
According to the formalism adopted in Eq. (4),
real-valued features assume the following form:
fi(ct,ct−1
</bodyText>
<equation confidence="0.96028125">
t−2, wt+2
t−2) = log pi(ct  |ct−1
t−2, wt+2
t−2). (6)
</equation>
<bodyText confidence="0.998815625">
For each so far presented type of binary feature,
a corresponding real-valued type can be easily de-
fined. The complete list is shown in Table 2. In
general, the context subspace was defined on the
basis of the offset parameters of each binary fea-
ture. For instance, all lexical features selecting
two words at distances -1 and 0 from the current
position t are modeled by the conditional distri-
bution p(ct  |wt−1, wt). While distributions of
lexical, syntactic and transition features are con-
ditioned on words or tags, dictionary and ortho-
graphic features are conditioned on binary vari-
ables.
An additional real-valued feature that was em-
ployed is the so called prior feature, i.e. the prob-
ability of a tag to occur:
</bodyText>
<equation confidence="0.995227">
Prior(x, y) = logp(ct)
</equation>
<bodyText confidence="0.999935357142857">
A major effect of using real-valued features is
the drastic reduction of model parameters. For
example, each complex lexical features discussed
before introduce just one parameter. Hence, the
small number of parameters eliminates the need
of smoothing the ME estimates.
Real-valued features present some drawbacks.
Their level of granularity, or discrimination, might
result much lower than their binary variants. For
many features, it might result difficult to compute
reliable probability values due to data sparseness.
For the last issue, smoothing techniques devel-
oped for statistical language models can be applied
(Manning and Schutze, 1999).
</bodyText>
<sectionHeader confidence="0.996275" genericHeader="method">
5 Mixed Feature Models
</sectionHeader>
<bodyText confidence="0.999857888888889">
This work, beyond investigating the use of real-
valued features, addresses the behavior of models
combining binary and real-valued features. The
reason is twofold: on one hand, real-valued fea-
tures allow to capture complex information with
fewer parameters; on the other hand, binary fea-
tures permit to keep a good level of granularity
over salient characteristics. Hence, finding a com-
promise between binary and real-valued features
</bodyText>
<page confidence="0.987941">
4
</page>
<bodyText confidence="0.998672833333333">
is known that the GIS algorithm requires feature
functions fi(x, y) to be non-negative. Hence, fea-
tures were re-scaled as follows:
might help to develop ME models which better
trade-off complexity vs. granularity of informa-
tion.
</bodyText>
<sectionHeader confidence="0.99223" genericHeader="method">
6 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9993325">
From the duality of ME and maximum likeli-
hood (Berger et al., 1996), optimal parameters
λ∗ for model (3) can be found by maximizing
the log-likelihood function over a training sample
</bodyText>
<equation confidence="0.9469805">
{(xt, yt) : t = 1,... , N}, i.e.:
log pλ(yt|xt). (7)
</equation>
<bodyText confidence="0.999927945945946">
Now, whereas binary features take only two values
and do not need any estimation phase, conditional
probability features have to be estimated on some
data sample. The question arises about how to ef-
ficiently use the available training data in order to
estimate the parameters and the feature distribu-
tions of the model, by avoiding over-fitting.
Two alternative techniques, borrowed from sta-
tistical language modeling, have been consid-
ered: the Held-out and the Leave-one-out methods
(Manning and Schutze, 1999).
Held-out method. The training sample S is split
into two parts used, respectively, to estimate the
feature distributions and the ME parameters.
Leave-one-out. ME parameters and feature dis-
tributions are estimated over the same sample S.
The idea is that for each addend in eq. (7), the cor-
responding sample point (xt, yt) is removed from
the training data used to estimate the feature distri-
butions of the model. In this way, it can be shown
that occurrences of novel observations are simu-
lated during the estimation of the ME parameters
(Federico and Bertoldi, 2004).
In our experiments, language modeling smooth-
ing techniques (Manning and Schutze, 1999) were
applied to estimate feature distributions pi(y|x).
In particular, smoothing was based on the dis-
counting method in Ney et al. (1994) combined to
interpolation with distributions using less context.
Given the small number of smoothing parameters
involved, leave-one-out probabilities were approx-
imated by just modifying count statistics on the
fly (Federico and Bertoldi, 2004). The rationale is
that smoothing parameters do not change signifi-
cantly after removing just one sample point.
For parameter estimation, the GIS algorithm
by Darroch and Ratcliff (1972) was applied. It
</bodyText>
<equation confidence="0.948554333333333">
1+E
fi(x, y) = log pi(y|x) + log , (8)
min pi
</equation>
<bodyText confidence="0.9981305">
where c is a small positive constant and the de-
nominator is a constant term defined by:
</bodyText>
<equation confidence="0.9816725">
minpi = (x minSpi(y|x). (9)
y)∈
</equation>
<bodyText confidence="0.996652363636364">
The factor (1 + c) was introduced to ensure that
real-valued features are always positive. This con-
dition is important to let features reflect the same
behavior of the conditional distributions, which
assign a positive probability to each event.
It is easy to verify that this scaling operation
does not affect the original model but only impacts
on the GIS calculations. Finally, a slack feature
was introduced by the algorithm to satisfy the con-
straint that all features sum up to a constant value
(Darroch and Ratcliff, 1972).
</bodyText>
<sectionHeader confidence="0.999451" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.9997814">
This section presents results of ME models applied
to two text-tagging tasks, Named Entity Recogni-
tion (NER) and Text Chunking (TC).
After a short introduction to the experimen-
tal framework, the detailed feature setting is pre-
sented. Then, experimental results are presented
for the following contrastive conditions: binary
versus real-valued features, training via held-out
versus leave-one-out, atomic versus complex fea-
tures.
</bodyText>
<subsectionHeader confidence="0.937478">
7.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999689538461538">
Named Entity Recognition English NER ex-
periments were carried out on the CoNLL-2003
shared task2. This benchmark is based on texts
from the Reuters Corpus which were manually
annotated with parts-of-speech, chunk tags, and
named entity categories. Four types of categories
are defined: person, organization, location and
miscellaneous, to include e.g. nations, artifacts,
etc. A filler class is used for the remaining words.
After including tags denoting the start of multi-
word entities, a total of 9 tags results. Data are
partitioned into training (200K words), develop-
ment (50K words), and test (46K words) samples.
</bodyText>
<footnote confidence="0.964961">
2Data and results in http://cnts.uia.ac.be/conll2003/ner.
</footnote>
<equation confidence="0.98240125">
λ∗ = arg max
λ
N
t=1
</equation>
<page confidence="0.969492">
5
</page>
<bodyText confidence="0.999129777777778">
Text Chunking English TC experiments were
conducted on the CoNLL-2000 shared task3.
Texts originate from the Wall Street Journal and
are annotated with part-of-speech tags and chunks.
The chunk set consists of 11 syntactic classes. The
set of tags which also includes start-markers con-
sists of 23 classes. Data is split into training (210K
words) and test (47K words) samples.
Evaluation Tagging performance of both tasks
is expressed in terms of F-score, namely the har-
monic mean of precision and recall. Differences in
performance have been statistically assessed with
respect to precision and recall, separately, by ap-
plying a standard test on proportions, with signif-
icance levels α = 0.05 and α = 0.1. Henceforth,
claimed differences in precision or recall will have
their corresponding significance level shown in
parenthesis.
</bodyText>
<subsectionHeader confidence="0.998754">
7.2 Settings and Baseline Models
</subsectionHeader>
<bodyText confidence="0.999942041666667">
Feature selection and setting for ME models is an
art. In these experiments we tried to use the same
set of features with minor modifications across
both tasks. In particular, used features and their
settings are shown in Table 3.
Training of models with GIS and estimation
of feature distributions used in-house developed
toolkits. Performance of binary feature models
was improved by smoothing features with Gaus-
sian priors (Chen and Rosenfeld, 1999) with mean
zero and standard deviation σ = 4. In general,
tuning of models was carried out on a development
set.
Most of the comparative experiments were per-
formed on the NER task. Three baseline models
using atomic features Lex, Syn, and Tran were
investigated first: model BaseBin, with all binary
features; model BaseReal, with all real-valued fea-
tures plus the prior feature; model BaseMix, with
real-valued Lex and binary Tran and Syn. Mod-
els BaseReal and BaseMix were trained with the
held-out method. In particular, feature distribu-
tions were estimated on the training data while ME
parameters on the development set.
</bodyText>
<subsectionHeader confidence="0.995395">
7.3 Binary vs. Real-valued Features
</subsectionHeader>
<bodyText confidence="0.9993445">
The first experiment compares performance of the
baseline models on the NER task. Experimental
results are summarized in Table 4. Models Base-
Bin, BaseReal, and BaseMix achieved F-scores of
</bodyText>
<footnote confidence="0.967302">
3Data and results in http://cnts.uia.ac.be/conll2000/chunking.
</footnote>
<table confidence="0.99654875">
Model ID Num P% R% F-score
BaseBin 580K 78.82 75.62 77.22
BaseReal 10 79.74 74.15 76.84
BaseMix 753 78.90 75.85 77.34
</table>
<tableCaption confidence="0.981858666666667">
Table 4: Performance of baseline models on the
NER task. Number of parameters, precision, re-
call, and F-score are reported for each model.
</tableCaption>
<table confidence="0.999787333333333">
Model Methods P% R% F-score
BaseMix Held-Out 78.90 75.85 77.34
BaseMix L-O-O 80.64 76.40 78.46
</table>
<tableCaption confidence="0.8620505">
Table 5: Performance of mixed feature models
with two different training methods.
</tableCaption>
<bodyText confidence="0.996775545454545">
77.22, 76.84, and 77.34. Statistically meaning-
ful differences were in terms of recall, between
BaseBin and BaseReal (α = 0.1), and between
BaseMix and BaseReal (α = 0.05).
Despite models BaseMix and BaseBin perform
comparably, the former has many fewer parame-
ters, i.e. 753 against 580,000. In fact, BaseMix re-
quires storing and estimating feature distributions,
which is however performed at a marginal compu-
tational cost and off-line with respect to GIS train-
ing.
</bodyText>
<subsectionHeader confidence="0.999268">
7.4 Training with Mixed Features
</subsectionHeader>
<bodyText confidence="0.999634647058824">
An experiment was conducted with the BaseMix
model to compare the held-out and leave-one-out
training methods. Results in terms of F-score are
reported in Table 5. By applying the leave-one-
out method F-score grows from 77.34 to 78.46,
with a meaningful improvement in recall (α =
0.05). With respect to models BaseBin and Base-
Real, leave-one-out estimation significantly im-
proved precision (α = 0.05).
In terms of training time, ME models with real-
valued features took significantly more GIS iter-
ations to converge. Figures of cost per iteration
and number of iterations are reported in Table 6.
(Computation times are measured on a single CPU
Pentium-4 2.8GHz.) Memory size of the training
process is instead proportional to the number n of
parameters.
</bodyText>
<subsectionHeader confidence="0.93402">
7.5 Complex Features
</subsectionHeader>
<bodyText confidence="0.999900666666667">
A final set of experiments aims at comparing the
baseline ME models augmented with complex fea-
tures, again either binary only (model FinBin),
</bodyText>
<page confidence="0.999014">
6
</page>
<table confidence="0.999809642857143">
Feature Index NE Task Chunking Task
Lex c, w, d N(w) &gt; 1, −2 &lt; d &lt; +2 −2 &lt; d &lt; +2
Syn c, T, p, d T E {Pos, Chnk}, d = 0 T = Pos, −2 &lt; d &lt; +2
Tran c, cl, d d = −2, −1 d = −2, −1
Lex+ c, s, k, ws+k−1 s = −1,0,k = 1 s = −1,0 k = 1
Syn+ s not used s = −1,0 k = 1
Orth+ c, T, s, k, ps+k−1 F = {Cap, CAP}, k = 2 F = Cap, k = 1
Dict+ s k = 3L = {LOC, PER, ORG, MISC} not used
Tran+ c, k, F, b+k k = 2 k = 2
−k
c, k, L, b+k
−k
c, k, ck
1
</table>
<tableCaption confidence="0.998294">
Table 3: Setting used for binary and real-valued features in the reported experiments.
</tableCaption>
<table confidence="0.99945275">
Model Single Iteration Iterations Total
BaseBin 54 sec 750 pz� 11 h
BaseReal 9.6 sec 35,000 pz� 93 h
BaseMix 42 sec 4,000 pz� 46 h
</table>
<tableCaption confidence="0.782247">
Table 6: Computational cost of parameter estima-
tion by different baseline models.
real-valued only (FinReal), or mixed (FinMix).
Results are provided both for NER and TC.
</tableCaption>
<bodyText confidence="0.958893321428571">
This time, compared models use different fea-
ture settings. In fact, while previous experiments
aimed at comparing the same features, in either
real or binary form, these experiments explore al-
ternatives to a full-fledged binary model. In par-
ticular, real-valued features are employed whose
binary versions would introduce a prohibitively
large number of parameters. Parameter estima-
tion of models including real-valued features al-
ways applies the leave-one-out method.
For the NER task, model FinBin adds Orth+
and Dict+; FinReal adds Lex+, Orth+ and
Dict+; and, FinMix adds real-valued Lex+ and
binary-valued Orth+ and Dict+.
In the TC task, feature configurations are as fol-
lows: FinBin uses Lex, Syn, Tran, and Orth+;
FinReal uses Lex, Syn, Tran, Prior, Orth+,
Lex+, Syn+, Tran+; and, finally, FinMix uses
binary Syn, Tran, Orth+ and real-valued Lex,
Lex+, Syn+.
Performance of the models on the two tasks are
reported in Table 7 and Table 8, respectively.
In the NER task, all final models outperform the
baseline model. Improvements in precision and
recall are all significant (α = 0.05). Model Fin-
Mix improves precision with respect to model Fin-
Bin (α = 0.05) and requires two order of magni-
tude fewer parameters.
</bodyText>
<table confidence="0.999313">
Model Num P% R% F-score
FinBin 673K 81.92 80.36 81.13
FinReal 19 83.58 74.03 78.07
FinMix 3K 84.34 80.38 82.31
</table>
<tableCaption confidence="0.968231">
Table 7: Results with complex features on the
NER task.
</tableCaption>
<table confidence="0.9998505">
Model Num P% R% F-score
FinBin 2M 91.04 91.48 91.26
FinReal 19 88.73 90.58 89.65
FinMix 6K 91.93 92.24 92.08
</table>
<tableCaption confidence="0.887384">
Table 8: Results with complex features on the TC
task.
</tableCaption>
<bodyText confidence="0.9968954">
In the TC task, the same trend is observed.
Again, best performance is achieved by the model
combining binary and real-valued features. In par-
ticular, all observable differences in terms of pre-
cision and recall are significant (α = 0.05).
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.996504568181818">
In summary, this paper addressed improvements to
ME models for text tagging applications. In par-
ticular, we showed how standard binary features
from the literature can be mapped into correspond-
ing log-probability distributions. ME training with
the so-obtained real-valued features can be accom-
plished by combining the GIS algorithm with the
leave-one-out or held-out methods.
With respect to the best performing systems at
the CoNLL shared tasks, our models exploit a rel-
atively smaller set of features and perform signifi-
cantly worse. Nevertheless, performance achieved
by our system are comparable with those reported
by other ME-based systems taking part in the eval-
uations.
Extensive experiments on named-entity recog-
nition and text chunking have provided support to
the following claims:
J.N. Darroch and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Liner models. Annals of Math-
ematical Statistics, 43:1470–1480.
• The introduction of real-valued features dras-
tically reduces the number of parameters of
the ME model with a small loss in perfor-
mance.
• The leave-one-out method is significantly
more effective than the held-out method for
training ME models including real-valued
features.
• The combination of binary and real-valued
features can lead to better ME models. In par-
ticular, state-of-the-art ME models with bi-
nary features are significantly improved by
adding complex real-valued features which
model long-span lexical dependencies.
Finally, the GIS training algorithm does not
seem to be the optimal choice for ME models in-
cluding real-valued features. Future work will in-
vestigate variants of and alternatives to the GIS
algorithm. Preliminary experiments on the Base-
Real model showed that training with the Simplex
algorithm (Press et al., 1988) converges to simi-
lar parameter settings 50 times faster than the GIS
algorithm.
</bodyText>
<sectionHeader confidence="0.998732" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.99998125">
This work was partially financed by the Euro-
pean Commission under the project FAME (IST-
2000-29323), and by the Autonomous Province of
Trento under the the FU-PAT project WebFaq.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998891272727273">
O. Bender, F. J. Och, and H. Ney. 2003. Maximum
entropy models for named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 148–151. Edmon-
ton, Canada.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39–72.
A. Borthwick. 1999. A Maximum Entropy approach
to Named Entity Recognition. Ph.D. thesis, Com-
puter Science Department - New York University,
New York, USA.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Techni-
cal Report CMUCS-99-108, Carnegie Mellon Uni-
versity.
M. Federico and N. Bertoldi. 2004. Broadcast news
lm adaptation overtime. Computer Speech and Lan-
guage, 18(4):417–435, October.
D. Klakow. 1998. Log-linear interpolation of language
models. In Proceedings of the International Confer-
ence of Spoken Language P rocessing (ICSLP), Sid-
ney, Australia.
R. Koeling. 2000. Chunking with maximum entropy
models. In Proceedings of CoNLL-2000, pages
139–141, Lisbon, Portugal.
C. D. Manning and H. Schutze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
A. Mikheev. 1998. Feature lattices for maximum en-
tropy modelling. In COLING-ACL, pages 848–854.
H. Ney, U. Essen, and R. Kneser. 1994. On structur-
ing probabilistic dependences in stochastic language
modeling. Computer Speech and Language, 8(1):1–
38.
F.J. Och and H. Ney. 2002. Discriminative training and
maximum entropy models for statistical machin e
translation. In ACL02: Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 295–302, PA, Philadelphia.
W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T.
Vetterling. 1988. Numerical Recipes in C. Cam-
bridge University Press, New York, NY.
</reference>
<page confidence="0.998507">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882390">
<title confidence="0.99988">Maximum Entropy Tagging with Binary and Real-Valued Features</title>
<author confidence="0.999954">Vanessa Sandrini Marcello Federico Mauro</author>
<affiliation confidence="0.96319">ITC-irst - Centro per la Ricerca Scientifica e</affiliation>
<address confidence="0.933031">38050 Povo (Trento) -</address>
<abstract confidence="0.998026772727273">Recent literature on text-tagging reported successful results by applying Maximum Entropy (ME) models. In general, ME taggers rely on carefully selected binary features, which try to capture discriminant information from the training data. This paper introduces a standard setting of binary features, inspired by the literature on named-entity recognition and text chunking, and derives corresponding realvalued features based on smoothed logprobabilities. The resulting ME models have orders of magnitude fewer parameters. Effective use of training data to estimate features and parameters is achieved by integrating a leaving-one-out method into the standard ME training algorithm. Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binaryfeature models with real-valued features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bender</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Maximum entropy models for named entity recognition.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>148--151</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2435" citStr="Bender et al., 2003" startWordPosition="367" endWordPosition="370"> Darroch and Ratcliff (1972). Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functionsl, popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). Experiments were conducted on t</context>
<context position="4373" citStr="Bender et al., 2003" startWordPosition="705" endWordPosition="708">t conditional probability of tag ct given the context (ct−1 1 , wT1 ), i.e. the entire sequence of words and the full sequence of previous tags. Typically, independence assumptions are introduced in order to reduce the context size. While this introduces some approximations in the probability distribution, it considerably reduces data sparseness in the sampling space. For this reason, the context is limited here to the two previous tags (ct−1 t−2) and to four words around the current word (wt+2 t−2). Moreover, limiting the context to the two previous tags permits to apply dynamic programming (Bender et al., 2003) to efficiently solve the maximization (2). Let y = ct denote the class to be guessed (y E Y) at time t and x = ct−1 t−2, wt+2 t−2 its context (x E X). The generic ME model results: exp(En i=1 λifi(x, y)) pA(y |x) = (3) Ey, exp(Eni= 1 λifi(x, y0)) The n feature functions fi(x, y) represent any kind of information about the event (x, y) which can be useful for the classification task. Typically, binary features are employed which model the verification of simple events within the target class and the context. In Mikheev (1998), binary features for text tagging are classified into two broad clas</context>
</contexts>
<marker>Bender, Och, Ney, 2003</marker>
<rawString>O. Bender, F. J. Och, and H. Ney. 2003. Maximum entropy models for named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 148–151. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1183" citStr="Berger et al., 1996" startWordPosition="163" endWordPosition="166">ity recognition and text chunking, and derives corresponding realvalued features based on smoothed logprobabilities. The resulting ME models have orders of magnitude fewer parameters. Effective use of training data to estimate features and parameters is achieved by integrating a leaving-one-out method into the standard ME training algorithm. Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binaryfeature models with real-valued features. 1 Introduction The Maximum Entropy (ME) statistical framework (Darroch and Ratcliff, 1972; Berger et al., 1996) has been successfully deployed in several NLP tasks. In recent evaluation campaigns, e.g. DARPA IE and CoNLL 2000-2003, ME models reached state-of-the-art performance on a range of text-tagging tasks. With few exceptions, best ME taggers rely on carefully designed sets of features. Features correspond to binary functions, which model events, observed in the (annotated) training data and supposed to be meaningful or discriminative for the task at hand. Hence, ME models result in a loglinear combination of a large set of features, whose weights can be estimated by the well known Generalized Ite</context>
<context position="14442" citStr="Berger et al., 1996" startWordPosition="2449" endWordPosition="2452">ures. The reason is twofold: on one hand, real-valued features allow to capture complex information with fewer parameters; on the other hand, binary features permit to keep a good level of granularity over salient characteristics. Hence, finding a compromise between binary and real-valued features 4 is known that the GIS algorithm requires feature functions fi(x, y) to be non-negative. Hence, features were re-scaled as follows: might help to develop ME models which better trade-off complexity vs. granularity of information. 6 Parameter Estimation From the duality of ME and maximum likelihood (Berger et al., 1996), optimal parameters λ∗ for model (3) can be found by maximizing the log-likelihood function over a training sample {(xt, yt) : t = 1,... , N}, i.e.: log pλ(yt|xt). (7) Now, whereas binary features take only two values and do not need any estimation phase, conditional probability features have to be estimated on some data sample. The question arises about how to efficiently use the available training data in order to estimate the parameters and the feature distributions of the model, by avoiding over-fitting. Two alternative techniques, borrowed from statistical language modeling, have been co</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>A Maximum Entropy approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department - New York University,</institution>
<location>New York, USA.</location>
<contexts>
<context position="2452" citStr="Borthwick, 1999" startWordPosition="371" endWordPosition="373"> (1972). Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functionsl, popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). Experiments were conducted on two publicly avail</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>A. Borthwick. 1999. A Maximum Entropy approach to Named Entity Recognition. Ph.D. thesis, Computer Science Department - New York University, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="19544" citStr="Chen and Rosenfeld, 1999" startWordPosition="3255" endWordPosition="3258"> and α = 0.1. Henceforth, claimed differences in precision or recall will have their corresponding significance level shown in parenthesis. 7.2 Settings and Baseline Models Feature selection and setting for ME models is an art. In these experiments we tried to use the same set of features with minor modifications across both tasks. In particular, used features and their settings are shown in Table 3. Training of models with GIS and estimation of feature distributions used in-house developed toolkits. Performance of binary feature models was improved by smoothing features with Gaussian priors (Chen and Rosenfeld, 1999) with mean zero and standard deviation σ = 4. In general, tuning of models was carried out on a development set. Most of the comparative experiments were performed on the NER task. Three baseline models using atomic features Lex, Syn, and Tran were investigated first: model BaseBin, with all binary features; model BaseReal, with all real-valued features plus the prior feature; model BaseMix, with real-valued Lex and binary Tran and Syn. Models BaseReal and BaseMix were trained with the held-out method. In particular, feature distributions were estimated on the training data while ME parameters</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. Chen and R. Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>N Bertoldi</author>
</authors>
<title>Broadcast news lm adaptation overtime.</title>
<date>2004</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="15691" citStr="Federico and Bertoldi, 2004" startWordPosition="2653" endWordPosition="2656">t and the Leave-one-out methods (Manning and Schutze, 1999). Held-out method. The training sample S is split into two parts used, respectively, to estimate the feature distributions and the ME parameters. Leave-one-out. ME parameters and feature distributions are estimated over the same sample S. The idea is that for each addend in eq. (7), the corresponding sample point (xt, yt) is removed from the training data used to estimate the feature distributions of the model. In this way, it can be shown that occurrences of novel observations are simulated during the estimation of the ME parameters (Federico and Bertoldi, 2004). In our experiments, language modeling smoothing techniques (Manning and Schutze, 1999) were applied to estimate feature distributions pi(y|x). In particular, smoothing was based on the discounting method in Ney et al. (1994) combined to interpolation with distributions using less context. Given the small number of smoothing parameters involved, leave-one-out probabilities were approximated by just modifying count statistics on the fly (Federico and Bertoldi, 2004). The rationale is that smoothing parameters do not change significantly after removing just one sample point. For parameter estim</context>
</contexts>
<marker>Federico, Bertoldi, 2004</marker>
<rawString>M. Federico and N. Bertoldi. 2004. Broadcast news lm adaptation overtime. Computer Speech and Language, 18(4):417–435, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
</authors>
<title>Log-linear interpolation of language models.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference of Spoken Language P rocessing (ICSLP),</booktitle>
<location>Sidney, Australia.</location>
<contexts>
<context position="11616" citStr="Klakow, 1998" startWordPosition="1993" endWordPosition="1994">) Table 2: Corresponding standard set of real-values features. 4 Real-valued Features A binary feature can be seen as a probability measure with support set made of a single event. According to this point of view, we might easily extend binary features to probability measures defined over larger event spaces. In fact, it results convenient to introduce features which are logarithms of conditional probabilities. It can be shown that in this way linear constraints of the ME model can be interpreted in terms of KullbackLeibler distances between the target model and the conditional distributions (Klakow, 1998). Let p1(y|x),p2(y|x),...,pn(y|x) be n different conditional probability distributions estimated on the training corpus. In our framework, each conditional probability pi is associated to a feature fi which is defined over a subspace [X]i x Y of the sample space X x Y. Hence, pi(y|x) should be read as a shorthand of p(y |[x]i). The corresponding real-valued feature is: fi(x, y) = logpi(y |x). (4) In this way, the ME in Eq. (3) can be rewritten as: �n i pi(y|x)λi pλ(y|x) = . (5) Ey&apos; Hi pi(y |x)λ z According to the formalism adopted in Eq. (4), real-valued features assume the following form: fi(</context>
</contexts>
<marker>Klakow, 1998</marker>
<rawString>D. Klakow. 1998. Log-linear interpolation of language models. In Proceedings of the International Conference of Spoken Language P rocessing (ICSLP), Sidney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Koeling</author>
</authors>
<title>Chunking with maximum entropy models.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000,</booktitle>
<pages>139--141</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2468" citStr="Koeling, 2000" startWordPosition="374" endWordPosition="375">ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functionsl, popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). Experiments were conducted on two publicly available benchmarks </context>
</contexts>
<marker>Koeling, 2000</marker>
<rawString>R. Koeling. 2000. Chunking with maximum entropy models. In Proceedings of CoNLL-2000, pages 139–141, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2038" citStr="Manning and Schutze, 1999" startWordPosition="302" endWordPosition="305">E taggers rely on carefully designed sets of features. Features correspond to binary functions, which model events, observed in the (annotated) training data and supposed to be meaningful or discriminative for the task at hand. Hence, ME models result in a loglinear combination of a large set of features, whose weights can be estimated by the well known Generalized Iterative Scaling (GIS) algorithm by Darroch and Ratcliff (1972). Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functionsl, popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distrib</context>
<context position="13663" citStr="Manning and Schutze, 1999" startWordPosition="2326" endWordPosition="2329">ng real-valued features is the drastic reduction of model parameters. For example, each complex lexical features discussed before introduce just one parameter. Hence, the small number of parameters eliminates the need of smoothing the ME estimates. Real-valued features present some drawbacks. Their level of granularity, or discrimination, might result much lower than their binary variants. For many features, it might result difficult to compute reliable probability values due to data sparseness. For the last issue, smoothing techniques developed for statistical language models can be applied (Manning and Schutze, 1999). 5 Mixed Feature Models This work, beyond investigating the use of realvalued features, addresses the behavior of models combining binary and real-valued features. The reason is twofold: on one hand, real-valued features allow to capture complex information with fewer parameters; on the other hand, binary features permit to keep a good level of granularity over salient characteristics. Hence, finding a compromise between binary and real-valued features 4 is known that the GIS algorithm requires feature functions fi(x, y) to be non-negative. Hence, features were re-scaled as follows: might hel</context>
<context position="15122" citStr="Manning and Schutze, 1999" startWordPosition="2558" endWordPosition="2561">maximizing the log-likelihood function over a training sample {(xt, yt) : t = 1,... , N}, i.e.: log pλ(yt|xt). (7) Now, whereas binary features take only two values and do not need any estimation phase, conditional probability features have to be estimated on some data sample. The question arises about how to efficiently use the available training data in order to estimate the parameters and the feature distributions of the model, by avoiding over-fitting. Two alternative techniques, borrowed from statistical language modeling, have been considered: the Held-out and the Leave-one-out methods (Manning and Schutze, 1999). Held-out method. The training sample S is split into two parts used, respectively, to estimate the feature distributions and the ME parameters. Leave-one-out. ME parameters and feature distributions are estimated over the same sample S. The idea is that for each addend in eq. (7), the corresponding sample point (xt, yt) is removed from the training data used to estimate the feature distributions of the model. In this way, it can be shown that occurrences of novel observations are simulated during the estimation of the ME parameters (Federico and Bertoldi, 2004). In our experiments, language </context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. D. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>Feature lattices for maximum entropy modelling.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>848--854</pages>
<contexts>
<context position="4904" citStr="Mikheev (1998)" startWordPosition="806" endWordPosition="807">to the two previous tags permits to apply dynamic programming (Bender et al., 2003) to efficiently solve the maximization (2). Let y = ct denote the class to be guessed (y E Y) at time t and x = ct−1 t−2, wt+2 t−2 its context (x E X). The generic ME model results: exp(En i=1 λifi(x, y)) pA(y |x) = (3) Ey, exp(Eni= 1 λifi(x, y0)) The n feature functions fi(x, y) represent any kind of information about the event (x, y) which can be useful for the classification task. Typically, binary features are employed which model the verification of simple events within the target class and the context. In Mikheev (1998), binary features for text tagging are classified into two broad classes: atomic and complex. Atomic features tell information about the current tag and one single item (word or tag) of the context. Complex features result as a combination of two or more atomic features. In this way, if the grouped events are not independent, complex features should capture higher correlations or dependencies, possibly useful to discriminate. In the following, a standard set of binary features is presented, which is generally employed for text-tagging tasks. The reader familiar with the topic can directly chec</context>
</contexts>
<marker>Mikheev, 1998</marker>
<rawString>A. Mikheev. 1998. Feature lattices for maximum entropy modelling. In COLING-ACL, pages 848–854.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<title>On structuring probabilistic dependences in stochastic language modeling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>38</pages>
<contexts>
<context position="15917" citStr="Ney et al. (1994)" startWordPosition="2687" endWordPosition="2690"> feature distributions are estimated over the same sample S. The idea is that for each addend in eq. (7), the corresponding sample point (xt, yt) is removed from the training data used to estimate the feature distributions of the model. In this way, it can be shown that occurrences of novel observations are simulated during the estimation of the ME parameters (Federico and Bertoldi, 2004). In our experiments, language modeling smoothing techniques (Manning and Schutze, 1999) were applied to estimate feature distributions pi(y|x). In particular, smoothing was based on the discounting method in Ney et al. (1994) combined to interpolation with distributions using less context. Given the small number of smoothing parameters involved, leave-one-out probabilities were approximated by just modifying count statistics on the fly (Federico and Bertoldi, 2004). The rationale is that smoothing parameters do not change significantly after removing just one sample point. For parameter estimation, the GIS algorithm by Darroch and Ratcliff (1972) was applied. It 1+E fi(x, y) = log pi(y|x) + log , (8) min pi where c is a small positive constant and the denominator is a constant term defined by: minpi = (x minSpi(y|</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>H. Ney, U. Essen, and R. Kneser. 1994. On structuring probabilistic dependences in stochastic language modeling. Computer Speech and Language, 8(1):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machin e translation.</title>
<date>2002</date>
<booktitle>In ACL02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>PA, Philadelphia.</location>
<contexts>
<context position="2256" citStr="Och and Ney, 2002" startWordPosition="338" endWordPosition="341">. Hence, ME models result in a loglinear combination of a large set of features, whose weights can be estimated by the well known Generalized Iterative Scaling (GIS) algorithm by Darroch and Ratcliff (1972). Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functionsl, popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each mode</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F.J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machin e translation. In ACL02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302, PA, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Press</author>
<author>B P Flannery</author>
<author>S A Teukolsky</author>
<author>W T Vetterling</author>
</authors>
<title>Numerical Recipes in C.</title>
<date>1988</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<marker>Press, Flannery, Teukolsky, Vetterling, 1988</marker>
<rawString>W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. 1988. Numerical Recipes in C. Cambridge University Press, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>