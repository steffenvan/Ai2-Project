<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9991755">
Aligning Predicate Argument Structures in Monolingual Comparable Texts:
A New Corpus for a New Task
</title>
<author confidence="0.982183">
Michael Roth and Anette Frank
</author>
<affiliation confidence="0.981978">
Department of Computational Linguistics
Heidelberg University
</affiliation>
<address confidence="0.551749">
Germany
</address>
<email confidence="0.996458">
{mroth,frank}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999875821428572">
Discourse coherence is an important aspect of
natural language that is still understudied in
computational linguistics. Our aim is to learn
factors that constitute coherent discourse from
data, with a focus on how to realize predicate-
argument structures (PAS) in a model that ex-
ceeds the sentence level. In particular, we aim
to study the case of non-realized arguments
as a coherence inducing factor. This task can
be broken down into two subtasks. The first
aligns predicates across comparable texts, ad-
mitting partial argument structure correspon-
dence. The resulting alignments and their con-
texts can then be used for developing a coher-
ence model for argument realization.
This paper introduces a large corpus of com-
parable monolingual texts as a prerequisite for
approaching this task, including an evaluation
set with manual predicate alignments. We il-
lustrate the potential of this new resource for
the empirical investigation of discourse coher-
ence phenomena. Initial experiments on the
task of predicting predicate alignments across
text pairs show promising results. Our findings
establish that manual and automatic predicate
alignments across texts are feasible and that
our data set holds potential for empirical re-
search into a variety of discourse-related tasks.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999851384615385">
Research in the fields of discourse and pragmatics
has led to a number of theories that try to explain and
formalize the effect of discourse coherence induc-
ing elements either locally or globally. For exam-
ple, Centering Theory (Grosz et al., 1995) provides
a framework to model local coherence by relating
the choice of referring expressions to the salience of
an entity at certain stages of a discourse. An exam-
ple for a global coherence model would be Rhetori-
cal Structure Theory (Mann and Thompson, 1988),
which addresses overall text structure by means of
coherence relations between the parts of a text.
In addition to such theories, computational ap-
proaches have been proposed to capture correspond-
ing phenomena empirically. A prominent example
is the entity-based model by Barzilay and Lapata
(2008). In their approach, local coherence is mod-
eled by the observation of sentence-to-sentence re-
alization patterns of individual entities. The learned
model reflects a key idea from Centering Theory,
namely that adjacent sentences in a coherent dis-
course are likely to involve the same entities.
One shortcoming of Barzilay and Lapata’s model
(and extensions of it) is that it only investigates overt
realization patterns in terms of grammatical func-
tions. These functions reflect explicit realizations of
predicate argument structures (PAS), but they do not
capture the full range of salience factors. In partic-
ular, the model does not reflect the importance of
discourse entities that fill core roles of the predicate,
but that remain implicit in the predicate’s local argu-
ment structure. We develop a specific set-up that al-
lows us to further investigate the factors that govern
such a null-instantiations of argument positions (cf.
Fillmore et al. (2003)), as a special form of coher-
ence inducing element in discourse. We henceforth
refer to such cases as non-realized arguments.
Our main hypothesis is that context specific re-
alization patterns for PAS can be automatically
</bodyText>
<page confidence="0.974436">
218
</page>
<note confidence="0.9730975">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999303607142857">
learned from a semantically parsed corpus of com-
parable text pairs. This assumption builds on
the success of previous research, where compara-
ble and parallel texts have been exploited for a
range of related learning tasks, e.g., unsupervised
discourse segmentation (Barzilay and Lee, 2004)
and bootstrapping semantic analyzers (Titov and
Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can investi-
gate the factors that determine discourse coherence
in the realization patterns for the involved partici-
pants. As a first step towards this overall goal, we
describe the construction of a resource that contains
more than 160,000 document pairs that are known to
talk about the same events and participants. Exam-
ple (1), extracted from our corpus of aligned texts,
illustrates this point: Both texts report on the same
event, in particular the (aligned) event of locating
victims in an avalanche. While (1.a) explicitly talks
about the location of this event, the role remains im-
plicit in the second sentence of (1.b), given that it
can be recovered from the preceding sentence. In
fact, realization of this argument would impede the
fluency of discourse by being overly repetitive.
</bodyText>
<figureCaption confidence="0.766176">
(1) a. ...The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
</figureCaption>
<bodyText confidence="0.9749713">
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
Our aim is to identify comparable predications
across pairs of texts, and to study the coherence
factors that determine the realization patterns of ar-
gument structures (including roles that remain im-
plicit) in discourse. This can be achieved by consid-
ering the full set of arguments that can be recovered
from the aligned predications, including both core
and non-core (i.e. adjunct) roles. However, in order
to relate PAS across texts to one another, we first
need to identify corresponding predicates.
In this paper, we construct a large data set to be
used for the induction of a coherence model for ar-
gument structure realization and related tasks. We
discuss the prospects of this data set for the study
of coherence factors in PAS realization. Finally, we
present first results on the initial task of predicate
alignment across comparable monolingual texts.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 introduces the new task to-
gether with a description of how we prepared a suit-
able data set. Section 4 discusses the potential bene-
fits of the created resource in more detail. Section 5
presents experiments on predicate alignment using
this new data set and outlines first results. Finally,
we conclude in Section 6 and discuss future work.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999777896551724">
Data sets comprising parallel texts have been re-
leased for various different tasks, including para-
phrase extraction and statistical machine translation
(SMT). While corpora for SMT are typically mul-
tilingual (e.g. Europarl, Koehn (2005)), there also
exist monolingual parallel corpora that consist of
multiple translations of one text into the same lan-
guage (Barzilay and McKeown, 2001; Huang et
al., 2002, inter alia). Each translation can pro-
vide alternative verbalizations of the same events
but little variation can be observed in context, as
the overall discourse remains the same. A higher
degree of variation can be found in the Microsoft
Research Paraphrase Corpus (e.g. MSRPC, Dolan
and Brockett (2005)), which consists of paraphrases
automatically extracted from different sources. In
the MSRPC, however, original discourse contexts
are not provided for each sentence. In contrast to
truly parallel monolingual corpora, there also exist
a range of comparable corpora that have been used
for tasks such as (multi-document) summarization
(McKeown and Radev, 1995, inter alia). Corpora for
this task are collected manually and hence are rather
small. Our work presents a method to automatically
construct a large corpus of text pairs describing the
same underlying events.
In this novel corpus, we identify common events
across texts and investigate the argument structures
that were realized in each context to establish a co-
</bodyText>
<page confidence="0.998243">
219
</page>
<bodyText confidence="0.999958863636364">
herent discourse. Different aspects related to this
setting have been studied in previous work. For ex-
ample, Filippova and Strube (2007) and Cahill and
Riester (2009) examine factors that determine con-
stituent order and Belz et al. (2009) study the con-
ditions for the use of different types of referring ex-
pressions. The specific set-up we examine allows
us to further investigate the factors that govern the
non-realization of an argument position, as a special
form of coherence inducing element in discourse.
As in the aforementioned work, we are specifically
interested in the generation of coherent discourses
(e.g. for summarization). Yet, our work also com-
plements research in discourse analysis. A recent
example for such work is the Semeval 2010 Task 10
(Ruppenhofer et al., 2010), which aims at linking
events and their participants in discourse. The pro-
vided data sets for this task, however, are critically
small (438 train and 525 test sentences). Eventu-
ally, the corpus we present in this paper could also
be beneficial for data-driven approaches to role link-
ing in discourse.
</bodyText>
<sectionHeader confidence="0.84776" genericHeader="method">
3 A Corpus for Aligning Predications
across Comparable Texts
</sectionHeader>
<bodyText confidence="0.999957538461539">
Our aim is to construct a corpus of comparable texts
that can be assumed to be about the same events,
but include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources for the same
underlying events.
The English Gigaword Fifth Edition (Parker et al.,
2011) corpus (henceforth just Gigaword) is one of
the largest corpus collections for English. It com-
prises a total of 9.8 million newswire articles from
seven distinct sources. For construction of our cor-
pus we make use of all combinations of agency pairs
in Gigaword.
</bodyText>
<subsectionHeader confidence="0.999269">
3.1 Corpus Creation
</subsectionHeader>
<bodyText confidence="0.999978612903226">
In order to extract pairs of articles describing the
same news event, we implemented the pairwise sim-
ilarity method presented by Wubben et al. (2009).
The method is based on measuring word overlap in
news headlines, weighting each word by its TF*IDF
score to give a higher impact to words occurring
with lower frequency. As our focus is to provide
a high-quality data set for predicate alignment and
follow-up tasks, we impose an additional date con-
straint to favor precision over recall. We apply this
constraint by requiring a pair of articles to be pub-
lished within a two-day time frame in order to be
considered as pairs of comparable news items.
Following this two-step procedure, we extracted a
total of 167,728 document pairs, an overall collec-
tion of 50 million word tokens. We inspected about
100 randomly selected document pairs and found
only two of them describing different events. This
is in line with the results of Wubben et al. who re-
ported a precision of 93% without explicitly impos-
ing a date constraint. Overall, we found that most
text pairs share a high degree of similarity and vary
only in length (up to 7.564 words with a mean and
median of 301 and 213 words, respectively) and de-
tail. Closer examination of a development set of
10 document pairs (described below) revealed that
we can indeed find multiple cases where roles are
not locally filled in predicate argument structures.
We show instances of this phenomenon, in which
aligned PAS help to resolve implicit role references,
in Section 4.
</bodyText>
<subsectionHeader confidence="0.999788">
3.2 Gold Standard Annotation
</subsectionHeader>
<bodyText confidence="0.999961777777778">
We pre-processed all texts using MATE tools
(Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline
of natural language processing modules including a
state-of-the-art semantic role labeler that computes
Prop/NomBank annotations (Palmer et al., 2005;
Meyers et al., 2008). The output was used to provide
pre-labeled verbal and nominal predicates for anno-
tation. We asked two students1 to tag alignments
of corresponding predicates in 70 text pairs derived
from the created corpus. All document pairs were
randomly chosen from the AFP and APW sections
of Gigaword with the constraint that each text con-
sists of 100 to 300 words2. We chose this constraint
as longer text pairs contain a high number of unre-
lated predicates, making this task difficult to manage
for the annotators.
Sure and possible links. Following standard prac-
tice in word alignment tasks (cf. Cohn et al. (2008))
</bodyText>
<footnote confidence="0.996574333333333">
1Both annotators are students in Computational Linguistics,
one undergraduate (A) and one postgraduate (B) student.
2This constraint is satisfied by 75.3% of the documents.
</footnote>
<page confidence="0.995417">
220
</page>
<bodyText confidence="0.865688909090909">
the annotators were instructed to distinguish be-
tween sure (S) and possible (P) alignments, depend-
ing on how certainly, in their opinion, two predi-
cates (including their arguments) describe the same
event. The following examples show cases of predi-
cate pairings marked as sure (S link) (2) and as pos-
sible (P link) alignments (3):
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [... ]3
b. The authority [... ] had already approved a sim-
ilar application by Nasdaq.4
</bodyText>
<listItem confidence="0.977759666666667">
(3) a. Myanmar’s military government said earlier this
year it has released some 220 political prisoners
[...]5
</listItem>
<bodyText confidence="0.982220470588235">
b. The government has been regularly releasing
members of Suu Kyi’s National League for
Democracy party [... ]6
Replaceability. As a guideline for deciding
whether two predicates are to be aligned, the
annotators were given the following two criteria: 1)
whether the predicates are replaceable in a given
context and 2) whether they share (potentially
implicit) arguments.
Missing context. In case one text does not provide
enough context to decide whether two predicates in
the paired documents refer to the same event, an
alignment should not be marked as sure.
Similar predicates. Annotators were told explic-
itly that sure links can be used even if two predicates
are semantically different but have the same mean-
ing in context. Example (4) illustrates such a case:
</bodyText>
<listItem confidence="0.9955435">
(4) a. The volcano roared back to life two weeks ago.
b. It began erupting last month.
</listItem>
<bodyText confidence="0.966257285714286">
1-to-1 vs. n-to-m. We asked the annotators to find
as many 1-to-1 correspondences as possible and to
prefer 1-to-1 matches over n-to-m alignments. In
case of multiple mentions (cf. Example (5)) of the
same event, we further asked the annotators to pro-
vide only one S link per predicate and mark remain-
ing cases as P links. If possible, the S link should
</bodyText>
<footnote confidence="0.99955175">
3Source document ID: AFP ENG 20071112.0235
4Source document ID: APW ENG 20071112.0645
5Source document ID: AFP ENG 20020301.0041
6Source document ID: APW ENG 20020301.0132
</footnote>
<bodyText confidence="0.860940789473684">
be used for the pairing of PAS with the highest in-
formation overlap (e.g. “performa3”–“performb2” in
(5)). If there is no difference in information over-
lap, the predicate pair that occurs first in both texts
should be marked as a sure alignment (e.g. “singa,”–
“performb,” in (5)). The intuition behind this guide-
line is that the first mention introduces the actual
event while later mentions just (co-)refer or add fur-
ther information.
(5) a. Susan Boyle said she will singa, in front of
Britain’s Prince Charles (... ) “It’s going to be
a privilege to be performinga2 before His Royal
Highness,” the singer said (... ) British copy-
right laws will allow her to performa3 the hit in
front of the prince and his wife.7
b. British singing sensation Susan Boyle is going
to performb, for Prince Charles (... ) The show
star will performb2 her version of Perfect Day
for Charles and his wife Camilla.8
</bodyText>
<subsectionHeader confidence="0.995902">
3.3 Development and Evaluation Data Sets
</subsectionHeader>
<bodyText confidence="0.999954291666667">
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86. Following Brockett (2007),
we computed agreement on labeled annotations, in-
cluding unaligned predicate pairs as an additional
null category. For the construction of a gold stan-
dard, we merged the alignments from both annota-
tors by taking the union of all possible alignments
and the intersection of all sure alignments. Cases
which involved a sure alignment on which the anno-
tators disagreed were resolved in a group discussion
with the first author. We split the final corpus into a
development set of 10 document pairs and a test set
of 60 document pairs.
Table 1 summarizes information about the result-
ing annotations in the development and test sets,
respectively. It gives information about the paired
texts (PT): number of predicates marked in prepro-
cessing (nouns and verbs), the set of manual predi-
cate alignments (PA): sure and possible, as well as
information about whether they were annotated for
predicates of the same PoS (N,V) or lemma.
Finally, as a rough indicator for diverging ar-
gument structures captured in the annotated align-
</bodyText>
<footnote confidence="0.99672">
7Source document ID: AFP ENG 20101102.0028
8Source document ID: APW ENG 20101102.0923
</footnote>
<page confidence="0.981324">
221
</page>
<table confidence="0.9568655">
Dev Set Test Set
nb. of PT 10 60
nb. marked predicates 395 3,453
nb. marked nouns 168 1,531
nb. marked verbs 227 1,922
sure PA/PT: avg. (total) 3.9 (35) 7.4 (446)
poss. PA/PT: avg. (total) 4.8 (43) 6.0 (361)
same PoS in PA (N/V) 88.5% (24/42) 82.4% (242/423)
same lemma in PA 53.8% (42) 47.5% (383)
unequal nb. args in PA 30.8% (24) 39.7% (320)
</table>
<tableCaption confidence="0.997695">
Table 1: Information on Paired Texts (PT) and manual
Predicate Alignments (PA) in development and test set
</tableCaption>
<bodyText confidence="0.986968">
ments, we analyzed the number of PAs that involve
a different number of arguments.
</bodyText>
<sectionHeader confidence="0.984685" genericHeader="method">
4 Potential of Aggregation
</sectionHeader>
<bodyText confidence="0.999848368421053">
In this section, we analyze the predicate alignments
in our manually annotated data set, to illustrate the
potential of aggregating corresponding PAS across
comparable texts.
We are particularly interested in cases of non-
realization of arguments, and thus take a closer look
at alignments involving roles that are not filled in
their local PAS. We extract a subset of such cases
by extracting pairs of aligned predicates that con-
tain a different number of realized arguments. We
deliberately focus on the more restricted core roles
in this exposition, but will consider the full range
of roles for developing a comprehensive coherence
model for argument structure realization.9 Our se-
lection of alignment examples is drawn from the de-
velopment set.
The following excerpts are from a pair of com-
parable texts describing a news report on Chadian
refugees crossing into Nigeria:
</bodyText>
<listItem confidence="0.6910155">
(6) a. The Chadians said [they]Arg0 had fled [ ]Arg1 in
fear of their lives.10
</listItem>
<subsectionHeader confidence="0.135242">
b. The United Nations says
</subsectionHeader>
<bodyText confidence="0.428603">
[some 20,000 refugees]Arg0 have fled
[into Cameroon]Arg1.11
In both examples, the Arg0 role of the predicate fled
is filled, but Arg1 has not been realized in (6.a). Note
</bodyText>
<footnote confidence="0.7910605">
9Accordingly, the number of PAs involving diverging role
realizations in Table 1 is strongly underestimated.
10Source document ID: AFP ENG 20080205.0230
11Source document ID: APW ENG 20080206.0766
</footnote>
<bodyText confidence="0.99914775">
that the sentence is still part of a coherent discourse
as fillers for the omitted role can be inferred from
the preceding discourse context. Aggregating the
aligned PAS presents an effective means to identify
such appropriate fillers.
Example (7) presents another text pair, reporting
on elections in Iraq, in which role realizations differ
for the same hold event.
</bodyText>
<listItem confidence="0.768405">
(7) a. He said (... ) [elections]Arg1 will be held [ ]Arg0
to form a government.12
</listItem>
<bodyText confidence="0.892102666666667">
b. The president (... ) said Wednesday
[his country]Arg0 will definitely hold
[elections]Arg1 in 2004.13
Here, the changes in argument realization go
along with a diathesis alternation, while the pair in
(6) exemplifies a case of lexical licensing for omis-
sion of a role.14
Example (8.b) illustrates a case in which the Arg1
of a decline event is involved in a preceding pred-
ication (rise) and thus has already been overtly re-
alized. The constructional properties of the subse-
quent predicates decline as a participle and noun, re-
spectively, are more adverse to overt realization of
the Arg1 role. Suppression of Arg1 in such cases
yields a much more coherent discourse as compared
to their realization. This is brought out by the con-
structed examples in (a’/b’), which are both highly
repetitive.
</bodyText>
<listItem confidence="0.821908571428572">
(8) a. The closely watched [index]Arg1 rose to 93.7
... after declining for ... months.15
a’. ? ... after the index declining for ... months.
b. Consumer confidence rose ... following three
months of dramatic decline [ ]Arg1.16
b’. ? ... following three months of dramatic decline
[of consumer confidence]Arg1.
</listItem>
<bodyText confidence="0.999145333333333">
As showcased by the previous examples, the de-
cision on whether to realize a role filler in a lo-
cal PAS can be rather complex. Obviously, the
</bodyText>
<footnote confidence="0.948629714285714">
12Source document ID: AFP ENG 20031015.0353
13Source document ID: APW ENG 20031015.0236
14These different configurations are termed constructional
vs. lexical licensors in the SemEval 2010 Task 10 (Ruppen-
hofer et al., 2010).
15Source document ID: AFP ENG 20011228.0365
16Source document ID: APW ENG 20011228.0572
</footnote>
<page confidence="0.993404">
222
</page>
<figureCaption confidence="0.957815333333333">
Figure 1: The predicates of two sentences (white: “The company has said it plans to restate its earnings for 2000
through 2002.”; gray: “The company had announced in January that it would have to restate earnings (...)”) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
</figureCaption>
<bodyText confidence="0.9999424">
above instances do not provide exhaustive informa-
tion for grounding all such decisions. A comprehen-
sive model of discourse coherence will need to esti-
mate the argument realization potential of different
predicates and roles from larger corpora. But as can
be seen from the discussed examples, training a se-
mantic model with suitable discourse features on all
predicate argument structures in a large corpus such
as ours will provide indicative range of realization
decisions.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999834">
This section presents an initial experiment using an
unsupervised graph-based clustering method for the
task of aligning predicates across comparable texts.
We describe the alignment model, two baselines as
well as the experimental setting and results.17
</bodyText>
<subsectionHeader confidence="0.970078">
5.1 Clustering Model
</subsectionHeader>
<bodyText confidence="0.996222684210527">
Similarity Measures. We define a number of sim-
ilarity measures between predicates, which make
use of complementary lexical information. One
source of information are token-based frequency
counts, which we compute over all documents
from the AFP and APW sections of Gigaword18.
Given two lemmatized predicates and their respec-
tive PAS, we employ the following four similarity
17The technicalities of this model, including detailed def-
initions of the similarity measures, are described elsewhere
(manuscript, under submission).
18These sections make up 56.6% of documents in Gigaword.
measures: Similarity in WordNet (simWN) and Verb-
Net (simVN), distributional similarity (simDist) and
bag-of-word similarity of arguments (simArgs). The
first three measures are type-based, whereas the lat-
ter is token-based.
Graph Representation. The input for graph clus-
tering is a bi-partite graph representation for pairs
of texts to be predicate-aligned. In this graph, each
node represents a PAS that was assigned during pre-
processing (cf. Section 3). Edges are inserted be-
tween pairs of predicates that are from two distinct
texts. A weight is assigned to each edge by a com-
bination of the introduced similarity measures.
Clustering algorithm. The graph clustering
method uses minimum cuts (or Mincuts) in order
to partition the bipartite text graph into clusters of
aligned predicates. Each Mincut operation divides
a graph into two disjoint sub-graphs, such that the
sum of weights of removed edges will be minimal.
As the goal is to induce clusters consisting of pairs
of similar predicates, a maximum number of two
nodes per cluster is set as stopping criterion. We
apply Mincut recursively to the input graph and
resulting sub-graphs until we reach the stopping
criterion. Figure 1 shows an example of a graph
clustered by the Mincut approach.
</bodyText>
<subsectionHeader confidence="0.99866">
5.2 Setting
</subsectionHeader>
<bodyText confidence="0.9992275">
We perform evaluations of the graph-based align-
ment model (henceforth called Clustering) on the
</bodyText>
<page confidence="0.99692">
223
</page>
<bodyText confidence="0.999765689655173">
task of inducing predicate alignments across com-
parable monolingual texts. We evaluate on the man-
ually annotated gold alignments in the test data set
described in Section 3.2.
Parameter Tuning. As the graph representation
becomes rather inefficient to handle using edges be-
tween all predicate pairs, we use the development
set of 10 text pairs to estimate a threshold for adding
edges. We found the best similarity threshold to be
an edge weight of 2.5. Note that the edge weights are
calculated as a weighted linear combination of four
different similarity measures. Subsequently, we also
tune the weighting scheme for similarity measures
on the development set. We found the best perform-
ing combination of weights to be 0.09, 0.19, 0.48
and 0.24 for simWN, simVN, simDist and simArgs, re-
spectively.
Baselines. A simple baseline for this task is to
align all predicates whose lemmas are identical
(SameLemma). As a more sophisticated baseline,
we make use of alignment tools commonly used in
statistical machine translation (SMT). We train our
own word alignment model using the state-of-the-art
tool Berkeley Aligner (Liang et al., 2006). As word
alignment tools require pairs of sentences as input,
we first extract paraphrases for this baseline using a
re-implementation of the paraphrase detection sys-
tem by Wan et al. (2006). In the following sections,
we abbreviate this model as WordAlign.
</bodyText>
<subsectionHeader confidence="0.815364">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.9996453125">
Following Cohn et al. (2008) we measure precision
as the number of predicted alignments also anno-
tated in the gold standard divided by the total num-
ber of predictions. Recall is measured as the num-
ber of correctly predicted sure alignments devided
by the total number of sure alignments in the gold
standard. We subsequently compute the Fl-score as
the harmonic mean between precision and recall.
Table 2 presents the results for our model and
the two baselines. From all four approaches,
WordAlign performs worst. We identify two main
reasons for this: On the one hand, the paraphrase
detection does not perform perfectly. Hence, the
extracted sentence pairs do not always contain gold
alignments. On the other hand, even sentence pairs
that contain gold alignments are generally less paral-
</bodyText>
<table confidence="0.999967">
Precision Recall F1
WordAlign 19.7% 15.2% 17.2%
SameLemma 40.3% 60.3% 48.3%
Clustering 59.7% 50.7% 54.8%
</table>
<tableCaption confidence="0.976921">
Table 2: Results for all models on our test set; significant
</tableCaption>
<bodyText confidence="0.9647449375">
improvements (p&lt;0.005) over the results given in each
previous line are marked in bold face.
lel compared to a typical SMT setting, which makes
them harder to align.
We observe that the majority of all sure align-
ments (60.3%) can be retrieved by applying the
SameLemma model, yet at a low precision (40.3%).
While the Clustering model only recalls 50.7% of
all cases, it clearly outperforms SameLemma in
terms of precision (+19.4% points), an important
factor for us as we plan to use the alignments in
subsequent tasks. With 54.8%, Clustering also
achieves the best overall Fl-score. We computed
statistical significance of result differences with a
paired t-test (Cohen, 1995), yielding significance at
the 99.5% level for precision and Fl-score.
</bodyText>
<subsectionHeader confidence="0.999818">
5.4 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999538833333333">
We perform an analysis of the output of the Clus-
tering model on the development set to categorize
correct and incorrect alignment decisions.19 In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors). Six
Type I errors (46%) occurred when the lemma of an
affected predicate occurred more than once in a text
and the model missed the correct link. Vice versa,
we find 18 Type II errors (78%) that were made be-
cause of a high predicate similarity despite low ar-
gument overlap. An example is given in (9).
</bodyText>
<listItem confidence="0.670333">
(9) a. The US alert (... ) followed intelligence reports
that ... 20
</listItem>
<bodyText confidence="0.88437725">
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious ... 21
While argument overlap itself can be low even for
correct alignments, the results clearly indicate that
</bodyText>
<footnote confidence="0.8983602">
19We decided to leave the test set untouched for further exper-
iments. Due to parameter tuning, the results on the development
set also provide us with an upper bound of the proposed model.
20Source document ID: AFP ENG 20101004.0367
21Source document ID: APW ENG 20101004.0207
</footnote>
<page confidence="0.997781">
224
</page>
<bodyText confidence="0.997186166666667">
a better integration of context is necessary: Exam-
ple (10.a) illustrates a case in which the agent of a
warning event is not realized. Here, contextual in-
formation is required to correctly align it to one of
the warning events in (10.b). This involves inference
beyond the local PAS.
</bodyText>
<listItem confidence="0.9625075">
(10) a. The US alert (... ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.20
</listItem>
<bodyText confidence="0.996251">
b. Japan has issued a travel alert ... (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (... ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning ... 21
On the positive side, Clustering achieves a precision
of 61.4% and a recall of 65.7% on the development
set. Example (11) shows a correctly aligned PAS
pair that involves non-realized arguments:
</bodyText>
<listItem confidence="0.731037333333333">
(11) a. ...the Governing Council has established
[a committee]Arg0 to draft [a constitution]Arg1.22
b. A .. resolution calls on the Governing
</listItem>
<bodyText confidence="0.9742784">
Council for elections and the drafting [ ]Arg0
[of anew constitution]Arg1.23
In (11.a), the follow-up sentences will refer back to
the committee that will draft the new Iraqi constitu-
tion, hence the institution has to be introduced in the
discourse at this point. In contrast, excerpt (11.b) is
the last sentence of a news report. Since it presents
a summary, introducing new (omissible) entities at
this point would not concord with general coherence
principles.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998779">
In this paper, we presented a novel corpus of compa-
rable texts that provides full discourse contexts for
alternative verbalizations. The motivation for the
construction of this corpus is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization. A special phe-
nomenon we are interested in are discourse-related
factors that license the omission of argument roles.
Our data set satisfies two conditions that are es-
sential for the purported task: the texts are about
</bodyText>
<footnote confidence="0.8027225">
22Source document ID: AFP ENG 20031015.0353
23Source document ID: APW ENG 20031015.0236.
</footnote>
<bodyText confidence="0.999980641025641">
the same events and constitute alternative verbaliza-
tions. Selected from the Gigaword corpus, the doc-
uments pertain to the news domain, and satisfy the
further constraint that we have access to the full sur-
rounding discourse context. The constructed corpus
could thus be profitable for a range of other tasks
that need to investigate factors for knowledge aggre-
gation, such as summarization, or inference in dis-
course, such as textual entailment.
In total, we derived more than 160,000 document
pairs from all pairwise combinations of newswire
sources in the English Gigaword Fifth Edition. Us-
ing a subset of these pairs, we constructed a devel-
opment and an evaluation data set with gold align-
ments that relate predications with (possibly partial)
PAS correspondence. We established that the anno-
tation task, while difficult, can be performed with
good inter-annotator agreement (r. at 0.86).
We presented first experiments on the task of au-
tomatically predicting predicate alignments. This
step is essential to gather empirical evidence of dif-
ferent PAS realizations for the same event, given
varying discourse contexts. Analysis of the data
shows that the aligned predications capture a wide
variety of sources and variations of coherence ef-
fects, including constructional, lexical and discourse
phenomena.
In future work, we will enhance our model by in-
corporating more refined semantic similarity mea-
sures including discourse-based criteria for estab-
lishing cross-document alignments. Given that our
data set includes sets of aligned documents from
several newswire sources, we will explore transitiv-
ity constraints across multiple document pairs in or-
der to further enhance the precision of the alignment
model. We will then proceed to the ultimate aim of
our work: the development of a coherence model for
argument structure realization, including the design
of an appropriate task and evaluation setting.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999699333333333">
We are grateful to the Landesgraduiertenf¨orderung
Baden-W¨urttemberg for funding within the research
initiative “Coherence in language processing” of
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation and Kathrin Spreyer, Tae-
Gil Noh and Carina Silberer for helpful discussion.
</bodyText>
<page confidence="0.997799">
225
</page>
<sectionHeader confidence="0.996314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984740776699">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2–7 May 2004,
pages 113–120.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics, Toulouse, pages
50–57.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79–87.
Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33–36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89–97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 817–825, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37–46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
23–30 June 2007, pages 320–327.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16.3:235–250.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Philipp Koehn, 2005. Europarl: A parallel corpus for
statistical machine translation, volume 5, pages 79–
86.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104–111.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory. Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Kathleen R. McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of the 18th Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval, Seattle, Wash., 9–13 July 1995, pages
74–82. Reprinted in Advances in Automatic Text Sum-
marization, Mani, I. and Maybury, M.T. (Eds.), Cam-
bridge, Mass.: MIT Press, 1999, pp.381-389.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45–50, Up-
psala, Sweden, July.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
</reference>
<page confidence="0.981194">
226
</page>
<reference confidence="0.998187933333333">
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11–16 July 2010, pages 958–967.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
”Para-farce” out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131–138.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122–
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.997941">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.701413">
<title confidence="0.999387">Aligning Predicate Argument Structures in Monolingual Comparable Texts: A New Corpus for a New Task</title>
<author confidence="0.989869">Roth</author>
<affiliation confidence="0.999186">Department of Computational</affiliation>
<address confidence="0.726195">Heidelberg</address>
<abstract confidence="0.999186379310345">Discourse coherence is an important aspect of natural language that is still understudied in computational linguistics. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicateargument structures (PAS) in a model that exceeds the sentence level. In particular, we aim to study the case of non-realized arguments as a coherence inducing factor. This task can be broken down into two subtasks. The first aligns predicates across comparable texts, admitting partial argument structure correspondence. The resulting alignments and their contexts can then be used for developing a coherence model for argument realization. This paper introduces a large corpus of comparable monolingual texts as a prerequisite for approaching this task, including an evaluation set with manual predicate alignments. We illustrate the potential of this new resource for the empirical investigation of discourse coherence phenomena. Initial experiments on the task of predicting predicate alignments across text pairs show promising results. Our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2341" citStr="Barzilay and Lapata (2008)" startWordPosition="354" endWordPosition="357">lly or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is modeled by the observation of sentence-to-sentence realization patterns of individual entities. The learned model reflects a key idea from Centering Theory, namely that adjacent sentences in a coherent discourse are likely to involve the same entities. One shortcoming of Barzilay and Lapata’s model (and extensions of it) is that it only investigates overt realization patterns in terms of grammatical functions. These functions reflect explicit realizations of predicate argument structures (PAS), but they do not capture the full range of salience factors. </context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<location>Boston, Mass.,</location>
<contexts>
<context position="3979" citStr="Barzilay and Lee, 2004" startWordPosition="604" endWordPosition="607">rse. We henceforth refer to such cases as non-realized arguments. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants. As a first step towards this overall goal, we describe the construction of a resource that contains more than 160,000 document pairs that are known to talk about the sa</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, Mass., 2–7 May 2004, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>50--57</pages>
<location>Toulouse,</location>
<contexts>
<context position="7189" citStr="Barzilay and McKeown, 2001" startWordPosition="1125" endWordPosition="1128">e potential benefits of the created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that hav</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
<author>Jette Viethen</author>
<author>Albert Gatt</author>
</authors>
<title>The grec main subject reference generation challenge 2009: overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation,</booktitle>
<pages>79--87</pages>
<contexts>
<context position="8479" citStr="Belz et al. (2009)" startWordPosition="1325" endWordPosition="1328">d Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a co219 herent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ruppenhofer et al., 2010), which aims at linking events and their particip</context>
</contexts>
<marker>Belz, Kow, Viethen, Gatt, 2009</marker>
<rawString>Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2009. The grec main subject reference generation challenge 2009: overview and evaluation results. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, pages 79–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Bernd Bohnet</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>A high-performance syntactic and semantic dependency parser.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Demonstration Volume,</booktitle>
<pages>33--36</pages>
<location>Beijing, China,</location>
<marker>Bj¨orkelund, Bohnet, Hafdell, Nugues, 2010</marker>
<rawString>Anders Bj¨orkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and semantic dependency parser. In Coling 2010: Demonstration Volume, pages 33–36, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="11618" citStr="Bohnet, 2010" startWordPosition="1851" endWordPosition="1852">imposing a date constraint. Overall, we found that most text pairs share a high degree of similarity and vary only in length (up to 7.564 words with a mean and median of 301 and 213 words, respectively) and detail. Closer examination of a development set of 10 document pairs (described below) revealed that we can indeed find multiple cases where roles are not locally filled in predicate argument structures. We show instances of this phenomenon, in which aligned PAS help to resolve implicit role references, in Section 4. 3.2 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2. We chose this constraint as lon</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the RTE</title>
<date>2007</date>
<institution>Corpus. Microsoft Research.</institution>
<contexts>
<context position="15706" citStr="Brockett (2007)" startWordPosition="2526" endWordPosition="2527">, in front of Britain’s Prince Charles (... ) “It’s going to be a privilege to be performinga2 before His Royal Highness,” the singer said (... ) British copyright laws will allow her to performa3 the hit in front of the prince and his wife.7 b. British singing sensation Susan Boyle is going to performb, for Prince Charles (... ) The show star will performb2 her version of Perfect Day for Charles and his wife Camilla.8 3.3 Development and Evaluation Data Sets In total, the annotators (A/B) aligned 487/451 sure and 221/180 possible alignments with a Kappa score (Cohen, 1960) of 0.86. Following Brockett (2007), we computed agreement on labeled annotations, including unaligned predicate pairs as an additional null category. For the construction of a gold standard, we merged the alignments from both annotators by taking the union of all possible alignments and the intersection of all sure alignments. Cases which involved a sure alignment on which the annotators disagreed were resolved in a group discussion with the first author. We split the final corpus into a development set of 10 document pairs and a test set of 60 document pairs. Table 1 summarizes information about the resulting annotations in t</context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the RTE 2006 Corpus. Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Arndt Riester</author>
</authors>
<title>Incorporating information status into generation ranking.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>817--825</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="8407" citStr="Cahill and Riester (2009)" startWordPosition="1313" endWordPosition="1316">hat have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a co219 herent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ru</context>
</contexts>
<marker>Cahill, Riester, 2009</marker>
<rawString>Aoife Cahill and Arndt Riester. 2009. Incorporating information status into generation ranking. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 817–825, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="15671" citStr="Cohen, 1960" startWordPosition="2521" endWordPosition="2522"> Susan Boyle said she will singa, in front of Britain’s Prince Charles (... ) “It’s going to be a privilege to be performinga2 before His Royal Highness,” the singer said (... ) British copyright laws will allow her to performa3 the hit in front of the prince and his wife.7 b. British singing sensation Susan Boyle is going to performb, for Prince Charles (... ) The show star will performb2 her version of Perfect Day for Charles and his wife Camilla.8 3.3 Development and Evaluation Data Sets In total, the annotators (A/B) aligned 487/451 sure and 221/180 possible alignments with a Kappa score (Cohen, 1960) of 0.86. Following Brockett (2007), we computed agreement on labeled annotations, including unaligned predicate pairs as an additional null category. For the construction of a gold standard, we merged the alignments from both annotators by taking the union of all possible alignments and the intersection of all sure alignments. Cases which involved a sure alignment on which the annotators disagreed were resolved in a group discussion with the first author. We split the final corpus into a development set of 10 document pairs and a test set of 60 document pairs. Table 1 summarizes information a</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical methods for artificial intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="26824" citStr="Cohen, 1995" startWordPosition="4307" endWordPosition="4308">e are marked in bold face. lel compared to a typical SMT setting, which makes them harder to align. We observe that the majority of all sure alignments (60.3%) can be retrieved by applying the SameLemma model, yet at a low precision (40.3%). While the Clustering model only recalls 50.7% of all cases, it clearly outperforms SameLemma in terms of precision (+19.4% points), an important factor for us as we plan to use the alignments in subsequent tasks. With 54.8%, Clustering also achieves the best overall Fl-score. We computed statistical significance of result differences with a paired t-test (Cohen, 1995), yielding significance at the 99.5% level for precision and Fl-score. 5.4 Analysis of Results We perform an analysis of the output of the Clustering model on the development set to categorize correct and incorrect alignment decisions.19 In total, the model missed 13 out of 35 sure alignments (Type I errors) and predicted 23 alignments not annotated in the gold standard (Type II errors). Six Type I errors (46%) occurred when the lemma of an affected predicate occurred more than once in a text and the model missed the correct link. Vice versa, we find 18 Type II errors (78%) that were made beca</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical methods for artificial intelligence. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Chris Callison-Burch</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing Corpora for Development and Evaluation of Paraphrase Systems.</title>
<date>2008</date>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="12437" citStr="Cohn et al. (2008)" startWordPosition="1980" endWordPosition="1983">al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2. We chose this constraint as longer text pairs contain a high number of unrelated predicates, making this task difficult to manage for the annotators. Sure and possible links. Following standard practice in word alignment tasks (cf. Cohn et al. (2008)) 1Both annotators are students in Computational Linguistics, one undergraduate (A) and one postgraduate (B) student. 2This constraint is satisfied by 75.3% of the documents. 220 the annotators were instructed to distinguish between sure (S) and possible (P) alignments, depending on how certainly, in their opinion, two predicates (including their arguments) describe the same event. The following examples show cases of predicate pairings marked as sure (S link) (2) and as possible (P link) alignments (3): (2) a. The regulator ruled on September 27 that Nasdaq too was qualified to bid for OMX [.</context>
<context position="25217" citStr="Cohn et al. (2008)" startWordPosition="4045" endWordPosition="4048"> baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently compute the Fl-score as the harmonic mean between precision and recall. Table 2 presents the results for our model and the two baselines. From all four approaches, WordAlign performs worst. We identify two main reasons for this: On the one hand, the paraphrase detection does not perform perfectly. Henc</context>
</contexts>
<marker>Cohn, Callison-Burch, Lapata, 2008</marker>
<rawString>Trevor Cohn, Chris Callison-Burch, and Mirella Lapata. 2008. Constructing Corpora for Development and Evaluation of Paraphrase Systems. 34(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing.</booktitle>
<contexts>
<context position="7515" citStr="Dolan and Brockett (2005)" startWordPosition="1177" endWordPosition="1180">cluding paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identi</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>William B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Generating constituent order in German clauses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>320--327</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8377" citStr="Filippova and Strube (2007)" startWordPosition="1308" endWordPosition="1311"> a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a co219 herent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work i</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Generating constituent order in German clauses. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic, 23–30 June 2007, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--3</pages>
<contexts>
<context position="3296" citStr="Fillmore et al. (2003)" startWordPosition="503" endWordPosition="506"> model (and extensions of it) is that it only investigates overt realization patterns in terms of grammatical functions. These functions reflect explicit realizations of predicate argument structures (PAS), but they do not capture the full range of salience factors. In particular, the model does not reflect the importance of discourse entities that fill core roles of the predicate, but that remain implicit in the predicate’s local argument structure. We develop a specific set-up that allows us to further investigate the factors that govern such a null-instantiations of argument positions (cf. Fillmore et al. (2003)), as a special form of coherence inducing element in discourse. We henceforth refer to such cases as non-realized arguments. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam R.L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16.3:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1782" citStr="Grosz et al., 1995" startWordPosition="265" endWordPosition="268">investigation of discourse coherence phenomena. Initial experiments on the task of predicting predicate alignments across text pairs show promising results. Our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks. 1 Introduction Research in the fields of discourse and pragmatics has led to a number of theories that try to explain and formalize the effect of discourse coherence inducing elements either locally or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is m</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shudong Huang</author>
<author>David Graff</author>
<author>George Doddington</author>
</authors>
<title>Multiple-Translation Chinese Corpus. Linguistic Data Consortium,</title>
<date>2002</date>
<location>Philadelphia.</location>
<contexts>
<context position="7209" citStr="Huang et al., 2002" startWordPosition="1129" endWordPosition="1132">created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that have been used for task</context>
</contexts>
<marker>Huang, Graff, Doddington, 2002</marker>
<rawString>Shudong Huang, David Graff, and George Doddington. 2002. Multiple-Translation Chinese Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation,</title>
<date>2005</date>
<volume>5</volume>
<pages>79--86</pages>
<contexts>
<context position="7040" citStr="Koehn (2005)" startWordPosition="1104" endWordPosition="1105">ted tasks. Section 3 introduces the new task together with a description of how we prepared a suitable data set. Section 4 discusses the potential benefits of the created resource in more detail. Section 5 presents experiments on predicate alignment using this new data set and outlines first results. Finally, we conclude in Section 6 and discuss future work. 2 Related Work Data sets comprising parallel texts have been released for various different tasks, including paraphrase extraction and statistical machine translation (SMT). While corpora for SMT are typically multilingual (e.g. Europarl, Koehn (2005)), there also exist monolingual parallel corpora that consist of multiple translations of one text into the same language (Barzilay and McKeown, 2001; Huang et al., 2002, inter alia). Each translation can provide alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse co</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn, 2005. Europarl: A parallel corpus for statistical machine translation, volume 5, pages 79– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Benjamin Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>104--111</pages>
<contexts>
<context position="24918" citStr="Liang et al., 2006" startWordPosition="3997" endWordPosition="4000">bination of four different similarity measures. Subsequently, we also tune the weighting scheme for similarity measures on the development set. We found the best performing combination of weights to be 0.09, 0.19, 0.48 and 0.24 for simWN, simVN, simDist and simArgs, respectively. Baselines. A simple baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently comp</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Benjamin Taskar, and Dan Klein. 2006. Alignment by agreement. In North American Association for Computational Linguistics (NAACL), pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory. Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="2042" citStr="Mann and Thompson, 1988" startWordPosition="309" endWordPosition="312">d that our data set holds potential for empirical research into a variety of discourse-related tasks. 1 Introduction Research in the fields of discourse and pragmatics has led to a number of theories that try to explain and formalize the effect of discourse coherence inducing elements either locally or globally. For example, Centering Theory (Grosz et al., 1995) provides a framework to model local coherence by relating the choice of referring expressions to the salience of an entity at certain stages of a discourse. An example for a global coherence model would be Rhetorical Structure Theory (Mann and Thompson, 1988), which addresses overall text structure by means of coherence relations between the parts of a text. In addition to such theories, computational approaches have been proposed to capture corresponding phenomena empirically. A prominent example is the entity-based model by Barzilay and Lapata (2008). In their approach, local coherence is modeled by the observation of sentence-to-sentence realization patterns of individual entities. The learned model reflects a key idea from Centering Theory, namely that adjacent sentences in a coherent discourse are likely to involve the same entities. One shor</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory. Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<editor>(Eds.),</editor>
<publisher>MIT Press,</publisher>
<location>Seattle, Wash., 9–13</location>
<note>Reprinted in Advances in Automatic Text</note>
<contexts>
<context position="7874" citStr="McKeown and Radev, 1995" startWordPosition="1228" endWordPosition="1231">e alternative verbalizations of the same events but little variation can be observed in context, as the overall discourse remains the same. A higher degree of variation can be found in the Microsoft Research Paraphrase Corpus (e.g. MSRPC, Dolan and Brockett (2005)), which consists of paraphrases automatically extracted from different sources. In the MSRPC, however, original discourse contexts are not provided for each sentence. In contrast to truly parallel monolingual corpora, there also exist a range of comparable corpora that have been used for tasks such as (multi-document) summarization (McKeown and Radev, 1995, inter alia). Corpora for this task are collected manually and hence are rather small. Our work presents a method to automatically construct a large corpus of text pairs describing the same underlying events. In this novel corpus, we identify common events across texts and investigate the argument structures that were realized in each context to establish a co219 herent discourse. Different aspects related to this setting have been studied in previous work. For example, Filippova and Strube (2007) and Cahill and Riester (2009) examine factors that determine constituent order and Belz et al. (</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>Kathleen R. McKeown and Dragomir Radev. 1995. Generating summaries of multiple news articles. In Proceedings of the 18th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, Seattle, Wash., 9–13 July 1995, pages 74–82. Reprinted in Advances in Automatic Text Summarization, Mani, I. and Maybury, M.T. (Eds.), Cambridge, Mass.: MIT Press, 1999, pp.381-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
</authors>
<date>2008</date>
<booktitle>NomBank v1.0. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="11829" citStr="Meyers et al., 2008" startWordPosition="1878" endWordPosition="1881">and detail. Closer examination of a development set of 10 document pairs (described below) revealed that we can indeed find multiple cases where roles are not locally filled in predicate argument structures. We show instances of this phenomenon, in which aligned PAS help to resolve implicit role references, in Section 4. 3.2 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2. We chose this constraint as longer text pairs contain a high number of unrelated predicates, making this task difficult to manage for the annotators. Sure and possible links. Following standard practice in word alignment tasks (cf. Cohn et al</context>
</contexts>
<marker>Meyers, Reeves, Macleod, 2008</marker>
<rawString>Adam Meyers, Ruth Reeves, and Catherine Macleod. 2008. NomBank v1.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>105</pages>
<contexts>
<context position="11807" citStr="Palmer et al., 2005" startWordPosition="1874" endWordPosition="1877">words, respectively) and detail. Closer examination of a development set of 10 document pairs (described below) revealed that we can indeed find multiple cases where roles are not locally filled in predicate argument structures. We show instances of this phenomenon, in which aligned PAS help to resolve implicit role references, in Section 4. 3.2 Gold Standard Annotation We pre-processed all texts using MATE tools (Bohnet, 2010; Bj¨orkelund et al., 2010), a pipeline of natural language processing modules including a state-of-the-art semantic role labeler that computes Prop/NomBank annotations (Palmer et al., 2005; Meyers et al., 2008). The output was used to provide pre-labeled verbal and nominal predicates for annotation. We asked two students1 to tag alignments of corresponding predicates in 70 text pairs derived from the created corpus. All document pairs were randomly chosen from the AFP and APW sections of Gigaword with the constraint that each text consists of 100 to 300 words2. We chose this constraint as longer text pairs contain a high number of unrelated predicates, making this task difficult to manage for the annotators. Sure and possible links. Following standard practice in word alignment</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Jumbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Fifth Edition. Linguistic Data Consortium,</title>
<date>2011</date>
<location>Philadelphia.</location>
<contexts>
<context position="9726" citStr="Parker et al., 2011" startWordPosition="1529" endWordPosition="1532">ided data sets for this task, however, are critically small (438 train and 525 test sentences). Eventually, the corpus we present in this paper could also be beneficial for data-driven approaches to role linking in discourse. 3 A Corpus for Aligning Predications across Comparable Texts Our aim is to construct a corpus of comparable texts that can be assumed to be about the same events, but include variation in textual presentation. This requirement fits well with the news domain, for which we can trace varying textual sources for the same underlying events. The English Gigaword Fifth Edition (Parker et al., 2011) corpus (henceforth just Gigaword) is one of the largest corpus collections for English. It comprises a total of 9.8 million newswire articles from seven distinct sources. For construction of our corpus we make use of all combinations of agency pairs in Gigaword. 3.1 Corpus Creation In order to extract pairs of articles describing the same news event, we implemented the pairwise similarity method presented by Wubben et al. (2009). The method is based on measuring word overlap in news headlines, weighting each word by its TF*IDF score to give a higher impact to words occurring with lower freque</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Jumbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth Edition. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval2010 Task 10: Linking Events and Their Participants in Discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations,</booktitle>
<pages>45--50</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9030" citStr="Ruppenhofer et al., 2010" startWordPosition="1413" endWordPosition="1416">9) examine factors that determine constituent order and Belz et al. (2009) study the conditions for the use of different types of referring expressions. The specific set-up we examine allows us to further investigate the factors that govern the non-realization of an argument position, as a special form of coherence inducing element in discourse. As in the aforementioned work, we are specifically interested in the generation of coherent discourses (e.g. for summarization). Yet, our work also complements research in discourse analysis. A recent example for such work is the Semeval 2010 Task 10 (Ruppenhofer et al., 2010), which aims at linking events and their participants in discourse. The provided data sets for this task, however, are critically small (438 train and 525 test sentences). Eventually, the corpus we present in this paper could also be beneficial for data-driven approaches to role linking in discourse. 3 A Corpus for Aligning Predications across Comparable Texts Our aim is to construct a corpus of comparable texts that can be assumed to be about the same events, but include variation in textual presentation. This requirement fits well with the news domain, for which we can trace varying textual </context>
<context position="20651" citStr="Ruppenhofer et al., 2010" startWordPosition="3335" endWordPosition="3339">7 ... after declining for ... months.15 a’. ? ... after the index declining for ... months. b. Consumer confidence rose ... following three months of dramatic decline [ ]Arg1.16 b’. ? ... following three months of dramatic decline [of consumer confidence]Arg1. As showcased by the previous examples, the decision on whether to realize a role filler in a local PAS can be rather complex. Obviously, the 12Source document ID: AFP ENG 20031015.0353 13Source document ID: APW ENG 20031015.0236 14These different configurations are termed constructional vs. lexical licensors in the SemEval 2010 Task 10 (Ruppenhofer et al., 2010). 15Source document ID: AFP ENG 20011228.0365 16Source document ID: APW ENG 20011228.0572 222 Figure 1: The predicates of two sentences (white: “The company has said it plans to restate its earnings for 2000 through 2002.”; gray: “The company had announced in January that it would have to restate earnings (...)”) from the Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts. above instances do not provide exhaustive information for grounding all such decisions. A comprehensive model of discourse coherence will need to estimate the argument realization potenti</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. SemEval2010 Task 10: Linking Events and Their Participants in Discourse. In Proceedings of the 5th International Workshop on Semantic Evaluations, pages 45–50, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Mikhail Kozhevnikov</author>
</authors>
<title>Bootstrapping semantic analyzers from non-contradictory texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>958--967</pages>
<location>Uppsala,</location>
<contexts>
<context position="4046" citStr="Titov and Kozhevnikov, 2010" startWordPosition="612" endWordPosition="615">ts. Our main hypothesis is that context specific realization patterns for PAS can be automatically 218 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218–227, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics learned from a semantically parsed corpus of comparable text pairs. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004) and bootstrapping semantic analyzers (Titov and Kozhevnikov, 2010). For our purposes, we are interested in finding corresponding PAS across comparable texts that are known to talk about the same events, and hence involve the same set of underlying event participants. By aligning predicates in such texts, we can investigate the factors that determine discourse coherence in the realization patterns for the involved participants. As a first step towards this overall goal, we describe the construction of a resource that contains more than 160,000 document pairs that are known to talk about the same events and participants. Example (1), extracted from our corpus </context>
</contexts>
<marker>Titov, Kozhevnikov, 2010</marker>
<rawString>Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrapping semantic analyzers from non-contradictory texts. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11–16 July 2010, pages 958–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>Cecile Paris</author>
</authors>
<title>Using dependency-based features to take the ”Para-farce” out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>131--138</pages>
<contexts>
<context position="25109" citStr="Wan et al. (2006)" startWordPosition="4028" endWordPosition="4031">ts to be 0.09, 0.19, 0.48 and 0.24 for simWN, simVN, simDist and simArgs, respectively. Baselines. A simple baseline for this task is to align all predicates whose lemmas are identical (SameLemma). As a more sophisticated baseline, we make use of alignment tools commonly used in statistical machine translation (SMT). We train our own word alignment model using the state-of-the-art tool Berkeley Aligner (Liang et al., 2006). As word alignment tools require pairs of sentences as input, we first extract paraphrases for this baseline using a re-implementation of the paraphrase detection system by Wan et al. (2006). In the following sections, we abbreviate this model as WordAlign. 5.3 Results Following Cohn et al. (2008) we measure precision as the number of predicted alignments also annotated in the gold standard divided by the total number of predictions. Recall is measured as the number of correctly predicted sure alignments devided by the total number of sure alignments in the gold standard. We subsequently compute the Fl-score as the harmonic mean between precision and recall. Table 2 presents the results for our model and the two baselines. From all four approaches, WordAlign performs worst. We id</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris. 2006. Using dependency-based features to take the ”Para-farce” out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
<author>Erwin Marsi</author>
</authors>
<title>Clustering and matching headlines for automatic paraphrase acquisition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG</booktitle>
<pages>122--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Wubben, van den Bosch, Krahmer, Marsi, 2009</marker>
<rawString>Sander Wubben, Antal van den Bosch, Emiel Krahmer, and Erwin Marsi. 2009. Clustering and matching headlines for automatic paraphrase acquisition. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 122– 125, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>