<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018481">
<title confidence="0.996451">
Reuse and Challenges in Evaluating Language Generation Systems:
Position Paper
</title>
<author confidence="0.996968">
Kalina Bontcheva
</author>
<affiliation confidence="0.998209">
University of Sheffield
</affiliation>
<address confidence="0.9015685">
Regent Court, 211 P ortobello Street
Sheffield S1 4DP, UK
</address>
<email confidence="0.999145">
kalina@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955461538462">
Although there is an increasing shift
towards evaluating Natural Language
Generation (NLG) systems, there are
still many NLG-specific open issues that
hinder effective comparative and quan-
titative evaluation in this field. The pa-
per starts off by describing a task-based,
i.e., black-box evaluation of a hyper-
text NLG system. Then we examine the
problem of glass-box, i.e., module spe-
cific, evaluation in language generation,
with focus on evaluating machine learn-
ing methods for text planning.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951">
Although there is an increasing shift towards eval-
uating Natural Language Generation (NLG) sys-
tems, there are still many NLG-specific open is-
sues that hinder effective comparative and quan-
titative evaluation in this field. As discussed in
(Dale and Mellish, 1998), because of the differ-
ences between language understanding and gener-
ation, most NLU evaluation techniques1 cannot be
applied to generation. The main problems come
from the lack of well-defined input and output for
NLG systems (see also (Wilks, 1992)). Differ-
ent systems assume different kinds of input, de-
pending on their domains, tasks and target media,
which makes comparative evaluation particularly
</bodyText>
<footnote confidence="0.926965">
1For a comprehensive review see (Sparck Jones and Gal-
liers, 1996).
</footnote>
<bodyText confidence="0.993056555555556">
difficult.2 It is also very hard to obtain a quanti-
tative, objective, measure of the quality of output
texts, especially across different domains and gen-
res. Therefore, NLG systems are normally evalu-
ated with respect to their usefulness for a partic-
ular (set of) task(s), which is established by mea-
suring user performance on these tasks, i.e., ex-
trinsic evaluation. This is often also referred to
as black-box evaluation, because it does not focus
on any specific module, but evaluates the system’s
performance as a whole. This paper presents one
such evaluation experiment with focus on the issue
of reusing resources such as questionnaires, and
task and experiment designs. It then examines the
problem of glass-box, i.e., module specific, eval-
uation in language generation, with focus on the
problem of evaluating machine learning methods
for text planning.
</bodyText>
<sectionHeader confidence="0.860548" genericHeader="method">
2 The System in Brief
</sectionHeader>
<bodyText confidence="0.999714285714286">
HYLITE+ (Bontcheva and Wilks, 2001;
Bontcheva, 2001b) is a dynamic hypertext
system3 that generates encyclopaedia-style ex-
planations of terms in two specialised domains:
chemistry and computers. The user interacts with
the system in a Web browser by specifying a term
she wants to look up. The system generates a
</bodyText>
<footnote confidence="0.959367375">
2The same is not true for understanding tasks since they
all operate on the same input, i.e., existing texts. So for ex-
ample, two part-of-speech taggers or information extraction
systems can be compared by running them on the same test
corpus and measuring their relative performance.
3In dynamic hypertext page content and links are created
on demand and are often adapted to the user and the previous
interaction.
</footnote>
<bodyText confidence="0.999879944444445">
hypertext explanation of the term; further infor-
mation can be obtained by following hypertext
links or specifying another query. The system is
based on applied NLG techniques, a re-usable user
modelling component (VIEWGEN), and a flexible
architecture with module feedback. The adaptiv-
ity is implemented on the basis of a user and a
discourse models which are used to determine,
for example, which concepts are unknown, so
clarifying information can be included for them.
The user model is updated dynamically, based on
the user’s interaction with the system. When a
user registers with the system for the first time,
her model is initialised from a set of stereotypes.
The system determines which stereotypes apply
on the basis of information provided by the user
herself. If no such information is provided, the
system assumes a novice user.
</bodyText>
<sectionHeader confidence="0.993689" genericHeader="method">
3 Extrinsic Evaluation of HYLITE+
</sectionHeader>
<bodyText confidence="0.99998815625">
Due to the fact that HYLITE+ generates hypertext
which content and links are adapted to the user,
it can be evaluated following strategies from two
fields: NLG and adaptive hypertext. After review-
ing the approaches, used for evaluation of the NLG
and adaptive hypertext systems most similar to
ours,e.g., (Cox et al., 1999), (Reiter et al., 1995),
(H¨o¨ok, 1998), we discovered that they were all
evaluated extrinsically by measuring human per-
formance on a set of tasks, given different versions
of the system. The experiments were typically fol-
lowed by an informal interview and/or question-
naire, used to gather some qualitative data, e.g.,
on the quality of the generated text.
Setting up and conducting such task-based ex-
periments is costly and time-consuming, therefore
we looked at opportunities for reusing materials
and methodologies from previous evaluation ex-
periments of similar systems from the two fields.
This resulted in a substantial reduction of the time
and effort needed to prepare the experiments. We
also used the findings of some of these experi-
ments in order to improve the design of our own
evaluation. For example, (Cox et al., 1999) used
pre-generated static pages as a baseline and the
study reported that the difference in the two sys-
tems’ response times might have influenced some
of the results. Therefore, we chose instead to have
both the baseline non-adaptive and the adaptive
systems to generate the pages in real time, which
eliminated the possible influence of the different
response times.
</bodyText>
<subsectionHeader confidence="0.9861985">
3.1 Choosing the Main Goals of the
Evaluation
</subsectionHeader>
<bodyText confidence="0.99818711627907">
The first issue that needs to be addressed when de-
signing the extrinsic, or black-box, evaluation is
to determine what are the goals of the experiment.
Hypermedia applications are evaluated along three
aspects: interface look and feel, representation of
the information structure, and application-specific
information (Wills et al., 1999). The informa-
tion structure is concerned with the hypertext net-
work (nodes and links) and navigation aids (e.g.,
site maps, links to related material, index). The
application-specific information concerns the hy-
permedia content – text, images, audio and video.
For our system there is no need to evaluate the in-
terface, since HYLITE+ uses simple HTML and
existing Web browsers (e.g. Netscape, Internet
Explorer) as rendering tools. Therefore, the evalu-
ation efforts were concentrated on the information
content and navigational structure of the generated
hypertext.
Information content was measured on the ba-
sis of:
average time to complete each task;
average number ofpages visited per task;
average number of distinct pages visited per
task;
percent of correctly answered questions per
task;
questionnaire results about content and com-
prehension of the generated pages;
user preference for any of the systems.
The navigational structure was measured by
the following metrics:
average time per page visited;
average number ofpages visited;
total number ofpages visited;
number of links followed;
usage of the browser Back button;
usage of the system’s topic list to find infor-
mation;
observation and subjective opinion on orien-
tation;
subjective opinion on navigation and ease of
finding information.
</bodyText>
<subsectionHeader confidence="0.999228">
3.2 Choosing the Methodology
</subsectionHeader>
<bodyText confidence="0.999963567567567">
The experiment has a repeated measures, task-
based design (also called within-subjects design),
i.e., the same users interacted with the two ver-
sions of the system, in order to complete a given
set of tasks. Prior to the experiment, the partici-
pants were asked to provide some background in-
formation (e.g., computing experience, familiarity
with Web browsers, and electronic encyclopaedia)
and fill in a multiple choice pre-test, that diagnosed
their domain knowledge.
The design of the tasks follows the design used
in the evaluation of two other adaptive hyperme-
dia applications – PUSH (H¨o¨ok, 1998) and (Wills
et al., 1999). Each of the participants was first
given a set of three tasks – each set contained one
browsing, one problem-solving, and one informa-
tion location task. The order was not randomised,
because the browsing task was also intended as a
task that would allow users to familiarise them-
selves with the system and the available informa-
tion; it was not used for deriving the quantitative
measures discussed above.
The participants performed the first set of tasks
with the non-adaptive/adaptive system and then
swapped systems for the second set of three tasks.
The types of tasks – browsing, problem-solving,
and information location – were chosen to reflect
the different uses of hypermedia information.
Qualitative data and feedback were obtained
using a questionnaire and semi-structured inter-
views, where the subjects could discuss their expe-
rience with the two systems. There were two main
types of questions and statements: those related to
the usability of the adaptive and baseline systems,
e.g., statements like “I found the adaptive system
difficult to use”; and those related to hypertext and
navigation, e.g., links, text length, structure.
</bodyText>
<subsectionHeader confidence="0.894025">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999991464285714">
Due to the small number of participants and the
differences in their prior domain knowledge and
browsing styles, the results obtained could not be
used to derive a statistically reliable comparison
between the measures obtained for the adaptive
and the non-adaptive versions, but the quantita-
tive results and user feedback are sufficiently en-
couraging to suggest that HYLITE+ adaptivity is of
benefit to the user.
The most important outcome of this small-scale
evaluation was that it showed the need to control
not just for user’s prior knowledge (e.g., novice,
advanced), but also for hypertext reading style.
Although previous studies of people browsing hy-
pertext (e.g., (Nielsen, 2000)) have distinguished
two types: skimmers and readers, in this exper-
iment we did not control for that, because the
tasks from which we derived the quantitative mea-
sures were concerned with locating information
and problem solving, not browsing. Still, our re-
sults showed the need to control for this variable,
regardless of the task type, because reading style
influences some of the quantitative measures (e.g.,
task performance, mean time per task, number of
visited pages, use of browser navigation buttons).
Due to space limitations no further details can be
provided in this paper, but see (Bontcheva, 2001a)
for a detailed discussion.
</bodyText>
<subsectionHeader confidence="0.674982">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999957875">
The methodology used for HYLITE’s black-box
evaluation was based on experience not only in the
field of language generation, but also in the field of
hypermedia, which motivated us to evaluate also
the usability of the system and elicit the users’ atti-
tudes towards the intelligent behaviour of our gen-
eration system. This emphasis on usability, which
comes from human-computer interaction, allowed
us to obtain results which ultimately had implica-
tions for the architecture of our generation system
(see (Bontcheva and Wilks, 2001) for further de-
tails) and which we would have not obtained oth-
erwise. This leads us to believe that reuse of evalu-
ation resources and methodologies from different,
but related fields, can be beneficial for NLP sys-
tems in general.
On the other hand, even though evaluating the
NLG system in a task-based fashion has had posi-
tive impact, there is still a need for glass-box eval-
uation on a module by module basis, especially
using quantitative evaluation metrics, in order to
be able to detect specific problems in the genera-
tion modules. This is the evaluation challenge that
we discuss in the rest of the paper.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="method">
4 The Challenge: Automatic
</sectionHeader>
<subsectionHeader confidence="0.514449">
Quantitative Evaluation of Content
Planners
</subsectionHeader>
<bodyText confidence="0.933099093023256">
Content planning, also called deep language gen-
eration, is the stage where the system needs to de-
cide what to say, i.e., select some predicates en-
coding the semantics of the text to be generated,
and then decide when to say them, i.e., choose an
ordering of these predicates that will result in the
generation of coherent discourse. Typically con-
tent plans are created manually by NLG experts
in collaboration with domain specialists, using a
corpus of target texts. However, this is a time
consuming process, so recently researchers have
started experimenting with using machine learn-
ing for content planning. This is the research
area which we will investigate as part of build-
ing an NLG system for the e-science Grid project
MIAKT4. The surface realisation module will be
reused from HYLITE+, while the HYLITE+ content
planner will be used as a baseline.
An integral part of the development of machine
learning approaches to NLP tasks is the ability to
perform automatic quantitative evaluation in order
to measure differences between different config-
urations of the module and also allow compara-
tive evaluation with other approaches. For exam-
ple, the MUC corpora and the associated scoring
tool are frequently used by researchers working on
machine learning for Information Extraction both
as part of the development process and also as
means for comparison of the performance of dif-
4The MIAKT project is sponsored by the UK Engi-
neering and Physical Sciences Research Council (grant
GR/R85150/01) and involves the University of Southampton,
University of Sheffield, the Open University, University of
Oxford, and King’s College London.
ferent systems (see e.g., (Marsh and Perzanowski,
1998)). Similarly, automatic quantitative evalua-
tion of content planners needs:
an annotated corpus;
an evaluation metric and a scoring tool, im-
plementing this metric.
Below we will discuss each of these components
and highlight the outstanding problems and chal-
lenges.
</bodyText>
<subsectionHeader confidence="0.910087">
4.1 Evaluation Corpora for Content
Planning
</subsectionHeader>
<bodyText confidence="0.9997575">
Research on content planning comes from two
fields: document summarisation which uses some
NLG techniques to generate the summaries; and
natural language generation where the systems
generate from some semantic representation, e.g.,
a domain knowledge base or numeric weather
data. Here we review some work from these fields
that has addressed the issue of evaluation corpora.
</bodyText>
<subsubsectionHeader confidence="0.629963">
4.1.1 Previous Work
</subsubsectionHeader>
<bodyText confidence="0.999979196969697">
(Kan and Mckeown, 2002) have developed a
corpus-trained summarisation system for indica-
tive summaries. As part of this work they an-
notated manually 100 bibliography entries with
indicative summaries and then used a decision
tree learner to annotate automatically another
1900 entries with 24 predicates like Audi-
ence, Topic, and Content. For example,
some annotations for the Audience predicate
are: For adult readers; This books
is intended for adult readers. The
annotated texts are then used to learn the kinds of
predicates present in the summaries, their order-
ing using bigram statistics, and surface realisation
patterns.
(Barzilay et al., 2002) have taken the problem
of learning sentence ordering for summarisation
one step further by considering multi-document
summarisation of news articles. Their experiments
show that ordering is significant for text compre-
hension and there is no one ideal ordering, rather
there is a set of acceptable orderings. Therefore,
an annotated corpus which provides only one of
the acceptable orderings is not sufficient to enable
the system to differentiate between the many good
orderings and the bad ones. To solve this prob-
lem they developed a corpus of multiple versions
of the same content, each version providing an ac-
ceptable ordering. This corpus5 consists of ten sets
of news articles, two to three articles per event.
Sentences were extracted manually from these sets
and human subjects were asked to order them so
that they form a readable text. In this way 100 or-
derings were acquired, 10 orderings per set. How-
ever, since this procedure involved a lot of human
input, the construction of such a corpus on a larger
scale is quite expensive.
The difference between the techniques used for
summarisation and those used for generation is
that the summarisation ones typically do not use
very detailed semantic representations, unlike the
full NLG systems. Consequently this means that
a corpus annotated for summarisation purposes is
likely to contain isufficient information for a full
NLG application, while corpus with detailed se-
mantic NLG annotation will most likely be use-
ful for a summarisation content planner. Since
the experience from building annotated corpora
for learning ordering for summarisation has shown
that they are expensive to build, then the creation
of semantically annotated corpora for NLG is go-
ing to be even more expensive. Therefore, reuse
and some automation are paramount.
So far, only very small semantically annotated
corpora for NLG have been created. For exam-
ple, (Duboue and McKeown, 2001) have collected
an annotated corpus of 24 transcripts of medical
briefings. They use 29 categories to classify the
200 tags used in their tagset. Each transcript had
an average of 33 tags with some tags being much
more frequent than others. Since the tags need
to convey the semantics of the text units, they
are highly domain specific, which means that any
other NLG system or learning approach that would
want to use this corpus for evaluation will have to
be retargetted to this domain.
</bodyText>
<subsubsectionHeader confidence="0.744264">
4.1.2 The Proposed Approach for NHAKT
</subsubsectionHeader>
<bodyText confidence="0.995482">
As evident from this discussion, there are still a
number of problems that need to be solved so that
a semantically annotated corpus of a useful size
</bodyText>
<footnote confidence="0.992031">
5Available at http://www.cs.columbia.edu/ noemie/ordering/.
</footnote>
<bodyText confidence="0.99991935483871">
can be created, thus enabling the comparative eval-
uation of different learning strategies and content
planning components. Previous work has typi-
cally started from already existing texts/transcripts
and then used humans to annotate them with se-
mantic predicates, which is an expensive opera-
tion. In addition, the experience from the Informa-
tion Extraction evaluations in MUC and ACE has
shown that even humans find it difficult to annotate
texts with deeper semantic information. For exam-
ple, the interannotator variability on the scenario
template task in MUC-7 was between 85.15 and
96.64 on the f-measures (Marsh and Perzanowski,
1998).
In the MIAKT project we will experiment with
a different approach to creating an annotated cor-
pus of orderings, which is similar to the approach
taken by (Barzilay et al., 2002), where humans
were given sentences and asked to order them in
an acceptable way. Since MIAKT is a full NLG sys-
tem we cannot use already existing sentences, as it
was possible in their summarisation systems. In-
stead, we will use the HYLITE+ surface realiser to
generate sentences for each of the semantic pred-
icates and then provide users with a graphical ed-
itor, where they can re-arrange the ordering of
these sentences by using drag and drop. In this
way, there will be no need for the users to anno-
tate with semantic information, because the sys-
tem will have the corresponding predicates from
which the sentences were generated. This idea is
similar to the way in which language generation
is used to support users with entering knowledge
base content (Power et al., 1998). The proposed
technique is called “What You See Is What You
Meant” (WYSIWYM) and allows a domain expert
to edit a NLG knowledge base reliably by interact-
ing with a text, generated by the system, which
presents both the knowledge already defined and
the options for extending it. In MIAKT we will use
instead the generator to produce the sentences, so
the user only needs to enter their order. We will
not need to use WYSIWYM editing for knowl-
edge entry, because the knowledge base will al-
ready exist.
The difference between using generated sen-
tences and sentences from human-written texts is
that the human-written ones tend to be more com-
plex and aggregate the content of similar predi-
cates. This co-occurence information may be im-
portant, because, in a sense, it conveys stronger
restrictions on ordering than those between two
sentences. Therefore we would like to experiment
with taking an already annotated corpus of human-
authored texts, e.g., MUC-7 and compare the re-
sults achieved by using this corpus and a corpus
of multiple orderings created by humans from the
automatically generated sentences. In general, the
question here is whether or not it is possible to
reuse a corpus annotated for information extrac-
tion for the training of a content planning NLG
component.
</bodyText>
<subsectionHeader confidence="0.981555">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999977720930233">
Previous work on learning order constraints has
used human subjects for evaluation. For example,
(Barzilay et al., 2002) asked humans to grade the
summaries, while (Duboue and McKeown, 2001)
manually analysed the derived constraints by com-
paring them to an existing text planner. However,
this is not sufficient if different planners or ver-
sions of the same planner are to be compared in a
quantitative fashion. In contrast, quantitative met-
rics for automatic evaluation of surface realisers
have been developed (Bangalore et al., 2000) and
they have been shown to correlate well with hu-
man judgement for quality and understandability.
These metrics are two kinds: using string edit
distance and using tree-based metrics. The string
edit distance ones measure the insertion, deletion,
and substitution errors between the reference sen-
tences in the corpus and the generated ones. Two
different measures were evaluated and the one that
treats deletions in one place and insertion in the
other as a single movement error was found to be
more appropriate. In the context of content plan-
ning we intend use the string edit distance metrics
by comparing the proposition sequence generated
by the planner against the “ideal” proposition se-
quence from the corpus.
The tree-based metrics were developed to re-
flect the intuition that not all moves are equally
bad in surface realisation. Therefore these metrics
use the dependency tree as a basis of calculating
the string edit distances. However, it is not very
clear whether this type of metrics will be appli-
cable to the content planning problem given that
we do not intend to use a planner that produces a
tree-like structure of the text (as do for example
RST-based planners, e.g., (Moore, 1995)).
If the reuse experiments in MIAKT are suc-
cessful, we will make our evaluation tool publi-
cally available, together with the annotated corpus
and the knowledge base of predicates, which we
hope will encourage other researchers to use them
for development and/or comparative evaluation of
content planners.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999648588235294">
In this paper we discussed the reuse of existing re-
souces and methodologies for extrinsic evaluation
of language generation systems. We also showed
that a number of challenges still exist in evalua-
tion of NLG systems and, more specifically, eval-
uation of content planners. While other fields like
machine translation and text summarisation al-
ready have some evaluation metrics and resources
available for reuse, language generation has so far
lagged behind and no comparative system evalu-
ation has ever been done on a larger scale, e.g.,
text summarisation systems are compared in the
DUC evaluation exercise. As a step towards com-
parative evaluation for NLG, we intend to make
available the annotated corpus, evaluation met-
ric(s) and tools to be developed as part of the re-
cently started MIAKT project.
</bodyText>
<sectionHeader confidence="0.999295" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999750363636364">
The work on MIAKT described here is being sup-
ported by the UK Engineering and Physical Sci-
ences Research Council (grant GR/R85150/01).
The work on HYLITE+ was supported by a PhD
fellowship by the University of Sheffield and an
Overseas Research Students Award. I also wish
to thank Yorick Wilks and Hamish Cunningham
for their comments on this work, the anony-
mous reviewers who helped me improve the paper,
and the human evaluators who participated in the
HYLITE+ experiments.
</bodyText>
<sectionHeader confidence="0.99817" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999464">
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation.
In International Conference on Natural Language
Generation (INLG 2000), Mitzpe Ramon, Israel.
Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2002. Inferring strategies for sen-
tence ordering in multidocument news summariza-
tion. Artificial Intelligence Research, 17:35–55.
Kalina Bontcheva and Yorick Wilks. 2001. Deal-
ing with dependencies between content planning and
surface realisation in a pipeline generation architec-
ture. In Proceedings of the International Joint Con-
ference in Artificial Intelligence (IJCAI’2001), Seat-
tle, USA, August.
Kalina Bontcheva. 2001a. Generating Adaptive Hy-
pertext Explanations. Ph.D. thesis, University of
Sheffield.
Kalina Bontcheva. 2001b. Tailoring the content of
dynamically generated explanations. In M. Bauer,
P. Gmytrasiewicz, and J. Vassileva, editors, User
Modelling 2001, volume 2109 of Lecture Notes in
Artificial Intelligence. Springer Verlag, Berling Hei-
delberg.
Richard Cox, Mick O’Donnell, and Jon Oberlander.
1999. Dynamic versus static hypermedia in museum
education: an evaluation of ILEX, the intelligent la-
belling explorer. In Susanne P. Lajoie and Martial
Vivet, editors, Artificial Intelligence in Education:
Open Learning Environment: New Computational
Technologies to Support Learning, Exploration and
Collaboration, pages 181 – 188. IOS Press, Ams-
terdam ; Oxford. Papers from the 9th International
Conference on Artificial Intelligence in Education
(AI-ED 99).
Robert Dale and Chris Mellish. 1998. Towards evalua-
tion in natural language generation. In Proceedings
of First International Conference on Language Re-
sources and Evaluation, pages 555 – 562, Granada,
Spain, 28-30 May.
Pablo A. Duboue and Kathleen R. McKeown. 2001.
Empirically estimanting order constraints for con-
tent planning in generation. In Proceedings ofACL-
EACL 2001, Toulouse, France, July.
Kristina H¨o¨ok. 1998. Evaluating the utility and usabil-
ity of an adaptive hypermedia system. Knowledge-
Based Systems, 10:311—319.
Min-Yen Kan and Kathleen R. Mckeown. 2002.
Corpus-trained text generation for summarization.
In Proceedings of the Second International Confer-
ence on Natural Language Generation (INLG 2002).
Elaine Marsh and Dennis Perzanowski. 1998.
MUC-7 evaluation of IE technology: Overview
of results. In Proceedings of the Seventh
Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/-
related projects/muc/index.html.
Johanna D. Moore. 1995. Participating in Explanatory
Dialogues. MIT Press, Cambridge, MA.
Jakob Nielsen. 2000. Designing Web Usability: The
Practice of Simplicity. New Riders Publishing.
Richard Power, Donia Scott, and Richard Evans. 1998.
What you see is what you meant: direct knowl-
edge editings with natural language feedback. In
13th European Conference on Artificial Intelligence
(ECAI’98), pages 677–681. John Wiley and Sons.
Ehud Reiter, Chris Mellish, and Jon Levine. 1995.
Automatic generation of technical documentation.
Journal ofApplied Artificial Intelligence, 9(3):259–
287.
Karen Sparck Jones and Julia R. Galliers. 1996.
Evaluating Natural Language Processing Systems:
An Analysis and Review. Number 1083 in Lec-
ture Notes in Artificial Intelligence. Springer Verlag,
Berlin, Heidelberg.
Yorick A. Wilks. 1992. Where am I coming from: The
reversibility of analysis and generation in natural
language processing. In Martin Puetz, editor, Thirty
Years ofLinguistic Evolution. John Benjamins.
G. B. Wills, I. Heath, R.M. Crowder, and
W. Hall. 1999. User evaluation of an in-
dustrial hypermedia application. Technical
report, M99/2, University of Southampton.
http://www.bib.ecs.soton.ac.uk/data/
1444/html/html/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290396">
<title confidence="0.9799265">Reuse and Challenges in Evaluating Language Generation Position Paper</title>
<author confidence="0.988239">Kalina</author>
<affiliation confidence="0.958667">University of</affiliation>
<note confidence="0.4330745">Regent Court, 211 P ortobello Sheffield S1 4DP,</note>
<email confidence="0.995883">kalina@dcs.shef.ac.uk</email>
<abstract confidence="0.998764428571429">Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are many open issues that hinder effective comparative and quantitative evaluation in this field. The pastarts off by describing a i.e., black-box evaluation of a hypertext NLG system. Then we examine the of i.e., module specific, evaluation in language generation, with focus on evaluating machine learning methods for text planning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
<author>Steve Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<contexts>
<context position="20673" citStr="Bangalore et al., 2000" startWordPosition="3276" endWordPosition="3279">xtraction for the training of a content planning NLG component. 4.2 Evaluation Metrics Previous work on learning order constraints has used human subjects for evaluation. For example, (Barzilay et al., 2002) asked humans to grade the summaries, while (Duboue and McKeown, 2001) manually analysed the derived constraints by comparing them to an existing text planner. However, this is not sufficient if different planners or versions of the same planner are to be compared in a quantitative fashion. In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al., 2000) and they have been shown to correlate well with human judgement for quality and understandability. These metrics are two kinds: using string edit distance and using tree-based metrics. The string edit distance ones measure the insertion, deletion, and substitution errors between the reference sentences in the corpus and the generated ones. Two different measures were evaluated and the one that treats deletions in one place and insertion in the other as a single movement error was found to be more appropriate. In the context of content planning we intend use the string edit distance metrics by</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>Srinivas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation.</rawString>
</citation>
<citation valid="true">
<date>2000</date>
<booktitle>In International Conference on Natural Language Generation (INLG</booktitle>
<location>Mitzpe Ramon,</location>
<marker>2000</marker>
<rawString>In International Conference on Natural Language Generation (INLG 2000), Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>Artificial Intelligence Research,</journal>
<pages>17--35</pages>
<contexts>
<context position="14580" citStr="Barzilay et al., 2002" startWordPosition="2282" endWordPosition="2285">developed a corpus-trained summarisation system for indicative summaries. As part of this work they annotated manually 100 bibliography entries with indicative summaries and then used a decision tree learner to annotate automatically another 1900 entries with 24 predicates like Audience, Topic, and Content. For example, some annotations for the Audience predicate are: For adult readers; This books is intended for adult readers. The annotated texts are then used to learn the kinds of predicates present in the summaries, their ordering using bigram statistics, and surface realisation patterns. (Barzilay et al., 2002) have taken the problem of learning sentence ordering for summarisation one step further by considering multi-document summarisation of news articles. Their experiments show that ordering is significant for text comprehension and there is no one ideal ordering, rather there is a set of acceptable orderings. Therefore, an annotated corpus which provides only one of the acceptable orderings is not sufficient to enable the system to differentiate between the many good orderings and the bad ones. To solve this problem they developed a corpus of multiple versions of the same content, each version p</context>
<context position="18059" citStr="Barzilay et al., 2002" startWordPosition="2840" endWordPosition="2843">pts and then used humans to annotate them with semantic predicates, which is an expensive operation. In addition, the experience from the Information Extraction evaluations in MUC and ACE has shown that even humans find it difficult to annotate texts with deeper semantic information. For example, the interannotator variability on the scenario template task in MUC-7 was between 85.15 and 96.64 on the f-measures (Marsh and Perzanowski, 1998). In the MIAKT project we will experiment with a different approach to creating an annotated corpus of orderings, which is similar to the approach taken by (Barzilay et al., 2002), where humans were given sentences and asked to order them in an acceptable way. Since MIAKT is a full NLG system we cannot use already existing sentences, as it was possible in their summarisation systems. Instead, we will use the HYLITE+ surface realiser to generate sentences for each of the semantic predicates and then provide users with a graphical editor, where they can re-arrange the ordering of these sentences by using drag and drop. In this way, there will be no need for the users to annotate with semantic information, because the system will have the corresponding predicates from whi</context>
<context position="20257" citStr="Barzilay et al., 2002" startWordPosition="3210" endWordPosition="3213">rdering than those between two sentences. Therefore we would like to experiment with taking an already annotated corpus of humanauthored texts, e.g., MUC-7 and compare the results achieved by using this corpus and a corpus of multiple orderings created by humans from the automatically generated sentences. In general, the question here is whether or not it is possible to reuse a corpus annotated for information extraction for the training of a content planning NLG component. 4.2 Evaluation Metrics Previous work on learning order constraints has used human subjects for evaluation. For example, (Barzilay et al., 2002) asked humans to grade the summaries, while (Duboue and McKeown, 2001) manually analysed the derived constraints by comparing them to an existing text planner. However, this is not sufficient if different planners or versions of the same planner are to be compared in a quantitative fashion. In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al., 2000) and they have been shown to correlate well with human judgement for quality and understandability. These metrics are two kinds: using string edit distance and using tree-based metrics</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Artificial Intelligence Research, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
<author>Yorick Wilks</author>
</authors>
<title>Dealing with dependencies between content planning and surface realisation in a pipeline generation architecture.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Joint Conference in Artificial Intelligence (IJCAI’2001),</booktitle>
<location>Seattle, USA,</location>
<contexts>
<context position="2371" citStr="Bontcheva and Wilks, 2001" startWordPosition="360" endWordPosition="363">by measuring user performance on these tasks, i.e., extrinsic evaluation. This is often also referred to as black-box evaluation, because it does not focus on any specific module, but evaluates the system’s performance as a whole. This paper presents one such evaluation experiment with focus on the issue of reusing resources such as questionnaires, and task and experiment designs. It then examines the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on the problem of evaluating machine learning methods for text planning. 2 The System in Brief HYLITE+ (Bontcheva and Wilks, 2001; Bontcheva, 2001b) is a dynamic hypertext system3 that generates encyclopaedia-style explanations of terms in two specialised domains: chemistry and computers. The user interacts with the system in a Web browser by specifying a term she wants to look up. The system generates a 2The same is not true for understanding tasks since they all operate on the same input, i.e., existing texts. So for example, two part-of-speech taggers or information extraction systems can be compared by running them on the same test corpus and measuring their relative performance. 3In dynamic hypertext page content a</context>
<context position="10845" citStr="Bontcheva and Wilks, 2001" startWordPosition="1692" endWordPosition="1695"> provided in this paper, but see (Bontcheva, 2001a) for a detailed discussion. 3.4 Discussion The methodology used for HYLITE’s black-box evaluation was based on experience not only in the field of language generation, but also in the field of hypermedia, which motivated us to evaluate also the usability of the system and elicit the users’ attitudes towards the intelligent behaviour of our generation system. This emphasis on usability, which comes from human-computer interaction, allowed us to obtain results which ultimately had implications for the architecture of our generation system (see (Bontcheva and Wilks, 2001) for further details) and which we would have not obtained otherwise. This leads us to believe that reuse of evaluation resources and methodologies from different, but related fields, can be beneficial for NLP systems in general. On the other hand, even though evaluating the NLG system in a task-based fashion has had positive impact, there is still a need for glass-box evaluation on a module by module basis, especially using quantitative evaluation metrics, in order to be able to detect specific problems in the generation modules. This is the evaluation challenge that we discuss in the rest of</context>
</contexts>
<marker>Bontcheva, Wilks, 2001</marker>
<rawString>Kalina Bontcheva and Yorick Wilks. 2001. Dealing with dependencies between content planning and surface realisation in a pipeline generation architecture. In Proceedings of the International Joint Conference in Artificial Intelligence (IJCAI’2001), Seattle, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
</authors>
<title>Generating Adaptive Hypertext Explanations.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sheffield.</institution>
<contexts>
<context position="2388" citStr="Bontcheva, 2001" startWordPosition="364" endWordPosition="365">ce on these tasks, i.e., extrinsic evaluation. This is often also referred to as black-box evaluation, because it does not focus on any specific module, but evaluates the system’s performance as a whole. This paper presents one such evaluation experiment with focus on the issue of reusing resources such as questionnaires, and task and experiment designs. It then examines the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on the problem of evaluating machine learning methods for text planning. 2 The System in Brief HYLITE+ (Bontcheva and Wilks, 2001; Bontcheva, 2001b) is a dynamic hypertext system3 that generates encyclopaedia-style explanations of terms in two specialised domains: chemistry and computers. The user interacts with the system in a Web browser by specifying a term she wants to look up. The system generates a 2The same is not true for understanding tasks since they all operate on the same input, i.e., existing texts. So for example, two part-of-speech taggers or information extraction systems can be compared by running them on the same test corpus and measuring their relative performance. 3In dynamic hypertext page content and links are crea</context>
<context position="10268" citStr="Bontcheva, 2001" startWordPosition="1605" endWordPosition="1606">2000)) have distinguished two types: skimmers and readers, in this experiment we did not control for that, because the tasks from which we derived the quantitative measures were concerned with locating information and problem solving, not browsing. Still, our results showed the need to control for this variable, regardless of the task type, because reading style influences some of the quantitative measures (e.g., task performance, mean time per task, number of visited pages, use of browser navigation buttons). Due to space limitations no further details can be provided in this paper, but see (Bontcheva, 2001a) for a detailed discussion. 3.4 Discussion The methodology used for HYLITE’s black-box evaluation was based on experience not only in the field of language generation, but also in the field of hypermedia, which motivated us to evaluate also the usability of the system and elicit the users’ attitudes towards the intelligent behaviour of our generation system. This emphasis on usability, which comes from human-computer interaction, allowed us to obtain results which ultimately had implications for the architecture of our generation system (see (Bontcheva and Wilks, 2001) for further details) a</context>
</contexts>
<marker>Bontcheva, 2001</marker>
<rawString>Kalina Bontcheva. 2001a. Generating Adaptive Hypertext Explanations. Ph.D. thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalina Bontcheva</author>
</authors>
<title>Tailoring the content of dynamically generated explanations.</title>
<date>2001</date>
<booktitle>User Modelling</booktitle>
<volume>2109</volume>
<editor>In M. Bauer, P. Gmytrasiewicz, and J. Vassileva, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berling Heidelberg.</location>
<contexts>
<context position="2388" citStr="Bontcheva, 2001" startWordPosition="364" endWordPosition="365">ce on these tasks, i.e., extrinsic evaluation. This is often also referred to as black-box evaluation, because it does not focus on any specific module, but evaluates the system’s performance as a whole. This paper presents one such evaluation experiment with focus on the issue of reusing resources such as questionnaires, and task and experiment designs. It then examines the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on the problem of evaluating machine learning methods for text planning. 2 The System in Brief HYLITE+ (Bontcheva and Wilks, 2001; Bontcheva, 2001b) is a dynamic hypertext system3 that generates encyclopaedia-style explanations of terms in two specialised domains: chemistry and computers. The user interacts with the system in a Web browser by specifying a term she wants to look up. The system generates a 2The same is not true for understanding tasks since they all operate on the same input, i.e., existing texts. So for example, two part-of-speech taggers or information extraction systems can be compared by running them on the same test corpus and measuring their relative performance. 3In dynamic hypertext page content and links are crea</context>
<context position="10268" citStr="Bontcheva, 2001" startWordPosition="1605" endWordPosition="1606">2000)) have distinguished two types: skimmers and readers, in this experiment we did not control for that, because the tasks from which we derived the quantitative measures were concerned with locating information and problem solving, not browsing. Still, our results showed the need to control for this variable, regardless of the task type, because reading style influences some of the quantitative measures (e.g., task performance, mean time per task, number of visited pages, use of browser navigation buttons). Due to space limitations no further details can be provided in this paper, but see (Bontcheva, 2001a) for a detailed discussion. 3.4 Discussion The methodology used for HYLITE’s black-box evaluation was based on experience not only in the field of language generation, but also in the field of hypermedia, which motivated us to evaluate also the usability of the system and elicit the users’ attitudes towards the intelligent behaviour of our generation system. This emphasis on usability, which comes from human-computer interaction, allowed us to obtain results which ultimately had implications for the architecture of our generation system (see (Bontcheva and Wilks, 2001) for further details) a</context>
</contexts>
<marker>Bontcheva, 2001</marker>
<rawString>Kalina Bontcheva. 2001b. Tailoring the content of dynamically generated explanations. In M. Bauer, P. Gmytrasiewicz, and J. Vassileva, editors, User Modelling 2001, volume 2109 of Lecture Notes in Artificial Intelligence. Springer Verlag, Berling Heidelberg.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Cox</author>
<author>Mick O’Donnell</author>
<author>Jon Oberlander</author>
</authors>
<title>Dynamic versus static hypermedia in museum education: an evaluation of ILEX, the intelligent labelling explorer.</title>
<date>1999</date>
<booktitle>In Susanne P. Lajoie and Martial Vivet, editors, Artificial Intelligence in Education: Open Learning Environment: New Computational Technologies to Support Learning, Exploration and Collaboration, pages 181 – 188.</booktitle>
<publisher>IOS Press,</publisher>
<location>Amsterdam</location>
<marker>Cox, O’Donnell, Oberlander, 1999</marker>
<rawString>Richard Cox, Mick O’Donnell, and Jon Oberlander. 1999. Dynamic versus static hypermedia in museum education: an evaluation of ILEX, the intelligent labelling explorer. In Susanne P. Lajoie and Martial Vivet, editors, Artificial Intelligence in Education: Open Learning Environment: New Computational Technologies to Support Learning, Exploration and Collaboration, pages 181 – 188. IOS Press, Amsterdam ; Oxford. Papers from the 9th International Conference on Artificial Intelligence in Education (AI-ED 99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Chris Mellish</author>
</authors>
<title>Towards evaluation in natural language generation.</title>
<date>1998</date>
<booktitle>In Proceedings of First International Conference on Language Resources and Evaluation, pages 555 – 562,</booktitle>
<location>Granada,</location>
<contexts>
<context position="982" citStr="Dale and Mellish, 1998" startWordPosition="141" endWordPosition="144"> hinder effective comparative and quantitative evaluation in this field. The paper starts off by describing a task-based, i.e., black-box evaluation of a hypertext NLG system. Then we examine the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on evaluating machine learning methods for text planning. 1 Introduction Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field. As discussed in (Dale and Mellish, 1998), because of the differences between language understanding and generation, most NLU evaluation techniques1 cannot be applied to generation. The main problems come from the lack of well-defined input and output for NLG systems (see also (Wilks, 1992)). Different systems assume different kinds of input, depending on their domains, tasks and target media, which makes comparative evaluation particularly 1For a comprehensive review see (Sparck Jones and Galliers, 1996). difficult.2 It is also very hard to obtain a quantitative, objective, measure of the quality of output texts, especially across d</context>
</contexts>
<marker>Dale, Mellish, 1998</marker>
<rawString>Robert Dale and Chris Mellish. 1998. Towards evaluation in natural language generation. In Proceedings of First International Conference on Language Resources and Evaluation, pages 555 – 562, Granada, Spain, 28-30 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Empirically estimanting order constraints for content planning in generation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACLEACL 2001,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="16504" citStr="Duboue and McKeown, 2001" startWordPosition="2589" endWordPosition="2592">ted for summarisation purposes is likely to contain isufficient information for a full NLG application, while corpus with detailed semantic NLG annotation will most likely be useful for a summarisation content planner. Since the experience from building annotated corpora for learning ordering for summarisation has shown that they are expensive to build, then the creation of semantically annotated corpora for NLG is going to be even more expensive. Therefore, reuse and some automation are paramount. So far, only very small semantically annotated corpora for NLG have been created. For example, (Duboue and McKeown, 2001) have collected an annotated corpus of 24 transcripts of medical briefings. They use 29 categories to classify the 200 tags used in their tagset. Each transcript had an average of 33 tags with some tags being much more frequent than others. Since the tags need to convey the semantics of the text units, they are highly domain specific, which means that any other NLG system or learning approach that would want to use this corpus for evaluation will have to be retargetted to this domain. 4.1.2 The Proposed Approach for NHAKT As evident from this discussion, there are still a number of problems th</context>
<context position="20327" citStr="Duboue and McKeown, 2001" startWordPosition="3221" endWordPosition="3224">o experiment with taking an already annotated corpus of humanauthored texts, e.g., MUC-7 and compare the results achieved by using this corpus and a corpus of multiple orderings created by humans from the automatically generated sentences. In general, the question here is whether or not it is possible to reuse a corpus annotated for information extraction for the training of a content planning NLG component. 4.2 Evaluation Metrics Previous work on learning order constraints has used human subjects for evaluation. For example, (Barzilay et al., 2002) asked humans to grade the summaries, while (Duboue and McKeown, 2001) manually analysed the derived constraints by comparing them to an existing text planner. However, this is not sufficient if different planners or versions of the same planner are to be compared in a quantitative fashion. In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al., 2000) and they have been shown to correlate well with human judgement for quality and understandability. These metrics are two kinds: using string edit distance and using tree-based metrics. The string edit distance ones measure the insertion, deletion, and s</context>
</contexts>
<marker>Duboue, McKeown, 2001</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2001. Empirically estimanting order constraints for content planning in generation. In Proceedings ofACLEACL 2001, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina H¨o¨ok</author>
</authors>
<title>Evaluating the utility and usability of an adaptive hypermedia system. KnowledgeBased Systems,</title>
<date>1998</date>
<pages>10--311</pages>
<marker>H¨o¨ok, 1998</marker>
<rawString>Kristina H¨o¨ok. 1998. Evaluating the utility and usability of an adaptive hypermedia system. KnowledgeBased Systems, 10:311—319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Kathleen R Mckeown</author>
</authors>
<title>Corpus-trained text generation for summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Natural Language Generation (INLG</booktitle>
<contexts>
<context position="13952" citStr="Kan and Mckeown, 2002" startWordPosition="2186" endWordPosition="2189">on metric and a scoring tool, implementing this metric. Below we will discuss each of these components and highlight the outstanding problems and challenges. 4.1 Evaluation Corpora for Content Planning Research on content planning comes from two fields: document summarisation which uses some NLG techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., a domain knowledge base or numeric weather data. Here we review some work from these fields that has addressed the issue of evaluation corpora. 4.1.1 Previous Work (Kan and Mckeown, 2002) have developed a corpus-trained summarisation system for indicative summaries. As part of this work they annotated manually 100 bibliography entries with indicative summaries and then used a decision tree learner to annotate automatically another 1900 entries with 24 predicates like Audience, Topic, and Content. For example, some annotations for the Audience predicate are: For adult readers; This books is intended for adult readers. The annotated texts are then used to learn the kinds of predicates present in the summaries, their ordering using bigram statistics, and surface realisation patte</context>
</contexts>
<marker>Kan, Mckeown, 2002</marker>
<rawString>Min-Yen Kan and Kathleen R. Mckeown. 2002. Corpus-trained text generation for summarization. In Proceedings of the Second International Conference on Natural Language Generation (INLG 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Marsh</author>
<author>Dennis Perzanowski</author>
</authors>
<title>MUC-7 evaluation of IE technology: Overview of results.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<note>http://www.itl.nist.gov/iaui/894.02/-related projects/muc/index.html.</note>
<contexts>
<context position="13223" citStr="Marsh and Perzanowski, 1998" startWordPosition="2078" endWordPosition="2081">he module and also allow comparative evaluation with other approaches. For example, the MUC corpora and the associated scoring tool are frequently used by researchers working on machine learning for Information Extraction both as part of the development process and also as means for comparison of the performance of dif4The MIAKT project is sponsored by the UK Engineering and Physical Sciences Research Council (grant GR/R85150/01) and involves the University of Southampton, University of Sheffield, the Open University, University of Oxford, and King’s College London. ferent systems (see e.g., (Marsh and Perzanowski, 1998)). Similarly, automatic quantitative evaluation of content planners needs: an annotated corpus; an evaluation metric and a scoring tool, implementing this metric. Below we will discuss each of these components and highlight the outstanding problems and challenges. 4.1 Evaluation Corpora for Content Planning Research on content planning comes from two fields: document summarisation which uses some NLG techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., a domain knowledge base or numeric weather data. Here we r</context>
<context position="17880" citStr="Marsh and Perzanowski, 1998" startWordPosition="2809" endWordPosition="2812">ated, thus enabling the comparative evaluation of different learning strategies and content planning components. Previous work has typically started from already existing texts/transcripts and then used humans to annotate them with semantic predicates, which is an expensive operation. In addition, the experience from the Information Extraction evaluations in MUC and ACE has shown that even humans find it difficult to annotate texts with deeper semantic information. For example, the interannotator variability on the scenario template task in MUC-7 was between 85.15 and 96.64 on the f-measures (Marsh and Perzanowski, 1998). In the MIAKT project we will experiment with a different approach to creating an annotated corpus of orderings, which is similar to the approach taken by (Barzilay et al., 2002), where humans were given sentences and asked to order them in an acceptable way. Since MIAKT is a full NLG system we cannot use already existing sentences, as it was possible in their summarisation systems. Instead, we will use the HYLITE+ surface realiser to generate sentences for each of the semantic predicates and then provide users with a graphical editor, where they can re-arrange the ordering of these sentences</context>
</contexts>
<marker>Marsh, Perzanowski, 1998</marker>
<rawString>Elaine Marsh and Dennis Perzanowski. 1998. MUC-7 evaluation of IE technology: Overview of results. In Proceedings of the Seventh Message Understanding Conference (MUC-7). http://www.itl.nist.gov/iaui/894.02/-related projects/muc/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
</authors>
<title>Participating in Explanatory Dialogues.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21876" citStr="Moore, 1995" startWordPosition="3477" endWordPosition="3478">etrics by comparing the proposition sequence generated by the planner against the “ideal” proposition sequence from the corpus. The tree-based metrics were developed to reflect the intuition that not all moves are equally bad in surface realisation. Therefore these metrics use the dependency tree as a basis of calculating the string edit distances. However, it is not very clear whether this type of metrics will be applicable to the content planning problem given that we do not intend to use a planner that produces a tree-like structure of the text (as do for example RST-based planners, e.g., (Moore, 1995)). If the reuse experiments in MIAKT are successful, we will make our evaluation tool publically available, together with the annotated corpus and the knowledge base of predicates, which we hope will encourage other researchers to use them for development and/or comparative evaluation of content planners. 5 Conclusion In this paper we discussed the reuse of existing resouces and methodologies for extrinsic evaluation of language generation systems. We also showed that a number of challenges still exist in evaluation of NLG systems and, more specifically, evaluation of content planners. While o</context>
</contexts>
<marker>Moore, 1995</marker>
<rawString>Johanna D. Moore. 1995. Participating in Explanatory Dialogues. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Nielsen</author>
</authors>
<title>Designing Web Usability: The Practice of Simplicity.</title>
<date>2000</date>
<publisher>New Riders Publishing.</publisher>
<contexts>
<context position="9658" citStr="Nielsen, 2000" startWordPosition="1508" endWordPosition="1509"> domain knowledge and browsing styles, the results obtained could not be used to derive a statistically reliable comparison between the measures obtained for the adaptive and the non-adaptive versions, but the quantitative results and user feedback are sufficiently encouraging to suggest that HYLITE+ adaptivity is of benefit to the user. The most important outcome of this small-scale evaluation was that it showed the need to control not just for user’s prior knowledge (e.g., novice, advanced), but also for hypertext reading style. Although previous studies of people browsing hypertext (e.g., (Nielsen, 2000)) have distinguished two types: skimmers and readers, in this experiment we did not control for that, because the tasks from which we derived the quantitative measures were concerned with locating information and problem solving, not browsing. Still, our results showed the need to control for this variable, regardless of the task type, because reading style influences some of the quantitative measures (e.g., task performance, mean time per task, number of visited pages, use of browser navigation buttons). Due to space limitations no further details can be provided in this paper, but see (Bontc</context>
</contexts>
<marker>Nielsen, 2000</marker>
<rawString>Jakob Nielsen. 2000. Designing Web Usability: The Practice of Simplicity. New Riders Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Power</author>
<author>Donia Scott</author>
<author>Richard Evans</author>
</authors>
<title>What you see is what you meant: direct knowledge editings with natural language feedback.</title>
<date>1998</date>
<booktitle>In 13th European Conference on Artificial Intelligence (ECAI’98),</booktitle>
<pages>677--681</pages>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="18835" citStr="Power et al., 1998" startWordPosition="2975" endWordPosition="2978">as it was possible in their summarisation systems. Instead, we will use the HYLITE+ surface realiser to generate sentences for each of the semantic predicates and then provide users with a graphical editor, where they can re-arrange the ordering of these sentences by using drag and drop. In this way, there will be no need for the users to annotate with semantic information, because the system will have the corresponding predicates from which the sentences were generated. This idea is similar to the way in which language generation is used to support users with entering knowledge base content (Power et al., 1998). The proposed technique is called “What You See Is What You Meant” (WYSIWYM) and allows a domain expert to edit a NLG knowledge base reliably by interacting with a text, generated by the system, which presents both the knowledge already defined and the options for extending it. In MIAKT we will use instead the generator to produce the sentences, so the user only needs to enter their order. We will not need to use WYSIWYM editing for knowledge entry, because the knowledge base will already exist. The difference between using generated sentences and sentences from human-written texts is that th</context>
</contexts>
<marker>Power, Scott, Evans, 1998</marker>
<rawString>Richard Power, Donia Scott, and Richard Evans. 1998. What you see is what you meant: direct knowledge editings with natural language feedback. In 13th European Conference on Artificial Intelligence (ECAI’98), pages 677–681. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Chris Mellish</author>
<author>Jon Levine</author>
</authors>
<title>Automatic generation of technical documentation.</title>
<date>1995</date>
<journal>Journal ofApplied Artificial Intelligence,</journal>
<volume>9</volume>
<issue>3</issue>
<pages>287</pages>
<contexts>
<context position="4290" citStr="Reiter et al., 1995" startWordPosition="670" endWordPosition="673">e first time, her model is initialised from a set of stereotypes. The system determines which stereotypes apply on the basis of information provided by the user herself. If no such information is provided, the system assumes a novice user. 3 Extrinsic Evaluation of HYLITE+ Due to the fact that HYLITE+ generates hypertext which content and links are adapted to the user, it can be evaluated following strategies from two fields: NLG and adaptive hypertext. After reviewing the approaches, used for evaluation of the NLG and adaptive hypertext systems most similar to ours,e.g., (Cox et al., 1999), (Reiter et al., 1995), (H¨o¨ok, 1998), we discovered that they were all evaluated extrinsically by measuring human performance on a set of tasks, given different versions of the system. The experiments were typically followed by an informal interview and/or questionnaire, used to gather some qualitative data, e.g., on the quality of the generated text. Setting up and conducting such task-based experiments is costly and time-consuming, therefore we looked at opportunities for reusing materials and methodologies from previous evaluation experiments of similar systems from the two fields. This resulted in a substanti</context>
</contexts>
<marker>Reiter, Mellish, Levine, 1995</marker>
<rawString>Ehud Reiter, Chris Mellish, and Jon Levine. 1995. Automatic generation of technical documentation. Journal ofApplied Artificial Intelligence, 9(3):259– 287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
<author>Julia R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review.</title>
<date>1996</date>
<journal>Number</journal>
<booktitle>in Lecture Notes in Artificial Intelligence.</booktitle>
<volume>1083</volume>
<publisher>Springer Verlag,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1451" citStr="Jones and Galliers, 1996" startWordPosition="212" endWordPosition="216">re still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field. As discussed in (Dale and Mellish, 1998), because of the differences between language understanding and generation, most NLU evaluation techniques1 cannot be applied to generation. The main problems come from the lack of well-defined input and output for NLG systems (see also (Wilks, 1992)). Different systems assume different kinds of input, depending on their domains, tasks and target media, which makes comparative evaluation particularly 1For a comprehensive review see (Sparck Jones and Galliers, 1996). difficult.2 It is also very hard to obtain a quantitative, objective, measure of the quality of output texts, especially across different domains and genres. Therefore, NLG systems are normally evaluated with respect to their usefulness for a particular (set of) task(s), which is established by measuring user performance on these tasks, i.e., extrinsic evaluation. This is often also referred to as black-box evaluation, because it does not focus on any specific module, but evaluates the system’s performance as a whole. This paper presents one such evaluation experiment with focus on the issue</context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>Karen Sparck Jones and Julia R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Number 1083 in Lecture Notes in Artificial Intelligence. Springer Verlag, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick A Wilks</author>
</authors>
<title>Where am I coming from: The reversibility of analysis and generation in natural language processing.</title>
<date>1992</date>
<editor>In Martin Puetz, editor,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="1232" citStr="Wilks, 1992" startWordPosition="182" endWordPosition="183">ge generation, with focus on evaluating machine learning methods for text planning. 1 Introduction Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field. As discussed in (Dale and Mellish, 1998), because of the differences between language understanding and generation, most NLU evaluation techniques1 cannot be applied to generation. The main problems come from the lack of well-defined input and output for NLG systems (see also (Wilks, 1992)). Different systems assume different kinds of input, depending on their domains, tasks and target media, which makes comparative evaluation particularly 1For a comprehensive review see (Sparck Jones and Galliers, 1996). difficult.2 It is also very hard to obtain a quantitative, objective, measure of the quality of output texts, especially across different domains and genres. Therefore, NLG systems are normally evaluated with respect to their usefulness for a particular (set of) task(s), which is established by measuring user performance on these tasks, i.e., extrinsic evaluation. This is ofte</context>
</contexts>
<marker>Wilks, 1992</marker>
<rawString>Yorick A. Wilks. 1992. Where am I coming from: The reversibility of analysis and generation in natural language processing. In Martin Puetz, editor, Thirty Years ofLinguistic Evolution. John Benjamins.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G B Wills</author>
<author>I Heath</author>
<author>R M Crowder</author>
</authors>
<location>and</location>
<marker>Wills, Heath, Crowder, </marker>
<rawString>G. B. Wills, I. Heath, R.M. Crowder, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hall</author>
</authors>
<title>User evaluation of an industrial hypermedia application.</title>
<date>1999</date>
<tech>Technical report, M99/2,</tech>
<institution>University of Southampton.</institution>
<note>http://www.bib.ecs.soton.ac.uk/data/ 1444/html/html/.</note>
<marker>Hall, 1999</marker>
<rawString>W. Hall. 1999. User evaluation of an industrial hypermedia application. Technical report, M99/2, University of Southampton. http://www.bib.ecs.soton.ac.uk/data/ 1444/html/html/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>