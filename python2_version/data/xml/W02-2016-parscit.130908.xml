<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010175">
<title confidence="0.996508">
Japanese Dependency Analysis using Cascaded Chunking
</title>
<author confidence="0.992967">
Taku Kudo and Yuji Matsumoto
</author>
<affiliation confidence="0.99673">
Graduate School of Information Science,
Nara Institute of Science and Technology
</affiliation>
<email confidence="0.995215">
{taku-ku,matsu}@is.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.993815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928166666667">
In this paper, we propose a new statistical Japanese
dependency parser using a cascaded chunking
model. Conventional Japanese statistical depen-
dency parsers are mainly based on a probabilistic
model, which is not always efficient or scalable. We
propose a new method that is simple and efficient,
since it parses a sentence deterministically only de-
ciding whether the current segment modifies the
segment on its immediate right hand side. Experi-
ments using the Kyoto University Corpus show that
the method outperforms previous systems as well as
improves the parsing and training efficiency.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992259375">
Dependency analysis has been recognized as a basic
process in Japanese sentence analysis, and a num-
ber of studies have been proposed. Japanese depen-
dency structure is usually defined in terms of the
relationship between phrasal units called bunsetsu
segments (hereafter “segments”).
Most of the previous statistical approaches for
Japanese dependency analysis (Fujio and Mat-
sumoto, 1998; Haruno et al., 1999; Uchimoto et
al., 1999; Kanayama et al., 2000; Uchimoto et al.,
2000; Kudo and Matsumoto, 2000) are based on
a probabilistic model consisting of the following
two steps. First, they estimate modification prob-
abilities, in other words, how probable one segment
tends to modify another. Second the optimal combi-
nation of dependencies is searched from the all can-
didates dependencies. Such a probabilistic model is
not always efficient since it needs to calculate the
probabilities for all possible dependencies and cre-
ates n˙(n−1)/2 (where n is the number of segments
in a sentence) training examples per sentence. In
addition, the probabilistic model assumes that each
pairs of dependency structure is independent.
In this paper, we propose a new Japanese depen-
dency parser which is more efficient and simpler
than the probabilistic model, yet performs better in
training and testing on the Kyoto University Corpus.
The method parses a sentence deterministically only
deciding whether the current segment modifies seg-
ment on its immediate right hand side. Moreover,
it does not assume the independence constraint be-
tween dependencies
</bodyText>
<sectionHeader confidence="0.997862" genericHeader="method">
2 A Probabilistic Model
</sectionHeader>
<bodyText confidence="0.986447818181818">
This section describes the general formulation of the
probabilistic model for parsing which has been ap-
plied to Japanese statistical dependency analysis.
First of all, we define a sentence as a sequence
of segments B = (b1, b2 ..., bm) and its syntac-
tic structure as a sequence of dependency patterns
D = (Dep(1), Dep(2), ... , Dep(m−1)) , where
Dep(i) = j means that the segment bi depends on
(modifies) segment bj. In this framework, we as-
sume that the dependency sequence D satisfies the
following two constraints.
</bodyText>
<listItem confidence="0.9905458">
1. Japanese is a head-final language. Thus, ex-
cept for the rightmost one, each segment mod-
ifies exactly one segment among the segments
appearing to its right.
2. Dependencies do not cross one another.
</listItem>
<bodyText confidence="0.998833142857143">
Statistical dependency analysis is defined as a
searching problem for the dependency pattern D
that maximizes the conditional probability P(D|B)
of the input sequence under the above-mentioned
constraints. If we assume that the dependency prob-
abilities are mutually independent, P(D|B) can be
rewritten as:
</bodyText>
<equation confidence="0.95828125">
P(D|B) = m−1� P(Dep(i)=j  |fij)
i=1
fij = (f1, ... , fn) E Rn.
P(Dep(i)=j  |fzj) represents the probability that bz
</equation>
<bodyText confidence="0.998902642857143">
modifies bj. fzj is an n dimensional feature vector
that represents various kinds of linguistic features
related to the segments bz and bj.
We obtain Dbest = argmaxD P(D|B) taking
into all the combination of these probabilities. Gen-
erally, the optimal solution Dbest can be identified
by using bottom-up parsing algorithm such as CYK
algorithm.
The problem in the dependency structure anal-
ysis is how to estimate the dependency probabili-
ties accurately. A number of statistical and machine
learning approaches, such as Maximum Likelihood
estimation (Fujio and Matsumoto, 1998), Decision
Trees (Haruno et al., 1999), Maximum Entropy
models (Uchimoto et al., 1999; Uchimoto et al.,
2000; Kanayama et al., 2000), and Support Vector
Machines (Kudo and Matsumoto, 2000), have been
applied to estimate these probabilities.
In order to apply a machine learning algorithm to
dependency analysis, we have to prepare the pos-
itive and negative examples. Usually, in a proba-
bilistic model, all possible pairs of segments that are
in a dependency relation are used as positive exam-
ples, and two segments that appear in a sentence but
are not in a dependency relation are used as nega-
tive examples. Thus, a total of n˙(n − 1)/2 training
examples (where n is the number of segments in a
sentence) must be produced per sentence.
</bodyText>
<sectionHeader confidence="0.997449" genericHeader="method">
3 Cascaded Chunking Model
</sectionHeader>
<bodyText confidence="0.997999230769231">
In the probabilistic model, we have to estimate the
probabilities of each dependency relation. However,
some machine learning algorithms, such as SVMs,
cannot estimate these probabilities directly. Kudo
and Matsumoto (2000) used the sigmoid function
to obtain pseudo probabilities in SVMs. However,
there is no theoretical endorsement for this heuris-
tics.
Moreover, the probabilistic model is not good
in its scalability since it usually requires a total of
n˙(n − 1)/2 training examples per sentence. It will
be hard to combine the probabilistic model with
some machine learning algorithms, such as SVMs,
which require a polynomial computational cost on
the number of given training examples.
In this paper, we introduce a new method for
Japanese dependency analysis, which does not re-
quire the probabilities of dependencies and parses
a sentence deterministically. The proposed method
can be combined with any type of machine learning
algorithm that has classification ability.
The original idea of our method stems from the
cascaded chucking method which has been applied
in English parsing (Abney, 1991). Let us introduce
the basic framework of the cascaded chunking pars-
ing method:
</bodyText>
<listItem confidence="0.996118545454545">
1. A sequence of base phrases is the input for this
algorithm.
2. Scanning from the beginning of the input sen-
tence, chunk a series of base phrases into a sin-
gle non-terminal node.
3. For each chunked phrase, leave only the head
phrase, and delete all the other phrases inside
the chunk
4. Finish the algorithm if a single non-terminal
node remains, otherwise return to the step 2
and repeat.
</listItem>
<bodyText confidence="0.999213333333333">
We apply this cascaded chunking parsing tech-
nique to Japanese dependency analysis. Since
Japanese is a head-final language, and the chunk-
ing can be regarded as the creation of a dependency
between two segments, we can simplify the process
of Japanese dependency analysis as follows:
</bodyText>
<listItem confidence="0.990414818181818">
1. Put an O tag on all segments. The O tag in-
dicates that the dependency relation of the cur-
rent segment is undecided.
2. For each segment with an O tag, decide
whether it modifies the segment on its immedi-
ate right hand side. If so, the O tag is replaced
with a D tag.
3. Delete all segments with a D tag that are imme-
diately followed by a segment with an O tag.
4. Terminate the algorithm if a single segment re-
mains, otherwise return to step 2 and repeat.
</listItem>
<bodyText confidence="0.995803090909091">
Figure 1 shows an example of the parsing process
with the cascaded chunking model.
The input for the model is the linguistic features
related to the modifier and modifiee, and the output
from the model is either of the tags (D or O). In
training, the model simulates the parsing algorithm
by consulting the correct answer from the training
annotated corpus. During the training, positive (D)
and negative (O) examples are collected. In testing,
the model consults the trained system and parses the
input with the cascaded chunking algorithm.
</bodyText>
<figure confidence="0.931741">
Tag: O
Finish
</figure>
<figureCaption confidence="0.9954985">
Figure 1: Example of the parsing process with cas-
caded chunking model
</figureCaption>
<bodyText confidence="0.999729666666667">
We think this proposed cascaded chunking model
has the following advantages compared with the tra-
ditional probabilistic models.
</bodyText>
<listItem confidence="0.991381">
• Simple and Efficient
</listItem>
<bodyText confidence="0.96019678125">
If we use the CYK algorithm, the probabilistic
model requires O(n3) parsing time, (where n
is the number of segments in a sentence.). On
the other hand, the cascaded chunking model
requires O(n2) in the worst case when all seg-
ments modify the rightmost segment. The ac-
tual parsing time is usually lower than O(n2),
since most of segments modify segment on its
immediate right hand side.
Furthermore, in the cascaded chunking model,
the training examples are extracted using the
parsing algorithm itself. The training exam-
ples required for the cascaded chunking model
is much smaller than that for the probabilistic
model. The model reduces the training cost
significantly and enables training using larger
amounts of annotated corpus.
• No assumption on the independence be-
tween dependency relations
The probabilistic model assumes that depen-
dency relations are independent. However,
there are some cases in which one cannot parse
a sentence correctly with this assumption. For
example, coordinate structures cannot be al-
ways parsed with the independence constraint.
The cascaded chunking model parses and es-
timates relations simultaneously. This means
that one can use all dependency relations,
which have narrower scope than that of the cur-
rent focusing relation being considered, as fea-
ture sets. We describe the details in the next
section.
</bodyText>
<listItem confidence="0.906985">
• Independence from machine learning algo-
rithm
</listItem>
<bodyText confidence="0.999975333333333">
The cascaded chunking model can be com-
bined with any machine learning algorithm that
works as a binary classifier, since the cascaded
chunking model parses a sentence determinis-
tically only deciding whether or not the current
segment modifies the segment on its immedi-
ate right hand side. Probabilities of dependen-
cies are not always necessary for the cascaded
chunking model.
</bodyText>
<subsectionHeader confidence="0.997247">
3.1 Dynamic and Static Features
</subsectionHeader>
<bodyText confidence="0.986652185185185">
Linguistic features that are supposed to be effective
in Japanese dependency analysis are: head words
and their parts-of-speech tags, functional words and
inflection forms of the words that appear at the end
of segments, distance between two segments, exis-
tence of punctuation marks. As those are solely de-
fined by the pair of segments, we refer to them as
the static features.
Japanese dependency relations are heavily con-
strained by such static features since the inflection
forms and postpositional particles constrain the de-
pendency relation. However, when a sentence is
long and there are more than one possible depen-
dency, static features, by themselves cannot deter-
mine the correct dependency.
To cope with this problem, Kudo and Matsumoto
(2000) introduced a new type of features called dy-
namic features, which are created dynamically dur-
ing the parsing process. For example, if some rela-
tion is determined, this modification relation may
have some influence on other dependency relation.
Therefore, once a segment has been determined to
modify another segment, such information is kept in
both of the segments and is added to them as a new
feature. Specifically, we take the following three
types of dynamic features in our experiments.
He her warm heart be moved
</bodyText>
<figure confidence="0.991134736842105">
( He was moved by her warm heart.)
Initialization
Input:
Tag: O O D D O
Input:
Tag: O O O O O
Deleted
Input:
Deleted
Input:
Tag: O D O
Deleted
Input:
Tag: D O
Deleted
Input:
Tag: O D D O
modify or not?
B B Modifier ... A A Modifiee ... C
</figure>
<figureCaption confidence="0.999847">
Figure 2: Three types of Dynamic Features
</figureCaption>
<bodyText confidence="0.977063666666667">
A. The segments which modify the current candi-
date modifiee. (boxes marked with A in Figure
2)
B. The segments which modify the current candi-
date modifier. (boxes marked with B in Figure
2)
C. The segment which is modified by the current
candidate modifiee. (boxes marked with C in
Figure 2)
</bodyText>
<sectionHeader confidence="0.981946" genericHeader="method">
4 Support Vector Machines
</sectionHeader>
<bodyText confidence="0.999894947368421">
Although any kind of machine learning algorithm
can be applied to the cascaded chunking model, we
use Support Vector Machines (Vapnik,1998) for our
experiments because of their state-of-the-art perfor-
mance and generalization ability.
SVM is a binary linear classifier trained
from the samples, each of which belongs ei-
ther to positive or negative class as follows:
(x1, y1), ... , (xl, yl) (xi E Rn, yi E {+1, −1}),
where xi is a feature vector of the i-th sample rep-
resented by an n dimensional vector, and yi is the
class (positive(+1) or negative(−1) class) label of
the i-th sample. SVMs find the optimal separating
hyperplane (w • x + b) based on the maximal mar-
gin strategy. The margin can be seen as the distance
between the critical examples and the separating hy-
perplane. We omit the details here, the maximal
margin strategy can be realized by the following op-
timization problem:
</bodyText>
<sectionHeader confidence="0.808359" genericHeader="method">
Minimize: L(w) = 12IlwIl2
</sectionHeader>
<subsectionHeader confidence="0.893023">
Subject to : yi[(w • xi) + b] &gt; 1 (i = 1, . . . , l).
</subsectionHeader>
<bodyText confidence="0.999947461538461">
Furthermore, SVMs have the potential to carry
out non-linear classifications. Though we leave the
details to (Vapnik, 1998), the optimization problem
can be rewritten into a dual form, where all feature
vectors appear as their dot products. By simply sub-
stituting every dot product of xi and xj in dual form
with a Kernel function K(xi, xj), SVMs can han-
dle non-linear hypotheses. Among many kinds of
Kernel functions available, we will focus on the d-
th polynomial kernel: K(xi, xj) = (xi • xj + 1)d.
Use of d-th polynomial kernel functions allows us to
build an optimal separating hyperplane which takes
into account all combinations of features up to d.
</bodyText>
<sectionHeader confidence="0.996326" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.963093">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.99827">
We used the following two annotated corpora for
our experiments.
</bodyText>
<listItem confidence="0.996836">
• Standard data set
</listItem>
<bodyText confidence="0.999720333333333">
This data set consists of the Kyoto University
text corpus Version 2.0 (Kurohashi and Nagao,
1997). We used 7,958 sentences from the ar-
ticles on January 1st to January 7th as training
examples, and 1,246 sentences from the arti-
cles on January 9th as the test data. This data
set was used in (Uchimoto et al., 1999; Uchi-
moto et al., 2000) and (Kudo and Matsumoto,
2000).
</bodyText>
<listItem confidence="0.997705">
• Large data set
</listItem>
<bodyText confidence="0.997232717391305">
In order to investigate the scalability of the cas-
caded chunking model, we prepared larger data
set. We used all 38,383 sentences of the Kyoto
University text corpus Version 3.0. The train-
ing and test data were generated by a two-fold
cross validation.
The feature sets used in our experiments are
shown in Table 1. The static features are basically
taken from Uchimoto’s list (Uchimoto et al., 1999).
Head Word (HW) is the rightmost content word
in the segment. Functional Word (FW) is set as fol-
lows:
- FW = the rightmost functional word, if there is
a functional word in the segment
- FW = the rightmost inflection form, if there is a
predicate in the segment
- FW = same as the HW, otherwise.
The static features include the information on ex-
istence of brackets, question marks and punctuation
marks, etc. Besides, there are features that show
the relative relation of two segments, such as dis-
tance, and existence of brackets, quotation marks
and punctuation marks between them.
For a segment X and its dynamic feature Y
(where Y is of type A or B), we set the Functional
Representation (FR) feature of X based on the FW
of X (X-FW) as follows:
- FR = lexical form of X-FW if POS of X-FW is
particle, adverb, adnominal or conjunction
- FR = inflectional form ofX-FW ifX-FW has an
inflectional form.
- FR = the POS tag ofX-FW, otherwise.
For a segment X and its dynamic feature C, we
set POS tag and POS-subcategory of the HW of X.
All our experiments are carried out on Alpha-
Sever 8400 (21164A 500Mhz) for training and
Linux (PentiumIII 1GHz) for testing. We used a
third degree polynomial kernel function, which is
exactly the same setting in (Kudo and Matsumoto,
2000).
Performance on the test data is measured using
dependency accuracy and sentence accuracy. De-
pendency accuracy is the percentage of correct de-
pendencies out of all dependency relations. Sen-
tence accuracy is the percentage of sentences in
which all dependencies are determined correctly.
</bodyText>
<subsectionHeader confidence="0.996393">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999988647058824">
The results for the new cascaded chunking model as
well as for the previous probabilistic model based
on SVMs (Kudo and Matsumoto, 2000) are summa-
rized in Table 2. We cannot employ the experiments
for the probabilistic model using large dataset, since
the data size is too large for our current SVMs learn-
ing program to terminate in a realistic time period.
Even though the number of training examples
used for the cascaded chunking model is less than
a quarter of that for the probabilistic model, and the
used feature set is the same, dependency accuracy
and sentence accuracy are improved using the cas-
caded chunking model (89.09% → 89.29%, 46.17%
→ 47.53%).
The time required for training and parsing are sig-
nificantly reduced by applying the cascaded chunk-
ing model (336h.→8h, 2.1sec.→ 0.5sec.).
</bodyText>
<subsectionHeader confidence="0.9921265">
5.3 Probabilistic model vs. Cascaded
Chunking model
</subsectionHeader>
<bodyText confidence="0.999875063829787">
As can be seen Table 2, the cascaded chunking
model is more accurate, efficient and scalable than
the probabilistic model. It is difficult to apply the
probabilistic model to the large data set, since it
takes no less than 336 hours (2 weeks) to carry out
the experiments even with the standard data set, and
SVMs require quadratic or more computational cost
on the number of training examples.
For the first impression, it may seems natural that
higher accuracy is achieved with the probabilistic
model, since all candidate dependency relations are
used as training examples. However, the experimen-
tal results show that the cascaded chunking model
performs better. Here we list what the most signif-
icant contributions are and how well the cascaded
chunking model behaves compared with the proba-
bilistic model.
The probabilistic model is trained with all candi-
date pairs of segments in the training corpus. The
problem of this training is that exceptional depen-
dency relations may be used as training examples.
For example, suppose a segment which appears to
right hand side of the correct modifiee and has a
similar content word, the pair with this segment be-
comes a negative example. However, this is nega-
tive because there is a better and correct candidate
at a different point in the sentence. Therefore, this
may not be a true negative example, meaning that
this can be positive in other sentences. In addition,
if a segment is not modified by a modifier because of
cross dependency constraints but has a similar con-
tent word with correct modifiee, this relation also
becomes an exception. Actually, we cannot ignore
these exceptions, since most segments modify a seg-
ment on its immediate right hand side. By using
all candidates of dependency relation as the training
examples, we have committed to a number of excep-
tions which are hard to be trained upon. Looking in
particular on a powerful heuristics for dependency
structure analysis: “A segment tends to modify a
nearer segment if possible,” it will be most impor-
tant to train whether the current segment modifies
the segment on its immediate right hand side. The
cascaded chunking model is designed along with
this heuristics and can remove the exceptional re-
lations which has less potential to improve perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.998932">
5.4 Effects of Dynamic Features
</subsectionHeader>
<bodyText confidence="0.981715363636363">
Figure 3 shows the relationship between the size of
the training data and the parsing accuracy. This fig-
ure also shows the accuracy with and without the
dynamic features. Generally, the results with the
dynamic feature set is better than the results with-
out it. The dynamic features constantly outperform
static features when the size of the training data is
large. In most cases, the improvements is consider-
able.
Table 3 summarizes the performance without
some dynamic features. From these results, we can
</bodyText>
<table confidence="0.996692333333333">
Static Features Modifier/Modifiee Head Word (surface-form, POS, POS-
segments subcategory, inflection-type, inflection-
form), Functional Word (surface-form,
POS, POS-subcategory, inflection-type,
inflection-form), brackets, quotation-
marks, punctuation-marks, position in
sentence (beginning, end)
Dynamic Features Between two seg- distance(1,2-5,6-), case-particles, brackets,
ments quotation-marks, punctuation-marks
Type A,B Form of inflection represented with Func-
Type C tional Representation
POS and POS-subcategory of Head word
</table>
<tableCaption confidence="0.972911">
Table 1: Features used in our experiments
</tableCaption>
<table confidence="0.999935625">
Data Set Standard Large
Model Cascaded Chunking Probabilistic Cascaded Chunking Probabilistic
Dependency Acc. (%) 89.29 89.09 90.46 N/A
Sentence Acc. (%) 47.53 46.17 53.16 N/A
# of training sentences 7,956 7,956 19,191 19,191
# of training examples 110,355 459,105 261,254 1,074,316
Training Time (hours) 8 336 48 N/A
Parsing Time (sec./sentence) 0.5 2.1 0.7 N/A
</table>
<tableCaption confidence="0.941805">
Table 2: Cascaded Chunking model vs Probabilistic model
</tableCaption>
<table confidence="0.970763083333333">
1000 2000 3000 4000 5000 6000 7000 8000
Number of Training Data (sentences)
Deleted type of Diff. from the model with dynamic features
dynamic feature
Dependency Acc. Sentence Acc.
A -0.28% -0.89%
B -0.10% -0.89%
C -0.28% -0.56%
AB -0.33% -1.21%
AC -0.55% -0.97%
BC -0.54% -1.61%
ABC -0.58% -2.34%
</table>
<figure confidence="0.9533504">
89.5
89
88.5
88
87.5
87
86.5
86
’dynamic-c’
’static-c’
</figure>
<figureCaption confidence="0.9072675">
Figure 3: Training Data vs. Accuracy (cascaded
chunking/standard data set)
</figureCaption>
<tableCaption confidence="0.720589">
Table 3: Effects of dynamic features with the stan-
dard data set
</tableCaption>
<bodyText confidence="0.9006175">
conclude that all dynamic features are effective in
improving the performance.
</bodyText>
<subsectionHeader confidence="0.984316">
5.5 Comparison with Related Work
</subsectionHeader>
<bodyText confidence="0.998920785714286">
Table 4 summarizes recent results on Japanese de-
pendency analysis.
Uchimoto et al. (2000) report that using the Ky-
oto University Corpus for their training and testing,
they achieve around 87.93% accuracy by building
statistical model based on the Maximum Entropy
framework. They extend the original probabilistic
model, which learns only two class; ‘modify‘ and
‘not modify‘, to the one that learns three classes;
‘between‘, ‘modify‘ and ‘beyond‘. Their model can
also avoid the influence of the exceptional depen-
dency relations. Using same training and test data,
we can achieve accuracy of 89.29%. The difference
is considerable.
</bodyText>
<table confidence="0.827517875">
Model Training Corpus (# of sentences) Acc. (%)
Our Model Cascaded Chunking (SVMs) Kyoto Univ. (19,191) 90.46
Kyoto Univ. (7,956) 89.29
Kudo et al 00 Probabilistic model (SVMs) Kyoto Univ. (7,956) 89.09
Uchimoto et al 00,98 Probabilistic model (ME + posterior context) Kyoto Univ. (7,956) 87.93
Kanayama et al 99 Probabilistic model (ME + HPSG) EDR (192,778) 88.55
Haruno et al 98 Probabilistic model (DT + Boosting) EDR (50,000) 85.03
Fujio et al 98 Probabilistic model (ML) EDR (190,000) 86.67
</table>
<tableCaption confidence="0.999956">
Table 4: Comparison with the related work
</tableCaption>
<bodyText confidence="0.999958035714286">
Kanayama et al. (2000) use an HPSG-based
Japanese grammar to restrict the candidate depen-
dencies. Their model uses at most three candidates
restricted by the grammar as features; the nearest,
the second nearest, and the farthest from the modi-
fier. Thus, their model can take longer context into
account, and disambiguate complex dependency re-
lations. However, the features are still static, and dy-
namic features are not used in their model. We can-
not directly compare their model with ours because
they use a different corpus, EDR corpus, which is
ten times as large as the corpus we used. Never-
theless, they reported an accuracy 88.55%, which is
worse than our model.
Haruno et al. (99) report that using the EDR
Corpus for their training and testing, they achieve
around 85.03% accuracy with Decision Tree and
Boosting. Although Decision Tree can take com-
binations of features as SVMs, it easily overfits on
its own. To avoid overfitting, Decision Tree is usu-
ally used as an weak learner for Boosting. Com-
bining Boosting technique with Decision Tree, the
performance may be improved. However, Haruno
et al. (99) report that the performance with Decision
Tree falls down when they added lexical entries with
lower frequencies as features even using Boosting.
We think that Decision Tree requires a careful fea-
ture selection for achieving higher accuracy.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9994584">
We presented a new Japanese dependency parser
using a cascaded chunking model which achieves
90.46% accuracy using the Kyoto University Cor-
pus. Our model parses a sentence deterministically
only deciding whether the current segment modifies
the segment on its immediate right hand side. Our
model outperforms the previous probabilistic model
with respect to accuracy and efficiency. In addition,
we showed that dynamic features significantly con-
tribute to improve the performance.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828228571429">
Steven Abney. 1991. Parsing By Chunking. In
Principle-Based Parsing. Kluwer Academic Pub-
lishers.
Masakazu Fujio and Yuji Matsumoto. 1998.
Japanese Dependency Structure Analysis based
on Lexicalized Statistics. In Proceedings of
EMNLP ’98, pages 87–96.
Msahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1999. Using Decision Trees to Con-
struct a Practical Parser. Machine Learning,
34:131–149.
Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mit-
suishi, and Jun’ichi Tsujii. 2000. A Hybrid
Japanese Parser with Hand-crafted Grammar and
Statistics. In Proceedings of the COLING 2000,
pages 411–417.
Taku Kudo and Yuji Matsumoto. 2000. Japanese
Dependency Structure Analysis based on Support
Vector Machines. In Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora, pages 18–25.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
University text corpus project. In Proceedings of
the ANLP, Japan, pages 115–118.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi
Isahara. 1999. Japanese Dependency Structure
Analysis Based on Maximum Entropy Models.
In Proceedings of the EACL, pages 196–203.
Kiyotaka Uchimoto, Masaki Murata, Satoshi
Sekine, and Hitoshi Isahara. 2000. Dependency
model using posterior context. In Procedings of
Sixth International Workshop on Parsing Tech-
nologies.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. Wiley-Interscience.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985559">
<title confidence="0.999055">Japanese Dependency Analysis using Cascaded Chunking</title>
<author confidence="0.993673">Kudo Matsumoto</author>
<affiliation confidence="0.9972765">Graduate School of Information Science, Nara Institute of Science and Technology</affiliation>
<abstract confidence="0.999820615384615">In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model. Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable. We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing By Chunking. In Principle-Based Parsing.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="5965" citStr="Abney, 1991" startWordPosition="931" endWordPosition="932">tence. It will be hard to combine the probabilistic model with some machine learning algorithms, such as SVMs, which require a polynomial computational cost on the number of given training examples. In this paper, we introduce a new method for Japanese dependency analysis, which does not require the probabilities of dependencies and parses a sentence deterministically. The proposed method can be combined with any type of machine learning algorithm that has classification ability. The original idea of our method stems from the cascaded chucking method which has been applied in English parsing (Abney, 1991). Let us introduce the basic framework of the cascaded chunking parsing method: 1. A sequence of base phrases is the input for this algorithm. 2. Scanning from the beginning of the input sentence, chunk a series of base phrases into a single non-terminal node. 3. For each chunked phrase, leave only the head phrase, and delete all the other phrases inside the chunk 4. Finish the algorithm if a single non-terminal node remains, otherwise return to the step 2 and repeat. We apply this cascaded chunking parsing technique to Japanese dependency analysis. Since Japanese is a head-final language, and</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing By Chunking. In Principle-Based Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Fujio</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Structure Analysis based on Lexicalized Statistics.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP ’98,</booktitle>
<pages>87--96</pages>
<contexts>
<context position="1200" citStr="Fujio and Matsumoto, 1998" startWordPosition="169" endWordPosition="173">current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentenc</context>
<context position="4095" citStr="Fujio and Matsumoto, 1998" startWordPosition="631" endWordPosition="634">Dep(i)=j |fzj) represents the probability that bz modifies bj. fzj is an n dimensional feature vector that represents various kinds of linguistic features related to the segments bz and bj. We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities. Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm. The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately. A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities. In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples. Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative e</context>
</contexts>
<marker>Fujio, Matsumoto, 1998</marker>
<rawString>Masakazu Fujio and Yuji Matsumoto. 1998. Japanese Dependency Structure Analysis based on Lexicalized Statistics. In Proceedings of EMNLP ’98, pages 87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Msahiko Haruno</author>
<author>Satoshi Shirai</author>
<author>Yoshifumi Ooyama</author>
</authors>
<title>Using Decision Trees to Construct a Practical Parser.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--131</pages>
<contexts>
<context position="1221" citStr="Haruno et al., 1999" startWordPosition="174" endWordPosition="177">e segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples </context>
<context position="4133" citStr="Haruno et al., 1999" startWordPosition="637" endWordPosition="640">at bz modifies bj. fzj is an n dimensional feature vector that represents various kinds of linguistic features related to the segments bz and bj. We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities. Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm. The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately. A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities. In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples. Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples. Thus, a total of n˙(n − 1)/2 </context>
</contexts>
<marker>Haruno, Shirai, Ooyama, 1999</marker>
<rawString>Msahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama. 1999. Using Decision Trees to Construct a Practical Parser. Machine Learning, 34:131–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Kentaro Torisawa</author>
<author>Yutaka Mitsuishi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A Hybrid Japanese Parser with Hand-crafted Grammar and Statistics.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING</booktitle>
<pages>411--417</pages>
<contexts>
<context position="1267" citStr="Kanayama et al., 2000" startWordPosition="182" endWordPosition="185">Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples per sentence. In addition, the probabilistic m</context>
<context position="4227" citStr="Kanayama et al., 2000" startWordPosition="652" endWordPosition="655">inguistic features related to the segments bz and bj. We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities. Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm. The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately. A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities. In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples. Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples. Thus, a total of n˙(n − 1)/2 training examples (where n is the number of segments in a sentence) must be produced per sente</context>
<context position="22227" citStr="Kanayama et al. (2000)" startWordPosition="3631" endWordPosition="3634">n achieve accuracy of 89.29%. The difference is considerable. Model Training Corpus (# of sentences) Acc. (%) Our Model Cascaded Chunking (SVMs) Kyoto Univ. (19,191) 90.46 Kyoto Univ. (7,956) 89.29 Kudo et al 00 Probabilistic model (SVMs) Kyoto Univ. (7,956) 89.09 Uchimoto et al 00,98 Probabilistic model (ME + posterior context) Kyoto Univ. (7,956) 87.93 Kanayama et al 99 Probabilistic model (ME + HPSG) EDR (192,778) 88.55 Haruno et al 98 Probabilistic model (DT + Boosting) EDR (50,000) 85.03 Fujio et al 98 Probabilistic model (ML) EDR (190,000) 86.67 Table 4: Comparison with the related work Kanayama et al. (2000) use an HPSG-based Japanese grammar to restrict the candidate dependencies. Their model uses at most three candidates restricted by the grammar as features; the nearest, the second nearest, and the farthest from the modifier. Thus, their model can take longer context into account, and disambiguate complex dependency relations. However, the features are still static, and dynamic features are not used in their model. We cannot directly compare their model with ours because they use a different corpus, EDR corpus, which is ten times as large as the corpus we used. Nevertheless, they reported an a</context>
</contexts>
<marker>Kanayama, Torisawa, Mitsuishi, Tsujii, 2000</marker>
<rawString>Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mitsuishi, and Jun’ichi Tsujii. 2000. A Hybrid Japanese Parser with Hand-crafted Grammar and Statistics. In Proceedings of the COLING 2000, pages 411–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Structure Analysis based on Support Vector Machines.</title>
<date>2000</date>
<booktitle>In Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="1317" citStr="Kudo and Matsumoto, 2000" startWordPosition="190" endWordPosition="193">show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples per sentence. In addition, the probabilistic model assumes that each pairs of dependency structu</context>
<context position="4283" citStr="Kudo and Matsumoto, 2000" startWordPosition="660" endWordPosition="663"> We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities. Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm. The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately. A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities. In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples. Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples. Thus, a total of n˙(n − 1)/2 training examples (where n is the number of segments in a sentence) must be produced per sentence. 3 Cascaded Chunking Model In the probabilistic mode</context>
<context position="10511" citStr="Kudo and Matsumoto (2000)" startWordPosition="1680" endWordPosition="1683">tags, functional words and inflection forms of the words that appear at the end of segments, distance between two segments, existence of punctuation marks. As those are solely defined by the pair of segments, we refer to them as the static features. Japanese dependency relations are heavily constrained by such static features since the inflection forms and postpositional particles constrain the dependency relation. However, when a sentence is long and there are more than one possible dependency, static features, by themselves cannot determine the correct dependency. To cope with this problem, Kudo and Matsumoto (2000) introduced a new type of features called dynamic features, which are created dynamically during the parsing process. For example, if some relation is determined, this modification relation may have some influence on other dependency relation. Therefore, once a segment has been determined to modify another segment, such information is kept in both of the segments and is added to them as a new feature. Specifically, we take the following three types of dynamic features in our experiments. He her warm heart be moved ( He was moved by her warm heart.) Initialization Input: Tag: O O D D O Input: T</context>
<context position="13762" citStr="Kudo and Matsumoto, 2000" startWordPosition="2259" endWordPosition="2262">ns allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d. 5 Experiments and Discussion 5.1 Experimental Setting We used the following two annotated corpora for our experiments. • Standard data set This data set consists of the Kyoto University text corpus Version 2.0 (Kurohashi and Nagao, 1997). We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data. This data set was used in (Uchimoto et al., 1999; Uchimoto et al., 2000) and (Kudo and Matsumoto, 2000). • Large data set In order to investigate the scalability of the cascaded chunking model, we prepared larger data set. We used all 38,383 sentences of the Kyoto University text corpus Version 3.0. The training and test data were generated by a two-fold cross validation. The feature sets used in our experiments are shown in Table 1. The static features are basically taken from Uchimoto’s list (Uchimoto et al., 1999). Head Word (HW) is the rightmost content word in the segment. Functional Word (FW) is set as follows: - FW = the rightmost functional word, if there is a functional word in the seg</context>
<context position="15456" citStr="Kudo and Matsumoto, 2000" startWordPosition="2563" endWordPosition="2566"> B), we set the Functional Representation (FR) feature of X based on the FW of X (X-FW) as follows: - FR = lexical form of X-FW if POS of X-FW is particle, adverb, adnominal or conjunction - FR = inflectional form ofX-FW ifX-FW has an inflectional form. - FR = the POS tag ofX-FW, otherwise. For a segment X and its dynamic feature C, we set POS tag and POS-subcategory of the HW of X. All our experiments are carried out on AlphaSever 8400 (21164A 500Mhz) for training and Linux (PentiumIII 1GHz) for testing. We used a third degree polynomial kernel function, which is exactly the same setting in (Kudo and Matsumoto, 2000). Performance on the test data is measured using dependency accuracy and sentence accuracy. Dependency accuracy is the percentage of correct dependencies out of all dependency relations. Sentence accuracy is the percentage of sentences in which all dependencies are determined correctly. 5.2 Experimental Results The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs (Kudo and Matsumoto, 2000) are summarized in Table 2. We cannot employ the experiments for the probabilistic model using large dataset, since the data size is too large for our </context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Japanese Dependency Structure Analysis based on Support Vector Machines. In Empirical Methods in Natural Language Processing and Very Large Corpora, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Kyoto University text corpus project.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP, Japan,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="13492" citStr="Kurohashi and Nagao, 1997" startWordPosition="2208" endWordPosition="2211">ry dot product of xi and xj in dual form with a Kernel function K(xi, xj), SVMs can handle non-linear hypotheses. Among many kinds of Kernel functions available, we will focus on the dth polynomial kernel: K(xi, xj) = (xi • xj + 1)d. Use of d-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d. 5 Experiments and Discussion 5.1 Experimental Setting We used the following two annotated corpora for our experiments. • Standard data set This data set consists of the Kyoto University text corpus Version 2.0 (Kurohashi and Nagao, 1997). We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data. This data set was used in (Uchimoto et al., 1999; Uchimoto et al., 2000) and (Kudo and Matsumoto, 2000). • Large data set In order to investigate the scalability of the cascaded chunking model, we prepared larger data set. We used all 38,383 sentences of the Kyoto University text corpus Version 3.0. The training and test data were generated by a two-fold cross validation. The feature sets used in our experiments are shown in Tabl</context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1997. Kyoto University text corpus project. In Proceedings of the ANLP, Japan, pages 115–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese Dependency Structure Analysis Based on Maximum Entropy Models.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL,</booktitle>
<pages>196--203</pages>
<contexts>
<context position="1244" citStr="Uchimoto et al., 1999" startWordPosition="178" endWordPosition="181">diate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples per sentence. In additi</context>
<context position="4180" citStr="Uchimoto et al., 1999" startWordPosition="644" endWordPosition="647">ture vector that represents various kinds of linguistic features related to the segments bz and bj. We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities. Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm. The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately. A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities. In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples. Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples. Thus, a total of n˙(n − 1)/2 training examples (where n is the number of seg</context>
<context position="13707" citStr="Uchimoto et al., 1999" startWordPosition="2249" endWordPosition="2252">i • xj + 1)d. Use of d-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d. 5 Experiments and Discussion 5.1 Experimental Setting We used the following two annotated corpora for our experiments. • Standard data set This data set consists of the Kyoto University text corpus Version 2.0 (Kurohashi and Nagao, 1997). We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data. This data set was used in (Uchimoto et al., 1999; Uchimoto et al., 2000) and (Kudo and Matsumoto, 2000). • Large data set In order to investigate the scalability of the cascaded chunking model, we prepared larger data set. We used all 38,383 sentences of the Kyoto University text corpus Version 3.0. The training and test data were generated by a two-fold cross validation. The feature sets used in our experiments are shown in Table 1. The static features are basically taken from Uchimoto’s list (Uchimoto et al., 1999). Head Word (HW) is the rightmost content word in the segment. Functional Word (FW) is set as follows: - FW = the rightmost fu</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Proceedings of the EACL, pages 196–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Masaki Murata</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency model using posterior context.</title>
<date>2000</date>
<booktitle>In Procedings of Sixth International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="1290" citStr="Uchimoto et al., 2000" startWordPosition="186" endWordPosition="189">yoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples per sentence. In addition, the probabilistic model assumes that each </context>
<context position="4203" citStr="Uchimoto et al., 2000" startWordPosition="648" endWordPosition="651">ents various kinds of linguistic features related to the segments bz and bj. We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities. Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm. The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately. A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities. In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples. Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples. Thus, a total of n˙(n − 1)/2 training examples (where n is the number of segments in a sentence) mu</context>
<context position="13731" citStr="Uchimoto et al., 2000" startWordPosition="2253" endWordPosition="2257">th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d. 5 Experiments and Discussion 5.1 Experimental Setting We used the following two annotated corpora for our experiments. • Standard data set This data set consists of the Kyoto University text corpus Version 2.0 (Kurohashi and Nagao, 1997). We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data. This data set was used in (Uchimoto et al., 1999; Uchimoto et al., 2000) and (Kudo and Matsumoto, 2000). • Large data set In order to investigate the scalability of the cascaded chunking model, we prepared larger data set. We used all 38,383 sentences of the Kyoto University text corpus Version 3.0. The training and test data were generated by a two-fold cross validation. The feature sets used in our experiments are shown in Table 1. The static features are basically taken from Uchimoto’s list (Uchimoto et al., 1999). Head Word (HW) is the rightmost content word in the segment. Functional Word (FW) is set as follows: - FW = the rightmost functional word, if there </context>
<context position="21124" citStr="Uchimoto et al. (2000)" startWordPosition="3457" endWordPosition="3460">ces) Deleted type of Diff. from the model with dynamic features dynamic feature Dependency Acc. Sentence Acc. A -0.28% -0.89% B -0.10% -0.89% C -0.28% -0.56% AB -0.33% -1.21% AC -0.55% -0.97% BC -0.54% -1.61% ABC -0.58% -2.34% 89.5 89 88.5 88 87.5 87 86.5 86 ’dynamic-c’ ’static-c’ Figure 3: Training Data vs. Accuracy (cascaded chunking/standard data set) Table 3: Effects of dynamic features with the standard data set conclude that all dynamic features are effective in improving the performance. 5.5 Comparison with Related Work Table 4 summarizes recent results on Japanese dependency analysis. Uchimoto et al. (2000) report that using the Kyoto University Corpus for their training and testing, they achieve around 87.93% accuracy by building statistical model based on the Maximum Entropy framework. They extend the original probabilistic model, which learns only two class; ‘modify‘ and ‘not modify‘, to the one that learns three classes; ‘between‘, ‘modify‘ and ‘beyond‘. Their model can also avoid the influence of the exceptional dependency relations. Using same training and test data, we can achieve accuracy of 89.29%. The difference is considerable. Model Training Corpus (# of sentences) Acc. (%) Our Model</context>
</contexts>
<marker>Uchimoto, Murata, Sekine, Isahara, 2000</marker>
<rawString>Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hitoshi Isahara. 2000. Dependency model using posterior context. In Procedings of Sixth International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="12722" citStr="Vapnik, 1998" startWordPosition="2079" endWordPosition="2080">n n dimensional vector, and yi is the class (positive(+1) or negative(−1) class) label of the i-th sample. SVMs find the optimal separating hyperplane (w • x + b) based on the maximal margin strategy. The margin can be seen as the distance between the critical examples and the separating hyperplane. We omit the details here, the maximal margin strategy can be realized by the following optimization problem: Minimize: L(w) = 12IlwIl2 Subject to : yi[(w • xi) + b] &gt; 1 (i = 1, . . . , l). Furthermore, SVMs have the potential to carry out non-linear classifications. Though we leave the details to (Vapnik, 1998), the optimization problem can be rewritten into a dual form, where all feature vectors appear as their dot products. By simply substituting every dot product of xi and xj in dual form with a Kernel function K(xi, xj), SVMs can handle non-linear hypotheses. Among many kinds of Kernel functions available, we will focus on the dth polynomial kernel: K(xi, xj) = (xi • xj + 1)d. Use of d-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d. 5 Experiments and Discussion 5.1 Experimental Setting We used the f</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>