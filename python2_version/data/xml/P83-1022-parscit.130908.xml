<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.878065">
DESIGN OF A KNOWLEDGE-BASED REPORT
GENERATOR
</title>
<author confidence="0.988167">
Karen Kukich
</author>
<affiliation confidence="0.867404">
University of Pittsburgh
Bell Telephone Laboratories
</affiliation>
<address confidence="0.802178">
Murray Ell, NJ 07974
</address>
<email confidence="0.549883">
ABSTRACT
</email>
<bodyText confidence="0.9991815">
Knowledge-Based Report Generation is a technique
for automatically generating natural language reports
from computer databases. It is so named because it
applies knowledge-based expert systems software to the
problem of text generation. The first application of the
technique, a system for generating natural language
stock reports from a daily stock quotes database, is par-
tially implemented. Three fundamental principles of the
technique are its use of domain-specific semantic and
linguistic knowledge, its use of macro-level semantic and
linguistic constructs (such as whole messages, a phrasal
lexicon, and a sentence-combining grammar), and its
production system approach to knowledge representa-
tion.
</bodyText>
<sectionHeader confidence="0.9880475" genericHeader="method">
I. WHAT IS KNOWLEDGE-BASED
REPORT GENERATION
</sectionHeader>
<bodyText confidence="0.99996918918919">
A knowledge-based report generator is a computer
program whose function â€¢ is to generate natural language
summaries from computer databases. For example,
knowledge-based report generators can be designed to
generate daily stock market reports from a stock quotes
database, daily weather reports from a meteorological
database, weekly sales reports from corporate databases,
or quarterly economic reports from U. S. Commerce
Department databases, etc. A separate generator must
be implemented for each domain of discourse because
each knowledge-based report generator contains
domain-specific knowledge which is used to infer
interesting messages from the database and to express
those messages in the sublanguage of the domain of
discourse. The technique of knowledge-based report
generation is generalizable across domains, however, and
the actual text generation component of the report gen-
erator, which comprises roughly one-quarter of the code,
is directly transportable and readily tailorable.
Knowledge-based report generation is a practical
approach to text generation. It&apos;s three fundamental
tenets are the following. First, it assumes that much
domain-specific semantic, linguistic, and rhetoric
knowledge is required in order for a computer to
automatically produce intelligent and fluent text.
Second, it assumes that production system languages,
such as those used to build expert systems, are well-
suited to the task of representing and integrating seman-
tic, linguistic, and rhetoric knowledge. Finally, it holds
that macro-level knowledge units, such as whole seman-
tic messages, a phrasal lexicon, clausal grammatical
categories, and a clause-combining grammar, provide an
appropriate level of knowledge representation for gen-
erating that type of text which may be categorized as
periodic summary reports. These three tenets guide the
design and implementation of a knowledge-based report
generation system.
</bodyText>
<sectionHeader confidence="0.9865525" genericHeader="method">
II. SAMPLE OUTPUT FROM A
KNOWLEDGE-BASED REPORT GENERATOR
</sectionHeader>
<bodyText confidence="0.9902775">
The first application of the technique of
knowledge-based report generation is a partially imple-
mented stock report generator called Ana. Data from a
Dow Jones stock quotes database serves as input to the
system, and the opening paragraphs of a stock market
summary are produced as output. As more semantic and
linguistic knowledge about the stock market is added to
the system, it will be able to generate longer, more
informative reports.
Figure 1 depicts a portion of the actual data submit-
ted to Ana for January 12, 1983. A hand drawn graph
of the same data is included. The following text samples
are Ana&apos;s interpretation of the data on two different
runs.
</bodyText>
<table confidence="0.996726066666667">
DOW JONES INDUSTRIALS AVERAGE -- 01/12/83
01/12 CLOSE 30 INDUS 1083.61
01/12 330PM 30 INDUS 1089.40
01/12 3PM 30 INDUS 1093.44
01/12 230PM 30 INDUS 1100.07
01/12 2PM 30 INDUS 1095.38
01/12 130PM 30 INDUS 1095.75
01/12 1PM 30 INDUS 1095.84
01/12 1230PM 30 INDUS 1095.75
01/12 NOON 30 INDUS 1092.35
01/12 1130AM 30 INDUS 1089.40
01/12 11AM 30 INDUS 1085.08
01/12 1030AM 30 INDUS 1085.36
01/11 CLOSE 30 INDUS 1083.79
CLOSING AVERAGE 1083.61 DOWN 0.18
</table>
<page confidence="0.994392">
145
</page>
<figure confidence="0.801424714285714">
1102
1098
1094
1090
1086
1082
10 10:30 1111:30 12 12:30 1 1:30 2 2:30 3 3:30 4
</figure>
<figureCaption confidence="0.811713">
Figure 1
</figureCaption>
<equation confidence="0.810479">
(1)
</equation>
<bodyText confidence="0.97017575">
after climbing steadily through most of
the morning , the stock market was pushed
downhill late in the day . stock prices posted
a small loss , with the indexes turning in a
mixed showing yesterday in brisk trading .
the Dow Jones average of 30 industrials
surrendered a 16.28 gain at 4pm and de-
clined slightly , finishing the day at 1083.61
, off 0.18 points .
(2)
wall street&apos;s securities markets rose
steadily through most of the morning , before
sliding downhill late in the day . the stock
market posted a small loss yesterday , with
the indexes finishing with mixed results in ac-
tive trading .
the Dow Jones average of 30 industrials
surrendered a 16.28 gain at 4pm and de-
clined slightly to finish at 1083.61 , off
0.18 points .
</bodyText>
<sectionHeader confidence="0.964831" genericHeader="method">
III. SYSTEM OVERVIEW
</sectionHeader>
<bodyText confidence="0.991547575">
In order to generate accurate and fluent summaries,
a knowledge-based report generator performs two main
tasks: first, it infers semantic messages from the data in
the database; second, it maps those messages into
phrases in its phrasal lexicon, stitching them together
according to the rules of its clause-combining grammar,
and incorporating rhetoric constraints in the process. As
the work of McKeown 1 and Mann and Moore2 demon-
strates. neither the problem of deciding what to say nor
the problem of determining how to say it is trivial, and
as Appelt3 has pointed out, the distinction between them
is not always clear.
A. System Architecture
A knowledge-based report generator consists of the
following four independent, sequential components: 1) a
fact generator, 2) a message generator, 3) a discourse
organizer, and 4) a text generator. Data from the data-
base serves as input to the first module, which produces
a stream of facts as output; facts serve as input to the
second module, which produces a set of messages as out-
put; messages form the input to the third module, which
organizes them and produces a set of ordered messages
as output; ordered messages form the input to the fourth
module, which produces final text as output. The
modules function independently and sequentially for the
sake of computational manageability at the expense of
psychological validity.
With the exception of the first module, which is a
straightforward C program, the entire system is coded in
the OPS5 production system language.4 At the time that
the sample output above was generated, module 2, the
message generator, consisted of 120 production rules;
module 3, the discourse organizer contained 16 produc-
tion rules; and module 4, the text generator, included
109 production rules and a phrasal dictionary of 519
entries. Real time processing requirements for each
module on a lightly loaded VAX 11/780 processor were
the following: phase 1 - 16 seconds, phase 2 - 34
seconds, phase 3 - 24 seconds, phase 4 - 1 minute, 59
seconds.
</bodyText>
<subsectionHeader confidence="0.383291">
B. Knowledge Constructs
</subsectionHeader>
<bodyText confidence="0.981031064516129">
The fundamental knowledge constructs of the sys-
tem are of two types: 1) static knowledge structures, or
memory elements, which can be thought of as n-
dimensional propositions, and 2) dynamic knowledge
structures, or production rules, which perform pattern-
recognition operations on n-dimensional propositions.
Static knowledge structures come in five flavors: facts.
messages, lexicon entries, medial text elements, and
various control elements. Dynamic knowledge constructs
occur in ten varieties: inference productions, ordering
productions, discourse mechanics productions, phrase
selection productions, syntax selection productions, ana-
phora selection productions, verb morphology produc-
tions, punctuation selection productions. writing produc-
tions, and various control productions.
C. Functions
The function of the first module is to perform the
arithmetic computation required to produce facts that
contain the relevant information needed to infer interest-
ing messages, and to write those facts in the OPS5
memory element format. For example, the fact that
indicates the closing status of the Dow Jones Average of
30 Industrials for January 12, 1983 is:
(make fact &amp;quot;fname CLSTAT &amp;quot;iname DJI ^itype
COMPOS &apos;date 01/12 &amp;quot;hour CLOSE &apos;open-
level 1084.25 &amp;quot;high-level 1105.13 &amp;quot;low-level
1075.88 &amp;quot;close-level 1083.61 &amp;quot;cumul-dir DN
&amp;quot;cumul-deg 0.18)
The function of the second module is to infer
interesting messages from the facts using inferencing
productions such as the following:
</bodyText>
<page confidence="0.987121">
146
</page>
<bodyText confidence="0.934887014925373">
(p instan-mixedup
(goal &apos;stat act &amp;quot;op instanmixed)
(fact ^fname CLSTAT iname DII
&amp;quot;cumul-dir UP ^repdate &lt;date&gt;)
(fact &amp;quot;fname ADVDEC &amp;quot;iname NYSE
&amp;quot;advances &lt;x&gt; &amp;quot;declines {&lt;y&gt; &gt; &lt;x&gt;})
--&gt;
(make message &amp;quot;top GENMKT &amp;quot;subtop MIX
&amp;quot;mix mixed &amp;quot;repdate &lt;date&gt;
&amp;quot;subjclass MKT &amp;quot;tim close)
(make goal &amp;quot;stat pend &amp;quot;op writemessage)
(remove 1)
This production infers that if the closing status of the
Dow had a direction of &apos;up&apos;, and yet the number of
declines exceeded the number of advances for the day,
then it can be said that the market was mixed. The mes-
sage that is produced looks like this:
(make message &amp;quot;repdate 01/12 &amp;quot;top GENMKT
&amp;quot;subsubtop nil ^subtop MIX &amp;quot;subjclass MKT
&amp;quot;dir nil &apos;deg nil &amp;quot;vardeg I nil j&amp;quot;varlev I nil I &apos;mix
mixed &amp;quot;chg nil &amp;quot;sco nil &amp;quot;tim close ^vartim I nil j
&amp;quot;dur nil &amp;quot;vol nil &amp;quot;who nil )
The inferencing process in phase 2 is hierarchically con-
trolled.
Module 3 performs the uncomplicated task of
grouping messages into paragraphs, ordering messages
within paragraphs, and assigning a priority number to
each message. Priorities are assigned as a function of
topic and subtopic. The system &amp;quot;knows&amp;quot; a default order-
ing sequence, and it &amp;quot;knows&amp;quot; some exception rules which
assign higher priorities to messages of special signifi-
cance, such as indicators hitting record highs. As in
module 2, processing is hierarchically controlled. Even-
tually, modules 2 and 3 should be combined so that their
knowledge could be shared.
The most complicated processing is performed by
module 4. This processing is not hierarchically con-
trolled, but instead more closely resembles control in an
ATN. Module 4, the text generator, coordinates and
executes the following activities: 1) selection of phrases
from the phrasal lexicon that both capture the semantic
meaning of the message and satisfy rhetorical con-
straints; 2) selection of appropriate syntactic forms for
predicate phrases, such as sentence, participial clause,
prepositional phrase. etc.; 3) selection of appropriate
anaphora for subject phrases 4) morphological processing
of verbs; 5) interjection of appropriate punctuation; and
6) control of discourse mechanics, such as inclusion of
more than one clause per sentence and more than one
sentence per paragraph.
The module 4 processor is able to coordinate and
execute these activities because it incorporates and
integrates the semantic, syntactic, and rhetoric
knowledge it needs into its static and dynamic knowledge
structures. For example, a phrasal lexicon entry that
might match the &amp;quot;mixed market&amp;quot; message is the follow-
ing:
(make phraselex &amp;quot;top GENMKT ^subtop MIX
&apos;mix mixed ^chg nil ^tim close &amp;quot;subjtype
NAME ^subjclass MKT &amp;quot;predfs turned &amp;quot;predfpl
turned &amp;quot;predpart turning &amp;quot;predinf to turn&apos;
&amp;quot;predrem lin a mixed showingj len 9 &apos;rand 5
&apos;imp 11)
An example of a syntax selection production that would
select the syntactic form subordinate-participial-clause as
an appropriate form for a phrase (as in &amp;quot;after rising
steadily through most of the morning&amp;quot;) is the following:
</bodyText>
<table confidence="0.712964052631579">
(p 5.selectsuborpartpre-selectsyntax
(goal &amp;quot;stat act &amp;quot;op selectsyntax) ; I
(sentreq &amp;quot;sentstat nil) ; 2
(message &apos;foe in &amp;quot;top &lt;t&gt; &amp;quot;tim &lt;&gt; nil
&amp;quot;subjclass &lt;sc&gt;) ; 3
(message &amp;quot;foc nil &amp;quot;top &lt;t&gt; &amp;quot;tim &lt;&gt; nil
^subjclass &lt;sc&gt;) ; 4
(paramsynforms &amp;quot;suborpartpre &lt;set&gt;) ; 5
(randnum ^randval &lt; &lt;set&gt;) ; 6
(lastsynform &apos;form &lt;&lt; initsent prepp &gt;&gt; ) ; 7
- (openingsynform &apos;form
&lt;&lt; suborsent suborpart &gt;&gt; )
- (message ^foc in ^tim close)
--&gt;
(remove 1)
(make synform &apos;form suborpart )
(modify 4 &amp;quot;foc peek )
(make goal &amp;quot;stat act &apos;op selectsubor)
D. Context-Dependent Grammar
</table>
<bodyText confidence="0.998979111111111">
Syntax selection productions, such as the example
above, comprise a context-dependent, right-branching,
clause-combining grammar. Because of the attribute-
value, pattern-recognition nature of these grammar rules
and their use of the lexicon, they may be viewed as a
high-level variant of a lexical functional grammar.5 The
efficacy of a low-level functional grammar for text gen-
eration has been demonstrated in McKeown&apos;s TEXT sys-
tem.6
For each message. in sequence, the system first
selects a predicate phrase that matches the semantic con-
tent of the message. and next selects a syntactic form,
such as sentence or prepositional phrase, into which form
the predicate phrase may be hammered. The system&apos;s
default goal is to form complex sentences by combining a
variable number of messages expressed in a variety of
syntactic forms in each sentence. Every message may be
expressed in the syntactic form of a simple sentence.
But under certain grammatical and rhetorical conditions,
which are specified in the syntax selection productions,
and which sometimes include looking ahead at the next
sequential message, the system opts for a different syn-
tactic form.
The right-branching behavior of the system implies
that at any point the system has the option to lay down a
period and start a new sentence. It also implies that
embedded subject-complement forms, such as relative
</bodyText>
<page confidence="0.992597">
147
</page>
<bodyText confidence="0.997184923076923">
clauses modifying subjects, are trickier to implement
(and have not been implemented as yet). That embed-
ded subject complements pose special difficulties should
not be considered discouraging. Developmental linguis-
tics research reveals that &amp;quot;operations on sentence sub-
jects, including subject complementation and relative
clauses modifying subjects&amp;quot; are among the last to appear
in the acquisition of complex sentences,7 and a
knowledge-based report generator incorporates the basic
mechanism for eventually matching messages to nominal-
izations of predicate phrases to create subject comple-
ments, as well as the mechanism for embedding relative
clauses.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="method">
IV. THE DOMAIN-SPECIFIC
KNOWLEDGE REQUIREMENT TENET
</sectionHeader>
<bodyText confidence="0.99675524">
How does one determine what knowledge must
incorporated into a knowledge-based report generator?
Because the goal of a knowledge-based report generator
is to produce reports that are indistinguishable from
reports written by people for the same database, it is log-
ical to turn to samples of naturally generated text from
the specific domain of discourse in order to gain insights
into the semantic, linguistic, and rhetoric knowledge
requirements of the report generator.
Research in machine translation8 and text under-
standing9 has demonstrated that not only does naturally
generated text disclose the lexicon and grammar of a
sublanguage, but it also reveals the essential semantic
classes and attributes of a domain of discourse, as well
as the relations between those classes and attributes.
Thus, samples of actual text may be used to build the
phrasal dictionary for a report generator and to define
the syntactic categories that a generator must have
knowledge of. Similarly, the semantic classes, attributes
and relations revealed in the text define the scope and
variety of the semantic knowledge the system must
incorporate in order to infer relevant and interesting
messages from the database.
Ana&apos;s phrasal lexicon consists of subjects, such as
&amp;quot;wall street&apos;s securities markets&amp;quot;, and predicates, such as
&amp;quot;were swept into a broad and steep decline&amp;quot;, which are
extracted from the text of naturally generated stock
reports. The syntactic categories Ana knows about are
the clausal level categories that are found in the same
text, such as, sentence, coordinate-sentence,
subordinate-sentence, subordinate-participial-clause,
prepositional-phrase, and others.
Semantic analysis of a sample of natural text stock
reports discloses that a hierarchy of approximately forty
message classes accounts for nearly all of the semantic
information contained in the &amp;quot;core market sentences&amp;quot; of
stock reports. The term &amp;quot;core market sentences&amp;quot; was
introduced by Kittredge to refer to those sentences which
can be inferred from the data in the data base without
reference to external events such as wars, strikes, and
corporate or government policy making.&amp;quot;) Thus, for
example, Ana could say &amp;quot;Eastman Kodak advanced 2 3/4
to 85 3/4;&amp;quot; but it could not append &amp;quot;it announced
development of the world&apos;s fastest color film for delivery
in 1983.&amp;quot;. Ana currently has knowledge of only six mes-
sage classes. These include the closing market status
message, the volume of trading message, and the mixed
market message, the interesting market fluctuations mes-
sage, the closing Dow status message, and the interesting
Dow fluctuations message.
</bodyText>
<sectionHeader confidence="0.930107" genericHeader="method">
V. THE PRODUCTION SYSTEM
KNOWLEDGE REPRESENTATION TENET
</sectionHeader>
<bodyText confidence="0.896095125">
The use of production systems for natural language
processing was suggested as early as 1972 by Heidorn,I
whose production language NLP is currently being used
for syntactic processing research. A production system
.for language understanding has been implemented in
OPS5 by Frederking.I2 Many benefits are derived from
using a production system to represent the knowledge
required for text generation. Two of the more important
advantages are the ability to integrate semantic, syntac-
tic, and rhetoric knowledge, and the ability to extend
and tailor the system easily.
A. Knowledge Integration
Knowledge integration is evident in the production
rule displayed earlier for selecting the syntactic form of
subordinate participial clause. In English, that produc-
tion said:
</bodyText>
<listItem confidence="0.9702685">
IF
1) there is an active goal to select a syntactic form
2) the sentence requirement has not been satisfied
3) the message currently in focus has topic &lt;t&gt;,
subject class &lt;sc&gt;, and some non-nil time
4) the next sequential message has the same topic,
subject class, and some non-nil time
5) the subordinate-participial-clause parameter
is set at value &lt;set&gt;
6) the current random number is less than &lt;set&gt;
7) the last syntactic form used was either a
prepositional phrase or a sentence initializer
8) the opening syntactic form of the last sentence
was not a subordinate sentence or a
subordinate participial clause
9) the time attribute of the message in focus
does not have value &apos;close&apos;
THEN
1) remove the goal of selecting a syntactic form
2) make the current syntactic form a subordinate
participial clause
3) modify the next sequential message to put it
in peripheral focus
4) set a goal to select a subordinating conjunction.
</listItem>
<bodyText confidence="0.811377">
It should be apparent from the explanation that the rule
integrates semantic knowledge, such as message topic
and time, syntactic knowledge, such as whether the
sentence requirement has been satisfied, and rhetoric
knowledge, such as the preference to avoid using subor-
dinate clauses as the opening form of two consecutive
sentences.
</bodyText>
<page confidence="0.991993">
148
</page>
<subsectionHeader confidence="0.631055">
B. Knowledge Tailoring and Extending
</subsectionHeader>
<bodyText confidence="0.89103275">
Conditions number 5 and 6, the syntactic form
parameter and the random number, are examples of con-
trol elements that are used for syntactic tailoring. A
syntactic form parameter may be preset at any value
between 1 and 11 by the system user. A value of 8, for
example, would result in an 80 percent chance that the
rule in which the parameter occurs would be satisfied if
all its other conditions were satisfied. Consequently, on
20 percent of the occasions when the rule would have
been otherwise satisfied, the syntactic form parameter
would prevent the rule from firing, and the system
would be forced to opt for a choice of some other syn-
tactic form. Thus, if the user prefers reports that are low
on subordinate participial clauses, the subordinate parti-
cipial clause parameter might be set at 3 or lower.
The following production contains the bank of
parameters as they were set to generate text sample (2)
above:
(p _1.setparams
(goal Astat act &apos;op setparams)
--&gt;
(remove 1)
(make paramsyllables &amp;quot;val 30)
(make parammessages &apos;val 3)
</bodyText>
<footnote confidence="0.9409606">
(make paramsynforms
&apos;sentence 11
&apos;coorsent 11
&amp;quot;suborsent 11
prepphrase 11
&amp;quot;suborsentpre 5
&amp;quot;suborpartpre 8
&amp;quot;suborsentpost 8
&amp;quot;suborpartpost 11
&amp;quot;subotpartsentpost 11
</footnote>
<bodyText confidence="0.999890705882353">
When sample text (1) was generated, all syntactic form
parameters were set at 11. The first two parameters in
the bank are rhetoric parameters. They control the
maximum length of sentences in syllables (roughly) and
in number of messages per sentence.
Not only does production system knowledge
representation allow syntactic tailoring, but it also per-
mits semantic tailoring. Ana could be tailored to focus
on particular stocks or groups of stocks to meet the
information needs of individual users. Furthermore, a
production system is readily extensible. Currently, Ana
has only a small amount of general knowledge about the
stock market and is far from a stock market expert. But
any knowledge that can be made explicit can be added to
the system prolonged incremental growth in the
knowledge of the system could someday result in a sys-
tem that truly is a stock market expert.
</bodyText>
<sectionHeader confidence="0.990414" genericHeader="method">
VI. THE MACRO-LEVEL
KNOWLEDGE CONSTRUCTS TENET
</sectionHeader>
<bodyText confidence="0.999940315789474">
The problem of dealing with the complexity of
natural language is made much more tractable by work-
ing in macro-level knowledge constructs, such as seman-
tic units consisting of whole messages, lexical iterri&lt;.
sisting of whole phrases, syntactic categories at the
clause level, and a clause-combining grammar. Macro-
level processing buys linguistic fluency at the cost of
semantic and linguistic flexibility. However, the loss of
flexibility appears to be not much greater than the con-
straints imposed by the grammar and semantics of the
sublanguage of the domain of discourse. Furthermore,
there may be more to the notion of macro-level semantic
and linguistic processing than mere computational
manageability.
The notion of a phrasal lexicon was suggested by
Becker,I3 who proposed that people generate utterances
&amp;quot;mostly by stitching together swatches of text that they
have heard before. Wilensky and Arens have experi-
mented with a phrasal lexicon in a language understand-
ing system. I4 I believe that natural language behavior
will eventually be understood in terms of a theory of
stratified natural language processing in which macro-
level knowledge constructs, such as those used in a
knowledge-based report generator, occur at one of the
higher cognitive strata.
A poor but useful analogy to mechanical gear-
shifting while driving a car can be drawn. Just as driv-
ing in third gear makes most efficient use of an
automobile&apos;s resources, so also does generating language
in third gear make most efficient use of human informa-
tion processing resources. That is, matching whole
phrases and applying a clause-combining grammar is
cognitively economical. But when only a near match for
a message can be found in a speaker&apos;s phrasal diction-
ary, the speaker must downshift into second gear, and
either perform some additional processing on the phrase
to transform it into the desired form to match the mes-
sage, or perform some processing on the message to
transform it into one that matches the phrase. And if
not even a near match for a message can be found, the
speaker must downshift into first gear and either con-
struct a phrase from elementary lexical items, including
words, prefixes, and suffixes, or reconstruct the mes-
sage.
As currently configured, a knowledge-based text
generator operates only in third gear. Because the units
of processing are linguistically mature whole phrases, the
report generation system can produce fluent text without
having the detailed knowledge-needed to construct
mature phrases from their elementary components. But
there is nothing except the time and insight of a system
implementor to prevent this detailed knowledge from
being added to the system. By experimenting with addi-
tional knowledge, a system could gradually be extended
to shift into lower gears, to exhibit greater interaction
between semantic and linguistic components, and to do
more flexible, if not creative, generation of semantic
</bodyText>
<page confidence="0.996994">
149
</page>
<bodyText confidence="0.999915290322581">
messages and linguistic phrases. A knowledge-based
report generator may be viewed as a starting tool for
modeling a stratiform theory of natural language pro-
cessing.
VII. CONCLUSION
Knowledge-based report generation is practical
because it tackles a moderately ill-defined problem with
an effective technique, namely, a macro-level,
knowledge-based, production system technique. Stock
market reports are typical instances of a whole class of
summary-type periodic reports for which the scope and
variety of semantic and linguistic complexity is great
enough to negate a straightforward algorithmic solution,
but constrained enough to allow a high-level cross-wise
slice of the variety of knowledge to be effectively incor-
porated into a production system. Even so, it will be
some time before the technique is cost effective. The
time required to add knowledge to a system is greater
than the time required to add productions to a traditional
expert system. Most of the time is spent doing seman-
tic analysis for the purpose of creating useful semantic
classes and attributes, and identifying the relations
between them. Coding itself goes quickly, but then the
system must be tested and calibrated (if the guesses on
the semantics were close) or redone entirely (if the
guesses were not close). Still, the initial success of the
technique suggests its value both as a basic research tool,
for exploring increasingly more detailed semantic and
linguistic processes, and as an applied research tool, for
designing extensible and tailorable automatic report gen-
erators.
</bodyText>
<sectionHeader confidence="0.994433" genericHeader="acknowledgments">
ACKNOWLEDGEMENT
</sectionHeader>
<bodyText confidence="0.998904333333333">
I. wish to express my deep appreciation to Michael
Lesk for his unfailing guidance and support in the
development of this project.
</bodyText>
<sectionHeader confidence="0.997456" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999892492063492">
1. Kathleen R. McKeown, &amp;quot;The TEXT System for
Natural Language Generation: An Overview,&amp;quot;
Proceedings of the Twentieth Annual Meeting of the
Association for Computational Linguistics, Toronto,
Canada (1982).
2. James A. Moore and William C. Mann, &amp;quot;A
Snapshot of KDS: A Knowledge Delivery System,&amp;quot;
in Proceedings of the 17th Annual Meeting of the
Association for Computational linguistics, La Jolla,
California (11-12 August 1979).
3. Douglas E. Appelt, &amp;quot;Problem Solving Applied to
Language Generation,&amp;quot; pp. 59-63 in Proceedings of
the 18th Annual Meeting of the Association for Com-
putational Linguistics, University of Pennsylvania,
Philadelphia, PA (June 19-22,1980).
4. C. L. Forgy, &amp;quot;OPS-5 User&apos;s Manual,&amp;quot; CMU-CS-
81-135, Dept of Computer Science, Carnegie-
Mellon University, Pittsburgh, PA 15213 (July
1981).
5. Joan Bresnan and Ronald M. Kaplan, &amp;quot;Lexical-
Functional Grammar: A Formal System for Gram-
matical Representation,&amp;quot; Occasional Paper #13,
MIT Center for Cognitive Science (1982).
6. Kathleen Rose McKeown, &amp;quot;Generating Natural
Language Text in Response to Questions about
Database Structure,&amp;quot; Doctoral Dissertation,
University of Pennsylvania Computer and Informa-
tion Science Department (1982).
7. Melissa Bowerman, &amp;quot;The Acquisition of Complex
Sentences,&amp;quot; pp. 285-305 in Language Acquisition,
ed. Michael Garman, Cambridge University Press,
Cambridge (1979).
8. Richard Kittredge and John Lehrberger, Sub-
languages: Studies of Language in Restricted Seman-
tic Domains, Walter DeGruyter, New York (in
press).
9. Naomi Sager, &amp;quot;Information Structures in Texts of a
Subianguage,&amp;quot; in The Information Community: Alli-
ance for Progress - Proceedings of the 44th ASIS
Annual Meeting, Volume I8, Knowlton Industry
Publications for the American Society for Informa-
tion Science, White Plains, N.Y. (October 1981).
10. Richard 1. Kittredge. &amp;quot;Semantic Processing of
Texts in Restricted Sublanguages,&amp;quot; Computers and
Mathematics with Applications 8(0), Pergamon Press
(1982).
11. George E. Heidorn, &amp;quot;Natural Language Inputs to a
Simulation Programming System,&amp;quot; NPS-
55HD72101A, Naval Postgraduate School, Mon-
terey, CA (October 1972).
12. Robert E. Frederking, A Production System
Approach to Language Understanding, To appear
(1983).
13. Joseph Becker, &amp;quot;The Phrasal Lexicon,&amp;quot; pp. 70-73
in Theoretical Issues in ,Vatural Language Process-
ing, ed. B. I. Nash-Webber, Cambridge, Mas-
sachusetts (10-13 June 1975).
14. Robert Wilensky and Yigel Arens, &amp;quot;PHRAN -- A
Knowledge-Based Natural Language Under-
stander,&amp;quot; pp. 117-121 in Proceedings of the 18th
Annual Meeting of the Association for Computational
Linguistics, University of Pennsylvania. Philadel-
phia, Pennsylvania (June 19-22, 1980).
</reference>
<page confidence="0.994069">
1.50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.992843">DESIGN OF A KNOWLEDGE-BASED REPORT GENERATOR</title>
<author confidence="0.999857">Karen Kukich</author>
<affiliation confidence="0.994695">University of Pittsburgh Bell Telephone Laboratories</affiliation>
<address confidence="0.999906">Murray Ell, NJ 07974</address>
<abstract confidence="0.997450971428571">Report Generation a technique generating natural language reports from computer databases. It is so named because it applies knowledge-based expert systems software to the problem of text generation. The first application of the technique, a system for generating natural language reports from daily stock quotes database, is partially implemented. Three fundamental principles of the technique are its use of domain-specific semantic and linguistic knowledge, its use of macro-level semantic and linguistic constructs (such as whole messages, a phrasal lexicon, and a sentence-combining grammar), and its production system approach to knowledge representation. I. WHAT IS KNOWLEDGE-BASED REPORT GENERATION A knowledge-based report generator is a computer program whose function â€¢ is to generate natural language summaries from computer databases. For example, knowledge-based report generators can be designed to generate daily stock market reports from a stock quotes database, daily weather reports from a meteorological database, weekly sales reports from corporate databases, or quarterly economic reports from U. S. Commerce Department databases, etc. A separate generator must be implemented for each domain of discourse because each knowledge-based report generator contains domain-specific knowledge which is used to infer interesting messages from the database and to express those messages in the sublanguage of the domain of discourse. The technique of knowledge-based report generation is generalizable across domains, however, and the actual text generation component of the report generator, which comprises roughly one-quarter of the code, is directly transportable and readily tailorable. Knowledge-based report generation is a practical to It&apos;s three fundamental tenets are the following. First, it assumes that much domain-specific semantic, linguistic, and rhetoric knowledge is required in order for a computer to automatically produce intelligent and fluent text. Second, it assumes that production system languages, such as those used to build expert systems, are wellsuited to the task of representing and integrating semantic, linguistic, and rhetoric knowledge. Finally, it holds that macro-level knowledge units, such as whole semantic messages, a phrasal lexicon, clausal grammatical categories, and a clause-combining grammar, provide an appropriate level of knowledge representation for generating that type of text which may be categorized as periodic summary reports. These three tenets guide the design and implementation of a knowledge-based report generation system. II. SAMPLE OUTPUT FROM A KNOWLEDGE-BASED REPORT GENERATOR The first application of the technique of knowledge-based report generation is a partially implemented stock report generator called Ana. Data from a Dow Jones stock quotes database serves as input to the system, and the opening paragraphs of a stock market summary are produced as output. As more semantic and linguistic knowledge about the stock market is added to the system, it will be able to generate longer, more informative reports. Figure 1 depicts a portion of the actual data submitted to Ana for January 12, 1983. A hand drawn graph of the same data is included. The following text samples are Ana&apos;s interpretation of the data on two different runs.</abstract>
<address confidence="0.809553590909091">DOW JONES INDUSTRIALS AVERAGE -- 01/12/83 01/12 CLOSE 30 INDUS 1083.61 01/12 330PM 30 INDUS 1089.40 01/12 3PM 30 INDUS 1093.44 01/12 230PM 30 INDUS 1100.07 01/12 2PM 30 INDUS 1095.38 01/12 130PM 30 INDUS 1095.75 01/12 1PM 30 INDUS 1095.84 01/12 1230PM 30 INDUS 1095.75 01/12 NOON 30 INDUS 1092.35 01/12 1130AM 30 INDUS 1089.40 01/12 11AM 30 INDUS 1085.08 01/12 1030AM 30 INDUS 1085.36 01/11 CLOSE 30 INDUS 1083.79 CLOSING AVERAGE 1083.61 DOWN 0.18 145 1102 1098 1094 1090 1086 1082</address>
<phone confidence="0.695538">10 10:30 1111:30 12 12:30 1 1:30 2 2:30 3 3:30 4</phone>
<abstract confidence="0.998123118895967">Figure 1 (1) after climbing steadily through most of the morning , the stock market was pushed downhill late in the day . stock prices posted a small loss , with the indexes turning in a mixed showing yesterday in brisk trading . the Dow Jones average of 30 industrials a 16.28 gain at 4pm and declined slightly , finishing the day at 1083.61 , off 0.18 points . (2) wall street&apos;s securities markets rose steadily through most of the morning , before sliding downhill late in the day . the stock market posted a small loss yesterday , with indexes finishing with mixed results in active trading . the Dow Jones average of 30 industrials a 16.28 gain at 4pm and declined slightly to finish at 1083.61 , off 0.18 points . III. SYSTEM OVERVIEW In order to generate accurate and fluent summaries, a knowledge-based report generator performs two main tasks: first, it infers semantic messages from the data in the database; second, it maps those messages into in phrasal stitching them together according to the rules of its clause-combining grammar, and incorporating rhetoric constraints in the process. As work of McKeown 1and Mann and demonneither the problem of deciding what to nor the problem of determining how to say it is trivial, and has pointed out, the distinction between them is not always clear. A. System Architecture A knowledge-based report generator consists of the following four independent, sequential components: 1) a generator, 2) a message 3) a discourse organizer, and 4) a text generator. Data from the dataas input to the first module, which produces a stream of facts as output; facts serve as input to the second module, which produces a set of messages as output; messages form the input to the third module, which organizes them and produces a set of ordered messages as output; ordered messages form the input to the fourth module, which produces final text as output. The modules function independently and sequentially for the sake of computational manageability at the expense of psychological validity. With the exception of the first module, which is a straightforward C program, the entire system is coded in OPS5 production system At the time that the sample output above was generated, module 2, the message generator, consisted of 120 production rules; module 3, the discourse organizer contained 16 production rules; and module 4, the text generator, included 109 production rules and a phrasal dictionary of 519 entries. Real time processing requirements for each module on a lightly loaded VAX 11/780 processor were the following: phase 1 - 16 seconds, phase 2 - 34 seconds, phase 3 - 24 seconds, phase 4 - 1 minute, 59 seconds. B. Knowledge Constructs fundamental constructs of the sysof two 1) static knowledge structures, or memory elements, which can be thought of as ndimensional propositions, and 2) dynamic knowledge structures, or production rules, which perform patternrecognition operations on n-dimensional propositions. Static knowledge structures come in five flavors: facts. messages, lexicon entries, medial text elements, and various control elements. Dynamic knowledge constructs occur in ten varieties: inference productions, ordering productions, discourse mechanics productions, phrase selection productions, syntax selection productions, anaphora selection productions, verb morphology producpunctuation selection productions. producvarious control productions. C. Functions The function of the first module is to perform the computation required to facts that contain the relevant information needed to infer interesting messages, and to write those facts in the OPS5 memory element format. For example, the fact that the closing status of the Dow Jones Average 30 Industrials for January 12, 1983 is: (make fact &amp;quot;fname CLSTAT &amp;quot;iname DJI ^itype 01/12 &amp;quot;hour CLOSE &apos;openlevel 1084.25 &amp;quot;high-level 1105.13 &amp;quot;low-level 1075.88 &amp;quot;close-level 1083.61 &amp;quot;cumul-dir DN &amp;quot;cumul-deg 0.18) function of the second module is to interesting messages from the facts using inferencing productions such as the following: 146 (p instan-mixedup (goal &apos;stat act &amp;quot;op instanmixed) (fact ^fname CLSTAT iname DII UP &lt;date&gt;) (fact &amp;quot;fname ADVDEC &amp;quot;iname NYSE &amp;quot;advances &lt;x&gt; &amp;quot;declines {&lt;y&gt; &gt; &lt;x&gt;}) --&gt; (make message &amp;quot;top GENMKT &amp;quot;subtop MIX &amp;quot;mix mixed &amp;quot;repdate &lt;date&gt; &amp;quot;subjclass MKT &amp;quot;tim close) (make goal &amp;quot;stat pend &amp;quot;op writemessage) (remove 1) This production infers that if the closing status of the Dow had a direction of &apos;up&apos;, and yet the number of declines exceeded the number of advances for the day, then it can be said that the market was mixed. The message that is produced looks like this: (make message &amp;quot;repdate 01/12 &amp;quot;top GENMKT &amp;quot;subsubtop nil ^subtop MIX &amp;quot;subjclass MKT &amp;quot;dir nil &apos;deg nil &amp;quot;vardeg I nil j&amp;quot;varlev I nil I &apos;mix mixed &amp;quot;chg nil &amp;quot;sco nil &amp;quot;tim close ^vartim I nil j &amp;quot;dur nil &amp;quot;vol nil &amp;quot;who nil ) The inferencing process in phase 2 is hierarchically controlled. Module 3 performs the uncomplicated task of grouping messages into paragraphs, ordering messages within paragraphs, and assigning a priority number to each message. Priorities are assigned as a function of topic and subtopic. The system &amp;quot;knows&amp;quot; a default ordering sequence, and it &amp;quot;knows&amp;quot; some exception rules which assign higher priorities to messages of special significance, such as indicators hitting record highs. As in module 2, processing is hierarchically controlled. Eventually, modules 2 and 3 should be combined so that their could The most complicated processing is performed by module 4. This processing is not hierarchically controlled, but instead more closely resembles control in an ATN. Module 4, the text generator, coordinates and executes the following activities: 1) selection of phrases from the phrasal lexicon that both capture the semantic meaning of the message and satisfy rhetorical constraints; 2) selection of appropriate syntactic forms for predicate phrases, such as sentence, participial clause, prepositional phrase. etc.; 3) selection of appropriate anaphora for subject phrases 4) morphological processing of verbs; 5) interjection of appropriate punctuation; and 6) control of discourse mechanics, such as inclusion of more than one clause per sentence and more than one sentence per paragraph. The module 4 processor is able to coordinate and execute these activities because it incorporates and integrates the semantic, syntactic, and rhetoric knowledge it needs into its static and dynamic knowledge structures. For example, a phrasal lexicon entry that might match the &amp;quot;mixed market&amp;quot; message is the following: (make phraselex &amp;quot;top GENMKT ^subtop MIX &apos;mix mixed ^chg nil ^tim close &amp;quot;subjtype NAME ^subjclass MKT &amp;quot;predfs turned &amp;quot;predfpl turned &amp;quot;predpart turning &amp;quot;predinf to turn&apos; &amp;quot;predrem lin a mixed showingj len 9 &apos;rand 5 &apos;imp 11) An example of a syntax selection production that would select the syntactic form subordinate-participial-clause as an appropriate form for a phrase (as in &amp;quot;after rising steadily through most of the morning&amp;quot;) is the following: (p 5.selectsuborpartpre-selectsyntax (goal &amp;quot;stat act &amp;quot;op selectsyntax) ; I (sentreq &amp;quot;sentstat nil) ; 2 (message &apos;foe in &amp;quot;top &lt;t&gt; &amp;quot;tim &lt;&gt; nil &amp;quot;subjclass &lt;sc&gt;) ; 3 (message &amp;quot;foc nil &amp;quot;top &lt;t&gt; &amp;quot;tim &lt;&gt; nil ^subjclass &lt;sc&gt;) ; 4 &amp;quot;suborpartpre ^randval &lt; (lastsynform &apos;form &lt;&lt; initsent prepp &gt;&gt; ) ; 7 - (openingsynform &apos;form &lt;&lt; suborsent suborpart &gt;&gt; ) - (message ^foc in ^tim close) --&gt; (remove 1) (make synform &apos;form suborpart ) (modify 4 &amp;quot;foc peek ) (make goal &amp;quot;stat act &apos;op selectsubor) D. Context-Dependent Grammar Syntax selection productions, such as the example above, comprise a context-dependent, right-branching, clause-combining grammar. Because of the attributevalue, pattern-recognition nature of these grammar rules and their use of the lexicon, they may be viewed as a variant of a lexical functional The efficacy of a low-level functional grammar for text generation has been demonstrated in McKeown&apos;s TEXT sys- For each message. in sequence, the system first selects a predicate phrase that matches the semantic content of the message. and next selects a syntactic form, such as sentence or prepositional phrase, into which form the predicate phrase may be hammered. The system&apos;s default goal is to form complex sentences by combining a variable number of messages expressed in a variety of syntactic forms in each sentence. Every message may be expressed in the syntactic form of a simple sentence. But under certain grammatical and rhetorical conditions, which are specified in the syntax selection productions, and which sometimes include looking ahead at the next message, the opts for a different syntactic form. right-branching behavior of the implies at any point the the to lay down a and a new sentence. also implies that embedded subject-complement forms, such as relative 147 clauses modifying subjects, are trickier to implement (and have not been implemented as yet). That embedded subject complements pose special difficulties should not be considered discouraging. Developmental linguistics research reveals that &amp;quot;operations on sentence subjects, including subject complementation and relative clauses modifying subjects&amp;quot; are among the last to appear the acquisition of complex and a knowledge-based report generator incorporates the basic mechanism for eventually matching messages to nominalizations of predicate phrases to create subject complements, as well as the mechanism for embedding relative clauses. IV. THE DOMAIN-SPECIFIC KNOWLEDGE REQUIREMENT TENET How does one determine what knowledge must incorporated into a knowledge-based report generator? Because the goal of a knowledge-based report generator is to produce reports that are indistinguishable from reports written by people for the same database, it is logical to turn to samples of naturally generated text from the specific domain of discourse in order to gain insights into the semantic, linguistic, and rhetoric knowledge requirements of the report generator. in machine and text underhas demonstrated that not only does naturally generated text disclose the lexicon and grammar of a sublanguage, but it also reveals the essential semantic classes and attributes of a domain of discourse, as well as the relations between those classes and attributes. Thus, samples of actual text may be used to build the phrasal dictionary for a report generator and to define the syntactic categories that a generator must have knowledge of. Similarly, the semantic classes, attributes and relations revealed in the text define the scope and variety of the semantic knowledge the system must incorporate in order to infer relevant and interesting messages from the database. Ana&apos;s phrasal lexicon consists of subjects, such as &amp;quot;wall street&apos;s securities markets&amp;quot;, and predicates, such as &amp;quot;were swept into a broad and steep decline&amp;quot;, which are extracted from the text of naturally generated stock reports. The syntactic categories Ana knows about are the clausal level categories that are found in the same text, such as, sentence, coordinate-sentence, subordinate-sentence, subordinate-participial-clause, prepositional-phrase, and others. Semantic analysis of a sample of natural text stock reports discloses that a hierarchy of approximately forty message classes accounts for nearly all of the semantic information contained in the &amp;quot;core market sentences&amp;quot; of stock reports. The term &amp;quot;core market sentences&amp;quot; was introduced by Kittredge to refer to those sentences which can be inferred from the data in the data base without reference to external events such as wars, strikes, and or government policy Thus, for example, Ana could say &amp;quot;Eastman Kodak advanced 2 3/4 to 85 3/4;&amp;quot; but it could not append &amp;quot;it announced development of the world&apos;s fastest color film for delivery in 1983.&amp;quot;. Ana currently has knowledge of only six message classes. These include the closing market status message, the volume of trading message, and the mixed market message, the interesting market fluctuations message, the closing Dow status message, and the interesting Dow fluctuations message. V. THE PRODUCTION SYSTEM KNOWLEDGE REPRESENTATION TENET The use of production systems for natural language processing was suggested as early as 1972 by Heidorn,I production language NLP is being syntactic processing A production language understanding has implemented in Many benefits are derived from using a production system to represent the knowledge required for text generation. Two of the more important advantages are the ability to integrate semantic, syntactic, and rhetoric knowledge, and the ability to extend and tailor the system easily. A. Knowledge Integration Knowledge integration is evident in the production rule displayed earlier for selecting the syntactic form of subordinate participial clause. In English, that production said: IF 1) there is an active goal to select a syntactic form 2) the sentence requirement has not been satisfied 3) the message currently in focus has topic &lt;t&gt;, subject class &lt;sc&gt;, and some non-nil time 4) the next sequential message has the same topic, subject class, and some non-nil time 5) the subordinate-participial-clause parameter set at value the current random number is 7) the last syntactic form used was either a prepositional phrase or a sentence initializer 8) the opening syntactic form of the last sentence was not a subordinate sentence or a subordinate participial clause 9) the time attribute of the message in focus does not have value &apos;close&apos; THEN 1) remove the goal of selecting a syntactic form 2) make the current syntactic form a subordinate participial clause modify next message to put it peripheral set a goal to select a subordinating should be apparent from the explanation that the integrates semantic knowledge, such as message topic and time, syntactic knowledge, such as whether the sentence requirement has been satisfied, and rhetoric knowledge, such as the preference to avoid using subordinate clauses as the opening form of two consecutive sentences. 148 B. Knowledge Tailoring and Extending Conditions number 5 and 6, the syntactic form parameter and the random number, are examples of control elements that are used for syntactic tailoring. A syntactic form parameter may be preset at any value between 1 and 11 by the system user. A value of 8, for example, would result in an 80 percent chance that the rule in which the parameter occurs would be satisfied if all its other conditions were satisfied. Consequently, on 20 percent of the occasions when the rule would have satisfied, the syntactic form parameter would prevent the rule from firing, and the system would be forced to opt for a choice of some other syntactic form. Thus, if the user prefers reports that are low on subordinate participial clauses, the subordinate participial clause parameter might be set at 3 or lower. following production contains of parameters as they were set to generate text sample (2) above: (p _1.setparams (goal Astat act &apos;op setparams) --&gt; (remove 1) (make paramsyllables &amp;quot;val 30) (make parammessages &apos;val 3) (make paramsynforms &apos;sentence 11 &apos;coorsent 11 &amp;quot;suborsent 11 prepphrase 11 &amp;quot;suborsentpre 5 &amp;quot;suborpartpre 8 &amp;quot;suborsentpost 8 &amp;quot;suborpartpost 11 &amp;quot;subotpartsentpost 11 sample text (1) was generated, syntactic form parameters were set at 11. The first two parameters in the bank are rhetoric parameters. They control the maximum length of sentences in syllables (roughly) and number of messages per only production system knowledge representation allow syntactic tailoring, but it also permits semantic tailoring. Ana could be tailored to focus on particular stocks or groups of stocks to meet the information needs of individual users. Furthermore, a system is readily Ana has only a small amount of general knowledge about the stock market and is far from a stock market expert. But knowledge that can explicit can be added to the system prolonged incremental growth in the knowledge of the system could someday result in a systhat truly a stock market expert. VI. THE MACRO-LEVEL KNOWLEDGE CONSTRUCTS TENET The problem of dealing with the complexity of natural language is made much more tractable by working in macro-level knowledge constructs, such as semanunits consisting of whole lexical sisting of whole phrases, syntactic categories at the clause level, and a clause-combining grammar. Macroprocessing buys linguistic fluency at the of and linguistic flexibility. the of appears to be not much than the conimposed by the grammar semantics the of the domain of Furthermore, may be more to notion of macro-level semantic and linguistic processing than mere computational manageability. The notion of a phrasal lexicon was suggested by who proposed that people generate utterances by stitching together swatches of text they have heard before. Wilensky and Arens have experimented with a phrasal lexicon in a language understandsystem. I believe that natural language behavior will eventually be understood in terms of a theory of stratified natural language processing in which macrolevel knowledge constructs, such as those used in a knowledge-based report generator, occur at one of the higher cognitive strata. A poor but useful analogy to mechanical gearshifting while driving a car can be drawn. Just as driving in third gear makes most efficient use of an automobile&apos;s resources, so also does generating language in third gear make most efficient use of human information processing resources. That is, matching whole phrases and applying a clause-combining grammar is cognitively economical. But when only a near match for message can be in a speaker&apos;s phrasal dictionary, the speaker must downshift into second gear, and either perform some additional processing on the phrase to transform it into the desired form to match the message, or perform some processing on the message to transform it into one that matches the phrase. And if not even a near match for a message can be found, the speaker must downshift into first gear and either construct a phrase from elementary lexical items, including words, prefixes, and suffixes, or reconstruct the message. As currently configured, a knowledge-based text generator operates only in third gear. Because the units processing are linguistically mature whole the system can produce fluent text without having the detailed knowledge-needed to construct mature phrases from their elementary components. But there is nothing except the time and insight of a system implementor to prevent this detailed knowledge from added to the system. experimenting with additional knowledge, a system could gradually be extended to shift into lower gears, to exhibit greater interaction between semantic and linguistic components, and to do more flexible, if not creative, generation of semantic 149 messages and linguistic phrases. A knowledge-based report generator may be viewed as a starting tool for modeling a stratiform theory of natural language processing. VII. CONCLUSION report practical because it tackles a moderately ill-defined problem with an effective technique, namely, a macro-level, knowledge-based, production system technique. Stock market reports are typical instances of a whole class of summary-type periodic reports for which the scope and variety of semantic and linguistic complexity is great enough to negate a straightforward algorithmic solution, but constrained enough to allow a high-level cross-wise slice of the variety of knowledge to be effectively incorporated into a production system. Even so, it will be some time before the technique is cost effective. The time required to add knowledge to a system is greater than the time required to add productions to a traditional expert system. Most of the time is spent doing semantic analysis for the purpose of creating useful semantic classes and attributes, and identifying the relations between them. Coding itself goes quickly, but then the system must be tested and calibrated (if the guesses on the semantics were close) or redone entirely (if the guesses were not close). Still, the initial success of the technique suggests its value both as a basic research tool, for exploring increasingly more detailed semantic and linguistic processes, and as an applied research tool, for designing extensible and tailorable automatic report generators. ACKNOWLEDGEMENT wish to express my deep appreciation to Michael Lesk for his unfailing guidance and support in the development of this project.</abstract>
<note confidence="0.89650455">REFERENCES 1. Kathleen R. McKeown, &amp;quot;The TEXT System for Natural Language Generation: An Overview,&amp;quot; Proceedings of the Twentieth Annual Meeting of the for Computational Linguistics, Canada (1982). 2. James A. Moore and William C. Mann, &amp;quot;A Snapshot of KDS: A Knowledge Delivery System,&amp;quot; of the 17th Annual Meeting of the for Computational linguistics, Jolla, California (11-12 August 1979). 3. Douglas E. Appelt, &amp;quot;Problem Solving Applied to Generation,&amp;quot; pp. 59-63 Proceedings of 18th Annual Meeting of the for Com- Linguistics, of Pennsylvania, Philadelphia, PA (June 19-22,1980). C. L. User&apos;s Manual,&amp;quot; 81-135, Dept of Computer Science, Carnegie- University, Pittsburgh, (July 1981).</note>
<author confidence="0.972977">Joan Bresnan</author>
<author confidence="0.972977">Ronald M Kaplan</author>
<affiliation confidence="0.875379666666667">Functional Grammar: A Formal System for Grammatical Representation,&amp;quot; Occasional Paper #13, Center for Cognitive Science</affiliation>
<author confidence="0.82976">Kathleen Rose McKeown</author>
<author confidence="0.82976">Natural</author>
<affiliation confidence="0.876685333333333">Language Text in Response to Questions about Database Structure,&amp;quot; Doctoral Dissertation, University of Pennsylvania Computer and Informa-</affiliation>
<note confidence="0.96469">tion Science Department (1982). Melissa Bowerman, &amp;quot;The of Complex pp. 285-305 in Acquisition,</note>
<affiliation confidence="0.685955">Michael Garman, Cambridge University</affiliation>
<address confidence="0.783645">Cambridge (1979).</address>
<author confidence="0.961982">Richard Kittredge</author>
<author confidence="0.961982">John Lehrberger</author>
<author confidence="0.961982">Sub-</author>
<affiliation confidence="0.888516">languages: Studies of Language in Restricted Seman-</affiliation>
<address confidence="0.915014">Domains, DeGruyter, New York</address>
<note confidence="0.842281724137931">press). 9. Naomi Sager, &amp;quot;Information Structures in Texts of a in Community: Alliance for Progress - Proceedings of the 44th ASIS Meeting, Volume I8, Industry Publications for the American Society for Information Science, White Plains, N.Y. (October 1981). 10. Richard 1. Kittredge. &amp;quot;Semantic Processing of in Restricted Sublanguages,&amp;quot; and with Applications Pergamon Press (1982). 11. George E. Heidorn, &amp;quot;Natural Language Inputs to a Programming System,&amp;quot; 55HD72101A, Naval Postgraduate School, Monterey, CA (October 1972). Robert E. Frederking, Production System to Language Understanding, appear (1983). Joseph Becker, &amp;quot;The Phrasal Lexicon,&amp;quot; pp. Issues in ,Vatural Language Process- B. I. Nash-Webber, Cambridge, Mas- June 1975). 14. Robert Wilensky and Yigel Arens, &amp;quot;PHRAN -- A Knowledge-Based Natural Language Underpp. Proceedings of the 18th the Association for Computational of Pennsylvania. Philadelphia, Pennsylvania (June 19-22, 1980). 1.50</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>The TEXT System for Natural Language Generation: An Overview,&amp;quot;</title>
<date>1982</date>
<booktitle>Proceedings of the Twentieth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Toronto, Canada</location>
<contexts>
<context position="4077" citStr="(1)" startWordPosition="614" endWordPosition="614">are Ana&apos;s interpretation of the data on two different runs. DOW JONES INDUSTRIALS AVERAGE -- 01/12/83 01/12 CLOSE 30 INDUS 1083.61 01/12 330PM 30 INDUS 1089.40 01/12 3PM 30 INDUS 1093.44 01/12 230PM 30 INDUS 1100.07 01/12 2PM 30 INDUS 1095.38 01/12 130PM 30 INDUS 1095.75 01/12 1PM 30 INDUS 1095.84 01/12 1230PM 30 INDUS 1095.75 01/12 NOON 30 INDUS 1092.35 01/12 1130AM 30 INDUS 1089.40 01/12 11AM 30 INDUS 1085.08 01/12 1030AM 30 INDUS 1085.36 01/11 CLOSE 30 INDUS 1083.79 CLOSING AVERAGE 1083.61 DOWN 0.18 145 1102 1098 1094 1090 1086 1082 10 10:30 1111:30 12 12:30 1 1:30 2 2:30 3 3:30 4 Figure 1 (1) after climbing steadily through most of the morning , the stock market was pushed downhill late in the day . stock prices posted a small loss , with the indexes turning in a mixed showing yesterday in brisk trading . the Dow Jones average of 30 industrials surrendered a 16.28 gain at 4pm and declined slightly , finishing the day at 1083.61 , off 0.18 points . (2) wall street&apos;s securities markets rose steadily through most of the morning , before sliding downhill late in the day . the stock market posted a small loss yesterday , with the indexes finishing with mixed results in active trading .</context>
<context position="19978" citStr="(1)" startWordPosition="3117" endWordPosition="3117">oice of some other syntactic form. Thus, if the user prefers reports that are low on subordinate participial clauses, the subordinate participial clause parameter might be set at 3 or lower. The following production contains the bank of parameters as they were set to generate text sample (2) above: (p _1.setparams (goal Astat act &apos;op setparams) --&gt; (remove 1) (make paramsyllables &amp;quot;val 30) (make parammessages &apos;val 3) (make paramsynforms &apos;sentence 11 &apos;coorsent 11 &amp;quot;suborsent 11 prepphrase 11 &amp;quot;suborsentpre 5 &amp;quot;suborpartpre 8 &amp;quot;suborsentpost 8 &amp;quot;suborpartpost 11 &amp;quot;subotpartsentpost 11 When sample text (1) was generated, all syntactic form parameters were set at 11. The first two parameters in the bank are rhetoric parameters. They control the maximum length of sentences in syllables (roughly) and in number of messages per sentence. Not only does production system knowledge representation allow syntactic tailoring, but it also permits semantic tailoring. Ana could be tailored to focus on particular stocks or groups of stocks to meet the information needs of individual users. Furthermore, a production system is readily extensible. Currently, Ana has only a small amount of general knowledge about</context>
</contexts>
<marker>1.</marker>
<rawString>Kathleen R. McKeown, &amp;quot;The TEXT System for Natural Language Generation: An Overview,&amp;quot; Proceedings of the Twentieth Annual Meeting of the Association for Computational Linguistics, Toronto, Canada (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James A Moore</author>
<author>William C Mann</author>
</authors>
<title>A Snapshot of KDS: A Knowledge Delivery System,&amp;quot;</title>
<date>1979</date>
<booktitle>in Proceedings of the 17th Annual Meeting of the Association for Computational linguistics,</booktitle>
<location>La Jolla, California</location>
<contexts>
<context position="4443" citStr="(2)" startWordPosition="683" endWordPosition="683">0AM 30 INDUS 1089.40 01/12 11AM 30 INDUS 1085.08 01/12 1030AM 30 INDUS 1085.36 01/11 CLOSE 30 INDUS 1083.79 CLOSING AVERAGE 1083.61 DOWN 0.18 145 1102 1098 1094 1090 1086 1082 10 10:30 1111:30 12 12:30 1 1:30 2 2:30 3 3:30 4 Figure 1 (1) after climbing steadily through most of the morning , the stock market was pushed downhill late in the day . stock prices posted a small loss , with the indexes turning in a mixed showing yesterday in brisk trading . the Dow Jones average of 30 industrials surrendered a 16.28 gain at 4pm and declined slightly , finishing the day at 1083.61 , off 0.18 points . (2) wall street&apos;s securities markets rose steadily through most of the morning , before sliding downhill late in the day . the stock market posted a small loss yesterday , with the indexes finishing with mixed results in active trading . the Dow Jones average of 30 industrials surrendered a 16.28 gain at 4pm and declined slightly to finish at 1083.61 , off 0.18 points . III. SYSTEM OVERVIEW In order to generate accurate and fluent summaries, a knowledge-based report generator performs two main tasks: first, it infers semantic messages from the data in the database; second, it maps those messages </context>
<context position="19667" citStr="(2)" startWordPosition="3074" endWordPosition="3074">t the rule in which the parameter occurs would be satisfied if all its other conditions were satisfied. Consequently, on 20 percent of the occasions when the rule would have been otherwise satisfied, the syntactic form parameter would prevent the rule from firing, and the system would be forced to opt for a choice of some other syntactic form. Thus, if the user prefers reports that are low on subordinate participial clauses, the subordinate participial clause parameter might be set at 3 or lower. The following production contains the bank of parameters as they were set to generate text sample (2) above: (p _1.setparams (goal Astat act &apos;op setparams) --&gt; (remove 1) (make paramsyllables &amp;quot;val 30) (make parammessages &apos;val 3) (make paramsynforms &apos;sentence 11 &apos;coorsent 11 &amp;quot;suborsent 11 prepphrase 11 &amp;quot;suborsentpre 5 &amp;quot;suborpartpre 8 &amp;quot;suborsentpost 8 &amp;quot;suborpartpost 11 &amp;quot;subotpartsentpost 11 When sample text (1) was generated, all syntactic form parameters were set at 11. The first two parameters in the bank are rhetoric parameters. They control the maximum length of sentences in syllables (roughly) and in number of messages per sentence. Not only does production system knowledge representation </context>
</contexts>
<marker>2.</marker>
<rawString>James A. Moore and William C. Mann, &amp;quot;A Snapshot of KDS: A Knowledge Delivery System,&amp;quot; in Proceedings of the 17th Annual Meeting of the Association for Computational linguistics, La Jolla, California (11-12 August 1979).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Problem Solving Applied to Language Generation,&amp;quot;</title>
<date>1980</date>
<booktitle>in Proceedings of the 18th Annual Meeting of the Association</booktitle>
<pages>59--63</pages>
<institution>for Computational Linguistics, University of Pennsylvania,</institution>
<location>Philadelphia, PA</location>
<marker>3.</marker>
<rawString>Douglas E. Appelt, &amp;quot;Problem Solving Applied to Language Generation,&amp;quot; pp. 59-63 in Proceedings of the 18th Annual Meeting of the Association for Computational Linguistics, University of Pennsylvania, Philadelphia, PA (June 19-22,1980).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Forgy</author>
</authors>
<title>OPS-5 User&apos;s Manual,&amp;quot;</title>
<date>1981</date>
<tech>CMU-CS81-135,</tech>
<pages>15213</pages>
<institution>Dept of Computer Science, CarnegieMellon University,</institution>
<location>Pittsburgh, PA</location>
<marker>4.</marker>
<rawString>C. L. Forgy, &amp;quot;OPS-5 User&apos;s Manual,&amp;quot; CMU-CS81-135, Dept of Computer Science, CarnegieMellon University, Pittsburgh, PA 15213 (July 1981).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Ronald M Kaplan</author>
</authors>
<title>LexicalFunctional Grammar: A Formal System for Grammatical Representation,&amp;quot;</title>
<date>1982</date>
<tech>Occasional Paper #13,</tech>
<institution>MIT Center for Cognitive Science</institution>
<marker>5.</marker>
<rawString>Joan Bresnan and Ronald M. Kaplan, &amp;quot;LexicalFunctional Grammar: A Formal System for Grammatical Representation,&amp;quot; Occasional Paper #13, MIT Center for Cognitive Science (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Rose McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions about Database Structure,&amp;quot;</title>
<date>1982</date>
<institution>Doctoral Dissertation, University of Pennsylvania Computer and Information Science Department</institution>
<marker>6.</marker>
<rawString>Kathleen Rose McKeown, &amp;quot;Generating Natural Language Text in Response to Questions about Database Structure,&amp;quot; Doctoral Dissertation, University of Pennsylvania Computer and Information Science Department (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melissa Bowerman</author>
</authors>
<title>The Acquisition of Complex Sentences,&amp;quot;</title>
<date>1979</date>
<pages>285--305</pages>
<editor>in Language Acquisition, ed. Michael Garman,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge</location>
<marker>7.</marker>
<rawString>Melissa Bowerman, &amp;quot;The Acquisition of Complex Sentences,&amp;quot; pp. 285-305 in Language Acquisition, ed. Michael Garman, Cambridge University Press, Cambridge (1979).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Kittredge</author>
<author>John Lehrberger</author>
</authors>
<title>Sublanguages: Studies of Language in Restricted Semantic Domains, Walter DeGruyter,</title>
<location>New York</location>
<note>(in press).</note>
<marker>8.</marker>
<rawString>Richard Kittredge and John Lehrberger, Sublanguages: Studies of Language in Restricted Semantic Domains, Walter DeGruyter, New York (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
</authors>
<title>Information Structures in Texts of a Subianguage,&amp;quot; in The Information Community: Alliance for Progress -</title>
<date>1981</date>
<booktitle>Proceedings of the 44th ASIS Annual Meeting, Volume I8, Knowlton Industry Publications for the American Society for Information Science,</booktitle>
<location>White Plains, N.Y.</location>
<marker>9.</marker>
<rawString>Naomi Sager, &amp;quot;Information Structures in Texts of a Subianguage,&amp;quot; in The Information Community: Alliance for Progress - Proceedings of the 44th ASIS Annual Meeting, Volume I8, Knowlton Industry Publications for the American Society for Information Science, White Plains, N.Y. (October 1981).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard</author>
</authors>
<title>Semantic Processing of Texts in Restricted Sublanguages,&amp;quot;</title>
<date>1982</date>
<journal>Computers and Mathematics with Applications</journal>
<volume>8</volume>
<issue>0</issue>
<publisher>Pergamon Press</publisher>
<marker>10.</marker>
<rawString>Richard 1. Kittredge. &amp;quot;Semantic Processing of Texts in Restricted Sublanguages,&amp;quot; Computers and Mathematics with Applications 8(0), Pergamon Press (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Heidorn</author>
</authors>
<title>Natural Language Inputs to a Simulation Programming System,&amp;quot; NPS55HD72101A,</title>
<date>1972</date>
<location>Naval Postgraduate School, Monterey, CA</location>
<marker>11.</marker>
<rawString>George E. Heidorn, &amp;quot;Natural Language Inputs to a Simulation Programming System,&amp;quot; NPS55HD72101A, Naval Postgraduate School, Monterey, CA (October 1972).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Frederking</author>
</authors>
<title>A Production System Approach to Language Understanding,</title>
<date>1983</date>
<note>To appear</note>
<marker>12.</marker>
<rawString>Robert E. Frederking, A Production System Approach to Language Understanding, To appear (1983).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Becker</author>
</authors>
<title>The Phrasal Lexicon,&amp;quot;</title>
<date>1975</date>
<booktitle>in Theoretical Issues in ,Vatural Language Processing,</booktitle>
<pages>70--73</pages>
<editor>ed. B. I. Nash-Webber,</editor>
<location>Cambridge, Massachusetts</location>
<marker>13.</marker>
<rawString>Joseph Becker, &amp;quot;The Phrasal Lexicon,&amp;quot; pp. 70-73 in Theoretical Issues in ,Vatural Language Processing, ed. B. I. Nash-Webber, Cambridge, Massachusetts (10-13 June 1975).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Wilensky</author>
<author>Yigel Arens</author>
</authors>
<title>PHRAN -- A Knowledge-Based Natural Language Understander,&amp;quot;</title>
<date>1980</date>
<booktitle>in Proceedings of the 18th Annual Meeting of the Association</booktitle>
<pages>117--121</pages>
<institution>for Computational Linguistics, University of Pennsylvania.</institution>
<location>Philadelphia, Pennsylvania</location>
<marker>14.</marker>
<rawString>Robert Wilensky and Yigel Arens, &amp;quot;PHRAN -- A Knowledge-Based Natural Language Understander,&amp;quot; pp. 117-121 in Proceedings of the 18th Annual Meeting of the Association for Computational Linguistics, University of Pennsylvania. Philadelphia, Pennsylvania (June 19-22, 1980).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>