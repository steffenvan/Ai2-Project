<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008627">
<title confidence="0.975712">
Detecting a Continuum of Compositionality in Phrasal Verbs.
</title>
<author confidence="0.99108">
Diana McCarthy &amp; Bill Keller &amp; John Carroll
</author>
<affiliation confidence="0.8302005">
Cognitive &amp; Computing Sciences,
University of Sussex
</affiliation>
<address confidence="0.992943">
Brighton BN1 9QH, UK
</address>
<email confidence="0.958527">
dianam,billk,johncaOcogs.susx.ac.uk
</email>
<sectionHeader confidence="0.98675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888294117647">
We investigate the use of an automatically
acquired thesaurus for measures designed
to indicate the compositionality of candi-
date multiword verbs, specifically English
phrasal verbs identified automatically using
a robust parser. We examine various mea-
sures using the nearest neighbours of the
phrasal verb, and in some cases the neigh-
bours of the simplex counterpart and show
that some of these correlate significantly
with human rankings of compositionality
on the test set. We also show that whilst
the compositionality judgements correlate
with some statistics commonly used for ex-
tracting multiwords, the relationship is not
as strong as that using the automatically
constructed thesaurus.
</bodyText>
<sectionHeader confidence="0.996382" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998083269230769">
Many people are working on acquisition of multiword
expressions, although terminology varies. In this pa-
per, we are interested in lexicalised expressions (Sag
et al., 2002) where special interpretation is required
because of some degree of non-compositionality or
semantic opacity. We are specifically concerned with
what are commonly referred to as phrasal verbs, or
verb and particle constructions (Baldwin and Villav-
icencio, 2002). As well as having idiosyncratic se-
mantics, phrasals also display specific syntactic be-
haviour such as permitting particle movement when
used in the transitive; for example:
Jo ate up her food Jo ate her food up
We are interested in phrasal verbs because we want
to acquire predicate selectional preferences for word
sense disambiguation (McCarthy et al., 2001). When
acquiring such lexical information for a verb it is im-
portant to know when there is a special interpreta-
tion required for the verb and particle combination
so that these combinations are handled separately
from the simplex case. Whilst it is possible to put
every single occurrence of a verb and particle combi-
nation into a lexicon this is not desirable. One wants
to achieve generalisation and avoid redundancy, only
storing details which cannot be created from what is
already there. Not every verb modified by a particle
may be a genuine multiword unit, but may instead
be a fully compositional verb modified by an adver-
bial e.g. fly up. Also very productive verb particle
combinations such as those involving verbs of mo-
tion, which often occur with a particle e.g. up, such
as wander, stroll, go etc... might be better handled
in the grammar (Villavicencio and Copestake, 2002).
Additionally, in lexical acquisition, and for word
sense disambiguation, it is important that related
senses of words are identified. For example, if the
verb eat is closer in meaning to a phrasal construc-
tion eat up, compared to other simplex verbs with
their phrasal constructions such as blow/blow up,
then the lexicon should reflect that. Having a mea-
sure of compositionality should help in this.
In this paper we are not concerned with evaluation
of precision and recall of the extraction of phrasal
verbs from a parser, although we have done some
preliminary experiments in this direction on the Wall
Street Journal (wsJ), see section 3. Instead, our fo-
cus is on methods of using an automatically acquired
thesaurus for detecting compositionality of candidate
phrasals output from our parser. We contrast this
with some statistics commonly used for multiword
extraction. The thesaurus is acquired from the gram-
matical relations occurring with verbs, both the tar-
get phrasals and their simplex counterparts. The
intuition is that the neighbours of the simplex verb
should be similar to those of the phrasal where the
phrasal has a compositional meaning, and that the
phrasal neighbours should include phrasal candidates
with the same particle.
For evaluation, we obtain a sample of multiword
candidates from our parser and then obtain human
judgements of compositionality using an ordinal scale
for compositionality. We demonstrate that there is
highly significant agreement on the rank order of
these judgements and use the average ranks for each
item as a gold-standard to compare various measures
aimed at detecting non-compositionality.
In the following section we look at related work.
In section 3 we show how phrasals are identified by
our parser. We talk about the generation of the gold-
standard set of compositionality judgements in sec-
tion 4. In section 5 we describe the construction of
the automatic thesaurus and the measurements we
explored for detecting compositionality. In section 6
we show the correlations of our measures with the
gold-standard, and compare these to some statistics
commonly used for identifying compositional multi-
words. In section 7 we analyse our findings, and
conclude (section 8) with directions for future work.
</bodyText>
<sectionHeader confidence="0.994103" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.99965175">
There has been a lot of recent work on extraction
of multiwords from corpora we focus specifically on
work involving multiword verbs, and detecting com-
positionality of multiwords.
</bodyText>
<subsectionHeader confidence="0.975511">
2.1 Multiword Verb Extraction
</subsectionHeader>
<bodyText confidence="0.995643470588235">
There have been a number of methods pro-
posed in the literature for extracting mul-
tiword verb constructions from corpora.
Baldwin and Villavicencio (2002) demonstrated
that combining syntactic evidence using automatic
PoS taggers and statistical chunkers, and feeding
evidence from a number of tokens into a memory
based-learner gave high precision and recall, us-
ing marked up WSJ text to gauge precision, and
phrasals listed in the Alvey Natural Language Tools
(ANLT) (Grover et al., 1993) attested in the same
corpus for recall. No distinction on opaqueness of
the verb and particle constructions was made.
Blaheta and Johnson (2001) used log-linear mod-
els to extract English multiword verbs involving verb
and particle constructions from parsed data; these
include phrasal and prepositional verbs.
</bodyText>
<footnote confidence="0.530877">
&apos;Prepositional verbs also have some degree of idiosyn-
cratic semantic interpretation, but the particle functions
as a preposition and selects for the following noun phrase.
There is therefore no particle movement e.g. *she referred
the problem to.
</footnote>
<bodyText confidence="0.999558363636364">
Krenn and Evert (2001) investigated German sup-
port verb constructions (identifiable on grammati-
cal grounds) and figurative expressions (having id-
iomatic interpretations). In their experiments, true
positives were typically defined as such according to
the annotator scanning the list. Krenn and Evert
found that different statistics are suited to different
types of collocation - there is no easy route for collo-
cation extraction. Moreover, they found that a sim-
ple co-occurrence frequency fares comparably, if not
better, than most statistical tests of significance.
</bodyText>
<subsectionHeader confidence="0.99988">
2.2 Composition.ality of Multiwords
</subsectionHeader>
<bodyText confidence="0.999979925925926">
Most people researching into multiwords assume
some degree of non-compositionality. Blaheta and
Johnson took human judgements on phrasality,
opaqueness (a dichotomous scale) and a subjective
judgement of relatedness (on a scale between 1 and
5). They showed that the opaqueness judgements
correlated with the relatedness (good collocation)
judgement. Also, those constructions judged to be
phrasals tended to have higher ranks (higher opaque-
ness and relatedness) than prepositional verb particle
constructions.
Both Lin (1999) and Schone and Jurafsky (2001)
have used distributional similarity to detect com-
positionality in multiwords. Schone and Jurafsky
used measures on the vectors representing the multi-
word candidates compared to measures for the words
that the multiword contains but this failed to im-
prove performance, using WordNet and other ma-
chine readable resources as gold-standards for eval-
uation. There was some success though in using la-
tent semantic analysis (LsA) models to identify mul-
tiwords by the fact that the component words are
typically non-substitutable, but they felt that much
of what is captured by this is already handled by the
statistics that they employ.
Lin (1999) had already done something similar to
the substitutability experiments using the method
he had proposed earlier (Lin, 1998a) for automatic
thesaurus construction. He identified general multi-
words involving several open-class words output from
his parser and filtered by the log-likelihood statistic.
Using the parser yielded much better results than
just a simple window for co-occurrence relationships.
Lin proposed that if there is a multiword obtained
by substitution of either the head or modifier in the
multiword with a near neighbour, then the mutual
information of this and the original multiword must
be significantly different for the original multiword
to be considered non-compositional. He evaluated
this manually on a sample. As well as finding non-
compositional multiwords, there were also a higher
proportion of parser errors that met these criteria.
Bannard et al. (2003) are investigating composi-
tionality by looking at the contribution of the
verb, and the particle to the semantics of the
verb and particle combination; this follows on
from Bannard&apos;s earlier work (2002) where he
showed that compositionality judgements correlate
with human judgements of similarity between the
head verb and the verb and particle combination.
Bannard et al. (2003) point out that Lin&apos;s method
of using substitution of component words in a multi-
word with semantic neighbours is a good indication
of productivity, but not necessarily of compositional-
ity, since an institutionalised non-productive combi-
nation, such as frying pan would not have near neigh-
bour substitutes, but would nevertheless be compo-
sitional. They explore four methods for detecting
compositionality using resources acquired from dis-
tributional data. They use these on 40 candidates
on 4 separate tasks which aim to determine whether
i) the item is compositional, ii) one component word
contributes its meaning iii) the verb contributes its
meaning iv) the particle contributes its meaning.
The classifications on each of these tasks according
to these methods are contrasted with a gold stan-
dard classification from 26 judges on the same data.
The methods exceed the mean agreement of the an-
notators in some cases, particularly as regards the
contribution from the particle.
Baldwin et al. (2003) are also exploring empiri-
cal models of compositionality using LSA with noun-
noun compounds and verb-particle constructions. In
their study, they compare the similarities of the com-
ponent words with WordNet based similarity scores
and demonstrate a moderate correlation, lower for
noun-noun compounds.
We are also exploring the relation between a verb
and verb and particle combination (we use the term
phrasal verb) using distributional techniques, but our
evaluation is somewhat different.
</bodyText>
<subsectionHeader confidence="0.989064">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999955630434783">
Evaluation of collocation extraction is a notoriously
thorny problem (Krenn and Evert, 2001; Pearce,
2002). People do use MRDs such as WordNet
(Schone and Jurafsky, 2001) even though they ac-
knowledge that there will be omissions in these re-
sources, and the phenomena in the resource may be
rare or simply not attested in the particular corpus
used for acquisition. Many researchers use manu-
ally annotated samples, where the judges make a bi-
nary decision on whether each candidate multiword
is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson,
2001; Krenn and Evert, 2001; Baldwin and Villavi-
cencio, 2002). As Krenn and Evert point out, there
is low agreement between annotators who are asked
to mark &amp;quot;typical&amp;quot; multiwords, or collocations. The
intuitions behind what is typical vary, and likewise
association scores vary in their ability to partition
the set depending on the notion of &amp;quot;typicality&amp;quot; em-
ployed by the annotators. Researchers also some-
times show how well the results accord with the con-
tents of MRDs, even though these cannot be taken
as definitive.
In this study we are less interested in the di-
chotomy of whether a putative phrasal candidate
is indeed a genuine multiword or not (although it
is more likely that those with low compositional-
ity are likely to be) but we use empirical meth-
ods to gauge the position of a candidate on a con-
tinuum between the fully opaque idiom and trans-
parent compositional phrases. Variability of id-
ioms on a scale of compositionality has been dis-
cussed by Nunberg et al. (1994) and in the psycho-
linguistics literature, see (Gibbs and Nayak, 1989).
Tseng (2000) also advocates use of a spectrum when
considering the semantics of prepositions. We will
consider compositionality as a continuous scale and
ask human judges to rank multiword candidates
along this. We investigate the use of these ranked
judgements for evaluating compositionality mea-
sures. We also look at the relation between these
judgements and appearance of the candidates in
gold-standard resources such as WordNet (Miller et
al., 1993) or the ANLT lexicon (Grover et al., 1993),
on the premise that non-compositional phrases are
more likely to be listed as multiwords in man-made
resources.
</bodyText>
<sectionHeader confidence="0.993886" genericHeader="method">
3 Parser Output
</sectionHeader>
<bodyText confidence="0.999847222222222">
For these experiments we use data from the
ninety million words of the written portion of
the British National Corpus parsed with the RASP
parser (Briscoe and Carroll, 2002). The output of
the parser is a set of grammatical relations (Carroll
et al., 1998) specifying the syntactic dependency be-
tween each head and its dependent(s), read off from
the phrase structure tree that is returned from the
disambiguation phase. The parser uses information
from ANLT such as phrasals in its dictionary. This
makes it more likely to spot phrasal constructions
from this list. We have already looked at recognition
of verb and particle constructions in the WSJ iden-
tified purely on syntactic grounds using the parses
provided with the WSJ Penn Treebank 2 (Marcus et
al., 1995) as a gold standard. The results for identi-
fying verb and particle tokens are reported in table 1,
both with and without the ANLT phrasal list (ANLT
</bodyText>
<table confidence="0.9999315">
Precision Recall
MINIPAR 78.9 44.1
RASP (without ANLT phr) 87.6 49.4
RASP (with ANLT phr) 92.6 64.2
</table>
<tableCaption confidence="0.7694085">
Table 1: Identification of verb and particle attach-
ments in WSJ data
</tableCaption>
<bodyText confidence="0.9998151875">
phr). We also give results for comparison obtained
on the same data for another wide coverage parser,
(MINIPAR (Lin, 1998b)). 2
In the RASP parser grammatical relation output we
identify phrasal verbs as being a verb modified by a
particle (tagged RP) under the ncmod (non-clausal
modifier) relation. It is quite possible that some par-
ticle tags have been given erroneously and that some
genuine particles are not recognised as such by the
parser, or are not attached to the verb by the parser.
We only look at tokens in isolation and therefore do
not collate evidence to look for syntactic evidence of
particle movement as Baldwin and Villavicencio do.
This would be a good way to improve phrasal extrac-
tion accuracy, particularly where a particle follows a
pronoun.
</bodyText>
<sectionHeader confidence="0.9966265" genericHeader="method">
4 Human Compositionality
Judgements
</sectionHeader>
<bodyText confidence="0.999464333333333">
In our experiments we asked human judges to rank
phrasal verb candidates as to how compositional they
are.
</bodyText>
<subsectionHeader confidence="0.996375">
4.1 Test set
</subsectionHeader>
<bodyText confidence="0.999984714285714">
From the full set of 4272 phrasal verb candidate types
output from the RASP parser we obtained 100 can-
didates randomly subject to the constraint that 33 3
each came from one of 3 frequency ranges (each range
covering an even number of phrasal types) from 20
to the maximum frequency. A further 16 manually
selected phrasals were added to this test set.
Three native English speakers ranked the 116 can-
didates on a numerical score 0 to 10 (10 for fully-
compositional, 0 for totally opaque), or gave a &amp;quot;don&apos;t
know&amp;quot; response. We discounted any item where any
of the judges had put such a &amp;quot;don&apos;t know&amp;quot;. This only
removed a total of 5 items, leaving a ranking from
all 3 judges on 111 candidates.
</bodyText>
<subsectionHeader confidence="0.821279">
4.2 Human Agreement
</subsectionHeader>
<bodyText confidence="0.8843955">
To investigate if the rankings from the 3 judges
agreed we employed the Kendall Coefficient of Con-
</bodyText>
<footnote confidence="0.970822">
2We are indebted to Mirella Lapata for the results
using MINIPAR.
3ThiS was 34 from the lowest frequency range.
</footnote>
<bodyText confidence="0.990859461538462">
cordance (W) (Siegel and Castellan, 1988). This
statistic is useful for determining inter-rater agree-
ment where there are 3 or more judges and the judge-
ments are ordinal, and one is interested in the ranks
rather than the actual numerical values. W ranges
between 0 (little agreement) and 1 (full agreement)
and bears a linear relationship to the average Spear-
man Rank-order Correlation Coefficient taken over
all possible pairs of the rankings.
W is calculated as shown in equation 1 below,
where n is the number of items (111 in this case),
Ri is the average rank for the ith item and k is the
number of raters.
</bodyText>
<equation confidence="0.9158385">
w = 12 Ein_o — 3n(n, + 1)2
Ti
</equation>
<bodyText confidence="0.9988825">
The second term in the denominator includes a
correction for ties where:
</bodyText>
<equation confidence="0.97621">
Ti = (t — (2)
</equation>
<bodyText confidence="0.999891285714286">
where ti is the number of tied ranks in the ith group-
ing of ranks.
The value k(n —1)W is approximately distributed
as X2 with n — 1 degrees of freedom. We obtained
a W score of 0.594 which gives a X2 score of 196.30
for 110 degrees of freedom which is highly significant
(probability of this value &lt;= 0.000001).
</bodyText>
<sectionHeader confidence="0.99126" genericHeader="method">
5 Detecting Compositionality
</sectionHeader>
<subsectionHeader confidence="0.70152">
5.1 Using an Automatically Constructed
</subsectionHeader>
<bodyText confidence="0.985666285714286">
Thesaurus:
Using the method proposed by Lin (1998a) we pro-
duced a thesaurus with 500 nearest neighbours for
the set of phrasal verbs as described above. Tuples
of the form &lt;verb, argument head, grammatical rela-
tion&gt; from the parsed BNC data were used for this
purpose where the verb was the multiword phrasal
and the grammatical relations used were subjects
and direct objects. We did likewise for the simplex
verbs contained within the phrasals (e.g. blow from
blow up).
We investigated various measures which compare
the nearest neighbours of the phrasal verb to the
neighbours of the corresponding simplex verb. We
also tried various measures on the neighbours of the
phrasal verb. We supply short labels for these for
ease of reference.
overlap The size of the overlap of the top X phrasal
neighbours with the same number of the corre-
sponding simplex verb&apos;s neighbours, not includ-
ing the simplex verb itself. We tried this for
</bodyText>
<figure confidence="0.996582655737705">
(1)
71(712 1)
neighbours of climb down neighbours of climb
clamber up
slither down
creep down
scramble down
skip down
scramble up
climb up
clamber
glance up
climb up
walk down
walk
jump
go up
rise
descend
cross
stumble down
come down
ascend
run up
reach
go down
leap
leap down
rush up
...
...
neighbours of climb down neighbours of climb
with phrasals as simplex:
clamber
slither
creep
scramble
skip
glance
stumble
step
wander
walk
slip
swing
leap
rush
disappear
fly
jump
go up
rise
descend
cross
come down
ascend
run up
reach
go down
...
...
</figure>
<table confidence="0.981044083333334">
Correlation with Measures Using the Thesaurus
measure correlation statistic Z score probability under H.
overlap PN SN 500 rs = - 0.032 -0.38 0.35
overlap PN SN 100 rs = 0.037 0.39 0.35
overlap PN SN 50 T., = 0.136 1.43 0.08
overlap PN SN 30 T., = 0.166 1.74 0.04
sameparticle PN 500 rs = 0.414 4.34 &lt;0.00003
sameparticle-simplex PN SN 500 T., = 0.49 5.17 &lt;0.00003
simplexasneighbour PN 500 Mann W 0.950 0.171
simplexrank PN 500 rs = -0.115 -1.21 0.113
simplexscore PN 500 rs = 0.052 0.54 0.295
overlapS PN SN 30 T., = 0.306 3.21 &lt;0.0007
overlapS PN SN 50 T., = 0.303 3.18 &lt;0.0007
overlapS PN SN 500 T., = 0.167 1.75 0.040
Correlation with Man-made Resources
WordNet Mann W 2.39 0.008
ANLT Phrasals Mann W 3.03 0.012
ANLT Prepositionals Mann W 0.430 0.334
Correlation with Statistics (used for multiword extraction)
X2 rs = -0.213 -2.22 0.0139
LLR T., = -0.168 -1.76 0.0392
MI T., = -0.248 -2.60 0.0047
phrasal Freq T., = -0.096 -1.01 0.156
simplex Freq rs = 0.092 0.96 0.169
</table>
<tableCaption confidence="0.999909">
Table 2: Correlation with human compositionality judgements
</tableCaption>
<bodyText confidence="0.999907886363636">
sures which use the automatic thesaurus we indicate
whether the measure relies only on the phrasal neigh-
bours (PN), or the simplex neighbours (SN) or some
combination of both (PN SN). In this first column, we
also indicate how many of the top ranked neighbours
were used. Where we are evaluating scores on a nu-
merical scale, such as the size of the overlap, we use
the ranks of the numerical values and compare these
to the average ranks of our gold-standard using the
Spearman Rank-Order Correlation Coefficient (r8).
Since we have a large enough sample, these can be
used to obtain a normally distributed Z score and
we can thus obtain the probability of obtaining a
score such as this by chance under the null hypoth-
esis (that there is no relationship). For the scores
which involve a binary decision, such as whether a
score is in WordNet or not, we use the Mann Whit-
ney U test, which compares the gold-standard ranks
for the partitioned set and gives a Z score. We use
one-tailed tests because we predict the direction of
the relationship. For all the scores using the auto-
matic thesaurus, we assume that the larger the value,
the more compositional the item.
For the statistics (commonly used for multiword
extraction) the relationship is in the other direction:
high values are indicative of a non-compositional
reading. We change the log-likelihood statistic to
add a sign where the joint frequency of particle and
verb is smaller than anticipated from that expected.
From these results we can see that some of
the measures from the automatic thesaurus corre-
late significantly with the human compositionality
judgements and that these correlations are slightly
stronger than those of any of the statistics used.
The statistics used all correlate (in the other direc-
tion) with the human compositionality judgements,
although this is slightly less so for the log-likelihood
ratio. The frequency of the verb and particle seems
to bear no significant relation to compositionality
judgements. This is interesting because Krenn and
Evert found that co-occurrence frequency was a good
indication of the German multiwords, although the
task there was identification of the multiwords, as
opposed to measuring compositionality.
</bodyText>
<sectionHeader confidence="0.898855" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.999872557692308">
MI is the statistic with the strongest value of T., and
the thesaurus measure with the strongest relation-
ship was sameparticle-simplex. These two mea-
sures correlated well together (T., = -0.51, z = -5.37)
and both are significantly correlated (using the Mann
Whitney U test) with whether the candidate is found
in either WordNet or ANLT, see table 3, although the
relationship using the automatic thesaurus is slightly
higher.
Lin uses a log-likelihood ratio to filter multiword
candidates before using his automatic thesaurus to
detect compositionality in multiwords containing 2
or more open class words. For phrasal candidates
at least, it might be worth using evidence from the
thesaurus on the unfiltered list.
We were surprised, and a little disappointed that
the straight overlap of neighbours did not give a sig-
nificant relationship, other than for the overlap of 30
neighbours. We believe this is due to the large scope
for open class words as neighbours, and that there is
often some element of meaning added by the parti-
cle. Thus the overlap where we reduce neighbours of
the phrasal to simplex form compensated for this.
We have not yet explored varying the number of
neighbours for methods other than the overlap and
overlapS. We feel that it would be worth exploring
the effect of the number of neighbours further, and
also to use the similarity scores of the neighbours,
rather than simple measures operating on the types
occurring as neighbours. This would help control for
the fact that for some verbs there are not many close
neighbours and neighbours further down the ranked
list may in fact be quite distant.
Whilst statistics are useful indicators of non-
compositionality, there are compositional multiwords
which have low values for these statistics, yet are
highly non-compositional. A good example is cock
up; it is the lowest ranked for compostionality by
the human judges, but its MI value is only 5.02, and
according to MI it is ranked between the somewhat
more compositional candidates tie down and come
down. The automatic thesaurus measures such as
sameparticle-simplex give a low compositionality
score and place it at the end between carry out and
latch on.
There are also candidates with high values of the
statistics, yet they are in the middle range of the
compositionality judgements, for example, plod on.
This is simply because of a high co-occurrence fre-
quency. Whether such an unexpectedly high co-
occurrence frequency warrants an entry in the lex-
icon depends on the type of lexicon being built.
</bodyText>
<sectionHeader confidence="0.997785" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999244">
We can see that there is a significant relationship be-
tween the human compositionality judgements and
some of the measures from the automatic thesaurus,
particularly those that endeavour to take into ac-
</bodyText>
<table confidence="0.988817333333333">
Measure in WordNet in ANLT
MI -2.61 -4.53
sameparticle-simplex 3.71 4.59
</table>
<tableCaption confidence="0.995249">
Table 3: Mann Whitney Z scores showing correlation
</tableCaption>
<bodyText confidence="0.983045214285714">
of measures with man-made resources
count the semantics of the particle. This relation-
ship is stronger than statistics which have previously
been used for filtering candidate multiwords which
suggests that it might be better not to filter with
statistics before looking at compositionality using an
automatic thesaurus.
We have not yet exploited these measures in the
construction of a lexicon for phrasal verbs. Identify-
ing non-compositional phrasals by employing thresh-
olds to force a binary decision is one option. This
would help in determining which candidate phrasals
should be treated separately from the simplex for
purposes such as selectional preference acquisition
and word sense disambiguation. The thresholds
might be acquired empirically from some training
data, such as the compositionality judgements we
have used. However, we believe that permitting mea-
surements and evaluation on a continuum of compo-
sitionality allows for a more natural exploration of
relationships, without imposing an arbitrary cut-off
point required only when finally categorising items
for a lexicon. It also could be useful to use the mea-
sures to tell whether the meaning comes from the
verb or the particle or both, as Bannard et al. (2003)
do, because if the verb contributes its meaning then
data for selectional preference acquisition might be
amalgamated with those of the simplex counterpart.
</bodyText>
<sectionHeader confidence="0.996394" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99993">
This work was supported by the EPSRC-funded
RASP project (grant GR/N36493), and the EU 5th
Framework project MEANING — Developing Multi-
lingual Web-scale Language Technologies (IST-2001-
34460). We are grateful to Timothy Baldwin and
Colin Bannard for their helpful comments and use-
ful references.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99925331775701">
T. Baldwin and A. Villavicencio. 2002. Extract-
ing the unextractable: A case study on verb-
particles. In Proceedings of the Sixth Confer-
ence on Computational Natural Language Learn-
ing (CoNLL 2002), pages 98-104, Taipei, Taiwan.
T. Baldwin, C. Bannard, T. Tanaka, and D. Wid-
dows. 2003. An empirical model of multiword
expression decomposability. In Proceedings of the
ACL Workshop on multiword expressions: analy-
sis, acquisition and treatment.
C. Bannard, T. Baldwin, and A. Lascarides. 2003.
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL Workshop on
multiword expressions: analysis, acquisition and
treatment.
C. Bannard. 2002. Statistical techniques for auto-
matically inferring the semantics of verb-particle
constructions. Technical Report WP-2002-06,
University of Edinburgh, School of Informatics.
http://lingo.stanford.edu/pubs/WP-2002-06.pdf.
D. Blaheta and M. Johnson. 2001. Unsupervised
learning of multi-word verbs. In Proceedings of
the ACL Workshop on Collocations, pages 54-60,
Toulouse, France.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC), pages 1499-
1504, Las Palmas, Canary Islands, Spain.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal.
In Proceedings of the International Conference on
Language Resources and Evaluation, pages 447-
454.
K.W. Church and P. Hanks. 1990. Word associa-
tion norms, mutual information and lexicography.
Computational Linguistics, 19(2):263-312.
T. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61-74.
R.W. Gibbs and N. P. Nayak. 1989. Psycholinguis-
tic studies on the syntactic behaviour of idioms.
Cognitive Psychology, 21:100-38.
C. Grover, J. Carroll, and T. Briscoe. 1993. The
Alvey Natural Language Tools grammar. Techni-
cal Report 284, Computer Laboratory, University
of Cambridge.
B. Krenn and S. Evert. 2001. Can we do better than
frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Workshop
on Collocations, pages 39-46, Toulouse, France.
D. Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL
98, Montreal, Canada.
D. Lin. 1998b. Dependency-based evalu-
ation of MINIPAR at LREC. In Pro-
ceedings of the Workshop on the Evalua-
tion of Parsing Systems, Granada, Spain.
http://www.cs.ualberta.ca/lindek/minipar.htm.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
pages 317-324, Univeristy of Maryland, College
Park, Maryland.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1995. The Penn Treebank: annotating
predicate argument structure. Technical report,
University of Pennsylvania. Distributed on The
Penn Treebank 2 CD-ROM by the Linguistic Data
Consortium.
D. McCarthy, J. Carroll, and J. Preiss. 2001. Dis-
ambiguating noun and verb senses using automat-
ically acquired selectional preferences. In Proceed-
ings of the SENSEVAL-2 workshop, pages 119-122.
G. Miller, R. Beckwith, C. Fellbaum, D Gross,
and K. Miller, 1993. Introduction to
WordNet: an On-Line Lexical Database.
ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.
G. Nunberg, I. A. Sag, and T. Wasow. 1994. Idioms.
Language, 70:491-538.
D. Pearce. 2002. A comparative evaluation of collo-
cation extraction techniques. In Proceedings of the
Third International Conference on Language Re-
sources and Evaluation (LREC), pages 1530-1536,
Las Palmas, Canary Islands, Spain.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A
pain in the neck for NLP. In Proceedings of the
Third International Conference on Intelligent Text
Processing and Computational Linguistics (CI-
CLING 2002), pages 1-15, Mexico City, Mexico.
P. Schone and D. Jurafsky. 2001. Is knowledge-free
induction of multiword unit dictionary headwords
a solved problem? In Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Language
Processing, pages 100-108, Hong Kong.
S. Siegel and N. John Castellan, editors. 1988. Non-
Parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New York.
J. L. Tseng. 2000. The Representation and Selec-
tion of Prepositions. Ph.D. thesis, University of
Edinburgh.
A. Villavicencio and A. Copestake. 2002. Verb-
particle constructions in a computational gram-
mar. In Proceedings of the 9th International Con-
ference on Head-Driven Phrase Structure Gram-
mar (HPSG-2002), Seoul, South Korea.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787274">
<title confidence="0.992744">Detecting a Continuum of Compositionality in Phrasal Verbs.</title>
<author confidence="0.882357">McCarthy Keller</author>
<affiliation confidence="0.999452">University of</affiliation>
<address confidence="0.914528">Brighton BN1 9QH,</address>
<email confidence="0.998563">dianam,billk,johncaOcogs.susx.ac.uk</email>
<abstract confidence="0.998804888888889">We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>A Villavicencio</author>
</authors>
<title>Extracting the unextractable: A case study on verbparticles.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>98--104</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1366" citStr="Baldwin and Villavicencio, 2002" startWordPosition="192" endWordPosition="196">sitionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus. 1 Introduction Many people are working on acquisition of multiword expressions, although terminology varies. In this paper, we are interested in lexicalised expressions (Sag et al., 2002) where special interpretation is required because of some degree of non-compositionality or semantic opacity. We are specifically concerned with what are commonly referred to as phrasal verbs, or verb and particle constructions (Baldwin and Villavicencio, 2002). As well as having idiosyncratic semantics, phrasals also display specific syntactic behaviour such as permitting particle movement when used in the transitive; for example: Jo ate up her food Jo ate her food up We are interested in phrasal verbs because we want to acquire predicate selectional preferences for word sense disambiguation (McCarthy et al., 2001). When acquiring such lexical information for a verb it is important to know when there is a special interpretation required for the verb and particle combination so that these combinations are handled separately from the simplex case. Wh</context>
<context position="5259" citStr="Baldwin and Villavicencio (2002)" startWordPosition="817" endWordPosition="820">how the correlations of our measures with the gold-standard, and compare these to some statistics commonly used for identifying compositional multiwords. In section 7 we analyse our findings, and conclude (section 8) with directions for future work. 2 Related Research There has been a lot of recent work on extraction of multiwords from corpora we focus specifically on work involving multiword verbs, and detecting compositionality of multiwords. 2.1 Multiword Verb Extraction There have been a number of methods proposed in the literature for extracting multiword verb constructions from corpora. Baldwin and Villavicencio (2002) demonstrated that combining syntactic evidence using automatic PoS taggers and statistical chunkers, and feeding evidence from a number of tokens into a memory based-learner gave high precision and recall, using marked up WSJ text to gauge precision, and phrasals listed in the Alvey Natural Language Tools (ANLT) (Grover et al., 1993) attested in the same corpus for recall. No distinction on opaqueness of the verb and particle constructions was made. Blaheta and Johnson (2001) used log-linear models to extract English multiword verbs involving verb and particle constructions from parsed data; </context>
<context position="11334" citStr="Baldwin and Villavicencio, 2002" startWordPosition="1739" endWordPosition="1743">t different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked to mark &amp;quot;typical&amp;quot; multiwords, or collocations. The intuitions behind what is typical vary, and likewise association scores vary in their ability to partition the set depending on the notion of &amp;quot;typicality&amp;quot; employed by the annotators. Researchers also sometimes show how well the results accord with the contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of whether a putative phrasal candidate is indeed a genuine multiword or not (although i</context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>T. Baldwin and A. Villavicencio. 2002. Extracting the unextractable: A case study on verbparticles. In Proceedings of the Sixth Conference on Computational Natural Language Learning (CoNLL 2002), pages 98-104, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>C Bannard</author>
<author>T Tanaka</author>
<author>D Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on</booktitle>
<contexts>
<context position="10224" citStr="Baldwin et al. (2003)" startWordPosition="1567" endWordPosition="1570">ecting compositionality using resources acquired from distributional data. They use these on 40 candidates on 4 separate tasks which aim to determine whether i) the item is compositional, ii) one component word contributes its meaning iii) the verb contributes its meaning iv) the particle contributes its meaning. The classifications on each of these tasks according to these methods are contrasted with a gold standard classification from 26 judges on the same data. The methods exceed the mean agreement of the annotators in some cases, particularly as regards the contribution from the particle. Baldwin et al. (2003) are also exploring empirical models of compositionality using LSA with nounnoun compounds and verb-particle constructions. In their study, they compare the similarities of the component words with WordNet based similarity scores and demonstrate a moderate correlation, lower for noun-noun compounds. We are also exploring the relation between a verb and verb and particle combination (we use the term phrasal verb) using distributional techniques, but our evaluation is somewhat different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; P</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL Workshop on multiword expressions: analysis, acquisition and treatment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>T Baldwin</author>
<author>A Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verbparticles.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on</booktitle>
<contexts>
<context position="8843" citStr="Bannard et al. (2003)" startWordPosition="1352" endWordPosition="1355"> by the log-likelihood statistic. Using the parser yielded much better results than just a simple window for co-occurrence relationships. Lin proposed that if there is a multiword obtained by substitution of either the head or modifier in the multiword with a near neighbour, then the mutual information of this and the original multiword must be significantly different for the original multiword to be considered non-compositional. He evaluated this manually on a sample. As well as finding noncompositional multiwords, there were also a higher proportion of parser errors that met these criteria. Bannard et al. (2003) are investigating compositionality by looking at the contribution of the verb, and the particle to the semantics of the verb and particle combination; this follows on from Bannard&apos;s earlier work (2002) where he showed that compositionality judgements correlate with human judgements of similarity between the head verb and the verb and particle combination. Bannard et al. (2003) point out that Lin&apos;s method of using substitution of component words in a multiword with semantic neighbours is a good indication of productivity, but not necessarily of compositionality, since an institutionalised non-</context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>C. Bannard, T. Baldwin, and A. Lascarides. 2003. A statistical approach to the semantics of verbparticles. In Proceedings of the ACL Workshop on multiword expressions: analysis, acquisition and treatment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
</authors>
<title>Statistical techniques for automatically inferring the semantics of verb-particle constructions.</title>
<date>2002</date>
<tech>Technical Report WP-2002-06,</tech>
<pages>2002--06</pages>
<institution>University of Edinburgh, School of Informatics.</institution>
<marker>Bannard, 2002</marker>
<rawString>C. Bannard. 2002. Statistical techniques for automatically inferring the semantics of verb-particle constructions. Technical Report WP-2002-06, University of Edinburgh, School of Informatics. http://lingo.stanford.edu/pubs/WP-2002-06.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>M Johnson</author>
</authors>
<title>Unsupervised learning of multi-word verbs.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL Workshop on Collocations,</booktitle>
<pages>54--60</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="5740" citStr="Blaheta and Johnson (2001)" startWordPosition="892" endWordPosition="895">ve been a number of methods proposed in the literature for extracting multiword verb constructions from corpora. Baldwin and Villavicencio (2002) demonstrated that combining syntactic evidence using automatic PoS taggers and statistical chunkers, and feeding evidence from a number of tokens into a memory based-learner gave high precision and recall, using marked up WSJ text to gauge precision, and phrasals listed in the Alvey Natural Language Tools (ANLT) (Grover et al., 1993) attested in the same corpus for recall. No distinction on opaqueness of the verb and particle constructions was made. Blaheta and Johnson (2001) used log-linear models to extract English multiword verbs involving verb and particle constructions from parsed data; these include phrasal and prepositional verbs. &apos;Prepositional verbs also have some degree of idiosyncratic semantic interpretation, but the particle functions as a preposition and selects for the following noun phrase. There is therefore no particle movement e.g. *she referred the problem to. Krenn and Evert (2001) investigated German support verb constructions (identifiable on grammatical grounds) and figurative expressions (having idiomatic interpretations). In their experim</context>
<context position="11277" citStr="Blaheta and Johnson, 2001" startWordPosition="1731" endWordPosition="1734">butional techniques, but our evaluation is somewhat different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked to mark &amp;quot;typical&amp;quot; multiwords, or collocations. The intuitions behind what is typical vary, and likewise association scores vary in their ability to partition the set depending on the notion of &amp;quot;typicality&amp;quot; employed by the annotators. Researchers also sometimes show how well the results accord with the contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of whether a putative phrasal c</context>
</contexts>
<marker>Blaheta, Johnson, 2001</marker>
<rawString>D. Blaheta and M. Johnson. 2001. Unsupervised learning of multi-word verbs. In Proceedings of the ACL Workshop on Collocations, pages 54-60, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="13124" citStr="Briscoe and Carroll, 2002" startWordPosition="2036" endWordPosition="2039">rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources. 3 Parser Output For these experiments we use data from the ninety million words of the written portion of the British National Corpus parsed with the RASP parser (Briscoe and Carroll, 2002). The output of the parser is a set of grammatical relations (Carroll et al., 1998) specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. The parser uses information from ANLT such as phrasals in its dictionary. This makes it more likely to spot phrasal constructions from this list. We have already looked at recognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a g</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>E. Briscoe and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pages 1499-1504, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="13207" citStr="Carroll et al., 1998" startWordPosition="2051" endWordPosition="2054">for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources. 3 Parser Output For these experiments we use data from the ninety million words of the written portion of the British National Corpus parsed with the RASP parser (Briscoe and Carroll, 2002). The output of the parser is a set of grammatical relations (Carroll et al., 1998) specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. The parser uses information from ANLT such as phrasals in its dictionary. This makes it more likely to spot phrasal constructions from this list. We have already looked at recognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard. The results for identifying verb and particle tokens are reported in </context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>J. Carroll, E. Briscoe, and A. Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the International Conference on Language Resources and Evaluation, pages 447-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 19(2):263-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Gibbs</author>
<author>N P Nayak</author>
</authors>
<title>Psycholinguistic studies on the syntactic behaviour of idioms.</title>
<date>1989</date>
<journal>Cognitive Psychology,</journal>
<pages>21--100</pages>
<contexts>
<context position="12322" citStr="Gibbs and Nayak, 1989" startWordPosition="1910" endWordPosition="1913"> results accord with the contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of whether a putative phrasal candidate is indeed a genuine multiword or not (although it is more likely that those with low compositionality are likely to be) but we use empirical methods to gauge the position of a candidate on a continuum between the fully opaque idiom and transparent compositional phrases. Variability of idioms on a scale of compositionality has been discussed by Nunberg et al. (1994) and in the psycholinguistics literature, see (Gibbs and Nayak, 1989). Tseng (2000) also advocates use of a spectrum when considering the semantics of prepositions. We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-mad</context>
</contexts>
<marker>Gibbs, Nayak, 1989</marker>
<rawString>R.W. Gibbs and N. P. Nayak. 1989. Psycholinguistic studies on the syntactic behaviour of idioms. Cognitive Psychology, 21:100-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>J Carroll</author>
<author>T Briscoe</author>
</authors>
<title>The Alvey Natural Language Tools grammar.</title>
<date>1993</date>
<tech>Technical Report 284,</tech>
<institution>Computer Laboratory, University of Cambridge.</institution>
<contexts>
<context position="5595" citStr="Grover et al., 1993" startWordPosition="869" endWordPosition="872"> focus specifically on work involving multiword verbs, and detecting compositionality of multiwords. 2.1 Multiword Verb Extraction There have been a number of methods proposed in the literature for extracting multiword verb constructions from corpora. Baldwin and Villavicencio (2002) demonstrated that combining syntactic evidence using automatic PoS taggers and statistical chunkers, and feeding evidence from a number of tokens into a memory based-learner gave high precision and recall, using marked up WSJ text to gauge precision, and phrasals listed in the Alvey Natural Language Tools (ANLT) (Grover et al., 1993) attested in the same corpus for recall. No distinction on opaqueness of the verb and particle constructions was made. Blaheta and Johnson (2001) used log-linear models to extract English multiword verbs involving verb and particle constructions from parsed data; these include phrasal and prepositional verbs. &apos;Prepositional verbs also have some degree of idiosyncratic semantic interpretation, but the particle functions as a preposition and selects for the following noun phrase. There is therefore no particle movement e.g. *she referred the problem to. Krenn and Evert (2001) investigated German</context>
<context position="12821" citStr="Grover et al., 1993" startWordPosition="1987" endWordPosition="1990">ionality has been discussed by Nunberg et al. (1994) and in the psycholinguistics literature, see (Gibbs and Nayak, 1989). Tseng (2000) also advocates use of a spectrum when considering the semantics of prepositions. We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources. 3 Parser Output For these experiments we use data from the ninety million words of the written portion of the British National Corpus parsed with the RASP parser (Briscoe and Carroll, 2002). The output of the parser is a set of grammatical relations (Carroll et al., 1998) specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. The parser uses information from ANLT such as phr</context>
</contexts>
<marker>Grover, Carroll, Briscoe, 1993</marker>
<rawString>C. Grover, J. Carroll, and T. Briscoe. 1993. The Alvey Natural Language Tools grammar. Technical Report 284, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krenn</author>
<author>S Evert</author>
</authors>
<title>Can we do better than frequency? A case study on extracting PP-verb collocations.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL Workshop on Collocations,</booktitle>
<pages>39--46</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="6175" citStr="Krenn and Evert (2001)" startWordPosition="956" endWordPosition="959">anguage Tools (ANLT) (Grover et al., 1993) attested in the same corpus for recall. No distinction on opaqueness of the verb and particle constructions was made. Blaheta and Johnson (2001) used log-linear models to extract English multiword verbs involving verb and particle constructions from parsed data; these include phrasal and prepositional verbs. &apos;Prepositional verbs also have some degree of idiosyncratic semantic interpretation, but the particle functions as a preposition and selects for the following noun phrase. There is therefore no particle movement e.g. *she referred the problem to. Krenn and Evert (2001) investigated German support verb constructions (identifiable on grammatical grounds) and figurative expressions (having idiomatic interpretations). In their experiments, true positives were typically defined as such according to the annotator scanning the list. Krenn and Evert found that different statistics are suited to different types of collocation - there is no easy route for collocation extraction. Moreover, they found that a simple co-occurrence frequency fares comparably, if not better, than most statistical tests of significance. 2.2 Composition.ality of Multiwords Most people resear</context>
<context position="10821" citStr="Krenn and Evert, 2001" startWordPosition="1654" endWordPosition="1657">e. Baldwin et al. (2003) are also exploring empirical models of compositionality using LSA with nounnoun compounds and verb-particle constructions. In their study, they compare the similarities of the component words with WordNet based similarity scores and demonstrate a moderate correlation, lower for noun-noun compounds. We are also exploring the relation between a verb and verb and particle combination (we use the term phrasal verb) using distributional techniques, but our evaluation is somewhat different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked</context>
</contexts>
<marker>Krenn, Evert, 2001</marker>
<rawString>B. Krenn and S. Evert. 2001. Can we do better than frequency? A case study on extracting PP-verb collocations. In Proceedings of the ACL Workshop on Collocations, pages 39-46, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="8078" citStr="Lin, 1998" startWordPosition="1239" endWordPosition="1240">candidates compared to measures for the words that the multiword contains but this failed to improve performance, using WordNet and other machine readable resources as gold-standards for evaluation. There was some success though in using latent semantic analysis (LsA) models to identify multiwords by the fact that the component words are typically non-substitutable, but they felt that much of what is captured by this is already handled by the statistics that they employ. Lin (1999) had already done something similar to the substitutability experiments using the method he had proposed earlier (Lin, 1998a) for automatic thesaurus construction. He identified general multiwords involving several open-class words output from his parser and filtered by the log-likelihood statistic. Using the parser yielded much better results than just a simple window for co-occurrence relationships. Lin proposed that if there is a multiword obtained by substitution of either the head or modifier in the multiword with a near neighbour, then the mutual information of this and the original multiword must be significantly different for the original multiword to be considered non-compositional. He evaluated this manu</context>
<context position="14156" citStr="Lin, 1998" startWordPosition="2212" endWordPosition="2213">ognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard. The results for identifying verb and particle tokens are reported in table 1, both with and without the ANLT phrasal list (ANLT Precision Recall MINIPAR 78.9 44.1 RASP (without ANLT phr) 87.6 49.4 RASP (with ANLT phr) 92.6 64.2 Table 1: Identification of verb and particle attachments in WSJ data phr). We also give results for comparison obtained on the same data for another wide coverage parser, (MINIPAR (Lin, 1998b)). 2 In the RASP parser grammatical relation output we identify phrasal verbs as being a verb modified by a particle (tagged RP) under the ncmod (non-clausal modifier) relation. It is quite possible that some particle tags have been given erroneously and that some genuine particles are not recognised as such by the parser, or are not attached to the verb by the parser. We only look at tokens in isolation and therefore do not collate evidence to look for syntactic evidence of particle movement as Baldwin and Villavicencio do. This would be a good way to improve phrasal extraction accuracy, pa</context>
<context position="17040" citStr="Lin (1998" startWordPosition="2721" endWordPosition="2722">e average rank for the ith item and k is the number of raters. w = 12 Ein_o — 3n(n, + 1)2 Ti The second term in the denominator includes a correction for ties where: Ti = (t — (2) where ti is the number of tied ranks in the ith grouping of ranks. The value k(n —1)W is approximately distributed as X2 with n — 1 degrees of freedom. We obtained a W score of 0.594 which gives a X2 score of 196.30 for 110 degrees of freedom which is highly significant (probability of this value &lt;= 0.000001). 5 Detecting Compositionality 5.1 Using an Automatically Constructed Thesaurus: Using the method proposed by Lin (1998a) we produced a thesaurus with 500 nearest neighbours for the set of phrasal verbs as described above. Tuples of the form &lt;verb, argument head, grammatical relation&gt; from the parsed BNC data were used for this purpose where the verb was the multiword phrasal and the grammatical relations used were subjects and direct objects. We did likewise for the simplex verbs contained within the phrasals (e.g. blow from blow up). We investigated various measures which compare the nearest neighbours of the phrasal verb to the neighbours of the corresponding simplex verb. We also tried various measures on </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR at LREC.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada, Spain. http://www.cs.ualberta.ca/lindek/minipar.htm.</location>
<contexts>
<context position="8078" citStr="Lin, 1998" startWordPosition="1239" endWordPosition="1240">candidates compared to measures for the words that the multiword contains but this failed to improve performance, using WordNet and other machine readable resources as gold-standards for evaluation. There was some success though in using latent semantic analysis (LsA) models to identify multiwords by the fact that the component words are typically non-substitutable, but they felt that much of what is captured by this is already handled by the statistics that they employ. Lin (1999) had already done something similar to the substitutability experiments using the method he had proposed earlier (Lin, 1998a) for automatic thesaurus construction. He identified general multiwords involving several open-class words output from his parser and filtered by the log-likelihood statistic. Using the parser yielded much better results than just a simple window for co-occurrence relationships. Lin proposed that if there is a multiword obtained by substitution of either the head or modifier in the multiword with a near neighbour, then the mutual information of this and the original multiword must be significantly different for the original multiword to be considered non-compositional. He evaluated this manu</context>
<context position="14156" citStr="Lin, 1998" startWordPosition="2212" endWordPosition="2213">ognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard. The results for identifying verb and particle tokens are reported in table 1, both with and without the ANLT phrasal list (ANLT Precision Recall MINIPAR 78.9 44.1 RASP (without ANLT phr) 87.6 49.4 RASP (with ANLT phr) 92.6 64.2 Table 1: Identification of verb and particle attachments in WSJ data phr). We also give results for comparison obtained on the same data for another wide coverage parser, (MINIPAR (Lin, 1998b)). 2 In the RASP parser grammatical relation output we identify phrasal verbs as being a verb modified by a particle (tagged RP) under the ncmod (non-clausal modifier) relation. It is quite possible that some particle tags have been given erroneously and that some genuine particles are not recognised as such by the parser, or are not attached to the verb by the parser. We only look at tokens in isolation and therefore do not collate evidence to look for syntactic evidence of particle movement as Baldwin and Villavicencio do. This would be a good way to improve phrasal extraction accuracy, pa</context>
<context position="17040" citStr="Lin (1998" startWordPosition="2721" endWordPosition="2722">e average rank for the ith item and k is the number of raters. w = 12 Ein_o — 3n(n, + 1)2 Ti The second term in the denominator includes a correction for ties where: Ti = (t — (2) where ti is the number of tied ranks in the ith grouping of ranks. The value k(n —1)W is approximately distributed as X2 with n — 1 degrees of freedom. We obtained a W score of 0.594 which gives a X2 score of 196.30 for 110 degrees of freedom which is highly significant (probability of this value &lt;= 0.000001). 5 Detecting Compositionality 5.1 Using an Automatically Constructed Thesaurus: Using the method proposed by Lin (1998a) we produced a thesaurus with 500 nearest neighbours for the set of phrasal verbs as described above. Tuples of the form &lt;verb, argument head, grammatical relation&gt; from the parsed BNC data were used for this purpose where the verb was the multiword phrasal and the grammatical relations used were subjects and direct objects. We did likewise for the simplex verbs contained within the phrasals (e.g. blow from blow up). We investigated various measures which compare the nearest neighbours of the phrasal verb to the neighbours of the corresponding simplex verb. We also tried various measures on </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Dependency-based evaluation of MINIPAR at LREC. In Proceedings of the Workshop on the Evaluation of Parsing Systems, Granada, Spain. http://www.cs.ualberta.ca/lindek/minipar.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-99,</booktitle>
<pages>317--324</pages>
<institution>Univeristy of Maryland, College Park,</institution>
<location>Maryland.</location>
<contexts>
<context position="7283" citStr="Lin (1999)" startWordPosition="1114" endWordPosition="1115">r, than most statistical tests of significance. 2.2 Composition.ality of Multiwords Most people researching into multiwords assume some degree of non-compositionality. Blaheta and Johnson took human judgements on phrasality, opaqueness (a dichotomous scale) and a subjective judgement of relatedness (on a scale between 1 and 5). They showed that the opaqueness judgements correlated with the relatedness (good collocation) judgement. Also, those constructions judged to be phrasals tended to have higher ranks (higher opaqueness and relatedness) than prepositional verb particle constructions. Both Lin (1999) and Schone and Jurafsky (2001) have used distributional similarity to detect compositionality in multiwords. Schone and Jurafsky used measures on the vectors representing the multiword candidates compared to measures for the words that the multiword contains but this failed to improve performance, using WordNet and other machine readable resources as gold-standards for evaluation. There was some success though in using latent semantic analysis (LsA) models to identify multiwords by the fact that the component words are typically non-substitutable, but they felt that much of what is captured b</context>
<context position="11250" citStr="Lin, 1999" startWordPosition="1729" endWordPosition="1730">sing distributional techniques, but our evaluation is somewhat different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked to mark &amp;quot;typical&amp;quot; multiwords, or collocations. The intuitions behind what is typical vary, and likewise association scores vary in their ability to partition the set depending on the notion of &amp;quot;typicality&amp;quot; employed by the annotators. Researchers also sometimes show how well the results accord with the contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of w</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>D. Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of ACL-99, pages 317-324, Univeristy of Maryland, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>M A Marcinkiewicz</author>
<author>R MacIntyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn Treebank: annotating predicate argument structure.</title>
<date>1995</date>
<booktitle>Distributed on The Penn Treebank 2 CD-ROM by the Linguistic Data Consortium.</booktitle>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="13717" citStr="Marcus et al., 1995" startWordPosition="2134" endWordPosition="2137">(Briscoe and Carroll, 2002). The output of the parser is a set of grammatical relations (Carroll et al., 1998) specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. The parser uses information from ANLT such as phrasals in its dictionary. This makes it more likely to spot phrasal constructions from this list. We have already looked at recognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard. The results for identifying verb and particle tokens are reported in table 1, both with and without the ANLT phrasal list (ANLT Precision Recall MINIPAR 78.9 44.1 RASP (without ANLT phr) 87.6 49.4 RASP (with ANLT phr) 92.6 64.2 Table 1: Identification of verb and particle attachments in WSJ data phr). We also give results for comparison obtained on the same data for another wide coverage parser, (MINIPAR (Lin, 1998b)). 2 In the RASP parser grammatical relation output we identify phrasal verbs as being a verb modified by a particle (tagged RP) under the ncmod (non-clausal m</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1995</marker>
<rawString>M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger. 1995. The Penn Treebank: annotating predicate argument structure. Technical report, University of Pennsylvania. Distributed on The Penn Treebank 2 CD-ROM by the Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>J Carroll</author>
<author>J Preiss</author>
</authors>
<title>Disambiguating noun and verb senses using automatically acquired selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of the SENSEVAL-2 workshop,</booktitle>
<pages>119--122</pages>
<contexts>
<context position="1728" citStr="McCarthy et al., 2001" startWordPosition="251" endWordPosition="254">where special interpretation is required because of some degree of non-compositionality or semantic opacity. We are specifically concerned with what are commonly referred to as phrasal verbs, or verb and particle constructions (Baldwin and Villavicencio, 2002). As well as having idiosyncratic semantics, phrasals also display specific syntactic behaviour such as permitting particle movement when used in the transitive; for example: Jo ate up her food Jo ate her food up We are interested in phrasal verbs because we want to acquire predicate selectional preferences for word sense disambiguation (McCarthy et al., 2001). When acquiring such lexical information for a verb it is important to know when there is a special interpretation required for the verb and particle combination so that these combinations are handled separately from the simplex case. Whilst it is possible to put every single occurrence of a verb and particle combination into a lexicon this is not desirable. One wants to achieve generalisation and avoid redundancy, only storing details which cannot be created from what is already there. Not every verb modified by a particle may be a genuine multiword unit, but may instead be a fully compositi</context>
</contexts>
<marker>McCarthy, Carroll, Preiss, 2001</marker>
<rawString>D. McCarthy, J. Carroll, and J. Preiss. 2001. Disambiguating noun and verb senses using automatically acquired selectional preferences. In Proceedings of the SENSEVAL-2 workshop, pages 119-122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: an On-Line Lexical Database.</title>
<date>1993</date>
<contexts>
<context position="12779" citStr="Miller et al., 1993" startWordPosition="1979" endWordPosition="1982">riability of idioms on a scale of compositionality has been discussed by Nunberg et al. (1994) and in the psycholinguistics literature, see (Gibbs and Nayak, 1989). Tseng (2000) also advocates use of a spectrum when considering the semantics of prepositions. We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources. 3 Parser Output For these experiments we use data from the ninety million words of the written portion of the British National Corpus parsed with the RASP parser (Briscoe and Carroll, 2002). The output of the parser is a set of grammatical relations (Carroll et al., 1998) specifying the syntactic dependency between each head and its dependent(s), read off from the phrase structure tree that is returned from the disambiguation phase. The par</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>G. Miller, R. Beckwith, C. Fellbaum, D Gross, and K. Miller, 1993. Introduction to WordNet: an On-Line Lexical Database. ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
<author>I A Sag</author>
<author>T Wasow</author>
</authors>
<date>1994</date>
<journal>Idioms. Language,</journal>
<pages>70--491</pages>
<contexts>
<context position="12253" citStr="Nunberg et al. (1994)" startWordPosition="1899" endWordPosition="1902">oyed by the annotators. Researchers also sometimes show how well the results accord with the contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of whether a putative phrasal candidate is indeed a genuine multiword or not (although it is more likely that those with low compositionality are likely to be) but we use empirical methods to gauge the position of a candidate on a continuum between the fully opaque idiom and transparent compositional phrases. Variability of idioms on a scale of compositionality has been discussed by Nunberg et al. (1994) and in the psycholinguistics literature, see (Gibbs and Nayak, 1989). Tseng (2000) also advocates use of a spectrum when considering the semantics of prepositions. We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compos</context>
</contexts>
<marker>Nunberg, Sag, Wasow, 1994</marker>
<rawString>G. Nunberg, I. A. Sag, and T. Wasow. 1994. Idioms. Language, 70:491-538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pearce</author>
</authors>
<title>A comparative evaluation of collocation extraction techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1530--1536</pages>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="10836" citStr="Pearce, 2002" startWordPosition="1658" endWordPosition="1659">) are also exploring empirical models of compositionality using LSA with nounnoun compounds and verb-particle constructions. In their study, they compare the similarities of the component words with WordNet based similarity scores and demonstrate a moderate correlation, lower for noun-noun compounds. We are also exploring the relation between a verb and verb and particle combination (we use the term phrasal verb) using distributional techniques, but our evaluation is somewhat different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked to mark &amp;quot;typic</context>
</contexts>
<marker>Pearce, 2002</marker>
<rawString>D. Pearce. 2002. A comparative evaluation of collocation extraction techniques. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pages 1530-1536, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics (CICLING</booktitle>
<pages>1--15</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="1105" citStr="Sag et al., 2002" startWordPosition="156" endWordPosition="159">he nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus. 1 Introduction Many people are working on acquisition of multiword expressions, although terminology varies. In this paper, we are interested in lexicalised expressions (Sag et al., 2002) where special interpretation is required because of some degree of non-compositionality or semantic opacity. We are specifically concerned with what are commonly referred to as phrasal verbs, or verb and particle constructions (Baldwin and Villavicencio, 2002). As well as having idiosyncratic semantics, phrasals also display specific syntactic behaviour such as permitting particle movement when used in the transitive; for example: Jo ate up her food Jo ate her food up We are interested in phrasal verbs because we want to acquire predicate selectional preferences for word sense disambiguation </context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics (CICLING 2002), pages 1-15, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem?</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>100--108</pages>
<location>Hong Kong.</location>
<contexts>
<context position="7314" citStr="Schone and Jurafsky (2001)" startWordPosition="1117" endWordPosition="1120">atistical tests of significance. 2.2 Composition.ality of Multiwords Most people researching into multiwords assume some degree of non-compositionality. Blaheta and Johnson took human judgements on phrasality, opaqueness (a dichotomous scale) and a subjective judgement of relatedness (on a scale between 1 and 5). They showed that the opaqueness judgements correlated with the relatedness (good collocation) judgement. Also, those constructions judged to be phrasals tended to have higher ranks (higher opaqueness and relatedness) than prepositional verb particle constructions. Both Lin (1999) and Schone and Jurafsky (2001) have used distributional similarity to detect compositionality in multiwords. Schone and Jurafsky used measures on the vectors representing the multiword candidates compared to measures for the words that the multiword contains but this failed to improve performance, using WordNet and other machine readable resources as gold-standards for evaluation. There was some success though in using latent semantic analysis (LsA) models to identify multiwords by the fact that the component words are typically non-substitutable, but they felt that much of what is captured by this is already handled by th</context>
<context position="10900" citStr="Schone and Jurafsky, 2001" startWordPosition="1667" endWordPosition="1670">nality using LSA with nounnoun compounds and verb-particle constructions. In their study, they compare the similarities of the component words with WordNet based similarity scores and demonstrate a moderate correlation, lower for noun-noun compounds. We are also exploring the relation between a verb and verb and particle combination (we use the term phrasal verb) using distributional techniques, but our evaluation is somewhat different. 2.3 Evaluation Evaluation of collocation extraction is a notoriously thorny problem (Krenn and Evert, 2001; Pearce, 2002). People do use MRDs such as WordNet (Schone and Jurafsky, 2001) even though they acknowledge that there will be omissions in these resources, and the phenomena in the resource may be rare or simply not attested in the particular corpus used for acquisition. Many researchers use manually annotated samples, where the judges make a binary decision on whether each candidate multiword is &amp;quot;genuine&amp;quot; or not (Lin, 1999; Blaheta and Johnson, 2001; Krenn and Evert, 2001; Baldwin and Villavicencio, 2002). As Krenn and Evert point out, there is low agreement between annotators who are asked to mark &amp;quot;typical&amp;quot; multiwords, or collocations. The intuitions behind what is t</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>P. Schone and D. Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 100-108, Hong Kong.</rawString>
</citation>
<citation valid="true">
<title>NonParametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<editor>S. Siegel and N. John Castellan, editors.</editor>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<marker>1988</marker>
<rawString>S. Siegel and N. John Castellan, editors. 1988. NonParametric Statistics for the Behavioral Sciences. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Tseng</author>
</authors>
<title>The Representation and Selection of Prepositions.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="12336" citStr="Tseng (2000)" startWordPosition="1914" endWordPosition="1915"> contents of MRDs, even though these cannot be taken as definitive. In this study we are less interested in the dichotomy of whether a putative phrasal candidate is indeed a genuine multiword or not (although it is more likely that those with low compositionality are likely to be) but we use empirical methods to gauge the position of a candidate on a continuum between the fully opaque idiom and transparent compositional phrases. Variability of idioms on a scale of compositionality has been discussed by Nunberg et al. (1994) and in the psycholinguistics literature, see (Gibbs and Nayak, 1989). Tseng (2000) also advocates use of a spectrum when considering the semantics of prepositions. We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this. We investigate the use of these ranked judgements for evaluating compositionality measures. We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources. 3</context>
</contexts>
<marker>Tseng, 2000</marker>
<rawString>J. L. Tseng. 2000. The Representation and Selection of Prepositions. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
<author>A Copestake</author>
</authors>
<title>Verbparticle constructions in a computational grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 9th International Conference on Head-Driven Phrase Structure Grammar (HPSG-2002),</booktitle>
<location>Seoul, South</location>
<contexts>
<context position="2616" citStr="Villavicencio and Copestake, 2002" startWordPosition="401" endWordPosition="404">ble to put every single occurrence of a verb and particle combination into a lexicon this is not desirable. One wants to achieve generalisation and avoid redundancy, only storing details which cannot be created from what is already there. Not every verb modified by a particle may be a genuine multiword unit, but may instead be a fully compositional verb modified by an adverbial e.g. fly up. Also very productive verb particle combinations such as those involving verbs of motion, which often occur with a particle e.g. up, such as wander, stroll, go etc... might be better handled in the grammar (Villavicencio and Copestake, 2002). Additionally, in lexical acquisition, and for word sense disambiguation, it is important that related senses of words are identified. For example, if the verb eat is closer in meaning to a phrasal construction eat up, compared to other simplex verbs with their phrasal constructions such as blow/blow up, then the lexicon should reflect that. Having a measure of compositionality should help in this. In this paper we are not concerned with evaluation of precision and recall of the extraction of phrasal verbs from a parser, although we have done some preliminary experiments in this direction on </context>
</contexts>
<marker>Villavicencio, Copestake, 2002</marker>
<rawString>A. Villavicencio and A. Copestake. 2002. Verbparticle constructions in a computational grammar. In Proceedings of the 9th International Conference on Head-Driven Phrase Structure Grammar (HPSG-2002), Seoul, South Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>