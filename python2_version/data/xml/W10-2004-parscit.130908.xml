<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000172">
<title confidence="0.994012">
HHMM Parsing with Limited Parallelism
</title>
<author confidence="0.998776">
Tim Miller
</author>
<affiliation confidence="0.980992666666667">
Department of Computer Science
and Engineering
University of Minnesota, Twin Cities
</affiliation>
<email confidence="0.996546">
tmill@cs.umn.edu
</email>
<author confidence="0.995563">
William Schuler
</author>
<affiliation confidence="0.916883">
University of Minnesota, Twin Cities
and The Ohio State University
</affiliation>
<email confidence="0.997891">
schuler@ling.ohio-state.edu
</email>
<sectionHeader confidence="0.995342" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9980474">
Hierarchical Hidden Markov Model
(HHMM) parsers have been proposed as
psycholinguistic models due to their broad
coverage within human-like working
memory limits (Schuler et al., 2008) and
ability to model human reading time
behavior according to various complexity
metrics (Wu et al., 2010). But HHMMs
have been evaluated previously only with
very wide beams of several thousand
parallel hypotheses, weakening claims to
the model’s efficiency and psychological
relevance. This paper examines the effects
of varying beam width on parsing accu-
racy and speed in this model, showing that
parsing accuracy degrades gracefully as
beam width decreases dramatically (to 2%
of the width used to achieve previous top
results), without sacrificing gains over a
baseline CKY parser.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860096153846">
Probabilistic parsers have been successful at ac-
curately estimating syntactic structure from free
text. Typically, these systems work by consider-
ing entire sentences (or utterances) at once, using
dynamic programming to obtain globally optimal
solutions from locally optimal sub-parses.
However, these methods usually do not attempt
to conform to human-like processing constraints,
e.g. leading to center embedding and garden path
effects (Chomsky and Miller, 1963; Bever, 1970).
For systems prioritizing accurate parsing perfor-
mance, there is little need to produce human-like
errors. But from a human modeling perspective,
the success of globally optimized whole-utterance
models raises the question of how humans can ac-
curately parse linguistic input without access to
this same global optimization. This question cre-
ates a niche in computational research for models
that are able to parse accurately while adhering as
closely as possible to human-like psycholinguistic
constraints.
Recent work on incremental parsers includes
work on Hierarchical Hidden Markov Model
(HHMM) parsers that operate in linear time by
maintaining a bounded store of incomplete con-
stituents (Schuler et al., 2008). Despite this seem-
ing limitation, corpus studies have shown that
through the use of grammar transforms, this parser
is able to cover nearly all sentences contained in
the Penn Treebank (Marcus et al., 1993) using a
small number of unconnected memory elements.
But this bounded-memory parsing comes at a
price. The HHMM parser obtains good coverage
within human-like memory bounds only by pur-
suing an ‘optionally arc-eager’ parsing strategy,
nondeterministically guessing which constituents
can be kept open for attachment (occupying an ac-
tive memory element), or closed for attachment
(freeing a memory element for subsequent con-
stituents). Although empirically determining the
number of parallel competing hypotheses used in
human sentence processing is difficult, previous
results in computational models have shown that
human-like behavior can be elicited at very low
levels of parallelism (Boston et al., 2008b; Brants
and Crocker, 2000), suggesting that large num-
bers of active hypotheses are not needed. Previ-
ously, the HHMM parser has only been evaluated
on large beam widths, leaving this aspect of its
psycholinguistic plausibility untested.
In this paper, the performance of an HHMM
parser will be evaluated in two experiments that
</bodyText>
<page confidence="0.98389">
27
</page>
<note confidence="0.9791365">
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27–35,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999670181818182">
vary the amount of parallelism allowed during
parsing, measuring the degree to which this de-
grades the system’s accuracy. In addition, the
evaluation will compare the HHMM parser to an
off-the-shelf probabilistic CKY parser to evaluate
the actual run time performance at various beam
widths. This serves two purposes, evaluating one
aspect of the plausibility of this parsing frame-
work as a psycholinguistic model, and evaluating
its potential utility as a tool for operating on un-
segmented text or speech.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999987483146067">
There are several criteria a parser must meet
in order to be plausible as a psycholinguistic
model of the human sentence-processing mecha-
nism (HSPM).
Incremental operation is perhaps the most obvi-
ous. The HSPM is able to process sentences in-
crementally, meaning that at each point in time of
processing input, it has some hypothesis of the in-
terpretation of that input, and each subsequent unit
of input serves to update that hypothesis.
The next criterion for psycholinguistic plausi-
bility is processing efficiency. The HSPM not
only operates incrementally, but in standard op-
eration it does not lag behind a speaker, even if,
for example, the speaker continues speaking at ex-
tended length without pause. Standard machine
approaches, such as chart parsers based on the
CKY algorithm, operate in worst-case cubic run
time on the length of input. Without knowing
where an utterance or sentence might end, such an
algorithm will take more time with each succes-
sive word and will eventually fall behind.
The third criterion is a reasonable limiting of
memory resources. This constraint means that the
HSPM, while possibly considering multiple hy-
potheses in parallel, is not limitlessly so, as evi-
denced by the existence of garden path sentences
(Bever, 1970; Lewis, 2000). If this were not the
case, garden-path sentences would not cause prob-
lems, as reaching the disambiguating word would
simply result in a change in the favored hypothe-
sis. In fact, garden path sentences typically cannot
be understood on a first pass and must be reread,
indicating that the correct analysis is attainable
and yet not present in the set of parallel hypotheses
of the first pass.
While parsers meeting these three criteria can
claim to not violate any psycholinguistic con-
straints, there has been much recent work in
testing psycholinguistically-motivated parsers to
make forward predictions about human sentence
processing, in order to provide positive evidence
for certain probabilistic parsing models as valid
psycholinguistic models of sentence processing.
This work has largely focused on correlating mea-
sures of parsing difficulty in computational models
with delays in reading time in human subjects.
Hale (2001) introduced the surprisal metric for
probabilistic parsers, which measures the log ra-
tio of the total probability mass at word t − 1
and word t. In other words, it measures how
much probability was lost in incorporating the
next word into the current hypotheses. Boston et
al. (2008a) show that surprisal is a significant pre-
dictor of reading time (as measured in self-paced
reading experiments) using a probabilistic depen-
dency parser. Roark et al. (2009) dissected parsing
difficulty metrics (including surprisal and entropy)
to separate out the effects of syntactic and lexical
difficulties, and showed that these new metrics are
strong predictors of reading difficulty.
Wu et al. (2010) evaluate the same Hierarchical
Hidden Markov Model parser used in this work in
terms of its ability to reproduce human-like results
for various complexity metrics, including some of
those mentioned above, and introduce a new met-
ric called embedding difference. This metric is
based on the idea of embedding depth, which is
the number of elements in the memory store re-
quired to hold a given hypothesis. Using more
memory elements corresponds to center embed-
ding in phrase structure trees, and presumably cor-
relates to some degree with complexity. Average
embedding for a time step is computed by com-
puting the weighted average number of required
memory elements (weighted by probability) for all
hypotheses on the beam. Embedding difference is
simply the change in this value when the next word
is encountered.
Outside of Wu et al., the most similar work
from a modeling perspective is an incremen-
tal parser implemented using Cascaded Hidden
Markov Models (CHMMs) (Crocker and Brants,
2000). This model is superficially similar to the
Hierarchical Hidden Markov Models described
below in that it relies on multiple levels of interde-
pendent HMMs to account for hierarchical struc-
ture in an incremental model. Crocker and Brants
use the system to parse ambiguous sentences (such
</bodyText>
<page confidence="0.997513">
28
</page>
<bodyText confidence="0.999952263157895">
as the athlete realized his goals were out of reach)
and examine the relative probabilities of two plau-
sible analyses at each time step. They then show
that the shifting of these two probabilities is con-
sistent with empirical evidence about how humans
perceive these sentences word by word.
However, as will be described below, the
HHMM has advantages over the CHMM from
a psycholinguistic modeling perspective. The
HHMM uses a limited memory store contain-
ing only four elements which is consistent with
many estimates of human short term memory lim-
its (Cowan, 2001; Miller, 1956). In addition to
modeling memory limits, the limited store acts as
a fixed-depth stack that ensures linear asymptotic
parsing time, and a grammar transform allows for
wide coverage of speech and newspaper corpora
within that limited memory store (Schuler et al.,
2010).
</bodyText>
<sectionHeader confidence="0.861932" genericHeader="method">
3 Hierarchical Hidden Markov Model
Parser
</sectionHeader>
<bodyText confidence="0.999890214285714">
Hidden Markov Models (HMMs) have long been
used to successfully model sequence data in which
there is a latent (hidden) variable at each time step
that generates the observed evidence at that time
step. These models are used for such applications
as part-of-speech tagging, and speech recognition.
Hierarchical Hidden Markov Models (HH-
MMs) are an extension of HMMs which can rep-
resent sequential data containing hierarchical rela-
tions. In HHMMs, complex hidden variables may
output evidence for several time steps in sequence.
This process may recurse, though a finite depth
is required to make any guarantees about perfor-
mance. Murphy and Paskin (2001) showed that
this model could be framed as a Dynamic Bayes
Network (DBN), so that inference is linear on the
length of the input sequence.
In the HHMM parser used here, the complex
hidden variables are syntactic states that gener-
ate sub-sequences of other syntactic states, even-
tually generating pre-terminals and words. This
section will describe how the trees must be trans-
formed, and then mapped to HHMM states. This
section will then continue with a formal definition
of an HHMM, followed by a description of how
this model can parse natural language, and finally
a discussion of what different aspects of the model
represent in terms of psycholinguistic modeling.
</bodyText>
<subsectionHeader confidence="0.994994">
3.1 Right-Corner Transform
</subsectionHeader>
<bodyText confidence="0.99989205">
In order to parse with an HHMM, phrase struc-
ture trees need to be mapped to a hierarchical se-
quence of states of nested HMMs. Since Mur-
phy and Paskin showed that the run time complex-
ity of the HHMM is exponential on the depth of
the nested HMMs, it is important to minimize the
depth of the model for optimal performance. In
order to do this, a tree transformation known as
a right-corner transform is applied to the phrase
structure trees comprising the training data, to
transform right-expanding sequences of complete
constituents into left-expanding sequences of in-
complete constituents Aη/Aµ, consisting of an in-
stance of an active constituent Aη lacking an in-
stance of an awaited constituent Aµ yet to be rec-
ognized. This transform can be defined as a syn-
chronous grammar that maps every context-free
rule expansion in a source tree (in Chomsky Nor-
mal Form) to a corresponding expansion in a right-
corner transformed tree:1
</bodyText>
<listItem confidence="0.988533538461539">
• Beginning case: the top of a right-expanding
sequence in an ordinary phrase structure tree
is mapped to the bottom of a left-expanding
sequence in a right-corner transformed tree:
(1)
• Middle case: each subsequent branch in
a right-expanding sequence of an ordinary
phrase structure tree is mapped to a branch in
a left-expanding sequence of the transformed
tree:
(2)
• Ending case: the bottom of a right-expanding
sequence in an ordinary phrase structure tree
</listItem>
<bodyText confidence="0.67794975">
1Here, rl and µ are tree node addresses, consisting of se-
quences of zeros, representing left branches, and ones, repre-
senting right branches, on a path from the root of the tree to
any given node.
</bodyText>
<figure confidence="0.99919716">
Aη·0
α
Aη·1
Q
Aη
Aη
Aη/Aη·1
Aη·0
Q
⇒
α
Aη
Aη
α Aη·µ
&apos;Y
Aη/Aη·µ·1
⇒
Aη·µ·0
Q
Aη·µ·1
&apos;Y
Aη·µ·0
Q
Aη/Aη·µ
α
</figure>
<page confidence="0.995322">
29
</page>
<bodyText confidence="0.993014">
HHMMs then factor the hidden state transition OA
into a reduce and shift phase (Equation 5), then
into a bounded set of depth-specific operations
(Equation 6):
</bodyText>
<equation confidence="0.710404">
�PΘA(st|st–1) = PΘR(rt|st–1)·PΘS(st|rt st–1)
</equation>
<figure confidence="0.987586681818182">
a) Aη
Aη0
Aη1
aη00
Aη010 Aη011
Aη01
Aη100 Aη101
Aη10
aη11
aη0100
aη1011
aη0111
aη1010
aη0110
aη1001
aη0101
aη1000
rt
b) Aη
Aη/Aη11
Aη/Aη1
Aη0
</figure>
<figureCaption confidence="0.784278">
Figure 1: Sample right-corner transform of
schematized tree before (a) and after (b) applica-
tion of transform.
is mapped to the top of a left-expanding se-
quence in a right-corner transformed tree:
The application of this transform is exemplified in
Figure 1.
</figureCaption>
<subsectionHeader confidence="0.998161">
3.2 Hierarchical Hidden Markov Models
</subsectionHeader>
<bodyText confidence="0.999936538461538">
Right-corner transformed trees are mapped to ran-
dom variables in a Hierarchical Hidden Markov
Model (Murphy and Paskin, 2001).
A Hierarchical Hidden Markov Model
(HHMM) is essentially a factored version of
a Hidden Markov Model (HMM), configured to
recognize bounded recursive structures (i.e. trees).
Like HMMs, HHMMs use Viterbi decoding to
obtain sequences of hidden states s1..T given
sequences of observations o1..T (words or audio
features), through independence assumptions
in a transition model OA and an observation
model OB (Baker, 1975; Jelinek et al., 1975):
</bodyText>
<equation confidence="0.968785375">
PΘA(st  |st−1) · PΘs(ot  |st)
(4)
PΘR,d(rdt  |rd+1
t sd t–1sd–1
t–1 )·
PΘS,d(sdt |rd+1
t rdt sd t–1sd–1
t )
</equation>
<listItem confidence="0.547681166666667">
which allow depth-specific variables to reduce
(through OR-Rdn,d), transition (OS-Trn,d), and ex-
pand (OS-Exp,d) like tape symbols in a pushdown
automaton with a bounded memory store, depend-
ing on whether the variable below has reduced
(rdt ∈RG) or not (rdt 6∈RG):2
</listItem>
<equation confidence="0.993154894736842">
PΘR,d(rd t  |rd+1
t sd t−1sd−1
t−1 )def =
if rt }R�G : Qrdt = r⊤11 ((7
if rt+1∈RG : PΘR-Rdn,d (rdt  |rt+sdt− 1 std−1) l )
PΘS,d(sd t  |rd+1
t rd t sd t−1sd−1
t
)def =
{ if rd+1
t 6∈RG, rdt 6∈RG : Qsdt = sdt−111
t )
t rdt sd t−1sd−1
if rd+1
t ∈RG, rdt 6∈RG : PΘS-Trn,d(sd t  |rd+1
t )
if rd+1
t ∈RG, rdt ∈RG : PΘS-Exp,d(sdt  |sd−1
(8)
</equation>
<bodyText confidence="0.970237363636364">
where s0t = s⊤ and rD+1
t = r⊥ for constants s⊤
(an incomplete root constituent), r⊥ (a complete
lexical constituent) and r⊤ (a null state resulting
from reduction failure) s.t. r⊥ ∈RG and r⊤ 6∈RG.
Right-corner transformed trees, as exemplified
in Figure 1(b), can then be aligned to HHMM
states as shown in Figure 2, and used to train an
HHMM as a parser.
Parsing with an HHMM simply involves pro-
cessing the input sequence, and estimating a most
likely hidden state sequence given this observed
input. Since the output is to be the best possible
parse, the Viterbi algorithm is used, which keeps
track of the highest probability state at each time
step, where the state is the store of incomplete syn-
tactic constituents being processed. State transi-
tions are computed using the models above, and
each state at each time step keeps a back pointer to
the state it most probably came from. Extracting
the highest probability parse requires extracting
2Here, [I is an indicator function: � 1 if φ is true, 0
</bodyText>
<figure confidence="0.976295833333333">
otherwise.
aη0100
Aη0/Aη0111
Aη0/Aη011
Aη0/Aη01 Aη010
00 Aη010/Aη0101
aη00
aη0101
aη0110
aη0111
�
aη1010
10
Aη10/Aη1011
Aη10/Aη101 010
Aη100
Aη100/Aη1001 001
aη1001
aη1000
aη1011 aη11
Aη10
Aη
α Aη·µ
aη·µ
Aη
⇒ Aη/Aη·µ
α
Aη·µ
aη·µ
(3)
</figure>
<equation confidence="0.82804047368421">
def
= argmax
s1..T
s1..T
T
H
t=1
def �=
r1..D
t
D
H
d=1
30
t=1 t=2 t=3 t=4 t=5 t=6 t=7
d=1
d=2
d=3
word
</equation>
<figureCaption confidence="0.9771305">
Figure 2: Mapping of schematized right-corner
tree into HHMM memory elements.
</figureCaption>
<bodyText confidence="0.998470473684211">
the most likely sequence, deterministically map-
ping that sequence back to a right-corner tree, and
reversing the right-corner transform to produce an
ordinary phrase structure tree.
Unfortunately exact inference is not tractable
with this model and dataset. The state space is
too large to manage for both space and time rea-
sons, and thus approximate inference is carried
out, through the use of a beam search. At each
time step, only the top N most probable hypoth-
esized states are maintained. Experiments de-
scribed in (Schuler, 2009) suggest that there does
not seem to be much lost in going from exact in-
ference using the CKY algorithm to a beam search
with a relatively large width. However, the op-
posite experiment, examining the effect of going
from a relatively wide beam to a very narrow beam
has not been thoroughly studied in this parsing ar-
chitecture.
</bodyText>
<sectionHeader confidence="0.974703" genericHeader="method">
4 Optionally Arc-eager Parsing
</sectionHeader>
<bodyText confidence="0.967185436619718">
The right-corner transform described in Sec-
tion 3.1 saves memory because it transforms any
right-expanding sequence with left-child subtrees
into a left-expanding sequence of incomplete con-
stituents, with the same sequence of subtrees as
right children. The left-branching sequences of
siblings resulting from this transform can then be
composed bottom-up through time by replacing
each left child category with the category of the
resulting parent, within the same memory element
(or depth level). For example, in Figure 3(a) a
left-child category NP/NP at time t=4 is composed
with a noun new of category NP/NNP (a noun
phrase lacking a proper noun yet to come), result-
ing in a new parent category NP/NNP at time t=5
replacing the left child category NP/NP in the top-
most d=1 memory element.
This in-element composition preserves ele-
ments of the bounded memory store for use in pro-
cessing descendants of this composed constituent,
yielding the human-like memory demands re-
ported in (Schuler et al., 2008). But whenever
an in-element composition like this is hypothe-
sized, it isolates an intermediate constituent (in
this example, the noun phrase ‘new york city’)
from subsequent composition. Allowing access
to this intermediate constituent — for example,
to allow ‘new york city’ to become a modifier
of ‘bonds’, which itself becomes an argument of
‘for’ — requires an analysis in which the interme-
diate constituent is stored in a separate memory
element, shown in Figure 3(b). This creates a lo-
cal ambiguity in the parser (in this case, from time
step t=4) that may have to be propagated across
several words before it can be resolved (in this
case, at time step t=7). This is essentially an am-
biguity between arc-eager (in-element) and arc-
standard (cross-element) composition strategies,
as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser
with an unbounded stack would only hypothesize
analysis (b), avoiding this ambiguity.3
The right-corner HHMM approach described
in this paper relies on a learned statistical model
to predict when in-element (arc-eager) compo-
sitions will occur, in addition to hypothesizing
parse trees. The model encodes a mixed strategy:
with some probability arc-eager or arc-standard
for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn
Wall Street Journal Treebank suggest that this kind
of optionally arc-eager strategy can be reliably sta-
tistically learned.
By placing firm limits on the number of open
incomplete constituents in working memory, the
Hierarchical HMM parser maintains parallel hy-
potheses on the beam which predict whether each
constituent will host a subsequent attachment or
not. Empirical results described in the next section
3It is important to note that neither the right-corner nor
left-corner parsing strategy by itself creates this ambiguity.
The ambiguity arises from the decision to use this option-
ally arc-eager strategy to reduce memory store allocation in
a bounded memory parser. Implementations of left-corner
parsers such as that of Henderson (2004) adopt a arc-standard
strategy, essentially always choosing analysis (b) above, and
thus do not introduce this kind of local ambiguity. But in
adopting this strategy, such parsers must maintain a stack
memory of unbounded size, and thus are not attractive as
models of human parsing in short-term memory (Resnik,
1992).
</bodyText>
<page confidence="0.999645">
31
</page>
<figure confidence="0.976864181818182">
t=1 t=2 t=3 t=4 t=5 t=6 t=7 b)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
a)
d=1
d=2
d=3
word
</figure>
<figureCaption confidence="0.822855">
Figure 3: Alternative analyses of ‘strong demand for new york city ...’: a) using in-element composition,
compatible with ‘strong demand for new york city is ...’ (in which the demand is for the city); and b)
</figureCaption>
<bodyText confidence="0.954414363636364">
using cross-element (or delayed) composition, compatible with either ‘strong demand for new york city
is ...’ (in which the demand is for the city) or ‘strong demand for new york city bonds is ...’ (in which a
forthcoming referent — in this case, bonds — is associated with the city, and is in demand). In-element
composition (a) saves memory but closes off access to the noun phrase headed by ‘city’, and so is not
incompatible with the ‘...bonds’ completion. Cross-element composition (b) requires more memory,
but allows access to the noun phrase headed by ‘city’, so is compatible with either completion. This
ambiguity is introduced at t=4 and propagated until at least t=7. An ordinary, non-right-corner stack
machine would exclusively use analysis (b), avoiding ambiguity.
show that this added demand on parallelism does
not substantially degrade parsing accuracy, even at
very narrow beam widths.
</bodyText>
<sectionHeader confidence="0.993091" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999943464285714">
The parsing model described in Section 3 has
previously been evaluated on the standard task
of parsing the Wall Street Journal section of the
Penn Treebank. This evaluation was optimized
for accuracy results, and reported a relatively wide
beam width of 2000 to achieve its best results.
However, most psycholinguistic models of the hu-
man sentence processing mechanism suggest that
if the HSPM does work in parallel, it does so with
a much lower number of concurrent hypotheses
(Boston et al., 2008b). Viewing the HHMM pars-
ing framework as a psycholinguistic model, a nec-
essary (though not sufficient) condition for it being
a valid model is that it be able to maintain rela-
tively accurate parsing capabilities even at much
lower beam widths.
Thus, the first experiments in this paper evalu-
ate the degradation of parsing accuracy depending
on beam width of the HHMM parser. Experiments
were conducted again on the WSJ Penn Treebank,
using sections 02-21 to train, and section 23 as the
test set. Punctuation was included in both train-
ing and testing. A set of varied beam widths were
considered, from a high of 2000 to a low of 15.
This range was meant to roughly correspond to
the range of parallelism used in other similar ex-
periments, using 2000 as a high end due to its us-
age in previous parsing experiments. However, it
should be noted that in fact the highest value of
2000 is already an approximate search – prelim-
inary experiments showed that exhaustive search
with the HHMM would require more than 100000
elements per time step (exact values may be much
higher but could not be collected because they ex-
hausted system memory).
The HHMM parser was compared to a custom
built (though standard) probabilistic CKY parser
implementation trained on the CNF trees used as
input to the right-corner transform, so that the
CKY parser was able to compete on a fair foot-
ing. The accuracy results of these experiments are
shown in Figure 4.
These results show fairly graceful decline in
parsing accuracy with a beam width starting at
2000 elements down to about 50 beam elements.
This beam width is much less than 1% of the ex-
haustive search, though it is around 1% of what
might be considered the highest reasonable beam
width for efficient parsing. The lowest beam
widths attempted, 15, 20, and 25, result in ac-
curacy below that of the CKY parser. The low-
est beam width attempted, 15, shows the sharpest
decline in accuracy, putting the HHMM system
nearly 8 points below the CKY parser in terms of
accuracy.
This compares reasonably well to results by
</bodyText>
<page confidence="0.994692">
32
</page>
<figure confidence="0.998835590909091">
0 100 200 300 400 500
Beam Width
0
10 20 30 40 50 60 70
Sentence Length
CKY
HHMM
14
8
4
12
10
6
2
Labeled F-Score 84 Seconds per sentence
82
80
78
76
74
72
70
</figure>
<figureCaption confidence="0.996219666666667">
Figure 4: Plot of parsing accuracy (labeled F-
score) vs. beam widths for an HHMM parser
(curved line). Top line is HHMM accuracy with
beam width of 2000 (upper bound). The bottom
line is CKY parser results. Points correspond to
beam widths of 15, 20, 25, 50, 100, 250, and 500.
</figureCaption>
<bodyText confidence="0.999575193548387">
Brants and Crocker (2000) showing that an in-
cremental chart-parsing algorithm can parse accu-
rately with pruning down to 1% of normal memory
usage. While that parsing algorithm is difficult to
compare directly to this HHMM parser, the reduc-
tion in beam width in this system to 50 beam el-
ements from an already approximated 2000 beam
elements shows similar robustness to approxima-
tion. Accuracy comparisons should be taken with
a grain of salt due to additional annotations per-
formed to the Treebank before training, but the
HHMM parser with a beam width of 50 obtains
approximately the same accuracy as the Brants
and Crocker incremental CKY parser pruning to
3% of chart size. At 1% pruning, Brants and
Crocker achieved around 75% accuracy, which
falls between the HHMM parser at beam widths
of 20 and 25.
Results by Boston et al. (2008b) are also dif-
ficult to compare directly due to a difference in
parsing algorithm and different research priority
(that paper was attempting to correlate parsing dif-
ficulty with reading difficulty). However, that pa-
per showed that a dependency parser using less
than ten beam elements (and as few as one) was
just as capable of predicting reading difficulty as
the parser using 100 beam elements.
A second experiment was conducted to eval-
uate the HHMM for its time efficiency in pars-
ing. This experiment is intended to address two
questions: Whether this framework is efficient
</bodyText>
<figureCaption confidence="0.5712365">
Figure 5: Plot of parsing time vs. sentence length
for HHMM and CKY parsers.
</figureCaption>
<bodyText confidence="0.999932411764706">
enough to be considered a viable psycholinguis-
tic model, and whether its parsing time and accu-
racy remain competitive with more standard cu-
bic time parsing technologies at low beam widths.
To evaluate this aspect, the HHMM parser was
run at low beam widths on sentences of varying
lengths. The baseline was the widely-used Stan-
ford parser (Klein and Manning, 2003), run in
‘vanilla PCFG’ mode. This parser was used rather
than the custom-built CKY parser from the pre-
vious experiment, to avoid the possibility that its
implementation was not efficient enough to pro-
vide a realistic test. The HHMM parser was imple-
mented as described in the previous section. These
experiments were run on a machine with a single
2.40 GHz Celeron CPU, with 512 MB of RAM. In
both implementations the parser timing includes
only time spent actually parsing sentences, ignor-
ing the overhead incurred by reading in model files
or training.
Figure 5 shows a plot of parsing time versus
sentence length for the HHMM parser for a beam
width of 20. Sentences shorter than 10 words were
not included for visual clarity (both parsers are ex-
tremely fast at that length). At this beam width,
the performance of the HHMM parser (labeled F-
score) was 74.03%, compared to 71% for a plain
CKY parser. As expected, the HHMM parsing
time increases linearly with sentence length, while
the CKY parsing time increases super-linearly.
(However, due to high constants in the run time
complexity of the HHMM, it was not a priori clear
that the HHMM would be faster for any sentence
of reasonable length.)
</bodyText>
<page confidence="0.997011">
33
</page>
<bodyText confidence="0.9998855">
The results of this experiment show that the
HHMM parser is indeed competitive with a proba-
bilistic CKY parser, in terms of parsing efficiency,
even while parsing with higher accuracy. At sen-
tences longer that 26 words (including punctua-
tion), the HHMM parser is faster than the CKY
parser. This advantage is clear for segmented text
such as the Wall Street Journal corpus. However,
this advantage is compounded when considering
unsegmented or ambiguously segmented text such
as transcribed speech or less formal written text, as
the HHMM parser can also make decisions about
where to put sentence breaks, and do so in linear
time.4
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999925121212121">
This paper furthers the case for the HHMM as a
viable psycholinguistic model of the human pars-
ing mechanism by showing that performance de-
grades gracefully as parallelism decreases, provid-
ing reasonably accurate parsing even at very low
beam widths. In addition, this work shows that
an HHMM parser run at low beam widths is com-
petitive in speed with parsers that don’t work in-
crementally, because of its asymptotically linear
runtime.
This is especially surprising given that the
HHMM uses parallel hypotheses on the beam to
predict whether constituents will remain open for
attachment or not. Success at low beam widths
suggests that this optionally arc-eager prediction
is something that is indeed relatively predictable
during parsing, lending credence to claims of psy-
cholinguistic relevance of HHMM parsing.
Future work should explore further directions
in improving parsing performance at low beam
widths. The lowest beam value experiments
presented here generally parsed fairly accurately
when they completed, but were already encounter-
ing problems with unparseable sentences that neg-
atively affected parser accuracy. The large accu-
racy decrease between beam sizes of 20 and 15 is
likely to be mostly due to the lack of any correct
analysis on the beam when the sentence is com-
pleted.
It should be noted, however, that no adjustments
were made to the parser’s syntactic model with
these beam variations. This syntactic model was
optimized for accuracy at the standard beam width
</bodyText>
<footnote confidence="0.965661">
4It does this probabilistically as a side effect of the pars-
ing, by choosing an analysis in which r°ERG (for any t).
</footnote>
<bodyText confidence="0.9995937">
of 2000, and thus contains some state splittings
that are beneficial at wide beam widths, but at
low beam widths are redundant and prevent oth-
erwise valid hypotheses from being maintained on
the beam. For applications in which speed is a
priority, future research can evaluate tradeoffs in
accuracy that occur at different beam widths with
a coarser-grained syntactic representation that al-
lows for more variation of hypotheses even on
very small beams.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999092">
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
</bodyText>
<sectionHeader confidence="0.999366" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999714852941176">
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233–250.
James Baker. 1975. The Dragon system: an overivew.
IEEE Transactions on Acoustics, Speech and Signal
Processing, 23(1):24–29.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J.˜R. Hayes, editor, Cognition
and the Development of Language, pages 279–362.
Wiley, New York.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008a. Parsing
costs as predictors of reading difficulty: An evalua-
tion using the Potsdam Sentence Corpus. Journal of
Eye Movement Research, 2(1):1–12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5–8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ’00, pages 111–118.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269–321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87–
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647–669.
</reference>
<page confidence="0.990464">
34
</page>
<reference confidence="0.999899161290322">
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159–166, Pittsburgh, PA.
James Henderson. 2004. Lookahead in determinis-
tic left-corner parsing. In Proc. Workshop on Incre-
mental Parsing: Bringing Engineering and Cogni-
tion Together, pages 26–33, Barcelona, Spain.
Frederick Jelinek, Lalit R. Bahl, and Robert L. Mercer.
1975. Design of a linguistic statistical decoder for
the recognition of continuous speech. IEEE Trans-
actions on Information Theory, 21:250–256.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423–430, Sapporo,
Japan.
Richard L. Lewis. 2000. Falsifying serial and paral-
lel parsing models: Empirical conundrums and an
overlooked paradigm. Journal of Psycholinguistic
Research, 29:241–248.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81–97.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833–840, Vancouver, BC, Canada.
Philip Resnik. 1992. Left-corner parsing and psy-
chological plausibility. In Proceedings of COLING,
pages 191–197, Nantes, France.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324–333.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785–792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ’09), pages
344–352, Boulder, Colorado.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 49th Annual Conference of the Association for
Computational Linguistics.
</reference>
<page confidence="0.999318">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.585628">
<title confidence="0.999491">HHMM Parsing with Limited Parallelism</title>
<author confidence="0.99722">Tim</author>
<affiliation confidence="0.997748">Department of Computer and University of Minnesota, Twin</affiliation>
<email confidence="0.998037">tmill@cs.umn.edu</email>
<author confidence="0.663926">William</author>
<affiliation confidence="0.978723">University of Minnesota, Twin and The Ohio State</affiliation>
<email confidence="0.999259">schuler@ling.ohio-state.edu</email>
<abstract confidence="0.996526619047619">Hierarchical Hidden Markov Model (HHMM) parsers have been proposed as psycholinguistic models due to their broad coverage within human-like working memory limits (Schuler et al., 2008) and ability to model human reading time behavior according to various complexity metrics (Wu et al., 2010). But HHMMs have been evaluated previously only with very wide beams of several thousand parallel hypotheses, weakening claims to the model’s efficiency and psychological relevance. This paper examines the effects of varying beam width on parsing accuracy and speed in this model, showing that parsing accuracy degrades gracefully as width decreases dramatically (to of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>J. Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="18380" citStr="Abney and Johnson (1991)" startWordPosition="2964" endWordPosition="2967"> to this intermediate constituent — for example, to allow ‘new york city’ to become a modifier of ‘bonds’, which itself becomes an argument of ‘for’ — requires an analysis in which the intermediate constituent is stored in a separate memory element, shown in Figure 3(b). This creates a local ambiguity in the parser (in this case, from time step t=4) that may have to be propagated across several words before it can be resolved (in this case, at time step t=7). This is essentially an ambiguity between arc-eager (in-element) and arcstandard (cross-element) composition strategies, as described by Abney and Johnson (1991). In contrast, an ordinary (purely arc-standard) parser with an unbounded stack would only hypothesize analysis (b), avoiding this ambiguity.3 The right-corner HHMM approach described in this paper relies on a learned statistical model to predict when in-element (arc-eager) compositions will occur, in addition to hypothesizing parse trees. The model encodes a mixed strategy: with some probability arc-eager or arc-standard for each possible expansion. Accuracy results on a right-corner HHMM model trained on the Penn Wall Street Journal Treebank suggest that this kind of optionally arc-eager str</context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Steven P. Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. J. Psycholinguistic Research, 20(3):233–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Baker</author>
</authors>
<title>The Dragon system: an overivew.</title>
<date>1975</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="13509" citStr="Baker, 1975" startWordPosition="2127" endWordPosition="2128">s exemplified in Figure 1. 3.2 Hierarchical Hidden Markov Models Right-corner transformed trees are mapped to random variables in a Hierarchical Hidden Markov Model (Murphy and Paskin, 2001). A Hierarchical Hidden Markov Model (HHMM) is essentially a factored version of a Hidden Markov Model (HMM), configured to recognize bounded recursive structures (i.e. trees). Like HMMs, HHMMs use Viterbi decoding to obtain sequences of hidden states s1..T given sequences of observations o1..T (words or audio features), through independence assumptions in a transition model OA and an observation model OB (Baker, 1975; Jelinek et al., 1975): PΘA(st |st−1) · PΘs(ot |st) (4) PΘR,d(rdt |rd+1 t sd t–1sd–1 t–1 )· PΘS,d(sdt |rd+1 t rdt sd t–1sd–1 t ) which allow depth-specific variables to reduce (through OR-Rdn,d), transition (OS-Trn,d), and expand (OS-Exp,d) like tape symbols in a pushdown automaton with a bounded memory store, depending on whether the variable below has reduced (rdt ∈RG) or not (rdt 6∈RG):2 PΘR,d(rd t |rd+1 t sd t−1sd−1 t−1 )def = if rt }R�G : Qrdt = r⊤11 ((7 if rt+1∈RG : PΘR-Rdn,d (rdt |rt+sdt− 1 std−1) l ) PΘS,d(sd t |rd+1 t rd t sd t−1sd−1 t )def = { if rd+1 t 6∈RG, rdt 6∈RG : Qsdt = sdt−1</context>
</contexts>
<marker>Baker, 1975</marker>
<rawString>James Baker. 1975. The Dragon system: an overivew. IEEE Transactions on Acoustics, Speech and Signal Processing, 23(1):24–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Bever</author>
</authors>
<title>The cognitive basis for linguistic structure.</title>
<date>1970</date>
<booktitle>Cognition and the Development of Language,</booktitle>
<pages>279--362</pages>
<editor>In J.˜R. Hayes, editor,</editor>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="1535" citStr="Bever, 1970" startWordPosition="216" endWordPosition="217">cally (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser. 1 Introduction Probabilistic parsers have been successful at accurately estimating syntactic structure from free text. Typically, these systems work by considering entire sentences (or utterances) at once, using dynamic programming to obtain globally optimal solutions from locally optimal sub-parses. However, these methods usually do not attempt to conform to human-like processing constraints, e.g. leading to center embedding and garden path effects (Chomsky and Miller, 1963; Bever, 1970). For systems prioritizing accurate parsing performance, there is little need to produce human-like errors. But from a human modeling perspective, the success of globally optimized whole-utterance models raises the question of how humans can accurately parse linguistic input without access to this same global optimization. This question creates a niche in computational research for models that are able to parse accurately while adhering as closely as possible to human-like psycholinguistic constraints. Recent work on incremental parsers includes work on Hierarchical Hidden Markov Model (HHMM) </context>
<context position="5462" citStr="Bever, 1970" startWordPosition="823" endWordPosition="824">e speaker continues speaking at extended length without pause. Standard machine approaches, such as chart parsers based on the CKY algorithm, operate in worst-case cubic run time on the length of input. Without knowing where an utterance or sentence might end, such an algorithm will take more time with each successive word and will eventually fall behind. The third criterion is a reasonable limiting of memory resources. This constraint means that the HSPM, while possibly considering multiple hypotheses in parallel, is not limitlessly so, as evidenced by the existence of garden path sentences (Bever, 1970; Lewis, 2000). If this were not the case, garden-path sentences would not cause problems, as reaching the disambiguating word would simply result in a change in the favored hypothesis. In fact, garden path sentences typically cannot be understood on a first pass and must be reread, indicating that the correct analysis is attainable and yet not present in the set of parallel hypotheses of the first pass. While parsers meeting these three criteria can claim to not violate any psycholinguistic constraints, there has been much recent work in testing psycholinguistically-motivated parsers to make </context>
</contexts>
<marker>Bever, 1970</marker>
<rawString>Thomas G. Bever. 1970. The cognitive basis for linguistic structure. In J.˜R. Hayes, editor, Cognition and the Development of Language, pages 279–362. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Ferrara Boston</author>
<author>John T Hale</author>
<author>Reinhold Kliegl</author>
<author>Umesh Patil</author>
<author>Shravan Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="3165" citStr="Boston et al., 2008" startWordPosition="456" endWordPosition="459">mes at a price. The HHMM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a memory element for subsequent constituents). Although empirically determining the number of parallel competing hypotheses used in human sentence processing is difficult, previous results in computational models have shown that human-like behavior can be elicited at very low levels of parallelism (Boston et al., 2008b; Brants and Crocker, 2000), suggesting that large numbers of active hypotheses are not needed. Previously, the HHMM parser has only been evaluated on large beam widths, leaving this aspect of its psycholinguistic plausibility untested. In this paper, the performance of an HHMM parser will be evaluated in two experiments that 27 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27–35, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics vary the amount of parallelism allowed during parsing, measuring the degree to </context>
<context position="6691" citStr="Boston et al. (2008" startWordPosition="1015" endWordPosition="1018">edictions about human sentence processing, in order to provide positive evidence for certain probabilistic parsing models as valid psycholinguistic models of sentence processing. This work has largely focused on correlating measures of parsing difficulty in computational models with delays in reading time in human subjects. Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and showed that these new metrics are strong predictors of reading difficulty. Wu et al. (2010) evaluate the same Hierarchical Hidden Markov Model parser used in this work in terms of its ability to reproduce human-like results for various complexity metrics, including some of those men</context>
<context position="21705" citStr="Boston et al., 2008" startWordPosition="3492" endWordPosition="3495"> parallelism does not substantially degrade parsing accuracy, even at very narrow beam widths. 5 Experimental Evaluation The parsing model described in Section 3 has previously been evaluated on the standard task of parsing the Wall Street Journal section of the Penn Treebank. This evaluation was optimized for accuracy results, and reported a relatively wide beam width of 2000 to achieve its best results. However, most psycholinguistic models of the human sentence processing mechanism suggest that if the HSPM does work in parallel, it does so with a much lower number of concurrent hypotheses (Boston et al., 2008b). Viewing the HHMM parsing framework as a psycholinguistic model, a necessary (though not sufficient) condition for it being a valid model is that it be able to maintain relatively accurate parsing capabilities even at much lower beam widths. Thus, the first experiments in this paper evaluate the degradation of parsing accuracy depending on beam width of the HHMM parser. Experiments were conducted again on the WSJ Penn Treebank, using sections 02-21 to train, and section 23 as the test set. Punctuation was included in both training and testing. A set of varied beam widths were considered, fr</context>
<context position="25033" citStr="Boston et al. (2008" startWordPosition="4086" endWordPosition="4089">HHMM parser, the reduction in beam width in this system to 50 beam elements from an already approximated 2000 beam elements shows similar robustness to approximation. Accuracy comparisons should be taken with a grain of salt due to additional annotations performed to the Treebank before training, but the HHMM parser with a beam width of 50 obtains approximately the same accuracy as the Brants and Crocker incremental CKY parser pruning to 3% of chart size. At 1% pruning, Brants and Crocker achieved around 75% accuracy, which falls between the HHMM parser at beam widths of 20 and 25. Results by Boston et al. (2008b) are also difficult to compare directly due to a difference in parsing algorithm and different research priority (that paper was attempting to correlate parsing difficulty with reading difficulty). However, that paper showed that a dependency parser using less than ten beam elements (and as few as one) was just as capable of predicting reading difficulty as the parser using 100 beam elements. A second experiment was conducted to evaluate the HHMM for its time efficiency in parsing. This experiment is intended to address two questions: Whether this framework is efficient Figure 5: Plot of par</context>
</contexts>
<marker>Boston, Hale, Kliegl, Patil, Vasishth, 2008</marker>
<rawString>Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl, Umesh Patil, and Shravan Vasishth. 2008a. Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement Research, 2(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Ferrara Boston</author>
<author>John T Hale</author>
<author>Reinhold Kliegl</author>
<author>Shravan Vasishth</author>
</authors>
<title>Surprising parser actions and reading difficulty.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT, Short Papers,</booktitle>
<pages>5--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3165" citStr="Boston et al., 2008" startWordPosition="456" endWordPosition="459">mes at a price. The HHMM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a memory element for subsequent constituents). Although empirically determining the number of parallel competing hypotheses used in human sentence processing is difficult, previous results in computational models have shown that human-like behavior can be elicited at very low levels of parallelism (Boston et al., 2008b; Brants and Crocker, 2000), suggesting that large numbers of active hypotheses are not needed. Previously, the HHMM parser has only been evaluated on large beam widths, leaving this aspect of its psycholinguistic plausibility untested. In this paper, the performance of an HHMM parser will be evaluated in two experiments that 27 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27–35, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics vary the amount of parallelism allowed during parsing, measuring the degree to </context>
<context position="6691" citStr="Boston et al. (2008" startWordPosition="1015" endWordPosition="1018">edictions about human sentence processing, in order to provide positive evidence for certain probabilistic parsing models as valid psycholinguistic models of sentence processing. This work has largely focused on correlating measures of parsing difficulty in computational models with delays in reading time in human subjects. Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and showed that these new metrics are strong predictors of reading difficulty. Wu et al. (2010) evaluate the same Hierarchical Hidden Markov Model parser used in this work in terms of its ability to reproduce human-like results for various complexity metrics, including some of those men</context>
<context position="21705" citStr="Boston et al., 2008" startWordPosition="3492" endWordPosition="3495"> parallelism does not substantially degrade parsing accuracy, even at very narrow beam widths. 5 Experimental Evaluation The parsing model described in Section 3 has previously been evaluated on the standard task of parsing the Wall Street Journal section of the Penn Treebank. This evaluation was optimized for accuracy results, and reported a relatively wide beam width of 2000 to achieve its best results. However, most psycholinguistic models of the human sentence processing mechanism suggest that if the HSPM does work in parallel, it does so with a much lower number of concurrent hypotheses (Boston et al., 2008b). Viewing the HHMM parsing framework as a psycholinguistic model, a necessary (though not sufficient) condition for it being a valid model is that it be able to maintain relatively accurate parsing capabilities even at much lower beam widths. Thus, the first experiments in this paper evaluate the degradation of parsing accuracy depending on beam width of the HHMM parser. Experiments were conducted again on the WSJ Penn Treebank, using sections 02-21 to train, and section 23 as the test set. Punctuation was included in both training and testing. A set of varied beam widths were considered, fr</context>
<context position="25033" citStr="Boston et al. (2008" startWordPosition="4086" endWordPosition="4089">HHMM parser, the reduction in beam width in this system to 50 beam elements from an already approximated 2000 beam elements shows similar robustness to approximation. Accuracy comparisons should be taken with a grain of salt due to additional annotations performed to the Treebank before training, but the HHMM parser with a beam width of 50 obtains approximately the same accuracy as the Brants and Crocker incremental CKY parser pruning to 3% of chart size. At 1% pruning, Brants and Crocker achieved around 75% accuracy, which falls between the HHMM parser at beam widths of 20 and 25. Results by Boston et al. (2008b) are also difficult to compare directly due to a difference in parsing algorithm and different research priority (that paper was attempting to correlate parsing difficulty with reading difficulty). However, that paper showed that a dependency parser using less than ten beam elements (and as few as one) was just as capable of predicting reading difficulty as the parser using 100 beam elements. A second experiment was conducted to evaluate the HHMM for its time efficiency in parsing. This experiment is intended to address two questions: Whether this framework is efficient Figure 5: Plot of par</context>
</contexts>
<marker>Boston, Hale, Kliegl, Vasishth, 2008</marker>
<rawString>Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl, and Shravan Vasishth. 2008b. Surprising parser actions and reading difficulty. In Proceedings of ACL08: HLT, Short Papers, pages 5–8, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Matthew Crocker</author>
</authors>
<title>Probabilistic parsing and psychological plausibility.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING ’00,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="3193" citStr="Brants and Crocker, 2000" startWordPosition="460" endWordPosition="463">MM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a memory element for subsequent constituents). Although empirically determining the number of parallel competing hypotheses used in human sentence processing is difficult, previous results in computational models have shown that human-like behavior can be elicited at very low levels of parallelism (Boston et al., 2008b; Brants and Crocker, 2000), suggesting that large numbers of active hypotheses are not needed. Previously, the HHMM parser has only been evaluated on large beam widths, leaving this aspect of its psycholinguistic plausibility untested. In this paper, the performance of an HHMM parser will be evaluated in two experiments that 27 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27–35, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics vary the amount of parallelism allowed during parsing, measuring the degree to which this degrades the syst</context>
<context position="24222" citStr="Brants and Crocker (2000)" startWordPosition="3947" endWordPosition="3950">the sharpest decline in accuracy, putting the HHMM system nearly 8 points below the CKY parser in terms of accuracy. This compares reasonably well to results by 32 0 100 200 300 400 500 Beam Width 0 10 20 30 40 50 60 70 Sentence Length CKY HHMM 14 8 4 12 10 6 2 Labeled F-Score 84 Seconds per sentence 82 80 78 76 74 72 70 Figure 4: Plot of parsing accuracy (labeled Fscore) vs. beam widths for an HHMM parser (curved line). Top line is HHMM accuracy with beam width of 2000 (upper bound). The bottom line is CKY parser results. Points correspond to beam widths of 15, 20, 25, 50, 100, 250, and 500. Brants and Crocker (2000) showing that an incremental chart-parsing algorithm can parse accurately with pruning down to 1% of normal memory usage. While that parsing algorithm is difficult to compare directly to this HHMM parser, the reduction in beam width in this system to 50 beam elements from an already approximated 2000 beam elements shows similar robustness to approximation. Accuracy comparisons should be taken with a grain of salt due to additional annotations performed to the Treebank before training, but the HHMM parser with a beam width of 50 obtains approximately the same accuracy as the Brants and Crocker </context>
</contexts>
<marker>Brants, Crocker, 2000</marker>
<rawString>Thorsten Brants and Matthew Crocker. 2000. Probabilistic parsing and psychological plausibility. In Proceedings of COLING ’00, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>George A Miller</author>
</authors>
<title>Introduction to the formal analysis of natural languages.</title>
<date>1963</date>
<booktitle>In Handbook of Mathematical Psychology,</booktitle>
<pages>269--321</pages>
<publisher>Wiley.</publisher>
<contexts>
<context position="1521" citStr="Chomsky and Miller, 1963" startWordPosition="212" endWordPosition="215">am width decreases dramatically (to 2% of the width used to achieve previous top results), without sacrificing gains over a baseline CKY parser. 1 Introduction Probabilistic parsers have been successful at accurately estimating syntactic structure from free text. Typically, these systems work by considering entire sentences (or utterances) at once, using dynamic programming to obtain globally optimal solutions from locally optimal sub-parses. However, these methods usually do not attempt to conform to human-like processing constraints, e.g. leading to center embedding and garden path effects (Chomsky and Miller, 1963; Bever, 1970). For systems prioritizing accurate parsing performance, there is little need to produce human-like errors. But from a human modeling perspective, the success of globally optimized whole-utterance models raises the question of how humans can accurately parse linguistic input without access to this same global optimization. This question creates a niche in computational research for models that are able to parse accurately while adhering as closely as possible to human-like psycholinguistic constraints. Recent work on incremental parsers includes work on Hierarchical Hidden Markov</context>
</contexts>
<marker>Chomsky, Miller, 1963</marker>
<rawString>Noam Chomsky and George A. Miller. 1963. Introduction to the formal analysis of natural languages. In Handbook of Mathematical Psychology, pages 269–321. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Cowan</author>
</authors>
<title>The magical number 4 in shortterm memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences,</title>
<date>2001</date>
<pages>24--87</pages>
<contexts>
<context position="8945" citStr="Cowan, 2001" startWordPosition="1379" endWordPosition="1380"> system to parse ambiguous sentences (such 28 as the athlete realized his goals were out of reach) and examine the relative probabilities of two plausible analyses at each time step. They then show that the shifting of these two probabilities is consistent with empirical evidence about how humans perceive these sentences word by word. However, as will be described below, the HHMM has advantages over the CHMM from a psycholinguistic modeling perspective. The HHMM uses a limited memory store containing only four elements which is consistent with many estimates of human short term memory limits (Cowan, 2001; Miller, 1956). In addition to modeling memory limits, the limited store acts as a fixed-depth stack that ensures linear asymptotic parsing time, and a grammar transform allows for wide coverage of speech and newspaper corpora within that limited memory store (Schuler et al., 2010). 3 Hierarchical Hidden Markov Model Parser Hidden Markov Models (HMMs) have long been used to successfully model sequence data in which there is a latent (hidden) variable at each time step that generates the observed evidence at that time step. These models are used for such applications as part-of-speech tagging,</context>
</contexts>
<marker>Cowan, 2001</marker>
<rawString>Nelson Cowan. 2001. The magical number 4 in shortterm memory: A reconsideration of mental storage capacity. Behavioral and Brain Sciences, 24:87– 185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Crocker</author>
<author>Thorsten Brants</author>
</authors>
<title>Widecoverage probabilistic sentence processing.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="8090" citStr="Crocker and Brants, 2000" startWordPosition="1237" endWordPosition="1240"> required to hold a given hypothesis. Using more memory elements corresponds to center embedding in phrase structure trees, and presumably correlates to some degree with complexity. Average embedding for a time step is computed by computing the weighted average number of required memory elements (weighted by probability) for all hypotheses on the beam. Embedding difference is simply the change in this value when the next word is encountered. Outside of Wu et al., the most similar work from a modeling perspective is an incremental parser implemented using Cascaded Hidden Markov Models (CHMMs) (Crocker and Brants, 2000). This model is superficially similar to the Hierarchical Hidden Markov Models described below in that it relies on multiple levels of interdependent HMMs to account for hierarchical structure in an incremental model. Crocker and Brants use the system to parse ambiguous sentences (such 28 as the athlete realized his goals were out of reach) and examine the relative probabilities of two plausible analyses at each time step. They then show that the shifting of these two probabilities is consistent with empirical evidence about how humans perceive these sentences word by word. However, as will be</context>
</contexts>
<marker>Crocker, Brants, 2000</marker>
<rawString>Matthew Crocker and Thorsten Brants. 2000. Widecoverage probabilistic sentence processing. Journal of Psycholinguistic Research, 29(6):647–669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="6409" citStr="Hale (2001)" startWordPosition="968" endWordPosition="969">nd yet not present in the set of parallel hypotheses of the first pass. While parsers meeting these three criteria can claim to not violate any psycholinguistic constraints, there has been much recent work in testing psycholinguistically-motivated parsers to make forward predictions about human sentence processing, in order to provide positive evidence for certain probabilistic parsing models as valid psycholinguistic models of sentence processing. This work has largely focused on correlating measures of parsing difficulty in computational models with delays in reading time in human subjects. Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and s</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Lookahead in deterministic left-corner parsing.</title>
<date>2004</date>
<booktitle>In Proc. Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>26--33</pages>
<location>Barcelona,</location>
<contexts>
<context position="19648" citStr="Henderson (2004)" startWordPosition="3154" endWordPosition="3155">ing firm limits on the number of open incomplete constituents in working memory, the Hierarchical HMM parser maintains parallel hypotheses on the beam which predict whether each constituent will host a subsequent attachment or not. Empirical results described in the next section 3It is important to note that neither the right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt a arc-standard strategy, essentially always choosing analysis (b) above, and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik, 1992). 31 t=1 t=2 t=3 t=4 t=5 t=6 t=7 b) d=1 d=2 d=3 word t=1 t=2 t=3 t=4 t=5 t=6 t=7 a) d=1 d=2 d=3 word Figure 3: Alternative analyses of ‘strong demand for new york city ...’: a) using in-element composition, compatible with ‘strong demand for new york city is ...’ (in which the dema</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Lookahead in deterministic left-corner parsing. In Proc. Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 26–33, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Lalit R Bahl</author>
<author>Robert L Mercer</author>
</authors>
<title>Design of a linguistic statistical decoder for the recognition of continuous speech.</title>
<date>1975</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>21--250</pages>
<contexts>
<context position="13532" citStr="Jelinek et al., 1975" startWordPosition="2129" endWordPosition="2132"> in Figure 1. 3.2 Hierarchical Hidden Markov Models Right-corner transformed trees are mapped to random variables in a Hierarchical Hidden Markov Model (Murphy and Paskin, 2001). A Hierarchical Hidden Markov Model (HHMM) is essentially a factored version of a Hidden Markov Model (HMM), configured to recognize bounded recursive structures (i.e. trees). Like HMMs, HHMMs use Viterbi decoding to obtain sequences of hidden states s1..T given sequences of observations o1..T (words or audio features), through independence assumptions in a transition model OA and an observation model OB (Baker, 1975; Jelinek et al., 1975): PΘA(st |st−1) · PΘs(ot |st) (4) PΘR,d(rdt |rd+1 t sd t–1sd–1 t–1 )· PΘS,d(sdt |rd+1 t rdt sd t–1sd–1 t ) which allow depth-specific variables to reduce (through OR-Rdn,d), transition (OS-Trn,d), and expand (OS-Exp,d) like tape symbols in a pushdown automaton with a bounded memory store, depending on whether the variable below has reduced (rdt ∈RG) or not (rdt 6∈RG):2 PΘR,d(rd t |rd+1 t sd t−1sd−1 t−1 )def = if rt }R�G : Qrdt = r⊤11 ((7 if rt+1∈RG : PΘR-Rdn,d (rdt |rt+sdt− 1 std−1) l ) PΘS,d(sd t |rd+1 t rd t sd t−1sd−1 t )def = { if rd+1 t 6∈RG, rdt 6∈RG : Qsdt = sdt−111 t ) t rdt sd t−1sd−1</context>
</contexts>
<marker>Jelinek, Bahl, Mercer, 1975</marker>
<rawString>Frederick Jelinek, Lalit R. Bahl, and Robert L. Mercer. 1975. Design of a linguistic statistical decoder for the recognition of continuous speech. IEEE Transactions on Information Theory, 21:250–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="26053" citStr="Klein and Manning, 2003" startWordPosition="4256" endWordPosition="4259">. A second experiment was conducted to evaluate the HHMM for its time efficiency in parsing. This experiment is intended to address two questions: Whether this framework is efficient Figure 5: Plot of parsing time vs. sentence length for HHMM and CKY parsers. enough to be considered a viable psycholinguistic model, and whether its parsing time and accuracy remain competitive with more standard cubic time parsing technologies at low beam widths. To evaluate this aspect, the HHMM parser was run at low beam widths on sentences of varying lengths. The baseline was the widely-used Stanford parser (Klein and Manning, 2003), run in ‘vanilla PCFG’ mode. This parser was used rather than the custom-built CKY parser from the previous experiment, to avoid the possibility that its implementation was not efficient enough to provide a realistic test. The HHMM parser was implemented as described in the previous section. These experiments were run on a machine with a single 2.40 GHz Celeron CPU, with 512 MB of RAM. In both implementations the parser timing includes only time spent actually parsing sentences, ignoring the overhead incurred by reading in model files or training. Figure 5 shows a plot of parsing time versus </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard L Lewis</author>
</authors>
<title>Falsifying serial and parallel parsing models: Empirical conundrums and an overlooked paradigm.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>29--241</pages>
<contexts>
<context position="5476" citStr="Lewis, 2000" startWordPosition="825" endWordPosition="826">tinues speaking at extended length without pause. Standard machine approaches, such as chart parsers based on the CKY algorithm, operate in worst-case cubic run time on the length of input. Without knowing where an utterance or sentence might end, such an algorithm will take more time with each successive word and will eventually fall behind. The third criterion is a reasonable limiting of memory resources. This constraint means that the HSPM, while possibly considering multiple hypotheses in parallel, is not limitlessly so, as evidenced by the existence of garden path sentences (Bever, 1970; Lewis, 2000). If this were not the case, garden-path sentences would not cause problems, as reaching the disambiguating word would simply result in a change in the favored hypothesis. In fact, garden path sentences typically cannot be understood on a first pass and must be reread, indicating that the correct analysis is attainable and yet not present in the set of parallel hypotheses of the first pass. While parsers meeting these three criteria can claim to not violate any psycholinguistic constraints, there has been much recent work in testing psycholinguistically-motivated parsers to make forward predic</context>
</contexts>
<marker>Lewis, 2000</marker>
<rawString>Richard L. Lewis. 2000. Falsifying serial and parallel parsing models: Empirical conundrums and an overlooked paradigm. Journal of Psycholinguistic Research, 29:241–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2458" citStr="Marcus et al., 1993" startWordPosition="354" endWordPosition="357"> global optimization. This question creates a niche in computational research for models that are able to parse accurately while adhering as closely as possible to human-like psycholinguistic constraints. Recent work on incremental parsers includes work on Hierarchical Hidden Markov Model (HHMM) parsers that operate in linear time by maintaining a bounded store of incomplete constituents (Schuler et al., 2008). Despite this seeming limitation, corpus studies have shown that through the use of grammar transforms, this parser is able to cover nearly all sentences contained in the Penn Treebank (Marcus et al., 1993) using a small number of unconnected memory elements. But this bounded-memory parsing comes at a price. The HHMM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a memory element for subsequent constituents). Although empirically determining the number of parallel competing hypotheses used in human sentence processing is difficult, previous results in computational models</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>The magical number seven, plus or minus two: Some limits on our capacity for processing information.</title>
<date>1956</date>
<journal>Psychological Review,</journal>
<pages>63--81</pages>
<contexts>
<context position="8960" citStr="Miller, 1956" startWordPosition="1381" endWordPosition="1382">rse ambiguous sentences (such 28 as the athlete realized his goals were out of reach) and examine the relative probabilities of two plausible analyses at each time step. They then show that the shifting of these two probabilities is consistent with empirical evidence about how humans perceive these sentences word by word. However, as will be described below, the HHMM has advantages over the CHMM from a psycholinguistic modeling perspective. The HHMM uses a limited memory store containing only four elements which is consistent with many estimates of human short term memory limits (Cowan, 2001; Miller, 1956). In addition to modeling memory limits, the limited store acts as a fixed-depth stack that ensures linear asymptotic parsing time, and a grammar transform allows for wide coverage of speech and newspaper corpora within that limited memory store (Schuler et al., 2010). 3 Hierarchical Hidden Markov Model Parser Hidden Markov Models (HMMs) have long been used to successfully model sequence data in which there is a latent (hidden) variable at each time step that generates the observed evidence at that time step. These models are used for such applications as part-of-speech tagging, and speech rec</context>
</contexts>
<marker>Miller, 1956</marker>
<rawString>George A. Miller. 1956. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63:81–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
<author>Mark A Paskin</author>
</authors>
<title>Linear time inference in hierarchical HMMs.</title>
<date>2001</date>
<booktitle>In Proc. NIPS,</booktitle>
<pages>833--840</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="9925" citStr="Murphy and Paskin (2001)" startWordPosition="1530" endWordPosition="1533">ave long been used to successfully model sequence data in which there is a latent (hidden) variable at each time step that generates the observed evidence at that time step. These models are used for such applications as part-of-speech tagging, and speech recognition. Hierarchical Hidden Markov Models (HHMMs) are an extension of HMMs which can represent sequential data containing hierarchical relations. In HHMMs, complex hidden variables may output evidence for several time steps in sequence. This process may recurse, though a finite depth is required to make any guarantees about performance. Murphy and Paskin (2001) showed that this model could be framed as a Dynamic Bayes Network (DBN), so that inference is linear on the length of the input sequence. In the HHMM parser used here, the complex hidden variables are syntactic states that generate sub-sequences of other syntactic states, eventually generating pre-terminals and words. This section will describe how the trees must be transformed, and then mapped to HHMM states. This section will then continue with a formal definition of an HHMM, followed by a description of how this model can parse natural language, and finally a discussion of what different a</context>
<context position="13088" citStr="Murphy and Paskin, 2001" startWordPosition="2063" endWordPosition="2066"> (Equation 6): �PΘA(st|st–1) = PΘR(rt|st–1)·PΘS(st|rt st–1) a) Aη Aη0 Aη1 aη00 Aη010 Aη011 Aη01 Aη100 Aη101 Aη10 aη11 aη0100 aη1011 aη0111 aη1010 aη0110 aη1001 aη0101 aη1000 rt b) Aη Aη/Aη11 Aη/Aη1 Aη0 Figure 1: Sample right-corner transform of schematized tree before (a) and after (b) application of transform. is mapped to the top of a left-expanding sequence in a right-corner transformed tree: The application of this transform is exemplified in Figure 1. 3.2 Hierarchical Hidden Markov Models Right-corner transformed trees are mapped to random variables in a Hierarchical Hidden Markov Model (Murphy and Paskin, 2001). A Hierarchical Hidden Markov Model (HHMM) is essentially a factored version of a Hidden Markov Model (HMM), configured to recognize bounded recursive structures (i.e. trees). Like HMMs, HHMMs use Viterbi decoding to obtain sequences of hidden states s1..T given sequences of observations o1..T (words or audio features), through independence assumptions in a transition model OA and an observation model OB (Baker, 1975; Jelinek et al., 1975): PΘA(st |st−1) · PΘs(ot |st) (4) PΘR,d(rdt |rd+1 t sd t–1sd–1 t–1 )· PΘS,d(sdt |rd+1 t rdt sd t–1sd–1 t ) which allow depth-specific variables to reduce (t</context>
</contexts>
<marker>Murphy, Paskin, 2001</marker>
<rawString>Kevin P. Murphy and Mark A. Paskin. 2001. Linear time inference in hierarchical HMMs. In Proc. NIPS, pages 833–840, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Left-corner parsing and psychological plausibility.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>191--197</pages>
<location>Nantes, France.</location>
<contexts>
<context position="19966" citStr="Resnik, 1992" startWordPosition="3204" endWordPosition="3205"> right-corner nor left-corner parsing strategy by itself creates this ambiguity. The ambiguity arises from the decision to use this optionally arc-eager strategy to reduce memory store allocation in a bounded memory parser. Implementations of left-corner parsers such as that of Henderson (2004) adopt a arc-standard strategy, essentially always choosing analysis (b) above, and thus do not introduce this kind of local ambiguity. But in adopting this strategy, such parsers must maintain a stack memory of unbounded size, and thus are not attractive as models of human parsing in short-term memory (Resnik, 1992). 31 t=1 t=2 t=3 t=4 t=5 t=6 t=7 b) d=1 d=2 d=3 word t=1 t=2 t=3 t=4 t=5 t=6 t=7 a) d=1 d=2 d=3 word Figure 3: Alternative analyses of ‘strong demand for new york city ...’: a) using in-element composition, compatible with ‘strong demand for new york city is ...’ (in which the demand is for the city); and b) using cross-element (or delayed) composition, compatible with either ‘strong demand for new york city is ...’ (in which the demand is for the city) or ‘strong demand for new york city bonds is ...’ (in which a forthcoming referent — in this case, bonds — is associated with the city, and is</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Left-corner parsing and psychological plausibility. In Proceedings of COLING, pages 191–197, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Langauge Processing,</booktitle>
<pages>324--333</pages>
<contexts>
<context position="6865" citStr="Roark et al. (2009)" startWordPosition="1042" endWordPosition="1045">ing. This work has largely focused on correlating measures of parsing difficulty in computational models with delays in reading time in human subjects. Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and showed that these new metrics are strong predictors of reading difficulty. Wu et al. (2010) evaluate the same Hierarchical Hidden Markov Model parser used in this work in terms of its ability to reproduce human-like results for various complexity metrics, including some of those mentioned above, and introduce a new metric called embedding difference. This metric is based on the idea of embedding depth, which is the number of elements in the memory store</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. Proceedings of the 2009 Conference on Empirical Methods in Natural Langauge Processing, pages 324–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Toward a psycholinguistically-motivated model of language.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>785--792</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2251" citStr="Schuler et al., 2008" startWordPosition="320" endWordPosition="323">like errors. But from a human modeling perspective, the success of globally optimized whole-utterance models raises the question of how humans can accurately parse linguistic input without access to this same global optimization. This question creates a niche in computational research for models that are able to parse accurately while adhering as closely as possible to human-like psycholinguistic constraints. Recent work on incremental parsers includes work on Hierarchical Hidden Markov Model (HHMM) parsers that operate in linear time by maintaining a bounded store of incomplete constituents (Schuler et al., 2008). Despite this seeming limitation, corpus studies have shown that through the use of grammar transforms, this parser is able to cover nearly all sentences contained in the Penn Treebank (Marcus et al., 1993) using a small number of unconnected memory elements. But this bounded-memory parsing comes at a price. The HHMM parser obtains good coverage within human-like memory bounds only by pursuing an ‘optionally arc-eager’ parsing strategy, nondeterministically guessing which constituents can be kept open for attachment (occupying an active memory element), or closed for attachment (freeing a mem</context>
<context position="17553" citStr="Schuler et al., 2008" startWordPosition="2830" endWordPosition="2833">h left child category with the category of the resulting parent, within the same memory element (or depth level). For example, in Figure 3(a) a left-child category NP/NP at time t=4 is composed with a noun new of category NP/NNP (a noun phrase lacking a proper noun yet to come), resulting in a new parent category NP/NNP at time t=5 replacing the left child category NP/NP in the topmost d=1 memory element. This in-element composition preserves elements of the bounded memory store for use in processing descendants of this composed constituent, yielding the human-like memory demands reported in (Schuler et al., 2008). But whenever an in-element composition like this is hypothesized, it isolates an intermediate constituent (in this example, the noun phrase ‘new york city’) from subsequent composition. Allowing access to this intermediate constituent — for example, to allow ‘new york city’ to become a modifier of ‘bonds’, which itself becomes an argument of ‘for’ — requires an analysis in which the intermediate constituent is stored in a separate memory element, shown in Figure 3(b). This creates a local ambiguity in the parser (in this case, from time step t=4) that may have to be propagated across several</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2008</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2008. Toward a psycholinguistically-motivated model of language. In Proceedings of COLING, pages 785–792, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Broad-coverage incremental parsing using human-like memory constraints.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="9228" citStr="Schuler et al., 2010" startWordPosition="1421" endWordPosition="1424">vidence about how humans perceive these sentences word by word. However, as will be described below, the HHMM has advantages over the CHMM from a psycholinguistic modeling perspective. The HHMM uses a limited memory store containing only four elements which is consistent with many estimates of human short term memory limits (Cowan, 2001; Miller, 1956). In addition to modeling memory limits, the limited store acts as a fixed-depth stack that ensures linear asymptotic parsing time, and a grammar transform allows for wide coverage of speech and newspaper corpora within that limited memory store (Schuler et al., 2010). 3 Hierarchical Hidden Markov Model Parser Hidden Markov Models (HMMs) have long been used to successfully model sequence data in which there is a latent (hidden) variable at each time step that generates the observed evidence at that time step. These models are used for such applications as part-of-speech tagging, and speech recognition. Hierarchical Hidden Markov Models (HHMMs) are an extension of HMMs which can represent sequential data containing hierarchical relations. In HHMMs, complex hidden variables may output evidence for several time steps in sequence. This process may recurse, tho</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-coverage incremental parsing using human-like memory constraints. Computational Linguistics, 36(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Parsing with a bounded stack using a model-based right-corner transform.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL ’09),</booktitle>
<pages>344--352</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="16188" citStr="Schuler, 2009" startWordPosition="2608" endWordPosition="2609"> Figure 2: Mapping of schematized right-corner tree into HHMM memory elements. the most likely sequence, deterministically mapping that sequence back to a right-corner tree, and reversing the right-corner transform to produce an ordinary phrase structure tree. Unfortunately exact inference is not tractable with this model and dataset. The state space is too large to manage for both space and time reasons, and thus approximate inference is carried out, through the use of a beam search. At each time step, only the top N most probable hypothesized states are maintained. Experiments described in (Schuler, 2009) suggest that there does not seem to be much lost in going from exact inference using the CKY algorithm to a beam search with a relatively large width. However, the opposite experiment, examining the effect of going from a relatively wide beam to a very narrow beam has not been thoroughly studied in this parsing architecture. 4 Optionally Arc-eager Parsing The right-corner transform described in Section 3.1 saves memory because it transforms any right-expanding sequence with left-child subtrees into a left-expanding sequence of incomplete constituents, with the same sequence of subtrees as rig</context>
</contexts>
<marker>Schuler, 2009</marker>
<rawString>William Schuler. 2009. Parsing with a bounded stack using a model-based right-corner transform. In Proceedings of the North American Association for Computational Linguistics (NAACL ’09), pages 344–352, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wu</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>William Schuler</author>
</authors>
<title>Complexity metrics in an incremental right-corner parser.</title>
<date>2010</date>
<booktitle>In Proceedings of the 49th Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7099" citStr="Wu et al. (2010)" startWordPosition="1076" endWordPosition="1079"> the log ratio of the total probability mass at word t − 1 and word t. In other words, it measures how much probability was lost in incorporating the next word into the current hypotheses. Boston et al. (2008a) show that surprisal is a significant predictor of reading time (as measured in self-paced reading experiments) using a probabilistic dependency parser. Roark et al. (2009) dissected parsing difficulty metrics (including surprisal and entropy) to separate out the effects of syntactic and lexical difficulties, and showed that these new metrics are strong predictors of reading difficulty. Wu et al. (2010) evaluate the same Hierarchical Hidden Markov Model parser used in this work in terms of its ability to reproduce human-like results for various complexity metrics, including some of those mentioned above, and introduce a new metric called embedding difference. This metric is based on the idea of embedding depth, which is the number of elements in the memory store required to hold a given hypothesis. Using more memory elements corresponds to center embedding in phrase structure trees, and presumably correlates to some degree with complexity. Average embedding for a time step is computed by com</context>
</contexts>
<marker>Wu, Bachrach, Cardenas, Schuler, 2010</marker>
<rawString>Stephen Wu, Asaf Bachrach, Carlos Cardenas, and William Schuler. 2010. Complexity metrics in an incremental right-corner parser. In Proceedings of the 49th Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>