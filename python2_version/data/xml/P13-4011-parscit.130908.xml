<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010146">
<title confidence="0.862744">
Meet EDGAR, a tutoring agent at MONSERRATE
</title>
<author confidence="0.768611">
Pedro Fialho, Luisa Coheur, S´ergio Curto, Pedro Cl´audio
ˆAngela Costa, Alberto Abad, Hugo Meinedo and Isabel Trancoso
</author>
<affiliation confidence="0.682391">
Spoken Language Systems Lab (L2F), INESC-ID
</affiliation>
<address confidence="0.7170335">
Rua Alves Redol 9
1000-029 Lisbon, Portugal
</address>
<email confidence="0.993337">
name.surname@l2f.inesc-id.pt
</email>
<sectionHeader confidence="0.993756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999205782608696">
In this paper we describe a platform for
embodied conversational agents with tu-
toring goals, which takes as input written
and spoken questions and outputs answers
in both forms. The platform is devel-
oped within a game environment, and cur-
rently allows speech recognition and syn-
thesis in Portuguese, English and Spanish.
In this paper we focus on its understand-
ing component that supports in-domain in-
teractions, and also small talk. Most in-
domain interactions are answered using
different similarity metrics, which com-
pare the perceived utterances with ques-
tions/sentences in the agent’s knowledge
base; small-talk capabilities are mainly
due to AIML, a language largely used by
the chatbots’ community. In this paper
we also introduce EDGAR, the butler of
MONSERRATE, which was developed in
the aforementioned platform, and that an-
swers tourists’ questions about MONSER-
RATE.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999710466666667">
Several initiatives have been taking place in the
last years, targeting the concept of Edutainment,
that is, education through entertainment. Fol-
lowing this strategy, virtual characters have ani-
mated several museums all over the world: the
3D animated Hans Christian Andersen is ca-
pable of establishing multimodal conversations
about the writer’s life and tales (Bernsen and
Dybkjr, 2005), Max is a virtual character em-
ployed as guide in the Heinz Nixdorf Museums
Forum (Pfeiffer et al., 2011), and Sergeant Black-
well, installed in the Cooper-Hewitt National De-
sign Museum in New York, is used by the U.S.
Army Recruiting Command as a hi-tech attrac-
tion and information source (Robinson et al.,
</bodyText>
<figureCaption confidence="0.998902">
Figure 1: EDGAR at MONSERRATE.
</figureCaption>
<bodyText confidence="0.99966625">
2008). DuARTE Digital (Mendes et al., 2009)
and EDGAR are also examples of virtual charac-
ters for the Portuguese language with the same
edutainment goal: DuARTE Digital answers ques-
tions about Cust´odia de Bel´em, a famous work of
the Portuguese jewelry; EDGAR is a virtual butler
that answers questions about MONSERRATE (Fig-
ure 1).
Considering the previous mentioned agents,
they all cover a specific domain of knowledge (al-
though a general Question/Answering system was
integrated in Max (Waltinger et al., 2011)). How-
ever, as expected, people tend also to make small
talk when interacting with these agents. There-
fore, it is important that these systems properly
deal with it. Several strategies are envisaged to
this end and EDGAR is of no exception. In this
paper, we describe the platform behind EDGAR,
which we developed aiming at the fast insertion of
in-domain knowledge, and to deal with small talk.
This platform is currently in the process of being
industrially applied by a company known for its
expertise in building and deploying kiosks. We
will provide the hardware and software required
to demonstrate EDGAR, both on a computer and
on a tablet.
This paper is organized as follows: in Sec-
tion 2 we present EDGAR’s development platform
</bodyText>
<page confidence="0.992561">
61
</page>
<note confidence="0.626605">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 61–66,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999477">
Figure 2: EDGAR architecture
</figureCaption>
<bodyText confidence="0.999990333333333">
and describe typical interactions, in Section 3 we
show how we move from in-domain interactions
to small talk, and in Section 4 we present an anal-
ysis on collected logs and their initial evaluation
results. Finally, in Section 5 we present some con-
clusions and point to future work.
</bodyText>
<sectionHeader confidence="0.9918875" genericHeader="method">
2 The Embodied Conversational Agent
platform
</sectionHeader>
<subsectionHeader confidence="0.999932">
2.1 Architecture overview
</subsectionHeader>
<bodyText confidence="0.999931882352941">
The architecture of the platform, generally de-
signed for the development of Embodied Con-
versational Agents (ECAs) (such as EDGAR), is
shown in Figure 2. In this platform, several mod-
ules intercommunicate by means of well defined
protocols, thus leveraging the capabilities of inde-
pendent modules focused on specific tasks, such
as speech recognition or 3D rendering/animation.
This independence allows us to use subsets of this
platform modules in scenarios with different re-
quirements (for instance, we can record characters
uttering a text).
Design and deployment of the front end of
EDGAR is performed in a game engine, which has
enabled the use of computer graphics technologies
and high quality assets, as seen in the video game
industry.
</bodyText>
<subsectionHeader confidence="0.998632">
2.2 Multimodal components
</subsectionHeader>
<bodyText confidence="0.673112666666667">
The game environment, where all the interac-
tion with EDGAR takes place, is developed in the
Unity1 platform, being composed of one highly
</bodyText>
<footnote confidence="0.960796">
1http://unity3d.com/
</footnote>
<bodyText confidence="0.998450333333333">
detailed character, made and animated by Rocket-
box studios2, a virtual keyboard and a push-while-
talking button.
In this platform, Automatic Speech Recogni-
tion (ASR) is performed by AUDIMUS (Meinedo
et al., 2003) for all languages, using generic acous-
tic and language models, recently compiled from
broadcast news data (Meinedo et al., 2010). Lan-
guage models were interpolated with all the do-
main questions defined in the Natural Language
Understanding (NLU) framework (see below),
while ASR includes features such as speech/non-
speech (SNS) detection and automatic gain control
(AGC). Speech captured in a public space raises
several ASR robustness issues, such as loudness
variability of spoken utterances, which is partic-
ularly bound to happen in a museological envi-
ronment (such as MONSERRATE) where silence is
usually incited. Thus, we have added a bounded
amplication to the captured signal, despite the
AGC mechanism, ensuring that too silent sounds
are not discarded by the SNS mechanism.
Upon a spoken input, AUDIMUS translates it
into a sentence, with a confidence value. An
empty recognition result, or one with low con-
fidence, triggers a control tag (“ REPEAT ”) to
the NLU module, which results in a request for
the user to repeat what was said. The answer re-
turned by the NLU module is synthesized in a lan-
guage dependent Text To Speech (TTS) system,
with DIXI (Paulo et al., 2008) being used for Por-
tuguese, while a recent version of FESTIVAL (Zen
et al., 2009) covers both English and Spanish. The
</bodyText>
<footnote confidence="0.967573">
2http://www.rocketbox-libraries.com/
</footnote>
<page confidence="0.998607">
62
</page>
<bodyText confidence="0.993809">
synthesized audio is played while the correspond-
ing phonemes are mapped into visemes, repre-
sented as skeletal animations, being synchronized
according to phoneme durations, available in all
the employed TTS engines.
Emotions are declared in the knowledge sources
of the agent. As shown in Figure 3, they are coor-
dinated with viseme animations.
</bodyText>
<figureCaption confidence="0.989387">
Figure 3: The EDGAR character in a joyful state.
</figureCaption>
<subsectionHeader confidence="0.99921">
2.3 Interacting with EDGAR
</subsectionHeader>
<bodyText confidence="0.9972312">
In a typical interaction, the user enters a ques-
tion with a virtual keyboard or says it to the mi-
crophone while pressing a button (Figure 4), in
the language chosen in the interface (as previously
said, Portuguese, English or Spanish).
</bodyText>
<figureCaption confidence="0.954756">
Figure 4: A question written in the EDGAR inter-
face.
</figureCaption>
<bodyText confidence="0.99989925">
Then, the ASR will transcribe it and the NLU
module will process it. Afterwards, the answer,
chosen by the NLU module, is heard through
the speakers, due to the TTS, and sequentially
written in a talk bubble, according to the pro-
duced speech. The answer is accompanied with
visemes, represented by movements of the char-
acter’s mouth/lips, and by facial emotions as
marked in the answers of the NLU knowledge
base. A demo of EDGAR, only for English interac-
tions, can be tested in https://edgar.l2f.
inesc-id.pt/m3/edgar.php.
</bodyText>
<sectionHeader confidence="0.907659" genericHeader="method">
3 The natural language understanding
component
</sectionHeader>
<subsectionHeader confidence="0.999728">
3.1 In-domain knowledge sources
</subsectionHeader>
<bodyText confidence="0.999473133333333">
The in-domain knowledge sources of the agent
are XML files, hand-crafted by domain experts.
This XML files have multilingual pairs consti-
tuted by different paraphrases of the same ques-
tion and possible answers. The main reason to
follow this approach (and contrary to other works
where grammars are used), is to ease the process
of creating/enriching the knowledge sources of the
agent being developed, which is typically done
by non experts in linguistics or computer science.
Thus, we opted for following a similar approach
of the work described, for instance, in (Leuski et
al., 2006), where the agents knowledge sources are
easy to create and maintain. An example of a ques-
tions/answers pair is:
</bodyText>
<figure confidence="0.336236454545455">
&lt;questions&gt;
&lt;q en=&amp;quot;How is everything?&amp;quot;
es=&amp;quot;Todo bien?&amp;quot;&gt;
Tudo bem?&lt;/q&gt;
&lt;/questions&gt;
&lt;answers&gt;
&lt;a en=&amp;quot;I am ok, thank you.&amp;quot;
es=&amp;quot;Estoy bien, gracias.&amp;quot;
emotion=&amp;quot;smile_02&amp;quot;&gt;
Estou bem, obrigado.&lt;/a&gt;
&lt;/answers&gt;
</figure>
<bodyText confidence="0.999788625">
As it can been see from this example, emotions
are defined in these files, associated to each ques-
tion/answer pair (emotion=“smile” in the exam-
ple, one of the possible smile emotions).
These knowledge sources can be (automati-
cally) extended with “synonyms”. We call them
“synonyms”, because they do not necessarily fit
in the usual definition of synonyms. Here we fol-
low a broader approach to this concept and if two
words, within the context of a sentence from the
knowledge source, will lead to the same answer,
then we consider them to be “synonyms”. For
instance “palace” or “castle” are not synonyms.
However, people tend to refer to MONSERRATE in
both forms. Thus, we consider them to be “syn-
onyms” and if one of these is used in the orig-
inal knowledge sources, the other is used to ex-
pand them. It should be clear that we will gener-
ate many incorrect questions with this procedure,
but empirical tests (out of the scope of this paper)
show that these questions do not hurt the system
performance. Moreover, they are useful for ASR
language model interpolation, which is based on
N-grams.
</bodyText>
<page confidence="0.996708">
63
</page>
<subsectionHeader confidence="0.996905">
3.2 Out-of-domain knowledge sources
</subsectionHeader>
<bodyText confidence="0.999714075">
The same format of the previously described
knowledge sources can be used to represent out-
of-domain knowledge. Here, we extensively used
the “synonyms” approach. For instance, words
wife and girlfriend are considered to be “syn-
onyms” as all the personal questions with these
words should be answered with the same sentence:
I do not want to talk about my private life.
Nevertheless, and taking into consideration the
work around small talk developed by the chat-
bots community (Klwer, 2011), we decided to
use the most popular language to build chat-
bots: the “Artificial Intelligence Markup Lan-
guage”, widely known as AIML, a derivative of
XML. With AIML, knowledge is coded as a set
of rules that will match the user input, associ-
ated with templates, the generators of the out-
put. A detailed description of AIML syntax can
be found in http://www.alicebot.org/
aiml.html. In what respects AIML inter-
preters, we opted to use Program D (java), which
we integrated in our platform. Currently, we use
AIML to deal with slang and to answer questions
that have to do with cinema and compliments.
As a curiosity, we should explain that we deal
with slang when input came from the keyboard,
and not when it is speech, as the language models
are not trained with this specific lexicon. The rea-
son we do that is because if the language models
were trained with slang, it would be possible to er-
roneously detect it in utterances and then answer
them accordingly, which could be extremely un-
pleasant. Therefore, EDGAR only deals with slang
when the input is the keyboard.
The current knowledge sources have 152 ques-
tion/answer pairs, corresponding to 763 questions
and 206 answers. For Portuguese, English and
Spanish the use of 226, 219 and 53 synonym re-
lations, led to the generation of 22 194, 16 378
and 1 716 new questions, respectively.
</bodyText>
<subsectionHeader confidence="0.999128">
3.3 Finding the appropriate answer
</subsectionHeader>
<bodyText confidence="0.999986289473684">
The NLU module is responsible for the answer se-
lection process. It has three main components.
The first one, STRATEGIES, is responsible to
choose an appropriate answer to the received inter-
action. Several strategies are implemented, includ-
ing the ones based on string matching, string dis-
tances (as for instance, Levenshtein, Jaccard and
Dice), N-gram Overlap and support vector ma-
chines (seeing the answer selection as a classifica-
tion problem). Currently, best results are attained
using a combination of Jaccard and bigram Over-
lap measures and word weight through the use of
tf-idf statistic. In this case, Jaccard takes into ac-
count how many words are shared between the
user’s interaction and the knowledge source en-
try, bigram Overlap gives preference to the shared
sequences of words and tf-idf contributes to the
results attained by previous measures, by given
weight to unfrequent words, which should have
more weight on the decision process (for example,
the word MONSERRATE occurs in the majority of
the questions in the corpus, so it is not very infor-
mative and should not have the same weight as, for
instance, the word architect or owner).
The second component, PLUGINS, deals with
two different situations. First, it accesses Pro-
gram D when interactions are not answered by the
STRATEGIES component. That is, when the tech-
nique used by STRATEGIES returns a value that
is lower than a threshold (dependent of the used
technique), the PLUGIN component runs Program
D in order to try to find an answer to the posed
question. Secondly, when the ASR has no confi-
dence of the attained transcription (and returns the
“ REPEAT ” tag) or Program D is not able to find
an answer, the PLUGINS component does the fol-
lowing (with the goal of taking the user again to
the agent topic of expertise):
</bodyText>
<listItem confidence="0.950360933333333">
• In the first time that this occurs, a sentence
such as Sorry, I did not understand you. is
chosen as the answer to be returned.
• The second time this occurs, EDGAR asks the
user I did not understand you again. Why
don’t you ask me X?, being X generated in
run time and being a question from a subset
of the questions from the knowledge sources.
Obviously, only in-domain (not expanded)
questions are considered for replacing X.
• The third time there is a misunderstanding,
EDGAR says We are not understanding each
other, let me talk about MONSERRATE. And
it randomly choses some answer to present to
the user.
</listItem>
<bodyText confidence="0.9998465">
The third component is the HISTORY-
TRACKER, which handles the agent knowledge
about previous interactions (kept until a default
time without interactions is reached).
</bodyText>
<page confidence="0.999351">
64
</page>
<sectionHeader confidence="0.954317" genericHeader="method">
4 Preliminary evaluation
</sectionHeader>
<bodyText confidence="0.9999719">
Edgar is more a domain-specific Question An-
swering (QA) than a task-oriented dialogue sys-
tem. Therefore, we evaluated it with the metrics
typically used in QA. The mapping of the dif-
ferent situations in true/false positives/negatives is
explained in the following.
We have manually transcribed 1086 spoken ut-
terances (in Portuguese), which were then labeled
with the following tags, some depending on the
answer given by EDGAR:
</bodyText>
<listItem confidence="0.99702844">
• 0: in-domain question incorrectly answered,
although there was information in the knowl-
edge sources (excluding Program D) to an-
swer it;
• 1: out-of-domain question, incorrectly an-
swered;
• 2: question correctly answered by Program
D;
• 3: question correctly answered by using
knowledge sources (excluding Program D);
• 4: in-domain question, incorrectly answered.
There is no information in the knowledge
source to answer it, but it should be;
• 5: multiple questions, partially answered;
• 6: multiple questions, unanswered;
• 7: question with implicit information (there,
him, etc.), unanswered;
• 8: question which is not “ipsis verbis” in the
knowledge source, but has a paraphrase there
and was not correctly answered;
• 9: question with a single word (garden,
palace), unanswered;
• 10: question that we do not want the system
to answer (some were answered, some were
not).
</listItem>
<bodyText confidence="0.95915">
The previous tags were mapped into:
</bodyText>
<listItem confidence="0.9982838">
• true positives: questions marked with 2, 3
and 5;
• true negatives: questions marked with 0 and
10 (the ones that were not answered by the
system);
• false positives: questions marked with 0 and
10 (the ones that were answered by the sys-
tem);
• false negatives: questions marked with 4, 6,
7, 8 and 9.
</listItem>
<bodyText confidence="0.99948475">
Then, two experiments were conducted: in the
first, the NLU module was applied to the manual
transcriptions; in the second, directly to the output
of the ASR. Table 1 shows the results.
</bodyText>
<table confidence="0.925457666666667">
NLU input = manual transcriptions
Precision Recall F-measure
0.92 0.60 0.72
acNLU input = ASR
Precision Recall F-measure
0.71 0.32 0.45
</table>
<tableCaption confidence="0.999539">
Table 1: NLU results
</tableCaption>
<bodyText confidence="0.99993280952381">
The ASR Word Error Rate (WER) is of 70%.
However, we detect some problems in the way we
were collecting the audio, and in more recent eval-
uations (by using 363 recent logs where previous
problems were corrected), that error decreased to a
WER of 52%, including speech from 111 children,
21 non native Portuguese speakers (thus, with a
different pronunciation), 23 individuals not talking
in Portuguese and 27 interactions where multiple
speakers overlap. Here, we should refer the work
presented in (Traum et al., 2012), where an eval-
uation of two virtual guides in a museum is pre-
sented. They also had to deal with speakers from
different ages and with question off-topic, and re-
port a ASR with 57% WER (however they major-
ity of their user are children: 76%).
We are currently preparing a new corpus for
evaluating the NLU module, however, the follow-
ing results remain: in the best scenario, if tran-
scription is perfect, the NLU module behaves as
indicated in Table 1 (manual transcriptions).
</bodyText>
<sectionHeader confidence="0.998745" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999676625">
We have described a platform for developing
ECAs with tutoring goals, that takes both speech
and text as input and output, and introduced
EDGAR, the butler of MONSERRATE, which was
developed in that platform. Special attention was
given to EDGAR’s NLU module, which couples
techniques that try to find distances between the
user input and sentences in the existing knowledge
</bodyText>
<page confidence="0.998728">
65
</page>
<bodyText confidence="0.996955266666667">
sources, with a framework imported from the chat-
bots community (AIML plus Program D). EDGAR
has been tested with real users for the last year and
we are currently performing a detailed evaluation
of it. There is much work to be done, including to
be able to deal with language varieties, which is an
important source of recognition errors. Moreover,
the capacity of dealing with out-of-domain ques-
tions is still a hot research topic and one of our
priorities in the near future. We have testified that
people are delighted when EDGAR answers out-
of-domain questions (Do you like soccer?/I rather
have a tea and read a good criminal book) and we
cannot forget that entertainment is also one of this
Embodied Conversational Agent (ECA)’s goal.
</bodyText>
<sectionHeader confidence="0.998553" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994812714285714">
This work was supported by national
funds through FCT – Fundac¸˜ao para a
Ciˆencia e a Tecnologia, under project PEst-
OE/EEI/LA0021/2013. Pedro Fialho, S´ergio
Curto and Pedro Cl´audio scholarships were sup-
ported under project FALACOMIGO (ProjectoVII
em co-promoc¸˜ao, QREN n 13449).
</bodyText>
<sectionHeader confidence="0.99684" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995705816901409">
N. O. Bernsen and L. Dybkjr. 2005. Meet hans chris-
tian andersen. In In Proceedings of Sixth SIGdial
Workshop on Discourse and Dialogue, pages 237–
241.
Tina Klwer. 2011. “i like your shirt” – dialogue acts
for enabling social talk in conversational agents. In
Proceedings of the 11th International Conference on
Intelligent Virtual Agents. International Conference
on Intelligent Virtual Agents (IVA), 11th, September
17-19, Reykjavik, Iceland. Springer.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In 7th SIGdial Workshop
on Discourse and Dialogue, Sydney, Australia.
Hugo Meinedo, Diamantino Caseiro, Jo˜ao Neto, and
Isabel Trancoso. 2003. Audimus.media: a broad-
cast news speech recognition system for the euro-
pean portuguese language. In Proceedings of the 6th
international conference on Computational process-
ing of the Portuguese language, PROPOR’03, pages
9–17, Berlin, Heidelberg. Springer-Verlag.
H. Meinedo, A. Abad, T. Pellegrini, I. Trancoso, and
J. P. Neto. 2010. The l2f broadcast news speech
recognition system. In Proceedings of Fala2010,
Vigo, Spain.
Ana Cristina Mendes, Rui Prada, and Lu´ısa Coheur.
2009. Adapting a virtual agent to users’ vocabu-
lary and needs. In Proceedings of the 9th Interna-
tional Conference on Intelligent Virtual Agents, IVA
’09, pages 529–530, Berlin, Heidelberg. Springer-
Verlag.
S´ergio Paulo, Lu´ıs C. Oliveira, Carlos Mendes, Lu´ıs
Figueira, Renato Cassaca, C´eu Viana, and Helena
Moniz. 2008. Dixi — a generic text-to-speech sys-
tem for european portuguese. In Proceedings of the
8th international conference on Computational Pro-
cessing of the Portuguese Language, PROPOR ’08,
pages 91–100, Berlin, Heidelberg. Springer-Verlag.
Thies Pfeiffer, Christian Liguda, Ipke Wachsmuth, and
Stefan Stein. 2011. Living with a virtual agent:
Seven years with an embodied conversational agent
at the heinz nixdorf museumsforum. In Proceedings
of the International Conference Re-Thinking Tech-
nology in Museums 2011 - Emerging Experiences,
pages 121 – 131. thinkk creative &amp; the University of
Limerick.
Susan Robinson, David Traum, Midhun Ittycheriah,
and Joe Henderer. 2008. What would you ask a
conversational agent? observations of human-agent
dialogues in a museum setting. In International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
Ulli Waltinger, Alexa Breuing, and Ipke Wachsmuth.
2011. Interfacing virtual agents with collaborative
knowledge: Open domain question answering us-
ing wikipedia-based topic models. In IJCAI, pages
1896–1902.
Heiga Zen, Keiichiro Oura, Takashi Nose, Junichi Ya-
magishi, Shinji Sako, Tomoki Toda, Takashi Ma-
suko, Alan W. Black, and Keiichi Tokuda. 2009.
Recent development of the HMM-based speech syn-
thesis system (HTS). In Proc. 2009 Asia-Pacific
Signal and Information Processing Association (AP-
SIPA), Sapporo, Japan, October.
</reference>
<page confidence="0.989006">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.081944">
<title confidence="0.981197">a tutoring agent at</title>
<author confidence="0.742818">Pedro Fialho</author>
<author confidence="0.742818">Luisa Coheur</author>
<author confidence="0.742818">S´ergio Curto</author>
<author confidence="0.742818">Pedro Costa</author>
<author confidence="0.742818">Alberto Abad</author>
<author confidence="0.742818">Hugo Meinedo</author>
<author confidence="0.742818">Isabel</author>
<affiliation confidence="0.468252">Spoken Language Systems Lab (L2F),</affiliation>
<address confidence="0.554793">Rua Alves Redol 1000-029 Lisbon,</address>
<email confidence="0.90151">name.surname@l2f.inesc-id.pt</email>
<abstract confidence="0.976618695652174">In this paper we describe a platform for embodied conversational agents with tutoring goals, which takes as input written and spoken questions and outputs answers in both forms. The platform is developed within a game environment, and currently allows speech recognition and synthesis in Portuguese, English and Spanish. In this paper we focus on its understanding component that supports in-domain interactions, and also small talk. Most indomain interactions are answered using different similarity metrics, which compare the perceived utterances with questions/sentences in the agent’s knowledge base; small-talk capabilities are mainly due to AIML, a language largely used by the chatbots’ community. In this paper also introduce the butler of which was developed in the aforementioned platform, and that antourists’ questions about</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N O Bernsen</author>
<author>L Dybkjr</author>
</authors>
<title>Meet hans christian andersen. In</title>
<date>2005</date>
<booktitle>In Proceedings of Sixth SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>237--241</pages>
<contexts>
<context position="1569" citStr="Bernsen and Dybkjr, 2005" startWordPosition="232" endWordPosition="235">language largely used by the chatbots’ community. In this paper we also introduce EDGAR, the butler of MONSERRATE, which was developed in the aforementioned platform, and that answers tourists’ questions about MONSERRATE. 1 Introduction Several initiatives have been taking place in the last years, targeting the concept of Edutainment, that is, education through entertainment. Following this strategy, virtual characters have animated several museums all over the world: the 3D animated Hans Christian Andersen is capable of establishing multimodal conversations about the writer’s life and tales (Bernsen and Dybkjr, 2005), Max is a virtual character employed as guide in the Heinz Nixdorf Museums Forum (Pfeiffer et al., 2011), and Sergeant Blackwell, installed in the Cooper-Hewitt National Design Museum in New York, is used by the U.S. Army Recruiting Command as a hi-tech attraction and information source (Robinson et al., Figure 1: EDGAR at MONSERRATE. 2008). DuARTE Digital (Mendes et al., 2009) and EDGAR are also examples of virtual characters for the Portuguese language with the same edutainment goal: DuARTE Digital answers questions about Cust´odia de Bel´em, a famous work of the Portuguese jewelry; EDGAR i</context>
</contexts>
<marker>Bernsen, Dybkjr, 2005</marker>
<rawString>N. O. Bernsen and L. Dybkjr. 2005. Meet hans christian andersen. In In Proceedings of Sixth SIGdial Workshop on Discourse and Dialogue, pages 237– 241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tina Klwer</author>
</authors>
<title>i like your shirt” – dialogue acts for enabling social talk in conversational agents.</title>
<date>2011</date>
<booktitle>In Proceedings of the 11th International Conference on Intelligent Virtual Agents. International Conference on Intelligent Virtual Agents (IVA), 11th,</booktitle>
<publisher>Springer.</publisher>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="10048" citStr="Klwer, 2011" startWordPosition="1606" endWordPosition="1607">r, they are useful for ASR language model interpolation, which is based on N-grams. 63 3.2 Out-of-domain knowledge sources The same format of the previously described knowledge sources can be used to represent outof-domain knowledge. Here, we extensively used the “synonyms” approach. For instance, words wife and girlfriend are considered to be “synonyms” as all the personal questions with these words should be answered with the same sentence: I do not want to talk about my private life. Nevertheless, and taking into consideration the work around small talk developed by the chatbots community (Klwer, 2011), we decided to use the most popular language to build chatbots: the “Artificial Intelligence Markup Language”, widely known as AIML, a derivative of XML. With AIML, knowledge is coded as a set of rules that will match the user input, associated with templates, the generators of the output. A detailed description of AIML syntax can be found in http://www.alicebot.org/ aiml.html. In what respects AIML interpreters, we opted to use Program D (java), which we integrated in our platform. Currently, we use AIML to deal with slang and to answer questions that have to do with cinema and compliments. </context>
</contexts>
<marker>Klwer, 2011</marker>
<rawString>Tina Klwer. 2011. “i like your shirt” – dialogue acts for enabling social talk in conversational agents. In Proceedings of the 11th International Conference on Intelligent Virtual Agents. International Conference on Intelligent Virtual Agents (IVA), 11th, September 17-19, Reykjavik, Iceland. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>Ronakkumar Patel</author>
<author>David Traum</author>
<author>Brandon Kennedy</author>
</authors>
<title>Building effective question answering characters.</title>
<date>2006</date>
<booktitle>In 7th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="8109" citStr="Leuski et al., 2006" startWordPosition="1286" endWordPosition="1289"> 3.1 In-domain knowledge sources The in-domain knowledge sources of the agent are XML files, hand-crafted by domain experts. This XML files have multilingual pairs constituted by different paraphrases of the same question and possible answers. The main reason to follow this approach (and contrary to other works where grammars are used), is to ease the process of creating/enriching the knowledge sources of the agent being developed, which is typically done by non experts in linguistics or computer science. Thus, we opted for following a similar approach of the work described, for instance, in (Leuski et al., 2006), where the agents knowledge sources are easy to create and maintain. An example of a questions/answers pair is: &lt;questions&gt; &lt;q en=&amp;quot;How is everything?&amp;quot; es=&amp;quot;Todo bien?&amp;quot;&gt; Tudo bem?&lt;/q&gt; &lt;/questions&gt; &lt;answers&gt; &lt;a en=&amp;quot;I am ok, thank you.&amp;quot; es=&amp;quot;Estoy bien, gracias.&amp;quot; emotion=&amp;quot;smile_02&amp;quot;&gt; Estou bem, obrigado.&lt;/a&gt; &lt;/answers&gt; As it can been see from this example, emotions are defined in these files, associated to each question/answer pair (emotion=“smile” in the example, one of the possible smile emotions). These knowledge sources can be (automatically) extended with “synonyms”. We call them “synonyms”, b</context>
</contexts>
<marker>Leuski, Patel, Traum, Kennedy, 2006</marker>
<rawString>Anton Leuski, Ronakkumar Patel, David Traum, and Brandon Kennedy. 2006. Building effective question answering characters. In 7th SIGdial Workshop on Discourse and Dialogue, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Meinedo</author>
<author>Diamantino Caseiro</author>
<author>Jo˜ao Neto</author>
<author>Isabel Trancoso</author>
</authors>
<title>Audimus.media: a broadcast news speech recognition system for the european portuguese language.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th international conference on Computational processing of the Portuguese language, PROPOR’03,</booktitle>
<pages>9--17</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4869" citStr="Meinedo et al., 2003" startWordPosition="760" endWordPosition="763">acters uttering a text). Design and deployment of the front end of EDGAR is performed in a game engine, which has enabled the use of computer graphics technologies and high quality assets, as seen in the video game industry. 2.2 Multimodal components The game environment, where all the interaction with EDGAR takes place, is developed in the Unity1 platform, being composed of one highly 1http://unity3d.com/ detailed character, made and animated by Rocketbox studios2, a virtual keyboard and a push-whiletalking button. In this platform, Automatic Speech Recognition (ASR) is performed by AUDIMUS (Meinedo et al., 2003) for all languages, using generic acoustic and language models, recently compiled from broadcast news data (Meinedo et al., 2010). Language models were interpolated with all the domain questions defined in the Natural Language Understanding (NLU) framework (see below), while ASR includes features such as speech/nonspeech (SNS) detection and automatic gain control (AGC). Speech captured in a public space raises several ASR robustness issues, such as loudness variability of spoken utterances, which is particularly bound to happen in a museological environment (such as MONSERRATE) where silence i</context>
</contexts>
<marker>Meinedo, Caseiro, Neto, Trancoso, 2003</marker>
<rawString>Hugo Meinedo, Diamantino Caseiro, Jo˜ao Neto, and Isabel Trancoso. 2003. Audimus.media: a broadcast news speech recognition system for the european portuguese language. In Proceedings of the 6th international conference on Computational processing of the Portuguese language, PROPOR’03, pages 9–17, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Meinedo</author>
<author>A Abad</author>
<author>T Pellegrini</author>
<author>I Trancoso</author>
<author>J P Neto</author>
</authors>
<title>The l2f broadcast news speech recognition system.</title>
<date>2010</date>
<booktitle>In Proceedings of Fala2010,</booktitle>
<location>Vigo,</location>
<contexts>
<context position="4998" citStr="Meinedo et al., 2010" startWordPosition="780" endWordPosition="783"> of computer graphics technologies and high quality assets, as seen in the video game industry. 2.2 Multimodal components The game environment, where all the interaction with EDGAR takes place, is developed in the Unity1 platform, being composed of one highly 1http://unity3d.com/ detailed character, made and animated by Rocketbox studios2, a virtual keyboard and a push-whiletalking button. In this platform, Automatic Speech Recognition (ASR) is performed by AUDIMUS (Meinedo et al., 2003) for all languages, using generic acoustic and language models, recently compiled from broadcast news data (Meinedo et al., 2010). Language models were interpolated with all the domain questions defined in the Natural Language Understanding (NLU) framework (see below), while ASR includes features such as speech/nonspeech (SNS) detection and automatic gain control (AGC). Speech captured in a public space raises several ASR robustness issues, such as loudness variability of spoken utterances, which is particularly bound to happen in a museological environment (such as MONSERRATE) where silence is usually incited. Thus, we have added a bounded amplication to the captured signal, despite the AGC mechanism, ensuring that too</context>
</contexts>
<marker>Meinedo, Abad, Pellegrini, Trancoso, Neto, 2010</marker>
<rawString>H. Meinedo, A. Abad, T. Pellegrini, I. Trancoso, and J. P. Neto. 2010. The l2f broadcast news speech recognition system. In Proceedings of Fala2010, Vigo, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Cristina Mendes</author>
<author>Rui Prada</author>
<author>Lu´ısa Coheur</author>
</authors>
<title>Adapting a virtual agent to users’ vocabulary and needs.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Virtual Agents, IVA ’09,</booktitle>
<pages>529--530</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1950" citStr="Mendes et al., 2009" startWordPosition="297" endWordPosition="300">ng this strategy, virtual characters have animated several museums all over the world: the 3D animated Hans Christian Andersen is capable of establishing multimodal conversations about the writer’s life and tales (Bernsen and Dybkjr, 2005), Max is a virtual character employed as guide in the Heinz Nixdorf Museums Forum (Pfeiffer et al., 2011), and Sergeant Blackwell, installed in the Cooper-Hewitt National Design Museum in New York, is used by the U.S. Army Recruiting Command as a hi-tech attraction and information source (Robinson et al., Figure 1: EDGAR at MONSERRATE. 2008). DuARTE Digital (Mendes et al., 2009) and EDGAR are also examples of virtual characters for the Portuguese language with the same edutainment goal: DuARTE Digital answers questions about Cust´odia de Bel´em, a famous work of the Portuguese jewelry; EDGAR is a virtual butler that answers questions about MONSERRATE (Figure 1). Considering the previous mentioned agents, they all cover a specific domain of knowledge (although a general Question/Answering system was integrated in Max (Waltinger et al., 2011)). However, as expected, people tend also to make small talk when interacting with these agents. Therefore, it is important that </context>
</contexts>
<marker>Mendes, Prada, Coheur, 2009</marker>
<rawString>Ana Cristina Mendes, Rui Prada, and Lu´ısa Coheur. 2009. Adapting a virtual agent to users’ vocabulary and needs. In Proceedings of the 9th International Conference on Intelligent Virtual Agents, IVA ’09, pages 529–530, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ergio Paulo</author>
<author>Lu´ıs C Oliveira</author>
<author>Carlos Mendes</author>
<author>Lu´ıs Figueira</author>
<author>Renato Cassaca</author>
<author>C´eu Viana</author>
<author>Helena Moniz</author>
</authors>
<title>Dixi — a generic text-to-speech system for european portuguese.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th international conference on Computational Processing of the Portuguese Language, PROPOR ’08,</booktitle>
<pages>91--100</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="6051" citStr="Paulo et al., 2008" startWordPosition="955" endWordPosition="958"> (such as MONSERRATE) where silence is usually incited. Thus, we have added a bounded amplication to the captured signal, despite the AGC mechanism, ensuring that too silent sounds are not discarded by the SNS mechanism. Upon a spoken input, AUDIMUS translates it into a sentence, with a confidence value. An empty recognition result, or one with low confidence, triggers a control tag (“ REPEAT ”) to the NLU module, which results in a request for the user to repeat what was said. The answer returned by the NLU module is synthesized in a language dependent Text To Speech (TTS) system, with DIXI (Paulo et al., 2008) being used for Portuguese, while a recent version of FESTIVAL (Zen et al., 2009) covers both English and Spanish. The 2http://www.rocketbox-libraries.com/ 62 synthesized audio is played while the corresponding phonemes are mapped into visemes, represented as skeletal animations, being synchronized according to phoneme durations, available in all the employed TTS engines. Emotions are declared in the knowledge sources of the agent. As shown in Figure 3, they are coordinated with viseme animations. Figure 3: The EDGAR character in a joyful state. 2.3 Interacting with EDGAR In a typical interact</context>
</contexts>
<marker>Paulo, Oliveira, Mendes, Figueira, Cassaca, Viana, Moniz, 2008</marker>
<rawString>S´ergio Paulo, Lu´ıs C. Oliveira, Carlos Mendes, Lu´ıs Figueira, Renato Cassaca, C´eu Viana, and Helena Moniz. 2008. Dixi — a generic text-to-speech system for european portuguese. In Proceedings of the 8th international conference on Computational Processing of the Portuguese Language, PROPOR ’08, pages 91–100, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thies Pfeiffer</author>
<author>Christian Liguda</author>
<author>Ipke Wachsmuth</author>
<author>Stefan Stein</author>
</authors>
<title>Living with a virtual agent: Seven years with an embodied conversational agent at the heinz nixdorf museumsforum.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference Re-Thinking Technology in Museums 2011 - Emerging Experiences, pages 121 – 131. thinkk creative &amp; the</booktitle>
<institution>University of Limerick.</institution>
<contexts>
<context position="1674" citStr="Pfeiffer et al., 2011" startWordPosition="251" endWordPosition="254">RRATE, which was developed in the aforementioned platform, and that answers tourists’ questions about MONSERRATE. 1 Introduction Several initiatives have been taking place in the last years, targeting the concept of Edutainment, that is, education through entertainment. Following this strategy, virtual characters have animated several museums all over the world: the 3D animated Hans Christian Andersen is capable of establishing multimodal conversations about the writer’s life and tales (Bernsen and Dybkjr, 2005), Max is a virtual character employed as guide in the Heinz Nixdorf Museums Forum (Pfeiffer et al., 2011), and Sergeant Blackwell, installed in the Cooper-Hewitt National Design Museum in New York, is used by the U.S. Army Recruiting Command as a hi-tech attraction and information source (Robinson et al., Figure 1: EDGAR at MONSERRATE. 2008). DuARTE Digital (Mendes et al., 2009) and EDGAR are also examples of virtual characters for the Portuguese language with the same edutainment goal: DuARTE Digital answers questions about Cust´odia de Bel´em, a famous work of the Portuguese jewelry; EDGAR is a virtual butler that answers questions about MONSERRATE (Figure 1). Considering the previous mentioned</context>
</contexts>
<marker>Pfeiffer, Liguda, Wachsmuth, Stein, 2011</marker>
<rawString>Thies Pfeiffer, Christian Liguda, Ipke Wachsmuth, and Stefan Stein. 2011. Living with a virtual agent: Seven years with an embodied conversational agent at the heinz nixdorf museumsforum. In Proceedings of the International Conference Re-Thinking Technology in Museums 2011 - Emerging Experiences, pages 121 – 131. thinkk creative &amp; the University of Limerick.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Robinson</author>
<author>David Traum</author>
<author>Midhun Ittycheriah</author>
<author>Joe Henderer</author>
</authors>
<title>What would you ask a conversational agent? observations of human-agent dialogues in a museum setting.</title>
<date>2008</date>
<booktitle>In International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Robinson, Traum, Ittycheriah, Henderer, 2008</marker>
<rawString>Susan Robinson, David Traum, Midhun Ittycheriah, and Joe Henderer. 2008. What would you ask a conversational agent? observations of human-agent dialogues in a museum setting. In International Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Priti Aggarwal</author>
<author>Ron Artstein</author>
<author>Susan Foutz</author>
<author>Jillian Gerten</author>
<author>Athanasios Katsamanis</author>
<author>Anton Leuski</author>
<author>Dan Noren</author>
<author>William Swartout</author>
</authors>
<title>Ada and grace: Direct interaction with museum visitors.</title>
<date>2012</date>
<booktitle>In The 12th International Conference on Intelligent Virtual Agents (IVA),</booktitle>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="16544" citStr="Traum et al., 2012" startWordPosition="2700" endWordPosition="2703">ure 0.92 0.60 0.72 acNLU input = ASR Precision Recall F-measure 0.71 0.32 0.45 Table 1: NLU results The ASR Word Error Rate (WER) is of 70%. However, we detect some problems in the way we were collecting the audio, and in more recent evaluations (by using 363 recent logs where previous problems were corrected), that error decreased to a WER of 52%, including speech from 111 children, 21 non native Portuguese speakers (thus, with a different pronunciation), 23 individuals not talking in Portuguese and 27 interactions where multiple speakers overlap. Here, we should refer the work presented in (Traum et al., 2012), where an evaluation of two virtual guides in a museum is presented. They also had to deal with speakers from different ages and with question off-topic, and report a ASR with 57% WER (however they majority of their user are children: 76%). We are currently preparing a new corpus for evaluating the NLU module, however, the following results remain: in the best scenario, if transcription is perfect, the NLU module behaves as indicated in Table 1 (manual transcriptions). 5 Conclusions and Future Work We have described a platform for developing ECAs with tutoring goals, that takes both speech an</context>
</contexts>
<marker>Traum, Aggarwal, Artstein, Foutz, Gerten, Katsamanis, Leuski, Noren, Swartout, 2012</marker>
<rawString>David Traum, Priti Aggarwal, Ron Artstein, Susan Foutz, Jillian Gerten, Athanasios Katsamanis, Anton Leuski, Dan Noren, and William Swartout. 2012. Ada and grace: Direct interaction with museum visitors. In The 12th International Conference on Intelligent Virtual Agents (IVA), Santa Cruz, CA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulli Waltinger</author>
<author>Alexa Breuing</author>
<author>Ipke Wachsmuth</author>
</authors>
<title>Interfacing virtual agents with collaborative knowledge: Open domain question answering using wikipedia-based topic models.</title>
<date>2011</date>
<booktitle>In IJCAI,</booktitle>
<pages>1896--1902</pages>
<contexts>
<context position="2421" citStr="Waltinger et al., 2011" startWordPosition="371" endWordPosition="374">ting Command as a hi-tech attraction and information source (Robinson et al., Figure 1: EDGAR at MONSERRATE. 2008). DuARTE Digital (Mendes et al., 2009) and EDGAR are also examples of virtual characters for the Portuguese language with the same edutainment goal: DuARTE Digital answers questions about Cust´odia de Bel´em, a famous work of the Portuguese jewelry; EDGAR is a virtual butler that answers questions about MONSERRATE (Figure 1). Considering the previous mentioned agents, they all cover a specific domain of knowledge (although a general Question/Answering system was integrated in Max (Waltinger et al., 2011)). However, as expected, people tend also to make small talk when interacting with these agents. Therefore, it is important that these systems properly deal with it. Several strategies are envisaged to this end and EDGAR is of no exception. In this paper, we describe the platform behind EDGAR, which we developed aiming at the fast insertion of in-domain knowledge, and to deal with small talk. This platform is currently in the process of being industrially applied by a company known for its expertise in building and deploying kiosks. We will provide the hardware and software required to demonst</context>
</contexts>
<marker>Waltinger, Breuing, Wachsmuth, 2011</marker>
<rawString>Ulli Waltinger, Alexa Breuing, and Ipke Wachsmuth. 2011. Interfacing virtual agents with collaborative knowledge: Open domain question answering using wikipedia-based topic models. In IJCAI, pages 1896–1902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heiga Zen</author>
<author>Keiichiro Oura</author>
<author>Takashi Nose</author>
<author>Junichi Yamagishi</author>
<author>Shinji Sako</author>
<author>Tomoki Toda</author>
<author>Takashi Masuko</author>
<author>Alan W Black</author>
<author>Keiichi Tokuda</author>
</authors>
<title>Recent development of the HMM-based speech synthesis system (HTS).</title>
<date>2009</date>
<booktitle>In Proc. 2009 Asia-Pacific Signal and Information Processing Association (APSIPA),</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6132" citStr="Zen et al., 2009" startWordPosition="970" endWordPosition="973">ed amplication to the captured signal, despite the AGC mechanism, ensuring that too silent sounds are not discarded by the SNS mechanism. Upon a spoken input, AUDIMUS translates it into a sentence, with a confidence value. An empty recognition result, or one with low confidence, triggers a control tag (“ REPEAT ”) to the NLU module, which results in a request for the user to repeat what was said. The answer returned by the NLU module is synthesized in a language dependent Text To Speech (TTS) system, with DIXI (Paulo et al., 2008) being used for Portuguese, while a recent version of FESTIVAL (Zen et al., 2009) covers both English and Spanish. The 2http://www.rocketbox-libraries.com/ 62 synthesized audio is played while the corresponding phonemes are mapped into visemes, represented as skeletal animations, being synchronized according to phoneme durations, available in all the employed TTS engines. Emotions are declared in the knowledge sources of the agent. As shown in Figure 3, they are coordinated with viseme animations. Figure 3: The EDGAR character in a joyful state. 2.3 Interacting with EDGAR In a typical interaction, the user enters a question with a virtual keyboard or says it to the microph</context>
</contexts>
<marker>Zen, Oura, Nose, Yamagishi, Sako, Toda, Masuko, Black, Tokuda, 2009</marker>
<rawString>Heiga Zen, Keiichiro Oura, Takashi Nose, Junichi Yamagishi, Shinji Sako, Tomoki Toda, Takashi Masuko, Alan W. Black, and Keiichi Tokuda. 2009. Recent development of the HMM-based speech synthesis system (HTS). In Proc. 2009 Asia-Pacific Signal and Information Processing Association (APSIPA), Sapporo, Japan, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>