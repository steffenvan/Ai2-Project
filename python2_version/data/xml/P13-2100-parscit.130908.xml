<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024989">
<title confidence="0.9984855">
Using Integer Linear Programming in Concept-to-Text Generation to
Produce More Compact Texts
</title>
<author confidence="0.982496">
Gerasimos Lampouras and Ion Androutsopoulos
</author>
<affiliation confidence="0.997376">
Department of Informatics
Athens University of Economics and Business
</affiliation>
<address confidence="0.94867">
Patission 76, GR-104 34 Athens, Greece
</address>
<email confidence="0.967096">
http://nlp.cs.aueb.gr/
</email>
<sectionHeader confidence="0.992678" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869833333333">
We present an ILP model of concept-to-
text generation. Unlike pipeline archi-
tectures, our model jointly considers the
choices in content selection, lexicaliza-
tion, and aggregation to avoid greedy de-
cisions and produce more compact texts.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916271186441">
Concept-to-text natural language generation
(NLG) generates texts from formal knowledge
representations (Reiter and Dale, 2000). With the
emergence of the Semantic Web (Antoniou and
van Harmelen, 2008), interest in concept-to-text
NLG has been revived and several methods
have been proposed to express axioms of OWL
ontologies (Grau et al., 2008) in natural language
(Bontcheva, 2005; Mellish and Sun, 2006; Gala-
nis and Androutsopoulos, 2007; Mellish and Pan,
2008; Schwitter et al., 2008; Schwitter, 2010;
Liang et al., 2011; Williams et al., 2011).
NLG systems typically employ a pipeline archi-
tecture. They usually start by selecting the logi-
cal facts to express. The next stage, text planning,
ranges from simply ordering the selected facts to
complex decisions about the rhetorical structure
of the text. Lexicalization then selects the words
and syntactic structures that will realize each fact,
specifying how each fact can be expressed as a
single sentence. Sentence aggregation then com-
bines sentences into longer ones. Another compo-
nent generates appropriate referring expressions,
and surface realization produces the final text.
Each stage of the pipeline is treated as a lo-
cal optimization problem, where the decisions of
the previous stages cannot be modified. This ar-
rangement produces texts that may not be optimal,
since the decisions of the stages have been shown
to be co-dependent (Danlos, 1984; Marciniak and
Strube, 2005; Belz, 2008). For example, content
selection and lexicalization may lead to more or
fewer sentence aggregation opportunities.
We present an Integer Linear Programming
(ILP) model that combines content selection, lex-
icalization, and sentence aggregation. Our model
does not consider text planning, nor referring ex-
pression generation, which we hope to include in
future work, but it is combined with an external
simple text planner and a referring expression gen-
eration component; we also do not discuss sur-
face realization. Unlike pipeline architectures, our
model jointly examines the possible choices in the
three NLG stages it considers, to avoid greedy local
decisions. Given an individual (entity) or class of
an OWL ontology and a set of facts (OWL axioms)
about the individual or class, we aim to produce a
text that expresses as many of the facts in as few
words as possible. This is important when space is
limited or expensive (e.g., product descriptions on
smartphones, advertisements in search engines).
Although the search space of our model is very
large and ILP problems are in general NP-hard, ILP
solvers can be used, they are very fast in practice,
and they guarantee finding a global optimum. Ex-
periments show that our ILP model outperforms,
in terms of compression, an NLG system that uses
the same components, but connected in a pipeline,
with no deterioration in fluency and clarity.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999873363636364">
Marciniak and Strube (2005) propose a general
ILP approach for language processing applications
where the decisions of classifiers that consider
particular, but co-dependent, subtasks need to be
combined. They also show how their approach
can be used to generate multi-sentence route di-
rections, in a setting with very different inputs and
processing stages than the ones we consider.
Barzilay and Lapata (2005) treat content selec-
tion as an optimization problem. Given a pool of
facts and scores indicating the importance of each
</bodyText>
<page confidence="0.96835">
561
</page>
<bodyText confidence="0.963261921052632">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–566,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
fact or pair of facts, they select the facts to express
by formulating an optimization problem similar
to energy minimization. In other work, Barzilay
and Lapata (2006) consider sentence aggregation.
Given a set of facts that a content selection stage
has produced, aggregation is viewed as the prob-
lem of partitioning the facts into optimal subsets.
Sentences expressing facts that are placed in the
same subset are aggregated to form a longer sen-
tence. An ILP model is used to find the partitioning
that maximizes the pairwise similarity of the facts
in each subset, subject to constraints limiting the
number of subsets and the facts in each subset.
Althaus et al. (2004) show that ordering a set
of sentences to maximize sentence-to-sentence co-
herence is equivalent to the traveling salesman
problem and, hence, NP-complete. They also show
how an ILP solver can be used in practice.
Joint optimization ILP models have also been
used in multi-document text summarization and
sentence compression (McDonald, 2007; Clarke
and Lapata, 2008; Berg-Kirkpatrick et al., 2011;
Galanis et al., 2012; Woodsend and Lapata, 2012),
where the input is text, not formal knowledge rep-
resetations. Statistical methods to jointly perform
content selection, lexicalization, and surface real-
ization have also been proposed in NLG (Liang et
al., 2009; Konstas and Lapata, 2012a; Konstas and
Lapata, 2012b), but they are currently limited to
generating single sentences from flat records.
To the best of our knowledge, this article is the
first one to consider content selection, lexicaliza-
tion, and sentence aggregation as an ILP joint opti-
mization problem in the context of multi-sentence
concept-to-text generation. It is also the first arti-
cle to consider ILP in NLG from OWL ontologies.
</bodyText>
<sectionHeader confidence="0.99731" genericHeader="method">
3 Our ILP model of NLG
</sectionHeader>
<bodyText confidence="0.984568022222222">
Let F = Jf1, ... , fn} be the set of all the facts fi
(OWL axioms) about the individual or class to be
described. OWL axioms can be represented as sets
of RDF triples of the form (S, R, O), where S is an
individual or class, O is another individual, class,
or datatype value, and R is a relation (property)
that connects S to O. Hence, we can assume that
each fact fi is a triple (Si, Ri, Oi).1
For each fact fi, a set Pi = Jpi1,pi2,...}
of alternative sentence plans is available. Each
1We actually convert the RDF triples to simpler message
triples, so that each message triple can be easily expressed by
a simple sentence, but we do not discuss this conversion here.
sentence plan pik specifies how to express fi =
(Si, Ri, Oi) as an alternative single sentence. In
our work, a sentence plan is a sequence of slots,
along with instructions specifying how to fill the
slots in; and each sentence plan is associated
with the relations it can express. For example,
(exhibit12, foundIn, athens) could be ex-
pressed using a sentence plan like “[ref (S)]
[findpast] [in] [ref (O)]”, where square brackets
denote slots, ref (S) and ref (O) are instructions
requiring referring expressions for S and O in
the corresponding slots, and “findpast” requires the
simple past form of “find”. In our example, the
sentence plan would lead to a sentence like “Ex-
hibit 12 was found in Athens”. We call elements
the slots with their instructions, but with “S”
and “O” accompanied by the individuals, classes,
or datatype values they refer to; in our exam-
ple, the elements are “[ref (S: exhibit12)]”,
“[findpast]”, “[in]”, “[ref (O: athens)]”. Dif-
ferent sentence plans may lead to more or fewer
aggregation opportunities; for example, sentences
with the same verb are easier to aggregate. We use
aggregation rules (Dalianis, 1999) that operate on
sentence plans and usually lead to shorter texts.
Let s1, ... , sm be disjoint subsets of F, each
containing 0 to n facts, with m &lt; n. A single
sentence is generated for each subset sj by aggre-
gating the sentences (more precisely, the sentence
plans) expressing the facts of sj.2 An empty sj
generates no sentence, i.e., the resulting text can
be at most m sentences long. Let us also define:
</bodyText>
<equation confidence="0.972655333333333">
� 1, if fact fi is selected
ai = (1) 0, otherwise
1, if sentence plan pik is used to express
fact fi, and fi is in subset sj
0, otherwise
(2)
� 1, if element et is used in subset sj
btj = (3)
0, otherwise
</equation>
<bodyText confidence="0.99977125">
and let B be the set of all the distinct elements (no
duplicates) from all the available sentence plans
that can express the facts of F. The length of an
aggregated sentence resulting from a subset sj can
be roughly estimated by counting the distinct el-
ements of the sentence plans that have been cho-
sen to express the facts of sj; elements that occur
more than once in the chosen sentence plans of sj
</bodyText>
<footnote confidence="0.72029125">
2All the sentences of every possible subset sj can be ag-
gregated, because all the sentences share the same subject,
the class or individual being described. If multiple aggrega-
tion rules apply, we use the one that leads to a shorter text.
</footnote>
<equation confidence="0.95236175">
⎧
⎨
⎩
likj =
</equation>
<page confidence="0.960673">
562
</page>
<bodyText confidence="0.9998075">
are counted only once, because they will probably
be expressed only once, due to aggregation.
Our objective function (4) maximizes the num-
ber of selected facts fi and minimizes the number
of distinct elements in each subset sj, i.e., the ap-
proximate length of the corresponding aggregated
sentence; an alternative explanation is that by min-
imizing the number of distinct elements in each sj,
we favor subsets that aggregate well. By a and b
we jointly denote all the ai and btj variables. The
two parts (sums) of the objective function are nor-
malized to [0, 1] by dividing by the total number
of available facts |F |and the number of subsets m
times the total number of distinct elements |B|. In
the first part of the objective, we treat all the facts
as equally important; if importance scores are also
available for the facts, they can be added as mul-
tipliers of αi. The parameters A1 and A2 are used
to tune the priority given to expressing many facts
vs. generating shorter texts; we set A1 + A2 = 1.
</bodyText>
<equation confidence="0.999764384615385">
X i = 1, ... ,n
et∈Bik btj ≥ |Bik |· likj, for j = 1,... , m (6)
k = 1, ... ,|Pi|
pik∈P (et)
X likj ≥ btj, for (7)
j= 1, ... ,m
t = 1, ... , |B|
X |B|
t=1 btj ≤ Bmax, for j = 1, ... ,m (8)
j= 1, ... ,m, i = 2,..., n
li,k,j ≤ 1, for i0 = 1, ... ,n − 1; i =6 i0
section(fi) =6 section(f0i)
(9)
</equation>
<bodyText confidence="0.999890551724138">
Constraint 5 ensures that for each selected fact,
only one sentence plan in only one subset is se-
lected; if a fact is not selected, no sentence plan
for the fact is selected either. |σ |denotes the car-
dinality of a set σ. In constraint 6, Bik is the set of
distinct elements et of the sentence plan pik. This
constraint ensures that if pik is selected in a subset
sj, then all the elements of pik are also present in
sj. If pik is not selected in sj, then some of its el-
ements may still be present in sj, if they appear in
another selected sentence plan of sj.
In constraint 7, P(et) is the set of sentence plans
that contain element et. If et is used in a subset sj,
then at least one of the sentence plans of P(et)
must also be selected in sj. If et is not used in sj,
then no sentence plan of P(et) may be selected in
sj. Lastly, constraint 8 limits the number of ele-
ments that a subset sj can contain to a maximum
allowed number Bmax, in effect limiting the max-
imum length of an aggregated sentence.
We assume that each relation R has been man-
ually mapped to a single topical section; e.g., re-
lations expressing the color, body, and flavor of
a wine may be grouped in one section, and rela-
tions about the wine’s producer in another. The
section of a fact fi = (Si, Ri, Oi) is the section
of its relation Ri. Constraint 9 ensures that facts
from different sections will not be placed in the
same subset sj, to avoid unnatural aggregations.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999907344827586">
We used NaturalOWL (Galanis and Androutsopou-
los, 2007; Galanis et al., 2009; Androutsopoulos
et al., 2013), an NLG system for OWL ontologies
that relies on a pipeline of content selection, text
planning, lexicalization, aggregation, referring ex-
pression generation, and surface realization.3 We
modified content selection, lexicalization, and ag-
gregation to use our ILP model, maintaining the
aggregation rules of the original system.4 For re-
ferring expression generation and surface realiza-
tion, the new system, called ILPNLG, invokes the
corresponding components of NaturalOWL.
The original system, called PIPELINE, assumes
that each relation has been mapped to a topical
section, as in ILPNLG. It also assumes that a man-
ually specified order of the sections and the rela-
tions of each section is available, which is used
by the text planner to order the selected facts (by
their relations). The subsequent components of the
pipeline are not allowed to change the order of the
facts, and aggregation operates only on sentence
plans of adjacent facts from the same section. In
ILPNLG, the manually specified order of sections
and relations is used to order the sentences of each
subset sj (before aggregating them), the aggre-
gated sentences in each section (each aggregated
sentence inherits the minimum order of its con-
stituents), and the sections (with their sentences).
We used the Wine Ontology, which had been
</bodyText>
<footnote confidence="0.9446714">
3All the software and data we used are freely available
from http://nlp.cs.aueb.gr/software.html.
We use version 2 of NaturalOWL.
4We use the Branch and Cut implementation of GLPK; see
sourceforge.net/projects/winglpk/.
</footnote>
<equation confidence="0.999118538461538">
btj (4)
m · |B|
subject to: m |Pi |likj, for i = 1, ... ,n (5)
ai = X X
j=1 k=1
ai
A2 ·
A1 ·
|F|
max
a,b
X |F|
i=1
|B|
X
t=1
m
X
j=1
|Pi|
X
k=1
likj +
|Pi,|
X
k&apos;=1
</equation>
<page confidence="0.993862">
563
</page>
<bodyText confidence="0.99958584">
used in previous experiments with PIPELINE.5 We
kept the 2 topical sections, the ordering of sec-
tions and relations, and the sentence plans that
had been used in the previous experiments, but we
added more sentence plans to ensure that 3 sen-
tence plans were available per fact. We gener-
ated texts for the 52 wine individuals of the on-
tology; we did not experiment with texts describ-
ing classes of wines, because we could not think
of multiple alternative sentence plans for many of
their axioms. For each individual, there were 5
facts on average and a maximum of 6 facts.
PIPELINE has a parameter M specifying the
maximum number of facts it is allowed to report
per text. When M is smaller than the number of
available facts |F |and all the facts are treated as
equally important, as in our experiments, it se-
lects randomly M of the available facts. We re-
peated the generation of PIPELINE’s texts for the
52 individuals for M = 2, 3, 4, 5, 6. For each M,
the texts of PIPELINE for the 52 individuals were
generated three times, each time using one of the
different alternative sentence plans of each rela-
tion. We also generated the texts using a variant of
PIPELINE, dubbed PIPELINESHORT, which always
selects the shortest (in elements) sentence plan
among the available ones. In all cases, PIPELINE
and PIPELINESHORT were allowed to form ag-
gregated sentences containing up to B,,,,,, = 22
distinct elements, which was the number of dis-
tinct elements of the longest aggregated sentence
in the previous experiments, where PIPELINE was
allowed to aggregate up to 3 original sentences.
With ILPNLG, we repeated the generation of the
texts of the 52 individuals using different values
of A1 (A2 = 1 − A1), which led to texts express-
ing from zero to all of the available facts. We set
the maximum number of fact subsets to m = 3,
which was the maximum number of aggregated
sentences observed in the texts of PIPELINE and
PIPELINESHORT. Again, we set B,,,,,, = 22.
We compared ILPNLG to PIPELINE and PIPELI-
NESHORT by measuring the average number of
facts they reported divided by the average text
length (in words). Figure 1 shows this ratio as a
function of the average number of reported facts,
along with 95% confidence intervals (of sample
means). PIPELINESHORT achieved better results
than PIPELINE, but the differences were small.
For A1 &lt; 0.2, ILPNLG produces empty texts,
</bodyText>
<footnote confidence="0.749688">
5Seewww.w3.org/TR/owl-guide/wine.rdf.
</footnote>
<figureCaption confidence="0.999266">
Figure 1: Facts/words ratio of the generated texts.
</figureCaption>
<bodyText confidence="0.959023804878049">
since it focuses on minimizing the number of dis-
tinct elements of each text. For A1 ≥ 0.225, it per-
forms better than the other systems. For A1 Pz 0.3,
it obtains the highest fact/words ratio by select-
ing the facts and sentence plans that lead to the
most compressive aggregations. For greater val-
ues of A1, it selects additional facts whose sen-
tence plans do not aggregate that well, which is
why the ratio declines. For small numbers of facts,
the two pipeline systems select facts and sentence
plans that offer very few aggregation opportuni-
ties; as the number of selected facts increases,
some more aggregation opportunities arise, which
is why the facts/words ratio of the two systems
improves. In all the experiments, the ILP solver
was very fast (average: 0.08 sec, worst: 0.14 sec).
Experiments with human judges also showed that
the texts of ILPNLG cannot be distinguished from
those of PIPELINESHORT in terms of fluency and
text clarity. Hence, the highest compactness of the
texts of ILPNLG does not come at the expense of
lower text quality. Space does not permit a more
detailed description of these experiments.
We show below texts produced by PIPELINE
(M = 4) and ILPNLG (A1 = 0.3).
PIPELINE: This is a strong Sauternes. It is made from Semil-
lon grapes and it is produced by Chateau D’ychem.
ILPNLG: This is a strong Sauternes. It is made from Semillon
grapes by Chateau D’ychem.
PIPELINE: This is a full Riesling and it has moderate flavor.
It is produced by Volrad.
ILPNLG: This is a full sweet moderate Riesling.
In the first pair, PIPELINE uses different verbs for
the grapes and producer, whereas ILPNLG uses the
same verb, which leads to a more compressive ag-
gregation; both texts describe the same wine and
report 4 facts. In the second pair, ILPNLG has cho-
sen to express the sweetness instead of the pro-
ducer, and uses the same verb (“be”) for all the
facts, leading to a shorter sentence; again both
texts describe the same wine and report 4 facts.
</bodyText>
<page confidence="0.993101">
564
</page>
<bodyText confidence="0.9938082">
In both examples, some facts are not aggregated
because they belong in different sections.
K. Bontcheva. 2005. Generating tailored textual sum-
maries from ontologies. In 2nd European Semantic
Web Conf., pages 531–545, Heraklion, Greece.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999904">
We presented an ILP model for NLG that jointly
considers the choices in content selection, lexical-
ization, and aggregation to avoid greedy local de-
cisions and produce more compact texts. Exper-
iments verified that our model can express more
facts per word, compared to a pipeline, which is
important when space is scarce. An off-the-shelf
ILP solver took approximately 0.1 sec for each
text. We plan to extend our model to include text
planning and referring expressions generation.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990282375">
This research has been co-financed by the Euro-
pean Union (European Social Fund – ESF) and
Greek national funds through the Operational Pro-
gram “Education and Lifelong Learning” of the
National Strategic Reference Framework (NSRF)
– Research Funding Program: Heracleitus II. In-
vesting in knowledge society through the Euro-
pean Social Fund.
</bodyText>
<sectionHeader confidence="0.997383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997811851351351">
E. Althaus, N. Karamanis, and A. Koller. 2004. Com-
puting locally coherent discourses. In 42nd Annual
Meeting ofACL, pages 399–406, Barcelona, Spain.
I. Androutsopoulos, G. Lampouras, and D. Gala-
nis. 2013. Generating natural language descrip-
tions from OWL ontologies: the NaturalOWL sys-
tem. Technical report, Natural Language Processing
Group, Department of Informatics, Athens Univer-
sity of Economics and Business.
G. Antoniou and F. van Harmelen. 2008. A Semantic
Web primer. MIT Press, 2nd edition.
R. Barzilay and M. Lapata. 2005. Collective content
selection for concept-to-text generation. In HLT-
EMNLP, pages 331–338, Vancouver, BC, Canada.
R. Barzilay and M. Lapata. 2006. Aggregation via
set partitioning for natural language generation. In
HLT-NAACL, pages 359–366, New York, NY.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431–455.
T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011.
Jointly learning to extract and compress. In 49th
Annual Meeting of ACL, pages 481–490, Portland,
OR.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal ofArtificial Intelligence Re-
search, 1(31):399–429.
H. Dalianis. 1999. Aggregation in natural language
generation. Comput. Intelligence, 15(4):384–414.
L. Danlos. 1984. Conceptual and linguistic decisions
in generation. In 10th COLING, pages 501–504,
Stanford, CA.
D. Galanis and I. Androutsopoulos. 2007. Generating
multilingual descriptions from linguistically anno-
tated OWL ontologies: the NaturalOWL system. In
11th European Workshop on Natural Lang. Genera-
tion, pages 143–146, Schloss Dagstuhl, Germany.
D. Galanis, G. Karakatsiotis, G. Lampouras, and I. An-
droutsopoulos. 2009. An open-source natural lan-
guage generator for OWL ontologies and its use in
Prot´eg´e and Second Life. In 12th Conf. of the Euro-
pean Chapter ofACL (demos), Athens, Greece.
D. Galanis, G. Lampouras, and I. Androutsopoulos.
2012. Extractive multi-document summarization
with Integer Linear Programming and Support Vec-
tor Regression. In COLING, pages 911–926, Mum-
bai, India.
B.C. Grau, I. Horrocks, B. Motik, B. Parsia, P. Patel-
Schneider, and U. Sattler. 2008. OWL 2: The next
step for OWL. Web Semantics, 6:309–322.
I. Konstas and M. Lapata. 2012a. Concept-to-text gen-
eration via discriminative reranking. In 50th Annual
Meeting ofACL, pages 369–378, Jeju Island, Korea.
I. Konstas and M. Lapata. 2012b. Unsupervised
concept-to-text generation with hypergraphs. In
HLT-NAACL, pages 752–761, Montr´eal, Canada.
P. Liang, M. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
47th Meeting ofACL and 4th AFNLP, pages 91–99,
Suntec, Singapore.
S.F. Liang, R. Stevens, D. Scott, and A. Rector. 2011.
Automatic verbalisation of SNOMED classes using
OntoVerbal. In 13th Conf. AI in Medicine, pages
338–342, Bled, Slovenia.
T. Marciniak and M. Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In 9th Con-
ference on Computational Natural Language Learn-
ing, pages 136–143, Ann Arbor, MI.
R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Eu-
ropean Conference on Information Retrieval, pages
557–564, Rome, Italy.
</reference>
<page confidence="0.981431">
565
</page>
<reference confidence="0.999825217391304">
C. Mellish and J.Z. Pan. 2008. Natural language di-
rected inference from ontologies. Artificial Intelli-
gence, 172:1285–1315.
C. Mellish and X. Sun. 2006. The Semantic Web as a
linguistic resource: opportunities for nat. lang. gen-
eration. Knowledge Based Systems, 19:298–303.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge Univ. Press.
R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and
G. Hart. 2008. A comparison of three controlled
nat. languages for OWL 1.1. In 4th OWL Experi-
ences and Directions Workshop, Washington DC.
R. Schwitter. 2010. Controlled natural languages for
knowledge representation. In 23rd COLING, pages
1113–1121, Beijing, China.
S. Williams, A. Third, and R. Power. 2011. Levels
of organization in ontology verbalization. In 13th
European Workshop on Natural Lang. Generation,
pages 158–163, Nancy, France.
K. Woodsend and M. Lapata. 2012. Multiple aspect
summarization using integer linear programming. In
EMNLP-CoNLL, pages 233–243, Jesu Island, Ko-
rea.
</reference>
<page confidence="0.998504">
566
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327594">
<title confidence="0.998293">Using Integer Linear Programming in Concept-to-Text Generation Produce More Compact Texts</title>
<author confidence="0.732366">Lampouras</author>
<affiliation confidence="0.986898">Department of Athens University of Economics and</affiliation>
<note confidence="0.484597">Patission 76, GR-104 34 Athens,</note>
<web confidence="0.991569">http://nlp.cs.aueb.gr/</web>
<abstract confidence="0.992570428571429">present an of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Althaus</author>
<author>N Karamanis</author>
<author>A Koller</author>
</authors>
<title>Computing locally coherent discourses.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting ofACL,</booktitle>
<pages>399--406</pages>
<location>Barcelona,</location>
<contexts>
<context position="4790" citStr="Althaus et al. (2004)" startWordPosition="738" endWordPosition="741">ss by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface real</context>
</contexts>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>E. Althaus, N. Karamanis, and A. Koller. 2004. Computing locally coherent discourses. In 42nd Annual Meeting ofACL, pages 399–406, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>G Lampouras</author>
<author>D Galanis</author>
</authors>
<title>Generating natural language descriptions from OWL ontologies: the NaturalOWL system.</title>
<date>2013</date>
<tech>Technical report,</tech>
<institution>Natural Language Processing Group, Department of Informatics, Athens University of Economics and Business.</institution>
<contexts>
<context position="11857" citStr="Androutsopoulos et al., 2013" startWordPosition="2036" endWordPosition="2039">max, in effect limiting the maximum length of an aggregated sentence. We assume that each relation R has been manually mapped to a single topical section; e.g., relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine’s producer in another. The section of a fact fi = (Si, Ri, Oi) is the section of its relation Ri. Constraint 9 ensures that facts from different sections will not be placed in the same subset sj, to avoid unnatural aggregations. 4 Experiments We used NaturalOWL (Galanis and Androutsopoulos, 2007; Galanis et al., 2009; Androutsopoulos et al., 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization.3 We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system.4 For referring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL. The original system, called PIPELINE, assumes that each relation has been mapped to a topical section, as in ILPNLG. It also a</context>
</contexts>
<marker>Androutsopoulos, Lampouras, Galanis, 2013</marker>
<rawString>I. Androutsopoulos, G. Lampouras, and D. Galanis. 2013. Generating natural language descriptions from OWL ontologies: the NaturalOWL system. Technical report, Natural Language Processing Group, Department of Informatics, Athens University of Economics and Business.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Antoniou</author>
<author>F van Harmelen</author>
</authors>
<title>A Semantic Web primer.</title>
<date>2008</date>
<publisher>MIT Press,</publisher>
<note>2nd edition.</note>
<marker>Antoniou, van Harmelen, 2008</marker>
<rawString>G. Antoniou and F. van Harmelen. 2008. A Semantic Web primer. MIT Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Collective content selection for concept-to-text generation. In</title>
<date>2005</date>
<booktitle>HLTEMNLP,</booktitle>
<pages>331--338</pages>
<location>Vancouver, BC,</location>
<contexts>
<context position="3805" citStr="Barzilay and Lapata (2005)" startWordPosition="581" endWordPosition="584">imum. Experiments show that our ILP model outperforms, in terms of compression, an NLG system that uses the same components, but connected in a pipeline, with no deterioration in fluency and clarity. 2 Related work Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined. They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–566, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics fact or pair of facts, they select the facts to express by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed a</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>R. Barzilay and M. Lapata. 2005. Collective content selection for concept-to-text generation. In HLTEMNLP, pages 331–338, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Aggregation via set partitioning for natural language generation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>359--366</pages>
<location>New York, NY.</location>
<contexts>
<context position="4284" citStr="Barzilay and Lapata (2006)" startWordPosition="653" endWordPosition="656">ulti-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 561–566, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics fact or pair of facts, they select the facts to express by formulating an optimization problem similar to energy minimization. In other work, Barzilay and Lapata (2006) consider sentence aggregation. Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalen</context>
</contexts>
<marker>Barzilay, Lapata, 2006</marker>
<rawString>R. Barzilay and M. Lapata. 2006. Aggregation via set partitioning for natural language generation. In HLT-NAACL, pages 359–366, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="1985" citStr="Belz, 2008" startWordPosition="293" endWordPosition="294">rds and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities. We present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation. Our model does not consider text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and a referring expression generation component; we also do not discuss surface realization. Unlike pipeline architectures, our model jointly examines the possible choices in the th</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>A. Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>D Gillick</author>
<author>D Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of ACL,</booktitle>
<pages>481--490</pages>
<location>Portland, OR.</location>
<contexts>
<context position="5186" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="797" endWordPosition="800">ger sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentenc</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011. Jointly learning to extract and compress. In 49th Annual Meeting of ACL, pages 481–490, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>1</volume>
<issue>31</issue>
<contexts>
<context position="5155" citStr="Clarke and Lapata, 2008" startWordPosition="793" endWordPosition="796"> aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem </context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal ofArtificial Intelligence Research, 1(31):399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dalianis</author>
</authors>
<title>Aggregation in natural language generation.</title>
<date>1999</date>
<journal>Comput. Intelligence,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="7724" citStr="Dalianis, 1999" startWordPosition="1232" endWordPosition="1233">n the corresponding slots, and “findpast” requires the simple past form of “find”. In our example, the sentence plan would lead to a sentence like “Exhibit 12 was found in Athens”. We call elements the slots with their instructions, but with “S” and “O” accompanied by the individuals, classes, or datatype values they refer to; in our example, the elements are “[ref (S: exhibit12)]”, “[findpast]”, “[in]”, “[ref (O: athens)]”. Different sentence plans may lead to more or fewer aggregation opportunities; for example, sentences with the same verb are easier to aggregate. We use aggregation rules (Dalianis, 1999) that operate on sentence plans and usually lead to shorter texts. Let s1, ... , sm be disjoint subsets of F, each containing 0 to n facts, with m &lt; n. A single sentence is generated for each subset sj by aggregating the sentences (more precisely, the sentence plans) expressing the facts of sj.2 An empty sj generates no sentence, i.e., the resulting text can be at most m sentences long. Let us also define: � 1, if fact fi is selected ai = (1) 0, otherwise 1, if sentence plan pik is used to express fact fi, and fi is in subset sj 0, otherwise (2) � 1, if element et is used in subset sj btj = (3</context>
</contexts>
<marker>Dalianis, 1999</marker>
<rawString>H. Dalianis. 1999. Aggregation in natural language generation. Comput. Intelligence, 15(4):384–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>Conceptual and linguistic decisions in generation.</title>
<date>1984</date>
<booktitle>In 10th COLING,</booktitle>
<pages>501--504</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="1944" citStr="Danlos, 1984" startWordPosition="287" endWordPosition="288">e text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities. We present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation. Our model does not consider text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and a referring expression generation component; we also do not discuss surface realization. Unlike pipeline architectures, our model jointl</context>
</contexts>
<marker>Danlos, 1984</marker>
<rawString>L. Danlos. 1984. Conceptual and linguistic decisions in generation. In 10th COLING, pages 501–504, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system.</title>
<date>2007</date>
<booktitle>In 11th European Workshop on Natural Lang. Generation,</booktitle>
<pages>143--146</pages>
<location>Schloss Dagstuhl, Germany.</location>
<contexts>
<context position="971" citStr="Galanis and Androutsopoulos, 2007" startWordPosition="131" endWordPosition="135"> pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another comp</context>
<context position="11804" citStr="Galanis and Androutsopoulos, 2007" startWordPosition="2027" endWordPosition="2031">hat a subset sj can contain to a maximum allowed number Bmax, in effect limiting the maximum length of an aggregated sentence. We assume that each relation R has been manually mapped to a single topical section; e.g., relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine’s producer in another. The section of a fact fi = (Si, Ri, Oi) is the section of its relation Ri. Constraint 9 ensures that facts from different sections will not be placed in the same subset sj, to avoid unnatural aggregations. 4 Experiments We used NaturalOWL (Galanis and Androutsopoulos, 2007; Galanis et al., 2009; Androutsopoulos et al., 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization.3 We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system.4 For referring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL. The original system, called PIPELINE, assumes that each relation has been</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2007</marker>
<rawString>D. Galanis and I. Androutsopoulos. 2007. Generating multilingual descriptions from linguistically annotated OWL ontologies: the NaturalOWL system. In 11th European Workshop on Natural Lang. Generation, pages 143–146, Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>G Karakatsiotis</author>
<author>G Lampouras</author>
<author>I Androutsopoulos</author>
</authors>
<title>An open-source natural language generator for OWL ontologies and its use</title>
<date>2009</date>
<booktitle>in Prot´eg´e and Second Life. In 12th Conf. of the European Chapter ofACL (demos),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="11826" citStr="Galanis et al., 2009" startWordPosition="2032" endWordPosition="2035">ximum allowed number Bmax, in effect limiting the maximum length of an aggregated sentence. We assume that each relation R has been manually mapped to a single topical section; e.g., relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine’s producer in another. The section of a fact fi = (Si, Ri, Oi) is the section of its relation Ri. Constraint 9 ensures that facts from different sections will not be placed in the same subset sj, to avoid unnatural aggregations. 4 Experiments We used NaturalOWL (Galanis and Androutsopoulos, 2007; Galanis et al., 2009; Androutsopoulos et al., 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization.3 We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system.4 For referring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL. The original system, called PIPELINE, assumes that each relation has been mapped to a topical s</context>
</contexts>
<marker>Galanis, Karakatsiotis, Lampouras, Androutsopoulos, 2009</marker>
<rawString>D. Galanis, G. Karakatsiotis, G. Lampouras, and I. Androutsopoulos. 2009. An open-source natural language generator for OWL ontologies and its use in Prot´eg´e and Second Life. In 12th Conf. of the European Chapter ofACL (demos), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galanis</author>
<author>G Lampouras</author>
<author>I Androutsopoulos</author>
</authors>
<title>Extractive multi-document summarization with Integer Linear Programming and Support Vector Regression. In</title>
<date>2012</date>
<booktitle>COLING,</booktitle>
<pages>911--926</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="5208" citStr="Galanis et al., 2012" startWordPosition="801" endWordPosition="804">sed to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text gene</context>
</contexts>
<marker>Galanis, Lampouras, Androutsopoulos, 2012</marker>
<rawString>D. Galanis, G. Lampouras, and I. Androutsopoulos. 2012. Extractive multi-document summarization with Integer Linear Programming and Support Vector Regression. In COLING, pages 911–926, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B C Grau</author>
<author>I Horrocks</author>
<author>B Motik</author>
<author>B Parsia</author>
<author>P PatelSchneider</author>
<author>U Sattler</author>
</authors>
<title>OWL 2: The next step for OWL. Web Semantics,</title>
<date>2008</date>
<pages>6--309</pages>
<contexts>
<context position="876" citStr="Grau et al., 2008" startWordPosition="118" endWordPosition="121">s.aueb.gr/ Abstract We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as</context>
</contexts>
<marker>Grau, Horrocks, Motik, Parsia, PatelSchneider, Sattler, 2008</marker>
<rawString>B.C. Grau, I. Horrocks, B. Motik, B. Parsia, P. PatelSchneider, and U. Sattler. 2008. OWL 2: The next step for OWL. Web Semantics, 6:309–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Konstas</author>
<author>M Lapata</author>
</authors>
<title>Concept-to-text generation via discriminative reranking.</title>
<date>2012</date>
<booktitle>In 50th Annual Meeting ofACL,</booktitle>
<pages>369--378</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5474" citStr="Konstas and Lapata, 2012" startWordPosition="841" endWordPosition="844">to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies. 3 Our ILP model of NLG Let F = Jf1, ... , fn} be the set of all the facts fi (OWL axioms) about the individual or class to be described. OWL axioms can be represented as sets of RDF tri</context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>I. Konstas and M. Lapata. 2012a. Concept-to-text generation via discriminative reranking. In 50th Annual Meeting ofACL, pages 369–378, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Konstas</author>
<author>M Lapata</author>
</authors>
<title>Unsupervised concept-to-text generation with hypergraphs.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>752--761</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5474" citStr="Konstas and Lapata, 2012" startWordPosition="841" endWordPosition="844">to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies. 3 Our ILP model of NLG Let F = Jf1, ... , fn} be the set of all the facts fi (OWL axioms) about the individual or class to be described. OWL axioms can be represented as sets of RDF tri</context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>I. Konstas and M. Lapata. 2012b. Unsupervised concept-to-text generation with hypergraphs. In HLT-NAACL, pages 752–761, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In 47th Meeting ofACL and 4th AFNLP,</booktitle>
<pages>91--99</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="5448" citStr="Liang et al., 2009" startWordPosition="837" endWordPosition="840">o maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first article to consider ILP in NLG from OWL ontologies. 3 Our ILP model of NLG Let F = Jf1, ... , fn} be the set of all the facts fi (OWL axioms) about the individual or class to be described. OWL axioms can be repr</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>P. Liang, M. Jordan, and D. Klein. 2009. Learning semantic correspondences with less supervision. In 47th Meeting ofACL and 4th AFNLP, pages 91–99, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Liang</author>
<author>R Stevens</author>
<author>D Scott</author>
<author>A Rector</author>
</authors>
<title>Automatic verbalisation of SNOMED classes using OntoVerbal.</title>
<date>2011</date>
<booktitle>In 13th Conf. AI in Medicine,</booktitle>
<pages>338--342</pages>
<location>Bled, Slovenia.</location>
<contexts>
<context position="1055" citStr="Liang et al., 2011" startWordPosition="146" endWordPosition="149">on, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces </context>
</contexts>
<marker>Liang, Stevens, Scott, Rector, 2011</marker>
<rawString>S.F. Liang, R. Stevens, D. Scott, and A. Rector. 2011. Automatic verbalisation of SNOMED classes using OntoVerbal. In 13th Conf. AI in Medicine, pages 338–342, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Marciniak</author>
<author>M Strube</author>
</authors>
<title>Beyond the pipeline: Discrete optimization in NLP.</title>
<date>2005</date>
<booktitle>In 9th Conference on Computational Natural Language Learning,</booktitle>
<pages>136--143</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1972" citStr="Marciniak and Strube, 2005" startWordPosition="289" endWordPosition="292">lization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). For example, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities. We present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation. Our model does not consider text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and a referring expression generation component; we also do not discuss surface realization. Unlike pipeline architectures, our model jointly examines the possible choi</context>
<context position="3421" citStr="Marciniak and Strube (2005)" startWordPosition="523" endWordPosition="526">at expresses as many of the facts in as few words as possible. This is important when space is limited or expensive (e.g., product descriptions on smartphones, advertisements in search engines). Although the search space of our model is very large and ILP problems are in general NP-hard, ILP solvers can be used, they are very fast in practice, and they guarantee finding a global optimum. Experiments show that our ILP model outperforms, in terms of compression, an NLG system that uses the same components, but connected in a pipeline, with no deterioration in fluency and clarity. 2 Related work Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined. They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. Barzilay and Lapata (2005) treat content selection as an optimization problem. Given a pool of facts and scores indicating the importance of each 561 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pag</context>
</contexts>
<marker>Marciniak, Strube, 2005</marker>
<rawString>T. Marciniak and M. Strube. 2005. Beyond the pipeline: Discrete optimization in NLP. In 9th Conference on Computational Natural Language Learning, pages 136–143, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In European Conference on Information Retrieval,</booktitle>
<pages>557--564</pages>
<location>Rome, Italy.</location>
<contexts>
<context position="5130" citStr="McDonald, 2007" startWordPosition="791" endWordPosition="792"> same subset are aggregated to form a longer sentence. An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP jo</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In European Conference on Information Retrieval, pages 557–564, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>J Z Pan</author>
</authors>
<title>Natural language directed inference from ontologies.</title>
<date>2008</date>
<journal>Artificial Intelligence,</journal>
<pages>172--1285</pages>
<contexts>
<context position="994" citStr="Mellish and Pan, 2008" startWordPosition="136" endWordPosition="139">jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropr</context>
</contexts>
<marker>Mellish, Pan, 2008</marker>
<rawString>C. Mellish and J.Z. Pan. 2008. Natural language directed inference from ontologies. Artificial Intelligence, 172:1285–1315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>X Sun</author>
</authors>
<title>The Semantic Web as a linguistic resource: opportunities for nat.</title>
<date>2006</date>
<journal>lang. generation. Knowledge Based Systems,</journal>
<pages>19--298</pages>
<contexts>
<context position="936" citStr="Mellish and Sun, 2006" startWordPosition="127" endWordPosition="130">text generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sente</context>
</contexts>
<marker>Mellish, Sun, 2006</marker>
<rawString>C. Mellish and X. Sun. 2006. The Semantic Web as a linguistic resource: opportunities for nat. lang. generation. Knowledge Based Systems, 19:298–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge Univ. Press.</publisher>
<contexts>
<context position="657" citStr="Reiter and Dale, 2000" startWordPosition="83" endWordPosition="86"> Concept-to-Text Generation to Produce More Compact Texts Gerasimos Lampouras and Ion Androutsopoulos Department of Informatics Athens University of Economics and Business Patission 76, GR-104 34 Athens, Greece http://nlp.cs.aueb.gr/ Abstract We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering t</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwitter</author>
<author>K Kaljurand</author>
<author>A Cregan</author>
<author>C Dolbear</author>
<author>G Hart</author>
</authors>
<title>A comparison of three controlled nat. languages for OWL 1.1.</title>
<date>2008</date>
<booktitle>In 4th OWL Experiences and Directions Workshop,</booktitle>
<location>Washington DC.</location>
<contexts>
<context position="1018" citStr="Schwitter et al., 2008" startWordPosition="140" endWordPosition="143">hoices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressio</context>
</contexts>
<marker>Schwitter, Kaljurand, Cregan, Dolbear, Hart, 2008</marker>
<rawString>R. Schwitter, K. Kaljurand, A. Cregan, C. Dolbear, and G. Hart. 2008. A comparison of three controlled nat. languages for OWL 1.1. In 4th OWL Experiences and Directions Workshop, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwitter</author>
</authors>
<title>Controlled natural languages for knowledge representation.</title>
<date>2010</date>
<booktitle>In 23rd COLING,</booktitle>
<pages>1113--1121</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1035" citStr="Schwitter, 2010" startWordPosition="144" endWordPosition="145">ion, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface r</context>
</contexts>
<marker>Schwitter, 2010</marker>
<rawString>R. Schwitter. 2010. Controlled natural languages for knowledge representation. In 23rd COLING, pages 1113–1121, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>A Third</author>
<author>R Power</author>
</authors>
<title>Levels of organization in ontology verbalization.</title>
<date>2011</date>
<booktitle>In 13th European Workshop on Natural Lang. Generation,</booktitle>
<pages>158--163</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1079" citStr="Williams et al., 2011" startWordPosition="150" endWordPosition="153">to avoid greedy decisions and produce more compact texts. 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Galanis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011). NLG systems typically employ a pipeline architecture. They usually start by selecting the logical facts to express. The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. Sentence aggregation then combines sentences into longer ones. Another component generates appropriate referring expressions, and surface realization produces the final text. Each sta</context>
</contexts>
<marker>Williams, Third, Power, 2011</marker>
<rawString>S. Williams, A. Third, and R. Power. 2011. Levels of organization in ontology verbalization. In 13th European Workshop on Natural Lang. Generation, pages 158–163, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>233--243</pages>
<location>Jesu Island,</location>
<contexts>
<context position="5236" citStr="Woodsend and Lapata, 2012" startWordPosition="805" endWordPosition="808">ioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete. They also show how an ILP solver can be used in practice. Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge represetations. Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records. To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. It is also the first</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>K. Woodsend and M. Lapata. 2012. Multiple aspect summarization using integer linear programming. In EMNLP-CoNLL, pages 233–243, Jesu Island, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>