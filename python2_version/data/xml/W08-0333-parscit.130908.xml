<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000198">
<title confidence="0.965258">
Fast, Easy, and Cheap: Construction of
Statistical Machine Translation Models with MapReduce
</title>
<author confidence="0.999505">
Christopher Dyer, Aaron Cordova, Alex Mont, Jimmy Lin
</author>
<affiliation confidence="0.9972865">
Laboratory for Computational Linguistics and Information Processing
University of Maryland
</affiliation>
<address confidence="0.934336">
College Park, MD 20742, USA
</address>
<email confidence="0.999535">
redpony@umd.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933615384615">
In recent years, the quantity of parallel train-
ing data available for statistical machine trans-
lation has increased far more rapidly than
the performance of individual computers, re-
sulting in a potentially serious impediment
to progress. Parallelization of the model-
building algorithms that process this data on
computer clusters is fraught with challenges
such as synchronization, data exchange, and
fault tolerance. However, the MapReduce
programming paradigm has recently emerged
as one solution to these issues: a powerful
functional abstraction hides system-level de-
tails from the researcher, allowing programs to
be transparently distributed across potentially
very large clusters of commodity hardware.
We describe MapReduce implementations of
two algorithms used to estimate the parame-
ters for two word alignment models and one
phrase-based translation model, all of which
rely on maximum likelihood probability esti-
mates. On a 20-machine cluster, experimental
results show that our solutions exhibit good
scaling characteristics compared to a hypo-
thetical, optimally-parallelized version of cur-
rent state-of-the-art single-core tools.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899052631579">
Like many other NLP problems, output quality of
statistical machine translation (SMT) systems in-
creases with the amount of training data. Brants et
al. (2007) demonstrated that increasing the quantity
of training data used for language modeling signifi-
cantly improves the translation quality of an Arabic-
English MT system, even with far less sophisticated
backoff models. However, the steadily increas-
ing quantities of training data do not come with-
out cost. Figure 1 shows the relationship between
the amount of parallel Arabic-English training data
used and both the translation quality of a state-of-
the-art phrase-based SMT system and the time re-
quired to perform the training with the widely-used
Moses toolkit on a commodity server.1 Building
a model using 5M sentence pairs (the amount of
Arabic-English parallel text publicly available from
the LDC) takes just over two days.2 This represents
an unfortunate state of affairs for the research com-
munity: excessively long turnaround on experiments
is an impediment to research progress.
It is clear that the needs of machine translation re-
searchers have outgrown the capabilities of individ-
ual computers. The only practical recourse is to dis-
tribute the computation across multiple cores, pro-
cessors, or machines. The development of parallel
algorithms involves a number of tradeoffs. First is
that of cost: a decision must be made between “ex-
otic” hardware (e.g., large shared memory machines,
InfiniBand interconnect) and commodity hardware.
There is significant evidence (Barroso et al., 2003)
that solutions based on the latter are more cost ef-
fective (and for resource-constrained academic in-
stitutions, often the only option).
Given appropriate hardware, MT researchers
must still contend with the challenge of developing
software. Quite simply, parallel programming is dif-
ficult. Due to communication and synchronization
</bodyText>
<footnote confidence="0.997144333333333">
1http://www.statmt.org/moses/
2All single-core timings reported in this paper were per-
formed on a 3GHz 64-bit Intel Xeon server with 8GB memory.
</footnote>
<page confidence="0.949753">
199
</page>
<note confidence="0.595905333333333">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 199–207,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
Corpus size (sentences)
</note>
<figureCaption confidence="0.9954615">
Figure 1: Translation quality and training time as a func-
tion of corpus size.
</figureCaption>
<bodyText confidence="0.999964196721312">
issues, concurrent operations are notoriously chal-
lenging to reason about. In addition, fault tolerance
and scalability are serious concerns on commodity
hardware prone to failure. With traditional paral-
lel programming models (e.g., MPI), the developer
shoulders the burden of handling these issues. As a
result, just as much (if not more) effort is devoted to
system issues as to solving the actual problem.
Recently, Google’s MapReduce framework (Dean
and Ghemawat, 2004) has emerged as an attractive
alternative to existing parallel programming models.
The MapReduce abstraction shields the programmer
from having to explicitly worry about system-level
issues such as synchronization, data exchange, and
fault tolerance (see Section 2 for details). The run-
time is able to transparently distribute computations
across large clusters of commodity hardware with
good scaling characteristics. This frees the program-
mer to focus on actual MT issues.
In this paper we present MapReduce implementa-
tions of training algorithms for two kinds of models
commonly used in statistical MT today: a phrase-
based translation model (Koehn et al., 2003) and
word alignment models based on pairwise lexi-
cal translation trained using expectation maximiza-
tion (Dempster et al., 1977). Currently, such models
take days to construct using standard tools with pub-
licly available training corpora; our MapReduce im-
plementation cuts this time to hours. As an benefit
to the community, it is our intention to release this
code under an open source license.
It is worthwhile to emphasize that we present
these results as a “sweet spot” in the complex design
space of engineering decisions. In light of possible
tradeoffs, we argue that our solution can be consid-
ered fast (in terms of running time), easy (in terms
of implementation), and cheap (in terms of hard-
ware costs). Faster running times could be achieved
with more expensive hardware. Similarly, a custom
implementation (e.g., in MPI) could extract finer-
grained parallelism and also yield faster running
times. In our opinion, these are not worthwhile
tradeoffs. In the first case, financial constraints
are obvious. In the second case, the programmer
must explicitly manage all the complexities that
come with distributed processing (see above). In
contrast, our algorithms were developed within a
matter of weeks, as part of a “cloud computing”
course project (Lin, 2008). Experimental results
demonstrate that MapReduce provides nearly opti-
mal scaling characteristics, while retaining a high-
level problem-focused abstraction.
The remainder of the paper is structured as fol-
lows. In the next section we provide an overview of
MapReduce. In Section 3 we describe several gen-
eral solutions to computing maximum likelihood es-
timates for finite, discrete probability distributions.
Sections 4 and 5 apply these techniques to estimate
phrase translation models and perform EM for two
word alignment models. Section 6 reviews relevant
prior work, and Section 7 concludes.
</bodyText>
<sectionHeader confidence="0.993438" genericHeader="introduction">
2 MapReduce
</sectionHeader>
<bodyText confidence="0.998965214285714">
MapReduce builds on the observation that many
tasks have the same basic structure: a computation is
applied over a large number of records (e.g., parallel
sentences) to generate partial results, which are then
aggregated in some fashion. The per-record compu-
tation and aggregation function are specified by the
programmer and vary according to task, but the ba-
sic structure remains fixed. Taking inspiration from
higher-order functions in functional programming,
MapReduce provides an abstraction at the point of
these two operations. Specifically, the programmer
defines a “mapper” and a “reducer” with the follow-
ing signatures (square brackets indicate a list of ele-
ments):
</bodyText>
<construct confidence="0.945901">
map: (k1, v1) —* [(k2, v2)]
reduce: (k2, [v2]) —* [(k3, v3)]
</construct>
<figure confidence="0.990372840909091">
10000 100000 1e+06 1e+07
45 min
1.5 hrs
30 min
2 days
15 min
12 hrs
1 day
6 hrs
3 hrs
Training time
Translation quality
0.6
0.55
0.5
0.45
0.4
0.35
0.3
Time (seconds)
Translation quality (BLEU)
200
Method 1
Map1
Reduce1
Map2
Reduce2
Map3
Reduce3
hA, Bi → hhA, Bi, 1i
hhA, Bi, c(A, B)i
hhA, Bi, c(A, B)i → hhA,* i, c(A, B)i
hhA,* i, c(A)i
hhA, Bi, c(A, B)i → hA, hB, c(A, B)ii
hA, hB, c(A,B)
c(A) ii
map
Method 2
Map1
Reduce1
hA, Bi → hhA, Bi, 1i; hhA,* i,1i
hhA, Bi, c(A,B)
c(A) i
Method 3
</figure>
<figureCaption confidence="0.995778333333333">
Figure 2: Illustration of the MapReduce framework: the
“mapper” is applied to all input records, which generates
results that are aggregated by the “reducer”.
</figureCaption>
<bodyText confidence="0.999044928571428">
Key/value pairs form the basic data structure in
MapReduce. The “mapper” is applied to every input
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The “reducer” is applied
to all values associated with the same intermediate
key to generate output key/value pairs. This two-
stage processing structure is illustrated in Figure 2.
Under this framework, a programmer need only
provide implementations of map and reduce. On top
of a distributed file system (Ghemawat et al., 2003),
the runtime transparently handles all other aspects
of execution, on clusters ranging from a few to a few
thousand workers on commodity hardware assumed
to be unreliable, and thus is tolerant to various faults
through a number of error recovery mechanisms.
The runtime also manages data exchange, includ-
ing splitting the input across multiple map workers
and the potentially very large sorting problem be-
tween the map and reduce phases whereby interme-
diate key/value pairs must be grouped by key.
For the MapReduce experiments reported in this
paper, we used Hadoop version 0.16.0,3 which is
an open-source Java implementation of MapRe-
duce, running on a 20-machine cluster (1 master,
19 slaves). Each machine has two processors (run-
ning at either 2.4GHz or 2.8GHz), 4GB memory
(map and reduce tasks were limited to 768MB), and
100GB disk. All software was implemented in Java.
</bodyText>
<footnote confidence="0.797764">
3http://hadoop.apache.org/
</footnote>
<tableCaption confidence="0.9196615">
Table 1: Three methods for computing PMLE(B|A).
The first element in each tuple is a key and the second
element is the associated value produced by the mappers
and reducers.
</tableCaption>
<sectionHeader confidence="0.961301" genericHeader="method">
3 Maximum Likelihood Estimates
</sectionHeader>
<bodyText confidence="0.99999175">
The two classes of models under consideration are
parameterized with conditional probability distribu-
tions over discrete events, generally estimated ac-
cording to the maximum likelihood criterion:
</bodyText>
<equation confidence="0.9968735">
(A, B) = c(A, B) (1)
c(A) EB, c(A, B&apos;)
</equation>
<bodyText confidence="0.999991789473684">
Since this calculation is fundamental to both ap-
proaches (they distinguish themselves only by where
the counts of the joint events come from—in the case
of the phrase model, they are observed directly, and
in the case of the word-alignment models they are
the number of expected events in a partially hidden
process given an existing model of that process), we
begin with an overview of how to compute condi-
tional probabilities in MapReduce.
We consider three possible solutions to this prob-
lem, shown in Table 1. Method 1 computes the count
for each pair hA, Bi, computes the marginal c(A),
and then groups all the values for a given A together,
such that the marginal is guaranteed to be first and
then the pair counts follow. This enables Reducer3
to only hold the marginal value in memory as it pro-
cesses the remaining values. Method 2 works simi-
larly, except that the original mapper emits two val-
ues for each pair hA, Bi that is encountered: one that
</bodyText>
<equation confidence="0.967289">
hA, Bii → hA, hBi : 1ii
hA, hB1 : c(A,B1)
c(A) i, hB2 : c(A,B2)
c(A) i ··· i
Map1
Reduce1
PMLE (B  |A) = c
</equation>
<page confidence="0.967359">
201
</page>
<bodyText confidence="0.9609793125">
will be the marginal and one that contributes to the
pair count. The reducer groups all pairs together by
the A value, processes the marginal first, and, like
Method 1, must only keep this value in memory as
it processes the remaining pair counts. Method 2 re-
quires more data to be processed by the MapReduce
framework, but only requires a single sort operation
(i.e., fewer MapReduce iterations).
Method 3 works slightly differently: rather than
computing the pair counts independently of each
other, the counts of all the B events jointly occurring
with a particular A = a event are stored in an asso-
ciative data structure in memory in the reducer. The
marginal c(A) can be computed by summing over
all the values in the associative data structure and
then a second pass normalizes. This requires that
the conditional distribution P(B|A = a) not have
so many parameters that it cannot be represented
in memory. A potential advantage of this approach
is that the MapReduce framework can use a “com-
biner” to group many (A, B) pairs into a single value
before the key/value pair leaves for the reducer.4 If
the underlying distribution from which pairs (A, B)
has certain characteristics, this can result in a signifi-
cant reduction in the number of keys that the mapper
emits (although the number of statistics will be iden-
tical). And since all keys must be sorted prior to the
reducer step beginning, reducing the number of keys
can have significant performance impact.
The graph in Figure 3 shows the performance
of the three problem decompositions on two model
types we are estimating, conditional phrase trans-
lation probabilities (1.5M sentences, max phrase
length=7), and conditional lexical translation prob-
abilities as found in a word alignment model (500k
sentences). In both cases, Method 3, which makes
use of more memory to store counts of all B events
associated with event A = a, completes at least 50%
more quickly. This efficiency is due to the Zipfian
distribution of both phrases and lexical items in our
corpora: a few frequent items account for a large
portion of the corpus. The memory requirements
were also observed to be quite reasonable for the
4Combiners operate like reducers, except they run directly
on the output of a mapper before the results leave memory.
They can be used when the reduction operation is associative
and commutative. For more information refer to Dean and Ghe-
mawat (2004).
</bodyText>
<figure confidence="0.46147">
Estimation method
</figure>
<figureCaption confidence="0.999715">
Figure 3: PMLE computation strategies.
Figure 4: A word-aligned sentence. Examples
</figureCaption>
<bodyText confidence="0.99195975">
of consistent phrase pairs include (vi, i saw),
(la mesa pequena, the small table), and
(mesa pequena, small table); but, note that, for
example, it is not possible to extract a consistent phrase
corresponding to the foreign string la mesa or the English
string the small.
models in question: representing P(B|A = a) in the
phrase model required at most 90k parameters, and
in the lexical model, 128k parameters (i.e., the size
of the vocabulary for language B). For the remainder
of the experiments reported, we confine ourselves to
the use of Method 3.
</bodyText>
<sectionHeader confidence="0.998659" genericHeader="method">
4 Phrase-Based Translation
</sectionHeader>
<bodyText confidence="0.999955727272727">
In phrase-based translation, the translation process
is modeled by splitting the source sentence into
phrases (a contiguous string of words) and translat-
ing the phrases as a unit (Och et al., 1999; Koehn
et al., 2003). Phrases are extracted from a word-
aligned parallel sentence according to the strategy
proposed by Och et al. (1999), where every word in
a phrase is aligned only to other words in the phrase,
and not to any words outside the phrase bounds. Fig-
ure 4 shows an example aligned sentence and some
of the consistent subphrases that may be extracted.
</bodyText>
<figure confidence="0.999246733333333">
Method 1
Mathod 3
Method 2
1600
1400
1200
1000
600
400
200
800
0
Phrase pairs
Word pairs
Time (seconds)
</figure>
<page confidence="0.726623">
202
</page>
<figureCaption confidence="0.9973015">
Figure 5: Phrase model extraction and scoring times at
various corpus sizes.
</figureCaption>
<bodyText confidence="0.999382230769231">
Constructing a model involves extracting all the
phrase pairs (e, f) and computing the conditional
phrase translation probabilities in both directions.5
With a minor adjustment to the techniques intro-
duced in Section 3, it is possible to estimate P(B|A)
and P(A|B) concurrently.
Figure 5 shows the time it takes to construct
a phrase-based translation model using the Moses
tool, running on a single core, as well as the time
it takes to build the same model using our MapRe-
duce implementation. For reference, on the same
graph we plot a hypothetical, optimally-parallelized
version of Moses, which would run in 1of the time
</bodyText>
<page confidence="0.553782">
38
</page>
<bodyText confidence="0.9993711875">
required for the single-core version on our cluster.6
Although these represent completely different im-
plementations, this comparison offers a sense of
MapReduce’s benefits. The framework provides a
conceptually simple solution to the problem, while
providing an implementation that is both scalable
and fault tolerant—in fact, transparently so since
the runtime hides all these complexities from the re-
searcher. From the graph it is clear that the overhead
associated with the framework itself is quite low, es-
pecially for large quantities of data. We concede that
it may be possible for a custom solution (e.g., with
MPI) to achieve even faster running times, but we
argue that devoting resources to developing such a
solution would not be cost-effective.
Next, we explore a class of models where the stan-
</bodyText>
<footnote confidence="0.97097125">
5Following Och and Ney (2002), it is customary to combine
both these probabilities as feature values in a log-linear model.
6In our cluster, only 19 machines actually compute, and each
has two single-core processors.
</footnote>
<bodyText confidence="0.837677">
dard tools work primarily in memory, but where the
computational complexity of the models is greater.
</bodyText>
<sectionHeader confidence="0.988294" genericHeader="method">
5 Word Alignment
</sectionHeader>
<bodyText confidence="0.999919666666667">
Although word-based translation models have been
largely supplanted by models that make use of larger
translation units, the task of generating a word align-
ment, the mapping between the words in the source
and target sentences that are translationally equiva-
lent, remains crucial to nearly all approaches to sta-
tistical machine translation.
The IBM models, together with a Hidden Markov
Model (HMM), form a class of generative mod-
els that are based on a lexical translation model
P(fj|ei) where each word fj in the foreign sentence
fm1 is generated by precisely one word ei in the sen-
tence el1, independently of the other translation de-
cisions (Brown et al., 1993; Vogel et al., 1996; Och
and Ney, 2000). Given these assumptions, we let
the sentence translation probability be mediated by
a latent alignment variable (am1 in the equations be-
low) that specifies the pairwise mapping between
words in the source and target languages. Assum-
ing a given sentence length m for fm1 , the translation
probability is defined as follows:
</bodyText>
<equation confidence="0.948465666666667">
1: P(fm 1 |el 1) = P(fm1 ,am1 |el 1)
am
1
= P(am1 |el1, fm1 )
am
1
</equation>
<bodyText confidence="0.859691714285714">
Once the model parameters have been estimated, the
single-best word alignment is computed according
to the following decision rule:
tm 1 = arg max
am P(am1 |el1, fm1 )
In this section, we consider the MapReduce imple-
mentation of two specific alignment models:
</bodyText>
<listItem confidence="0.968860666666667">
1. IBM Model 1, where P(am 1 |el 1, fm1 ) is uniform
over all possible alignments.
2. The HMM alignment model where
</listItem>
<page confidence="0.491386">
P(am1 |el1, fm1 ) = Hmj=1 P(aj|aj_1).
</page>
<figure confidence="0.977057842105263">
10000 100000 1e+06 1e+07
Corpus size (sentences)
1.5 min
60 min
20 min
2 days
12 hrs
5 min
3 hrs
Moses training time
MapReduce training (38 M/R)
Optimal (Moses/38)
Time (seconds)
m
H P(fj|eat)
j=1
m
H P(fj|eat)
j=1
</figure>
<page confidence="0.998156">
203
</page>
<bodyText confidence="0.999984272727273">
Estimating the parameters for these models is more
difficult (and more computationally expensive) than
with the models considered in the previous section:
rather than simply being able to count the word pairs
and alignment relationships and estimate the mod-
els directly, we must use an existing model to com-
pute the expected counts for all possible alignments,
and then use these counts to update the new model.7
This training strategy is referred to as expectation-
maximization (EM) and is guaranteed to always im-
prove the quality of the prior model at each iteration
(Brown et al., 1993; Dempster et al., 1977).
Although it is necessary to compute a sum over all
possible alignments, the independence assumptions
made in these models allow the total probability of
generating a particular observation to be efficiently
computed using dynamic programming.8 The HMM
alignment model uses the forward-backward algo-
rithm (Baum et al., 1970), which is also an in-
stance of EM. Even with dynamic programming,
this requires O(5lm) operations for Model 1, and
O(5lm2) for the HMM model, where m and l are
the average lengths of the foreign and English sen-
tences in the training corpus, and 5 is the number of
sentences. Figure 6 shows measurements of the av-
erage iteration run-time for Model 1 and the HMM
alignment model as implemented in Giza++ (Och
and Ney, 2003), a state-of-the-art C++ implemen-
tation of the IBM and HMM alignment models that
is widely used. Five iterations are generally neces-
sary to train the models, so the time to carry out full
training of the models is approximately five times the
per-iteration run-time.
</bodyText>
<subsectionHeader confidence="0.998267">
5.1 EM with MapReduce
</subsectionHeader>
<bodyText confidence="0.999809333333333">
Expectation-maximization algorithms can be ex-
pressed quite naturally in the MapReduce frame-
work (Chu et al., 2006). In general, for discrete gen-
erative models, mappers iterate over the training in-
stances and compute the partial expected counts for
all the unobservable events in the model that should
</bodyText>
<footnote confidence="0.969717857142857">
7For the first iteration, when there is no prior model, a
heuristic, random, or uniform distribution may be chosen.
8For IBM Models 3-5, which are not our primary focus, dy-
namic programming is not possible, but the general strategy for
computing expected counts from a previous model and updat-
ing remains identical and therefore the techniques we suggest
in this section are applicable to those models as well.
</footnote>
<figure confidence="0.9181975">
10000 100000 1e+06
Corpus size (sentences)
</figure>
<figureCaption confidence="0.999392666666667">
Figure 6: Per-iteration average run-times for Giza++ im-
plementations of Model 1 and HMM training on corpora
of various sizes.
</figureCaption>
<bodyText confidence="0.999976269230769">
be associated with the given training instance. Re-
ducers aggregate these partial counts to compute
the total expected joint counts. The updated model
is estimated using the maximum likelihood crite-
rion, which just involves computing the appropri-
ate marginal and dividing (as with the phrase-based
models), and the same techniques suggested in Sec-
tion 3 can be used with no modification for this
purpose. For word alignment models, Method 3
is possible since word pairs distribute according to
Zipf’s law (meaning there is ample opportunity for
the combiners to combine records), and the number
of parameters for P(e|fj = f) is at most the num-
ber of items in the vocabulary of E, which tends to
be on the order of hundreds of thousands of words,
even for large corpora.
Since the alignment models we are considering
are fundamentally based on a lexical translation
probability model, i.e., the conditional probability
distribution P(e|f), we describe in some detail how
EM updates the parameters for this model.9 Using
the model parameters from the previous iteration (or
starting from an arbitrary or heuristic set of param-
eters during the first iteration), an expected count is
computed for every l x m pair (eZ7 fj) for each par-
allel sentence in the training corpus. Figure 7 illus-
</bodyText>
<footnote confidence="0.734594166666667">
9Although computation of expected count for a word pair
in a given training instance obviously depends on which model
is being used, the set of word pairs for which partial counts are
produced for each training instance, as well as the process of ag-
gregating the partial counts and updating the model parameters,
is identical across this entire class of models.
</footnote>
<figure confidence="0.994323909090909">
3 hrs
60 min
20 min
3m20s
90 s
30 s
10 s
3 s
Model 1
HMM
Average iteration latency (seconds)
</figure>
<page confidence="0.929609">
204
</page>
<figureCaption confidence="0.9620845">
Figure 7: Each cell in (a) contains the expected counts for
the word pair (ei, fj). In (b) the example training data is
marked to show which training instances contribute par-
tial counts for the pair (house, maison).
</figureCaption>
<figure confidence="0.981315">
10000 100000 1e+06
Corpus size (sentences)
</figure>
<figureCaption confidence="0.9665715">
Figure 8: Average per-iteration latency to train HMM
and Model 1 using the MapReduce EM trainer, compared
to an optimal parallelization of Giza++ across the same
number of processors.
</figureCaption>
<bodyText confidence="0.999577375">
trates the relationship between the individual train-
ing instances and the global expected counts for a
particular word pair. After collecting counts, the
conditional probability P(f|e) is computed by sum-
ming over all columns for each f and dividing. Note
that under this training regime, a non-zero probabil-
ity P(fj|ei) will be possible only if ei and fj co-
occur in at least one training instance.
</bodyText>
<subsectionHeader confidence="0.998628">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999971619047619">
Figure 8 shows the timing results of the MapReduce
implementation of Model 1 and the HMM alignment
model. Similar to the phrase extraction experiments,
we show as reference the running time of a hy-
pothetical, optimally-parallelized version of Giza++
on our cluster (i.e., values in Figure 6 divided by
38). Whereas in the single-core implementation the
added complexity of the HMM model has a signif-
icant impact on the per-iteration running time, the
data exchange overhead dominates in the perfor-
mance of both models in a MapReduce environment,
making running time virtually indistinguishable. For
these experiments, after each EM iteration, the up-
dated model parameters (which are computed in a
distributed fashion) are compiled into a compressed
representation which is then distributed to all the
processors in the cluster at the beginning of the next
iteration. The time taken for this process is included
in the iteration latencies shown in the graph. In fu-
ture work, we plan to use a distributed model repre-
sentation to improve speed and scalability.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.99981164516129">
Expectation-maximization algorithms have been
previously deployed in the MapReduce framework
in the context of several different applications (Chu
et al., 2006; Das et al., 2007; Wolfe et al., 2007).
Wolfe et al. (2007) specifically looked at the perfor-
mance of Model 1 on MapReduce and discuss how
several different strategies can minimize the amount
of communication required but they ultimately ad-
vocate abandoning the MapReduce model. While
their techniques do lead to modest performance im-
provements, we question the cost-effectiveness of
the approach in general, since it sacrifices many of
the advantages provided by the MapReduce envi-
ronment. In our future work, we instead intend to
make use of an approach suggested by Das et al.
(2007), who show that a distributed database run-
ning in tandem with MapReduce can be used to
provide the parameters for very large mixture mod-
els efficiently. Moreover, since the database is dis-
tributed across the same nodes as the MapReduce
jobs, many of the same data locality benefits that
Wolfe et al. (2007) sought to capitalize on will be
available without abandoning the guarantees of the
MapReduce paradigm.
Although it does not use MapReduce, the MTTK
tool suite implements distributed Model 1, 2 and
HMM training using a “home-grown” paralleliza-
tion scheme (Deng and Byrne, 2006). However, the
tool relies on a cluster where all nodes have access to
the same shared networked file storage, a restriction
that MapReduce does not impose.
</bodyText>
<figure confidence="0.982676181818182">
(a) maison la bleue fleur
the
blue
house
flower
la maison
the house
the blue house the flower
la maison bleue la fleur
Time (seconds)
60 min
20 min
3m20s
3 hrs
90 s
30 s
10 s
3 s
Optimal Model 1 (Giza/38)
Optimal HMM (Giza/38)
MapReduce Model 1 (38 M/R)
MapReduce HMM (38 M/R)
</figure>
<page confidence="0.996523">
205
</page>
<bodyText confidence="0.999972636363636">
There has been a fair amount of work inspired by
the problems of long latencies and excessive space
requirements in the construction of phrase-based
and hierarchical phrase-based translation models.
Several authors have advocated indexing the train-
ing data with a suffix array and computing the nec-
essary statistics during or immediately prior to de-
coding (Callison-Burch et al., 2005; Lopez, 2007).
Although this technique works quite well, the stan-
dard channel probability P(f|e) cannot be com-
puted, which is not a limitation of MapReduce.10
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.998442532258065">
We have shown that an important class of model-
building algorithms in statistical machine transla-
tion can be straightforwardly recast into the MapRe-
duce framework, yielding a distributed solution
that is cost-effective, scalable, robust, and exact
(i.e., doesn’t resort to approximations). Alterna-
tive strategies for parallelizing these algorithms ei-
ther impose significant demands on the developer,
the hardware infrastructure, or both; or, they re-
quire making unwarranted independence assump-
tions, such as dividing the training data into chunks
and building separate models. We have further
shown that on a 20-machine cluster of commodity
hardware, the MapReduce implementations have ex-
cellent performance and scaling characteristics.
Why does this matter? Given the difficulty of im-
plementing model training algorithms (phrase-based
model estimation is difficult because of the size of
data involved, and word-based alignment models are
a challenge because of the computational complex-
ity associated with computing expected counts), a
handful of single-core tools have come to be widely
used. Unfortunately, they have failed to scale with
the amount of training data available. The long la-
tencies associated with these tools on large datasets
imply that any kind of experimentation that relies on
making changes to variables upstream of the word
alignment process (such as, for example, altering the
training data f —* f&apos;, building anew model P (f&apos;|e),
and reevaluating) is severely limited by this state of
affairs. It is our hope that by reducing the cost of this
10It is an open question whether the channel probability
and inverse channel probabilities are both necessary. Lopez
(2008) presents results suggesting that P(f|e) is not necessary,
whereas Subotin (2008) finds the opposite.
these pieces of the translation pipeline, we will see a
greater diversity of experimental manipulations. To-
wards that end, we intend to release this code under
an open source license.
For our part, we plan to continue pushing the lim-
its of current word alignment models by moving to-
wards a distributed representation of the model pa-
rameters used in the expectation step of EM and
abandoning the compiled model representation. Fur-
thermore, initial experiments indicate that reorder-
ing the training data can lead to better data local-
ity which can further improve performance. This
will enable us to scale to larger corpora as well as
to explore different uses of translation models, such
as techniques for processing comparable corpora,
where a strict sentence alignment is not possible un-
der the limitations of current tools.
Finally, we note that the algorithms and tech-
niques we have described here can be readily ex-
tended to problems in other areas of NLP and be-
yond. HMMs, for example, are widely used in
ASR, named entity detection, and biological se-
quence analysis. In these areas, model estimation
can be a costly process, and therefore we believe
this work will be of interest for these applications
as well. It is our expectation that MapReduce will
also provide solutions that are fast, easy, and cheap.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999909363636364">
This work was supported by the GALE program of
the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. We would also
like to thank the generous hardware support of IBM
and Google via the Academic Cloud Computing Ini-
tiative. Specifically, thanks go out to Dennis Quan
and Eugene Hung from IBM for their tireless sup-
port of our efforts. Philip Resnik and Miles Osborne
provided helpful comments on an early draft. The
last author would like to thank Esther and Kiri for
their kind support.
</bodyText>
<sectionHeader confidence="0.998976" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9786128">
Luiz Andr´e Barroso, Jeffrey Dean, and Urs H¨olzle. 2003.
Web search for a planet: The Google cluster architec-
ture. IEEE Micro, 23(2):22–28.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
</reference>
<page confidence="0.981769">
206
</page>
<reference confidence="0.998546230769231">
ring in the statistical analysis of probabilistic functions
of Markov chains. Annals of Mathematical Statistics,
41(1):164–171.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858–867, Prague, Czech Re-
public.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL
2005), pages 255–262, Ann Arbor, Michigan.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-
tun. 2006. Map-Reduce for machine learning on mul-
ticore. In Advances in Neural Information Processing
Systems 19 (NIPS 2006), pages 281–288, Vancouver,
British Columbia, Canada.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personalization:
scalable online collaborative filtering. In Proceedings
of the 16th International Conference on World Wide
Web (WWW 2007), pages 271–280, Banff, Alberta,
Canada.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the 6th Symposium on Operating System
Design and Implementation (OSDI 2004), pages 137–
150, San Francisco, California.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistics Society,
39(1):1–38.
Yonggang Deng and William J. Byrne. 2006. MTTK:
An alignment toolkit for statistical machine transla-
tion. In Proceedings of the 2006 Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2006), Companion Volume, pages 265–
268, New York, New York.
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-
ung. 2003. The Google File System. In Proceedings
of the 19th ACM Symposium on Operating Systems
Principles (SOSP-03), pages 29–43, Bolton Landing,
New York.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2003), pages 48–54, Edmonton, Alberta, Canada.
Jimmy Lin. 2008. Exploring large-data issues in the cur-
riculum: A case study with MapReduce. In Proceed-
ings of the Third Workshop on Issues in Teaching Com-
putational Linguistics at ACL 2008, Columbus, Ohio.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 976–985, Prague, Czech
Republic.
Adam Lopez. 2008. Machine Translation by Pattern
Matching. Ph.D. dissertation, University of Maryland,
College Park, MD.
Franz Josef Och and Hermann Ney. 2000. A comparison
of alignment models for statistical machine translation.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING 2000), pages
1086–1090, Saarbrucken, Germany.
Franz Josef Och and Hermann Ney. 2002. Discrimini-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL 2002), pages 295–302, Philadelphia,
Pennsylvania.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
20–28, College Park, Maryland.
Michael Subotin. 2008. Exponential models for machine
translation. Master’s thesis, University of Maryland,
College Park, MD.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics (COLING 1996), pages 836–
841, Copenhagen, Denmark.
Jason Wolfe, Aria Delier Haghighi, and Daniel Klein.
2007. Fully distributed EM for very large datasets.
Technical Report UCB/EECS-2007-178, EECS De-
partment, University of California, Berkeley.
</reference>
<page confidence="0.998142">
207
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977840">
<title confidence="0.9961525">Fast, Easy, and Cheap: Construction Statistical Machine Translation Models with MapReduce</title>
<author confidence="0.999183">Christopher Dyer</author>
<author confidence="0.999183">Aaron Cordova</author>
<author confidence="0.999183">Alex Mont</author>
<author confidence="0.999183">Jimmy</author>
<affiliation confidence="0.9989165">Laboratory for Computational Linguistics and Information University of</affiliation>
<address confidence="0.996811">College Park, MD 20742,</address>
<email confidence="0.999884">redpony@umd.edu</email>
<abstract confidence="0.999675703703704">In recent years, the quantity of parallel training data available for statistical machine translation has increased far more rapidly than the performance of individual computers, resulting in a potentially serious impediment to progress. Parallelization of the modelbuilding algorithms that process this data on computer clusters is fraught with challenges such as synchronization, data exchange, and fault tolerance. However, the MapReduce programming paradigm has recently emerged as one solution to these issues: a powerful functional abstraction hides system-level details from the researcher, allowing programs to be transparently distributed across potentially very large clusters of commodity hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luiz Andr´e Barroso</author>
<author>Jeffrey Dean</author>
<author>Urs H¨olzle</author>
</authors>
<title>Web search for a planet: The Google cluster architecture.</title>
<date>2003</date>
<journal>IEEE Micro,</journal>
<volume>23</volume>
<issue>2</issue>
<marker>Barroso, Dean, H¨olzle, 2003</marker>
<rawString>Luiz Andr´e Barroso, Jeffrey Dean, and Urs H¨olzle. 2003. Web search for a planet: The Google cluster architecture. IEEE Micro, 23(2):22–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
<author>George Soules</author>
<author>Norman Weiss</author>
</authors>
<title>A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains.</title>
<date>1970</date>
<journal>Annals of Mathematical Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<contexts>
<context position="19350" citStr="Baum et al., 1970" startWordPosition="3118" endWordPosition="3121">pected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization (EM) and is guaranteed to always improve the quality of the prior model at each iteration (Brown et al., 1993; Dempster et al., 1977). Although it is necessary to compute a sum over all possible alignments, the independence assumptions made in these models allow the total probability of generating a particular observation to be efficiently computed using dynamic programming.8 The HMM alignment model uses the forward-backward algorithm (Baum et al., 1970), which is also an instance of EM. Even with dynamic programming, this requires O(5lm) operations for Model 1, and O(5lm2) for the HMM model, where m and l are the average lengths of the foreign and English sentences in the training corpus, and 5 is the number of sentences. Figure 6 shows measurements of the average iteration run-time for Model 1 and the HMM alignment model as implemented in Giza++ (Och and Ney, 2003), a state-of-the-art C++ implementation of the IBM and HMM alignment models that is widely used. Five iterations are generally necessary to train the models, so the time to carry </context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Annals of Mathematical Statistics, 41(1):164–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>858--867</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1608" citStr="Brants et al. (2007)" startWordPosition="220" endWordPosition="223">ty hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools. 1 Introduction Like many other NLP problems, output quality of statistical machine translation (SMT) systems increases with the amount of training data. Brants et al. (2007) demonstrated that increasing the quantity of training data used for language modeling significantly improves the translation quality of an ArabicEnglish MT system, even with far less sophisticated backoff models. However, the steadily increasing quantities of training data do not come without cost. Figure 1 shows the relationship between the amount of parallel Arabic-English training data used and both the translation quality of a state-ofthe-art phrase-based SMT system and the time required to perform the training with the widely-used Moses toolkit on a commodity server.1 Building a model us</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858–867, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="17349" citStr="Brown et al., 1993" startWordPosition="2776" endWordPosition="2779">een largely supplanted by models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P(fj|ei) where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1, independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for fm1 , the translation probability is defined as follows: 1: P(fm 1 |el 1) = P(fm1 ,am1 |el 1) am 1 = P(am1 |el1, fm1 ) am 1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: tm 1 = arg max am P(am1 |el1, fm1 ) </context>
<context position="19001" citStr="Brown et al., 1993" startWordPosition="3066" endWordPosition="3069">eat) j=1 m H P(fj|eat) j=1 203 Estimating the parameters for these models is more difficult (and more computationally expensive) than with the models considered in the previous section: rather than simply being able to count the word pairs and alignment relationships and estimate the models directly, we must use an existing model to compute the expected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization (EM) and is guaranteed to always improve the quality of the prior model at each iteration (Brown et al., 1993; Dempster et al., 1977). Although it is necessary to compute a sum over all possible alignments, the independence assumptions made in these models allow the total probability of generating a particular observation to be efficiently computed using dynamic programming.8 The HMM alignment model uses the forward-backward algorithm (Baum et al., 1970), which is also an instance of EM. Even with dynamic programming, this requires O(5lm) operations for Model 1, and O(5lm2) for the HMM model, where m and l are the average lengths of the foreign and English sentences in the training corpus, and 5 is t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Colin Bannard</author>
<author>Josh Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>255--262</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="26773" citStr="Callison-Burch et al., 2005" startWordPosition="4351" endWordPosition="4354">se flower la maison the house the blue house the flower la maison bleue la fleur Time (seconds) 60 min 20 min 3m20s 3 hrs 90 s 30 s 10 s 3 s Optimal Model 1 (Giza/38) Optimal HMM (Giza/38) MapReduce Model 1 (38 M/R) MapReduce HMM (38 M/R) 205 There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, the standard channel probability P(f|e) cannot be computed, which is not a limitation of MapReduce.10 7 Conclusions We have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the MapReduce framework, yielding a distributed solution that is cost-effective, scalable, robust, and exact (i.e., doesn’t resort to approximations). Alternative strategies for parallelizing these algorithms either impose significant demands on the developer, the hardware infrastructure</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>Chris Callison-Burch, Colin Bannard, and Josh Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 255–262, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheng T Chu</author>
<author>Sang K Kim</author>
<author>Yi A Lin</author>
<author>Yuanyuan Yu</author>
<author>Gary R Bradski</author>
<author>Andrew Y Ng</author>
<author>Kunle Olukotun</author>
</authors>
<title>Map-Reduce for machine learning on multicore.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 19 (NIPS</booktitle>
<pages>281--288</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="20174" citStr="Chu et al., 2006" startWordPosition="3260" endWordPosition="3263">sentences in the training corpus, and 5 is the number of sentences. Figure 6 shows measurements of the average iteration run-time for Model 1 and the HMM alignment model as implemented in Giza++ (Och and Ney, 2003), a state-of-the-art C++ implementation of the IBM and HMM alignment models that is widely used. Five iterations are generally necessary to train the models, so the time to carry out full training of the models is approximately five times the per-iteration run-time. 5.1 EM with MapReduce Expectation-maximization algorithms can be expressed quite naturally in the MapReduce framework (Chu et al., 2006). In general, for discrete generative models, mappers iterate over the training instances and compute the partial expected counts for all the unobservable events in the model that should 7For the first iteration, when there is no prior model, a heuristic, random, or uniform distribution may be chosen. 8For IBM Models 3-5, which are not our primary focus, dynamic programming is not possible, but the general strategy for computing expected counts from a previous model and updating remains identical and therefore the techniques we suggest in this section are applicable to those models as well. 10</context>
<context position="24781" citStr="Chu et al., 2006" startWordPosition="4017" endWordPosition="4020">, after each EM iteration, the updated model parameters (which are computed in a distributed fashion) are compiled into a compressed representation which is then distributed to all the processors in the cluster at the beginning of the next iteration. The time taken for this process is included in the iteration latencies shown in the graph. In future work, we plan to use a distributed model representation to improve speed and scalability. 6 Related work Expectation-maximization algorithms have been previously deployed in the MapReduce framework in the context of several different applications (Chu et al., 2006; Das et al., 2007; Wolfe et al., 2007). Wolfe et al. (2007) specifically looked at the performance of Model 1 on MapReduce and discuss how several different strategies can minimize the amount of communication required but they ultimately advocate abandoning the MapReduce model. While their techniques do lead to modest performance improvements, we question the cost-effectiveness of the approach in general, since it sacrifices many of the advantages provided by the MapReduce environment. In our future work, we instead intend to make use of an approach suggested by Das et al. (2007), who show th</context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2006</marker>
<rawString>Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu, Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun. 2006. Map-Reduce for machine learning on multicore. In Advances in Neural Information Processing Systems 19 (NIPS 2006), pages 281–288, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinandan S Das</author>
<author>Mayur Datar</author>
<author>Ashutosh Garg</author>
<author>Shyam Rajaram</author>
</authors>
<title>Google news personalization: scalable online collaborative filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web (WWW</booktitle>
<pages>271--280</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="24799" citStr="Das et al., 2007" startWordPosition="4021" endWordPosition="4024">eration, the updated model parameters (which are computed in a distributed fashion) are compiled into a compressed representation which is then distributed to all the processors in the cluster at the beginning of the next iteration. The time taken for this process is included in the iteration latencies shown in the graph. In future work, we plan to use a distributed model representation to improve speed and scalability. 6 Related work Expectation-maximization algorithms have been previously deployed in the MapReduce framework in the context of several different applications (Chu et al., 2006; Das et al., 2007; Wolfe et al., 2007). Wolfe et al. (2007) specifically looked at the performance of Model 1 on MapReduce and discuss how several different strategies can minimize the amount of communication required but they ultimately advocate abandoning the MapReduce model. While their techniques do lead to modest performance improvements, we question the cost-effectiveness of the approach in general, since it sacrifices many of the advantages provided by the MapReduce environment. In our future work, we instead intend to make use of an approach suggested by Das et al. (2007), who show that a distributed d</context>
</contexts>
<marker>Das, Datar, Garg, Rajaram, 2007</marker>
<rawString>Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th International Conference on World Wide Web (WWW 2007), pages 271–280, Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI 2004),</booktitle>
<pages>137--150</pages>
<location>San Francisco, California.</location>
<contexts>
<context position="4223" citStr="Dean and Ghemawat, 2004" startWordPosition="612" endWordPosition="615">c�2008 Association for Computational Linguistics Corpus size (sentences) Figure 1: Translation quality and training time as a function of corpus size. issues, concurrent operations are notoriously challenging to reason about. In addition, fault tolerance and scalability are serious concerns on commodity hardware prone to failure. With traditional parallel programming models (e.g., MPI), the developer shoulders the burden of handling these issues. As a result, just as much (if not more) effort is devoted to system issues as to solving the actual problem. Recently, Google’s MapReduce framework (Dean and Ghemawat, 2004) has emerged as an attractive alternative to existing parallel programming models. The MapReduce abstraction shields the programmer from having to explicitly worry about system-level issues such as synchronization, data exchange, and fault tolerance (see Section 2 for details). The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics. This frees the programmer to focus on actual MT issues. In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistica</context>
<context position="13484" citStr="Dean and Ghemawat (2004)" startWordPosition="2141" endWordPosition="2145">od 3, which makes use of more memory to store counts of all B events associated with event A = a, completes at least 50% more quickly. This efficiency is due to the Zipfian distribution of both phrases and lexical items in our corpora: a few frequent items account for a large portion of the corpus. The memory requirements were also observed to be quite reasonable for the 4Combiners operate like reducers, except they run directly on the output of a mapper before the results leave memory. They can be used when the reduction operation is associative and commutative. For more information refer to Dean and Ghemawat (2004). Estimation method Figure 3: PMLE computation strategies. Figure 4: A word-aligned sentence. Examples of consistent phrase pairs include (vi, i saw), (la mesa pequena, the small table), and (mesa pequena, small table); but, note that, for example, it is not possible to extract a consistent phrase corresponding to the foreign string la mesa or the English string the small. models in question: representing P(B|A = a) in the phrase model required at most 90k parameters, and in the lexical model, 128k parameters (i.e., the size of the vocabulary for language B). For the remainder of the experimen</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. In Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI 2004), pages 137– 150, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistics Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="5014" citStr="Dempster et al., 1977" startWordPosition="730" endWordPosition="733"> system-level issues such as synchronization, data exchange, and fault tolerance (see Section 2 for details). The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics. This frees the programmer to focus on actual MT issues. In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today: a phrasebased translation model (Koehn et al., 2003) and word alignment models based on pairwise lexical translation trained using expectation maximization (Dempster et al., 1977). Currently, such models take days to construct using standard tools with publicly available training corpora; our MapReduce implementation cuts this time to hours. As an benefit to the community, it is our intention to release this code under an open source license. It is worthwhile to emphasize that we present these results as a “sweet spot” in the complex design space of engineering decisions. In light of possible tradeoffs, we argue that our solution can be considered fast (in terms of running time), easy (in terms of implementation), and cheap (in terms of hardware costs). Faster running </context>
<context position="19025" citStr="Dempster et al., 1977" startWordPosition="3070" endWordPosition="3073">t) j=1 203 Estimating the parameters for these models is more difficult (and more computationally expensive) than with the models considered in the previous section: rather than simply being able to count the word pairs and alignment relationships and estimate the models directly, we must use an existing model to compute the expected counts for all possible alignments, and then use these counts to update the new model.7 This training strategy is referred to as expectationmaximization (EM) and is guaranteed to always improve the quality of the prior model at each iteration (Brown et al., 1993; Dempster et al., 1977). Although it is necessary to compute a sum over all possible alignments, the independence assumptions made in these models allow the total probability of generating a particular observation to be efficiently computed using dynamic programming.8 The HMM alignment model uses the forward-backward algorithm (Baum et al., 1970), which is also an instance of EM. Even with dynamic programming, this requires O(5lm) operations for Model 1, and O(5lm2) for the HMM model, where m and l are the average lengths of the foreign and English sentences in the training corpus, and 5 is the number of sentences. </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistics Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William J Byrne</author>
</authors>
<title>MTTK: An alignment toolkit for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2006), Companion Volume,</booktitle>
<pages>265--268</pages>
<location>New York, New York.</location>
<contexts>
<context position="25951" citStr="Deng and Byrne, 2006" startWordPosition="4209" endWordPosition="4212">pproach suggested by Das et al. (2007), who show that a distributed database running in tandem with MapReduce can be used to provide the parameters for very large mixture models efficiently. Moreover, since the database is distributed across the same nodes as the MapReduce jobs, many of the same data locality benefits that Wolfe et al. (2007) sought to capitalize on will be available without abandoning the guarantees of the MapReduce paradigm. Although it does not use MapReduce, the MTTK tool suite implements distributed Model 1, 2 and HMM training using a “home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. (a) maison la bleue fleur the blue house flower la maison the house the blue house the flower la maison bleue la fleur Time (seconds) 60 min 20 min 3m20s 3 hrs 90 s 30 s 10 s 3 s Optimal Model 1 (Giza/38) Optimal HMM (Giza/38) MapReduce Model 1 (38 M/R) MapReduce HMM (38 M/R) 205 There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchi</context>
</contexts>
<marker>Deng, Byrne, 2006</marker>
<rawString>Yonggang Deng and William J. Byrne. 2006. MTTK: An alignment toolkit for statistical machine translation. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2006), Companion Volume, pages 265– 268, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjay Ghemawat</author>
<author>Howard Gobioff</author>
<author>Shun-Tak Leung</author>
</authors>
<title>The Google File System. In</title>
<date>2003</date>
<booktitle>Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP-03),</booktitle>
<pages>29--43</pages>
<location>Bolton Landing, New York.</location>
<contexts>
<context position="8654" citStr="Ghemawat et al., 2003" startWordPosition="1324" endWordPosition="1327">mework: the “mapper” is applied to all input records, which generates results that are aggregated by the “reducer”. Key/value pairs form the basic data structure in MapReduce. The “mapper” is applied to every input key/value pair to generate an arbitrary number of intermediate key/value pairs. The “reducer” is applied to all values associated with the same intermediate key to generate output key/value pairs. This twostage processing structure is illustrated in Figure 2. Under this framework, a programmer need only provide implementations of map and reduce. On top of a distributed file system (Ghemawat et al., 2003), the runtime transparently handles all other aspects of execution, on clusters ranging from a few to a few thousand workers on commodity hardware assumed to be unreliable, and thus is tolerant to various faults through a number of error recovery mechanisms. The runtime also manages data exchange, including splitting the input across multiple map workers and the potentially very large sorting problem between the map and reduce phases whereby intermediate key/value pairs must be grouped by key. For the MapReduce experiments reported in this paper, we used Hadoop version 0.16.0,3 which is an ope</context>
</contexts>
<marker>Ghemawat, Gobioff, Leung, 2003</marker>
<rawString>Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. 2003. The Google File System. In Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP-03), pages 29–43, Bolton Landing, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL</booktitle>
<pages>48--54</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="4887" citStr="Koehn et al., 2003" startWordPosition="711" endWordPosition="714">existing parallel programming models. The MapReduce abstraction shields the programmer from having to explicitly worry about system-level issues such as synchronization, data exchange, and fault tolerance (see Section 2 for details). The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics. This frees the programmer to focus on actual MT issues. In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today: a phrasebased translation model (Koehn et al., 2003) and word alignment models based on pairwise lexical translation trained using expectation maximization (Dempster et al., 1977). Currently, such models take days to construct using standard tools with publicly available training corpora; our MapReduce implementation cuts this time to hours. As an benefit to the community, it is our intention to release this code under an open source license. It is worthwhile to emphasize that we present these results as a “sweet spot” in the complex design space of engineering decisions. In light of possible tradeoffs, we argue that our solution can be conside</context>
<context position="14386" citStr="Koehn et al., 2003" startWordPosition="2288" endWordPosition="2291">nsistent phrase corresponding to the foreign string la mesa or the English string the small. models in question: representing P(B|A = a) in the phrase model required at most 90k parameters, and in the lexical model, 128k parameters (i.e., the size of the vocabulary for language B). For the remainder of the experiments reported, we confine ourselves to the use of Method 3. 4 Phrase-Based Translation In phrase-based translation, the translation process is modeled by splitting the source sentence into phrases (a contiguous string of words) and translating the phrases as a unit (Och et al., 1999; Koehn et al., 2003). Phrases are extracted from a wordaligned parallel sentence according to the strategy proposed by Och et al. (1999), where every word in a phrase is aligned only to other words in the phrase, and not to any words outside the phrase bounds. Figure 4 shows an example aligned sentence and some of the consistent subphrases that may be extracted. Method 1 Mathod 3 Method 2 1600 1400 1200 1000 600 400 200 800 0 Phrase pairs Word pairs Time (seconds) 202 Figure 5: Phrase model extraction and scoring times at various corpus sizes. Constructing a model involves extracting all the phrase pairs (e, f) a</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2003), pages 48–54, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
</authors>
<title>Exploring large-data issues in the curriculum: A case study with MapReduce.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics at ACL</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6156" citStr="Lin, 2008" startWordPosition="915" endWordPosition="916">ementation), and cheap (in terms of hardware costs). Faster running times could be achieved with more expensive hardware. Similarly, a custom implementation (e.g., in MPI) could extract finergrained parallelism and also yield faster running times. In our opinion, these are not worthwhile tradeoffs. In the first case, financial constraints are obvious. In the second case, the programmer must explicitly manage all the complexities that come with distributed processing (see above). In contrast, our algorithms were developed within a matter of weeks, as part of a “cloud computing” course project (Lin, 2008). Experimental results demonstrate that MapReduce provides nearly optimal scaling characteristics, while retaining a highlevel problem-focused abstraction. The remainder of the paper is structured as follows. In the next section we provide an overview of MapReduce. In Section 3 we describe several general solutions to computing maximum likelihood estimates for finite, discrete probability distributions. Sections 4 and 5 apply these techniques to estimate phrase translation models and perform EM for two word alignment models. Section 6 reviews relevant prior work, and Section 7 concludes. 2 Map</context>
</contexts>
<marker>Lin, 2008</marker>
<rawString>Jimmy Lin. 2008. Exploring large-data issues in the curriculum: A case study with MapReduce. In Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics at ACL 2008, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>976--985</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="26787" citStr="Lopez, 2007" startWordPosition="4355" endWordPosition="4356"> the blue house the flower la maison bleue la fleur Time (seconds) 60 min 20 min 3m20s 3 hrs 90 s 30 s 10 s 3 s Optimal Model 1 (Giza/38) Optimal HMM (Giza/38) MapReduce Model 1 (38 M/R) MapReduce HMM (38 M/R) 205 There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, the standard channel probability P(f|e) cannot be computed, which is not a limitation of MapReduce.10 7 Conclusions We have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the MapReduce framework, yielding a distributed solution that is cost-effective, scalable, robust, and exact (i.e., doesn’t resort to approximations). Alternative strategies for parallelizing these algorithms either impose significant demands on the developer, the hardware infrastructure, or both; or,</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 976–985, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Machine Translation by Pattern Matching.</title>
<date>2008</date>
<institution>University of Maryland, College Park, MD.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="28638" citStr="Lopez (2008)" startWordPosition="4636" endWordPosition="4637">be widely used. Unfortunately, they have failed to scale with the amount of training data available. The long latencies associated with these tools on large datasets imply that any kind of experimentation that relies on making changes to variables upstream of the word alignment process (such as, for example, altering the training data f —* f&apos;, building anew model P (f&apos;|e), and reevaluating) is severely limited by this state of affairs. It is our hope that by reducing the cost of this 10It is an open question whether the channel probability and inverse channel probabilities are both necessary. Lopez (2008) presents results suggesting that P(f|e) is not necessary, whereas Subotin (2008) finds the opposite. these pieces of the translation pipeline, we will see a greater diversity of experimental manipulations. Towards that end, we intend to release this code under an open source license. For our part, we plan to continue pushing the limits of current word alignment models by moving towards a distributed representation of the model parameters used in the expectation step of EM and abandoning the compiled model representation. Furthermore, initial experiments indicate that reordering the training d</context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>Adam Lopez. 2008. Machine Translation by Pattern Matching. Ph.D. dissertation, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<pages>1086--1090</pages>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="17389" citStr="Och and Ney, 2000" startWordPosition="2784" endWordPosition="2787">ke use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P(fj|ei) where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1, independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for fm1 , the translation probability is defined as follows: 1: P(fm 1 |el 1) = P(fm1 ,am1 |el 1) am 1 = P(am1 |el1, fm1 ) am 1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: tm 1 = arg max am P(am1 |el1, fm1 ) In this section, we consider the MapRedu</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), pages 1086–1090, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminitive training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>295--302</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="16379" citStr="Och and Ney (2002)" startWordPosition="2618" endWordPosition="2621"> solution to the problem, while providing an implementation that is both scalable and fault tolerant—in fact, transparently so since the runtime hides all these complexities from the researcher. From the graph it is clear that the overhead associated with the framework itself is quite low, especially for large quantities of data. We concede that it may be possible for a custom solution (e.g., with MPI) to achieve even faster running times, but we argue that devoting resources to developing such a solution would not be cost-effective. Next, we explore a class of models where the stan5Following Och and Ney (2002), it is customary to combine both these probabilities as feature values in a log-linear model. 6In our cluster, only 19 machines actually compute, and each has two single-core processors. dard tools work primarily in memory, but where the computational complexity of the models is greater. 5 Word Alignment Although word-based translation models have been largely supplanted by models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminitive training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 295–302, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19771" citStr="Och and Ney, 2003" startWordPosition="3195" endWordPosition="3198">w the total probability of generating a particular observation to be efficiently computed using dynamic programming.8 The HMM alignment model uses the forward-backward algorithm (Baum et al., 1970), which is also an instance of EM. Even with dynamic programming, this requires O(5lm) operations for Model 1, and O(5lm2) for the HMM model, where m and l are the average lengths of the foreign and English sentences in the training corpus, and 5 is the number of sentences. Figure 6 shows measurements of the average iteration run-time for Model 1 and the HMM alignment model as implemented in Giza++ (Och and Ney, 2003), a state-of-the-art C++ implementation of the IBM and HMM alignment models that is widely used. Five iterations are generally necessary to train the models, so the time to carry out full training of the models is approximately five times the per-iteration run-time. 5.1 EM with MapReduce Expectation-maximization algorithms can be expressed quite naturally in the MapReduce framework (Chu et al., 2006). In general, for discrete generative models, mappers iterate over the training instances and compute the partial expected counts for all the unobservable events in the model that should 7For the f</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="14365" citStr="Och et al., 1999" startWordPosition="2284" endWordPosition="2287">le to extract a consistent phrase corresponding to the foreign string la mesa or the English string the small. models in question: representing P(B|A = a) in the phrase model required at most 90k parameters, and in the lexical model, 128k parameters (i.e., the size of the vocabulary for language B). For the remainder of the experiments reported, we confine ourselves to the use of Method 3. 4 Phrase-Based Translation In phrase-based translation, the translation process is modeled by splitting the source sentence into phrases (a contiguous string of words) and translating the phrases as a unit (Och et al., 1999; Koehn et al., 2003). Phrases are extracted from a wordaligned parallel sentence according to the strategy proposed by Och et al. (1999), where every word in a phrase is aligned only to other words in the phrase, and not to any words outside the phrase bounds. Figure 4 shows an example aligned sentence and some of the consistent subphrases that may be extracted. Method 1 Mathod 3 Method 2 1600 1400 1200 1000 600 400 200 800 0 Phrase pairs Word pairs Time (seconds) 202 Figure 5: Phrase model extraction and scoring times at various corpus sizes. Constructing a model involves extracting all the </context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Subotin</author>
</authors>
<title>Exponential models for machine translation. Master’s thesis,</title>
<date>2008</date>
<institution>University of Maryland, College Park, MD.</institution>
<contexts>
<context position="28719" citStr="Subotin (2008)" startWordPosition="4647" endWordPosition="4648">ining data available. The long latencies associated with these tools on large datasets imply that any kind of experimentation that relies on making changes to variables upstream of the word alignment process (such as, for example, altering the training data f —* f&apos;, building anew model P (f&apos;|e), and reevaluating) is severely limited by this state of affairs. It is our hope that by reducing the cost of this 10It is an open question whether the channel probability and inverse channel probabilities are both necessary. Lopez (2008) presents results suggesting that P(f|e) is not necessary, whereas Subotin (2008) finds the opposite. these pieces of the translation pipeline, we will see a greater diversity of experimental manipulations. Towards that end, we intend to release this code under an open source license. For our part, we plan to continue pushing the limits of current word alignment models by moving towards a distributed representation of the model parameters used in the expectation step of EM and abandoning the compiled model representation. Furthermore, initial experiments indicate that reordering the training data can lead to better data locality which can further improve performance. This </context>
</contexts>
<marker>Subotin, 2008</marker>
<rawString>Michael Subotin. 2008. Exponential models for machine translation. Master’s thesis, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics (COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="17369" citStr="Vogel et al., 1996" startWordPosition="2780" endWordPosition="2783">ed by models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P(fj|ei) where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1, independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for fm1 , the translation probability is defined as follows: 1: P(fm 1 |el 1) = P(fm1 ,am1 |el 1) am 1 = P(am1 |el1, fm1 ) am 1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: tm 1 = arg max am P(am1 |el1, fm1 ) In this section, we </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics (COLING 1996), pages 836– 841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Wolfe</author>
<author>Aria Delier Haghighi</author>
<author>Daniel Klein</author>
</authors>
<title>Fully distributed EM for very large datasets.</title>
<date>2007</date>
<tech>Technical Report UCB/EECS-2007-178,</tech>
<institution>EECS Department, University of California, Berkeley.</institution>
<contexts>
<context position="24820" citStr="Wolfe et al., 2007" startWordPosition="4025" endWordPosition="4028">ed model parameters (which are computed in a distributed fashion) are compiled into a compressed representation which is then distributed to all the processors in the cluster at the beginning of the next iteration. The time taken for this process is included in the iteration latencies shown in the graph. In future work, we plan to use a distributed model representation to improve speed and scalability. 6 Related work Expectation-maximization algorithms have been previously deployed in the MapReduce framework in the context of several different applications (Chu et al., 2006; Das et al., 2007; Wolfe et al., 2007). Wolfe et al. (2007) specifically looked at the performance of Model 1 on MapReduce and discuss how several different strategies can minimize the amount of communication required but they ultimately advocate abandoning the MapReduce model. While their techniques do lead to modest performance improvements, we question the cost-effectiveness of the approach in general, since it sacrifices many of the advantages provided by the MapReduce environment. In our future work, we instead intend to make use of an approach suggested by Das et al. (2007), who show that a distributed database running in ta</context>
</contexts>
<marker>Wolfe, Haghighi, Klein, 2007</marker>
<rawString>Jason Wolfe, Aria Delier Haghighi, and Daniel Klein. 2007. Fully distributed EM for very large datasets. Technical Report UCB/EECS-2007-178, EECS Department, University of California, Berkeley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>