<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014748">
<title confidence="0.986073">
Kea: Sentiment Analysis of Phrases Within Short Texts
</title>
<author confidence="0.990931">
Ameeta Agrawal, Aijun An
</author>
<affiliation confidence="0.9938925">
Department of Computer Science and Engineering
York University, Toronto, Canada M3J 1P3
</affiliation>
<email confidence="0.992838">
{ameeta, aan}@cse.yorku.ca
</email>
<sectionHeader confidence="0.993728" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999711588235294">
Sentiment Analysis has become an in-
creasingly important research topic. This
paper describes our approach to building a
system for the Sentiment Analysis in Twit-
ter task of the SemEval-2014 evaluation.
The goal is to classify a phrase within a
short piece of text as positive, negative
or neutral. In the evaluation, classifiers
trained on Twitter data are tested on data
from other domains such as SMS, blogs as
well as sarcasm. The results indicate that
apart from sarcasm, classifiers built for
sentiment analysis of phrases from tweets
can be generalized to other short text do-
mains quite effectively. However, in cross-
domain experiments, SMS data is found to
generalize even better than Twitter data.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.951570934782609">
In recent years, new forms of communication such
as microblogging and text messaging have become
quite popular. While there is no limit to the range
of information conveyed by tweets and short texts,
people often use these messages to share their sen-
timents. Working with these informal text gen-
res presents challenges for natural language pro-
cessing beyond those typically encountered when
working with more traditional text genres. Tweets
and short texts are shorter, the language is very
informal, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology such as, RT for “re-tweet”
and #hashtags for tagging (Rosenthal et al., 2014).
Although several systems have tackled the task
of analyzing sentiment from entire tweets, the
task of analyzing sentiments of phrases (a word
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
or more) within a tweet has remained largely un-
explored. This paper describes the details of
our system that participated in the subtask A
of Semeval-2014 Task 9: Sentiment Analysis in
Twitter (Rosenthal et al., 2014). The goal of this
task is to determine whether a phrase within a mes-
sage is positive, negative or neutral in that context.
Here, a message indicates any short informal piece
of text such as a tweet, SMS data, or a sentence
from Live Journal blog, which is a social network-
ing service where Internet users keep an online di-
ary. A phrase could be a word or a few consecutive
words within a message.
The novelty of this task lies in the fact that a
model built using only Twitter data is used to clas-
sify instances from other short text domains such
as SMS and Live Journal. Moreover, a short test
corpus of sarcastic tweets is also used to test the
performance of the sentiment classifier.
The main contributions of this paper include
a) developing a sentiment analysis classifer for
phrases; b) training on Twitter data and testing on
other domains such as SMS and Live Journal data
to see how well the classifier generalizes to differ-
ent types of text, and c) testing on sarcastic tweets.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999320928571429">
Sentiment analysis from Twitter data has attracted
much attention from the research community in
the past few years (Asiaee T. et al., 2012; Go et
al., 2009; Pang et al., 2002; Pang and Lee, 2004;
Wilson et al., 2005). However, most of these
approaches classify entire tweets by their overall
sentiment (positive, negative, or neutral).
The task at hand is to classify the sentiment of a
phrase within a short message. The challenges of
classifying contextual polarity of phrases has been
previously explored by first determining whether
the phrase is neutral or polar, and then disam-
biguating the polarity of the polar phrases (Wil-
son et al., 2005). Another approach entails using
</bodyText>
<page confidence="0.973121">
380
</page>
<note confidence="0.729191">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9984588">
manually developed patterns (Nasukawa and Yi,
2003). Both these techniques, however, experi-
mented with general web pages and online reviews
but not Twitter data.
Previously, a few systems that participated in
Semeval-2013: Sentiment Analysis in Twitter task
(Wilson et al., 2013; Mohammad et al., 2013;
Gunther and Furrer, 2013) tackled the problem of
sentiment analysis of phrases by training on data
that exclusively came from tweets and tested on
a corpus made up of tweets and SMS data. This
time though, the task is to see how well a system
trained on tweets will perform on not only SMS
data, but also blog sentences from Live Journal, as
well as sarcastic tweets.
</bodyText>
<sectionHeader confidence="0.994751" genericHeader="method">
3 Task Setup
</sectionHeader>
<bodyText confidence="0.999984333333333">
Formally, given a message containing a phrase
(one or more words), the task is to determine
whether that phrase is positive, negative or neutral
in that context. We were able to download 8880
tweets (7910 for training, and 970 for develop-
ment) from the corpus made available by the task
organizers, where each tweet includes a phrase
marked as positive, negative or neutral. Keywords
and hashtags were used to identify and collect
messages, which were then annotated using Ama-
zon Mechanical Turk. This task setup is further
described in the task description paper (Rosenthal
et al., 2014).
The evaluation consists of Twitter data as well
as surprise genres such as SMS, Live Journal and
Twitter Sarcasm. The purpose of hidden test gen-
res was to see how well a system trained on tweets
will perform on previously unseen domains.
</bodyText>
<sectionHeader confidence="0.98984" genericHeader="method">
4 System Description
</sectionHeader>
<bodyText confidence="0.926984">
This section describes the system components.
</bodyText>
<subsectionHeader confidence="0.995115">
4.1 Supervised Machine Learning
</subsectionHeader>
<bodyText confidence="0.999805">
During development time, we experimented with
various supervised machine learning classifiers,
but the final model was trained using Support Vec-
tor Machines (SVM) with a linear kernel as it out-
performed all other classifiers. The c value was
empirically selected and set to 1.
</bodyText>
<subsectionHeader confidence="0.719914">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.99330825">
For all tweets, the URL links and @username
mentions are replaced by “URL” and “username”
placeholders, respectively. The following features
were included in the final model:
</bodyText>
<listItem confidence="0.996715371428571">
• Prior polarities: Previous research (Agrawal
and An, 2013; Mohammad et al., 2013) has
shown prior polarities of words to be one
of the most important features in contex-
tual sentiment analysis of phrases. So, for
one of the features, the sum of the sentis-
cores of all the terms in the phrase was com-
puted from SentiWordNet (Esuli and Sebas-
tiani, 2006). For another feature, the prior
polarity of the phrase was estimated by aver-
aging the positive/negative strength of all its
terms by looking them up in the Subjectivity
Clues database (Wilson et al., 2005).
• Emoticons: An emoticon lexicon containing
frequent positive and negative emoticons, as
well as some of their misspellings that are
generally found in tweets, was created manu-
ally1. The prior positive and negative emoti-
con features contain the counts of all positive
and negative emoticons in the phrase.
• Lengths: Counts of the total number of words
in the phrase, the average number of char-
acters in the phrase, and the total number of
words in the message were included.
• Punctuation: Whether the phrase contains
punctuation such as ’?’, ’!’, ’...’, etc.
• Clusters: Word cluster IDs were obtained for
each term via unsupervised Brown clustering
of tweets (Owoputi et al., 2013). For exam-
ple, words such as anyone, anybody, any1,
ne1 and anyonee are all represented by clus-
ter path 0111011110. This allows grouping
multiple (mis)spellings of a word together,
which would otherwise be unique unigrams.
• Unigrams: Each phrase consists of one or
</listItem>
<bodyText confidence="0.8964685">
more words, with the average number of
words in a phrase being 2. We used only un-
igrams as bigrams were found to reduce the
accuracy on the development set.
</bodyText>
<sectionHeader confidence="0.996457" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.9985635">
The task organizers made available a test data set
composed of 10681 instances. Table 1 describes
</bodyText>
<footnote confidence="0.994266">
1http://goo.gl/fh6Pjr
</footnote>
<page confidence="0.972073">
381
</page>
<table confidence="0.999569538461539">
Test sets (# instances) Sentiment Example Phrase to be classified (in bold)
Twitter (6908) positive No school at the Cuse till Wednesday #hyped
negative i know it’s in january, but i can’t wait for Winter Jam !
neutral Bye bye Kyiv! See you in December:-*
SMS (2334) positive later on wanna catch a movie?
negative U had ur dinner already? She just wont believe wat i said, haiz..
neutral Im free on sat ... Ok we watch together lor
LiveJournal (1315) positive And Tess you are going to prom too on the same day as us as well
negative Does not seem likely that there would be any confusion.
neutral if i am ever king i will make it up to you.
TwitterSarcasm (124) positive @ImagineMore CHEER up. It’s Monday after all. #mondayblues
negative I may or may not be getting sick...perfect. #idontwantit
neutral @Ken Rosenthal mistakes? C’mon Kenny!! ;)
</table>
<tableCaption confidence="0.999881">
Table 1: Test corpus details.
</tableCaption>
<bodyText confidence="0.984782545454545">
the breakdown of the various types of text, with
example phrases that are to be classified.
As expected, Live Journal has a slightly more
formal sentence structure with properly spelt
words, whereas Twitter and SMS data include
more creative spellings. Clearly, the sarcasm cat-
egory includes messages with two contradictory
sentiments in close proximity. The challenge of
this task lies precisely in the fact that one classifier
trained on Twitter data should be able to general-
ize reasonably well on different types of text.
</bodyText>
<subsectionHeader confidence="0.989944">
5.1 Task Results
</subsectionHeader>
<bodyText confidence="0.999981385964912">
We participated in the constrained version of the
task which meant working with only the provided
Twitter training data without any additional an-
notated messages. The macro-average F1-scores
of the positive and negative classes, which were
the evaluation criteria for the task, of our sys-
tem (trained on Twitter training data and tested on
Twitter test, SMS and Live Journal blog data) are
presented in Table 2.
There are two interesting observations here:
firstly, even though the classifier was trained solely
on tweets, it performs equally well on SMS and
Live Journal data; and secondly, the sarcasm cate-
gory has the poorest overall performance, unsur-
prisingly. This suggests that cross-domain sen-
timent classification of phrases in short texts is
a feasible option. However, sarcasm seems to
be a subtle sentiment and calls for exploring fea-
tures that capture not only semantic but also syn-
tactic nuances. The low recall of the negative
sarcastic instances could be due to the fact that
30% of the negative phrases are hashtags (e.g.,
#don’tjudge, #smartmove, #killmenow, #sadlife,
#runninglate, #asthmaticproblems, #idontwantit),
that require term-splitting.
Further analysis reveals that generally the pos-
itive class has better F1-scores than the negative
class across all domains, except for the SMS data.
One possible reason for this could be the fact that,
while in all data sets (Twitter train, Twitter test,
Sarcasm test) the ratio of positive to negative in-
stances is nearly 2:1, the SMS test set is the only
one with class distribution different from the train-
ing set (with less positive instances than negative).
The extremely low F1-score for the neutral class is
perhaps also due to the skewed class distribution,
where in all data sets, the neutral instances only
make up about 4 to 9% of the data.
The positive class also has a better recall than
the negative class across all domains, which sug-
gests that the system is able to identify most of the
positive test instances, perhaps due to the bigger
proportion of positive training instances as well as
positive words in the polarity lexicons. One simple
way of improving the recall of the negative class
could be by increasing the number of negative in-
stances in the training set. In fact, in a prelimi-
nary experiment with an increased number of neg-
ative instances (resampled using SMOTE (Chawla
et al., 2002)), the macro-average F1-score of the
SMS data set improved by 0.5 points and that of
the Sarcasm set by almost 2 points. However,
there was no notable improvement in the Twitter
and Live Journal test sets.
We also ran some ablation experiments on the
test corpus after the submission to observe the in-
fluence of individual features on the classification
</bodyText>
<page confidence="0.995576">
382
</page>
<table confidence="0.999385166666667">
POS. NEG. NEU. AVG.
P R F P R F P R F
Twitter 87.6 89.7 88.6 82.4 76.2 79.2 23.3 28.2 25.5 83.90
SMS 75.9 89.9 82.3 89.8 82.4 86.0 32.7 10.7 16.1 84.14
LiveJournal 76.1 87.3 81.3 81.8 80.2 81.0 42.1 16.7 23.9 81.16
Sarcasm 77.0 93.9 84.6 72.2 35.1 47.3 16.7 20.0 18.2 65.94
</table>
<tableCaption confidence="0.999733">
Table 2: Macro-average F1-scores. P, R and F represent precision, recall and F1-score, respectively.
</tableCaption>
<bodyText confidence="0.9996281">
process. Table 3 reports the macro-average F1-
scores of the experiments. The “all features*”
scores here are different from those submitted as
the four test corpora were tested individually here
as opposed to all instances mixed into one data set.
The row “- prior polarities” indicates a feature set
that excludes the prior polarities feature, and its ef-
fect on the F1-score. MCB is the Majority Class
Baseline, whereas unigrams uses only the phrase
unigrams, with no additional features.
</bodyText>
<table confidence="0.999705111111111">
Twitter SMS Jour. Sarc.
MCB 39.65 31.45 33.40 39.80
unigrams 81.85 82.15 79.95 74.85
all features* 86.20 87.80 81.90 78.05
- prior polarity -1.8 -0.1 -0.05 -1.95
- lengths -0.3 0 -0.20 -1.3
- punctuation -0.45 -0.45 +0.10 -2.95
- emoticon lex -0.15 0 +0.05 0
- word clusters -0.15 -1.25 +0.05 -0.25
</table>
<tableCaption confidence="0.999964">
Table 3: Ablation tests: Trained on Twitter only.
</tableCaption>
<bodyText confidence="0.944872">
A few observations from the feature ablation
study include:
</bodyText>
<listItem confidence="0.776981222222222">
• The prior polarities and lengths seem to be
two of the most distinguishing features for
Twitter and Twitter Sarcasm, whereas for
SMS data, the word clusters are quite useful.
• While for Twitter Sarcasm, punctuation
seems to be the most important feature, it
has the opposite effect on the Live Journal
blog data. This may be because the punctua-
tion features learned from Twitter data do not
translate that well to blog data due to their
dissimilar writing styles.
• Even though the classifier was trained on
Twitter data, it has quite a strong performance
on the SMS data, which is rather unsurprising
in retrospect as both genres have similar char-
acter limits, which leads to creative spellings
and slang.
• While using all the features leads to almost 5
F1-score points improvement over unigrams
baseline in Twitter, SMS and Sarcasm data
sets, they increase only 2 F1-score points in
Live Journal blog data set, suggesting that
this feature set is only marginally suited for
blog instances. This prompted us to explore
the hypothesis: how well do SMS and Live
Journal data generalize to other domains, dis-
cussed in the following section.
</listItem>
<subsectionHeader confidence="0.998563">
5.2 Cross-domain Experiments
</subsectionHeader>
<bodyText confidence="0.999895631578948">
In this section, we test how well the classifiers
trained on one type of text classify other types of
text. In table 4, for example, the last row shows the
results of a model trained on Journal data (1000 in-
stances) and tested on Twitter, SMS and Sarcasm
test sets, and 10-fold cross-validated on Journal
data. Since this experiment measures the gener-
alizability of different data sets, we randomly se-
lected 500 positive and 500 negative instances for
each data set, in order to minimize the influence
of the size of the training data set on the classifi-
cation process. Note that this experiment does not
include the neutral class. As expected, the best
results on the test sets are obtained when using
cross-validation (except on Twitter set). However,
the model built using SMS data has the best or the
second-best result overall, which suggests that out
of the three types of text, it is the SMS data that
generalize the best.
</bodyText>
<table confidence="0.996477">
Test
Twitter SMS Journal
Twitter (1000) 76.4 (cv) 80.2 78.1
SMS (1000) 76.8 87.1 (cv) 79.4
Journal (1000) 73.8 82.8 85.3 (cv)
</table>
<tableCaption confidence="0.999609">
Table 4: Cross-domain training and tests.
</tableCaption>
<page confidence="0.998857">
383
</page>
<sectionHeader confidence="0.998797" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999927230769231">
This paper presents the details of our system that
participated in the subtask A of SemEval:2014:
Sentiment Analysis in Twitter. An SVM classifier
was trained on a feature set consisting of prior po-
larities, word clusters and various Twitter-specific
features. Our experiments indicate that prior po-
larities are one of the most important features in
the sentiment analysis of phrases from short texts.
Furthermore, a classifier trained on just tweets can
generalize considerably well to other texts such
as SMS and blog sentences, but not to sarcasm,
which calls for more research. Lastly, SMS data
generalizes to other texts better than Twitter data.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998543666666667">
We would like to thank the organizers of this task
for their effort and the reviewers for their use-
ful feedback. This research is funded in part by
the Centre for Information Visualization and Data
Driven Design (CIV/DDD) established by the On-
tario Research Fund.
</bodyText>
<sectionHeader confidence="0.997862" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997067194444444">
Ameeta Agrawal and Aijun An. 2013. Kea:
Expression-level sentiment analysis from Twitter
data. In Proceedings of the Seventh International
Workshop on Semantic Evaluation, SemEval ’13,
June.
Amir Asiaee T., Mariano Tepper, Arindam Banerjee,
and Guillermo Sapiro. 2012. If you are happy and
you know it... tweet. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, CIKM ’12, pages 1602–
1606, New York, NY, USA. ACM.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16(1):321–
357, June.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation, pages
417–422.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–6.
Tobias Gunther and Lenz Furrer. 2013. Gu-mlt-lt:
Sentiment analysis of short messages using linguis-
tic features and stochastic gradient descent. In Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ’13, June.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the seventh international workshop on
Semantic Evaluation Exercises (SemEval-2013), At-
lanta, Georgia, USA, June.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural lan-
guage processing. In Proceedings of the 2nd inter-
national conference on Knowledge capture, K-CAP
’03, pages 70–77, New York, NY, USA. ACM.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380–390.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using summarization based
on minimum cuts. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’04, Stroudsburg, PA, USA.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - Volume 10, EMNLP ’02,
pages 79–86, Stroudsburg, PA, USA.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. Semeval-2013 task 9:
Sentiment analysis in twitter. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), August.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354, Stroudsburg, PA, USA.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation (SemEval-2013), June.
</reference>
<page confidence="0.998984">
384
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.569787">
<title confidence="0.998279">Kea: Sentiment Analysis of Phrases Within Short Texts</title>
<author confidence="0.994153">Ameeta Agrawal</author>
<author confidence="0.994153">Aijun</author>
<affiliation confidence="0.992109">Department of Computer Science and</affiliation>
<note confidence="0.599819">York University, Toronto, Canada M3J</note>
<abstract confidence="0.998119555555555">Sentiment Analysis has become an increasingly important research topic. This paper describes our approach to building a system for the Sentiment Analysis in Twitter task of the SemEval-2014 evaluation. The goal is to classify a phrase within a short piece of text as positive, negative or neutral. In the evaluation, classifiers trained on Twitter data are tested on data from other domains such as SMS, blogs as well as sarcasm. The results indicate that apart from sarcasm, classifiers built for sentiment analysis of phrases from tweets can be generalized to other short text domains quite effectively. However, in crossdomain experiments, SMS data is found to generalize even better than Twitter data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ameeta Agrawal</author>
<author>Aijun An</author>
</authors>
<title>Kea: Expression-level sentiment analysis from Twitter data.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="6146" citStr="Agrawal and An, 2013" startWordPosition="992" endWordPosition="995">ystem Description This section describes the system components. 4.1 Supervised Machine Learning During development time, we experimented with various supervised machine learning classifiers, but the final model was trained using Support Vector Machines (SVM) with a linear kernel as it outperformed all other classifiers. The c value was empirically selected and set to 1. 4.2 Features For all tweets, the URL links and @username mentions are replaced by “URL” and “username” placeholders, respectively. The following features were included in the final model: • Prior polarities: Previous research (Agrawal and An, 2013; Mohammad et al., 2013) has shown prior polarities of words to be one of the most important features in contextual sentiment analysis of phrases. So, for one of the features, the sum of the sentiscores of all the terms in the phrase was computed from SentiWordNet (Esuli and Sebastiani, 2006). For another feature, the prior polarity of the phrase was estimated by averaging the positive/negative strength of all its terms by looking them up in the Subjectivity Clues database (Wilson et al., 2005). • Emoticons: An emoticon lexicon containing frequent positive and negative emoticons, as well as so</context>
</contexts>
<marker>Agrawal, An, 2013</marker>
<rawString>Ameeta Agrawal and Aijun An. 2013. Kea: Expression-level sentiment analysis from Twitter data. In Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Asiaee T</author>
<author>Mariano Tepper</author>
<author>Arindam Banerjee</author>
<author>Guillermo Sapiro</author>
</authors>
<title>If you are happy and you know it... tweet.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12,</booktitle>
<pages>1602--1606</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>T, Tepper, Banerjee, Sapiro, 2012</marker>
<rawString>Amir Asiaee T., Mariano Tepper, Arindam Banerjee, and Guillermo Sapiro. 2012. If you are happy and you know it... tweet. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 1602– 1606, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Kevin W Bowyer</author>
<author>Lawrence O Hall</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>SMOTE: Synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>357</pages>
<contexts>
<context position="11707" citStr="Chawla et al., 2002" startWordPosition="1920" endWordPosition="1923">neutral instances only make up about 4 to 9% of the data. The positive class also has a better recall than the negative class across all domains, which suggests that the system is able to identify most of the positive test instances, perhaps due to the bigger proportion of positive training instances as well as positive words in the polarity lexicons. One simple way of improving the recall of the negative class could be by increasing the number of negative instances in the training set. In fact, in a preliminary experiment with an increased number of negative instances (resampled using SMOTE (Chawla et al., 2002)), the macro-average F1-score of the SMS data set improved by 0.5 points and that of the Sarcasm set by almost 2 points. However, there was no notable improvement in the Twitter and Live Journal test sets. We also ran some ablation experiments on the test corpus after the submission to observe the influence of individual features on the classification 382 POS. NEG. NEU. AVG. P R F P R F P R F Twitter 87.6 89.7 88.6 82.4 76.2 79.2 23.3 28.2 25.5 83.90 SMS 75.9 89.9 82.3 89.8 82.4 86.0 32.7 10.7 16.1 84.14 LiveJournal 76.1 87.3 81.3 81.8 80.2 81.0 42.1 16.7 23.9 81.16 Sarcasm 77.0 93.9 84.6 72.2</context>
</contexts>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16(1):321– 357, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="6439" citStr="Esuli and Sebastiani, 2006" startWordPosition="1045" endWordPosition="1049">outperformed all other classifiers. The c value was empirically selected and set to 1. 4.2 Features For all tweets, the URL links and @username mentions are replaced by “URL” and “username” placeholders, respectively. The following features were included in the final model: • Prior polarities: Previous research (Agrawal and An, 2013; Mohammad et al., 2013) has shown prior polarities of words to be one of the most important features in contextual sentiment analysis of phrases. So, for one of the features, the sum of the sentiscores of all the terms in the phrase was computed from SentiWordNet (Esuli and Sebastiani, 2006). For another feature, the prior polarity of the phrase was estimated by averaging the positive/negative strength of all its terms by looking them up in the Subjectivity Clues database (Wilson et al., 2005). • Emoticons: An emoticon lexicon containing frequent positive and negative emoticons, as well as some of their misspellings that are generally found in tweets, was created manually1. The prior positive and negative emoticon features contain the counts of all positive and negative emoticons in the phrase. • Lengths: Counts of the total number of words in the phrase, the average number of ch</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of the 5th Conference on Language Resources and Evaluation, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--6</pages>
<location>Stanford,</location>
<contexts>
<context position="3344" citStr="Go et al., 2009" startWordPosition="540" endWordPosition="543">h as SMS and Live Journal. Moreover, a short test corpus of sarcastic tweets is also used to test the performance of the sentiment classifier. The main contributions of this paper include a) developing a sentiment analysis classifer for phrases; b) training on Twitter data and testing on other domains such as SMS and Live Journal data to see how well the classifier generalizes to different types of text, and c) testing on sarcastic tweets. 2 Related Work Sentiment analysis from Twitter data has attracted much attention from the research community in the past few years (Asiaee T. et al., 2012; Go et al., 2009; Pang et al., 2002; Pang and Lee, 2004; Wilson et al., 2005). However, most of these approaches classify entire tweets by their overall sentiment (positive, negative, or neutral). The task at hand is to classify the sentiment of a phrase within a short message. The challenges of classifying contextual polarity of phrases has been previously explored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Gunther</author>
<author>Lenz Furrer</author>
</authors>
<title>Gu-mlt-lt: Sentiment analysis of short messages using linguistic features and stochastic gradient descent.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="4339" citStr="Gunther and Furrer, 2013" startWordPosition="693" endWordPosition="696">ing whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384, Dublin, Ireland, August 23-24, 2014. manually developed patterns (Nasukawa and Yi, 2003). Both these techniques, however, experimented with general web pages and online reviews but not Twitter data. Previously, a few systems that participated in Semeval-2013: Sentiment Analysis in Twitter task (Wilson et al., 2013; Mohammad et al., 2013; Gunther and Furrer, 2013) tackled the problem of sentiment analysis of phrases by training on data that exclusively came from tweets and tested on a corpus made up of tweets and SMS data. This time though, the task is to see how well a system trained on tweets will perform on not only SMS data, but also blog sentences from Live Journal, as well as sarcastic tweets. 3 Task Setup Formally, given a message containing a phrase (one or more words), the task is to determine whether that phrase is positive, negative or neutral in that context. We were able to download 8880 tweets (7910 for training, and 970 for development) </context>
</contexts>
<marker>Gunther, Furrer, 2013</marker>
<rawString>Tobias Gunther and Lenz Furrer. 2013. Gu-mlt-lt: Sentiment analysis of short messages using linguistic features and stochastic gradient descent. In Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013),</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="4312" citStr="Mohammad et al., 2013" startWordPosition="689" endWordPosition="692">lored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384, Dublin, Ireland, August 23-24, 2014. manually developed patterns (Nasukawa and Yi, 2003). Both these techniques, however, experimented with general web pages and online reviews but not Twitter data. Previously, a few systems that participated in Semeval-2013: Sentiment Analysis in Twitter task (Wilson et al., 2013; Mohammad et al., 2013; Gunther and Furrer, 2013) tackled the problem of sentiment analysis of phrases by training on data that exclusively came from tweets and tested on a corpus made up of tweets and SMS data. This time though, the task is to see how well a system trained on tweets will perform on not only SMS data, but also blog sentences from Live Journal, as well as sarcastic tweets. 3 Task Setup Formally, given a message containing a phrase (one or more words), the task is to determine whether that phrase is positive, negative or neutral in that context. We were able to download 8880 tweets (7910 for training</context>
<context position="6170" citStr="Mohammad et al., 2013" startWordPosition="996" endWordPosition="999"> section describes the system components. 4.1 Supervised Machine Learning During development time, we experimented with various supervised machine learning classifiers, but the final model was trained using Support Vector Machines (SVM) with a linear kernel as it outperformed all other classifiers. The c value was empirically selected and set to 1. 4.2 Features For all tweets, the URL links and @username mentions are replaced by “URL” and “username” placeholders, respectively. The following features were included in the final model: • Prior polarities: Previous research (Agrawal and An, 2013; Mohammad et al., 2013) has shown prior polarities of words to be one of the most important features in contextual sentiment analysis of phrases. So, for one of the features, the sum of the sentiscores of all the terms in the phrase was computed from SentiWordNet (Esuli and Sebastiani, 2006). For another feature, the prior polarity of the phrase was estimated by averaging the positive/negative strength of all its terms by looking them up in the Subjectivity Clues database (Wilson et al., 2005). • Emoticons: An emoticon lexicon containing frequent positive and negative emoticons, as well as some of their misspellings</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd international conference on Knowledge capture, K-CAP ’03,</booktitle>
<pages>70--77</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4062" citStr="Nasukawa and Yi, 2003" startWordPosition="651" endWordPosition="654"> classify entire tweets by their overall sentiment (positive, negative, or neutral). The task at hand is to classify the sentiment of a phrase within a short message. The challenges of classifying contextual polarity of phrases has been previously explored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384, Dublin, Ireland, August 23-24, 2014. manually developed patterns (Nasukawa and Yi, 2003). Both these techniques, however, experimented with general web pages and online reviews but not Twitter data. Previously, a few systems that participated in Semeval-2013: Sentiment Analysis in Twitter task (Wilson et al., 2013; Mohammad et al., 2013; Gunther and Furrer, 2013) tackled the problem of sentiment analysis of phrases by training on data that exclusively came from tweets and tested on a corpus made up of tweets and SMS data. This time though, the task is to see how well a system trained on tweets will perform on not only SMS data, but also blog sentences from Live Journal, as well a</context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: capturing favorability using natural language processing. In Proceedings of the 2nd international conference on Knowledge capture, K-CAP ’03, pages 70–77, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<contexts>
<context position="7331" citStr="Owoputi et al., 2013" startWordPosition="1193" endWordPosition="1196">nd negative emoticons, as well as some of their misspellings that are generally found in tweets, was created manually1. The prior positive and negative emoticon features contain the counts of all positive and negative emoticons in the phrase. • Lengths: Counts of the total number of words in the phrase, the average number of characters in the phrase, and the total number of words in the message were included. • Punctuation: Whether the phrase contains punctuation such as ’?’, ’!’, ’...’, etc. • Clusters: Word cluster IDs were obtained for each term via unsupervised Brown clustering of tweets (Owoputi et al., 2013). For example, words such as anyone, anybody, any1, ne1 and anyonee are all represented by cluster path 0111011110. This allows grouping multiple (mis)spellings of a word together, which would otherwise be unique unigrams. • Unigrams: Each phrase consists of one or more words, with the average number of words in a phrase being 2. We used only unigrams as bigrams were found to reduce the accuracy on the development set. 5 Experiments and Discussion The task organizers made available a test data set composed of 10681 instances. Table 1 describes 1http://goo.gl/fh6Pjr 381 Test sets (# instances) </context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3383" citStr="Pang and Lee, 2004" startWordPosition="548" endWordPosition="551"> a short test corpus of sarcastic tweets is also used to test the performance of the sentiment classifier. The main contributions of this paper include a) developing a sentiment analysis classifer for phrases; b) training on Twitter data and testing on other domains such as SMS and Live Journal data to see how well the classifier generalizes to different types of text, and c) testing on sarcastic tweets. 2 Related Work Sentiment analysis from Twitter data has attracted much attention from the research community in the past few years (Asiaee T. et al., 2012; Go et al., 2009; Pang et al., 2002; Pang and Lee, 2004; Wilson et al., 2005). However, most of these approaches classify entire tweets by their overall sentiment (positive, negative, or neutral). The task at hand is to classify the sentiment of a phrase within a short message. The challenges of classifying contextual polarity of phrases has been previously explored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384, Dublin, I</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02,</booktitle>
<pages>79--86</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3363" citStr="Pang et al., 2002" startWordPosition="544" endWordPosition="547"> Journal. Moreover, a short test corpus of sarcastic tweets is also used to test the performance of the sentiment classifier. The main contributions of this paper include a) developing a sentiment analysis classifer for phrases; b) training on Twitter data and testing on other domains such as SMS and Live Journal data to see how well the classifier generalizes to different types of text, and c) testing on sarcastic tweets. 2 Related Work Sentiment analysis from Twitter data has attracted much attention from the research community in the past few years (Asiaee T. et al., 2012; Go et al., 2009; Pang et al., 2002; Pang and Lee, 2004; Wilson et al., 2005). However, most of these approaches classify entire tweets by their overall sentiment (positive, negative, or neutral). The task at hand is to classify the sentiment of a phrase within a short message. The challenges of classifying contextual polarity of phrases has been previously explored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), page</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP ’02, pages 79–86, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<contexts>
<context position="1610" citStr="Rosenthal et al., 2014" startWordPosition="246" endWordPosition="249">ng and text messaging have become quite popular. While there is no limit to the range of information conveyed by tweets and short texts, people often use these messages to share their sentiments. Working with these informal text genres presents challenges for natural language processing beyond those typically encountered when working with more traditional text genres. Tweets and short texts are shorter, the language is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology such as, RT for “re-tweet” and #hashtags for tagging (Rosenthal et al., 2014). Although several systems have tackled the task of analyzing sentiment from entire tweets, the task of analyzing sentiments of phrases (a word This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ or more) within a tweet has remained largely unexplored. This paper describes the details of our system that participated in the subtask A of Semeval-2014 Task 9: Sentiment Analysis in Twitter (Rosenthal et al., 2014). The goal of this task is </context>
<context position="5283" citStr="Rosenthal et al., 2014" startWordPosition="856" endWordPosition="859"> as sarcastic tweets. 3 Task Setup Formally, given a message containing a phrase (one or more words), the task is to determine whether that phrase is positive, negative or neutral in that context. We were able to download 8880 tweets (7910 for training, and 970 for development) from the corpus made available by the task organizers, where each tweet includes a phrase marked as positive, negative or neutral. Keywords and hashtags were used to identify and collect messages, which were then annotated using Amazon Mechanical Turk. This task setup is further described in the task description paper (Rosenthal et al., 2014). The evaluation consists of Twitter data as well as surprise genres such as SMS, Live Journal and Twitter Sarcasm. The purpose of hidden test genres was to see how well a system trained on tweets will perform on previously unseen domains. 4 System Description This section describes the system components. 4.1 Supervised Machine Learning During development time, we experimented with various supervised machine learning classifiers, but the final model was trained using Support Vector Machines (SVM) with a linear kernel as it outperformed all other classifiers. The c value was empirically selecte</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. Semeval-2013 task 9: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2014), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3405" citStr="Wilson et al., 2005" startWordPosition="552" endWordPosition="555"> of sarcastic tweets is also used to test the performance of the sentiment classifier. The main contributions of this paper include a) developing a sentiment analysis classifer for phrases; b) training on Twitter data and testing on other domains such as SMS and Live Journal data to see how well the classifier generalizes to different types of text, and c) testing on sarcastic tweets. 2 Related Work Sentiment analysis from Twitter data has attracted much attention from the research community in the past few years (Asiaee T. et al., 2012; Go et al., 2009; Pang et al., 2002; Pang and Lee, 2004; Wilson et al., 2005). However, most of these approaches classify entire tweets by their overall sentiment (positive, negative, or neutral). The task at hand is to classify the sentiment of a phrase within a short message. The challenges of classifying contextual polarity of phrases has been previously explored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384, Dublin, Ireland, August 23-24, </context>
<context position="6645" citStr="Wilson et al., 2005" startWordPosition="1080" endWordPosition="1083">ely. The following features were included in the final model: • Prior polarities: Previous research (Agrawal and An, 2013; Mohammad et al., 2013) has shown prior polarities of words to be one of the most important features in contextual sentiment analysis of phrases. So, for one of the features, the sum of the sentiscores of all the terms in the phrase was computed from SentiWordNet (Esuli and Sebastiani, 2006). For another feature, the prior polarity of the phrase was estimated by averaging the positive/negative strength of all its terms by looking them up in the Subjectivity Clues database (Wilson et al., 2005). • Emoticons: An emoticon lexicon containing frequent positive and negative emoticons, as well as some of their misspellings that are generally found in tweets, was created manually1. The prior positive and negative emoticon features contain the counts of all positive and negative emoticons in the phrase. • Lengths: Counts of the total number of words in the phrase, the average number of characters in the phrase, and the total number of words in the message were included. • Punctuation: Whether the phrase contains punctuation such as ’?’, ’!’, ’...’, etc. • Clusters: Word cluster IDs were obt</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013),</booktitle>
<contexts>
<context position="4289" citStr="Wilson et al., 2013" startWordPosition="685" endWordPosition="688">s been previously explored by first determining whether the phrase is neutral or polar, and then disambiguating the polarity of the polar phrases (Wilson et al., 2005). Another approach entails using 380 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 380–384, Dublin, Ireland, August 23-24, 2014. manually developed patterns (Nasukawa and Yi, 2003). Both these techniques, however, experimented with general web pages and online reviews but not Twitter data. Previously, a few systems that participated in Semeval-2013: Sentiment Analysis in Twitter task (Wilson et al., 2013; Mohammad et al., 2013; Gunther and Furrer, 2013) tackled the problem of sentiment analysis of phrases by training on data that exclusively came from tweets and tested on a corpus made up of tweets and SMS data. This time though, the task is to see how well a system trained on tweets will perform on not only SMS data, but also blog sentences from Live Journal, as well as sarcastic tweets. 3 Task Setup Formally, given a message containing a phrase (one or more words), the task is to determine whether that phrase is positive, negative or neutral in that context. We were able to download 8880 tw</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2013), June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>