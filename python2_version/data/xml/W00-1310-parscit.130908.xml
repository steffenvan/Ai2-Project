<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.976499">
Nonlocal Language Modeling
based on Context Co-occurrence Vectors
</title>
<author confidence="0.957338">
Sadao Kurohashi and Manabu On
</author>
<affiliation confidence="0.960149">
Graduate School of Informatics
Kyoto University
</affiliation>
<address confidence="0.720343">
Yoshida-honmachi, Sakyo, Kyoto, 606-8501 Japan
</address>
<email confidence="0.996034">
kuro@i.kyoto-u.ac.jp, ori@pine.kuee.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.99474" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965692307692">
This paper presents a novel nonlocal language
model which utilizes contextual information.
A reduced vector space model calculated from
co-occurrences of word pairs provides word
co-occurrence vectors. The sum of word co-
occurrence vectors represents the context of a
document, and the cosine similarity between
the context vector and the word co-occurrence
vectors represents the long-distance lexical de-
pendencies. Experiments on the Mainichi
Newspaper corpus show significant improve-
ment in perplexity (5.0% overall and 27.2%
on target vocabulary)
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976423076923">
Human pattern recognition rarely handles iso-
lated or independent objects. We recog-
nize objects in various spatiotemporal circum-
stances such as an object in a scene, a word
in an utterance. These circumstances work
as conditions, eliminating ambiguities and en-
abling robust recognition. The most challeng-
ing topics in machine pattern recognition are
in what representation and to what extent
those circumstances are utilized.
In language processing, a context—that is,
a portion of the utterance or the text before
the object—is an important circumstance.
One way of representing a context is statis-
tical language models which provide a word
sequence probability, P(wr), where wl de-
notes the sequence wi wi. In other words,
they provide the conditional probability of a
word given with the previous word sequence,
P(wilwri), which shows the prediction of a
word in a given context.
The most common language models used
nowadays are N-gram models based on a
(N — 1)-th order Markov process: event pre-
dictions depend on at most (N — 1) previous
events. Therefore, they offer the following ap-
</bodyText>
<equation confidence="0.8133065">
proximation:
P(wirwl —1) Pe, P(wilw1_—k+1) (1)
</equation>
<bodyText confidence="0.99998445">
A common value for N is 2 (bigram language
model) or 3 (trigram language model); only
a short local context of one or two words is
considered.
Even such a local context is effective in
some cases. For example, in Japanese, after
the word kokumu &apos;state affairs&apos;, words such as
dazjin &apos;minister&apos; and shou &apos;department&apos; likely
follow; kaijin &apos;monster&apos; and shou &apos;prize&apos; do
not. After dake de &apos;only at&apos;, you can often
find wa (topic-marker), but you hardly find
ga (nominative-marker) or wo (accusative-
marker). These examples show behaviors of
compound nouns and function word sequences
are well handled by bigram and trigram mod-
els. These models are exploited in several ap-
plications such as speech recognition, optical
character recognition and morphological anal-
ysis.
Local language models, however, cannot
predict much in some cases. For instance, the
word probability distribution after de wa &apos;at
(topic-marker)&apos; is very flat. However, even if
the probability distribution is flat in local lan-
guage models, the probability of daijin &apos;min-
ister&apos; and kaijin &apos;monster&apos; must be very differ-
ent in documents concerning politics. Bigram
and trigram models are obviously powerless
to such kind of nonlocal, long-distance lexical
dependencies.
This paper presents a nonlocal language
model. The important information concern-
ing long-distance lexical dependencies is the
word co-occurrence information. For example,
words such as politics, government, admin-
istration, department, tend to co-occur with
dazjin &apos;minister&apos;. It is easy to measure co-
occurrences of word pairs from a training cor-
pus, but utilizing them as a representation of
context is the problem. We present a vector
</bodyText>
<page confidence="0.913192">
80
</page>
<equation confidence="0.573343142857143">
D1 D2 D3 D4 D5 D6 D7 D8 WI W2 W3 W4 W5 W6
til 1 1 0 1 0 1 0 1 0 W1 4 9 1 9 9 1
W2 1 0 1 1 0 0 0 0 W2 3 0 9 0 0
W3 0 1 0 0 1 1 0 1 W3 4 1 1 9
W4 1 1 1 0 0 0 0 0 W4 3 0 0
W5 0 0 0 0 1 0 1 0 W5 9 1
w6 0 0 0 0 1 0 0 1 W6 9
</equation>
<figureCaption confidence="0.9968825">
Figure 1: Word-document co-occurrence ma-
trix.
</figureCaption>
<bodyText confidence="0.9985894">
representation of word co-occurrence informa-
tion, and show that the context can be repre-
sented as a sum of word co-occurrence vectors
in a document and it is incorporated in a non-
local language model.
</bodyText>
<sectionHeader confidence="0.942631" genericHeader="method">
2 Word Co-occurrence Vector
</sectionHeader>
<subsectionHeader confidence="0.987483">
2.1 .Word-Document Co-occurrence
Matrix
</subsectionHeader>
<bodyText confidence="0.99545432">
Word co-occurrences are directly represented
in a matrix whose rows correspond to words
and whose columns correspond to documents
(e.g. a newspaper article). The element of
the matrix is 1 if the word of the row ap-
pears in the document of the column (Figure
1). We call such a matrix a word-document
co-occurrence matrix.
The row-vectors of a word-document co-
occurrence matrix represent the co-occurrence
information of words. If two words tend to ap-
pear in the same documents, that is, tend to
co-occur. their row-vectors are similar, that is,
they point in similar directions.
The more document is considered, the more
reliable and realistic the co-occurrence infor-
mation will be. Then, the row size of a word-
document co-occurrence matrix may become
very large. Since enormous amounts of online
text are available these days, row size can be-
come more than a million documents. Then,
it is not practical to use a word-document co-
occurrence matrix as it is. It is necessary to
reduce row size and to simulate the tendency
in the original matrix by a reduced matrix.
</bodyText>
<subsectionHeader confidence="0.9986135">
2.2 Reduction of Word-Document
Co-occurrence Matrix
</subsectionHeader>
<bodyText confidence="0.987487333333333">
The aim of a word-document co-occurrence
matrix is to measure co-occurrence of two
words by the angle of the two row-vectors.
In the reduction of a matrix, angles of two
row-vectors in the original matrix should be
maintained in the reduced matrix.
</bodyText>
<figureCaption confidence="0.995114">
Figure 2: Word-word co-occurrence matrix.
</figureCaption>
<bodyText confidence="0.935532">
As such a matrix reduction, we utilized a
learning method developed by HNC Software
(Ilgen and Rushall, 1996). 1
</bodyText>
<figureCaption confidence="0.922452090909091">
1. Not the word-document co-occurrence
matrix is constructed from the learning
corpus, but a word-word co-occurrence
matrix. In this matrix, the rows and
columns correspond to words and the i-
th diagonal element denotes the number
of documents in which the word wi ap-
pears, F(wi). The i,j-th element denotes
the number of documents in which both
words wi and wi appear, F(wi,wi) (Fig-
ure 2).
</figureCaption>
<bodyText confidence="0.992087">
The important information in a word-
document co-occurrence matrix is the co-
sine of the angle of the row-vector of wi
and that of wj, which can be .calculated
by the word-word co-occurrence matrix
as follows:
</bodyText>
<equation confidence="0.96701">
F(wi.wj)
VF(wi)VF(wi)
</equation>
<bodyText confidence="0.999040916666667">
This is because ./F(w1) corresponds to
the magnitude of the row-vector of WI,
and F(wi, wi) corresponds to the dot
product of the row-vector of wi and
that of wi in the word-document co-
occurrence matrix.
9. Given a reduced row size, a matrix is ini-
tialized as follows: matrix elements are
chosen from a normal distribution ran-
domly, then each row-vector is normal-
ized to magnitude 1.0. The random unit
row-vector of the word wi is denoted as
</bodyText>
<equation confidence="0.3520385">
Rand
Wei .
</equation>
<bodyText confidence="0.614169833333333">
Random unit row-vectors in high di-
mensional floating point spaces have a
&apos;The goal of HNC was the enhancement of text
retrieval. The reduced word vectors were regarded as
semantic representation of words and used to represent
documents and queries.
</bodyText>
<figure confidence="0.973770346153846">
(2)
81
(CC • WC)2 Pc
kaigi (conference)
senkyo (election)
yosan (budget)
daijin (minister)
yakyu (baseball)
kaijin (monster)
sugaku (mathematics)
TOTAL
0.237962 0.002702
0.150773 0.001712
0.128907 0.001463
0.018549 0.000211
0.004556 0.000052
0.000002 0.000000
0.000001 0.000000
88.079230 1.000000
tori wa kakugi de kankyo mondai ni tuite
(Prime Minister) (Cabinet meeting) (environment) (issue)
WC
WC
wc
WC
I cc
</figure>
<figureCaption confidence="0.999843">
Figure 3: An example of context co-occurrence probabilities.
</figureCaption>
<bodyText confidence="0.976861142857143">
property that is referred to a &amp;quot;quasi-
orthogonality&amp;quot;. That is, the expected
Value of the dot product between any
pair of random row-vectors, wc Rand and
is approximately equal to zero
(i.e. all vectors are approximately or-
thogonal).
</bodyText>
<listItem confidence="0.8552915">
3. The trained row-vector, wei is calculated
as follows:
</listItem>
<equation confidence="0.570864333333333">
Rand
W Ci = W Rand -I- Ectijwc;
(3)
</equation>
<bodyText confidence="0.9474165">
aii corresponds to the degree of the co-
occurrence of two words. By adding
</bodyText>
<subsectionHeader confidence="0.682708">
Rand Inc
</subsectionHeader>
<bodyText confidence="0.979604428571429">
to
we. Rand depending on aii, the
learning formula (3) achieves that two
words that tend to co-occur will have
trained vectors that point in similar di-
rections. q is a design parameter chosen
to optimize performance. The formula
(4) is to normalize vectors to magnitude
1.0.
We call the trained row-vector wci of the
word wi a word co-occurrence vector.
The background of the above method is a
stochastic gradient descent procedure for min-
imizing the cost function:
</bodyText>
<equation confidence="0.891644">
1
J = — wci • wci
</equation>
<page confidence="0.883039">
5,3
</page>
<bodyText confidence="0.941118">
subject to the constraints tFwciII = 1.
The procedure iterates the following calcu-
lation:
</bodyText>
<figure confidence="0.8850015">
new OJ
W Ci = WC2 17awci
WCi E(aii _ wc,•wei)wc;
(6)
wanietv
IItvcewII
</figure>
<bodyText confidence="0.9996402">
The learning method by HNC is a rather
simple approximation of the procedure, doing
just one step of it. Note that wci wei is
approximately zero for the initialized random
vectors.
</bodyText>
<sectionHeader confidence="0.983799" genericHeader="method">
3 Context Co-occurrence Vector
</sectionHeader>
<bodyText confidence="0.9989911">
The next question is how to represent the
context of a document based on word co-
occurrence vectors. We propose a simple
model which represents the context as the sum
of the word co-occurrence vectors associated
with content words in a document so far. It
should be noted that the vector is normalized
to unit length. We call the resulting vector a
context co-occurrence vector.
Word co-occurrence vectors have the prop-
erty that words which tend to co-occur have
vectors that point in similar directions. Con-
text co-occurrence vectors are expected to
have the similar property. That is, if a word
tends to appear in a given context, the word
co-occurrence vector of the word and the con-
text co-occurrence vector of the context will
point in similar directions.
Such a context co-occurrence vector can be
seen to predict the occurrence of words in a
</bodyText>
<equation confidence="0.732905">
Wei
Wei =
I lwei I
(4)
)2 (5)
Cni ew
(7)
82
i-1 P(Cclwri) x if wi E
(tviin ) = P(C flwill) X P(tvilwrCf) if wi E Cf
</equation>
<bodyText confidence="0.783392">
where
</bodyText>
<equation confidence="0.99797825">
P(Ccitrii-1)
P(wiltviCiCe)
p(cif itv1-1.)
&apos;Cf)
with
.\1P(C) + A2P(Cclwi-1) + A3P(Cciwi-2wi--1)
Ad. P( wil Cc) + Ac2P(wi wi— i.Cc) + Ac3P(wiiw1-2wi-1Cc)
-1-A„Pc(wilwi-lCc)
= 1 - P(Cciwiri)
= A P( -I- X P( I
, Af3P(tviltvi_2Wi_iCf)
Ai + + A3 = 1, Aci + Ac2 + Ac3 Acc = 1, AD. + Af2 + Af3 = 1.
</equation>
<figureCaption confidence="0.997909">
Figure 4: Context language model.
</figureCaption>
<bodyText confidence="0.999732333333333">
given context, and is utilized as a component
of statistical language modeling, as shown in
the next section.
</bodyText>
<sectionHeader confidence="0.9553225" genericHeader="method">
4 Language Modeling using
Context Co-occurrence Vector
</sectionHeader>
<subsectionHeader confidence="0.990231">
4.1 Context Co-occurrence
Probability
</subsectionHeader>
<bodyText confidence="0.999937285714286">
The dot product of a context co-occurrence
vector and a word co-occurrence vector shows
the degree of affinity of the context and the
word. The probability of a content word based
on such dot products, called a context co-
occurrence probability, can be calculated as
follows:
</bodyText>
<equation confidence="0.73144375">
• i
Pc(wilwiCi Cc) = f (ccirl wc)
2.4wiEC, f(cc&apos; wcj)
(8)
</equation>
<bodyText confidence="0.999838">
where cei-1 denotes the context co-occurrence
vector of the left context, wi • • • wi-1, and Cc
denotes a content word class. Pc(
means the conditional probability of wi given
that a content word follows wr 1.
One choice for the function f (x) is the iden-
tity. However, a linear contribution of dot
products to the probability results in poorer
estimates, since the differences of dot prod-
ucts of related words (tend to co-occur) and
unrelated words are not so large. Experiments
showed that a,2 or x3 is a better estimate.
An example of context co-occurrence prob-
abilities is shown in Figure 3.
</bodyText>
<subsectionHeader confidence="0.9648125">
4.2 Language Modeling using Context
Co-occurrence Probability
</subsectionHeader>
<bodyText confidence="0.999944733333333">
Context co-occurrence probabilities can han-
dle long-distance lexical dependencies while a
standard trigram model can handle local con-
texts more clearly: in this way they comple-
ment each other. Therefore, language model-
ing of their linear interpolation is employed.
Note that the linear interpolation of unigram,
bigram and trigram models is simply referred
to `trigram model&apos; in this paper.
The proposed language model, called a con-
text language model, computes probabilities
as shown in Figure 4. Since context co-
occurrence probabilities are considered only
for content words (C,), probabilities are cal-
culated separately for content words (Ce) and
function words (c1).
P(Celwill) denotes the probability that a
content word follows w1-1, which is approx-
imated by a trigram model. P(wilwiliCe)
denotes the probability that wi follows will
given that a content word follows wr, which
is a linear interpolation of a standard trigram
model and the context co-occurrence proba-
bilities.
In the case of a function word, since the
context co-occurrence probability is not con-
sidered, P C f) is just a standard tri-
gram model.
A&apos;s adapt using an EM re-estimation proce-
dure on the held-out data.
</bodyText>
<equation confidence="0.789291">
wilwrice)
</equation>
<page confidence="0.998781">
83
</page>
<tableCaption confidence="0.999487">
Table 1: Perplexity results for the standard trigram model and the context language model.
</tableCaption>
<table confidence="0.998596611111111">
Language Model Perplexity on Perplexity on
the entire the target
vocabulary . b &amp;quot; &apos;
vocabulary
Standard Trigram Model 107.7 1930.2
Context Language Model
Vector size q f (x) 106.3 (-1.3%) 1663.8 (-13.8%)
500 0.5 X2
1000 0.3 X2 102.7 (-4.7%) 1495.9 (-22.5%)
1000 0.5 x 103.6 (-3.9%) 1496.1 (-22.5%)
1000 0.5 X2 102.4 (-5.0%) 1406.2 (-27.2%)
1000 0.5 X3 102.4 (-5.0%) 1416.8 (-26.9%)
1000 1.0 X2 102.5 (-4.8%) 1430.3 (-25.9%)
2000 0.5 X2 102.4 (-5.0%) 1408.1 (-27.1%)
Standard Bigram Model 130.28 2719.67
Context Language Model 125.06 (-4.0%) 2075.10 (-23.7%)
1000 0.5 x
1000 0.5 x2 122.85 (-5.7%) 1933.68 (-28.9%)
</table>
<figure confidence="0.8893726">
bei kabushiki shilyo no kyutou wo haikei ni Wall-gai ga kakkyou wo teishi ,
&apos;US&apos; stock&amp;quot;market&apos; &apos;sudden rise&apos; background&amp;quot;Wall Street&apos; activity&apos; &apos;show&apos;
wagayonoharu wo shite iru .
&apos;prosperity&apos; &apos;enjoy&apos; do&apos;
haute kara kako saikou eki wo
&apos;enter&apos; &apos;past&apos; maximum&amp;quot;profit&apos;
shouken kaisha . toushi ginkou wa 1996 nen ni
&apos;company&apos; investment&amp;quot;hank&apos; &apos;yeas&apos;
&apos;stock&apos;
ouka
koushin . &apos;96 nen no kabushiki souba wa &apos;95 nen
&apos;year&apos; &apos;market&apos; &apos;year&apos;
&apos;renew&apos; &apos;stock&apos;
ni tsuzuki kyushin . mata kabuka kyushin wo
&apos;continue&apos; rapid increase&apos; &apos;stock price&apos; rapidly increase&apos;
haikei
&apos;background&apos; &apos;corporation&apos;
ni kigyou no
shinkabu hakkou ga kako saikou to natta .
&apos;new stock&apos; issue&apos; &apos;past&apos; maximum&amp;quot;become&apos;
</figure>
<figureCaption confidence="0.9891825">
Figure 5: Comparison of probabilities of content words by the trigram model and the context
model. (Note that wa, ga, wo, ni, to and no are Japanese postpositions.)
</figureCaption>
<subsectionHeader confidence="0.997036">
4.3 Test Set Perplexity
</subsectionHeader>
<bodyText confidence="0.997243303030303">
By using the Mainichi Newspaper corpus
(from 1991 to 1997, 440,000 articles), test
set perplexities of a standard trigram/bigrarn
model and the proposed context language
model are compared. The articles of six
years were used for the learning of word co-
occurrence vectors, unigrams, bigrams and
trigrams; the articles of half a year were used
as a held-out data for EM re-estimation of A&apos;s;
the remaining articles (half a year) for com-
puting test set perplexities.
Word co-occurrence vectors were computed
for the top 50,000 frequent content words (ex-
cluding pronouns, numerals, temporal nouns,
and light verbs) in the corpus, and unigram,
bigram and trigram were computed for the top
60,000 frequent words.
The upper part of Table 1 shows the com-
parison results of the standard trigram model
and the context language model. For the best
parameters (marked by *), the overall per-
plexity decreased 5.0% and the perplexity on
target vocabulary (50,000 content words) de-
creased 27.2% relative to the standard trigram
model. For the best parameters, A&apos;s were
adapted as follows:
A1 = 0.08, A2 = 0.50, A3 = 0.42
Aci= 0.03, Aa = 0.30, Ac3 = 0.30, Acc = 0.17
A11 = 0.06, Af2 = 0.57, Af3 = 0.37
As for parameter settings, note that per-
formance is decreased by using shorter word
co-occurrence vector size. The variation of
q does not change the performance so much.
</bodyText>
<page confidence="0.994707">
84
</page>
<bodyText confidence="0.998238566666666">
f (x) = x2 and f (x) = x3 are almost the same,
better than f (x)- = x.
The lower part of Table 1 shows the compar-
ison results of the standard bigram model and
the context language model. Here, the context
language model is based on the bigram model,
that is, the terms concerning trigram in Fig-
ure 4 were eliminated. The result was similar,
but the perplexity decreased a bit more; 5.7%
overall and 28.9% on target vocabulary.
Figure 5 shows a test article in which the
probabilities of content words by the trigram
model and the context model are compared. If
that by the context model is bigger (i.e. the
context model predicts better), the word is
boxed; if not, the word is underlined.
The figure shows that the context model
usually performs better after a function word,
where the trigram model usually has little pre-
diction. On the other hand, the trigram model
performs better after a content word (i.e. in
a compound noun) because a clear prediction
by the trigram model is reduced by paying
attention to the relatively vague context co-
occurrence probability ( A „ is 0.17).
The proposed model is a constant interpo-
lation of a trigram model and the context co-
occurrence probabilities. More adaptive inter-
polation depending on the N-gram probabil-
ity distribution may improve the performance.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9992484">
Cache language models (Kuhn and de Mon.
1990) boost the probability of the words al-
ready seen in the history.
Trigger models (Lau et al.. 1993), even more
general, try to capture the co-occurrences be-
tween words. While the basic idea of our
model is similar to trigger models, they handle
co-occurrences of word pairs independently
and do not use a representation of the whole
context. This omission is also done in ap-
plications such as word sense disambiguation
(Yarowsky, 1994; FUNG et al., 1999).
Our model is the most related to Coccaro
and Jurafsky (1998), in that a reduced vec-
tor space approach was taken and context is
represented by the accumulation of word co-
occurrence vectors. Their model was reported
to decrease the test set perplexity by 12%,
compared to the bigram model. The major
differences are:
</bodyText>
<listItem confidence="0.6061655">
1. SVD (Singular Value Decomposition)
was used to reduce the matrix which is
</listItem>
<bodyText confidence="0.999755757575758">
common in the Latent Semantic Analysis
(Deerwester et al., 1990), and
9. context co-occurrence probabilities were
computed for all words, and the degree
of combination of context co-occurrence
probabilities and N-gram probabilities
was computed for each word, depending
on its distribution over the set of docu-
ments.
As for the first point, we utilized the
computationally-light, iteration-based proce-
dure. One reason for this is that the com-
putational cost of SVD is very high when
millions or more documents are processed.
Furthermore, considering an extension of our
model with a cognitive viewpoint, we believe
an iteration-based model seems more reason-
able than an algebraic model such as SVD.
As for the second point, we doubt the ap-
propriateness to use the word&apos;s distribution
as a measure of combination of two models.
What we need to do is to distinguish words
to which semantics should be considered and
other words. We judged the distinction of con-
tent words and function words is good enough
for that purpose, and developed their trigram-
based distinction as shown in Figure 4.
Several topic-based models have been pro-
posed based on the observation that certain
words tend to have different probability dis-
tributions in different topics. For example,
Florian and Yarowsky (1999) proposed the fol-
lowing model:
</bodyText>
<equation confidence="0.884032">
P(wilwiri)=EP(tIs)-Pi(wilwl:1,1)
</equation>
<bodyText confidence="0.9815320625">
(9)
where t denotes a topic id. Topics are
obtained by hierarchical clustering from a
training corpus, and a topic-specific language
model, Pt, is learned from the clustered docu-
ments. Reductions in perplexity relative to a
bigram model were 10.5% for the entire text
and 33.5% for the target vocabulary.
Topic-based models capture long-distance
lexical dependencies via intermediate topics.
In other words, the estimated distribution of
topics. P(t1w1), is the representation of a con-
text. Our model does not use such interme-
diate topics, but accesses word co-occurrence
information directly and represents a context
as the accumulation of this information.
</bodyText>
<page confidence="0.999668">
85
</page>
<sectionHeader confidence="0.998942" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9752951">
In this paper we described a novel language
model of incorporating long-distance lexical
dependencies based on context co-occurrence
vectors. Reduced vector representation of
word co-occurrences enables rather simple but
effective representation of the context. Sig-
nificant reductions in perplexity are obtained
relative to a standard trigram model, both on
the entire text (5.0%) and on the target vo-
cabulary (27.2%).
</bodyText>
<sectionHeader confidence="0.998165" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99357975">
The research described in this paper was sup-
ported in part by JSPS-RFTF96P00502 (The
Japan Society for the Promotion of Science,
Research for the Future Program).
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999463314285714">
Noah Coccaro and Daniel Jurafsky. 1998. To-
wards better integration of semantic predictors
in statistical language modeling. In Proceedings
of ICSLP-98, volume 6, pages 2403-2406.
Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for
Information Science, 41(6):391-407.
Radu Florian and David Yarowsky. 1999. Dy-
namic nonlocal language modeling via hierar-
chical topic-based adaptation. In Proceedings of
the 37rd Annual Meeting of ACL, pages 167-
174.
Pascale FUNG, LIU Xiaohu, and CHEUNG Chi
Shun. 1999. Mixed language query disambigua-
tion. In Proceedings of the 37rd Annual Meeting
of ACL, pages 333-340.
Mard R. Ilgen and David A. Rushall. 1996. Re-
cent advances in HNC&apos;s context vector informa-
tion retrieval technology. In TIPSTER PRO-
GRAM PHASE II, pages 149-158.
R. Kuhn and IL de Mori. 1990. A cache-based
natural language model for speech recognition.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12(6):570-583.
R. Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Trigger based language models: a max-
imum entropy approach. In Proceedings of
ICASSP, pages 45-48.
David Yarowsky. 1994. Decision lists for lexical
ambiguity resolution : Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of ACL, pages
88-995.
</reference>
<page confidence="0.998523">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.356066">
<title confidence="0.9985045">Nonlocal Language based on Context Co-occurrence Vectors</title>
<author confidence="0.93212">Sadao Kurohashi</author>
<author confidence="0.93212">Manabu</author>
<affiliation confidence="0.7457075">Graduate School of Kyoto</affiliation>
<address confidence="0.924876">Yoshida-honmachi, Sakyo, Kyoto, 606-8501</address>
<email confidence="0.976695">kuro@i.kyoto-u.ac.jp,ori@pine.kuee.kyoto-u.ac.jp</email>
<abstract confidence="0.967011785714286">This paper presents a novel nonlocal language model which utilizes contextual information. A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors. The sum of word cooccurrence vectors represents the context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the long-distance lexical dependencies. Experiments on the Mainichi Newspaper corpus show significant improvement in perplexity (5.0% overall and 27.2% on target vocabulary)</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in statistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98,</booktitle>
<volume>6</volume>
<pages>2403--2406</pages>
<contexts>
<context position="17237" citStr="Coccaro and Jurafsky (1998)" startWordPosition="2890" endWordPosition="2893">obability distribution may improve the performance. 5 Related Work Cache language models (Kuhn and de Mon. 1990) boost the probability of the words already seen in the history. Trigger models (Lau et al.. 1993), even more general, try to capture the co-occurrences between words. While the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context. This omission is also done in applications such as word sense disambiguation (Yarowsky, 1994; FUNG et al., 1999). Our model is the most related to Coccaro and Jurafsky (1998), in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors. Their model was reported to decrease the test set perplexity by 12%, compared to the bigram model. The major differences are: 1. SVD (Singular Value Decomposition) was used to reduce the matrix which is common in the Latent Semantic Analysis (Deerwester et al., 1990), and 9. context co-occurrence probabilities were computed for all words, and the degree of combination of context co-occurrence probabilities and N-gram probabilities was computed for each word, dependin</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>Noah Coccaro and Daniel Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. In Proceedings of ICSLP-98, volume 6, pages 2403-2406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="17633" citStr="Deerwester et al., 1990" startWordPosition="2956" endWordPosition="2959">and do not use a representation of the whole context. This omission is also done in applications such as word sense disambiguation (Yarowsky, 1994; FUNG et al., 1999). Our model is the most related to Coccaro and Jurafsky (1998), in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors. Their model was reported to decrease the test set perplexity by 12%, compared to the bigram model. The major differences are: 1. SVD (Singular Value Decomposition) was used to reduce the matrix which is common in the Latent Semantic Analysis (Deerwester et al., 1990), and 9. context co-occurrence probabilities were computed for all words, and the degree of combination of context co-occurrence probabilities and N-gram probabilities was computed for each word, depending on its distribution over the set of documents. As for the first point, we utilized the computationally-light, iteration-based procedure. One reason for this is that the computational cost of SVD is very high when millions or more documents are processed. Furthermore, considering an extension of our model with a cognitive viewpoint, we believe an iteration-based model seems more reasonable th</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>David Yarowsky</author>
</authors>
<title>Dynamic nonlocal language modeling via hierarchical topic-based adaptation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37rd Annual Meeting of ACL,</booktitle>
<pages>167--174</pages>
<contexts>
<context position="18858" citStr="Florian and Yarowsky (1999)" startWordPosition="3153" endWordPosition="3156">than an algebraic model such as SVD. As for the second point, we doubt the appropriateness to use the word&apos;s distribution as a measure of combination of two models. What we need to do is to distinguish words to which semantics should be considered and other words. We judged the distinction of content words and function words is good enough for that purpose, and developed their trigrambased distinction as shown in Figure 4. Several topic-based models have been proposed based on the observation that certain words tend to have different probability distributions in different topics. For example, Florian and Yarowsky (1999) proposed the following model: P(wilwiri)=EP(tIs)-Pi(wilwl:1,1) (9) where t denotes a topic id. Topics are obtained by hierarchical clustering from a training corpus, and a topic-specific language model, Pt, is learned from the clustered documents. Reductions in perplexity relative to a bigram model were 10.5% for the entire text and 33.5% for the target vocabulary. Topic-based models capture long-distance lexical dependencies via intermediate topics. In other words, the estimated distribution of topics. P(t1w1), is the representation of a context. Our model does not use such intermediate topi</context>
</contexts>
<marker>Florian, Yarowsky, 1999</marker>
<rawString>Radu Florian and David Yarowsky. 1999. Dynamic nonlocal language modeling via hierarchical topic-based adaptation. In Proceedings of the 37rd Annual Meeting of ACL, pages 167-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale FUNG</author>
<author>LIU Xiaohu</author>
<author>CHEUNG Chi Shun</author>
</authors>
<title>Mixed language query disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37rd Annual Meeting of ACL,</booktitle>
<pages>333--340</pages>
<contexts>
<context position="17175" citStr="FUNG et al., 1999" startWordPosition="2879" endWordPosition="2882">ore adaptive interpolation depending on the N-gram probability distribution may improve the performance. 5 Related Work Cache language models (Kuhn and de Mon. 1990) boost the probability of the words already seen in the history. Trigger models (Lau et al.. 1993), even more general, try to capture the co-occurrences between words. While the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context. This omission is also done in applications such as word sense disambiguation (Yarowsky, 1994; FUNG et al., 1999). Our model is the most related to Coccaro and Jurafsky (1998), in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors. Their model was reported to decrease the test set perplexity by 12%, compared to the bigram model. The major differences are: 1. SVD (Singular Value Decomposition) was used to reduce the matrix which is common in the Latent Semantic Analysis (Deerwester et al., 1990), and 9. context co-occurrence probabilities were computed for all words, and the degree of combination of context co-occurrence probabilities</context>
</contexts>
<marker>FUNG, Xiaohu, Shun, 1999</marker>
<rawString>Pascale FUNG, LIU Xiaohu, and CHEUNG Chi Shun. 1999. Mixed language query disambiguation. In Proceedings of the 37rd Annual Meeting of ACL, pages 333-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mard R Ilgen</author>
<author>David A Rushall</author>
</authors>
<title>Recent advances in HNC&apos;s context vector information retrieval technology.</title>
<date>1996</date>
<booktitle>In TIPSTER PROGRAM PHASE II,</booktitle>
<pages>149--158</pages>
<contexts>
<context position="5674" citStr="Ilgen and Rushall, 1996" startWordPosition="943" endWordPosition="946"> practical to use a word-document cooccurrence matrix as it is. It is necessary to reduce row size and to simulate the tendency in the original matrix by a reduced matrix. 2.2 Reduction of Word-Document Co-occurrence Matrix The aim of a word-document co-occurrence matrix is to measure co-occurrence of two words by the angle of the two row-vectors. In the reduction of a matrix, angles of two row-vectors in the original matrix should be maintained in the reduced matrix. Figure 2: Word-word co-occurrence matrix. As such a matrix reduction, we utilized a learning method developed by HNC Software (Ilgen and Rushall, 1996). 1 1. Not the word-document co-occurrence matrix is constructed from the learning corpus, but a word-word co-occurrence matrix. In this matrix, the rows and columns correspond to words and the ith diagonal element denotes the number of documents in which the word wi appears, F(wi). The i,j-th element denotes the number of documents in which both words wi and wi appear, F(wi,wi) (Figure 2). The important information in a worddocument co-occurrence matrix is the cosine of the angle of the row-vector of wi and that of wj, which can be .calculated by the word-word co-occurrence matrix as follows:</context>
</contexts>
<marker>Ilgen, Rushall, 1996</marker>
<rawString>Mard R. Ilgen and David A. Rushall. 1996. Recent advances in HNC&apos;s context vector information retrieval technology. In TIPSTER PROGRAM PHASE II, pages 149-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>IL de Mori</author>
</authors>
<title>A cache-based natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>12--6</pages>
<marker>Kuhn, de Mori, 1990</marker>
<rawString>R. Kuhn and IL de Mori. 1990. A cache-based natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6):570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
<author>Ronald Rosenfeld</author>
<author>Salim Roukos</author>
</authors>
<title>Trigger based language models: a maximum entropy approach.</title>
<date>1993</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>45--48</pages>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>R. Lau, Ronald Rosenfeld, and Salim Roukos. 1993. Trigger based language models: a maximum entropy approach. In Proceedings of ICASSP, pages 45-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution : Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of ACL,</booktitle>
<pages>88--995</pages>
<contexts>
<context position="17155" citStr="Yarowsky, 1994" startWordPosition="2877" endWordPosition="2878">probabilities. More adaptive interpolation depending on the N-gram probability distribution may improve the performance. 5 Related Work Cache language models (Kuhn and de Mon. 1990) boost the probability of the words already seen in the history. Trigger models (Lau et al.. 1993), even more general, try to capture the co-occurrences between words. While the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently and do not use a representation of the whole context. This omission is also done in applications such as word sense disambiguation (Yarowsky, 1994; FUNG et al., 1999). Our model is the most related to Coccaro and Jurafsky (1998), in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors. Their model was reported to decrease the test set perplexity by 12%, compared to the bigram model. The major differences are: 1. SVD (Singular Value Decomposition) was used to reduce the matrix which is common in the Latent Semantic Analysis (Deerwester et al., 1990), and 9. context co-occurrence probabilities were computed for all words, and the degree of combination of context co-occu</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution : Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of ACL, pages 88-995.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>