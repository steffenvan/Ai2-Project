<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<note confidence="0.444786">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 87-94.
Association for Computational Linguistics.
</note>
<title confidence="0.994101">
Extensions to HMM-based Statistical Word Alignment Models
</title>
<author confidence="0.998228">
Kristina Toutanova, H. Tolga Ilhan and Christopher D. Manning
</author>
<affiliation confidence="0.9874725">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.777768">
Stanford, CA 94305-9040 USA
</address>
<email confidence="0.994031333333333">
kristina@cs.stanford.edu
ilhan@stanford.edu
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999668916666667">
This paper describes improved HMM-based word
level alignment models for statistical machine
translation. We present a method for using part of
speech tag information to improve alignment accu-
racy, and an approach to modeling fertility and cor-
respondence to the empty word in an HMM align-
ment model. We present accuracy results from eval-
uating Viterbi alignments against human-judged
alignments on the Canadian Hansards corpus, as
compared to a bigram HMM, and IBM model 4.
The results show up to 16% alignment error reduc-
tion.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941105263158">
The main task in statistical machine translation is
to model the string translation probability
where the string in one language is translated
into another language as string . We refer to
as the source language string and as the target
language string in accordance with the noisy chan-
nel terminology used in the IBM models of (Brown
et al., 1993). Word-level translation models assume
a pairwise mapping between the words of the source
and target strings. This mapping is generated by
alignment models. In this paper we present exten-
sions to the HMM alignment model of (Vogel et al.,
1996; Och and Ney, 2000b). Some of our extensions
are applicable to other alignment models as well and
are of general utility.&apos;
For most language pairs huge amounts of parallel
corpora are not readily available whereas monolin-
gual resources such as taggers are more often avail-
able. Little research has gone into exploring the po-
</bodyText>
<footnote confidence="0.84158025">
&apos;This paper was supported in part by the National Science
Foundation under Grants IIS-0085896 and IIS-9982226. The
authors would also like to thank the various reviewers for their
helpful comments on earlier versions.
</footnote>
<bodyText confidence="0.999510043478261">
tential of part of speech information to better model
translation probabilities and permutation probabili-
ties. Melamed (2000) uses a very broad classifica-
tion of words (content, function and several punctu-
ation classes) to estimate class-specific parameters
for translation models. Fung and Wu (1995) adapt
English tags for Chinese language modeling using
Coerced Markov Models. They use English POS
classes as states of the Markov Model to generate
Chinese language words. In this paper we use POS
tag information to incorporate prior knowledge of
word translation and to model local word order vari-
ation. We show that using this information can help
in the translation modeling task.
Many alignment models assume a one to many
mapping from source language words to target lan-
guage words, such as the IBM models 1-5 of Brown
et al. (1993) and the HMM alignment model of (Vo-
gel et al., 1996). In addition, the IBM Models 3,
4 and 5 include a fertility model where is
the number of words aligned to a source word. In
HMM-based alignment word fertilities are not mod-
eled. The alignment positions of target words are the
states in an HMM. The alignment probabilities for
word depend only on the alignment of the pre-
vious word if using a first order HMM. There-
fore, source words are not awarded/penalized for be-
ing aligned to more than one target word. We present
an extension to HMM alignment that approximately
models word fertility.
Another assumption of existing alignment mod-
els is that there is a special Null word in the source
sentence from which all target words that do not
have other correspondences in the source language
are generated. Use of such a Null word has proven
problematic in many models. We also assume the
existence of a special Null word in the source lan-
guage that generates words in the target language.
However, we define a different model that better
constrains and conditions generation from Null. We
assume that the generation probability of words by
Null depends on other words in the target sentence.
Next we present the general equations for decom-
position of the translation probability using part of
speech tags and later we will go into more detail of
our extensions.
</bodyText>
<sectionHeader confidence="0.7501245" genericHeader="method">
2 Part of Speech Tags in a Translation
Model
</sectionHeader>
<bodyText confidence="0.999757518518519">
Augmenting the model with part of
speech tag information leads to the following equa-
tions. We use, or vector notation e, f to de-
note English and French strings. (and represent
the lengths of the French and English strings respec-
tively.) Let us define eT and fT as possible POS tag
sequences of the sentences e and f. We can rewrite
the string translation probability as fol-
lows (using Bayes rule to give the last line):
If we also assume that the taggers in both languages
generate a single tag sequence for each sentence then
the equation for machine translation by the noisy
channel model simplifies to
This is the decomposition of the string translation
probability into a language model and translation
model. In this paper we only address the transla-
tion model and assume that there exists a one-to-one
alignment from target to source words. Therefore,
Here each gives the index of the word to
which is aligned. The models we present in
this paper will differ in the decompositions of align-
ment probabilities, tag translation and word trans-
lation probabilities in Eqn. 1. Section 3 describes
the baseline model in more detail. Section 4 illus-
trates examples where the baseline model performs
poorly. Section 5 presents our extensions and Sec-
tion 6 presents experimental results.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="method">
3 Baseline Model
</sectionHeader>
<bodyText confidence="0.999747409090909">
Translation of French and English sentences shows a
strong localization effect. Words close to each other
in the source language remain close in the transla-
tion. Furthermore, most of the time the alignment
shows monotonicity. This means that pairwise align-
ments stay close to the diagonal line of the
plane. It has been shown (Vogel et al., 1996; Och
et al., 1999; Och and Ney, 2000a) that HMM based
alignment models are effective at capturing such lo-
calization.
We use as a baseline the model presented by
(Och and Ney, 2000a). A basic bigram HMM-based
model gives us
In this HMM model,2 alignment probabilities are
independent of word position and depend only on
jump width ( ).3 The Och and Ney (2000a)
model includes refinements including special treat-
ment of a jump to Null and smoothing with a uni-
form prior which we also included in our initial
model. As in their model we set the probability for
jump from any state to Null to a fixed value (✑)
which we estimated from held-out data.
</bodyText>
<footnote confidence="0.97676625">
2Each HMM state is [ ] emittingas output.
3In order for the model not to be deficient, we normalize the
jump probabilities at each EM step so that jumping outside of
the borders of the sentence is not possible.
</footnote>
<table confidence="0.6062552">
One possible way to rewrite
without loss of generality is:
P( ) P( ) P( ) P( ) total
Aln1 . national unity 0.0292 0.5741 0.05357 0.9766 8.77x
Aln2 . unity unity 0.0862 0.2789 0.31 0.9766 7.2x
</table>
<figureCaption confidence="0.99752">
Figure 1: The baseline model makes a simple alignment error.
</figureCaption>
<figure confidence="0.862799375">
j−2 j−1 j
PRE ADV PON PRP VB VB DT JJ VB PRE DT NN CC DT NN JJ PON
en outre , elle pourrait constituer une serieuse menace pour la confederation et le
unite nationale .
NULL in addition ,it could become a serious threat to confederation and national unity .
IN NN , PRP MD VB DT JJ NN TO NN CC JJ NN .
Aln1
Aln2
</figure>
<sectionHeader confidence="0.824203" genericHeader="method">
4 Alignment Irregularities
</sectionHeader>
<bodyText confidence="0.993231439024391">
Although the baseline Hidden Markov alignment
model successfully generates smooth alignments,
there are a fair number of alignment examples where
pairwise match shows local irregularities. One in-
stance of this is the transition of the NP JJ NN
rule to NP NN JJ from English to French. We can
list two main reasons why word translation proba-
bilities may not catch such irregularities to mono-
tonicity. First, it may be the case that both the
English adjective and noun are words that are un-
known. In this case the translation probabilities will
be close to each other after smoothing. Second, the
adjective-noun pair may consist of words that are
frequently seen together in English. National re-
serve and Canadian parliament, are examples of
such pairs. As a result there will be an indirect asso-
ciation between the English noun and the translation
of the English adjective. In both cases, word transla-
tion probabilities will not be differentiating enough
and alignment probabilities become the dominating
factor to determine wherealigns.
Figure 1 illustrates how our baseline HMM model
makes an alignment mistake of this sort. The ta-
ble in the figure displays alignment and translation
probabilities of two competing alignments (namely
Aln1 and Aln2) for the last three words. In both
alignments, the shownand are periods at the
end of the French and English sentences. The first
alignment maps nationale to national and unit´e to
unity. (i.e. national and =unity). The
second alignment maps both nationale and unit´e to
unity (i.e. unity and unity). Start-
ing from the unity-unit´e alignment, the jump width
sequences( ), ( ) for Aln1
and Aln2 are , 2 and0, 1 respectively. The
table shows that the gain from use of monotonic
alignment probabilities dominates over the lowered
word translation probability. Although national and
nationale are strongly correlated according to the
translation probabilities, jump widths of and 2
are less probable than jump widths of 0 and 1.
</bodyText>
<sectionHeader confidence="0.995789" genericHeader="method">
5 Extensions
</sectionHeader>
<bodyText confidence="0.99979275">
In this section we describe our improvements on the
HMM model. We present evaluation results in Sec-
tion 6 after describing the technical details of our
models here.
</bodyText>
<subsectionHeader confidence="0.965926">
5.1 POS Tags for Translation Probabilities
</subsectionHeader>
<bodyText confidence="0.999964666666667">
Our model with part of speech tags for translation
probabilities uses the following simplification of the
translation probability shown in Eqn. 1.4
</bodyText>
<equation confidence="0.531384">
(3)
</equation>
<bodyText confidence="0.954228266666667">
In this model we introduce tag translation probabil-
ities as an extra factor to Eqn. 2. Intuitively the role
of this factor is to boost the translation probabilities
for words of parts of speech that can often be trans-
lations of each other. Thus this probability distribu-
tion provides prior knowledge of the possible trans-
lations of a word based only on its part of speech.
However, P( ) should not be too sharp or
4Since we are only concerned with alignment here and not
generation of candidate translations the factor P( e,eT) can
be ignored and we omit it from the equations for the rest of the
paper.
it will dominate the alignment probabilities and the
probabilities . We use the following linear
interpolation to smooth tag translation probabilities:
</bodyText>
<equation confidence="0.533484">
(4)
</equation>
<bodyText confidence="0.999526555555556">
T is the size of the French tag set and is set to be
0.1 in our experiments. The tag translation model is
so heavily smoothed with a uniform distribution be-
cause in EM the tag translation probabilities quickly
become very sharp and can easily overrule the align-
ment and word translation probabilities. The Results
section shows that the addition of this factor reduces
the alignment error rate, with the improvement being
especially large when the training data size is small.
</bodyText>
<subsectionHeader confidence="0.997705">
5.2 Tag Sequences for Jump Probabilities
</subsectionHeader>
<bodyText confidence="0.999110836065574">
This section describes an extension to the bigram
HMM model that uses source and target language
tag sequences as conditioning information when
predicting the alignment of target language words.
In the decomposition of the joint proba-
bility shown in Eqn. 1
the factor for alignment probabilities is
.
A bigram HMM model assumes independence of
from anything but the previous alignment posi-
tion and the length of the English sentence.
Brown et al. (1993) and Och et al. (1999) variably
condition this probability on the English word in
position and/or the French word in position
. As conditioning directly on words would yield
a large number of parameters and would be imprac-
tical, they cluster the words automatically into bilin-
gual word classes.
The question arises then whether we would have
larger gains by conditioning on the part of speech
tags of those words or even more words around the
alignment position. For example, if we use the fol-
lowing conditioning information:
we could model probabilities of transpositions and
insertion of function words in the target language
that have no corresponding words in the source lan-
guage ( is Null) similarly to the channel oper-
ations of the (Yamada and Knight, 2001) syntax-
based statistical translation model. Since the syntac-
tic knowledge provided by POS tags is quite limited,
this is a crude model of transpositions and Null in-
sertions at the preterminal level. However we could
still expect that it would help in modeling local
word order variations. For example, in the sentence
J’aime la chute ‘I love the fall’ the probability of
aligning la ( DT) to the will be boosted
by knowing VBP and DT.
Similarly, in the sentence J’aime des chiens ‘I love
dogs’ the probability of aligning la to Null
will be increased by knowing VBP and
NNS. VBP followed by NNS crudely
conducts the information that the verb is followed by
a noun phrase which does not include a determiner.
We conducted a series of experiments where
the alignment probabilities are conditioned on
different subsets of the part of speech tags
.
In order to be able to condition on
when generating an alignment position for ,
we have to change the generative model for the
sentence f and its tag sequence fT to generate the
part of speech tags for the French words before
choosing alignment positions for them. The French
POS tags could be generated for example from
a prior distribution or from the previous
French tags as in an HMM for part-of-speech tag-
ging. The generative model becomes:
This model makes the assumption that target words
are independent of their tags given the correspond-
ing source word and models only the dependence of
alignment positions on part of speech tags.
</bodyText>
<subsectionHeader confidence="0.999221">
5.3 Modeling Fertility
</subsectionHeader>
<bodyText confidence="0.9833694">
A major advantage of the IBM models 3–5 over the
HMM alignment model is the presence of a model
of source word fertility. Thus knowledge that some
words translate as phrases in the target language is
incorporated in the model.
The HMM model has no memory, apart from the
previous alignment, about how many words it has
aligned to a source word. Yet even this memory is
not used to decide whether to generate more words
from a given English word. The decision to gener-
ate again (to make a jump of size 0) is independent
of the word and is estimated over all words in the
corpus.
We extended the HMM model to decide whether
to generate more words from the previous English
word or to move on to a different word de-
pending on the identity of the English word .
We introduced a factor stay where the
boolean random variable stay depends on the En-
glish word aligned to. Since in most cases
words with fertility greater than one generate words
that are consecutive in the target language, this
extension approximates fertility modeling. More
specifically, the baseline model (i.e., Eqn. 2) is
changed as follows:
where
in Eqn. 5 is the Kronecker delta func-
tion. Basically, the new alignment probabilities
state that a jump width of zero de-
pends on the English word. If we define the fertility
of a word as the number of consecutive words from
the target language it generates, then the probabil-
ity distribution for the fertility of an English word e
according to this model is geometric with a proba-
bility of success stay . The expectation is
.5 Even though the fit of this distribution
stay
to the real fertility distribution may not be very good,
this approximation improves alignment accuracy in
practice.
Sparsity is a problem in estimating stay probabil-
ities P(stay ). We use the probability of a jump
of size zero from the baseline model as our prior to
do smoothing as follows:
stay stay (6)
</bodyText>
<equation confidence="0.922139">
SE[X] __+ + + ...
where X is the number of Bernoulli trials until the first success.
</equation>
<bodyText confidence="0.982745666666667">
in this equation is the alignment probability
from the baseline model with zero jump distance.
.
</bodyText>
<subsectionHeader confidence="0.871615">
5.4 Translation Model for Null
</subsectionHeader>
<bodyText confidence="0.999300974358975">
As originally proposed by Brown et al. (1993),
words in the target sentence for which there are no
corresponding English words are assumed to be gen-
erated by the special English word Null. Null ap-
pears in every English sentence and often serves to
generate syntactic elements in the target language
that are missing in the source. A probability distri-
bution Nullfor generation probabilities of the
Null is re-estimated from a training corpus.
Modeling a Null word has proven problematic. It
has required many special fixes to keep models from
aligning everything to Null or to keep them from
aligning nothing to Null (Och and Ney, 2000b). This
might stem from the problem that the Null is respon-
sible for generating syntactic elements of the target
language as well as generating words that make the
target language sentence more idiomatic and stylis-
tic. The intuition for our model of translation proba-
bilities for target words that do not have correspond-
ing source words is that these words are generated
from the special English Null and also from the next
word in the target language by a mixture model. The
pair la conf´ed´eration in Figure 1 is an example of
such case where conf´ed´eration contributes extra in-
formation in generation of la. The formula for the
probability of a target word given that it does not
have a corresponding aligning word in the source is:
We re-estimate the probabilities
Null from the training cor-
pus using EM. The dependence of a French
word on the next French word requires a change
in the generative model to first propose align-
ments for all words in the French sentence and
to then generate the French words given their
alignments, starting from the end of the sentence
and going towards the beginning. For the new
model there is an efficient dynamic programming
algorithm for computations in EM similar to the
forward-backward algorithm. The probability
</bodyText>
<equation confidence="0.8505176">
stay
stay (5)
Null
Null(7)
again decom-
</equation>
<bodyText confidence="0.995224666666667">
poses into forward and backward probabilities.
The forward probability is
and the backward
probability is .
These can be computed recursively and used for
efficient computation of posteriors in EM.
</bodyText>
<sectionHeader confidence="0.999827" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99942740625">
We present results on word level alignment accu-
racy using the Hansards corpus. Our test data con-
sists of manually aligned sentences which are
the same data set used by (Och and Ney, 2000b).6
In the annotated sentences every alignment between
two words is labeled as either a sure (S) or possible
(P) alignment. (S P). We used the following quan-
tity (called alignment error rate or AER) to evaluate
the alignment quality of our models, which is also
the evaluation metric used by (Och and Ney, 2000b):
We divided this annotated data into a validation
set of sentences and a final test set of sen-
tences. The validation set was used to select tuning
parameters such as in Eqn. 4, 6 and 7. We report
AER results on the final test set of sentences
which contain a total of English and
French words. We experimented with training cor-
pora of different sizes ranging from 5K to 50K sen-
tences. We concentrated on small to medium data
sets to assess the ability of our models to deal with
sparse data.
Table 1 shows the percentage of words in the cor-
pus that were seen less than the specified number of
times. For example, in our 10K training corpus
of all word types were seen only once. As seen from
the table the sparsity is great even for large corpora.
The models we implemented and compare in this
section are the following:
Baseline is the baseline HMM model described
in section 2
Tags is an HMM model that includes tags for
translation probabilities (section 5.1)
</bodyText>
<footnote confidence="0.7688035">
6We want to thank Franz Och for sharing the annotated data
with us.
</footnote>
<bodyText confidence="0.998783555555556">
SG is an HMM model that includes stay proba-
bilities (section 5.3)
Null is an HMM model that includes the new gen-
eration model for words by Null (section 5.4)
Tags+Null, Tags+SG, and Tags+Null+SG are
combinations of the above models
Table 2 shows AER results for our improved
models on training corpora of increasing size. The
model Null outperforms the baseline at every data
set size,with the error reduction being larger for big-
ger training sets (up to 9.2% error reduction). The
SG model reduces the baseline error rate by up to
10%. The model Tags reduces the error rate for the
smallest dataset by 7.6%. The combination of Tags
and the SG or Null models outperforms the individ-
ual models in the combination since they address
different problems and make orthogonal mistakes.
The combination of SG and Tags reduces the base-
line error rate by up to 16% and the combination of
Null and Tags reduces the error rate by up to 12.3%.
All of these error reductions are statistically signifi-
cant at the confidence level according to the
paired t-test. The combination Tags+Null+SG fur-
ther reduces the error rate. For small datasets, there
seems to be a stronger overlap between the strengths
of the Null and SG models because some fertility
related phenomena can be accounted for by both
models. When an English word is wrongly align-
ing to several consecutive French words because of
indirect association, while the correct alignment of
some of them is to the empty word, both the Null and
SG models can combat the problem— one by better
modeling correspondence to Null, and the other by
discouraging large fertilities.
Figure 2 displays learning curves for three mod-
els: Och, Tags, and Tags+Null. Och is the HMM
alignment model of (Och and Ney, 2000b). To ob-
tain results from the Och model we ran GIZA++.7
Both the Tags and Och models use word classes.
However the word classes used in the latter are
learned automatically from parallel bilingual cor-
pora while the classes used in the former are hu-
man defined part of speech tags. Figure 2 shows
that the Tags model outperforms the Och model
when the training data size is small. As the train-
</bodyText>
<footnote confidence="0.627777">
7GIZA++ can be downloaded from http://www-i6.
informatik.rwth-aachen.de/och/software/GIZA++.html
</footnote>
<tableCaption confidence="0.998935">
Table 1: Percentage of words in the corpus by frequency
</tableCaption>
<table confidence="0.9990804">
= 1 3 5 10
Corpus English French English French English French English French
10K 47% 50% 61% 66% 74% 77% 84% 87%
25K 43% 44% 57% 59% 69% 72% 80% 83%
50K 42% 44% 55% 57% 67% 69% 78% 81%
</table>
<tableCaption confidence="0.934379">
Table 2: Alignment Error Rate by Model and Corpus Size
</tableCaption>
<table confidence="0.9983555">
Corpus Baseline Null SG Tags Tags+SG Tags+Null Tags+Null+SG
5K 17.53 16.86 16.72 16.20 15.31 15.36 15.14
15K 15.03 14.29 13.52 13.90 12.63 13.22 12.52
25K 13.85 13.05 12.79 13.10 11.91 12.30 11.79
35K 13.19 11.98 12.03 12.60 11.45 11.56 11.07
50K 12.63 11.76 11.78 12.10 11.19 11.11 10.69
</table>
<bodyText confidence="0.999853111111111">
ing size increases the Och model catches up with
the Tags model and even surpasses it slightly. This
suggests that when large amounts of parallel text are
not available monolingual part of speech classes can
improve alignment quality more than automatically
induced classes. When more data is available au-
tomatically induced bilingual word classes seem to
provide more improvement but it still remains to be
explored whether the combination of part-of-speech
knowledge with induction of bilingual classes will
perform even better. The third curve in the figure for
Tags+Null illustrates the relative improvement of
the Null model over the Tags model as the training
set size increases. We see that the performance gap
between the two models becomes wider for larger
training data sizes. This reflects the improved esti-
mation of the generation probabilities for Null which
require target word specific parameters. We used
</bodyText>
<footnote confidence="0.6999895">
5K 15K 25K 35K 45K
Training Set Size
</footnote>
<figureCaption confidence="0.998405">
Figure 2: Och vs. Tags and Tags+Null.
</figureCaption>
<bodyText confidence="0.999701933333333">
both paired t-test and Wilcoxon signed rank tests to
show the improvements are statistically significant.
The signed rank test uses the normalized test statis-
tic . is the sum of the ranks that have
positive signs. Ties are assigned the average rank of
the tied group. Since there are 400 test sentences, we
have 400 paired samples where the elements of each
pair are the AERs of the models being compared.
The difference between Och and Tags at 5K, 10K,
and 15K is significant at the level accord-
ing to both tests. The difference between Och and
Tags+Null is significant for all training set sizes at
the level.
We also assessed the gains from using part of
speech tags in the alignment probabilities according
to the model described in section 5.2. Table 3 shows
the error rate of the basic HMM alignment model
as compared to an HMM model that conditions on
tag sequences of source and target word tags in the
neighborhood of the French word and the English
word for a training set size of 10K. The results
we achieved showed an improvement of our model
over a model that does not include conditioning on
tags. The improvement in accuracy is best when
using the current and previous French word parts
of speech and does not increase when adding more
conditioning information. The improvement from
part of speech tag sequences for alignment proba-
bilities was not as good as we had expected, how-
ever, which leads us to believe that more sophisti-
</bodyText>
<figure confidence="0.778319857142857">
AER
18.5
16.5
14.5
12.5
10.5
Och
Tags
Null+Tags
Table 3: POS Conditioning of Jump Probabilities
Model AER
Baseline 16.37
15.97
15.74
15.86
15.88
15.94
cated syntax is needed to model local word order
variation.
5K 15K 25K 35K 45K
Training Set Size
</figure>
<figureCaption confidence="0.999128">
Figure 3: IBM- vs SG+Tags
</figureCaption>
<bodyText confidence="0.999985375">
In Figure 3 we compare the IBM- model to our
SG+Tags model. Such a comparison makes sense
because IBM- uses a fertility model for English
words and SG approximates fertility modeling and
because IBM- uses word classes as does our Tags
model. For smaller training set sizes our model per-
forms much better than IBM- but when more data
is available IBM- becomes slightly better. This
confirms the observation from Figure 2 that auto-
matically induced bilingual classes perform better
when trained on large amounts of data. Also as our
fertility model estimates one parameter for each En-
glish word and IBM- estimates as many parame-
ters as the maximum fertility allowed, at small train-
ing set sizes our model parameters can be estimated
more reliably.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999780230769231">
In this paper we presented three extensions to
HMM-based alignment models. We showed that
incorporating part of speech tag information of the
source and target languages in the translation model
improves word alignment accuracy. We also pre-
sented a method for approximately modeling fertil-
ity in an HMM-based model and a new generative
model for target language words that do not have
correspondences in the source language. The pro-
posed models do not increase significantly the com-
plexity of the learning algorithms while providing a
better account for some phenomena in natural lan-
guage translation.
</bodyText>
<sectionHeader confidence="0.999103" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639258064516">
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. In Computational
Linguistics, volume 19(2), pages 263–311.
Pascale Fung and Dekai Wu. 1995. Coerced markov
models for cross-lingual tag relations. In Sixth Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, volume 1, pages
240–255.
Dan I. Melamed. 2000. Models of translational equiv-
alence among words. In Computational Linguistics,
volume 26(2), pages 221–249.
F. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In Proc.
COLING ’00: The 18th Int. Conf. on Computational
Linguistics, pages 1086–1090.
F. Josef Och and H. Ney. 2000b. Improved statistical
alignment models. In Proc. of the 39th Annual Meet-
ing of the ACL.
F. Och, C. Tillmann, and H. Ney. 1999. Improved align-
ment models for statistical machine translation. In
Proc. of the Joint Conf. of Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 20–28.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proc.
COLING ’96: The 16th Int. Conf. on Computational
Linguistics, pages 836– 841.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual
Meeting of the ACL, pages 523–530.
</reference>
<figure confidence="0.998272444444444">
AER
20
18
16
14
12
10
IBM4
SG+Tags
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.221657">
<note confidence="0.90173725">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 87-94. Association for Computational Linguistics. Extensions to HMM-based Statistical Word Alignment Models</note>
<author confidence="0.794573">H Tolga Ilhan D Toutanova</author>
<affiliation confidence="0.7775395">Department of Computer Stanford</affiliation>
<address confidence="0.993575">Stanford, CA 94305-9040</address>
<email confidence="0.999459">manning@cs.stanford.edu</email>
<abstract confidence="0.971820153846154">This paper describes improved HMM-based word level alignment models for statistical machine translation. We present a method for using part of speech tag information to improve alignment accuracy, and an approach to modeling fertility and correspondence to the empty word in an HMM alignment model. We present accuracy results from evaluating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4. The results show up to 16% alignment error reduction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1336" citStr="Brown et al., 1993" startWordPosition="194" endWordPosition="197"> in an HMM alignment model. We present accuracy results from evaluating Viterbi alignments against human-judged alignments on the Canadian Hansards corpus, as compared to a bigram HMM, and IBM model 4. The results show up to 16% alignment error reduction. 1 Introduction The main task in statistical machine translation is to model the string translation probability where the string in one language is translated into another language as string . We refer to as the source language string and as the target language string in accordance with the noisy channel terminology used in the IBM models of (Brown et al., 1993). Word-level translation models assume a pairwise mapping between the words of the source and target strings. This mapping is generated by alignment models. In this paper we present extensions to the HMM alignment model of (Vogel et al., 1996; Och and Ney, 2000b). Some of our extensions are applicable to other alignment models as well and are of general utility.&apos; For most language pairs huge amounts of parallel corpora are not readily available whereas monolingual resources such as taggers are more often available. Little research has gone into exploring the po&apos;This paper was supported in part</context>
<context position="2960" citStr="Brown et al. (1993)" startWordPosition="456" endWordPosition="459">to estimate class-specific parameters for translation models. Fung and Wu (1995) adapt English tags for Chinese language modeling using Coerced Markov Models. They use English POS classes as states of the Markov Model to generate Chinese language words. In this paper we use POS tag information to incorporate prior knowledge of word translation and to model local word order variation. We show that using this information can help in the translation modeling task. Many alignment models assume a one to many mapping from source language words to target language words, such as the IBM models 1-5 of Brown et al. (1993) and the HMM alignment model of (Vogel et al., 1996). In addition, the IBM Models 3, 4 and 5 include a fertility model where is the number of words aligned to a source word. In HMM-based alignment word fertilities are not modeled. The alignment positions of target words are the states in an HMM. The alignment probabilities for word depend only on the alignment of the previous word if using a first order HMM. Therefore, source words are not awarded/penalized for being aligned to more than one target word. We present an extension to HMM alignment that approximately models word fertility. Another</context>
<context position="11541" citStr="Brown et al. (1993)" startWordPosition="1927" endWordPosition="1930">s factor reduces the alignment error rate, with the improvement being especially large when the training data size is small. 5.2 Tag Sequences for Jump Probabilities This section describes an extension to the bigram HMM model that uses source and target language tag sequences as conditioning information when predicting the alignment of target language words. In the decomposition of the joint probability shown in Eqn. 1 the factor for alignment probabilities is . A bigram HMM model assumes independence of from anything but the previous alignment position and the length of the English sentence. Brown et al. (1993) and Och et al. (1999) variably condition this probability on the English word in position and/or the French word in position . As conditioning directly on words would yield a large number of parameters and would be impractical, they cluster the words automatically into bilingual word classes. The question arises then whether we would have larger gains by conditioning on the part of speech tags of those words or even more words around the alignment position. For example, if we use the following conditioning information: we could model probabilities of transpositions and insertion of function w</context>
<context position="15963" citStr="Brown et al. (1993)" startWordPosition="2699" endWordPosition="2702">n is .5 Even though the fit of this distribution stay to the real fertility distribution may not be very good, this approximation improves alignment accuracy in practice. Sparsity is a problem in estimating stay probabilities P(stay ). We use the probability of a jump of size zero from the baseline model as our prior to do smoothing as follows: stay stay (6) SE[X] __+ + + ... where X is the number of Bernoulli trials until the first success. in this equation is the alignment probability from the baseline model with zero jump distance. . 5.4 Translation Model for Null As originally proposed by Brown et al. (1993), words in the target sentence for which there are no corresponding English words are assumed to be generated by the special English word Null. Null appears in every English sentence and often serves to generate syntactic elements in the target language that are missing in the source. A probability distribution Nullfor generation probabilities of the Null is re-estimated from a training corpus. Modeling a Null word has proven problematic. It has required many special fixes to keep models from aligning everything to Null or to keep them from aligning nothing to Null (Och and Ney, 2000b). This m</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, volume 19(2), pages 263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Dekai Wu</author>
</authors>
<title>Coerced markov models for cross-lingual tag relations.</title>
<date>1995</date>
<booktitle>In Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<volume>1</volume>
<pages>240--255</pages>
<contexts>
<context position="2421" citStr="Fung and Wu (1995)" startWordPosition="364" endWordPosition="367">l resources such as taggers are more often available. Little research has gone into exploring the po&apos;This paper was supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226. The authors would also like to thank the various reviewers for their helpful comments on earlier versions. tential of part of speech information to better model translation probabilities and permutation probabilities. Melamed (2000) uses a very broad classification of words (content, function and several punctuation classes) to estimate class-specific parameters for translation models. Fung and Wu (1995) adapt English tags for Chinese language modeling using Coerced Markov Models. They use English POS classes as states of the Markov Model to generate Chinese language words. In this paper we use POS tag information to incorporate prior knowledge of word translation and to model local word order variation. We show that using this information can help in the translation modeling task. Many alignment models assume a one to many mapping from source language words to target language words, such as the IBM models 1-5 of Brown et al. (1993) and the HMM alignment model of (Vogel et al., 1996). In addi</context>
</contexts>
<marker>Fung, Wu, 1995</marker>
<rawString>Pascale Fung and Dekai Wu. 1995. Coerced markov models for cross-lingual tag relations. In Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, volume 1, pages 240–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>26</volume>
<issue>2</issue>
<pages>221--249</pages>
<contexts>
<context position="2246" citStr="Melamed (2000)" startWordPosition="340" endWordPosition="341">icable to other alignment models as well and are of general utility.&apos; For most language pairs huge amounts of parallel corpora are not readily available whereas monolingual resources such as taggers are more often available. Little research has gone into exploring the po&apos;This paper was supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226. The authors would also like to thank the various reviewers for their helpful comments on earlier versions. tential of part of speech information to better model translation probabilities and permutation probabilities. Melamed (2000) uses a very broad classification of words (content, function and several punctuation classes) to estimate class-specific parameters for translation models. Fung and Wu (1995) adapt English tags for Chinese language modeling using Coerced Markov Models. They use English POS classes as states of the Markov Model to generate Chinese language words. In this paper we use POS tag information to incorporate prior knowledge of word translation and to model local word order variation. We show that using this information can help in the translation modeling task. Many alignment models assume a one to m</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>Dan I. Melamed. 2000. Models of translational equivalence among words. In Computational Linguistics, volume 26(2), pages 221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proc. COLING ’00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="1597" citStr="Och and Ney, 2000" startWordPosition="238" endWordPosition="241">duction The main task in statistical machine translation is to model the string translation probability where the string in one language is translated into another language as string . We refer to as the source language string and as the target language string in accordance with the noisy channel terminology used in the IBM models of (Brown et al., 1993). Word-level translation models assume a pairwise mapping between the words of the source and target strings. This mapping is generated by alignment models. In this paper we present extensions to the HMM alignment model of (Vogel et al., 1996; Och and Ney, 2000b). Some of our extensions are applicable to other alignment models as well and are of general utility.&apos; For most language pairs huge amounts of parallel corpora are not readily available whereas monolingual resources such as taggers are more often available. Little research has gone into exploring the po&apos;This paper was supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226. The authors would also like to thank the various reviewers for their helpful comments on earlier versions. tential of part of speech information to better model translation probabilit</context>
<context position="6050" citStr="Och and Ney, 2000" startWordPosition="988" endWordPosition="991">in Eqn. 1. Section 3 describes the baseline model in more detail. Section 4 illustrates examples where the baseline model performs poorly. Section 5 presents our extensions and Section 6 presents experimental results. 3 Baseline Model Translation of French and English sentences shows a strong localization effect. Words close to each other in the source language remain close in the translation. Furthermore, most of the time the alignment shows monotonicity. This means that pairwise alignments stay close to the diagonal line of the plane. It has been shown (Vogel et al., 1996; Och et al., 1999; Och and Ney, 2000a) that HMM based alignment models are effective at capturing such localization. We use as a baseline the model presented by (Och and Ney, 2000a). A basic bigram HMM-based model gives us In this HMM model,2 alignment probabilities are independent of word position and depend only on jump width ( ).3 The Och and Ney (2000a) model includes refinements including special treatment of a jump to Null and smoothing with a uniform prior which we also included in our initial model. As in their model we set the probability for jump from any state to Null to a fixed value (✑) which we estimated from held-</context>
<context position="16553" citStr="Och and Ney, 2000" startWordPosition="2798" endWordPosition="2801">osed by Brown et al. (1993), words in the target sentence for which there are no corresponding English words are assumed to be generated by the special English word Null. Null appears in every English sentence and often serves to generate syntactic elements in the target language that are missing in the source. A probability distribution Nullfor generation probabilities of the Null is re-estimated from a training corpus. Modeling a Null word has proven problematic. It has required many special fixes to keep models from aligning everything to Null or to keep them from aligning nothing to Null (Och and Ney, 2000b). This might stem from the problem that the Null is responsible for generating syntactic elements of the target language as well as generating words that make the target language sentence more idiomatic and stylistic. The intuition for our model of translation probabilities for target words that do not have corresponding source words is that these words are generated from the special English Null and also from the next word in the target language by a mixture model. The pair la conf´ed´eration in Figure 1 is an example of such case where conf´ed´eration contributes extra information in gener</context>
<context position="18244" citStr="Och and Ney, 2000" startWordPosition="3084" endWordPosition="3087">and going towards the beginning. For the new model there is an efficient dynamic programming algorithm for computations in EM similar to the forward-backward algorithm. The probability stay stay (5) Null Null(7) again decomposes into forward and backward probabilities. The forward probability is and the backward probability is . These can be computed recursively and used for efficient computation of posteriors in EM. 6 Results We present results on word level alignment accuracy using the Hansards corpus. Our test data consists of manually aligned sentences which are the same data set used by (Och and Ney, 2000b).6 In the annotated sentences every alignment between two words is labeled as either a sure (S) or possible (P) alignment. (S P). We used the following quantity (called alignment error rate or AER) to evaluate the alignment quality of our models, which is also the evaluation metric used by (Och and Ney, 2000b): We divided this annotated data into a validation set of sentences and a final test set of sentences. The validation set was used to select tuning parameters such as in Eqn. 4, 6 and 7. We report AER results on the final test set of sentences which contain a total of English and French</context>
<context position="21335" citStr="Och and Ney, 2000" startWordPosition="3627" endWordPosition="3630">there seems to be a stronger overlap between the strengths of the Null and SG models because some fertility related phenomena can be accounted for by both models. When an English word is wrongly aligning to several consecutive French words because of indirect association, while the correct alignment of some of them is to the empty word, both the Null and SG models can combat the problem— one by better modeling correspondence to Null, and the other by discouraging large fertilities. Figure 2 displays learning curves for three models: Och, Tags, and Tags+Null. Och is the HMM alignment model of (Och and Ney, 2000b). To obtain results from the Och model we ran GIZA++.7 Both the Tags and Och models use word classes. However the word classes used in the latter are learned automatically from parallel bilingual corpora while the classes used in the former are human defined part of speech tags. Figure 2 shows that the Tags model outperforms the Och model when the training data size is small. As the train7GIZA++ can be downloaded from http://www-i6. informatik.rwth-aachen.de/och/software/GIZA++.html Table 1: Percentage of words in the corpus by frequency = 1 3 5 10 Corpus English French English French Englis</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000a. A comparison of alignment models for statistical machine translation. In Proc. COLING ’00: The 18th Int. Conf. on Computational Linguistics, pages 1086–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Josef Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. of the 39th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1597" citStr="Och and Ney, 2000" startWordPosition="238" endWordPosition="241">duction The main task in statistical machine translation is to model the string translation probability where the string in one language is translated into another language as string . We refer to as the source language string and as the target language string in accordance with the noisy channel terminology used in the IBM models of (Brown et al., 1993). Word-level translation models assume a pairwise mapping between the words of the source and target strings. This mapping is generated by alignment models. In this paper we present extensions to the HMM alignment model of (Vogel et al., 1996; Och and Ney, 2000b). Some of our extensions are applicable to other alignment models as well and are of general utility.&apos; For most language pairs huge amounts of parallel corpora are not readily available whereas monolingual resources such as taggers are more often available. Little research has gone into exploring the po&apos;This paper was supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226. The authors would also like to thank the various reviewers for their helpful comments on earlier versions. tential of part of speech information to better model translation probabilit</context>
<context position="6050" citStr="Och and Ney, 2000" startWordPosition="988" endWordPosition="991">in Eqn. 1. Section 3 describes the baseline model in more detail. Section 4 illustrates examples where the baseline model performs poorly. Section 5 presents our extensions and Section 6 presents experimental results. 3 Baseline Model Translation of French and English sentences shows a strong localization effect. Words close to each other in the source language remain close in the translation. Furthermore, most of the time the alignment shows monotonicity. This means that pairwise alignments stay close to the diagonal line of the plane. It has been shown (Vogel et al., 1996; Och et al., 1999; Och and Ney, 2000a) that HMM based alignment models are effective at capturing such localization. We use as a baseline the model presented by (Och and Ney, 2000a). A basic bigram HMM-based model gives us In this HMM model,2 alignment probabilities are independent of word position and depend only on jump width ( ).3 The Och and Ney (2000a) model includes refinements including special treatment of a jump to Null and smoothing with a uniform prior which we also included in our initial model. As in their model we set the probability for jump from any state to Null to a fixed value (✑) which we estimated from held-</context>
<context position="16553" citStr="Och and Ney, 2000" startWordPosition="2798" endWordPosition="2801">osed by Brown et al. (1993), words in the target sentence for which there are no corresponding English words are assumed to be generated by the special English word Null. Null appears in every English sentence and often serves to generate syntactic elements in the target language that are missing in the source. A probability distribution Nullfor generation probabilities of the Null is re-estimated from a training corpus. Modeling a Null word has proven problematic. It has required many special fixes to keep models from aligning everything to Null or to keep them from aligning nothing to Null (Och and Ney, 2000b). This might stem from the problem that the Null is responsible for generating syntactic elements of the target language as well as generating words that make the target language sentence more idiomatic and stylistic. The intuition for our model of translation probabilities for target words that do not have corresponding source words is that these words are generated from the special English Null and also from the next word in the target language by a mixture model. The pair la conf´ed´eration in Figure 1 is an example of such case where conf´ed´eration contributes extra information in gener</context>
<context position="18244" citStr="Och and Ney, 2000" startWordPosition="3084" endWordPosition="3087">and going towards the beginning. For the new model there is an efficient dynamic programming algorithm for computations in EM similar to the forward-backward algorithm. The probability stay stay (5) Null Null(7) again decomposes into forward and backward probabilities. The forward probability is and the backward probability is . These can be computed recursively and used for efficient computation of posteriors in EM. 6 Results We present results on word level alignment accuracy using the Hansards corpus. Our test data consists of manually aligned sentences which are the same data set used by (Och and Ney, 2000b).6 In the annotated sentences every alignment between two words is labeled as either a sure (S) or possible (P) alignment. (S P). We used the following quantity (called alignment error rate or AER) to evaluate the alignment quality of our models, which is also the evaluation metric used by (Och and Ney, 2000b): We divided this annotated data into a validation set of sentences and a final test set of sentences. The validation set was used to select tuning parameters such as in Eqn. 4, 6 and 7. We report AER results on the final test set of sentences which contain a total of English and French</context>
<context position="21335" citStr="Och and Ney, 2000" startWordPosition="3627" endWordPosition="3630">there seems to be a stronger overlap between the strengths of the Null and SG models because some fertility related phenomena can be accounted for by both models. When an English word is wrongly aligning to several consecutive French words because of indirect association, while the correct alignment of some of them is to the empty word, both the Null and SG models can combat the problem— one by better modeling correspondence to Null, and the other by discouraging large fertilities. Figure 2 displays learning curves for three models: Och, Tags, and Tags+Null. Och is the HMM alignment model of (Och and Ney, 2000b). To obtain results from the Och model we ran GIZA++.7 Both the Tags and Och models use word classes. However the word classes used in the latter are learned automatically from parallel bilingual corpora while the classes used in the former are human defined part of speech tags. Figure 2 shows that the Tags model outperforms the Och model when the training data size is small. As the train7GIZA++ can be downloaded from http://www-i6. informatik.rwth-aachen.de/och/software/GIZA++.html Table 1: Percentage of words in the corpus by frequency = 1 3 5 10 Corpus English French English French Englis</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Josef Och and H. Ney. 2000b. Improved statistical alignment models. In Proc. of the 39th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="6031" citStr="Och et al., 1999" startWordPosition="984" endWordPosition="987">ion probabilities in Eqn. 1. Section 3 describes the baseline model in more detail. Section 4 illustrates examples where the baseline model performs poorly. Section 5 presents our extensions and Section 6 presents experimental results. 3 Baseline Model Translation of French and English sentences shows a strong localization effect. Words close to each other in the source language remain close in the translation. Furthermore, most of the time the alignment shows monotonicity. This means that pairwise alignments stay close to the diagonal line of the plane. It has been shown (Vogel et al., 1996; Och et al., 1999; Och and Ney, 2000a) that HMM based alignment models are effective at capturing such localization. We use as a baseline the model presented by (Och and Ney, 2000a). A basic bigram HMM-based model gives us In this HMM model,2 alignment probabilities are independent of word position and depend only on jump width ( ).3 The Och and Ney (2000a) model includes refinements including special treatment of a jump to Null and smoothing with a uniform prior which we also included in our initial model. As in their model we set the probability for jump from any state to Null to a fixed value (✑) which we e</context>
<context position="11563" citStr="Och et al. (1999)" startWordPosition="1932" endWordPosition="1935">gnment error rate, with the improvement being especially large when the training data size is small. 5.2 Tag Sequences for Jump Probabilities This section describes an extension to the bigram HMM model that uses source and target language tag sequences as conditioning information when predicting the alignment of target language words. In the decomposition of the joint probability shown in Eqn. 1 the factor for alignment probabilities is . A bigram HMM model assumes independence of from anything but the previous alignment position and the length of the English sentence. Brown et al. (1993) and Och et al. (1999) variably condition this probability on the English word in position and/or the French word in position . As conditioning directly on words would yield a large number of parameters and would be impractical, they cluster the words automatically into bilingual word classes. The question arises then whether we would have larger gains by conditioning on the part of speech tags of those words or even more words around the alignment position. For example, if we use the following conditioning information: we could model probabilities of transpositions and insertion of function words in the target lan</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1578" citStr="Vogel et al., 1996" startWordPosition="234" endWordPosition="237">r reduction. 1 Introduction The main task in statistical machine translation is to model the string translation probability where the string in one language is translated into another language as string . We refer to as the source language string and as the target language string in accordance with the noisy channel terminology used in the IBM models of (Brown et al., 1993). Word-level translation models assume a pairwise mapping between the words of the source and target strings. This mapping is generated by alignment models. In this paper we present extensions to the HMM alignment model of (Vogel et al., 1996; Och and Ney, 2000b). Some of our extensions are applicable to other alignment models as well and are of general utility.&apos; For most language pairs huge amounts of parallel corpora are not readily available whereas monolingual resources such as taggers are more often available. Little research has gone into exploring the po&apos;This paper was supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226. The authors would also like to thank the various reviewers for their helpful comments on earlier versions. tential of part of speech information to better model tra</context>
<context position="3012" citStr="Vogel et al., 1996" startWordPosition="466" endWordPosition="470">n models. Fung and Wu (1995) adapt English tags for Chinese language modeling using Coerced Markov Models. They use English POS classes as states of the Markov Model to generate Chinese language words. In this paper we use POS tag information to incorporate prior knowledge of word translation and to model local word order variation. We show that using this information can help in the translation modeling task. Many alignment models assume a one to many mapping from source language words to target language words, such as the IBM models 1-5 of Brown et al. (1993) and the HMM alignment model of (Vogel et al., 1996). In addition, the IBM Models 3, 4 and 5 include a fertility model where is the number of words aligned to a source word. In HMM-based alignment word fertilities are not modeled. The alignment positions of target words are the states in an HMM. The alignment probabilities for word depend only on the alignment of the previous word if using a first order HMM. Therefore, source words are not awarded/penalized for being aligned to more than one target word. We present an extension to HMM alignment that approximately models word fertility. Another assumption of existing alignment models is that the</context>
<context position="6013" citStr="Vogel et al., 1996" startWordPosition="980" endWordPosition="983">on and word translation probabilities in Eqn. 1. Section 3 describes the baseline model in more detail. Section 4 illustrates examples where the baseline model performs poorly. Section 5 presents our extensions and Section 6 presents experimental results. 3 Baseline Model Translation of French and English sentences shows a strong localization effect. Words close to each other in the source language remain close in the translation. Furthermore, most of the time the alignment shows monotonicity. This means that pairwise alignments stay close to the diagonal line of the plane. It has been shown (Vogel et al., 1996; Och et al., 1999; Och and Ney, 2000a) that HMM based alignment models are effective at capturing such localization. We use as a baseline the model presented by (Och and Ney, 2000a). A basic bigram HMM-based model gives us In this HMM model,2 alignment probabilities are independent of word position and depend only on jump width ( ).3 The Och and Ney (2000a) model includes refinements including special treatment of a jump to Null and smoothing with a uniform prior which we also included in our initial model. As in their model we set the probability for jump from any state to Null to a fixed va</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proc. COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836– 841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of the ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="12304" citStr="Yamada and Knight, 2001" startWordPosition="2055" endWordPosition="2058">ning directly on words would yield a large number of parameters and would be impractical, they cluster the words automatically into bilingual word classes. The question arises then whether we would have larger gains by conditioning on the part of speech tags of those words or even more words around the alignment position. For example, if we use the following conditioning information: we could model probabilities of transpositions and insertion of function words in the target language that have no corresponding words in the source language ( is Null) similarly to the channel operations of the (Yamada and Knight, 2001) syntaxbased statistical translation model. Since the syntactic knowledge provided by POS tags is quite limited, this is a crude model of transpositions and Null insertions at the preterminal level. However we could still expect that it would help in modeling local word order variations. For example, in the sentence J’aime la chute ‘I love the fall’ the probability of aligning la ( DT) to the will be boosted by knowing VBP and DT. Similarly, in the sentence J’aime des chiens ‘I love dogs’ the probability of aligning la to Null will be increased by knowing VBP and NNS. VBP followed by NNS crude</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proc. of the 39th Annual Meeting of the ACL, pages 523–530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>