<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9393985">
Discriminative Training of Clustering Functions:
Theory and Experiments with Entity Identification
</title>
<author confidence="0.997689">
Xin Li and Dan Roth
</author>
<affiliation confidence="0.9998915">
Department of Computer Science
University of Illinois, Urbana, IL 61801
</affiliation>
<email confidence="0.989875">
(xli1,danr)@cs.uiuc.edu
</email>
<sectionHeader confidence="0.994876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999970555555555">
Clustering is an optimization procedure that
partitions a set of elements to optimize some
criteria, based on a fixed distance metric de-
fined between the elements. Clustering ap-
proaches have been widely applied in natural
language processing and it has been shown re-
peatedly that their success depends on defin-
ing a good distance metric, one that is appro-
priate for the task and the clustering algorithm
used. This paper develops a framework in
which clustering is viewed as a learning task,
and proposes a way to train a distance metric
that is appropriate for the chosen clustering al-
gorithm in the context of the given task. Ex-
periments in the context of the entity identifi-
cation problem exhibit significant performance
improvements over state-of-the-art clustering
approaches developed for this problem.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816631578947">
Clustering approaches have been widely applied to nat-
ural language processing (NLP) problems. Typically,
natural language elements (words, phrases, sentences,
etc.) are partitioned into non-overlapping classes, based
on some distance (or similarity) metric defined between
them, in order to provide some level of syntactic or se-
mantic abstraction. A key example is that of class-based
language models (Brown et al., 1992; Dagan et al., 1999)
where clustering approaches are used in order to parti-
tion words, determined to be similar, into sets. This
enables estimating more robust statistics since these are
computed over collections of “similar” words. A large
number of different metrics and algorithms have been ex-
perimented with these problems (Dagan et al., 1999; Lee,
1997; Weeds et al., 2004). Similarity between words was
also used as a metric in a distributional clustering algo-
rithm in (Pantel and Lin, 2002), and it shows that func-
tionally similar words can be grouped together and even
separated to smaller groups based on their senses. At a
</bodyText>
<page confidence="0.695953">
64
</page>
<bodyText confidence="0.999792928571428">
higher level, (Mann and Yarowsky, 2003) disambiguated
personal names by clustering people’s home pages using
a TFIDF similarity, and several other researchers have ap-
plied clustering at the same level in the context of the
entity identification problem (Bilenko et al., 2003; Mc-
Callum and Wellner, 2003; Li et al., 2004). Similarly, ap-
proaches to coreference resolution (Cardie and Wagstaff,
1999) use clustering to identify groups of references to
the same entity.
Clustering is an optimization procedure that takes as
input (1) a collection of domain elements along with (2)
a distance metric between them and (3) an algorithm se-
lected to partition the data elements, with the goal of op-
timizing some form of clustering quality with respect to
the given distance metric. For example, the K-Means
clustering approach (Hartigan and Wong, 1979) seeks to
maximize a measure of tightness of the resulting clusters
based on the Euclidean distance. Clustering is typically
called an unsupervised method, since data elements are
used without labels during the clustering process and la-
bels are not used to provide feedback to the optimiza-
tion process. E.g., labels are not taken into account when
measuring the quality of the partition. However, in many
cases, supervision is used at the application level when
determining an appropriate distance metric (e.g., (Lee,
1997; Weeds et al., 2004; Bilenko et al., 2003) and more).
This scenario, however, has several setbacks. First, the
process of clustering, simply a function that partitions a
set of elements into different classes, involves no learn-
ing and thus lacks flexibility. Second, clustering quality is
typically defined with respect to a fixed distance metric,
without utilizing any direct supervision, so the practical
clustering outcome could be disparate from one’s inten-
tion. Third, when clustering with a given algorithm and
a fixed metric, one in fact makes some implicit assump-
tions on the data and the task (e.g., (Kamvar et al., 2002);
more on that below). For example, the optimal conditions
under which for K-means works are that the data is gen-
erated from a uniform mixture of Gaussian models; this
may not hold in reality.
This paper proposes a new clustering framework that
addresses all the problems discussed above. Specifically,
</bodyText>
<note confidence="0.984522">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 64–71, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99997085">
we define clustering as a learning task: in the training
stage, a partition function, parameterized by a distance
metric, is trained with respect to a specific clustering al-
gorithm, with supervision. Some of the distinct proper-
ties of this framework are that: (1) The training stage is
formalized as an optimization problem in which a parti-
tion function is learned in a way that minimizes a clus-
tering error. (2) The clustering error is well-defined and
driven by feedback from labeled data. (3) Training a
distance metric with respect to any given clustering al-
gorithm seeks to minimize the clustering error on train-
ing data that, under standard learning theory assumptions,
can be shown to imply small error also in the application
stage. (4) We develop a general learning algorithm that
can be used to learn an expressive distance metric over
the feature space (e.g., it can make use of kernels).
While our approach makes explicit use of labeled data,
we argue that, in fact, many clustering applications in nat-
ural language also exploit this information off-line, when
exploring which metrics are appropriate for the task. Our
framework makes better use of this resource by incorpo-
rating it directly into the metric training process; training
is driven by true clustering error, computed via the spe-
cific algorithm chosen to partition the data.
We study this new framework empirically on the en-
tity identification problem – identifying whether differ-
ent mentions of real world entities, such as “JFK” and
“John Kennedy”, within and across text documents, ac-
tually represent the same concept (McCallum and Well-
ner, 2003; Li et al., 2004). Our experimental results ex-
hibit a significant performance improvement over exist-
ing approaches (20% − 30% F1 error reduction) on all
three types of entities we study, and indicate its promis-
ing prospective in other natural language tasks.
The rest of this paper discusses existing clustering ap-
proaches (Sec. 2) and then introduces our Supervised Dis-
criminative Clustering framework (SDC) (Sec. 3) and a
general learner for training in it (Sec. 4). Sec. 5 describes
the entity identification problem and Sec. 6 compares dif-
ferent clustering approaches on this task.
</bodyText>
<sectionHeader confidence="0.738429" genericHeader="method">
2 Clustering in Natural Language Tasks
</sectionHeader>
<bodyText confidence="0.807179">
Clustering is the task of partitioning a set of elements
</bodyText>
<equation confidence="0.450958">
S ⊆ X into a disjoint decomposition 1 p(S) = {S1, S2,
· · · , SK} of S. We associate with it a partition function
p = pS : X → C = {1, 2,... K} that maps each x ∈ S
</equation>
<bodyText confidence="0.891237">
to a class index pS(x) = k iff x ∈ Sk. The subscript S
in pS and pS(x) is omitted when clear from the context.
Notice that, unlike a classifier, the image x ∈ S under a
partition function depends on S.
In practice, a clustering algorithm A (e.g. K-Means),
and a distance metric d (e.g., Euclidean distance), are typ-
</bodyText>
<subsectionHeader confidence="0.338408">
1Overlapping partitions will not be discussed here.
</subsectionHeader>
<bodyText confidence="0.999586625">
ically used to generate a function h to approximate the
true partition function p. Denote h(S) = Ad(S), the par-
tition of S by h. A distance (equivalently, a similarity)
function d that measures the proximity between two ele-
ments is a pairwise function X × X → R+, which can
be parameterized to represent a family of functions —
metric properties are not discussed in this paper. For ex-
ample, given any two element x1 =&lt; x(1)
</bodyText>
<equation confidence="0.999524">
1 , · · · , x(m) 1&gt;
and x2 =&lt; x(1)
2 , · · · , x(m) 2&gt; in an m-dimensional space,
a linearly weighted Euclidean distance with parameters
0 = {wl}m1 is defined as:
wl · |x(l)
1 − x(l)
2 |2 (1)
</equation>
<bodyText confidence="0.998238166666667">
When supervision (e.g. class index of elements) is un-
available, the quality of a partition function h operating
on S ⊆ X, is measured with respect to the distance met-
ric defined over X. Suppose h partitions S into disjoint
sets h(S) = {S�k}K1 , one quality function used in the K-
Means algorithm is defined as:
</bodyText>
<equation confidence="0.91882">
d(x, µ�k)2, (2)
</equation>
<bodyText confidence="0.9988605">
where µ&apos;k is the mean of elements in set Sk. However, this
measure can be computed irrespective of the algorithm.
</bodyText>
<subsectionHeader confidence="0.968928">
2.1 What is a Good Metric?
</subsectionHeader>
<bodyText confidence="0.99999256">
A good metric is one in which close proximity correlates
well with the likelihood of being in the same class. When
applying clustering to some task, people typically decide
on the clustering quality measure qS(h) they want to op-
timize, and then chose a specific clustering algorithm A
and a distance metric d to generate a ‘good’ partition
function h. However, it is clear that without any super-
vision, the resulting function is not guaranteed to agree
with the target function p (or one’s original intention).
Given this realization, there has been some work on
selecting a good distance metric for a family of related
problems and on learning a metric for specific tasks. For
the former, the focus is on developing and selecting good
distance (similarity) metrics that reflect well pairwise
proximity between domain elements. The “goodness”
of a metric is empirically measured when combined with
different clustering algorithms on different problems. For
example (Lee, 1997; Weeds et al., 2004) compare similar-
ity metrics such as the Cosine, Manhattan and Euclidean
distances, Kullback-Leibler divergence, Jensen-Shannon
divergence, and Jaccard’s Coefficient, that could be ap-
plied in general clustering tasks, on the task of measur-
ing distributional similarity. (Cohen et al., 2003) com-
pares a number of string and token-based similarity met-
rics on the task of matching entity names and found that,
</bodyText>
<equation confidence="0.998911">
tuuv
dθ(x1,x2) ≡
Xm
l=1
K
X
k=1
qS(h) ≡
X
xESk
</equation>
<page confidence="0.568512">
65
</page>
<bodyText confidence="0.9990695">
overall, the best-performing method is a hybrid scheme
(SoftTFIDF) combining a TFIDF weighting scheme of
tokens with the Jaro-Winkler string-distance scheme that
is widely used for record linkage in databases.
</bodyText>
<figure confidence="0.991957">
(a) Single-Linkage with (b) K-Means with (c) K-Means with a
Euclidean Euclidean Linear Metric
d(x1,x2) = [(x1(1) -x2(1))2+(x1(2) -x2(2))2]1/2 d(x1,x2) = |(x1(1) +x2(1))-(x1(2) +x2(2))|
</figure>
<figureCaption confidence="0.81865175">
Figure 1: Different combinations of clustering algorithms
with distance metrics. The 12 points, positioned in a two-
dimensional space &lt; X(1), X(2) &gt;, are clustered into two
groups containing solid and hollow points respectively.
</figureCaption>
<bodyText confidence="0.9998428125">
Moreover, it is not clear whether there exists any
universal metric that is good for many different prob-
lems (or even different data sets for similar problems)
and is appropriate for any clustering algorithm. For the
word-based distributional similarity mentioned above,
this point was discussed in (Geffet and Dagan, 2004)
when it is shown that proximity metrics that are appro-
priate for class-based language models may not be ap-
propriate for other tasks. We illustrate this critical point in
Fig. 1. (a) and (b) show that even for the same data collec-
tion, different clustering algorithms with the same met-
ric could generate different outcomes. (b) and (c) show
that with the same clustering algorithm, different metrics
could also produce different outcomes. Therefore, a good
distance metric should be both domain-specific and asso-
ciated with a specific clustering algorithm.
</bodyText>
<subsectionHeader confidence="0.999158">
2.2 Metric Learning via Pairwise Classification
</subsectionHeader>
<bodyText confidence="0.999989775">
Several works (Cohen et al., 2003; Cohen and Rich-
man, 2002; McCallum and Wellner, 2003; Li et al.,
2004) have tried to remedy the aforementioned problems
by attempting to learn a distance function in a domain-
specific way via pairwise classification. In the training
stage, given a set of labeled element pairs, a function
f : X x X → 10, 11 is trained to classify any two el-
ements as to whether they belong to the same class (1)
or not (0), independently of other elements. The dis-
tance between the two elements is defined by converting
the prediction confidence of the pairwise classifier, and
clustering is then performed based on this distance func-
tion. Particularly, (Li et al., 2004) applied this approach
to measuring name similarity in the entity identification
problem, where a pairwise classifier (LMR) is trained us-
ing the SNoW learning architecture (Roth, 1998) based
on variations of Perceptron and Winnow, and using a col-
lection of relational features between a pair of names.
The distance between two names is defined as a softmax
over the classifier’s output. As expected, experimental
evidence (Cohen et al., 2003; Cohen and Richman, 2002;
Li et al., 2004) shows that domain-specific distance func-
tions improve over a fixed metric. This can be explained
by the flexibility provided by adapting the metric to the
domain as well as the contribution of supervision that
guides the adaptation of the metric.
A few works (Xing et al., 2002; Bar-Hillel et al., 2003;
Schultz and Joachims, 2004; Mochihashi et al., 2004)
outside the NLP domain have also pursued this general
direction, and some have tried to learn the metric with
limited amount of supervision, no supervision or by in-
corporating other information sources such as constraints
on the class memberships of the data elements. In most of
these cases, the algorithm practically used in clustering,
(e.g. K-Means), is not considered in the learning proce-
dure, or only implicitly exploited by optimizing the same
objective function. (Bach and Jordan, 2003; Bilenko et
al., 2004) indeed suggest to learn a metric directly in a
clustering task but the learning procedure is specific for
one clustering algorithm.
</bodyText>
<sectionHeader confidence="0.987223" genericHeader="method">
3 Supervised Discriminative Clustering
</sectionHeader>
<bodyText confidence="0.998739">
To solve the limitations of existing approaches, we de-
velop the Supervised Discriminative Clustering Frame-
work (SDC), that can train a distance function with re-
spect to any chosen clustering algorithm in the context of
a given task, guided by supervision.
</bodyText>
<figureCaption confidence="0.995454">
Figure 2: Supervised Discriminative Clustering
</figureCaption>
<bodyText confidence="0.941351583333333">
Stage: h(S’ )
Fig. 2 presents this framework, in which a cluster-
ing task is explicitly split into training and application
stages, and the chosen clustering algorithm involves in
both stages. In the training stage, supervision is directly
integrated into measuring the clustering error errS(h, p)
of a partition function h by exploiting the feedback given
by the true partition p. The goal of training is to find a par-
tition function h* in a hypothesis space H that minimizes
the error. Consequently, given a new data set S&apos; in the ap-
plication stage, under some standard learning theory as-
sumptions, the hope is that the learned partition function
</bodyText>
<figure confidence="0.998689684210527">
A unlabeled
data set S’
A distance
metric d
A labeled data set S
A Supervised
Learner
+
a clustering
algorithm A
A partition
h(S’)
A partition function
h(S) = Ad(S)
Training Stage:
Goal: h*=argmin
errS(h,p)
Application
66
</figure>
<bodyText confidence="0.474688">
can generalize well and achieve small error as well.
</bodyText>
<subsectionHeader confidence="0.986102">
3.1 Supervised and Unsupervised Training
</subsectionHeader>
<bodyText confidence="0.999921777777778">
Let p be the target function over X, h be a function in the
hypothesis space H, and h(S) = {S&apos;k}K1 . In principle,
given data set S C_ X, if the true partition p(S) = {Sk}K1
of S is available, one can measure the deviation of h from
p over S, using an error function errS(h, p) → R+. We
distinguish an error function from a quality function (as
in Equ. 2) as follows: an error function measures the dis-
agreement between clustering and the target partition (or
one’s intention) when supervision is given, while a qual-
ity is defined without any supervision.
For clustering, there is generally no direct way to com-
pare the true class index p(x) of each element with that
given by a hypothesis h(x), so an alternative is to mea-
sure the disagreement between p and h over pairs of el-
ements. Given a labeled data set S and p(S), one error
function, namely weighted clustering error, is defined as
a sum of the pairwise errors over any two elements in S,
weighted by the distance between them:
</bodyText>
<equation confidence="0.93752475">
�
1
errS(h, p) = |S|2
(3)
</equation>
<bodyText confidence="0.999898166666667">
where D = maxxi xjES d(xi,xj) is the maximum dis-
tance between any two elements in S and I is an indica-
tor function. Aij - I[(p(xi) = p(xj) &amp; h(xi) =� h(xj)]
and Bij - I[(p(xi) =� p(xj) &amp; h(xi) = h(xj)] represent
two types of pairwise errors respectively.
Just like the quality defined in Equ. 2, this error is a
function of the metric d. Intuitively, the contribution of a
pair of elements that should belong to the same class but
are split by h, grows with their distance, and vice versa.
However, this measure is significantly different from the
quality, in that it does not just measure the tightness of the
partition given by h, but rather the difference between the
tightness of the partitions given by h and by p.
Given a set of observed data, the goal of training is to
learn a good partition function, parameterized by specific
clustering algorithms and distance functions. Depending
on whether training data is labeled or unlabeled, we can
further define supervised and unsupervised training.
</bodyText>
<construct confidence="0.802817666666667">
Definition 3.1 Supervised Training: Given a labeled
data set S and p(S), a family ofpartition functions H,
and the error function errS(h, p)(h E H), the problem
is to find an optimalfunction h* s.t.
h* = argminhEH errS(h, p).
Definition 3.2 Unsupervised Training: Given an unla-
beled data set S (p(S) is unknown), a family ofpartition
functions H, and a quality function qS(h)(h E H), the
problem is to find an optimal partition function h* s.t.
</construct>
<equation confidence="0.942918">
h* = argmaxhEH qS(h).
</equation>
<bodyText confidence="0.9998654">
With this formalization, SDC along with supervised
training, can be distinguished clearly from (1) unsuper-
vised clustering approaches, (2) clustering over pairwise
classification; and (3) related works that exploit partial
supervision in metric learning as constraints.
</bodyText>
<subsectionHeader confidence="0.999449">
3.2 Clustering via Metric Learning
</subsectionHeader>
<bodyText confidence="0.996278333333333">
By fixing the clustering algorithm in the training stage,
we can further define supervised metric learning, a spe-
cial case of supervised training.
</bodyText>
<construct confidence="0.996502666666667">
Definition 3.3 Supervised Metric Learning: Given a la-
beled data set S and p(S), and a family ofpartition func-
tions H = {h} that are parameterized by a chosen clus-
tering algorithm A and a family of distance metrics dθ
(B E Q), the problem is to seek an optimal metric dθ.
with respect to A, s.t. for h(S) = A de(S)
</construct>
<equation confidence="0.99114">
B* = argminθ errS(h,p). (4)
</equation>
<bodyText confidence="0.998863777777778">
Learning the metric parameters B requires parameteriz-
ing h as a function of B, when the algorithm A is chosen
and fixed in h. In the later experiments of Sec. 5, we
try to learn weighted Manhattan distances for the single-
link algorithm and other algorithms, in the task of en-
tity identification. In this case, when pairwise features
are extracted for any elements x1, x2 E X, (x1, x2) =&lt;
01, 02, · · · , 0m &gt;, the linearly weighted Manhattan dis-
tance, parameterized by (B = {wl}m1 ) is defined as:
</bodyText>
<equation confidence="0.9503615">
�m
l=1
</equation>
<bodyText confidence="0.997079666666667">
where wl is the weight over feature 0l(x1,x2). Since
measurement of the error is dependent on the metric,
as shown in Equ. 3, one needs to enforce some con-
straints on the parameters. One constraint is Eml=1 |wl |=
1, which prevents the error from being scale-dependent
(e.g., metrics giving smaller distance are always better).
</bodyText>
<sectionHeader confidence="0.981695" genericHeader="method">
4 A General Learner for SDC
</sectionHeader>
<bodyText confidence="0.999984533333334">
In addition to the theoretical SDC framework, we also de-
velop a practical learning algorithm based on gradient de-
scent (in Fig. 3), that can train a distance function for any
chosen clustering algorithm (such as Single-Linkage and
K-Means), as in the setting of supervised metric learning.
The training procedure incorporates the clustering algo-
rithm (step 2.a) so that the metric is trained with respect
to the specific algorithm that will be applied in evalua-
tion. The convergence of this general training procedure
depends on the convexity of the error as a function of B.
For example, since the error function we use is linear in B,
the algorithm is guaranteed to converge to a global mini-
mum. In this case, for rate of convergence, one can appeal
to general results that typically imply, when there exists
a parameter vector with zero error, that convergence rate
</bodyText>
<equation confidence="0.9115224">
[d(xi, xj)·Aij+(D−d(xi, xj))·Bij]
xi,xj ES
d(x1, x2) _
wl · 0l(x1,x2) (5)
67
</equation>
<bodyText confidence="0.999666666666667">
depends on the ‘separation” of the training data, which
roughly means the minimal error archived with this pa-
rameter vector. Results such as (Freund and Schapire,
1998) can be used to extend the rate of convergence re-
sult a bit beyond the separable case, when a small number
of the pairs are not separable.
</bodyText>
<sectionHeader confidence="0.517315" genericHeader="method">
Algorithm: SDC-Learner
</sectionHeader>
<bodyText confidence="0.97104075">
Input: S and p(S): the labeled data set. A: the clustering
algorithm. errS(h, p): the clustering error function. α &gt; 0
: the learning rate. T (typically T is large) : the number of
iterations allowed.
</bodyText>
<figure confidence="0.968884625">
Output: O* : the parameters in the distance function d.
1. In the initial (I-) step, we randomly choose O0 for d.
After this step we have the initial d0 and h0.
2. Then we iterate over t (t = 1, 2, · · ·),
(a) Partition S using ht−1(S) __ A dt−1(S);
(b) Compute errS(ht−1, p) and update O using the
formula: Ot = Ot−1 − α · aerrS(ht−1�p)
aet−1 .
</figure>
<listItem confidence="0.800527666666667">
(c) Normalization: Ot = 1� · Ot, where Z = ||Ot||.
3. Stopping Criterion: If t &gt; T, the algorithm exits and
outputs the metric in the iteration with the least error.
</listItem>
<figureCaption confidence="0.991336">
Figure 3: A general training algorithm for SDC
</figureCaption>
<bodyText confidence="0.998326666666667">
For the weighted clustering error in Equ. 3, and linearly
weighted Manhattan distances as in Equ. 5, the update
rule in Step 2(b) becomes
</bodyText>
<equation confidence="0.9979495">
wtl = wt−1
l − α · [ψt−1
l (p, S) − ψt−1
l (h, S)]. (6)
</equation>
<bodyText confidence="0.550703666666667">
where ψl (p, S) ≡ |2�ES�l (xi, x�) ·I[p(xiS |) =
p(xj)] and ψl(h,S) ≡  |1|2 Exi,xj∈S φl (xi ,xj)
I[h(xi) = h(xj)], and α &gt; 0 is the learning rate.
</bodyText>
<sectionHeader confidence="0.973862" genericHeader="method">
5 Entity Identification in Text
</sectionHeader>
<bodyText confidence="0.999531533333333">
We conduct experimental study on the task of entity iden-
tification in text (Bilenko et al., 2003; McCallum and
Wellner, 2003; Li et al., 2004). A given entity – rep-
resenting a person, a location or an organization – may
be mentioned in text in multiple, ambiguous ways. Con-
sider, for example, an open domain question answering
system (Voorhees, 2002) that attempts, given a question
like: “When was President Kennedy born?” to search a
large collection of articles in order to pinpoint the con-
cise answer: “on May 29, 1917.” The sentence, and even
the document that contains the answer, may not contain
the name “President Kennedy”; it may refer to this en-
tity as “Kennedy”, “JFK” or “John Fitzgerald Kennedy”.
Other documents may state that “John F. Kennedy, Jr.
was born on November 25, 1960”, but this fact refers to
our target entity’s son. Other mentions, such as “Senator
Kennedy” or “Mrs. Kennedy” are even “closer” to the
writing of the target entity, but clearly refer to different
entities. Understanding natural language requires identi-
fying whether different mentions of a name, within and
across documents, represent the same entity.
We study this problem for three entity types – People,
Location and Organization. Although deciding the coref-
erence of names within the same document might be rela-
tively easy, since within a single document identical men-
tions typically refer to the same entity, identifying coref-
erence across-document is much harder. With no stan-
dard corpora for studying the problem in a general setting
– both within and across documents, we created our own
corpus. This is done by collecting about 8,600 names
from 300 randomly sampled 1998-2000 New York Times
articles in the TREC corpus (Voorhees, 2002). These
names are first annotated by a named entity tagger, then
manually verified and given as input to an entity identi-
fier.
Since the number of classes (entities) for names is very
large, standard multi-class classification is not feasible.
Instead, we compare SDC with several pairwise classifi-
cation and clustering approaches. Some of them (for ex-
ample, those based on SoftTFIDF similarity) do not make
use of any domain knowledge, while others do exploit su-
pervision, such as LMR and SDC. Other works (Bilenko
et al., 2003) also exploited supervision in this problem by
discriminative training of a pairwise classifier but were
shown to be inferior.
</bodyText>
<listItem confidence="0.835978">
1. SoftTFIDF Classifier – a pairwise classifier deciding
</listItem>
<bodyText confidence="0.944694153846154">
whether any two names refer to the same entity, imple-
mented by thresholding a state-of-art SoftTFIDF similar-
ity metric for string comparison (Cohen et al., 2003). Dif-
ferent thresholds have been experimented but only the best
results are reported.
2. LMR Classifier (P|W) – a SNoW-based pairwise classi-
fier (Li et al., 2004) (described in Sec. 2.2) that learns a
linear function for each class over a collection of relational
features between two names: including string and token-
level features and structural features (listed in Table 1).
For pairwise classifiers like LMR and SoftTFIDF, predic-
tion is made over pairs of names so transitivity of predic-
tions is not guaranteed as in clustering.
</bodyText>
<listItem confidence="0.99703525">
3. Clustering over SoftTFIDF – a clustering approach based
on the SoftTFIDF similarity metric.
4. Clustering over LMR (P|W) – a clustering approach (Li et
al., 2004) by converting the LMR classifier into a similar-
ity metric (see Sec. 2.2).
5. SDC – our new supervised clustering approach. The dis-
tance metric is represented as a linear function over a set
of pairwise features as defined in Equ. 5.
</listItem>
<bodyText confidence="0.94115225">
The above approaches (2), (4) and (5) learn a classifier
or a distance metric using the same feature set as in Ta-
ble 1. Different clustering algorithms 2, such as Single-
Linkage, Complete-Linkage, Graph clustering (George,
</bodyText>
<footnote confidence="0.73039625">
2The clustering package Cluster by Michael Eisen at Stan-
ford University is adopted for K-medoids and CLUTO by
(George, 2003) is used for other algorithms. Details of these
algorithms can be found there.
</footnote>
<page confidence="0.693875">
68
</page>
<table confidence="0.999629428571429">
Honorific Equal active if both tokens are honorifics and identical.
Honorific Equivalence active if both tokens are honorifics, not identical, but equivalent.
Honorific Mismatch active for different honorifics.
Equality active if both tokens are identical.
Case-Insensitive Equal active if the tokens are case-insensitive equal.
Nickname active if tokens have a “nickname” relation.
Prefix Equality active if the prefixes of both tokens are equal.
Substring active if one of the tokens is a substring of the other.
Abbreviation active if one of the tokens is an abbreviation of the other.
Prefix Edit Distance active if the prefixes of both tokens have an edit-distance of 1.
Edit Distance active if the tokens have an edit-distance of 1.
Initial active if one of the tokens is an initial of another.
Symbol Map active if one token is a symbolic representative of the other.
Structural recording the location of the tokens that generate other features in two names.
</table>
<tableCaption confidence="0.999666">
Table 1: Features employed by LMR and SDC.
</tableCaption>
<bodyText confidence="0.99758025">
2003) – seeking a minimum cut of a nearest neighbor
graph, Repeated Bisections and K-medoids (Chu et al.,
2001) (a variation of K-means) are experimented in (5).
The number of entities in a data set is always given.
</bodyText>
<sectionHeader confidence="0.979339" genericHeader="evaluation">
6 Experimental Study
</sectionHeader>
<bodyText confidence="0.999970037037037">
Our experimental study focuses on (1) evaluating the
supervised discriminative clustering approach on entity
identification; (2) comparing it with existing pairwise
classification and clustering approaches widely used in
similar tasks; and (3) further analyzing the characteris-
tics of this new framework.
We use the TREC corpus to evaluate different ap-
proaches in identifying three types of entities: People,
Locations and Organization. For each type, we generate
three pairs of training and test sets, each containing about
300 names. We note that the three entity types yield very
different data sets, exhibited by some statistical proper-
ties3. Results on each entity type will be averaged over
the three sets and ten runs of two-fold cross-validation for
each of them. For SDC, given a training set with anno-
tated name pairs, a distance function is first trained using
the algorithm in Fig. 3 (in 20 iterations) with respect to
a clustering algorithm and then be used to partition the
corresponding test set with the same algorithm.
For a comparative evaluation, the outcomes of each ap-
proach on a test set of names are converted to a classifi-
cation over all possible pairs of names (including non-
matching pairs). Only examples in the set Mp, those
that are predicated to belong to the same entity (posi-
tive predictions) are used in the evaluation, and are com-
pared with the set Ma of examples annotated as positive.
The performance of an approach is then evaluated by F1
</bodyText>
<equation confidence="0.660543">
2|Mp � Ma|
</equation>
<bodyText confidence="0.956005">
value, defined as: F1 = |Mp|+|Ma |.
</bodyText>
<footnote confidence="0.88212">
3The average SoftTFIDF similarity between names of the
same entity is 0.81, 0.89 and 0.95 for people, locations and or-
ganizations respectively.
</footnote>
<subsectionHeader confidence="0.998102">
6.1 Comparison of Different Approaches
</subsectionHeader>
<bodyText confidence="0.999971230769231">
Fig. 4 presents the performance of different approaches
(described in Sec. 5) on identifying the three entity types.
We experimented with different clustering algorithms but
only the results by Single-Linkage are reported for Clus-
ter over LMR (P|W) and SDC, since they are the best.
SDC works well for all three entity types in spite of
their different characteristics. The best F1 values of SDC
are 92.7%, 92.4% and 95.7% for people, locations and
organizations respectively, about 20% − 30% error re-
duction compared with the best performance of the other
approaches. This is an indication that this new approach
which integrates metric learning and supervision in a uni-
fied framework, has significant advantages 4.
</bodyText>
<subsectionHeader confidence="0.999366">
6.2 Further Analysis of SDC
</subsectionHeader>
<bodyText confidence="0.998247588235294">
In the next experiments, we will further analyze the char-
acteristics of SDC by evaluating it in different settings.
Different Training Sizes Fig. 5 reports the relationship
between the performance of SDC and different training
sizes. The learning curves for other learning-based ap-
proaches are also shown. We find that SDC exhibits good
learning ability with limited supervision. When training
examples are very limited, for example, only 10% of all
300 names, pairwise classifiers based on Perceptron and
Winnow exhibit advantages over SDC. However, when
supervision become reasonable (30%+ examples), SDC
starts to outperform all other approaches.
Different Clustering Algorithms Fig. 6 shows the
performance of applying different clustering algorithms
(see Sec. 5) in the SDC approach. Single-Linkage and
Complete-Linkage outperform all other algorithms. One
possible reason is that this task has a great number of
</bodyText>
<footnote confidence="0.8831122">
4We note that in this experiment, the relative comparison
between the pairwise classifiers and the clustering approaches
over them is not consistent for all entity types. This can be
partially explained by the theoretical analysis in (Li et al., 2004)
and the difference between entity types.
</footnote>
<page confidence="0.854767">
69
</page>
<figureCaption confidence="0.906961714285714">
Figure 4: Performance of different approaches. The results are reported for SDC with a learning rate α = 100.0.
The Single-Linkage algorithm is applied whenever clustering is performed. Results are reported in Fl and averaged
over the three data sets for each entity type and 10 runs of two-fold cross-validation. Each training set typically
contains 300 annotated names.
Figure 5: Performance for different training sizes. Five learning-based approaches are compared. Single-Linkage is
applied whenever clustering is performed. X-axis denotes different percentages of 300 names used in training. Results
are reported in Fl and averaged over the three data sets for each entity type.
</figureCaption>
<figure confidence="0.994521461538461">
96
94
92
90
88
86
84
82
80
(a) People
92
90
88
86
84
82
80
SoftTFIDF
LMR (P)
LMR (W)
Cluster over SoftTFIDF
Cluster over LMR (P)
Cluster over LMR (W)
SDC
(b) Locations
96
94
92
90
88
86
84
82
80
(c) Organizations
100
100
95
95
90
85
80
75
70
10 20 30 40 50 60 70 80 90 100
(a) People
90
85
80
75
70
10 20 30 40 50 60 70 80 90 100
(b) Locations
90
85
80
75
70
10 20 30 40 50 60 70 80 90 100
(c) Organizations
LMR (P)
100
LMR (W)
Cluster over LMR (P)
Cluster over LMR (W)
95
SDC
96
94
92
90
88
α=1.0
α=10.0
α=100.0
α=1000.0
People Locations Organizations
Different Entity Types
Different Entity Types
People Locations Organizations
90
80
70
60
50
40
Graph
K−Medoids
RB
Complete−Linkage
Single−Linkage
</figure>
<figureCaption confidence="0.76104325">
Figure 6: Different clustering algorithms. Five cluster-
ing algorithms are compared in SDC (α = 100.0). Re-
sults are averaged over the three data sets for each entity
type and 10 runs of two-fold cross-validations.
</figureCaption>
<bodyText confidence="0.983329181818182">
)
classes (100 − 200 entities) for 300 names in each sin-
gle data set. The results indicate that the metric learn-
ing process relies on properties of the data set, as well as
the clustering algorithm. Even if a good distance metric
could be learned in SDC, choosing an appropriate algo-
rithm for the specific task is still important.
Different Learning Rates We also experimented with
different learning rates in the SDC approach as shown in
Fig. 7. It seems that SDC is not very sensitive to different
learning rates as long as it is in a reasonable range.
</bodyText>
<page confidence="0.484497">
86
</page>
<figureCaption confidence="0.933772">
Figure 7: Performance for different learning rates.
</figureCaption>
<bodyText confidence="0.852511333333333">
SDC with different learning rates (α = 1.0, 10.0, 100.0,
1000.0) compared in this setting. Single-Linkage cluster-
ing algorithm is applied.
</bodyText>
<subsectionHeader confidence="0.936488">
6.3 Discussion
</subsectionHeader>
<bodyText confidence="0.998672">
(% 1
The reason that SDC can outperform existing clustering
approaches can be explained by the advantages of SDC –
training the distance function with respect to the chosen
clustering algorithm, guided by supervision, but they do
not explain why it can also outperform the pairwise clas-
sifiers. One intuitive explanation is that supervision in the
entity identification task or similar tasks is typically given
on whether two names correspond to the same entity –
entity-level annotation. Therefore it does not necessarily
mean whether they are similar in appearance. For exam-
</bodyText>
<page confidence="0.657476">
70
</page>
<bodyText confidence="0.999868214285714">
ple, “Brian” and “Wilson” could both refer to a person
“Brian Wilson” in different contexts, and thus this name
pair is a positive example in training a pairwise classi-
fier. However, with features that only capture the appear-
ance similarity between names, such apparently different
names become training noise. This is what exactly hap-
pened when we train the LMR classifier with such name
pairs. SDC, however, can employ this entity-level anno-
tation and avoid the problem through transitivity in clus-
tering. In the above example, if there is “Brian Wilson”
in the data set, then “Brian” and “Wilson” can be both
clustered into the same group with “Brian Wilson”. Such
cases do not frequently occur for locations and organiza-
tion but still exist.
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998638">
In this paper, we explicitly formalize clustering as a learn-
ing task, and propose a unified framework for training
a metric for any chosen clustering algorithm, guided by
domain-specific supervision. Our experiments exhibit the
advantage of this approach over existing approaches on
Entity Identification. Further research in this direction
will focus on (1) applying it to more NLP tasks, e.g.
coreference resolution; (2) analyzing the related theoret-
ical issues, e.g. the convergence of the algorithm; and
(3) comparing it experimentally with related approaches,
such as (Xing et al., 2002) and (McCallum and Wellner,
2003).
Acknowledgement This research is supported by
NSF grants IIS-9801638 and ITR IIS-0085836, an ONR
MURI Award and an equipment donation from AMD.
</bodyText>
<sectionHeader confidence="0.998405" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884151515152">
F. R. Bach and M. I. Jordan. 2003. Learning spectral clustering.
In NIPS-03.
A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. 2003.
Learning distance functions using equivalence relations. In
ICML-03, pages 11–18.
M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and S. Fien-
berg. 2003. Adaptive name matching in information integra-
tion. IEEE Intelligent Systems, pages 16–23.
M Bilenko, S. Basu, and R. J. Mooney. 2004. Integrating con-
straints and metric learning in semi-supervised clustering. In
ICML-04, pages 81–88.
P. Brown, P. deSouza R. Mercer, V. Pietra, and J. Lai. 1992.
Class-based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
C. Cardie and K. Wagstaff. 1999. Noun phrase coreference as
clustering. In EMNLP-99, pages 82–89.
S. C. Chu, J. F. Roddick, and J. S. Pan. 2001. A comparative
study and extensions to k-medoids algorithms. In ICOTA-01.
W. Cohen and J. Richman. 2002. Learning to match and clus-
ter large high-dimensional data sets for data integration. In
KDD-02, pages 475–480.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A comparison
of string metrics for name-matching tasks. In IIWeb Work-
shop 2003, pages 73–78.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based mod-
els of word cooccurrence probabilities. Machine Learning,
34(1-3):43–69.
Y. Freund and R. Schapire. 1998. Large margin classification
using the Perceptron algorithm. In COLT-98.
M. Geffet and I. Dagan. 2004. Automatic feature vector quality
and distributional similarity. In COLING-04.
K. George. 2003. Cluto: A clustering toolkit. Technical report,
Dept of Computer Science, University of Minnesota.
J. Hartigan and M. Wong. 1979. A k-means clustering algo-
rithm. Applied Statistics, 28(1):100–108.
S. Kamvar, D. Klein, and C. Manning. 2002. Interpreting and
extending classical agglomerative clustering algorithms us-
ing a model-based approach. In ICML-02, pages 283–290.
L. Lee. 1997. Similarity-Based Approaches to Natural Lan-
guage Processing. Ph.D. thesis, Harvard University, Cam-
bridge, MA.
X. Li, P. Morie, and D. Roth. 2004. Identification and trac-
ing of ambiguous names: Discriminative and generative ap-
proaches. In AAAI-04, pages 419–424.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation. In CoNLL-03, pages 33–40.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In IJCAI Workshop on Information Integration
on the Web.
D. Mochihashi, G. Kikui, and K. Kita. 2004. Learning non-
structural distance metric by minimum cluster distortions. In
COLING-04.
P. Pantel and D. Lin. 2002. Discovering word senses from text.
In KDD-02, pages 613–619.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In AAAI-98, pages 806–813.
M. Schultz and T. Joachims. 2004. Learning a distance metric
from relative comparisons. In NIPS-04.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In TREC-02, pages 115–123.
J. Weeds, D. Weir, and D. McCarthy. 2004. Characterising
measures of lexical distributional similarity. In COLING-04.
E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. 2002.
Distance metric learning, with application to clustering with
side-information. In NIPS-02.
</reference>
<page confidence="0.942905">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975262">
<title confidence="0.999634">Discriminative Training of Clustering Theory and Experiments with Entity Identification</title>
<author confidence="0.998579">Xin Li</author>
<author confidence="0.998579">Dan</author>
<affiliation confidence="0.99811">Department of Computer University of Illinois, Urbana, IL</affiliation>
<email confidence="0.988757">(xli1,danr)@cs.uiuc.edu</email>
<abstract confidence="0.999583421052632">Clustering is an optimization procedure that partitions a set of elements to optimize some criteria, based on a fixed distance metric defined between the elements. Clustering approaches have been widely applied in natural language processing and it has been shown repeatedly that their success depends on defining a good distance metric, one that is appropriate for the task and the clustering algorithm used. This paper develops a framework in which clustering is viewed as a learning task, and proposes a way to train a distance metric that is appropriate for the chosen clustering algorithm in the context of the given task. Experiments in the context of the entity identification problem exhibit significant performance improvements over state-of-the-art clustering approaches developed for this problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F R Bach</author>
<author>M I Jordan</author>
</authors>
<title>Learning spectral clustering.</title>
<date>2003</date>
<booktitle>In NIPS-03.</booktitle>
<contexts>
<context position="13522" citStr="Bach and Jordan, 2003" startWordPosition="2210" endWordPosition="2213">n of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) indeed suggest to learn a metric directly in a clustering task but the learning procedure is specific for one clustering algorithm. 3 Supervised Discriminative Clustering To solve the limitations of existing approaches, we develop the Supervised Discriminative Clustering Framework (SDC), that can train a distance function with respect to any chosen clustering algorithm in the context of a given task, guided by supervision. Figure 2: Supervised Discriminative Clustering Stage: h(S’ ) Fig. 2 presents this framework, in which a clustering task is explicitly split into trai</context>
</contexts>
<marker>Bach, Jordan, 2003</marker>
<rawString>F. R. Bach and M. I. Jordan. 2003. Learning spectral clustering. In NIPS-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bar-Hillel</author>
<author>T Hertz</author>
<author>N Shental</author>
<author>D Weinshall</author>
</authors>
<title>Learning distance functions using equivalence relations.</title>
<date>2003</date>
<booktitle>In ICML-03,</booktitle>
<pages>11--18</pages>
<contexts>
<context position="12973" citStr="Bar-Hillel et al., 2003" startWordPosition="2124" endWordPosition="2127">tecture (Roth, 1998) based on variations of Perceptron and Winnow, and using a collection of relational features between a pair of names. The distance between two names is defined as a softmax over the classifier’s output. As expected, experimental evidence (Cohen et al., 2003; Cohen and Richman, 2002; Li et al., 2004) shows that domain-specific distance functions improve over a fixed metric. This can be explained by the flexibility provided by adapting the metric to the domain as well as the contribution of supervision that guides the adaptation of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) indeed suggest to learn a m</context>
</contexts>
<marker>Bar-Hillel, Hertz, Shental, Weinshall, 2003</marker>
<rawString>A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. 2003. Learning distance functions using equivalence relations. In ICML-03, pages 11–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilenko</author>
<author>R Mooney</author>
<author>W Cohen</author>
<author>P Ravikumar</author>
<author>S Fienberg</author>
</authors>
<title>Adaptive name matching in information integration.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>16--23</pages>
<contexts>
<context position="2379" citStr="Bilenko et al., 2003" startWordPosition="365" endWordPosition="368">een experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected to partition the data elements, with the goal of optimizing some form of clustering quality with respect to the given distance metric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of</context>
<context position="21624" citStr="Bilenko et al., 2003" startWordPosition="3638" endWordPosition="3641">g Criterion: If t &gt; T, the algorithm exits and outputs the metric in the iteration with the least error. Figure 3: A general training algorithm for SDC For the weighted clustering error in Equ. 3, and linearly weighted Manhattan distances as in Equ. 5, the update rule in Step 2(b) becomes wtl = wt−1 l − α · [ψt−1 l (p, S) − ψt−1 l (h, S)]. (6) where ψl (p, S) ≡ |2�ES�l (xi, x�) ·I[p(xiS |) = p(xj)] and ψl(h,S) ≡ |1|2 Exi,xj∈S φl (xi ,xj) I[h(xi) = h(xj)], and α &gt; 0 is the learning rate. 5 Entity Identification in Text We conduct experimental study on the task of entity identification in text (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). A given entity – representing a person, a location or an organization – may be mentioned in text in multiple, ambiguous ways. Consider, for example, an open domain question answering system (Voorhees, 2002) that attempts, given a question like: “When was President Kennedy born?” to search a large collection of articles in order to pinpoint the concise answer: “on May 29, 1917.” The sentence, and even the document that contains the answer, may not contain the name “President Kennedy”; it may refer to this entity as “Kennedy”, “JFK” or “John Fitzge</context>
<context position="23796" citStr="Bilenko et al., 2003" startWordPosition="3995" endWordPosition="3998">m 300 randomly sampled 1998-2000 New York Times articles in the TREC corpus (Voorhees, 2002). These names are first annotated by a named entity tagger, then manually verified and given as input to an entity identifier. Since the number of classes (entities) for names is very large, standard multi-class classification is not feasible. Instead, we compare SDC with several pairwise classification and clustering approaches. Some of them (for example, those based on SoftTFIDF similarity) do not make use of any domain knowledge, while others do exploit supervision, such as LMR and SDC. Other works (Bilenko et al., 2003) also exploited supervision in this problem by discriminative training of a pairwise classifier but were shown to be inferior. 1. SoftTFIDF Classifier – a pairwise classifier deciding whether any two names refer to the same entity, implemented by thresholding a state-of-art SoftTFIDF similarity metric for string comparison (Cohen et al., 2003). Different thresholds have been experimented but only the best results are reported. 2. LMR Classifier (P|W) – a SNoW-based pairwise classifier (Li et al., 2004) (described in Sec. 2.2) that learns a linear function for each class over a collection of re</context>
</contexts>
<marker>Bilenko, Mooney, Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and S. Fienberg. 2003. Adaptive name matching in information integration. IEEE Intelligent Systems, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilenko</author>
<author>S Basu</author>
<author>R J Mooney</author>
</authors>
<title>Integrating constraints and metric learning in semi-supervised clustering.</title>
<date>2004</date>
<booktitle>In ICML-04,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="13545" citStr="Bilenko et al., 2004" startWordPosition="2214" endWordPosition="2217">works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) indeed suggest to learn a metric directly in a clustering task but the learning procedure is specific for one clustering algorithm. 3 Supervised Discriminative Clustering To solve the limitations of existing approaches, we develop the Supervised Discriminative Clustering Framework (SDC), that can train a distance function with respect to any chosen clustering algorithm in the context of a given task, guided by supervision. Figure 2: Supervised Discriminative Clustering Stage: h(S’ ) Fig. 2 presents this framework, in which a clustering task is explicitly split into training and application st</context>
</contexts>
<marker>Bilenko, Basu, Mooney, 2004</marker>
<rawString>M Bilenko, S. Basu, and R. J. Mooney. 2004. Integrating constraints and metric learning in semi-supervised clustering. In ICML-04, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>P deSouza R Mercer</author>
<author>V Pietra</author>
<author>J Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1468" citStr="Brown et al., 1992" startWordPosition="217" endWordPosition="220">. Experiments in the context of the entity identification problem exhibit significant performance improvements over state-of-the-art clustering approaches developed for this problem. 1 Introduction Clustering approaches have been widely applied to natural language processing (NLP) problems. Typically, natural language elements (words, phrases, sentences, etc.) are partitioned into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller g</context>
</contexts>
<marker>Brown, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. Brown, P. deSouza R. Mercer, V. Pietra, and J. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>K Wagstaff</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<booktitle>In EMNLP-99,</booktitle>
<pages>82--89</pages>
<contexts>
<context position="2502" citStr="Cardie and Wagstaff, 1999" startWordPosition="384" endWordPosition="387">s also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected to partition the data elements, with the goal of optimizing some form of clustering quality with respect to the given distance metric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of tightness of the resulting clusters based on the Euclidean distance. Clustering is typically called an unsupervised method</context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>C. Cardie and K. Wagstaff. 1999. Noun phrase coreference as clustering. In EMNLP-99, pages 82–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Chu</author>
<author>J F Roddick</author>
<author>J S Pan</author>
</authors>
<title>A comparative study and extensions to k-medoids algorithms.</title>
<date>2001</date>
<booktitle>In ICOTA-01.</booktitle>
<contexts>
<context position="26621" citStr="Chu et al., 2001" startWordPosition="4455" endWordPosition="4458"> the other. Abbreviation active if one of the tokens is an abbreviation of the other. Prefix Edit Distance active if the prefixes of both tokens have an edit-distance of 1. Edit Distance active if the tokens have an edit-distance of 1. Initial active if one of the tokens is an initial of another. Symbol Map active if one token is a symbolic representative of the other. Structural recording the location of the tokens that generate other features in two names. Table 1: Features employed by LMR and SDC. 2003) – seeking a minimum cut of a nearest neighbor graph, Repeated Bisections and K-medoids (Chu et al., 2001) (a variation of K-means) are experimented in (5). The number of entities in a data set is always given. 6 Experimental Study Our experimental study focuses on (1) evaluating the supervised discriminative clustering approach on entity identification; (2) comparing it with existing pairwise classification and clustering approaches widely used in similar tasks; and (3) further analyzing the characteristics of this new framework. We use the TREC corpus to evaluate different approaches in identifying three types of entities: People, Locations and Organization. For each type, we generate three pair</context>
</contexts>
<marker>Chu, Roddick, Pan, 2001</marker>
<rawString>S. C. Chu, J. F. Roddick, and J. S. Pan. 2001. A comparative study and extensions to k-medoids algorithms. In ICOTA-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>J Richman</author>
</authors>
<title>Learning to match and cluster large high-dimensional data sets for data integration.</title>
<date>2002</date>
<booktitle>In KDD-02,</booktitle>
<pages>475--480</pages>
<contexts>
<context position="11556" citStr="Cohen and Richman, 2002" startWordPosition="1885" endWordPosition="1889"> that are appropriate for class-based language models may not be appropriate for other tasks. We illustrate this critical point in Fig. 1. (a) and (b) show that even for the same data collection, different clustering algorithms with the same metric could generate different outcomes. (b) and (c) show that with the same clustering algorithm, different metrics could also produce different outcomes. Therefore, a good distance metric should be both domain-specific and associated with a specific clustering algorithm. 2.2 Metric Learning via Pairwise Classification Several works (Cohen et al., 2003; Cohen and Richman, 2002; McCallum and Wellner, 2003; Li et al., 2004) have tried to remedy the aforementioned problems by attempting to learn a distance function in a domainspecific way via pairwise classification. In the training stage, given a set of labeled element pairs, a function f : X x X → 10, 11 is trained to classify any two elements as to whether they belong to the same class (1) or not (0), independently of other elements. The distance between the two elements is defined by converting the prediction confidence of the pairwise classifier, and clustering is then performed based on this distance function. P</context>
</contexts>
<marker>Cohen, Richman, 2002</marker>
<rawString>W. Cohen and J. Richman. 2002. Learning to match and cluster large high-dimensional data sets for data integration. In KDD-02, pages 475–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>P Ravikumar</author>
<author>S Fienberg</author>
</authors>
<title>A comparison of string metrics for name-matching tasks.</title>
<date>2003</date>
<booktitle>In IIWeb Workshop</booktitle>
<pages>73--78</pages>
<contexts>
<context position="9780" citStr="Cohen et al., 2003" startWordPosition="1608" endWordPosition="1611"> tasks. For the former, the focus is on developing and selecting good distance (similarity) metrics that reflect well pairwise proximity between domain elements. The “goodness” of a metric is empirically measured when combined with different clustering algorithms on different problems. For example (Lee, 1997; Weeds et al., 2004) compare similarity metrics such as the Cosine, Manhattan and Euclidean distances, Kullback-Leibler divergence, Jensen-Shannon divergence, and Jaccard’s Coefficient, that could be applied in general clustering tasks, on the task of measuring distributional similarity. (Cohen et al., 2003) compares a number of string and token-based similarity metrics on the task of matching entity names and found that, tuuv dθ(x1,x2) ≡ Xm l=1 K X k=1 qS(h) ≡ X xESk 65 overall, the best-performing method is a hybrid scheme (SoftTFIDF) combining a TFIDF weighting scheme of tokens with the Jaro-Winkler string-distance scheme that is widely used for record linkage in databases. (a) Single-Linkage with (b) K-Means with (c) K-Means with a Euclidean Euclidean Linear Metric d(x1,x2) = [(x1(1) -x2(1))2+(x1(2) -x2(2))2]1/2 d(x1,x2) = |(x1(1) +x2(1))-(x1(2) +x2(2))| Figure 1: Different combinations of cl</context>
<context position="11531" citStr="Cohen et al., 2003" startWordPosition="1881" endWordPosition="1884">at proximity metrics that are appropriate for class-based language models may not be appropriate for other tasks. We illustrate this critical point in Fig. 1. (a) and (b) show that even for the same data collection, different clustering algorithms with the same metric could generate different outcomes. (b) and (c) show that with the same clustering algorithm, different metrics could also produce different outcomes. Therefore, a good distance metric should be both domain-specific and associated with a specific clustering algorithm. 2.2 Metric Learning via Pairwise Classification Several works (Cohen et al., 2003; Cohen and Richman, 2002; McCallum and Wellner, 2003; Li et al., 2004) have tried to remedy the aforementioned problems by attempting to learn a distance function in a domainspecific way via pairwise classification. In the training stage, given a set of labeled element pairs, a function f : X x X → 10, 11 is trained to classify any two elements as to whether they belong to the same class (1) or not (0), independently of other elements. The distance between the two elements is defined by converting the prediction confidence of the pairwise classifier, and clustering is then performed based on </context>
<context position="24141" citStr="Cohen et al., 2003" startWordPosition="4048" endWordPosition="4051"> compare SDC with several pairwise classification and clustering approaches. Some of them (for example, those based on SoftTFIDF similarity) do not make use of any domain knowledge, while others do exploit supervision, such as LMR and SDC. Other works (Bilenko et al., 2003) also exploited supervision in this problem by discriminative training of a pairwise classifier but were shown to be inferior. 1. SoftTFIDF Classifier – a pairwise classifier deciding whether any two names refer to the same entity, implemented by thresholding a state-of-art SoftTFIDF similarity metric for string comparison (Cohen et al., 2003). Different thresholds have been experimented but only the best results are reported. 2. LMR Classifier (P|W) – a SNoW-based pairwise classifier (Li et al., 2004) (described in Sec. 2.2) that learns a linear function for each class over a collection of relational features between two names: including string and tokenlevel features and structural features (listed in Table 1). For pairwise classifiers like LMR and SoftTFIDF, prediction is made over pairs of names so transitivity of predictions is not guaranteed as in clustering. 3. Clustering over SoftTFIDF – a clustering approach based on the S</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A comparison of string metrics for name-matching tasks. In IIWeb Workshop 2003, pages 73–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1489" citStr="Dagan et al., 1999" startWordPosition="221" endWordPosition="224"> context of the entity identification problem exhibit significant performance improvements over state-of-the-art clustering approaches developed for this problem. 1 Introduction Clustering approaches have been widely applied to natural language processing (NLP) problems. Typically, natural language elements (words, phrases, sentences, etc.) are partitioned into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their </context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the Perceptron algorithm.</title>
<date>1998</date>
<booktitle>In COLT-98. M. Geffet</booktitle>
<contexts>
<context position="20234" citStr="Freund and Schapire, 1998" startWordPosition="3367" endWordPosition="3370">is general training procedure depends on the convexity of the error as a function of B. For example, since the error function we use is linear in B, the algorithm is guaranteed to converge to a global minimum. In this case, for rate of convergence, one can appeal to general results that typically imply, when there exists a parameter vector with zero error, that convergence rate [d(xi, xj)·Aij+(D−d(xi, xj))·Bij] xi,xj ES d(x1, x2) _ wl · 0l(x1,x2) (5) 67 depends on the ‘separation” of the training data, which roughly means the minimal error archived with this parameter vector. Results such as (Freund and Schapire, 1998) can be used to extend the rate of convergence result a bit beyond the separable case, when a small number of the pairs are not separable. Algorithm: SDC-Learner Input: S and p(S): the labeled data set. A: the clustering algorithm. errS(h, p): the clustering error function. α &gt; 0 : the learning rate. T (typically T is large) : the number of iterations allowed. Output: O* : the parameters in the distance function d. 1. In the initial (I-) step, we randomly choose O0 for d. After this step we have the initial d0 and h0. 2. Then we iterate over t (t = 1, 2, · · ·), (a) Partition S using ht−1(S) _</context>
</contexts>
<marker>Freund, Schapire, 1998</marker>
<rawString>Y. Freund and R. Schapire. 1998. Large margin classification using the Perceptron algorithm. In COLT-98. M. Geffet and I. Dagan. 2004. Automatic feature vector quality and distributional similarity. In COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K George</author>
</authors>
<title>Cluto: A clustering toolkit.</title>
<date>2003</date>
<journal>Applied Statistics,</journal>
<tech>Technical report,</tech>
<volume>28</volume>
<issue>1</issue>
<institution>Dept of Computer Science, University of</institution>
<contexts>
<context position="25419" citStr="George, 2003" startWordPosition="4262" endWordPosition="4263">tering approach (Li et al., 2004) by converting the LMR classifier into a similarity metric (see Sec. 2.2). 5. SDC – our new supervised clustering approach. The distance metric is represented as a linear function over a set of pairwise features as defined in Equ. 5. The above approaches (2), (4) and (5) learn a classifier or a distance metric using the same feature set as in Table 1. Different clustering algorithms 2, such as SingleLinkage, Complete-Linkage, Graph clustering (George, 2The clustering package Cluster by Michael Eisen at Stanford University is adopted for K-medoids and CLUTO by (George, 2003) is used for other algorithms. Details of these algorithms can be found there. 68 Honorific Equal active if both tokens are honorifics and identical. Honorific Equivalence active if both tokens are honorifics, not identical, but equivalent. Honorific Mismatch active for different honorifics. Equality active if both tokens are identical. Case-Insensitive Equal active if the tokens are case-insensitive equal. Nickname active if tokens have a “nickname” relation. Prefix Equality active if the prefixes of both tokens are equal. Substring active if one of the tokens is a substring of the other. Abb</context>
</contexts>
<marker>George, 2003</marker>
<rawString>K. George. 2003. Cluto: A clustering toolkit. Technical report, Dept of Computer Science, University of Minnesota. J. Hartigan and M. Wong. 1979. A k-means clustering algorithm. Applied Statistics, 28(1):100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kamvar</author>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Interpreting and extending classical agglomerative clustering algorithms using a model-based approach. In</title>
<date>2002</date>
<booktitle>ICML-02,</booktitle>
<pages>283--290</pages>
<contexts>
<context position="4101" citStr="Kamvar et al., 2002" startWordPosition="640" endWordPosition="643">al., 2004; Bilenko et al., 2003) and more). This scenario, however, has several setbacks. First, the process of clustering, simply a function that partitions a set of elements into different classes, involves no learning and thus lacks flexibility. Second, clustering quality is typically defined with respect to a fixed distance metric, without utilizing any direct supervision, so the practical clustering outcome could be disparate from one’s intention. Third, when clustering with a given algorithm and a fixed metric, one in fact makes some implicit assumptions on the data and the task (e.g., (Kamvar et al., 2002); more on that below). For example, the optimal conditions under which for K-means works are that the data is generated from a uniform mixture of Gaussian models; this may not hold in reality. This paper proposes a new clustering framework that addresses all the problems discussed above. Specifically, Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 64–71, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics we define clustering as a learning task: in the training stage, a partition function, parameterized by a distance metric, is tr</context>
</contexts>
<marker>Kamvar, Klein, Manning, 2002</marker>
<rawString>S. Kamvar, D. Klein, and C. Manning. 2002. Interpreting and extending classical agglomerative clustering algorithms using a model-based approach. In ICML-02, pages 283–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Similarity-Based Approaches to Natural Language Processing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="1826" citStr="Lee, 1997" startWordPosition="276" endWordPosition="277">partitioned into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004).</context>
<context position="3470" citStr="Lee, 1997" startWordPosition="541" endWordPosition="542">distance metric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of tightness of the resulting clusters based on the Euclidean distance. Clustering is typically called an unsupervised method, since data elements are used without labels during the clustering process and labels are not used to provide feedback to the optimization process. E.g., labels are not taken into account when measuring the quality of the partition. However, in many cases, supervision is used at the application level when determining an appropriate distance metric (e.g., (Lee, 1997; Weeds et al., 2004; Bilenko et al., 2003) and more). This scenario, however, has several setbacks. First, the process of clustering, simply a function that partitions a set of elements into different classes, involves no learning and thus lacks flexibility. Second, clustering quality is typically defined with respect to a fixed distance metric, without utilizing any direct supervision, so the practical clustering outcome could be disparate from one’s intention. Third, when clustering with a given algorithm and a fixed metric, one in fact makes some implicit assumptions on the data and the ta</context>
<context position="9470" citStr="Lee, 1997" startWordPosition="1566" endWordPosition="1567"> clear that without any supervision, the resulting function is not guaranteed to agree with the target function p (or one’s original intention). Given this realization, there has been some work on selecting a good distance metric for a family of related problems and on learning a metric for specific tasks. For the former, the focus is on developing and selecting good distance (similarity) metrics that reflect well pairwise proximity between domain elements. The “goodness” of a metric is empirically measured when combined with different clustering algorithms on different problems. For example (Lee, 1997; Weeds et al., 2004) compare similarity metrics such as the Cosine, Manhattan and Euclidean distances, Kullback-Leibler divergence, Jensen-Shannon divergence, and Jaccard’s Coefficient, that could be applied in general clustering tasks, on the task of measuring distributional similarity. (Cohen et al., 2003) compares a number of string and token-based similarity metrics on the task of matching entity names and found that, tuuv dθ(x1,x2) ≡ Xm l=1 K X k=1 qS(h) ≡ X xESk 65 overall, the best-performing method is a hybrid scheme (SoftTFIDF) combining a TFIDF weighting scheme of tokens with the Ja</context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>L. Lee. 1997. Similarity-Based Approaches to Natural Language Processing. Ph.D. thesis, Harvard University, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>P Morie</author>
<author>D Roth</author>
</authors>
<title>Identification and tracing of ambiguous names: Discriminative and generative approaches.</title>
<date>2004</date>
<booktitle>In AAAI-04,</booktitle>
<pages>419--424</pages>
<contexts>
<context position="2425" citStr="Li et al., 2004" startWordPosition="374" endWordPosition="377">, 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected to partition the data elements, with the goal of optimizing some form of clustering quality with respect to the given distance metric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of tightness of the resulting clusters based on </context>
<context position="6214" citStr="Li et al., 2004" startWordPosition="981" endWordPosition="984">age also exploit this information off-line, when exploring which metrics are appropriate for the task. Our framework makes better use of this resource by incorporating it directly into the metric training process; training is driven by true clustering error, computed via the specific algorithm chosen to partition the data. We study this new framework empirically on the entity identification problem – identifying whether different mentions of real world entities, such as “JFK” and “John Kennedy”, within and across text documents, actually represent the same concept (McCallum and Wellner, 2003; Li et al., 2004). Our experimental results exhibit a significant performance improvement over existing approaches (20% − 30% F1 error reduction) on all three types of entities we study, and indicate its promising prospective in other natural language tasks. The rest of this paper discusses existing clustering approaches (Sec. 2) and then introduces our Supervised Discriminative Clustering framework (SDC) (Sec. 3) and a general learner for training in it (Sec. 4). Sec. 5 describes the entity identification problem and Sec. 6 compares different clustering approaches on this task. 2 Clustering in Natural Languag</context>
<context position="11602" citStr="Li et al., 2004" startWordPosition="1894" endWordPosition="1897"> may not be appropriate for other tasks. We illustrate this critical point in Fig. 1. (a) and (b) show that even for the same data collection, different clustering algorithms with the same metric could generate different outcomes. (b) and (c) show that with the same clustering algorithm, different metrics could also produce different outcomes. Therefore, a good distance metric should be both domain-specific and associated with a specific clustering algorithm. 2.2 Metric Learning via Pairwise Classification Several works (Cohen et al., 2003; Cohen and Richman, 2002; McCallum and Wellner, 2003; Li et al., 2004) have tried to remedy the aforementioned problems by attempting to learn a distance function in a domainspecific way via pairwise classification. In the training stage, given a set of labeled element pairs, a function f : X x X → 10, 11 is trained to classify any two elements as to whether they belong to the same class (1) or not (0), independently of other elements. The distance between the two elements is defined by converting the prediction confidence of the pairwise classifier, and clustering is then performed based on this distance function. Particularly, (Li et al., 2004) applied this ap</context>
<context position="21670" citStr="Li et al., 2004" startWordPosition="3646" endWordPosition="3649">puts the metric in the iteration with the least error. Figure 3: A general training algorithm for SDC For the weighted clustering error in Equ. 3, and linearly weighted Manhattan distances as in Equ. 5, the update rule in Step 2(b) becomes wtl = wt−1 l − α · [ψt−1 l (p, S) − ψt−1 l (h, S)]. (6) where ψl (p, S) ≡ |2�ES�l (xi, x�) ·I[p(xiS |) = p(xj)] and ψl(h,S) ≡ |1|2 Exi,xj∈S φl (xi ,xj) I[h(xi) = h(xj)], and α &gt; 0 is the learning rate. 5 Entity Identification in Text We conduct experimental study on the task of entity identification in text (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). A given entity – representing a person, a location or an organization – may be mentioned in text in multiple, ambiguous ways. Consider, for example, an open domain question answering system (Voorhees, 2002) that attempts, given a question like: “When was President Kennedy born?” to search a large collection of articles in order to pinpoint the concise answer: “on May 29, 1917.” The sentence, and even the document that contains the answer, may not contain the name “President Kennedy”; it may refer to this entity as “Kennedy”, “JFK” or “John Fitzgerald Kennedy”. Other documents may state that </context>
<context position="24303" citStr="Li et al., 2004" startWordPosition="4075" endWordPosition="4078">omain knowledge, while others do exploit supervision, such as LMR and SDC. Other works (Bilenko et al., 2003) also exploited supervision in this problem by discriminative training of a pairwise classifier but were shown to be inferior. 1. SoftTFIDF Classifier – a pairwise classifier deciding whether any two names refer to the same entity, implemented by thresholding a state-of-art SoftTFIDF similarity metric for string comparison (Cohen et al., 2003). Different thresholds have been experimented but only the best results are reported. 2. LMR Classifier (P|W) – a SNoW-based pairwise classifier (Li et al., 2004) (described in Sec. 2.2) that learns a linear function for each class over a collection of relational features between two names: including string and tokenlevel features and structural features (listed in Table 1). For pairwise classifiers like LMR and SoftTFIDF, prediction is made over pairs of names so transitivity of predictions is not guaranteed as in clustering. 3. Clustering over SoftTFIDF – a clustering approach based on the SoftTFIDF similarity metric. 4. Clustering over LMR (P|W) – a clustering approach (Li et al., 2004) by converting the LMR classifier into a similarity metric (see </context>
<context position="30366" citStr="Li et al., 2004" startWordPosition="5053" endWordPosition="5056">en supervision become reasonable (30%+ examples), SDC starts to outperform all other approaches. Different Clustering Algorithms Fig. 6 shows the performance of applying different clustering algorithms (see Sec. 5) in the SDC approach. Single-Linkage and Complete-Linkage outperform all other algorithms. One possible reason is that this task has a great number of 4We note that in this experiment, the relative comparison between the pairwise classifiers and the clustering approaches over them is not consistent for all entity types. This can be partially explained by the theoretical analysis in (Li et al., 2004) and the difference between entity types. 69 Figure 4: Performance of different approaches. The results are reported for SDC with a learning rate α = 100.0. The Single-Linkage algorithm is applied whenever clustering is performed. Results are reported in Fl and averaged over the three data sets for each entity type and 10 runs of two-fold cross-validation. Each training set typically contains 300 annotated names. Figure 5: Performance for different training sizes. Five learning-based approaches are compared. Single-Linkage is applied whenever clustering is performed. X-axis denotes different p</context>
</contexts>
<marker>Li, Morie, Roth, 2004</marker>
<rawString>X. Li, P. Morie, and D. Roth. 2004. Identification and tracing of ambiguous names: Discriminative and generative approaches. In AAAI-04, pages 419–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In CoNLL-03,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2144" citStr="Mann and Yarowsky, 2003" startWordPosition="329" endWordPosition="332">e used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected t</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>G. Mann and D. Yarowsky. 2003. Unsupervised personal name disambiguation. In CoNLL-03, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="2407" citStr="McCallum and Wellner, 2003" startWordPosition="369" endWordPosition="373">these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. Clustering is an optimization procedure that takes as input (1) a collection of domain elements along with (2) a distance metric between them and (3) an algorithm selected to partition the data elements, with the goal of optimizing some form of clustering quality with respect to the given distance metric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of tightness of the resulting </context>
<context position="6196" citStr="McCallum and Wellner, 2003" startWordPosition="976" endWordPosition="980">pplications in natural language also exploit this information off-line, when exploring which metrics are appropriate for the task. Our framework makes better use of this resource by incorporating it directly into the metric training process; training is driven by true clustering error, computed via the specific algorithm chosen to partition the data. We study this new framework empirically on the entity identification problem – identifying whether different mentions of real world entities, such as “JFK” and “John Kennedy”, within and across text documents, actually represent the same concept (McCallum and Wellner, 2003; Li et al., 2004). Our experimental results exhibit a significant performance improvement over existing approaches (20% − 30% F1 error reduction) on all three types of entities we study, and indicate its promising prospective in other natural language tasks. The rest of this paper discusses existing clustering approaches (Sec. 2) and then introduces our Supervised Discriminative Clustering framework (SDC) (Sec. 3) and a general learner for training in it (Sec. 4). Sec. 5 describes the entity identification problem and Sec. 6 compares different clustering approaches on this task. 2 Clustering </context>
<context position="11584" citStr="McCallum and Wellner, 2003" startWordPosition="1890" endWordPosition="1893"> class-based language models may not be appropriate for other tasks. We illustrate this critical point in Fig. 1. (a) and (b) show that even for the same data collection, different clustering algorithms with the same metric could generate different outcomes. (b) and (c) show that with the same clustering algorithm, different metrics could also produce different outcomes. Therefore, a good distance metric should be both domain-specific and associated with a specific clustering algorithm. 2.2 Metric Learning via Pairwise Classification Several works (Cohen et al., 2003; Cohen and Richman, 2002; McCallum and Wellner, 2003; Li et al., 2004) have tried to remedy the aforementioned problems by attempting to learn a distance function in a domainspecific way via pairwise classification. In the training stage, given a set of labeled element pairs, a function f : X x X → 10, 11 is trained to classify any two elements as to whether they belong to the same class (1) or not (0), independently of other elements. The distance between the two elements is defined by converting the prediction confidence of the pairwise classifier, and clustering is then performed based on this distance function. Particularly, (Li et al., 200</context>
<context position="21652" citStr="McCallum and Wellner, 2003" startWordPosition="3642" endWordPosition="3645"> the algorithm exits and outputs the metric in the iteration with the least error. Figure 3: A general training algorithm for SDC For the weighted clustering error in Equ. 3, and linearly weighted Manhattan distances as in Equ. 5, the update rule in Step 2(b) becomes wtl = wt−1 l − α · [ψt−1 l (p, S) − ψt−1 l (h, S)]. (6) where ψl (p, S) ≡ |2�ES�l (xi, x�) ·I[p(xiS |) = p(xj)] and ψl(h,S) ≡ |1|2 Exi,xj∈S φl (xi ,xj) I[h(xi) = h(xj)], and α &gt; 0 is the learning rate. 5 Entity Identification in Text We conduct experimental study on the task of entity identification in text (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). A given entity – representing a person, a location or an organization – may be mentioned in text in multiple, ambiguous ways. Consider, for example, an open domain question answering system (Voorhees, 2002) that attempts, given a question like: “When was President Kennedy born?” to search a large collection of articles in order to pinpoint the concise answer: “on May 29, 1917.” The sentence, and even the document that contains the answer, may not contain the name “President Kennedy”; it may refer to this entity as “Kennedy”, “JFK” or “John Fitzgerald Kennedy”. Other documen</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mochihashi</author>
<author>G Kikui</author>
<author>K Kita</author>
</authors>
<title>Learning nonstructural distance metric by minimum cluster distortions.</title>
<date>2004</date>
<booktitle>In COLING-04.</booktitle>
<contexts>
<context position="13027" citStr="Mochihashi et al., 2004" startWordPosition="2132" endWordPosition="2135">n and Winnow, and using a collection of relational features between a pair of names. The distance between two names is defined as a softmax over the classifier’s output. As expected, experimental evidence (Cohen et al., 2003; Cohen and Richman, 2002; Li et al., 2004) shows that domain-specific distance functions improve over a fixed metric. This can be explained by the flexibility provided by adapting the metric to the domain as well as the contribution of supervision that guides the adaptation of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) indeed suggest to learn a metric directly in a clustering task but the learning p</context>
</contexts>
<marker>Mochihashi, Kikui, Kita, 2004</marker>
<rawString>D. Mochihashi, G. Kikui, and K. Kita. 2004. Learning nonstructural distance metric by minimum cluster distortions. In COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In KDD-02,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="1966" citStr="Pantel and Lin, 2002" startWordPosition="298" endWordPosition="301">e some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same ent</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In KDD-02, pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In AAAI-98,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="12370" citStr="Roth, 1998" startWordPosition="2025" endWordPosition="2026">ning stage, given a set of labeled element pairs, a function f : X x X → 10, 11 is trained to classify any two elements as to whether they belong to the same class (1) or not (0), independently of other elements. The distance between the two elements is defined by converting the prediction confidence of the pairwise classifier, and clustering is then performed based on this distance function. Particularly, (Li et al., 2004) applied this approach to measuring name similarity in the entity identification problem, where a pairwise classifier (LMR) is trained using the SNoW learning architecture (Roth, 1998) based on variations of Perceptron and Winnow, and using a collection of relational features between a pair of names. The distance between two names is defined as a softmax over the classifier’s output. As expected, experimental evidence (Cohen et al., 2003; Cohen and Richman, 2002; Li et al., 2004) shows that domain-specific distance functions improve over a fixed metric. This can be explained by the flexibility provided by adapting the metric to the domain as well as the contribution of supervision that guides the adaptation of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In AAAI-98, pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schultz</author>
<author>T Joachims</author>
</authors>
<title>Learning a distance metric from relative comparisons.</title>
<date>2004</date>
<booktitle>In NIPS-04.</booktitle>
<contexts>
<context position="13001" citStr="Schultz and Joachims, 2004" startWordPosition="2128" endWordPosition="2131">d on variations of Perceptron and Winnow, and using a collection of relational features between a pair of names. The distance between two names is defined as a softmax over the classifier’s output. As expected, experimental evidence (Cohen et al., 2003; Cohen and Richman, 2002; Li et al., 2004) shows that domain-specific distance functions improve over a fixed metric. This can be explained by the flexibility provided by adapting the metric to the domain as well as the contribution of supervision that guides the adaptation of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) indeed suggest to learn a metric directly in a clusteri</context>
</contexts>
<marker>Schultz, Joachims, 2004</marker>
<rawString>M. Schultz and T. Joachims. 2004. Learning a distance metric from relative comparisons. In NIPS-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC-2002 question answering track.</title>
<date>2002</date>
<booktitle>In TREC-02,</booktitle>
<pages>115--123</pages>
<contexts>
<context position="21878" citStr="Voorhees, 2002" startWordPosition="3683" endWordPosition="3684">date rule in Step 2(b) becomes wtl = wt−1 l − α · [ψt−1 l (p, S) − ψt−1 l (h, S)]. (6) where ψl (p, S) ≡ |2�ES�l (xi, x�) ·I[p(xiS |) = p(xj)] and ψl(h,S) ≡ |1|2 Exi,xj∈S φl (xi ,xj) I[h(xi) = h(xj)], and α &gt; 0 is the learning rate. 5 Entity Identification in Text We conduct experimental study on the task of entity identification in text (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). A given entity – representing a person, a location or an organization – may be mentioned in text in multiple, ambiguous ways. Consider, for example, an open domain question answering system (Voorhees, 2002) that attempts, given a question like: “When was President Kennedy born?” to search a large collection of articles in order to pinpoint the concise answer: “on May 29, 1917.” The sentence, and even the document that contains the answer, may not contain the name “President Kennedy”; it may refer to this entity as “Kennedy”, “JFK” or “John Fitzgerald Kennedy”. Other documents may state that “John F. Kennedy, Jr. was born on November 25, 1960”, but this fact refers to our target entity’s son. Other mentions, such as “Senator Kennedy” or “Mrs. Kennedy” are even “closer” to the writing of the targe</context>
<context position="23267" citStr="Voorhees, 2002" startWordPosition="3910" endWordPosition="3911">esent the same entity. We study this problem for three entity types – People, Location and Organization. Although deciding the coreference of names within the same document might be relatively easy, since within a single document identical mentions typically refer to the same entity, identifying coreference across-document is much harder. With no standard corpora for studying the problem in a general setting – both within and across documents, we created our own corpus. This is done by collecting about 8,600 names from 300 randomly sampled 1998-2000 New York Times articles in the TREC corpus (Voorhees, 2002). These names are first annotated by a named entity tagger, then manually verified and given as input to an entity identifier. Since the number of classes (entities) for names is very large, standard multi-class classification is not feasible. Instead, we compare SDC with several pairwise classification and clustering approaches. Some of them (for example, those based on SoftTFIDF similarity) do not make use of any domain knowledge, while others do exploit supervision, such as LMR and SDC. Other works (Bilenko et al., 2003) also exploited supervision in this problem by discriminative training </context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>E. Voorhees. 2002. Overview of the TREC-2002 question answering track. In TREC-02, pages 115–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
<author>D McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In COLING-04.</booktitle>
<contexts>
<context position="1847" citStr="Weeds et al., 2004" startWordPosition="278" endWordPosition="281"> into non-overlapping classes, based on some distance (or similarity) metric defined between them, in order to provide some level of syntactic or semantic abstraction. A key example is that of class-based language models (Brown et al., 1992; Dagan et al., 1999) where clustering approaches are used in order to partition words, determined to be similar, into sets. This enables estimating more robust statistics since these are computed over collections of “similar” words. A large number of different metrics and algorithms have been experimented with these problems (Dagan et al., 1999; Lee, 1997; Weeds et al., 2004). Similarity between words was also used as a metric in a distributional clustering algorithm in (Pantel and Lin, 2002), and it shows that functionally similar words can be grouped together and even separated to smaller groups based on their senses. At a 64 higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people’s home pages using a TFIDF similarity, and several other researchers have applied clustering at the same level in the context of the entity identification problem (Bilenko et al., 2003; McCallum and Wellner, 2003; Li et al., 2004). Similarly, approache</context>
<context position="3490" citStr="Weeds et al., 2004" startWordPosition="543" endWordPosition="546">tric. For example, the K-Means clustering approach (Hartigan and Wong, 1979) seeks to maximize a measure of tightness of the resulting clusters based on the Euclidean distance. Clustering is typically called an unsupervised method, since data elements are used without labels during the clustering process and labels are not used to provide feedback to the optimization process. E.g., labels are not taken into account when measuring the quality of the partition. However, in many cases, supervision is used at the application level when determining an appropriate distance metric (e.g., (Lee, 1997; Weeds et al., 2004; Bilenko et al., 2003) and more). This scenario, however, has several setbacks. First, the process of clustering, simply a function that partitions a set of elements into different classes, involves no learning and thus lacks flexibility. Second, clustering quality is typically defined with respect to a fixed distance metric, without utilizing any direct supervision, so the practical clustering outcome could be disparate from one’s intention. Third, when clustering with a given algorithm and a fixed metric, one in fact makes some implicit assumptions on the data and the task (e.g., (Kamvar et</context>
<context position="9491" citStr="Weeds et al., 2004" startWordPosition="1568" endWordPosition="1571"> without any supervision, the resulting function is not guaranteed to agree with the target function p (or one’s original intention). Given this realization, there has been some work on selecting a good distance metric for a family of related problems and on learning a metric for specific tasks. For the former, the focus is on developing and selecting good distance (similarity) metrics that reflect well pairwise proximity between domain elements. The “goodness” of a metric is empirically measured when combined with different clustering algorithms on different problems. For example (Lee, 1997; Weeds et al., 2004) compare similarity metrics such as the Cosine, Manhattan and Euclidean distances, Kullback-Leibler divergence, Jensen-Shannon divergence, and Jaccard’s Coefficient, that could be applied in general clustering tasks, on the task of measuring distributional similarity. (Cohen et al., 2003) compares a number of string and token-based similarity metrics on the task of matching entity names and found that, tuuv dθ(x1,x2) ≡ Xm l=1 K X k=1 qS(h) ≡ X xESk 65 overall, the best-performing method is a hybrid scheme (SoftTFIDF) combining a TFIDF weighting scheme of tokens with the Jaro-Winkler string-dis</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>J. Weeds, D. Weir, and D. McCarthy. 2004. Characterising measures of lexical distributional similarity. In COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E P Xing</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
<author>S Russell</author>
</authors>
<title>Distance metric learning, with application to clustering with side-information.</title>
<date>2002</date>
<booktitle>In NIPS-02.</booktitle>
<contexts>
<context position="12948" citStr="Xing et al., 2002" startWordPosition="2120" endWordPosition="2123">SNoW learning architecture (Roth, 1998) based on variations of Perceptron and Winnow, and using a collection of relational features between a pair of names. The distance between two names is defined as a softmax over the classifier’s output. As expected, experimental evidence (Cohen et al., 2003; Cohen and Richman, 2002; Li et al., 2004) shows that domain-specific distance functions improve over a fixed metric. This can be explained by the flexibility provided by adapting the metric to the domain as well as the contribution of supervision that guides the adaptation of the metric. A few works (Xing et al., 2002; Bar-Hillel et al., 2003; Schultz and Joachims, 2004; Mochihashi et al., 2004) outside the NLP domain have also pursued this general direction, and some have tried to learn the metric with limited amount of supervision, no supervision or by incorporating other information sources such as constraints on the class memberships of the data elements. In most of these cases, the algorithm practically used in clustering, (e.g. K-Means), is not considered in the learning procedure, or only implicitly exploited by optimizing the same objective function. (Bach and Jordan, 2003; Bilenko et al., 2004) in</context>
</contexts>
<marker>Xing, Ng, Jordan, Russell, 2002</marker>
<rawString>E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. 2002. Distance metric learning, with application to clustering with side-information. In NIPS-02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>