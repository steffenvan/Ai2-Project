<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001681">
<title confidence="0.607596">
The TALP-UPC phrase-based translation system for EACL-WMT 2009
</title>
<note confidence="0.807127666666667">
José A.R. Fonollosa and Maxim Khalilov and Marta R. Costa-jussà and
José B. Marifio and Carlos A. Henríquez Q. and Adolfo Hernández H. and
Rafael E. Banchs
</note>
<address confidence="0.290261">
TALP Research Center
Universitat Politècnica de Catalunya, Barcelona 08034
</address>
<email confidence="0.998465">
{adrian,khalilov,mruiz,canton,carloshq,adolfohh,rbanchs}@talp.upc.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99973">
This study presents the TALP-UPC sub-
mission to the EACL Fourth Worskhop
on Statistical Machine Translation 2009
evaluation campaign. It outlines the ar-
chitecture and configuration of the 2009
phrase-based statistical machine transla-
tion (SMT) system, putting emphasis on
the major novelty of this year: combina-
tion of SMT systems implementing differ-
ent word reordering algorithms.
Traditionally, we have concentrated on
the Spanish-to-English and English-to-
Spanish News Commentary translation
tasks.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999664512820513">
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Politècnica de Catalunya) is a permanent par-
ticipant of the ACL WMT shared translations
tasks, traditionally concentrating on the Spanish-
to-English and vice versa language pairs. In this
paper, we describe the 2009 system’s architecture
and design describing individual components and
distinguishing features of our model.
This year’s system stands aside from the
previous years’ configurations which were per-
formed following an N-gram-based (tuple-based)
approach to SMT. By contrast to them, this
year we investigate the translation models (TMs)
interpolation for a state-of-the-art phrase-based
translation system. Inspired by the work pre-
sented in (Schwenk and Estève, 2008), we attack
this challenge using the coefficients obtained for
the corresponding monolingual language models
(LMs) for TMs interpolation.
On the second step, we have performed
additional word reordering experiments, com-
paring the results obtained with a statisti-
cal method (R. Costa-jussà and R. Fonollosa,
2009) and syntax-based algorithm (Khalilov and
R. Fonollosa, 2008). Further the outputs of
the systems were combined selecting the trans-
lation with the Minimum Bayes Risk (MBR) al-
gorithm (Kumar, 2004) that allowed significantly
outperforming the baseline configuration.
The remainder of this paper is organized as
follows: Section 2 presents the TALP-UPC’09
phrase-based system, along with the translation
models interpolation procedure and other minor
novelties of this year. Section 3 reports on the ex-
perimental setups and outlines the results of the
participation in the EACL WMT 2009 evaluation
campaign. Section 4 concludes the paper with dis-
cussions.
</bodyText>
<sectionHeader confidence="0.986591" genericHeader="method">
2 TALP-UPC phrase-based SMT
</sectionHeader>
<bodyText confidence="0.94606124">
The system developed for this year’s shared
task is based on a state-of-the-art SMT sys-
tem implemented within the open-source MOSES
toolkit (Koehn et al., 2007). A phrase-based trans-
lation is considered as a three step algorithm:
(1) the source sequence of words is segmented
in phrases, (2) each phrase is translated into tar-
get language using translation table, (3) the target
phrases are reordered to be inherent in the target
language.
A bilingual phrase (which in the context of SMT
do not necessarily coincide with their linguistic
analogies) is any pair of m source words and n
target words that satisfies two basic constraints:
(1) words are consecutive along both sides of the
bilingual phrase and (2) no word on either side of
the phrase is aligned to a word outside the phrase.
Given a sentence pair and a corresponding word-
to-word alignment, phrases are extracted follow-
ing the criterion in (Och and Ney, 2004). The
probability of the phrases is estimated by relative
frequencies of their appearance in the training cor-
pus.
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.999704">
85
</page>
<bodyText confidence="0.999170142857143">
Classically, a phrase-based translation system
implements a log-linear model in which a foreign
language sentence fr1 = f1, f2, ..., f� is trans-
lated into another language ej1 = e1, e2, ..., ej by
searching for the translation hypothesis ej1 maxi-
mizing a log-linear combination of several feature
models (Brown et al., 1990):
</bodyText>
<equation confidence="0.994869">
r M ll
ej1 = arg max { amhm(ej1, fi )1
e1 l m=1 J
</equation>
<bodyText confidence="0.999941333333333">
where the feature functions hm refer to the system
models and the set of Am refers to the weights cor-
responding to these models.
</bodyText>
<subsectionHeader confidence="0.983986">
2.1 Translation models interpolation
</subsectionHeader>
<bodyText confidence="0.999855472222222">
We implemented a TM interpolation strategy fol-
lowing the ideas proposed in (Schwenk and Es-
tève, 2008), where the authors present a promis-
ing technique of target LMs linear interpolation;
in (Koehn and Schroeder, 2007) where a log-linear
combination of TMs is performed; and specifi-
cally in (Foster and Kuhn, 2007) where the authors
present various ways of TM combination and ana-
lyze in detail the TM domain adaptation.
In the framework of the evaluation campaign,
there were two Spanish-to-English parallel train-
ing corpora available: Europarl v.4 corpus (about
50M tokens) and News Commentary (NC) corpus
(about 2M tokens). The test dataset provided by
the organizers this year was from the news do-
main, so we considered the Europarl training cor-
pus as &amp;quot;out-of-domain&amp;quot; data and the News Com-
mentary as &amp;quot;in-domain&amp;quot; training material. Unfor-
tunately, the in-domain corpus is much smaller in
size, however the Europarl corpus can be also used
to increase the final translation and reordering ta-
bles in spite of its different nature.
A straightforward approach to the TM interpo-
lation would be an iterative TM reconstruction ad-
justing scale coefficients on each step of the loop
with use of the highest BLEU score as a maxi-
mization criterion.
However, we did not expect a significant gain
from this time-consumption strategy and we de-
cided to follow a simpler approach. In the pre-
sented results, we obtained the best interpola-
tion weight following the standard entropy-based
optimization of the target-side LM. We adjust
the weight coefficient AEuroparl (ANC = 1 −
AEuroparl) of the linear interpolation of the target-
side LMs:
</bodyText>
<equation confidence="0.997198">
P(w) = AEuroparl &apos; PEuroparl + ANC &apos; PNC (1)
</equation>
<bodyText confidence="0.999893393939394">
where PEuroparl and PKC are probabilities as-
signed to the word sequence w by the LM esti-
mated on Europarl and NC data, respectively.
The scale factor values are automatically opti-
mized to obtain the lowest perplexity ppl(w) pro-
duced by the interpolated LM P(w). We used the
standard script compute − best − mix from the
SRI LM package (Stolcke, 2002) for optimization.
On the next step, the optimized coefficients
AEuroparl and ANC are generalized on the interpo-
lated translation and reordering models. In other
words, reordering and translation models are in-
terpolated using the same weights which yield the
lowest perplexity for LM interpolation.
The word-to-word alignment was obtained from
the joint (merged) database (Europarl + NC).
Then, we separately computed the translation and
reordering tables corresponding to the in- and out-
of-domain parts of the joint alignment. The final
tables, as well as the final target LM were obtained
using linear interpolation. The weights were se-
lected using a minimum perplexity criterion esti-
mated on the corresponding interpolated combina-
tion of the target-side LMs.
The optimized coefficient values are: for Span-
ish: NC weight = 0.526, Europarl weight = 0.474;
for English: NC weight = 0.503, Europarl weight
= 0.497. The perplexity results obtained using
monolingual LMs and the 2009 development set
(English and Spanish references) can be found in
Table 1, while the corresponding improvement in
BLEU score is presented in Section 3.3 and sum-
mary of the obtained results (Table 4).
</bodyText>
<table confidence="0.995782">
Europarl NC Interpolated
English 463.439 489.915 353.305
Spanish 308.802 347.092 246.573
</table>
<tableCaption confidence="0.836307">
Table 1: Perplexity results obtained on the Dev
2009 corpus and the monolingual LMs.
</tableCaption>
<bodyText confidence="0.999269">
Note that the corresponding reordering models
are interpolated with the same weights.
</bodyText>
<subsectionHeader confidence="0.997216">
2.2 Statistical Machine Reordering
</subsectionHeader>
<bodyText confidence="0.99997">
The idea of the Statistical Machine Reordering
(SMR) stems from the idea of using the power-
ful techniques developed for SMT and to translate
</bodyText>
<page confidence="0.984498">
86
</page>
<bodyText confidence="0.9998965">
the source language (S) into a reordered source
language (S’), which more closely matches the
order of the target language. To infer more re-
orderings, it makes use of word classes. To cor-
rectly integrate the SMT and SMR systems, both
are concatenated by using a word graph which of-
fers weighted reordering hypotheses to the SMT
system. The details are described in (?).
</bodyText>
<subsectionHeader confidence="0.99696">
2.3 Syntax-based Reordering
</subsectionHeader>
<bodyText confidence="0.999874">
Syntax-based Reordering (SBR) approach deals
with the word reordering problem and is based on
non-isomorphic parse subtree transfer as described
in details in (Khalilov and R. Fonollosa, 2008).
Local and long-range word reorderings are
driven by automatically extracted permutation pat-
terns operating with source language constituents.
Once the reordering patterns are extracted, they
are further applied to monotonize the bilingual
corpus in the same way as shown in the previ-
ous subsection. The target-side parse tree is con-
sidered as a filter constraining reordering rules to
the set of patterns covered both by the source- and
target-side subtrees.
</bodyText>
<subsectionHeader confidence="0.99325">
2.4 System Combination
</subsectionHeader>
<bodyText confidence="0.995884538461538">
Over the past few years the MBR algorithm uti-
lization to find the best consensus outputs of dif-
ferent translation systems has proved to improve
the translation accuracy (Kumar, 2004). The sys-
tem combination is performed on the 200-best
lists which are generated by the three systems:
(1) MOSES-based system without pre-translation
monotonization (baseline), (2) MOSES-based
SMT enhanced with SMR monotonization and (3)
MOSES-based SMT augmented with SBR mono-
tonization. The results presented in Table 4 show
that the combined output significantly outperforms
the baseline system configuration.
</bodyText>
<sectionHeader confidence="0.997149" genericHeader="evaluation">
3 Experiments and results
</sectionHeader>
<bodyText confidence="0.999780444444445">
We followed the evaluation baseline instructions 1
to train the MOSES-based translation system.
In some experiments we used MBR decod-
ing (Kumar and Byrne, 2004) with the smoothed
BLEU score as a similarity criteria, that al-
lowed gaining 0.2 BLEU points comparing to the
standard procedure of outputting the translation
with the highest probability (HP). We applied the
Moses implementation of this algorithm to the list
</bodyText>
<footnote confidence="0.951869">
1http://www.statmt.org/wmt09/baseline.html
</footnote>
<table confidence="0.8741365">
of 200 best translations generated by the TALP-
UPC system. The results obtained over the official
2009 Test dataset can be found in Table 2.
Task HP MBR
EsEn 24.48 24.62
EnEs 23.46 23.64
</table>
<tableCaption confidence="0.99933">
Table 2: MBR versus MERT decoding.
</tableCaption>
<bodyText confidence="0.999914">
The &amp;quot;recase&amp;quot; script provided within the base-
line was supplemented with and additional mod-
ule, which restore the original case for unknown
words (many of them are proper names and loos-
ing of case information leads to a significant per-
formance degradation).
</bodyText>
<subsectionHeader confidence="0.998702">
3.1 Language models
</subsectionHeader>
<bodyText confidence="0.9831995">
The target-side language models were estimated
using the SRILM toolkit (Stolcke, 2002). We tried
to use all the available in-domain training mate-
rial: apart from the corresponding portions of the
bilingual NC corpora we involved the following
monolingual corpora:
</bodyText>
<listItem confidence="0.9906449">
• News monolingual corpus (49M tokens for
English and 49M for Spanish)
• Europarl monolingual corpus (about 504M
tokens for English and 463M for Spanish)
• A collection of News development and test
sets from previous evaluations (151K tokens
for English and 175K for Spanish)
• A collection of Europarl development and
test sets from previous evaluations (295K to-
kens for English and 311K for Spanish)
</listItem>
<bodyText confidence="0.9999354">
Five LMs per language were estimated on the
corresponding datasets and interpolated follow-
ing the maximum perplexity criteria. Hence, the
larger LMs incorporating in- and out-of-domain
data were used in decoding.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Spanish enclitics separation
</subsectionHeader>
<bodyText confidence="0.999953">
For the Spanish portion of the corpus we imple-
mented an enclitics separation procedure on the
preprocessing step, i.e. the pronouns attached to
the verb were separated and contractions as del
or al were splitted into de el or a el. Conse-
quently, training data sparseness due to Spanish
morphology was reduced improving the perfor-
mance of the overall translation system. As a
</bodyText>
<page confidence="0.998338">
87
</page>
<table confidence="0.915425166666667">
post-processing, the segmentation was recovered • Baseline+TMI+MBR: the same as the latter
in the English-to-Spanish direction using target- but with MBR decoding,
side Part-of-Speech tags (de Gispert, 2006).
Task Bleu CI Bleu CS NIST CI NIST CS
EsEn 25.93 24.54 7.275 7.017
EnEs 24.85 23.37 6.963 6.689
</table>
<subsectionHeader confidence="0.935254">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9647102">
The automatic scores provided by the WMT’09
organizers for TALP-UPC submissions calculated
over the News 2009 dataset can be found in Ta-
ble 3. BLEU and NIST case-insensitive (CI) and
case-sensitive (CS) metrics are considered.
</bodyText>
<tableCaption confidence="0.752197333333333">
Table 3: BLEU and NIST scores for preliminary
official test dataset 2009 (primary submission)
with 500 sentences excluded.
</tableCaption>
<bodyText confidence="0.999444333333333">
The TALP-UPC primary submission was
ranked the 3rd among 28 presented translations
for the Spanish-to-English task and the 4th for the
English-to-Spanish task among 9 systems.
The following system configurations and the in-
ternal results obtained are reported:
</bodyText>
<listItem confidence="0.945837882352941">
• Baseline: Moses-based SMT, as proposed
on the web-page of the evaluation campaign
with Spanish enclitics separation and modi-
fied version of “recase“ tool,
• Baseline+TMI: Baseline enhanced with TM
interpolation as described in subsection 2.1,
• Baseline+TMI+SMR: the same as Base-
line+TMI but with SMR technique applied to
monotonize the source portion of the corpus,
as described in subsection 2.2,
• Baseline+SBR: the same as Baseline but with
SBR algorithm applied to monotonize the
source portion of the corpus, as described in
subsection 2.3,
• System Combination: a combined output of
the 3 previous systems done with the MBR
algorithm, as described in subsection 2.4.
</listItem>
<bodyText confidence="0.9986679375">
Impact of TM interpolation and MBR decod-
ing is more significant for the English-to-Spanish
translation task, for which the target-side mono-
lingual corpus is smaller than for the Spanish-to-
English translation.
We did not have time to meet the evalua-
tion deadline for providing the system combi-
nation output. Nevertheless, during the post-
evaluation period we performed the experiments
reported in the last three lines of Table 4 (Base-
line+TMI+SMR, Baseline+SBR and System com-
bination).
Note that the results presented in Table 4 differ
from the ones which can be found the Table 3 due
to selective conditions of preliminary evaluation
done by the Shared Task organizers.
</bodyText>
<table confidence="0.998154533333333">
System News 2009 Test CI News 2009 Test CS
Spanish-to-English
Baseline 25.82 24.37
Baseline+TMI 25.84 24.47
Baseline+TMI+MBR (Primary) 26.04 24.62
Baseline+SMR 24.95 23.62
Baseline+SBR 24.24 22.89
System combination 26.44 25.00
English-to-Spanish
Baseline 24.56 23.05
Baseline+TMI 25.01 23.41
Baseline+TMI+MBR (Primary) 25.16 23.64
Baseline+SMR 24.09 22.65
Baseline+SBR 23.52 22.05
System combination 25.39 23.86
</table>
<tableCaption confidence="0.999754">
Table 4: Experiments summary.
</tableCaption>
<page confidence="0.99878">
88
</page>
<sectionHeader confidence="0.999753" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999980384615385">
In this paper, we present the TALP-UPC phrase-
based translation system developed for the EACL-
WMT 2009 evaluation campaign. The major nov-
elties of this year are translation models interpola-
tion done in linear way and combination of SMT
systems implementing different word reordering
algorithms. The system was ranked pretty well for
both translation tasks in which our institution has
participated.
Unfortunately, the promising reordering tech-
niques and the combination of their outputs were
not applied within the evaluation deadline, how-
ever we report the obtained results in the paper.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.995391666666667">
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928226415094">
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J.D. Lafferty, R. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79–
85.
A. de Gispert. 2006. Introducing linguistic knowledge
into Statistical Machine Translation. Ph.D. thesis,
Universitat Politècnica de Catalunya, December.
G. Foster and R. Kuhn. 2007. Mixture-model adap-
tation for SMT. In In Annual Meeting of the Asso-
ciation for Computational Linguistics: Proc. of the
Second Workshop on Statistical Machine Transla-
tion (WMT), pages 128–135, Prague, Czech Repub-
lic, June.
M. Khalilov and J. R. Fonollosa. 2008. A new subtree-
transfer approach to syntax-based reordering for sta-
tistical machine translation. Technical report, Uni-
versitat Politècnica de Catalunya.
Ph. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In In Annual Meeting of the Association for Compu-
tational Linguistics: Proc. of the Second Workshop
on Statistical Machine Translation (WMT), pages
224–227, Prague, Czech Republic, June.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL) 2007, pages 177–180.
Sh. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In In
HLTNAACL’04, pages 169–176.
Sh. Kumar. 2004. Minimum Bayes-Risk Techniques in
Automatic Speech Recognition and Statistical Ma-
chine Translation. Ph.D. thesis, Johns Hopkins Uni-
versity.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 3(4):417–449, December.
M. R. Costa-jussà and J. R. Fonollosa. 2009. An
Ngram reordering model. Computer Speech and
Language. ISSN 0885-2308, accepted for publica-
tion.
H. Schwenk and Y. Estève. 2008. Data selection and
smoothing in an open-source system for the 2008
nist machine translation evaluation. In Proceedings
of the Interspeech’08, pages 2727–2730, Brisbane,
Australia, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of the Int. Conf.
on Spoken Language Processing, pages 901–904.
</reference>
<page confidence="0.999752">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237705">
<title confidence="0.724991">The TALP-UPC phrase-based translation system for EACL-WMT 2009</title>
<author confidence="0.739105">A R Fonollosa Khalilov R Costa-jussà B Marifio A Henríquez Q Hernández H Rafael E</author>
<affiliation confidence="0.9963815">TALP Research Universitat Politècnica de Catalunya, Barcelona</affiliation>
<email confidence="0.999827">adrian@talp.upc.edu</email>
<email confidence="0.999827">khalilov@talp.upc.edu</email>
<email confidence="0.999827">mruiz@talp.upc.edu</email>
<email confidence="0.999827">canton@talp.upc.edu</email>
<email confidence="0.999827">carloshq@talp.upc.edu</email>
<email confidence="0.999827">adolfohh@talp.upc.edu</email>
<email confidence="0.999827">rbanchs@talp.upc.edu</email>
<abstract confidence="0.9422788">This study presents the TALP-UPC submission to the EACL Fourth Worskhop on Statistical Machine Translation 2009 evaluation campaign. It outlines the architecture and configuration of the 2009 phrase-based statistical machine translation (SMT) system, putting emphasis on the major novelty of this year: combination of SMT systems implementing different word reordering algorithms. Traditionally, we have concentrated on the Spanish-to-English and English-to- Commentary tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>85</pages>
<contexts>
<context position="4185" citStr="Brown et al., 1990" startWordPosition="622" endWordPosition="625">4). The probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence fr1 = f1, f2, ..., f� is translated into another language ej1 = e1, e2, ..., ej by searching for the translation hypothesis ej1 maximizing a log-linear combination of several feature models (Brown et al., 1990): r M ll ej1 = arg max { amhm(ej1, fi )1 e1 l m=1 J where the feature functions hm refer to the system models and the set of Am refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J.D. Lafferty, R. Mercer, and P.S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79– 85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A de Gispert</author>
</authors>
<title>Introducing linguistic knowledge into Statistical Machine Translation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Politècnica de Catalunya,</institution>
<marker>de Gispert, 2006</marker>
<rawString>A. de Gispert. 2006. Introducing linguistic knowledge into Statistical Machine Translation. Ph.D. thesis, Universitat Politècnica de Catalunya, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Mixture-model adaptation for SMT. In</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics: Proc. of the Second Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4716" citStr="Foster and Kuhn, 2007" startWordPosition="716" endWordPosition="719">esis ej1 maximizing a log-linear combination of several feature models (Brown et al., 1990): r M ll ej1 = arg max { amhm(ej1, fi )1 e1 l m=1 J where the feature functions hm refer to the system models and the set of Am refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &amp;quot;out-of-domain&amp;quot; data and the News Commentary as &amp;quot;in-domain&amp;quot; training material. Unfortunately, the in-domain corpus is much smaller in size, however the Europar</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>G. Foster and R. Kuhn. 2007. Mixture-model adaptation for SMT. In In Annual Meeting of the Association for Computational Linguistics: Proc. of the Second Workshop on Statistical Machine Translation (WMT), pages 128–135, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Khalilov</author>
<author>J R Fonollosa</author>
</authors>
<title>A new subtreetransfer approach to syntax-based reordering for statistical machine translation.</title>
<date>2008</date>
<tech>Technical report, Universitat Politècnica de Catalunya.</tech>
<marker>Khalilov, Fonollosa, 2008</marker>
<rawString>M. Khalilov and J. R. Fonollosa. 2008. A new subtreetransfer approach to syntax-based reordering for statistical machine translation. Technical report, Universitat Politècnica de Catalunya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
<author>J Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation. In</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics: Proc. of the Second Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>224--227</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4620" citStr="Koehn and Schroeder, 2007" startWordPosition="700" endWordPosition="703">f� is translated into another language ej1 = e1, e2, ..., ej by searching for the translation hypothesis ej1 maximizing a log-linear combination of several feature models (Brown et al., 1990): r M ll ej1 = arg max { amhm(ej1, fi )1 e1 l m=1 J where the feature functions hm refer to the system models and the set of Am refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news domain, so we considered the Europarl training corpus as &amp;quot;out-of-domain&amp;quot; data and the News Commentary as &amp;quot;in-domain&amp;quot; tra</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Ph. Koehn and J. Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In In Annual Meeting of the Association for Computational Linguistics: Proc. of the Second Workshop on Statistical Machine Translation (WMT), pages 224–227, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang Koehn</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open-source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL)</booktitle>
<pages>177--180</pages>
<contexts>
<context position="2806" citStr="Koehn et al., 2007" startWordPosition="395" endWordPosition="398"> outperforming the baseline configuration. The remainder of this paper is organized as follows: Section 2 presents the TALP-UPC’09 phrase-based system, along with the translation models interpolation procedure and other minor novelties of this year. Section 3 reports on the experimental setups and outlines the results of the participation in the EACL WMT 2009 evaluation campaign. Section 4 concludes the paper with discussions. 2 TALP-UPC phrase-based SMT The system developed for this year’s shared task is based on a state-of-the-art SMT system implemented within the open-source MOSES toolkit (Koehn et al., 2007). A phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented in phrases, (2) each phrase is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned t</context>
</contexts>
<marker>Koehn, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open-source toolkit for statistical machine translation. In Proceedings of the Association for Computational Linguistics (ACL) 2007, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation. In</title>
<date>2004</date>
<booktitle>In HLTNAACL’04,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="9893" citStr="Kumar and Byrne, 2004" startWordPosition="1536" endWordPosition="1539">n accuracy (Kumar, 2004). The system combination is performed on the 200-best lists which are generated by the three systems: (1) MOSES-based system without pre-translation monotonization (baseline), (2) MOSES-based SMT enhanced with SMR monotonization and (3) MOSES-based SMT augmented with SBR monotonization. The results presented in Table 4 show that the combined output significantly outperforms the baseline system configuration. 3 Experiments and results We followed the evaluation baseline instructions 1 to train the MOSES-based translation system. In some experiments we used MBR decoding (Kumar and Byrne, 2004) with the smoothed BLEU score as a similarity criteria, that allowed gaining 0.2 BLEU points comparing to the standard procedure of outputting the translation with the highest probability (HP). We applied the Moses implementation of this algorithm to the list 1http://www.statmt.org/wmt09/baseline.html of 200 best translations generated by the TALPUPC system. The results obtained over the official 2009 Test dataset can be found in Table 2. Task HP MBR EsEn 24.48 24.62 EnEs 23.46 23.64 Table 2: MBR versus MERT decoding. The &amp;quot;recase&amp;quot; script provided within the baseline was supplemented with and a</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Sh. Kumar and W. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In In HLTNAACL’04, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumar</author>
</authors>
<title>Minimum Bayes-Risk Techniques in Automatic Speech Recognition and Statistical Machine Translation.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="2160" citStr="Kumar, 2004" startWordPosition="300" endWordPosition="301">-of-the-art phrase-based translation system. Inspired by the work presented in (Schwenk and Estève, 2008), we attack this challenge using the coefficients obtained for the corresponding monolingual language models (LMs) for TMs interpolation. On the second step, we have performed additional word reordering experiments, comparing the results obtained with a statistical method (R. Costa-jussà and R. Fonollosa, 2009) and syntax-based algorithm (Khalilov and R. Fonollosa, 2008). Further the outputs of the systems were combined selecting the translation with the Minimum Bayes Risk (MBR) algorithm (Kumar, 2004) that allowed significantly outperforming the baseline configuration. The remainder of this paper is organized as follows: Section 2 presents the TALP-UPC’09 phrase-based system, along with the translation models interpolation procedure and other minor novelties of this year. Section 3 reports on the experimental setups and outlines the results of the participation in the EACL WMT 2009 evaluation campaign. Section 4 concludes the paper with discussions. 2 TALP-UPC phrase-based SMT The system developed for this year’s shared task is based on a state-of-the-art SMT system implemented within the </context>
<context position="9295" citStr="Kumar, 2004" startWordPosition="1452" endWordPosition="1453">automatically extracted permutation patterns operating with source language constituents. Once the reordering patterns are extracted, they are further applied to monotonize the bilingual corpus in the same way as shown in the previous subsection. The target-side parse tree is considered as a filter constraining reordering rules to the set of patterns covered both by the source- and target-side subtrees. 2.4 System Combination Over the past few years the MBR algorithm utilization to find the best consensus outputs of different translation systems has proved to improve the translation accuracy (Kumar, 2004). The system combination is performed on the 200-best lists which are generated by the three systems: (1) MOSES-based system without pre-translation monotonization (baseline), (2) MOSES-based SMT enhanced with SMR monotonization and (3) MOSES-based SMT augmented with SBR monotonization. The results presented in Table 4 show that the combined output significantly outperforms the baseline system configuration. 3 Experiments and results We followed the evaluation baseline instructions 1 to train the MOSES-based translation system. In some experiments we used MBR decoding (Kumar and Byrne, 2004) w</context>
</contexts>
<marker>Kumar, 2004</marker>
<rawString>Sh. Kumar. 2004. Minimum Bayes-Risk Techniques in Automatic Speech Recognition and Statistical Machine Translation. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="3568" citStr="Och and Ney, 2004" startWordPosition="525" endWordPosition="528">is translated into target language using translation table, (3) the target phrases are reordered to be inherent in the target language. A bilingual phrase (which in the context of SMT do not necessarily coincide with their linguistic analogies) is any pair of m source words and n target words that satisfies two basic constraints: (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. Given a sentence pair and a corresponding wordto-word alignment, phrases are extracted following the criterion in (Och and Ney, 2004). The probability of the phrases is estimated by relative frequencies of their appearance in the training corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85–89, Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics 85 Classically, a phrase-based translation system implements a log-linear model in which a foreign language sentence fr1 = f1, f2, ..., f� is translated into another language ej1 = e1, e2, ..., ej by searching for the translation hypothesis ej1 maximizing a log-linear combination of several feature models (Br</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 3(4):417–449, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-jussà</author>
<author>J R Fonollosa</author>
</authors>
<title>An Ngram reordering model. Computer Speech and Language. ISSN 0885-2308, accepted for publication.</title>
<date>2009</date>
<marker>Costa-jussà, Fonollosa, 2009</marker>
<rawString>M. R. Costa-jussà and J. R. Fonollosa. 2009. An Ngram reordering model. Computer Speech and Language. ISSN 0885-2308, accepted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>Y Estève</author>
</authors>
<title>Data selection and smoothing in an open-source system for the 2008 nist machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Interspeech’08,</booktitle>
<pages>2727--2730</pages>
<location>Brisbane, Australia,</location>
<contexts>
<context position="1653" citStr="Schwenk and Estève, 2008" startWordPosition="223" endWordPosition="226">of the ACL WMT shared translations tasks, traditionally concentrating on the Spanishto-English and vice versa language pairs. In this paper, we describe the 2009 system’s architecture and design describing individual components and distinguishing features of our model. This year’s system stands aside from the previous years’ configurations which were performed following an N-gram-based (tuple-based) approach to SMT. By contrast to them, this year we investigate the translation models (TMs) interpolation for a state-of-the-art phrase-based translation system. Inspired by the work presented in (Schwenk and Estève, 2008), we attack this challenge using the coefficients obtained for the corresponding monolingual language models (LMs) for TMs interpolation. On the second step, we have performed additional word reordering experiments, comparing the results obtained with a statistical method (R. Costa-jussà and R. Fonollosa, 2009) and syntax-based algorithm (Khalilov and R. Fonollosa, 2008). Further the outputs of the systems were combined selecting the translation with the Minimum Bayes Risk (MBR) algorithm (Kumar, 2004) that allowed significantly outperforming the baseline configuration. The remainder of this p</context>
<context position="4504" citStr="Schwenk and Estève, 2008" startWordPosition="681" endWordPosition="685">ase-based translation system implements a log-linear model in which a foreign language sentence fr1 = f1, f2, ..., f� is translated into another language ej1 = e1, e2, ..., ej by searching for the translation hypothesis ej1 maximizing a log-linear combination of several feature models (Brown et al., 1990): r M ll ej1 = arg max { amhm(ej1, fi )1 e1 l m=1 J where the feature functions hm refer to the system models and the set of Am refers to the weights corresponding to these models. 2.1 Translation models interpolation We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estève, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In the framework of the evaluation campaign, there were two Spanish-to-English parallel training corpora available: Europarl v.4 corpus (about 50M tokens) and News Commentary (NC) corpus (about 2M tokens). The test dataset provided by the organizers this year was from the news doma</context>
</contexts>
<marker>Schwenk, Estève, 2008</marker>
<rawString>H. Schwenk and Y. Estève. 2008. Data selection and smoothing in an open-source system for the 2008 nist machine translation evaluation. In Proceedings of the Interspeech’08, pages 2727–2730, Brisbane, Australia, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Int. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="6420" citStr="Stolcke, 2002" startWordPosition="1004" endWordPosition="1005">the best interpolation weight following the standard entropy-based optimization of the target-side LM. We adjust the weight coefficient AEuroparl (ANC = 1 − AEuroparl) of the linear interpolation of the targetside LMs: P(w) = AEuroparl &apos; PEuroparl + ANC &apos; PNC (1) where PEuroparl and PKC are probabilities assigned to the word sequence w by the LM estimated on Europarl and NC data, respectively. The scale factor values are automatically optimized to obtain the lowest perplexity ppl(w) produced by the interpolated LM P(w). We used the standard script compute − best − mix from the SRI LM package (Stolcke, 2002) for optimization. On the next step, the optimized coefficients AEuroparl and ANC are generalized on the interpolated translation and reordering models. In other words, reordering and translation models are interpolated using the same weights which yield the lowest perplexity for LM interpolation. The word-to-word alignment was obtained from the joint (merged) database (Europarl + NC). Then, we separately computed the translation and reordering tables corresponding to the in- and outof-domain parts of the joint alignment. The final tables, as well as the final target LM were obtained using lin</context>
<context position="10779" citStr="Stolcke, 2002" startWordPosition="1677" endWordPosition="1678">statmt.org/wmt09/baseline.html of 200 best translations generated by the TALPUPC system. The results obtained over the official 2009 Test dataset can be found in Table 2. Task HP MBR EsEn 24.48 24.62 EnEs 23.46 23.64 Table 2: MBR versus MERT decoding. The &amp;quot;recase&amp;quot; script provided within the baseline was supplemented with and additional module, which restore the original case for unknown words (many of them are proper names and loosing of case information leads to a significant performance degradation). 3.1 Language models The target-side language models were estimated using the SRILM toolkit (Stolcke, 2002). We tried to use all the available in-domain training material: apart from the corresponding portions of the bilingual NC corpora we involved the following monolingual corpora: • News monolingual corpus (49M tokens for English and 49M for Spanish) • Europarl monolingual corpus (about 504M tokens for English and 463M for Spanish) • A collection of News development and test sets from previous evaluations (151K tokens for English and 175K for Spanish) • A collection of Europarl development and test sets from previous evaluations (295K tokens for English and 311K for Spanish) Five LMs per languag</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proceedings of the Int. Conf. on Spoken Language Processing, pages 901–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>