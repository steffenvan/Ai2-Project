<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001062">
<note confidence="0.81614">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 37-42, Lisbon, Portugal, 2000.
</note>
<title confidence="0.9989335">
Incorporating Position Information into a Maximum
Entropy/Minimum Divergence Translation Model
</title>
<author confidence="0.831465">
George Foster
</author>
<bodyText confidence="0.4458615">
RALI, Universite de Montréal
fosterOiro.umontreal.ca
</bodyText>
<sectionHeader confidence="0.978239" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962125">
I describe two methods for incorporating infor-
mation about the relative positions of bilingual
word pairs into a Maximum Entropy/Minimum
Divergence translation model. The better of the
two achieves over 40% lower test corpus perplex-
ity than an equivalent combination of a trigram
language model and the classical IBM transla-
tion model 2.
</bodyText>
<sectionHeader confidence="0.99869" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991987285714286">
Statistical Machine Translation (SMT) systems
use a model of p(t Is), the probability that a text
s in the source language will translate into a text
t in the target language, to determine the best
translation for a given source text. A straight-
forward way of modeling this distribution is to
apply a chain-rule expansion of the form:
</bodyText>
<equation confidence="0.878389">
It&apos;
p(tIs) = (1)
i=1
</equation>
<bodyText confidence="0.997673525">
where ti denotes the ith token in t.1 The objects
to be modeled in this case belong to the family
of conditional distributions p(wIhi, s), the prob-
ability of the ith word in t, given the tokens
which precede it and the source text.
The main motivation for modeling p(t1s) in
terms of p(wIhi, s) is that it simplifies the &amp;quot;de-
coding&amp;quot; problem of finding the most likely tar-
get text. In particular, if hi is known, finding
the best word at the current position requires
only a straightforward search through the target
&apos;This ignores the issue of normalization over target
texts of all possible lengths, which can be easily enforced
when desired by using a stop token or a prior distribution
over lengths.
vocabulary, and efficient dynamic-programming
based heuristics can be used to extend this to
sequences of words. This is very important for
applications such as TransType (Foster et al.,
1997; Langlais et al., 2000), where the task is
to make real-time predictions of the text a hu-
man translator will type next, based on the
source text under translation and some prefix
of the target text that has already been typed.
The standard &amp;quot;noisy channel&amp;quot; approach used in
SMT, where p(t1s) cx p(t)p(slt), is generally too
expensive for such applications because it does
not permit direct calculation of the probabil-
ity of a word or sequence of words beginning at
the current position. Complex and expensive
search strategies are required to find the best
target text in this approach (Garcia-Varea et
al., 1998; Niessen et al., 1998; Och et al., 1999;
Wang and Waibel, 1998).
The challenge in modeling p(u) &apos;hi, s) is to
combine two disparate sources of conditioning
information in an effective way. One obvious
strategy is to use a linear combination of sep-
arate language and translation components, of
the form:
</bodyText>
<equation confidence="0.997837">
P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2)
</equation>
<bodyText confidence="0.999771384615385">
where p(wihi) is a language model, p(wii,$) is
a translation model, and A e [0, 1] is a com-
bining weight. However, this appears to be
a weak technique (Langlais and Foster, 2000),
even when A is allowed to depend on various
features of the context (hi, s).
In previous work (Foster, 2000), I de-
scribed a Maximum Entropy/Minimum Diver-
gence (MEMD) model (Berger et al., 1996)
for s) which incorporates a trigram lan-
guage model and a translation component which
is an analog of the well-known IBM transla-
tion model 1 (Brown et al., 1993). This model
</bodyText>
<page confidence="0.998892">
37
</page>
<bodyText confidence="0.999883153846154">
significantly outperforms an equivalent linear
combination of a trigram and model 1 in test-
corpus perplexity, despite using several orders of
magnitude fewer translation parameters. Like
model 1, its translation component is based only
on the occurrences in s of words which are po-
tential translations for w, and does not take
into account the positions of these words rel-
ative to w. An obvious enhancement is to in-
corporate such positional information into the
MEMD model, thereby making its translation
component analogous to the IBM model 2. This
is the problem I address in this paper.
</bodyText>
<sectionHeader confidence="0.987603" genericHeader="introduction">
2 Models
</sectionHeader>
<subsectionHeader confidence="0.962558">
2.1 Linear Model
</subsectionHeader>
<bodyText confidence="0.999711666666667">
As a baseline for comparison I used a linear com-
bination as in (2) of a standard interpolated tri-
gram language model and the IBM translation
model 2 (IBM2), with the combining weight A
optimized using the EM algorithm. IBM2 is de-
rived as follows:2
</bodyText>
<equation confidence="0.9915335">
s) = p(w,
j=0
Ep(wimp(jli,i)
3=0
</equation>
<bodyText confidence="0.99989825">
where 1 = Is&apos;, and the hidden variable j gives
the position in s of the (single) source token si
assumed to give rise to w, or 0 if there is none.
The model consists of a set of word-pair param-
eters p(tis) and position parameters p(j1i,1); in
model 1 (IBM1) the latter are fixed at 1/(l+1),
as each position, including the empty position
0, is considered equally likely to contain a trans-
lation for w. Maximum likelihood estimates for
these parameters can be obtained with the EM
algorithm over a bilingual training corpus, as
described in (Brown et al., 1993).
</bodyText>
<subsectionHeader confidence="0.911031">
2.2 MEMD Model 1
</subsectionHeader>
<bodyText confidence="0.9982645">
A MEMD model for p(w lhi, s) has the general
form:
</bodyText>
<equation confidence="0.97712">
q(wihi, exp(6 • f (w, h, s))
s) =
z(hi, s)
</equation>
<footnote confidence="0.9915274">
2Model 2 was originally formulated for p(t1s), but
since target words are predicted independently it can
also be used for p(wIh„ s). The only necessary modifica-
tion in this case is that the position parameters can no
longer be conditioned on itl.
</footnote>
<bodyText confidence="0.986123685714286">
where , s) is a reference distribution,
f(w, hi, s) maps (w, h, s) into an n-dimensional
feature vector, d is a corresponding vector of
feature weights (the parameters of the model),
and Z(hi, s) = Ew q(w1h, , s) exp(d f(w, hi)) is
a normalizing factor. For a given choice of q
and f, the ITS algorithm (Berger et al., 1996)
can be used to find maximum likelihood values
for the parameters It can be shown (Della
Pietra et al., 1995) that these are the also the
values which minimize the Kullback-Liebler di-
vergence D(pliq) between the model and the
reference distribution under the constraint that
the expectations of the features (ie, the compo-
nents off) with respect to the model must equal
their expectations with respect to the empirical
distribution derived from the training corpus.
Thus the reference distribution serves as a kind
of prior, and should reflect some initial knowl-
edge about the true distribution; and the use
of any feature is justified to the extent that its
empirical expectation is accurate.
In the present context, the natural choice for
the reference distribution q is a trigram lan-
guage model. To create a MEMD analog to
IBM model 1 (MEMD1), I used boolean fea-
tures corresponding to bilingual word pairs:
ist(w, s) =
{ 1, s E S and t = w
0, else
where (s, t) is a (source,target) word pair. Using
the notational convention that ast is 0 whenever
the corresponding feature fst does not exist in
the model, MEMD1 can be written compactly
as:
</bodyText>
<equation confidence="0.943616">
P(wihi, s) = q(wihi)exp(E asii,)/Z(hi,$).
sEs
</equation>
<bodyText confidence="0.999544833333333">
Due to the theoretical properties of MEMD
outlined above, it is necessary to select a sub-
set of all possible features ht to avoid overfitting
the training corpus. Using a reduced feature set
is also computationally advantageous, since the
time taken to calculate the normalization con-
stant Z(hi, s) grows linearly with the expected
number of features which are active per source
word s E s. This is in contrast to IBM1, where
use of all available word-pair parameters p(tis)
is standard, and engenders only a very slight
overfitting effect. In (Foster, 2000) I describe an
</bodyText>
<page confidence="0.995325">
38
</page>
<bodyText confidence="0.994282">
effective technique for selecting MEMD word-
pair features.
</bodyText>
<subsectionHeader confidence="0.892456">
2.3 MEMD Model 2
</subsectionHeader>
<bodyText confidence="0.999944666666667">
IBM2 incorporates position information by in-
troducing a hidden position variable and mak-
ing independence hypotheses. This approach is
not applicable to MEMD models, whose fea-
tures must capture events which are directly
observable in the training corpus.3 It would be
possible to use pure position features of the form
fui, which capture the presence of any word
pair at position (i, j, 1) and are superficially sim-
ilar to IBM2&apos;s position parameters, but these
would add almost no information to MEMD1.
On the other hand, features like fstui, indicat-
ing the presence of a specific pair (s, t) at posi-
tion (i, j, /), would cause severe data sparseness
problems.
</bodyText>
<subsectionHeader confidence="0.929693">
Encoding Positions as Feature Values
</subsectionHeader>
<bodyText confidence="0.929266285714286">
A simple solution to this dilemma is to let the
value of a word-pair feature reflect the current
position of the pair rather just its presence or
absence. A reasonable choice for this is the
value of the corresponding IBM2 position pa-
rameter p(jli,1):
ht(w,i,$) s E s and t = w
else
where j, is the position of s in s, or the most
likely position according to IBM2 if it occurs
more than once: j, = argmax3:53=8P(iii,/). Us-
ing the same convention as in the previous sec-
tion, the resulting model (MEMD2R) can be
written:
</bodyText>
<equation confidence="0.752705666666667">
exp(Eses aswP(is 1))
=
Z(hi, s)
</equation>
<bodyText confidence="0.999494857142857">
MEMD2R is simple and compact but poses a
technical difficulty due to its use of real-valued
features, in that the ITS training algorithm re-
quires integer or boolean features for efficient
implemention. Since likelihood is a concave
function of 5, any hillclimbing method such as
gradient ascent4 is guaranteed to find maximum
</bodyText>
<footnote confidence="0.934159666666667">
3Although it is possible to extend the basic framework
to allow for embedded Hidden Markov Models (Lafferty,
1995).
41 found that the &amp;quot;stochastic&amp;quot; variant of this algo-
rithm, in which model parameters are updated after each
training example, gave the best performance.
</footnote>
<bodyText confidence="0.999571333333333">
likelihood parameter values, but convergence is
slower than ITS and requires tuning a gradient
step parameter. Unfortunately, apart from this
problem, MEMD2R also turns out to perform
slightly worse than MEMD1, as described be-
low.
</bodyText>
<subsectionHeader confidence="0.915843">
Using Class-based Position Features
</subsectionHeader>
<bodyText confidence="0.999957787878788">
Since the basic problem with incorporating po-
sition information is one of insufficient data, a
natural solution is to try to group word pair and
position combinations with similar behaviour
into classes such that the frequency of each
class in the training corpus is high enough for
reliable estimation. To do this, I made two
preliminary assumptions: 1) word pairs with
similar MEMD1 weights should be grouped to-
gether; and 2) position configurations with sim-
ilar IBM2 probabilities should be grouped to-
gether. This converts the problem from one
of finding classes in the five-dimensional space
(s,t,i, j, 1) to one of identifying rectangular ar-
eas on a 2-dimensional grid where one axis con-
tains position configurations (i, j, 1), ordered by
p(jii,1); and the other contains word pairs (s, t),
ordered by ast. To simplify further, I parti-
tioned both axes so as to approximately bal-
ance the total corpus frequency of all word pairs
or position configurations within each parti-
tion. Thus the only parameters required to com-
pletely specify a classification are the number of
position and word-pair partitions. Each combi-
nation of a position partition and a word pair
partition corresponds to a class, and all classes
can be expected to have roughly the same em-
pirical counts.
The model (MEMD2B) based on this scheme
has one feature for each class; if A designates the
set of triples (i, j, 1) in a position partition and
B designates the set of pairs (s, t) in a word-pair
partition, then for all A, B there is a feature:
</bodyText>
<equation confidence="0.833465333333333">
fA,B(w,i,$) =i=i (51(i, j,l) E A A
(sj, w)EB A
j=isjI,
</equation>
<bodyText confidence="0.999932333333333">
where 8[X] is 1 when X is true and 0 other-
wise. For robustness, I used these position fea-
tures along with pure MEMD1-style word-pair
features ht. The weights 01A,B on the position
features can thus be interpreted as correction
terms for the pure word-pair weights ces,t which
</bodyText>
<page confidence="0.99903">
39
</page>
<table confidence="0.9988602">
segment file pairs sentence pairs English tokens French tokens
train 922 1,639,250 29,547,936 31,826,112
held-out 1 30 54,758 978,394 1,082,350
held-out 2 30 59,435 1,111,454 1,241,581
test 30 53,676 984,809 1,103,320
</table>
<tableCaption confidence="0.999631">
Table 1: Corpus segmentation. The train segment was the main training corpus; the held-out 1
</tableCaption>
<bodyText confidence="0.977104511627907">
segment was used for combining weights for the trigram and the overall linear model; and the
held-out 2 segment was used for the MEMD2B partition search.
p(TS)1/I7-I, where p is the model being eval-
uated, and (8, T) is the test corpus, considered
to be a set of statistically independent sentence
aft(i,j3,/),B(s,t))pairs (s, t). Perplexity is a good indicator of
Performance for the TransType application de-
scribed in the introduction, and it has also been
used in the evaluation of full-fledged SMT sys-
tems (Al-Onaizan et al., 1999). To ensure a fair
comparison, all models used the same target vo-
cabulary. For all MEMD models, I used 20,000
word-pair features selected using the method
described in (Foster, 2000); this is suboptimal
but gives reasonably good performance and fa-
cilitates experimentation.
reflect the proximity of the words in the pair.
The model is:
where A(i, js, /) gives the partition for the cur-
rent position, B(s, t) gives the partition for the
current word pair, and following the usual con-
vention, a A(i, ,i),B(s,t) is zero if these are unde-
fined.
To find the optimal number of position par-
titions m and word-pair partitions n, I per-
formed a greedy search, beginning at a small ini-
tial point (m, n) and at each iteration training
two MEMD2B models characterized by (km, n)
and (m, kn), where k &gt; 1 is a scaling factor
(note that both these models contain kmn po-
sition features). The model which gives the
best performance on a validation corpus is used
as the starting point for the next iteration.
Since training MEMD models is very expen-
sive, to speed up the search I relaxed the con-
vergence criterion from a training corpus per-
plexity5 drop of &lt; .1% (requiring 20-30 ITS it-
erations) to &lt; .6% (requiring approximately 10
IIS iterations). I stopped the search when the
best model&apos;s performance on the validation cor-
pus did not decrease significantly from that of
the model at the previous step, indicating that
overtraining was beginning to occur.
</bodyText>
<sectionHeader confidence="0.999753" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999966428571429">
I tested the models on the Canadian Hansard
corpus, with English as the source language
and French as the target language. After sen-
tence alignment using the method described
in (Simard et al., 1992), the corpus was split
into disjoint segments as shown in table 1.
To evaluate performance, I used perplexity:
</bodyText>
<footnote confidence="0.787419">
5Defined in the next section
</footnote>
<bodyText confidence="0.999732407407408">
Figures 1 and 2 show, respectively, the path
taken by the MEMD2B partition search, and
the validation corpus perplexities of each model
tested during the search. As shown in figure 1,
the search consisted of 6 iterations. Since on all
previous iterations no increase in position parti-
tions beyond the initial value of 10 was selected,
on the 5th iteration I tried decreasing the num-
ber of position partitions to 5. This model was
not selected either, so on the final step only the
number of word-pair partitions was augmented,
yielding an optimal combination of 10 position
partitions and 4000 word-pair partitions.
Table 2 gives the final results for all mod-
els. The IBM models tested here incorporate
a reduced set of 1M word-pair parameters, se-
lected using the method described in (Foster,
2000), which gives slightly better test-corpus
performance than the unrestricted set of all 35M
word pairs which cooccur within aligned sen-
tence pairs in the training corpus.
The basic MEMD1 model (without position
parameters) attains about 30% lower perplex-
ity than the model 2 baseline, and MEMD2B
with an optimal-sized set of position param-
eters achieves in a further drop of over 10%.
Interestingly, the difference between IBM1 and
</bodyText>
<equation confidence="0.764758">
P(TD I s) = q(wihi)exP(EsEs asw +
Z(hi,$)
</equation>
<page confidence="0.989072">
40
</page>
<table confidence="0.997750777777778">
model word-pair position perplexity improvement
parameters parameters over baseline
trigram 0 0 61.0
trigram + IBM1 1,000,000 0 43.2
trigram + IBM2 1,000,000 115,568 35.2 0%
MEMD1 20,000 0 24.5 30.4%
MEMD2R 20,000 0 28.4 19.3%
MEMD2B 20,000 10 x 10 22.1 37.2%
MEMD2B 20,000 10 x 4000 20.2 42.6%
</table>
<tableCaption confidence="0.988422666666667">
Table 2: Model performances. Linear interpolation is designated with a + sign; and the MEMD2B
position parameters are given as mx , where m and n are the numbers of position partitions and
word-pair partitions respectively.
</tableCaption>
<figure confidence="0.999408894736842">
5 10 20 50
number of position partitions
4000
2000
1000
500
250
50
10
validation corpus perplexity
1000
4000
50 250 500
2000
word-pair dasses
10 position classes -4---
20 position dasses
50 position classes -5--
5 position dasses _
</figure>
<figureCaption confidence="0.8840455">
Figure 1: MEMD2B partition search path, be-
ginning at the point (10, 10). Arrows out of each
point show the configurations tested at each it-
eration.
</figureCaption>
<bodyText confidence="0.94349275">
IBM2&apos;s performance (18.5% lower perplexity for
IBM2) is about the same as the difference be-
tween MEMD1 and MEMD2B (17.6% lower for
MEMD2B).
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.940978821428571">
This paper deals with the problem of incorpo-
rating information about the positions of bilin-
gual word pairs into a MEMD model which is
analogous to the classical IBM model 1, thereby
creating a MEMD analog to the IBM model 2. I
proposed and evaluated two methods for accom-
plishing this: using IBM2 position parameter
probabilities as MEMD feature values, which
was unsuccessful; and adding features which
Figure 2: Validation corpus perplexities for var-
ious MEMD2B models. Each connected line in
this graph corresponds to a vertical column of
search points in figure 1.
capture the occurrence of a word-pair with a
MEMD1 weight that falls into a specific range
of values at a position to which IBM2 assigns
a probability in a certain range. The second
model achieved over 40% lower test perplex-
ity than a linear combination of a trigram and
IBM2, despite using several orders of magnitude
fewer parameters.
This work represents a novel approach to
translation modeling which is most appropriate
for applications like TransType which need to
make rapid predictions of upcoming text. How-
ever, it is not inconceivable that it could also
be used for full-fledged MT. One partial impedi-
ment to this is that the MEMD framework lacks
</bodyText>
<page confidence="0.998033">
41
</page>
<bodyText confidence="0.999521">
a mechanism equivalant to the EM algorithm
for estimating probabilities associated with hid-
den variables. The solution I have proposed
here can be seen as a first step to investigat-
ing ways of getting around this problem.
</bodyText>
<sectionHeader confidence="0.989962" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997505">
This work was carried out as part of the
TransType project at RALI, funded by the Nat-
ural Sciences and Engineering Research Council
of Canada.
</bodyText>
<sectionHeader confidence="0.997669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99902304">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation:
Final report, JHU workshop 1999. Technical
report, The Center for Language and Speech
Processing, The Johns Hopkins University,
www.clsp.jhu.edu/ws99/projects/mt/final_report.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy
approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39-71.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of Machine Translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263-312, June.
S. Della Pietra, V. Della Pietra, and J. Lafferty.
1995. Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, CMU.
George Foster, Pierre Isabelle, and Pierre Plamon-
don. 1997. Target-text Mediated Interactive Ma-
chine Translation. Machine Translation, 12:175-
194.
George Foster. 2000. A Maximum Entropy / Min-
imum Divergence translation model. In Proceed-
ings of the 38th Annual Meeting of the Association
for Computational Linguistics (ACL-38), Hong
Kong, October.
Ismael Garcia-Varea, Francisco Casacuberta, and
Hermann Ney. 1998. An iterative, DP-based
search algorithm for statistical machine trans-
lation. In Proceedings of the 5th International
Conference on Spoken Language Processing (IC-
SLP) 1998, Sydney, Australia, December. pages
1135-1138.
John D. Lafferty. 1995. Gibbs-markov models. In
Computing Science and Statistics: Proceedings of
the 27th Symposium on the Interface. Interface
Foundation.
Ph. Langlais and G. Foster. 2000. Using context-
dependent interpolation to combine statistical
language and translation models for interactive
MT. In Content-Based Multimedia Information
Access (RIA 0), Paris, France, April.
Ph. Langlais, G. Foster, and G. Lapalme. 2000. Unit
completion for a computer-aided translation typ-
ing system. In Proceedings of the 5th Conference
on Applied Natural Language Processing (ANLP-
5), Seattle, Washington, May.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann.
1998. A DP based search algorithm for statistical
machine translation. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL) and 17th International
Conference on Computational Linguistics (COL-
ING) 1998, pages 960-967, Montréal, Canada,
August.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proceedings of the
4nd Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), College Park,
Maryland.
Michel Simard, George F. Foster, and Pierre Is-
abelle. 1992. Using cognates to align sentences in
bilingual corpora. In Proceedings of the 4th Con-
ference on Theoretical and Methodological Is-
sues in Machine Translation (TMI), Montréal,
Québec.
Ye-yi Wang and Alex Waibel. 1998. Fast decod-
ing for statistical machine translation. In Proceed-
ings of the 5th International Conference on Spo-
ken Language Processing (ICSLP) 1998, Sydney,
Australia, December, pages 2775-2778.
</reference>
<page confidence="0.999295">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788238">
<note confidence="0.978285">of CoNLL-2000 and LLL-2000, 37-42, Lisbon, Portugal, 2000.</note>
<title confidence="0.9695175">Incorporating Position Information into a Entropy/Minimum Divergence Translation Model</title>
<author confidence="0.995661">George Foster</author>
<email confidence="0.966871">defosterOiro.umontreal.ca</email>
<abstract confidence="0.990332777777778">I describe two methods for incorporating information about the relative positions of bilingual word pairs into a Maximum Entropy/Minimum Divergence translation model. The better of the achieves over test corpus perplexity than an equivalent combination of a trigram language model and the classical IBM translation model 2.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<date>1999</date>
<booktitle>Statistical machine translation: Final report, JHU workshop</booktitle>
<tech>Technical report,</tech>
<institution>The Center for Language and Speech Processing, The Johns Hopkins University,</institution>
<contexts>
<context position="12156" citStr="Al-Onaizan et al., 1999" startWordPosition="2061" endWordPosition="2064">le 1: Corpus segmentation. The train segment was the main training corpus; the held-out 1 segment was used for combining weights for the trigram and the overall linear model; and the held-out 2 segment was used for the MEMD2B partition search. p(TS)1/I7-I, where p is the model being evaluated, and (8, T) is the test corpus, considered to be a set of statistically independent sentence aft(i,j3,/),B(s,t))pairs (s, t). Perplexity is a good indicator of Performance for the TransType application described in the introduction, and it has also been used in the evaluation of full-fledged SMT systems (Al-Onaizan et al., 1999). To ensure a fair comparison, all models used the same target vocabulary. For all MEMD models, I used 20,000 word-pair features selected using the method described in (Foster, 2000); this is suboptimal but gives reasonably good performance and facilitates experimentation. reflect the proximity of the words in the pair. The model is: where A(i, js, /) gives the partition for the current position, B(s, t) gives the partition for the current word pair, and following the usual convention, a A(i, ,i),B(s,t) is zero if these are undefined. To find the optimal number of position partitions m and wor</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report, JHU workshop 1999. Technical report, The Center for Language and Speech Processing, The Johns Hopkins University, www.clsp.jhu.edu/ws99/projects/mt/final_report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="3186" citStr="Berger et al., 1996" startWordPosition="525" endWordPosition="528"> s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model 37 significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters. Like model 1, its translation component is based only on the occurrences in s of words which are potential translations for w, and does not take into account the positions of these words relative to w. An obvious enhancement is to incorporat</context>
<context position="5504" citStr="Berger et al., 1996" startWordPosition="941" endWordPosition="944">form: q(wihi, exp(6 • f (w, h, s)) s) = z(hi, s) 2Model 2 was originally formulated for p(t1s), but since target words are predicted independently it can also be used for p(wIh„ s). The only necessary modification in this case is that the position parameters can no longer be conditioned on itl. where , s) is a reference distribution, f(w, hi, s) maps (w, h, s) into an n-dimensional feature vector, d is a corresponding vector of feature weights (the parameters of the model), and Z(hi, s) = Ew q(w1h, , s) exp(d f(w, hi)) is a normalizing factor. For a given choice of q and f, the ITS algorithm (Berger et al., 1996) can be used to find maximum likelihood values for the parameters It can be shown (Della Pietra et al., 1995) that these are the also the values which minimize the Kullback-Liebler divergence D(pliq) between the model and the reference distribution under the constraint that the expectations of the features (ie, the components off) with respect to the model must equal their expectations with respect to the empirical distribution derived from the training corpus. Thus the reference distribution serves as a kind of prior, and should reflect some initial knowledge about the true distribution; and </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy approach to Natural Language Processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<date>1993</date>
<contexts>
<context position="3347" citStr="Brown et al., 1993" startWordPosition="554" endWordPosition="557">and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model 37 significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters. Like model 1, its translation component is based only on the occurrences in s of words which are potential translations for w, and does not take into account the positions of these words relative to w. An obvious enhancement is to incorporate such positional information into the MEMD model, thereby making its translation component analogous to the IBM model 2. This is the problem I address in this p</context>
<context position="4820" citStr="Brown et al., 1993" startWordPosition="811" endWordPosition="814">2 is derived as follows:2 s) = p(w, j=0 Ep(wimp(jli,i) 3=0 where 1 = Is&apos;, and the hidden variable j gives the position in s of the (single) source token si assumed to give rise to w, or 0 if there is none. The model consists of a set of word-pair parameters p(tis) and position parameters p(j1i,1); in model 1 (IBM1) the latter are fixed at 1/(l+1), as each position, including the empty position 0, is considered equally likely to contain a translation for w. Maximum likelihood estimates for these parameters can be obtained with the EM algorithm over a bilingual training corpus, as described in (Brown et al., 1993). 2.2 MEMD Model 1 A MEMD model for p(w lhi, s) has the general form: q(wihi, exp(6 • f (w, h, s)) s) = z(hi, s) 2Model 2 was originally formulated for p(t1s), but since target words are predicted independently it can also be used for p(wIh„ s). The only necessary modification in this case is that the position parameters can no longer be conditioned on itl. where , s) is a reference distribution, f(w, hi, s) maps (w, h, s) into an n-dimensional feature vector, d is a corresponding vector of feature weights (the parameters of the model), and Z(hi, s) = Ew q(w1h, , s) exp(d f(w, hi)) is a normal</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993.</rawString>
</citation>
<citation valid="true">
<title>The mathematics of Machine Translation: Parameter estimation.</title>
<date></date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<marker></marker>
<rawString>The mathematics of Machine Translation: Parameter estimation. Computational Linguistics, 19(2):263-312, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144, CMU.</tech>
<contexts>
<context position="5613" citStr="Pietra et al., 1995" startWordPosition="961" endWordPosition="964">get words are predicted independently it can also be used for p(wIh„ s). The only necessary modification in this case is that the position parameters can no longer be conditioned on itl. where , s) is a reference distribution, f(w, hi, s) maps (w, h, s) into an n-dimensional feature vector, d is a corresponding vector of feature weights (the parameters of the model), and Z(hi, s) = Ew q(w1h, , s) exp(d f(w, hi)) is a normalizing factor. For a given choice of q and f, the ITS algorithm (Berger et al., 1996) can be used to find maximum likelihood values for the parameters It can be shown (Della Pietra et al., 1995) that these are the also the values which minimize the Kullback-Liebler divergence D(pliq) between the model and the reference distribution under the constraint that the expectations of the features (ie, the components off) with respect to the model must equal their expectations with respect to the empirical distribution derived from the training corpus. Thus the reference distribution serves as a kind of prior, and should reflect some initial knowledge about the true distribution; and the use of any feature is justified to the extent that its empirical expectation is accurate. In the present </context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Pierre Plamondon</author>
</authors>
<date>1997</date>
<booktitle>Target-text Mediated Interactive Machine Translation. Machine Translation,</booktitle>
<pages>12--175</pages>
<contexts>
<context position="1853" citStr="Foster et al., 1997" startWordPosition="296" endWordPosition="299">n terms of p(wIhi, s) is that it simplifies the &amp;quot;decoding&amp;quot; problem of finding the most likely target text. In particular, if hi is known, finding the best word at the current position requires only a straightforward search through the target &apos;This ignores the issue of normalization over target texts of all possible lengths, which can be easily enforced when desired by using a stop token or a prior distribution over lengths. vocabulary, and efficient dynamic-programming based heuristics can be used to extend this to sequences of words. This is very important for applications such as TransType (Foster et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The standard &amp;quot;noisy channel&amp;quot; approach used in SMT, where p(t1s) cx p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probability of a word or sequence of words beginning at the current position. Complex and expensive search strategies are required to find the best target text in this approach (Garcia-Varea </context>
</contexts>
<marker>Foster, Isabelle, Plamondon, 1997</marker>
<rawString>George Foster, Pierre Isabelle, and Pierre Plamondon. 1997. Target-text Mediated Interactive Machine Translation. Machine Translation, 12:175-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>A Maximum Entropy / Minimum Divergence translation model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-38),</booktitle>
<location>Hong Kong,</location>
<contexts>
<context position="2991" citStr="Foster, 2000" startWordPosition="493" endWordPosition="494">s are required to find the best target text in this approach (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998). The challenge in modeling p(u) &apos;hi, s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model 37 significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters. Like model 1, its translation component is bas</context>
<context position="7253" citStr="Foster, 2000" startWordPosition="1240" endWordPosition="1241">ctly as: P(wihi, s) = q(wihi)exp(E asii,)/Z(hi,$). sEs Due to the theoretical properties of MEMD outlined above, it is necessary to select a subset of all possible features ht to avoid overfitting the training corpus. Using a reduced feature set is also computationally advantageous, since the time taken to calculate the normalization constant Z(hi, s) grows linearly with the expected number of features which are active per source word s E s. This is in contrast to IBM1, where use of all available word-pair parameters p(tis) is standard, and engenders only a very slight overfitting effect. In (Foster, 2000) I describe an 38 effective technique for selecting MEMD wordpair features. 2.3 MEMD Model 2 IBM2 incorporates position information by introducing a hidden position variable and making independence hypotheses. This approach is not applicable to MEMD models, whose features must capture events which are directly observable in the training corpus.3 It would be possible to use pure position features of the form fui, which capture the presence of any word pair at position (i, j, 1) and are superficially similar to IBM2&apos;s position parameters, but these would add almost no information to MEMD1. On th</context>
<context position="12338" citStr="Foster, 2000" startWordPosition="2093" endWordPosition="2094"> segment was used for the MEMD2B partition search. p(TS)1/I7-I, where p is the model being evaluated, and (8, T) is the test corpus, considered to be a set of statistically independent sentence aft(i,j3,/),B(s,t))pairs (s, t). Perplexity is a good indicator of Performance for the TransType application described in the introduction, and it has also been used in the evaluation of full-fledged SMT systems (Al-Onaizan et al., 1999). To ensure a fair comparison, all models used the same target vocabulary. For all MEMD models, I used 20,000 word-pair features selected using the method described in (Foster, 2000); this is suboptimal but gives reasonably good performance and facilitates experimentation. reflect the proximity of the words in the pair. The model is: where A(i, js, /) gives the partition for the current position, B(s, t) gives the partition for the current word pair, and following the usual convention, a A(i, ,i),B(s,t) is zero if these are undefined. To find the optimal number of position partitions m and word-pair partitions n, I performed a greedy search, beginning at a small initial point (m, n) and at each iteration training two MEMD2B models characterized by (km, n) and (m, kn), whe</context>
<context position="14745" citStr="Foster, 2000" startWordPosition="2507" endWordPosition="2508">he search consisted of 6 iterations. Since on all previous iterations no increase in position partitions beyond the initial value of 10 was selected, on the 5th iteration I tried decreasing the number of position partitions to 5. This model was not selected either, so on the final step only the number of word-pair partitions was augmented, yielding an optimal combination of 10 position partitions and 4000 word-pair partitions. Table 2 gives the final results for all models. The IBM models tested here incorporate a reduced set of 1M word-pair parameters, selected using the method described in (Foster, 2000), which gives slightly better test-corpus performance than the unrestricted set of all 35M word pairs which cooccur within aligned sentence pairs in the training corpus. The basic MEMD1 model (without position parameters) attains about 30% lower perplexity than the model 2 baseline, and MEMD2B with an optimal-sized set of position parameters achieves in a further drop of over 10%. Interestingly, the difference between IBM1 and P(TD I s) = q(wihi)exP(EsEs asw + Z(hi,$) 40 model word-pair position perplexity improvement parameters parameters over baseline trigram 0 0 61.0 trigram + IBM1 1,000,00</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. A Maximum Entropy / Minimum Divergence translation model. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-38), Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismael Garcia-Varea</author>
<author>Francisco Casacuberta</author>
<author>Hermann Ney</author>
</authors>
<title>An iterative, DP-based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP)</booktitle>
<pages>1135--1138</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2465" citStr="Garcia-Varea et al., 1998" startWordPosition="399" endWordPosition="402"> et al., 1997; Langlais et al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The standard &amp;quot;noisy channel&amp;quot; approach used in SMT, where p(t1s) cx p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probability of a word or sequence of words beginning at the current position. Complex and expensive search strategies are required to find the best target text in this approach (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998). The challenge in modeling p(u) &apos;hi, s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi,</context>
</contexts>
<marker>Garcia-Varea, Casacuberta, Ney, 1998</marker>
<rawString>Ismael Garcia-Varea, Francisco Casacuberta, and Hermann Ney. 1998. An iterative, DP-based search algorithm for statistical machine translation. In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP) 1998, Sydney, Australia, December. pages 1135-1138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
</authors>
<title>Gibbs-markov models.</title>
<date>1995</date>
<booktitle>In Computing Science and Statistics: Proceedings of the 27th Symposium on the Interface. Interface Foundation.</booktitle>
<contexts>
<context position="9035" citStr="Lafferty, 1995" startWordPosition="1545" endWordPosition="1546"> more than once: j, = argmax3:53=8P(iii,/). Using the same convention as in the previous section, the resulting model (MEMD2R) can be written: exp(Eses aswP(is 1)) = Z(hi, s) MEMD2R is simple and compact but poses a technical difficulty due to its use of real-valued features, in that the ITS training algorithm requires integer or boolean features for efficient implemention. Since likelihood is a concave function of 5, any hillclimbing method such as gradient ascent4 is guaranteed to find maximum 3Although it is possible to extend the basic framework to allow for embedded Hidden Markov Models (Lafferty, 1995). 41 found that the &amp;quot;stochastic&amp;quot; variant of this algorithm, in which model parameters are updated after each training example, gave the best performance. likelihood parameter values, but convergence is slower than ITS and requires tuning a gradient step parameter. Unfortunately, apart from this problem, MEMD2R also turns out to perform slightly worse than MEMD1, as described below. Using Class-based Position Features Since the basic problem with incorporating position information is one of insufficient data, a natural solution is to try to group word pair and position combinations with similar</context>
</contexts>
<marker>Lafferty, 1995</marker>
<rawString>John D. Lafferty. 1995. Gibbs-markov models. In Computing Science and Statistics: Proceedings of the 27th Symposium on the Interface. Interface Foundation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Langlais</author>
<author>G Foster</author>
</authors>
<title>Using contextdependent interpolation to combine statistical language and translation models for interactive MT.</title>
<date>2000</date>
<booktitle>In Content-Based Multimedia Information Access (RIA 0),</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="2991" citStr="Langlais and Foster, 2000" startWordPosition="491" endWordPosition="494">rch strategies are required to find the best target text in this approach (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998). The challenge in modeling p(u) &apos;hi, s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entropy/Minimum Divergence (MEMD) model (Berger et al., 1996) for s) which incorporates a trigram language model and a translation component which is an analog of the well-known IBM translation model 1 (Brown et al., 1993). This model 37 significantly outperforms an equivalent linear combination of a trigram and model 1 in testcorpus perplexity, despite using several orders of magnitude fewer translation parameters. Like model 1, its translation component is bas</context>
</contexts>
<marker>Langlais, Foster, 2000</marker>
<rawString>Ph. Langlais and G. Foster. 2000. Using contextdependent interpolation to combine statistical language and translation models for interactive MT. In Content-Based Multimedia Information Access (RIA 0), Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster Langlais</author>
<author>G Lapalme</author>
</authors>
<title>Unit completion for a computer-aided translation typing system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP5),</booktitle>
<location>Seattle, Washington,</location>
<marker>Langlais, Lapalme, 2000</marker>
<rawString>Ph. Langlais, G. Foster, and G. Lapalme. 2000. Unit completion for a computer-aided translation typing system. In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP5), Seattle, Washington, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niessen</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>A DP based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING)</booktitle>
<pages>960--967</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="2487" citStr="Niessen et al., 1998" startWordPosition="403" endWordPosition="406">al., 2000), where the task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The standard &amp;quot;noisy channel&amp;quot; approach used in SMT, where p(t1s) cx p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probability of a word or sequence of words beginning at the current position. Complex and expensive search strategies are required to find the best target text in this approach (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998). The challenge in modeling p(u) &apos;hi, s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work </context>
</contexts>
<marker>Niessen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998. A DP based search algorithm for statistical machine translation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING) 1998, pages 960-967, Montréal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>College Park, Maryland.</location>
<contexts>
<context position="2505" citStr="Och et al., 1999" startWordPosition="407" endWordPosition="410">task is to make real-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The standard &amp;quot;noisy channel&amp;quot; approach used in SMT, where p(t1s) cx p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probability of a word or sequence of words beginning at the current position. Complex and expensive search strategies are required to find the best target text in this approach (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998). The challenge in modeling p(u) &apos;hi, s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I </context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP), College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>George F Foster</author>
<author>Pierre Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Montréal, Québec.</location>
<contexts>
<context position="13807" citStr="Simard et al., 1992" startWordPosition="2348" endWordPosition="2351">nsive, to speed up the search I relaxed the convergence criterion from a training corpus perplexity5 drop of &lt; .1% (requiring 20-30 ITS iterations) to &lt; .6% (requiring approximately 10 IIS iterations). I stopped the search when the best model&apos;s performance on the validation corpus did not decrease significantly from that of the model at the previous step, indicating that overtraining was beginning to occur. 3 Results I tested the models on the Canadian Hansard corpus, with English as the source language and French as the target language. After sentence alignment using the method described in (Simard et al., 1992), the corpus was split into disjoint segments as shown in table 1. To evaluate performance, I used perplexity: 5Defined in the next section Figures 1 and 2 show, respectively, the path taken by the MEMD2B partition search, and the validation corpus perplexities of each model tested during the search. As shown in figure 1, the search consisted of 6 iterations. Since on all previous iterations no increase in position partitions beyond the initial value of 10 was selected, on the 5th iteration I tried decreasing the number of position partitions to 5. This model was not selected either, so on the</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Michel Simard, George F. Foster, and Pierre Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of the 4th Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Montréal, Québec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Fast decoding for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP)</booktitle>
<pages>2775--2778</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2529" citStr="Wang and Waibel, 1998" startWordPosition="411" endWordPosition="414">al-time predictions of the text a human translator will type next, based on the source text under translation and some prefix of the target text that has already been typed. The standard &amp;quot;noisy channel&amp;quot; approach used in SMT, where p(t1s) cx p(t)p(slt), is generally too expensive for such applications because it does not permit direct calculation of the probability of a word or sequence of words beginning at the current position. Complex and expensive search strategies are required to find the best target text in this approach (Garcia-Varea et al., 1998; Niessen et al., 1998; Och et al., 1999; Wang and Waibel, 1998). The challenge in modeling p(u) &apos;hi, s) is to combine two disparate sources of conditioning information in an effective way. One obvious strategy is to use a linear combination of separate language and translation components, of the form: P(wIhi,$) = AP(wIhi) + (1 - A)P(wii,$). (2) where p(wihi) is a language model, p(wii,$) is a translation model, and A e [0, 1] is a combining weight. However, this appears to be a weak technique (Langlais and Foster, 2000), even when A is allowed to depend on various features of the context (hi, s). In previous work (Foster, 2000), I described a Maximum Entr</context>
</contexts>
<marker>Wang, Waibel, 1998</marker>
<rawString>Ye-yi Wang and Alex Waibel. 1998. Fast decoding for statistical machine translation. In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP) 1998, Sydney, Australia, December, pages 2775-2778.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>