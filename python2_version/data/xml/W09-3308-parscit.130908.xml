<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013954">
<title confidence="0.9970205">
Acquiring High Quality Non-Expert Knowledge from
On-demand Workforce
</title>
<author confidence="0.909821">
Donghui Feng Sveva Besana Remi Zajac
</author>
<affiliation confidence="0.870186">
AT&amp;T Interactive Research
</affiliation>
<address confidence="0.864906">
Glendale, CA, 91203
</address>
<email confidence="0.995633">
{dfeng, sbesana, rzajac}@attinteractive.com
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998981705882353">
Being expensive and time consuming, human
knowledge acquisition has consistently been a
major bottleneck for solving real problems. In
this paper, we present a practical framework
for acquiring high quality non-expert knowl-
edge from on-demand workforce using Ama-
zon Mechanical Turk (MTurk). We show how
to apply this framework to collect large-scale
human knowledge on AOL query classifica-
tion in a fast and efficient fashion. Based on
extensive experiments and analysis, we dem-
onstrate how to detect low-quality labels from
massive data sets and their impact on collect-
ing high-quality knowledge. Our experimental
findings also provide insight into the best
practices on balancing cost and data quality for
using MTurk.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999778295081967">
Human knowledge acquisition is critical for
training intelligent systems to solve real prob-
lems, both for industry applications and aca-
demic research. For example, many machine
learning and natural language processing tasks
require non-trivial human labeled data for super-
vised learning-based approaches. Traditionally
this has been collected from domain experts,
which we refer to as expert knowledge.
However, acquiring in-house expert knowl-
edge is usually very expensive, time consuming,
and has consistently been a major bottleneck for
many research problems. For example, tremen-
dous efforts have been put into creating TREC
corpora (Voorhees, 2003).
As a result, several research projects spon-
sored by NSF and DARPA aim to construct
valuable data resources via human labeling; these
are exemplified by PennTree Bank (Marcus et
al., 1993), FrameNet (Baker et al., 1998), and
OntoNotes (Hovy et al., 2006).
In addition, there are projects such as Open
Mind Common Sense (OMCS) (Stork, 1999;
Singh et al., 2002), ISI LEARNER (Chklovski,
2003), and the Fact Entry Tool by Cycorp (Be-
lasco et al., 2002) where knowledge is gathered
from volunteers.
One interesting approach followed by von
Ahn and Dabbish (2004), applied to image label-
ing on the Web, is to collect valuable input from
entertained labelers. Turning label acquisition
into a computer game addresses tediousness,
which is one of the main reasons that it is hard to
gather large quantities of data from volunteers.
More recently researchers have begun to ex-
plore approaches for acquiring human knowl-
edge from an on-demand workforce such as
Amazon Mechanical Turk1. MTurk is a market-
place for jobs that require human intelligence.
There has been an increase in demand for
crowdsourcing prompted by both the academic
community and industry needs. For instance,
Microsoft/Powerset uses MTurk for search rele-
vance evaluation and other companies are lever-
aging turkers to clean their data sources.
However, while it is cheap and fast to obtain
large-scale non-expert labels using MTurk, it is
still unclear how to leverage its capability more
efficiently and economically to obtain sufficient
useful and high-quality data for solving real
problems.
In this paper, we present a practical frame-
work for acquiring high quality non-expert
knowledge using MTurk. As a case study we
have applied this framework to obtain human
classifications on AOL queries (determining
whether a query might be a local search or not).
Based on extensive experiments and analysis, we
show how to detect bad labelers/labels from
massive data sets and how to build high-quality
labeling sets. Our experiments also provide in-
</bodyText>
<footnote confidence="0.888045">
1 Amazon Mechanical Turk: http://www.mturk.com/
</footnote>
<page confidence="0.979653">
51
</page>
<note confidence="0.983843">
Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 51–56,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999879444444444">
sight into the best practices for balancing cost
and data quality when using MTurk.
The remainder of this paper is organized as
follows: In Section 2, we review related work
using MTurk. We describe our methodology in
Section 3 and in Section 4 we present our ex-
perimental results and further analysis. In Sec-
tion 5 we draw conclusions and discuss our plans
for future work.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99998087804878">
It is either infeasible or very time and cost con-
suming to acquire in-house expert human knowl-
edge. To obtain valuable human knowledge (e.g.,
in the format of labeled data), many research
projects in the natural language community have
been funded to create large-scale corpora and
knowledge bases, such as PenTreeBank (Marcus
et al., 1993), FrameNet (Baker et al., 1998),
PropBank (Palmer et al., 2005), and OntoNotes
(Hovy et al., 2006).
MTurk has been attracting much attention
within several research areas since its release. Su
et al. (2007) use MTurk to collect large-scale
review data. Kaisser and Lowe (2008) report
their work on generating research collections of
question-answering pairs using MTurk. Sorokin
and Forsyth (2008) outsource image-labeling
tasks to MTurk. Kittur et al. (2008) use MTurk
as the paradigm for user studies. In the natural
language community Snow et al. (2008) report
their work on collecting linguistic annotation for
a variety of natural language tasks including
word sense disambiguation, word similarity, and
textual entailment recognition.
However, most of the reported work focuses
on how to apply data collected from MTurk to
their applications. In our work, we concentrate
on presenting a practical framework for using
MTurk by separating the process into a valida-
tion phase and a large-scale submission phase.
By analyzing workers’ behavior and their data
quality, we investigate how to detect low-quality
labels and their impact on collected human
knowledge; in addition, during the validation
step we study how to best use MTurk to balance
payments and data quality. Although our work is
based on the submission of a classification task,
the framework and approaches can be adapted
for other types of tasks.
In the next section, we will discuss in more
detail our practical framework for using MTurk.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999872">
3.1 Amazon Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.999969533333333">
Amazon launched their MTurk service in 2005.
This service was initially used for internal pro-
jects and eventually fulfilled the demand for us-
ing human intelligence to perform various tasks
that computers currently cannot do or do very
well.
MTurk users naturally fall into two roles: a re-
quester and a turker. As a requester, you can de-
fine your Human Intelligent Tasks (HITs), de-
sign suitable templates, and submit your tasks to
be completed by turkers. A turker may choose
from HITs that she is eligible to work on and get
paid after the requester approves her work. The
work presented in this paper is mostly from the
perspective of a requester.
</bodyText>
<subsectionHeader confidence="0.998675">
3.2 Key Issues
</subsectionHeader>
<bodyText confidence="0.9585258">
While it is quite easy to start using MTurk, re-
questers have to confront the following: how can
we obtain sufficient useful and high-quality data
for solving real problems efficiently and eco-
nomically?
In practice, there are three key issues to con-
sider when answering this question.
Key Issues Description
Data Is the labeled data good enough for
Quality practical use?
Cost What is the sweet spot for payment?
Scale How efficiently can MTurk be used
when handling large-scale data sets?
Can the submitted job be done in a
timely manner?
</bodyText>
<tableCaption confidence="0.906488">
Table 1. Key issues for using MTurk.
</tableCaption>
<bodyText confidence="0.9916685">
Requesters want to obtain high-quality data on
a large scale without overpaying turkers. Our
proposed framework will address these key is-
sues.
</bodyText>
<subsectionHeader confidence="0.997239">
3.3 Approaches
</subsectionHeader>
<bodyText confidence="0.981810666666667">
Since not all tasks collecting non-expert knowl-
edge share the same characteristics and suitable
applications, there is not a one-size-fits-all solu-
tion as the best practice when using MTurk.
In our approach, we divide the process into
two phases:
</bodyText>
<listItem confidence="0.9999845">
• Validation Phase.
• Large-scale Submission Phase.
</listItem>
<bodyText confidence="0.9999415">
The first phase gives us information used to
determine if MTurk is a valid approach for a
given problem and what the optimal parameters
for high quality and a short turn-around time are.
</bodyText>
<page confidence="0.990788">
52
</page>
<bodyText confidence="0.999922189189189">
We have to determine the right cost for the task
and the optimal number of labels. We empiri-
cally determine these parameters with an MTurk
submission using a small amount of data. These
optimal parameters are then used for the large-
scale submission phase.
Most data labeling tasks require subjective
judgments. One cannot expect labeling results
from different labelers to always be the same.
The degree of agreement among turkers varies
depending on the complexity and ambiguity of
individual tasks. Typically we need to obtain
multiple labels for each HIT by assigning multi-
ple turkers to the same task.
Researchers mainly use the following two
quantitative measures to assess inter-agreement:
observed agreement and kappa statistics.
P(A) is the observed agreement among anno-
tators. It represents the portion where annotators
produce identical labels. This is very natural and
straightforward. However, people argue this may
not necessarily reflect the exact degree of agree-
ment due to chance agreement.
P(E) is the hypothetical probability of chance
agreement. In other words, P(E) represents the
degree of agreement if both annotators conduct
annotations randomly (according to their own
prior probability).
We can also use the kappa coefficient as a
quantitative measure of inter-person agreement.
It is a commonly used measure to remove the
effect of chance agreement. It was first intro-
duced in statistics (Cohen, 1960) and has been
widely used in the language technology commu-
nity, especially for corpus-driven approaches
(Carletta, 1996; Krippendorf, 1980). Kappa is
defined with the following equation:
</bodyText>
<equation confidence="0.997862666666667">
P(A) − P(E)
kappa =
1− P(E)
</equation>
<bodyText confidence="0.914830333333333">
Generally it is viewed more robust than ob-
served agreement P(A) because it removes
chance agreement P(E) .
</bodyText>
<table confidence="0.6871664">
DetectOutlier(P)
for each turker p e P
collect the label set L from p
for each label l e L
/* compared with others’ majority voting */
</table>
<tableCaption confidence="0.36690925">
compute its agreement with others
compute P(A)p (or kappap)
analyze the distribution of P(A)
return outlier turkers
</tableCaption>
<figureCaption confidence="0.998604">
Figure 1. Outlier detection algorithm.
</figureCaption>
<bodyText confidence="0.997457">
We use these measures to automatically detect
outlier turkers producing low-quality results.
Figure 1 shows our algorithm for automatically
detecting outlier turkers.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99998575">
Based on our proposed framework and ap-
proaches, as a case study we conducted experi-
ments on a classification task using MTurk.
The classification task requires the turker to
determine whether a web query is a local search
or not. For example, is the user typing this query
looking for a local business or not? The labeled
data set can be used to train a query classifier for
a web search system.
This capability will make search systems able
to distinguish local search queries from other
types of queries and to apply specific search al-
gorithms and data resources to better serve users’
information needs.
For example, if a person types “largest biomed
company in San Diego” and the web search sys-
tems can recognize this query as a local search
query, it will apply local search algorithms on
listing data instead of or as well as generating a
general web search request.
</bodyText>
<subsectionHeader confidence="0.996957">
4.1 Validation Phase
</subsectionHeader>
<bodyText confidence="0.99998464">
We downloaded the publicly available AOL
query log2 and used this as our corpus. We first
scanned all queries with geographic locations
(including states, cities, and neighborhoods) and
then randomly selected a set of queries for our
experiments.
For the validation phase, 700 queries were
first labeled in-house by domain experts and we
refer to this set as expert labels. To obtain the
optimal parameters including the desired number
of labels and payment price, we designed our
HITs and experiments in the following way:
We put 10 queries into one HIT, requested 15
labels for each query/HIT, and varied payment
for each HIT in four separate runs. Our payments
include $0.01, $0.02, $0.05, and $0.10 per HIT.
The goal is to have HITs completed in a timely
fashion and have them yield high-quality data.
We submitted our HITs to MTurk in four dif-
ferent runs with the following prices: $0.01,
$0.02, $0.03, and $0.10. According to our pre-
defined evaluation measures and our outlier de-
tection algorithm, we investigated how to obtain
the optimal parameters. Figure 2. shows the task
completion statistics for the four different runs.
</bodyText>
<footnote confidence="0.553742">
2 AOL Log Data: http://www.gregsadetsky.com/aol-data/
</footnote>
<page confidence="0.99855">
53
</page>
<figureCaption confidence="0.99973">
Figure 2. Task completion statistics.
</figureCaption>
<bodyText confidence="0.9000848">
As shown in Figure 2, with the increase of
payments, the average hourly rate increases from
$0.72 to $9.73 and the total turn-around time
dramatically decreases from more than 47 hours
to about 1.5 hours. In the meantime, people tend
to become more focused on the tasks and spend
less time per HIT.
In addition, as we increase payment, more
people tend to stay with the task and take it more
seriously as evidenced by the quality of the la-
beled data. This results in fewer numbers of
workers overall as well as fewer outliers as
shown in Figure 3.
Figure 3. Total number of workers and outliers.
We investigate two types of agreements, inter-
turker agreement and agreement between turkers
and our in-house experts. For inter non-expert
agreements, we compute each turker’s agreement
with all others’ majority voting results.
Payment
(USD) 0.01 0.02 0.05 0.10
Median of 0.8074 0.8583 0.9346 0.9028
inter-
turker
agreement
</bodyText>
<tableCaption confidence="0.928295">
Table 2. Median of inter-turker agreements.
</tableCaption>
<bodyText confidence="0.999482529411765">
As in our outlier detection algorithm, we ana-
lyzed the distribution of inter-turker agreements.
Table 2 shows the median values of inter-turker
agreement as we vary the payment prices. The
median value keeps on increasing when the price
increases from $0.01, to $0.02 and $0.05. How-
ever, it drops as the price increases from $0.05 to
$0.10. This implies that turkers do not necessar-
ily improve their work quality as they get paid
more. One of the possible explanations for this
phenomenon is that when the reward is high
people tend to work towards completing the task
as fast as possible instead of focusing on submit-
ting high-quality data. This trend may be intrin-
sic to the task we have submitted and further ex-
periments will show if this turker behavior is
task-independent.
</bodyText>
<figureCaption confidence="0.663206">
Figure 5. Inter non-expert agreement.
</figureCaption>
<bodyText confidence="0.999593083333333">
We also analyzed agreement between non-
experts and experts. Figure 4 depicts the trend of
the agreement scores with the increase of number
of labels and payments. For example, given
seven labels per query, in the experiment with
the $0.05 payment, the majority voting of non-
expert labels has an agreement of 0.9465 with
expert labeling. As explained earlier we do not
necessarily obtain the best data qual-
ity/agreement with the $0.10 payment. Instead,
we get the highest agreement with the $0.05
payment. We have determined this rate to be the
</bodyText>
<figureCaption confidence="0.960257">
Figure 4. Agreement with experts.
</figureCaption>
<page confidence="0.994709">
54
</page>
<bodyText confidence="0.999888727272728">
sweet spot in terms of cost. Also, seven labels
per query produce a very high agreement with no
further significant improvement when we in-
crease the number of labels.
For inter non-expert agreements, we found
similar trends in terms of different payments and
number of labels as shown in Figure 5.
As mentioned above, our algorithm is able to
detect turkers producing low-quality data. One
natural question is: how will their labels affect
the overall data quality?
We studied this problem in two different
ways. We evaluated the data quality by removing
either all polluted queries or only outliers’ labels.
Here polluted queries refer to those queries re-
ceiving at least one label from outliers. By re-
moving polluted queries, we only investigate the
clean data set without any outlier labels. The
other alternative is to only remove outliers’ la-
bels for specific queries but others’ labels for
those queries will be kept. Both the agreement
between experts and non-experts and inter-non-
experts agreement show similar trends: data
quality without outliers’ labels is slightly better
since there is less noise. However, as outliers’
labels may span a large number of queries, it
may not be feasible to remove all polluted que-
ries. For example, in one of our experiments,
outliers’ labels pollute more than half of all the
records. We cannot simply remove all the queries
with outliers’ labels due to consideration of cost.
On the other hand, the effect of outliers’ labels
is not that significant if a certain number of re-
quested labels per query are collected. As shown
in Figure 6, noisy data from outliers can be over-
ridden by assigning more labelers.
Figure 6. Agreement with Experts (removing
outliers’ labels (payment = $0.05)).
From the validation phase of the query classi-
fication task, we determine that the optimal pa-
rameters are paying $0.05 per HIT and request-
ing seven labels per query. Given this number of
labels, the effect of outliers’ labels can be over-
ridden for the final result.
</bodyText>
<subsectionHeader confidence="0.998164">
4.2 Large-scale Submission Phase
</subsectionHeader>
<bodyText confidence="0.999273222222222">
Having obtained the optimal parameters from the
validation phase, we are then ready to make a
large-scale submission.
For this phase, we paid $0.05 per HIT and re-
quested seven labels per query/HIT. Following
similar filtering and sampling approaches as in
the validation phase, we selected 22.5k queries
from the AOL search log. Table 3 shows the de-
tected outliers for this large-scale submission.
</bodyText>
<table confidence="0.994953333333333">
Total Number of Turkers 228
Number of Outlier Turkers 23
Outlier Ratio 10.09%
</table>
<tableCaption confidence="0.997807">
Table 3. Number of turkers and outliers.
</tableCaption>
<bodyText confidence="0.9984498">
Based on the distribution of inter-turker
agreement, any turkers with agreement less than
0.6501 are recognized as outliers. For a total
number of 15,750 HITs, 228 turkers contributed
to the labeling effort and 10.09% of them were
recognized as outliers.
Table 4 shows the number of labels from the
outliers and the approval ratio of collected data.
About 10.08% of labels are from outlier turkers
and rejected.
</bodyText>
<table confidence="0.985647666666667">
Total Number of Labels 157,500
Number of Outlier Labels 15,870
Approval Ratio 89.92%
</table>
<tableCaption confidence="0.99416">
Table 4. Total number of labels.
</tableCaption>
<bodyText confidence="0.999818714285714">
We have experimented using MTurk for a web
query classification task. With learned optimal
parameters from the validation phase, we col-
lected large-scale high-quality non-expert labels
in a fast and economical way. These data will be
used to train query classifiers to enhance web
search systems handling local search queries.
</bodyText>
<sectionHeader confidence="0.998269" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999888923076923">
In this paper, we presented a practical framework
for acquiring high quality non-expert knowledge
from an on-demand and scalable workforce. Us-
ing Amazon Mechanical Turk, we collected
large-scale human classification knowledge on
web search queries.
To learn the best practices when using MTurk,
we presented a two-phase approach, a validation
phase and a large-scale submission phase. We
conducted extensive experiments to obtain the
optimal parameters on the number of labelers
and payments in the validation phase. We also
presented an algorithm to automatically detect
</bodyText>
<page confidence="0.9951">
55
</page>
<bodyText confidence="0.999797473684211">
outlier turkers based on the agreement analysis
and investigated the effect of removing an inac-
curately labeled set.
Acquiring high-quality human knowledge will
remain a major concern and a bottleneck for in-
dustry applications and academic problems. Un-
like traditional ways of collecting in-house hu-
man knowledge, MTurk provides an alternative
way to acquire non-expert knowledge. As shown
in our experiments, given appropriate quality
control, we have been able to acquire high-
quality data in a very fast and efficient way. We
believe MTurk will attract more attention and
usage in broader areas.
In the future, we are planning to investigate
how this framework can be applied to different
types of human knowledge acquisition tasks and
how to leverage large-scale labeled data sets for
solving natural language processing problems.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889898734177">
Baker, C.F., Fillmore, C.J., and Lowe, J.B. 1998. The
Berkeley FrameNet Project. In Proc. of COLING-
ACL-1998.
Belasco, A., Curtis, J., Kahlert, R., Klein, C., Mayans,
C., and Reagan, P. 2002. Representing Knowledge
Gaps Effectively. In Practical Aspects of Knowl-
edge Management, (PAKM).
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics. 22(2):249–254.
Chklovski, T. 2003. LEARNER: A System for Ac-
quiring Commonsense Knowledge by Analogy. In
Proc. of Second International Conference on
Knowledge Capture (KCAP 2003).
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Meas-
urement. Vol.20, No.1, pp.37-46.
Colowick, S.M. and Pool, J. 2007. Disambiguating for
the web: a test of two methods. In Proc. of the 4th
international Conference on Knowledge Capture
(K-CAP 2007).
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and
Weischedel, R. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT-NAACL-2006.
Kaisser, M. and Lowe, J.B. 2008. Creating a Research
Collection of Question Answer Sentence Pairs with
Amazon&apos;s Mechanical Turk. In Proc. of the Fifth
International Conference on Language Resources
and Evaluation (LREC-2008).
Kittur, A., Chi, E. H., and Suh, B. 2008. Crowdsourc-
ing user studies with Mechanical Turk. In Proc. of
the 26th Annual ACM Conference on Human Fac-
tors in Computing Systems (CHI-2008).
Krippendorf, K. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications.
Marcus, M., Marcinkiewicz, M.A., and Santorini, B.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics.
19:2, June 1993.
Nakov, P. 2008. Paraphrasing Verbs for Noun Com-
pound Interpretation. In Proc. of the Workshop on
Multiword Expressions (MWE-2008).
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The
Proposition Bank: A Corpus Annotated with Se-
mantic Roles. Computational Linguistics. 31:1.
Sheng, V.S., Provost, F., and Ipeirotis, P.G. 2008. Get
another label? improving data quality and data
mining using multiple, noisy labelers. In Proc. of
the 14th ACM SIGKDD international Conference
on Knowledge Discovery and Data Mining (KDD-
2008).
Singh, P., Lin, T., Mueller, E., Lim, G., Perkins, T.,
and Zhu, W. 2002. Open Mind Common Sense:
Knowledge acquisition from the general public. In
Meersman, R. and Tari, Z. (Eds.), LNCS: Vol.
2519. On the Move to Meaningful Internet Sys-
tems: DOA/CoopIS/ODBASE (pp. 1223-1237).
Springer-Verlag.
Snow, R., O’Connor, B., Jurafsky, D., and Ng, A.Y.
2008. Cheap and Fast – But is it Good? Evaluating
Non-Expert Annotations for Natural Language
Tasks . In Proc. of EMNLP-2008.
Sorokin, A. and Forsyth, D. 2008. Utility data annota-
tion with Amazon Mechanical Turk. In Proc. of the
First IEEE Workshop on Internet Vision at CVPR-
2008.
Stork, D.G. 1999. The Open Mind Initiative. IEEE
Expert Systems and Their Applications. pp. 16-20,
May/June 1999.
Su, Q., Pavlov, D., Chow, J., and Baker, W.C. 2007.
Internet-scale collection of human-reviewed data.
In Proc. of the 16th international Conference on
World Wide Web (WWW-2007).
Von Ahn, L. and Dabbish, L. 2004. Labeling Images
with a Computer Game. In Proc. of ACM Confer-
ence on Human Factors in Computing Systmes
(CHI). pp. 319-326.
Voorhees, E.M. 2003. Overview of TREC 2003. In
Proc. of TREC-2003.
</reference>
<page confidence="0.99842">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960522">
<title confidence="0.9900595">Acquiring High Quality Non-Expert Knowledge On-demand Workforce</title>
<author confidence="0.999287">Donghui Feng Sveva Besana Remi Zajac</author>
<affiliation confidence="0.999923">AT&amp;T Interactive</affiliation>
<address confidence="0.999892">Glendale, CA, 91203</address>
<email confidence="0.998107">dfeng@attinteractive.com</email>
<email confidence="0.998107">sbesana@attinteractive.com</email>
<email confidence="0.998107">rzajac@attinteractive.com</email>
<abstract confidence="0.99903">Being expensive and time consuming, human knowledge acquisition has consistently been a major bottleneck for solving real problems. In this paper, we present a practical framework for acquiring high quality non-expert knowledge from on-demand workforce using Amazon Mechanical Turk (MTurk). We show how to apply this framework to collect large-scale human knowledge on AOL query classification in a fast and efficient fashion. Based on extensive experiments and analysis, we demonstrate how to detect low-quality labels from massive data sets and their impact on collecting high-quality knowledge. Our experimental findings also provide insight into the best practices on balancing cost and data quality for using MTurk.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proc. of COLINGACL-1998.</booktitle>
<contexts>
<context position="1814" citStr="Baker et al., 1998" startWordPosition="262" endWordPosition="265">led data for supervised learning-based approaches. Traditionally this has been collected from domain experts, which we refer to as expert knowledge. However, acquiring in-house expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large quantities of data from volunte</context>
<context position="4560" citStr="Baker et al., 1998" startWordPosition="706" endWordPosition="709">ction 2, we review related work using MTurk. We describe our methodology in Section 3 and in Section 4 we present our experimental results and further analysis. In Section 5 we draw conclusions and discuss our plans for future work. 2 Related Work It is either infeasible or very time and cost consuming to acquire in-house expert human knowledge. To obtain valuable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of nat</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, C.F., Fillmore, C.J., and Lowe, J.B. 1998. The Berkeley FrameNet Project. In Proc. of COLINGACL-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belasco</author>
<author>J Curtis</author>
<author>R Kahlert</author>
<author>C Klein</author>
<author>C Mayans</author>
<author>P Reagan</author>
</authors>
<title>Representing Knowledge Gaps Effectively.</title>
<date>2002</date>
<booktitle>In Practical Aspects of Knowledge Management,</booktitle>
<location>(PAKM).</location>
<contexts>
<context position="2043" citStr="Belasco et al., 2002" startWordPosition="301" endWordPosition="305">onsuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large quantities of data from volunteers. More recently researchers have begun to explore approaches for acquiring human knowledge from an on-demand workforce such as Amazon Mechanical Turk1. MTurk is a marketplace for jobs that require human intelligence. There has</context>
</contexts>
<marker>Belasco, Curtis, Kahlert, Klein, Mayans, Reagan, 2002</marker>
<rawString>Belasco, A., Curtis, J., Kahlert, R., Klein, C., Mayans, C., and Reagan, P. 2002. Representing Knowledge Gaps Effectively. In Practical Aspects of Knowledge Management, (PAKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics.</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="9511" citStr="Carletta, 1996" startWordPosition="1501" endWordPosition="1502">necessarily reflect the exact degree of agreement due to chance agreement. P(E) is the hypothetical probability of chance agreement. In other words, P(E) represents the degree of agreement if both annotators conduct annotations randomly (according to their own prior probability). We can also use the kappa coefficient as a quantitative measure of inter-person agreement. It is a commonly used measure to remove the effect of chance agreement. It was first introduced in statistics (Cohen, 1960) and has been widely used in the language technology community, especially for corpus-driven approaches (Carletta, 1996; Krippendorf, 1980). Kappa is defined with the following equation: P(A) − P(E) kappa = 1− P(E) Generally it is viewed more robust than observed agreement P(A) because it removes chance agreement P(E) . DetectOutlier(P) for each turker p e P collect the label set L from p for each label l e L /* compared with others’ majority voting */ compute its agreement with others compute P(A)p (or kappap) analyze the distribution of P(A) return outlier turkers Figure 1. Outlier detection algorithm. We use these measures to automatically detect outlier turkers producing low-quality results. Figure 1 shows</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, J. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics. 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
</authors>
<title>LEARNER: A System for Acquiring Commonsense Knowledge by Analogy.</title>
<date>2003</date>
<booktitle>In Proc. of Second International Conference on Knowledge Capture (KCAP</booktitle>
<contexts>
<context position="1985" citStr="Chklovski, 2003" startWordPosition="292" endWordPosition="293">se expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large quantities of data from volunteers. More recently researchers have begun to explore approaches for acquiring human knowledge from an on-demand workforce such as Amazon Mechanical Turk1. MTurk is a marke</context>
</contexts>
<marker>Chklovski, 2003</marker>
<rawString>Chklovski, T. 2003. LEARNER: A System for Acquiring Commonsense Knowledge by Analogy. In Proc. of Second International Conference on Knowledge Capture (KCAP 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement. Vol.20, No.1,</booktitle>
<pages>37--46</pages>
<contexts>
<context position="9392" citStr="Cohen, 1960" startWordPosition="1484" endWordPosition="1485">re annotators produce identical labels. This is very natural and straightforward. However, people argue this may not necessarily reflect the exact degree of agreement due to chance agreement. P(E) is the hypothetical probability of chance agreement. In other words, P(E) represents the degree of agreement if both annotators conduct annotations randomly (according to their own prior probability). We can also use the kappa coefficient as a quantitative measure of inter-person agreement. It is a commonly used measure to remove the effect of chance agreement. It was first introduced in statistics (Cohen, 1960) and has been widely used in the language technology community, especially for corpus-driven approaches (Carletta, 1996; Krippendorf, 1980). Kappa is defined with the following equation: P(A) − P(E) kappa = 1− P(E) Generally it is viewed more robust than observed agreement P(A) because it removes chance agreement P(E) . DetectOutlier(P) for each turker p e P collect the label set L from p for each label l e L /* compared with others’ majority voting */ compute its agreement with others compute P(A)p (or kappap) analyze the distribution of P(A) return outlier turkers Figure 1. Outlier detection</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement. Vol.20, No.1, pp.37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Colowick</author>
<author>J Pool</author>
</authors>
<title>Disambiguating for the web: a test of two methods.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th international Conference on Knowledge Capture (K-CAP</booktitle>
<marker>Colowick, Pool, 2007</marker>
<rawString>Colowick, S.M. and Pool, J. 2007. Disambiguating for the web: a test of two methods. In Proc. of the 4th international Conference on Knowledge Capture (K-CAP 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: The 90% Solution.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL-2006.</booktitle>
<contexts>
<context position="1849" citStr="Hovy et al., 2006" startWordPosition="268" endWordPosition="271">ed approaches. Traditionally this has been collected from domain experts, which we refer to as expert knowledge. However, acquiring in-house expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large quantities of data from volunteers. More recently researchers have</context>
<context position="4627" citStr="Hovy et al., 2006" startWordPosition="717" endWordPosition="720">logy in Section 3 and in Section 4 we present our experimental results and further analysis. In Section 5 we draw conclusions and discuss our plans for future work. 2 Related Work It is either infeasible or very time and cost consuming to acquire in-house expert human knowledge. To obtain valuable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of natural language tasks including word sense disambiguation, word simil</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and Weischedel, R. 2006. OntoNotes: The 90% Solution. In Proc. of HLT-NAACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaisser</author>
<author>J B Lowe</author>
</authors>
<title>Creating a Research Collection of Question Answer Sentence Pairs with Amazon&apos;s Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proc. of the Fifth International Conference on Language Resources and Evaluation (LREC-2008).</booktitle>
<contexts>
<context position="4805" citStr="Kaisser and Lowe (2008)" startWordPosition="745" endWordPosition="748">d Work It is either infeasible or very time and cost consuming to acquire in-house expert human knowledge. To obtain valuable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of natural language tasks including word sense disambiguation, word similarity, and textual entailment recognition. However, most of the reported work focuses on how to apply data collected from MTurk to their applications. In our work, we concentrate</context>
</contexts>
<marker>Kaisser, Lowe, 2008</marker>
<rawString>Kaisser, M. and Lowe, J.B. 2008. Creating a Research Collection of Question Answer Sentence Pairs with Amazon&apos;s Mechanical Turk. In Proc. of the Fifth International Conference on Language Resources and Evaluation (LREC-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kittur</author>
<author>E H Chi</author>
<author>B Suh</author>
</authors>
<title>Crowdsourcing user studies with Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proc. of the 26th Annual ACM Conference on Human Factors in Computing Systems (CHI-2008).</booktitle>
<contexts>
<context position="4988" citStr="Kittur et al. (2008)" startWordPosition="770" endWordPosition="773">research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of natural language tasks including word sense disambiguation, word similarity, and textual entailment recognition. However, most of the reported work focuses on how to apply data collected from MTurk to their applications. In our work, we concentrate on presenting a practical framework for using MTurk by separating the process into a validation phase and a large-scale submission phase. By analyzing workers’ behavior and their dat</context>
</contexts>
<marker>Kittur, Chi, Suh, 2008</marker>
<rawString>Kittur, A., Chi, E. H., and Suh, B. 2008. Crowdsourcing user studies with Mechanical Turk. In Proc. of the 26th Annual ACM Conference on Human Factors in Computing Systems (CHI-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorf</author>
</authors>
<title>Content Analysis: An introduction to its methodology.</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="9531" citStr="Krippendorf, 1980" startWordPosition="1503" endWordPosition="1504">ect the exact degree of agreement due to chance agreement. P(E) is the hypothetical probability of chance agreement. In other words, P(E) represents the degree of agreement if both annotators conduct annotations randomly (according to their own prior probability). We can also use the kappa coefficient as a quantitative measure of inter-person agreement. It is a commonly used measure to remove the effect of chance agreement. It was first introduced in statistics (Cohen, 1960) and has been widely used in the language technology community, especially for corpus-driven approaches (Carletta, 1996; Krippendorf, 1980). Kappa is defined with the following equation: P(A) − P(E) kappa = 1− P(E) Generally it is viewed more robust than observed agreement P(A) because it removes chance agreement P(E) . DetectOutlier(P) for each turker p e P collect the label set L from p for each label l e L /* compared with others’ majority voting */ compute its agreement with others compute P(A)p (or kappap) analyze the distribution of P(A) return outlier turkers Figure 1. Outlier detection algorithm. We use these measures to automatically detect outlier turkers producing low-quality results. Figure 1 shows our algorithm for a</context>
</contexts>
<marker>Krippendorf, 1980</marker>
<rawString>Krippendorf, K. 1980. Content Analysis: An introduction to its methodology. Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<tech>19:2,</tech>
<contexts>
<context position="1783" citStr="Marcus et al., 1993" startWordPosition="257" endWordPosition="260">s require non-trivial human labeled data for supervised learning-based approaches. Traditionally this has been collected from domain experts, which we refer to as expert knowledge. However, acquiring in-house expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large </context>
<context position="4529" citStr="Marcus et al., 1993" startWordPosition="701" endWordPosition="704">r is organized as follows: In Section 2, we review related work using MTurk. We describe our methodology in Section 3 and in Section 4 we present our experimental results and further analysis. In Section 5 we draw conclusions and discuss our plans for future work. 2 Related Work It is either infeasible or very time and cost consuming to acquire in-house expert human knowledge. To obtain valuable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Marcus, M., Marcinkiewicz, M.A., and Santorini, B. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics. 19:2, June 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
</authors>
<title>Paraphrasing Verbs for Noun Compound Interpretation.</title>
<date>2008</date>
<booktitle>In Proc. of the Workshop on Multiword Expressions (MWE-2008).</booktitle>
<marker>Nakov, 2008</marker>
<rawString>Nakov, P. 2008. Paraphrasing Verbs for Noun Compound Interpretation. In Proc. of the Workshop on Multiword Expressions (MWE-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: A Corpus Annotated with Semantic Roles. Computational Linguistics.</title>
<date>2005</date>
<pages>31--1</pages>
<contexts>
<context position="4592" citStr="Palmer et al., 2005" startWordPosition="711" endWordPosition="714"> using MTurk. We describe our methodology in Section 3 and in Section 4 we present our experimental results and further analysis. In Section 5 we draw conclusions and discuss our plans for future work. 2 Related Work It is either infeasible or very time and cost consuming to acquire in-house expert human knowledge. To obtain valuable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of natural language tasks including wo</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., and Kingsbury, P. 2005. The Proposition Bank: A Corpus Annotated with Semantic Roles. Computational Linguistics. 31:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V S Sheng</author>
<author>F Provost</author>
<author>P G Ipeirotis</author>
</authors>
<title>Get another label? improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proc. of the 14th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining (KDD2008).</booktitle>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Sheng, V.S., Provost, F., and Ipeirotis, P.G. 2008. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proc. of the 14th ACM SIGKDD international Conference on Knowledge Discovery and Data Mining (KDD2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singh</author>
<author>T Lin</author>
<author>E Mueller</author>
<author>G Lim</author>
<author>T Perkins</author>
<author>W Zhu</author>
</authors>
<title>Open Mind Common Sense: Knowledge acquisition from the general public.</title>
<date>2002</date>
<booktitle>In Meersman,</booktitle>
<pages>1223--1237</pages>
<editor>(Eds.),</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1954" citStr="Singh et al., 2002" startWordPosition="286" endWordPosition="289">owledge. However, acquiring in-house expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large quantities of data from volunteers. More recently researchers have begun to explore approaches for acquiring human knowledge from an on-demand workforce such as Amazon Mec</context>
</contexts>
<marker>Singh, Lin, Mueller, Lim, Perkins, Zhu, 2002</marker>
<rawString>Singh, P., Lin, T., Mueller, E., Lim, G., Perkins, T., and Zhu, W. 2002. Open Mind Common Sense: Knowledge acquisition from the general public. In Meersman, R. and Tari, Z. (Eds.), LNCS: Vol. 2519. On the Move to Meaningful Internet Systems: DOA/CoopIS/ODBASE (pp. 1223-1237). Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks .</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP-2008.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Snow, R., O’Connor, B., Jurafsky, D., and Ng, A.Y. 2008. Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks . In Proc. of EMNLP-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sorokin</author>
<author>D Forsyth</author>
</authors>
<title>Utility data annotation with Amazon Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proc. of the First IEEE Workshop on Internet Vision at CVPR2008.</booktitle>
<contexts>
<context position="4926" citStr="Sorokin and Forsyth (2008)" startWordPosition="761" endWordPosition="764">aluable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of natural language tasks including word sense disambiguation, word similarity, and textual entailment recognition. However, most of the reported work focuses on how to apply data collected from MTurk to their applications. In our work, we concentrate on presenting a practical framework for using MTurk by separating the process into a validation phase and a large-scale </context>
</contexts>
<marker>Sorokin, Forsyth, 2008</marker>
<rawString>Sorokin, A. and Forsyth, D. 2008. Utility data annotation with Amazon Mechanical Turk. In Proc. of the First IEEE Workshop on Internet Vision at CVPR2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Stork</author>
</authors>
<title>The Open Mind Initiative.</title>
<date>1999</date>
<booktitle>IEEE Expert Systems and Their Applications.</booktitle>
<pages>16--20</pages>
<contexts>
<context position="1933" citStr="Stork, 1999" startWordPosition="284" endWordPosition="285"> as expert knowledge. However, acquiring in-house expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is to collect valuable input from entertained labelers. Turning label acquisition into a computer game addresses tediousness, which is one of the main reasons that it is hard to gather large quantities of data from volunteers. More recently researchers have begun to explore approaches for acquiring human knowledge from an on-demand workfor</context>
</contexts>
<marker>Stork, 1999</marker>
<rawString>Stork, D.G. 1999. The Open Mind Initiative. IEEE Expert Systems and Their Applications. pp. 16-20, May/June 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Su</author>
<author>D Pavlov</author>
<author>J Chow</author>
<author>W C Baker</author>
</authors>
<title>Internet-scale collection of human-reviewed data.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th international Conference on World Wide Web (WWW-2007).</booktitle>
<contexts>
<context position="4735" citStr="Su et al. (2007)" startWordPosition="734" endWordPosition="737">raw conclusions and discuss our plans for future work. 2 Related Work It is either infeasible or very time and cost consuming to acquire in-house expert human knowledge. To obtain valuable human knowledge (e.g., in the format of labeled data), many research projects in the natural language community have been funded to create large-scale corpora and knowledge bases, such as PenTreeBank (Marcus et al., 1993), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and OntoNotes (Hovy et al., 2006). MTurk has been attracting much attention within several research areas since its release. Su et al. (2007) use MTurk to collect large-scale review data. Kaisser and Lowe (2008) report their work on generating research collections of question-answering pairs using MTurk. Sorokin and Forsyth (2008) outsource image-labeling tasks to MTurk. Kittur et al. (2008) use MTurk as the paradigm for user studies. In the natural language community Snow et al. (2008) report their work on collecting linguistic annotation for a variety of natural language tasks including word sense disambiguation, word similarity, and textual entailment recognition. However, most of the reported work focuses on how to apply data c</context>
</contexts>
<marker>Su, Pavlov, Chow, Baker, 2007</marker>
<rawString>Su, Q., Pavlov, D., Chow, J., and Baker, W.C. 2007. Internet-scale collection of human-reviewed data. In Proc. of the 16th international Conference on World Wide Web (WWW-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Von Ahn</author>
<author>L Dabbish</author>
</authors>
<title>Labeling Images with a Computer Game.</title>
<date>2004</date>
<booktitle>In Proc. of ACM Conference on Human Factors in Computing Systmes (CHI).</booktitle>
<pages>319--326</pages>
<marker>Von Ahn, Dabbish, 2004</marker>
<rawString>Von Ahn, L. and Dabbish, L. 2004. Labeling Images with a Computer Game. In Proc. of ACM Conference on Human Factors in Computing Systmes (CHI). pp. 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of TREC</title>
<date>2003</date>
<booktitle>In Proc. of TREC-2003.</booktitle>
<contexts>
<context position="1594" citStr="Voorhees, 2003" startWordPosition="228" endWordPosition="229">l for training intelligent systems to solve real problems, both for industry applications and academic research. For example, many machine learning and natural language processing tasks require non-trivial human labeled data for supervised learning-based approaches. Traditionally this has been collected from domain experts, which we refer to as expert knowledge. However, acquiring in-house expert knowledge is usually very expensive, time consuming, and has consistently been a major bottleneck for many research problems. For example, tremendous efforts have been put into creating TREC corpora (Voorhees, 2003). As a result, several research projects sponsored by NSF and DARPA aim to construct valuable data resources via human labeling; these are exemplified by PennTree Bank (Marcus et al., 1993), FrameNet (Baker et al., 1998), and OntoNotes (Hovy et al., 2006). In addition, there are projects such as Open Mind Common Sense (OMCS) (Stork, 1999; Singh et al., 2002), ISI LEARNER (Chklovski, 2003), and the Fact Entry Tool by Cycorp (Belasco et al., 2002) where knowledge is gathered from volunteers. One interesting approach followed by von Ahn and Dabbish (2004), applied to image labeling on the Web, is</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Voorhees, E.M. 2003. Overview of TREC 2003. In Proc. of TREC-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>