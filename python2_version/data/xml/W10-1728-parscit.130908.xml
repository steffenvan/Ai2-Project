<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048350">
<title confidence="0.998593">
To Cache or not to Cache?
Experiments with Adaptive Models in Statistical Machine Translation
</title>
<author confidence="0.997441">
J¨org Tiedemann
</author>
<affiliation confidence="0.9900045">
Department of Linguistics and Philology
Uppsala University, Uppsala/Sweden
</affiliation>
<email confidence="0.995062">
jorg.tiedemann@lingfil.uu.se
</email>
<sectionHeader confidence="0.993804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880625">
We report results of our submissions to
the WMT 2010 shared translation task in
which we applied a system that includes
adaptive language and translation mod-
els. Adaptation is implemented using ex-
ponentially decaying caches storing pre-
vious translations as the history for new
predictions. Evidence from the cache is
then mixed with the global background
model. The main problem in this setup is
error propagation and our submissions es-
sentially failed to improve over the com-
petitive baseline. There are slight im-
provements in lexical choice but the global
performance decreases in terms of BLEU
scores.
</bodyText>
<sectionHeader confidence="0.990026" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.990930056603773">
The main motivation of our submission was to
test the use of adaptive language and translation
models in a standard phrase-based SMT setting
for the adaptation to wider context beyond sen-
tence boundaries. Adaptive language models have
a long tradition in the speech recognition commu-
nity and various approaches have been proposed
to reduce model perplexity in this way. The gen-
eral task is to adjust statistical models to essen-
tial properties of natural language which are usu-
ally not captured by standard n-gram models or
other local dependency models. First of all, it is
known that repetition is very common especially
among content words (see, for example, words
like “honey”, “milk”, “land” and “flowing” in fig-
ure 1). In most cases a repeated occurrence of a
content word is much more likely than its first ap-
pearance, which is not predicted in this way by a
static language model. Secondly, the use of ex-
pressions is related to the topic in the current dis-
course and the chance of using the same topic-
related expressions again in running text is higher
than a mixed-topic model would predict.
In translation another phenomenon can be ob-
served, namely the consistency of translations.
Polysemous terms are usually not ambiguous in
their context and, hence, their translations become
consistent according to the contextual sense. Even
the choice between synonymous translations is
rather consistent in translated texts as we can see
in the example of subtitle translations in figure 1
(taken from the OPUS corpus (Tiedemann, 2009)).
The 10 commandments Kerd ma lui
To some land flowing with milk Mari honey ...
and honey! Mari, gumman
Till ett land fullt av mj¨olk och Sweetheart,
honung. where are you
going?
¨Alskling, var
ska du?
...
Who was that,
honey?
Vem var det,
gumman?
I’ve never tasted honey.
Jag har aldrig smakat honung.
...
But will sympathy lead us to
this land flowing with milk and
honey?
Men kan sympati leda oss till detta
mj¨olkens och honungens land?
</bodyText>
<figureCaption confidence="0.999685">
Figure 1: Repetition and translation consistency
</figureCaption>
<bodyText confidence="0.999831875">
Ambiguous terms like “honey” are consistently
translated into the Swedish counterpart “honung”
(in the sense of the actual substance) or “gumman”
(in the metaphoric sense). Observe that this is true
even in the latter case where synonymous transla-
tions such as “¨alskling” would be possible as well.
In other words, deciding to stick to consistent lexi-
cal translations should be preferred in MT because
the chance of alternative translations in repeated
cases is low. Here again, common static transla-
tion models do not capture this property at all.
In the following we explain our attempt to inte-
grate contextual dependencies using cache-based
adaptive models in a standard SMT setup. We
have already successfully applied this technique
to a domain-adaptation task (Tiedemann, 2010).
</bodyText>
<page confidence="0.985781">
189
</page>
<note confidence="0.453626">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 189–194,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999726">
Now we would like to investigate the robustness
of this model in a more general case where some
in-domain training data is available and input data
is less repetitive.
</bodyText>
<sectionHeader confidence="0.872502" genericHeader="introduction">
2 Cache-based Adaptive Models
</sectionHeader>
<bodyText confidence="0.999970857142857">
The basic idea behind cache-based models is to
mix a large static background model with a small
local model that is dynamically estimated from re-
cent items from the input stream. Dynamic cache
language models have been introduced by (Kuhn
and Mori, 1990) and are often implemented in the
form of linear mixtures:
</bodyText>
<equation confidence="0.998929">
P(wn|history) = (1 − A)Pbackground(wn|history) +
APcache(wn|history)
</equation>
<bodyText confidence="0.997687833333333">
The background model is usually a standard n-
gram model taking limited amount of local context
from the history into account and the cache model
is often implemented as a simple (unsmoothed)
unigram model using the elements stored in a
fixed-size cache (100-5000 words) to estimate
its parameters. Another improvement can be
achieved by making the importance of cached el-
ements a function of recency. This can be done
by introducing a decaying factor in the estima-
tion of cache probabilities (Clarkson and Robin-
son, 1997):
</bodyText>
<equation confidence="0.970634">
Pcache(wn|wn−k..wn−1) �
</equation>
<bodyText confidence="0.999939907692308">
This is basically the model that we applied in our
experiments as it showed the largest perplexity re-
duction in our previous experiments on domain
adaptation.
Similarly, translation models can be adapted as
well. This is especially useful to account for trans-
lation consistency forcing the decoder to prefer
identical translations for repeated terms. In our
approach we try to model recency again using a
decay factor to compute translation model scores
from the cache in the following way (only for
source language phrases fn for which a transla-
tion option exist in the cache; we use a score of
zero otherwise):
The importance of a cached translation option ex-
ponentially decays and we normalize the sum of
cached occurrences by the number of translation
options with the same foreign language item that
we condition on.
Plugging this in into a standard phrase-based SMT
engine is rather straightforward. The use of cache-
based language models in SMT have been in-
vestigated before (Raab, 2007). In our case we
used Moses as the base decoder (Koehn et al.,
2007). The cache-based language model can be
integrated in the decoder by simply adjusting the
call to the language modeling toolkit appropri-
ately. We implemented the exponentially decaying
cache model within the standard SRILM toolkit
(Stolcke, 2002) and added command line argu-
ments to Moses to switch to that model and to set
cache parameters such as interpolation, cache size
and decay. Adding the translation model cache is
a bit more tricky. For this we added a new feature
function to the global log-linear model and im-
plemented the decaying cache as explained above
within the decoder. Again, simple command-line
arguments can be used to switch caching on or off
and to adjust cache parameters.
One important issue is to decide when and what
to cache. As we explore a lot of different options
in decoding it is not feasible to adapt the cache
continuously. This would mean a lot of cache op-
erations trying to add and remove hypotheses from
the cache memory. Therefore, we opted for a con-
text model that considers history only from previ-
ous sentences. Once decoding is finished transla-
tion options from the best hypothesis found in de-
coding are put into language and translation model
cache. This is arguably a strong approximation of
the adaptive approach. However, considering our
special concern about wider context across sen-
tence boundaries this seems to be a reasonable
compromise between completeness and efficiency.
Another issue is related to the selection of items
to be cached. As discussed earlier repetition is
most likely to be found among content words.
Similarly, translation consistency is less likely to
be true for function words. In the best case one
would know the likelihood of specific terms to
be repeated. This could be trained on some de-
velopment data possibly in connection with word
classes instead of fully lexicalized parameters in
order to overcome data sparseness and to improve
generality. Even though this idea is very tempt-
</bodyText>
<equation confidence="0.987907">
I(wn = wi)e−α(n−i)
1 n−1E
Z i=n−k
�(enI�Ki= 1 I((en, fn) = (ei, fi)) * e
ycache —ai =
�K i=1 I(fn = fi)
</equation>
<page confidence="0.927854">
190
</page>
<bodyText confidence="0.999956107142857">
ing it would require a substantial extension of our
model and would introduce language and domain-
specific parameters. Therefore, we just added a
simplistic approach filtering tokens by their length
in characters instead. Assuming that longer items
are more likely to be content words we simply set
a threshold to decide whether to add a term to the
cache or not. This threshold can be adjusted using
command-line arguments.
Finally, we also need to be careful about noise
in the cache. This is essential as the caching ap-
proach is prone to error propagation. However,
detecting noise is difficult. If there would be a no-
tion of noise in translation hypotheses, the decoder
would avoid it. In related work (Nepveu et al.,
2004) have studied cache-based translation mod-
els in connection with interactive machine trans-
lation. In that case, one can assume correct input
after post-editing the translation suggestions. One
way to approach noise reduction in non-interactive
MT is to make use of transition costs in the transla-
tion lattice. Assuming that this cost (which is esti-
mated internally within the decoder during the ex-
pansion of translation hypotheses) refers to some
kind of confidence we can discard translation op-
tions above a certain threshold, which is what we
did in the implementation of our translation model
cache.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999050896551724">
We followed the setup proposed in the shared
translation task. Primarily we concentrated our
efforts on German-English (de-en) and English-
German (en-de) using the constrained track, i.e.
using the provided training and development data
from Europarl and the News domain. Later we
also added experiments for Spanish (es) and En-
glish using a similar setup.
Our baseline system incorporates the following
components: We trained two separate 5-gram lan-
guage models for each language with the standard
smoothing strategies (interpolation and Kneser-
Ney discounting), one for Europarl and one for the
News data. All of them were estimated using the
SRILM toolkit except the English News LM for
which we applied RandLM (Talbot and Osborne,
2007) to cope with the large amount of training
data. We also included two separate translation
models, one for the combined Europarl and News
data and one for the News data only. They were
estimated using the standard tools GIZA++ (Och
and Ney, 2003) and Moses (Koehn et al., 2007)
applying default settings and lowercased training
data. Lexicalized reordering was trained on the
combined data set. All baseline models were then
tuned on the News test data from 2008 using mini-
mum error rate training (MERT) (Och, 2003). The
results in terms of lower-case BLEU scores are
listed in table 1.
</bodyText>
<table confidence="0.9998977">
BLEU 1 n-gram scores 4
2 3
de-en baseline 21.3 57.4 27.8 15.1 8.6
de-en cache 21.5 58.1 28.1 15.2 8.7
en-de baseline 15.6 52.5 21.7 10.6 5.5
en-de cache 14.4 52.6 21.0 9.9 4.9
es-en baseline 26.7 61.7 32.7 19.9 12.6
es-en cache 26.1 62.6 32.7 19.8 12.5
en-es baseline 26.9 61.5 33.3 20.5 12.9
en-es cache 23.0 60.6 30.4 17.6 10.4
</table>
<tableCaption confidence="0.999969">
Table 1: Results on the WMT10 test set.
</tableCaption>
<bodyText confidence="0.999925666666667">
In the adaptation experiments we applied exactly
the same models using the feature weights from
the baseline with the addition of the caching com-
ponents in both, language models and translation
models. Cache parameters are not particularly
tuned for the task in our initial experiments which
could be one reason for the disappointing results
we obtained. Some of them can be integrated in
the MERT procedure, for example, the interpola-
tion weight of the translation cache. However, tun-
ing these parameters with the standard procedures
appears to be difficult as we will see in later ex-
periments presented in section 3.2. Initially we
used settings that appeared to be useful in previ-
ous experiments. In particular, we used a language
model cache of 10,000 words with a decay of
α = 0.0005 and an interpolation weight of 0.001.
A cache was used in all language models except
the English News model for which caching was
not available (because we did not implement this
feature for RandLM). The translation cache size
was set to 5,000 with a decay factor of 0.001. The
weight for the translation cache was set to 0.001.
Furthermore, we filtered items for the translation
cache using a length constraint of 4 characters or
more and a transition cost threshold (log score) of
-4.
The final results of the adaptive runs are shown
in table 1. In all but one case the cache-based re-
sult is below the baseline which is, of course, quite
disappointing. For German-English a small im-
provement can be observed. However, this may
be rather accidental. In general, it seems that
</bodyText>
<page confidence="0.997173">
191
</page>
<bodyText confidence="0.9966835">
the adaptive approach cannot cope with the noise
added to the cache.
</bodyText>
<subsectionHeader confidence="0.97409">
3.1 Discussion
</subsectionHeader>
<bodyText confidence="0.997020309859155">
There are two important observations that should
be mentioned here. First of all, the adaptive ap-
proach assumes coherent text input. However, the
WMT test-set is composed of many short news
headlines with various topics involved. We, there-
fore, also ran the adaptive approach on individual
news segments. The results are illustrated in figure
2.
Basically, the results do not change compared
to the previous run. Still, cache-based models per-
form worse on average except for the German-
English test-set for which we obtained a slight
but insignificant improvement. Figure 2 plots the
BLEU score differences between standard models
and cached models for the individual news items.
We can see a very blurred picture of these indi-
vidual scores and the general conclusion is that
caching failed. One problem is that the individ-
ual news items are very short (around 20 sentences
each) which is probably too little for caching to
show any positive effect. Surprising, however, is
the negative influence of caching even on these
small documents which is quite similar to the runs
on the entire sets. The drop in performance for
English-Spanish is especially striking. We have
no explanation at this point for this exceptional be-
havior.
A second observation is the variation in individ-
ual n-gram precision scores (see table 1). In all but
one case the unigram precision goes up which in-
dicates that the cache models often improve lexical
choice at least in terms of individual words. The
first example in figure 2 could be seen as a slight
improvement due to a consistent lexical choice of
“missile” (instead of “rocket”).
The main problem, however, in the adaptive ap-
proach seems to appear in local contexts which
might be due to the simplistic language modeling
cache. It would be interesting to study possibilities
of integrating local dependencies into the cache
models. However, there are serious problems with
data sparseness. Initial experiments with a bigram
LM cache did not produce any improvements so
far.
Another crucial problem with the cache-based
model is of course error propagation. An exam-
ple which is probably due to this issue can be seen
baseline until the end of the journey , are , in turn , tech-
nical damage to the rocket.
cache until the end of the journey , in turn , technical
damage to the missile.
reference but near the end of the flight there was technical
damage to the missile.
baseline iran has earlier criticism of its human rights
record.
cache iran rejected previous criticism of its human
rights record.
reference iran has dismissed previous criticism of its hu-
man rights record.
baseline facing conservationists is accused of extortion
cache facing conservationists is accused of extortion
reference Nature protection officers accused of blackmail
baseline the leitmeritz-polizei accused the chairman of
the b¨urgervereinigung ” naturschutzgemein-
schaft leitmeritz ” because of blackmail .
cache the leitmeritz-polizei accused the chairman of
the b¨urgervereinigung ” naturschutzgemein-
schaft leitmeritz ” because of extortion.
reference The Litomerice police have accused the chair-
man of the Litomerice Nature Protection Soci-
ety civil association of blackmail.
</bodyText>
<tableCaption confidence="0.789187">
Table 2: German to English example translations.
</tableCaption>
<bodyText confidence="0.99980425">
in table 2 in the last two translations (propagation
of the translation option “extortion”). This prob-
lem is difficult to get around especially in case
of bad baseline translations. One possible idea
would be to implement a two-pass procedure to
run over the entire input first only to fill the cache
and to identify reliable evidence for certain trans-
lation options (possibly focusing on simple trans-
lation tasks such as short sentences). Then, in the
second pass the adaptive model can be applied to
prefer repetition and consistency according to the
parameters learned in the first pass.
</bodyText>
<subsectionHeader confidence="0.999076">
3.2 Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.9962205">
Another question is if the cache parameters re-
quire careful optimization in order to make this
approach effective. An attempt to investigate the
influence of the cache components by simply vary-
ing the interpolation weights gave us the following
results for English-German (see table 3).
</bodyText>
<table confidence="0.996837285714286">
fixed cache TM parameters fixed cache LM parameters
ALM BLEU ATM BLEU
0.1 14.12 0.1 12.75
0.01 14.39 0.01 13.04
0.005 14.40 0.005 13.57
0.001 14.44 0.001 14.42
0.0005 14.43 0.0005 14.57
</table>
<tableCaption confidence="0.821673666666667">
Table 3: Results for English to German with vary-
ing mixture weights.
Looking at these results the tendency of the scores
</tableCaption>
<page confidence="0.975661">
192
</page>
<figure confidence="0.9967468">
4
4
2 2
0 0
-2 -2
-4 -4
-6
-6
-8
-8
en-de de-en
-10
-10
4
en-es
es-en
4
2 2
0 0
-2 -2
-4 -4
-6 -6
-8 -8
-10
-10
</figure>
<figureCaption confidence="0.9873845">
Figure 2: BLEU score differences between a standard model and a cached model for individual news
segments from the WMT test-set.
</figureCaption>
<bodyText confidence="0.999823222222222">
seems to suggest that switching off caching is the
right thing to do (as one might have expected al-
ready from the initial experimental results). We
did not perform the same type of investigation for
the other language pairs but we expect a similar
behavior.
Even though these results did not encourage us
very much to investigate the possibilities of cache
parameter optimization any further we still tried to
look at the integration of the interpolation weights
into the MERT procedure. The weight of the TM
cache is especially suited for MERT as this com-
ponent is implemented in terms of a separate fea-
ture function within the global log-linear model
used in decoding. The LM mixture model, on
the other hand, is implemented internally within
SRILM and therefore not so straightforward to in-
tegrate into standard MERT. We, therefore, dou-
bled the number of LM’s included in the SMT
model using two standard LM’s and two LM’s
with cache (one for Europarl and one for News
in both cases). The latter are actually mixtures as
well using a fixed interpolation weight of ALM =
0.5 between the cached component and the back-
ground model. In this way the cached LM’s bene-
fit from the smoothing with the static background
model. Individual weights for all four LM’s are
then learned in the global MERT procedure. Un-
fortunately, other cache parameters cannot be op-
timized in this way as they do not produce any par-
ticular values for individual translation hypotheses
in decoding.
We applied this tuning setup to the English-
German translation task and ran MERT on the
same development data as before. Actually,
caching slows down translation quite substantially
which makes MERT very slow. Due to the se-
quential caching procedure it is also not possible
to parallelize tuning. Furthermore, the extra pa-
rameters seem to cause problems in convergence
and we had to stop the optimization after 30 iter-
ations when BLEU scores seemed to start stabi-
lizing around 14.9 (in the standard setup only 12
iterations were required to complete tuning). Un-
fortunately, the result is again quite disappointing
(see table 4).
Actually, the final BLEU score after tuning is even
lower than in our initial runs with fixed cache
parameters taken from previous unrelated exper-
iments. This is very surprising and it looks like
that MERT just failed to find settings close to the
global optimum because of some strong local sub-
optimal points in the search space. One would ex-
pect that it should be possible to obtain at least the
</bodyText>
<page confidence="0.996134">
193
</page>
<table confidence="0.999792">
BLEU on dev-set (no caching) 15.2
BLEU on dev-set (with caching) 14.9
Europarl LM 0.000417
News LM 0.057042
Europarl LM (with cache) 0.002429
News LM (with cache) -0.000604
ATM 0.000749
BLEU on test-set (no caching) 15.6
BLEU on test-set (with caching) 12.7
</table>
<tableCaption confidence="0.999698">
Table 4: Tuning cache parameters.
</tableCaption>
<bodyText confidence="0.999936357142857">
same score on the development set which was not
the case in our experiment. However, as already
mentioned, we had to interrupt tuning and there
is still some chance that MERT would have im-
proved in later iterations. At least intuitively, there
seems to be some logic behind the tuned weights
(shown in table 4). The out-of-domain LM (Eu-
roparl) obtains a higher weight with caching than
without and the in-domain LM (News) is better
without it and, therefore, the cached version ob-
tains a negative weight. Furthermore, the TM
cache weight is quite similar to the one we used in
the initial experiments. However, applying these
settings to the test-set did not work at all.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999986363636364">
In our WMT10 experiments cache-based adaptive
models failed to improve translation quality. Pre-
vious experiments have shown that they can be
useful in adapting SMT models to new domains.
However, they seem to have their limitations in the
general case with mixed topics involved. A gen-
eral problem is error propagation and the corrup-
tion of local dependencies due to over-simplified
cache models. Parameter optimization seems to
be difficult as well. These issues should be inves-
tigated further in future research.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999552490566038">
P.R. Clarkson and A. J. Robinson. 1997. Language
model adaptation using mixtures and an exponen-
tially decaying cache. In International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 799–802, Munich, Germany.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
ACL ’07: Proceedings of the 45th Annual Meeting
of the ACL, pages 177–180, Morristown, NJ, USA.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12(6):570–583.
Laurent Nepveu, Lapalme, Guy, Langlais, Philippe,
and George Foster. 2004. Adaptive Language and
Translation Models for Interactive Machine Trans-
lation. In Proceedings of the 9th Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 190–197, Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 160–167,
Morristown, NJ, USA.
Martin Raab. 2007. Language Modeling for Machine
Translation. VDM Verlag, Saarbr¨ucken, Germany.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on spoken language pro-
cessing (ICSLP 2002), pages 901–904, Denver, CO,
USA.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In ACL ’07: Proceedings of the 45th Annual
Meeting of the ACL, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
J¨org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Recent Advances in Natural Language
Processing, volume V, pages 237–248. John Ben-
jamins, Amsterdam/Philadelphia.
J¨org Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In ACL 2010 Workshop on
Domain Adaptation for Natural Language Process-
ing (DANLP).
</reference>
<page confidence="0.998769">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.7206345">To Cache or not to Cache? Experiments with Adaptive Models in Statistical Machine Translation</title>
<author confidence="0.875539">J¨org</author>
<affiliation confidence="0.9998795">Department of Linguistics and Uppsala University,</affiliation>
<email confidence="0.985771">jorg.tiedemann@lingfil.uu.se</email>
<abstract confidence="0.981033637149028">We report results of our submissions to the WMT 2010 shared translation task in which we applied a system that includes adaptive language and translation models. Adaptation is implemented using exponentially decaying caches storing previous translations as the history for new predictions. Evidence from the cache is then mixed with the global background model. The main problem in this setup is error propagation and our submissions essentially failed to improve over the competitive baseline. There are slight improvements in lexical choice but the global performance decreases in terms of BLEU scores. 1 Motivation The main motivation of our submission was to test the use of adaptive language and translation models in a standard phrase-based SMT setting for the adaptation to wider context beyond sentence boundaries. Adaptive language models have a long tradition in the speech recognition community and various approaches have been proposed to reduce model perplexity in this way. The general task is to adjust statistical models to essential properties of natural language which are usually not captured by standard n-gram models or other local dependency models. First of all, it is known that repetition is very common especially among content words (see, for example, words like “honey”, “milk”, “land” and “flowing” in figure 1). In most cases a repeated occurrence of a content word is much more likely than its first appearance, which is not predicted in this way by a static language model. Secondly, the use of expressions is related to the topic in the current disand the chance of using the same topicrelated expressions again in running text is higher than a mixed-topic model would predict. In translation another phenomenon can be observed, namely the consistency of translations. Polysemous terms are usually not ambiguous in their context and, hence, their translations become consistent according to the contextual sense. Even the choice between synonymous translations is rather consistent in translated texts as we can see in the example of subtitle translations in figure 1 (taken from the OPUS corpus (Tiedemann, 2009)). The 10 commandments Kerd ma lui To some land flowing with milk Till ett land fullt av mj¨olk och where are you going? var ska du? ... Who was that, Vem var det, never tasted har aldrig smakat ... But will sympathy lead us to this land flowing with milk and Men kan sympati leda oss till detta och Figure 1: Repetition and translation consistency Ambiguous terms like “honey” are consistently translated into the Swedish counterpart “honung” (in the sense of the actual substance) or “gumman” (in the metaphoric sense). Observe that this is true even in the latter case where synonymous translations such as “¨alskling” would be possible as well. In other words, deciding to stick to consistent lexical translations should be preferred in MT because the chance of alternative translations in repeated cases is low. Here again, common static translation models do not capture this property at all. In the following we explain our attempt to integrate contextual dependencies using cache-based adaptive models in a standard SMT setup. We have already successfully applied this technique to a domain-adaptation task (Tiedemann, 2010). 189 of the Joint 5th Workshop on Statistical Machine Translation and pages Sweden, 15-16 July 2010. Association for Computational Linguistics Now we would like to investigate the robustness of this model in a more general case where some in-domain training data is available and input data is less repetitive. 2 Cache-based Adaptive Models The basic idea behind cache-based models is to mix a large static background model with a small local model that is dynamically estimated from recent items from the input stream. Dynamic cache language models have been introduced by (Kuhn and Mori, 1990) and are often implemented in the form of linear mixtures: = (1 + The background model is usually a standard ngram model taking limited amount of local context from the history into account and the cache model is often implemented as a simple (unsmoothed) unigram model using the elements stored in a fixed-size cache (100-5000 words) to estimate its parameters. Another improvement can be achieved by making the importance of cached elements a function of recency. This can be done by introducing a decaying factor in the estimation of cache probabilities (Clarkson and Robinson, 1997): This is basically the model that we applied in our experiments as it showed the largest perplexity reduction in our previous experiments on domain adaptation. Similarly, translation models can be adapted as well. This is especially useful to account for translation consistency forcing the decoder to prefer identical translations for repeated terms. In our approach we try to model recency again using a decay factor to compute translation model scores from the cache in the following way (only for language phrases which a translation option exist in the cache; we use a score of zero otherwise): The importance of a cached translation option exponentially decays and we normalize the sum of cached occurrences by the number of translation options with the same foreign language item that we condition on. Plugging this in into a standard phrase-based SMT engine is rather straightforward. The use of cachebased language models in SMT have been investigated before (Raab, 2007). In our case we used Moses as the base decoder (Koehn et al., 2007). The cache-based language model can be integrated in the decoder by simply adjusting the call to the language modeling toolkit appropriately. We implemented the exponentially decaying cache model within the standard SRILM toolkit (Stolcke, 2002) and added command line arguments to Moses to switch to that model and to set cache parameters such as interpolation, cache size and decay. Adding the translation model cache is a bit more tricky. For this we added a new feature function to the global log-linear model and implemented the decaying cache as explained above within the decoder. Again, simple command-line arguments can be used to switch caching on or off and to adjust cache parameters. One important issue is to decide when and what to cache. As we explore a lot of different options in decoding it is not feasible to adapt the cache continuously. This would mean a lot of cache operations trying to add and remove hypotheses from the cache memory. Therefore, we opted for a context model that considers history only from previous sentences. Once decoding is finished translation options from the best hypothesis found in decoding are put into language and translation model cache. This is arguably a strong approximation of the adaptive approach. However, considering our special concern about wider context across sentence boundaries this seems to be a reasonable compromise between completeness and efficiency. Another issue is related to the selection of items to be cached. As discussed earlier repetition is most likely to be found among content words. Similarly, translation consistency is less likely to be true for function words. In the best case one would know the likelihood of specific terms to be repeated. This could be trained on some development data possibly in connection with word classes instead of fully lexicalized parameters in order to overcome data sparseness and to improve Even though this idea is very tempt- 1 Z 1 190 ing it would require a substantial extension of our model and would introduce language and domainspecific parameters. Therefore, we just added a simplistic approach filtering tokens by their length in characters instead. Assuming that longer items are more likely to be content words we simply set a threshold to decide whether to add a term to the cache or not. This threshold can be adjusted using command-line arguments. Finally, we also need to be careful about noise in the cache. This is essential as the caching approach is prone to error propagation. However, detecting noise is difficult. If there would be a notion of noise in translation hypotheses, the decoder would avoid it. In related work (Nepveu et al., 2004) have studied cache-based translation models in connection with interactive machine translation. In that case, one can assume correct input after post-editing the translation suggestions. One way to approach noise reduction in non-interactive MT is to make use of transition costs in the translation lattice. Assuming that this cost (which is estimated internally within the decoder during the expansion of translation hypotheses) refers to some kind of confidence we can discard translation options above a certain threshold, which is what we did in the implementation of our translation model cache. 3 Experiments We followed the setup proposed in the shared translation task. Primarily we concentrated our efforts on German-English (de-en) and English- German (en-de) using the constrained track, i.e. using the provided training and development data from Europarl and the News domain. Later we also added experiments for Spanish (es) and English using a similar setup. Our baseline system incorporates the following components: We trained two separate 5-gram language models for each language with the standard smoothing strategies (interpolation and Kneser- Ney discounting), one for Europarl and one for the News data. All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. We also included two separate translation models, one for the combined Europarl and News data and one for the News data only. They were estimated using the standard tools GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007) applying default settings and lowercased training data. Lexicalized reordering was trained on the combined data set. All baseline models were then tuned on the News test data from 2008 using minimum error rate training (MERT) (Och, 2003). The results in terms of lower-case BLEU scores are listed in table 1. BLEU 1 n-gram scores 4 2 3 de-en baseline 21.3 57.4 27.8 15.1 8.6 de-en cache 21.5 58.1 28.1 15.2 8.7 en-de baseline 15.6 52.5 21.7 10.6 5.5 en-de cache 14.4 52.6 21.0 9.9 4.9 es-en baseline 26.7 61.7 32.7 19.9 12.6 es-en cache 26.1 62.6 32.7 19.8 12.5 en-es baseline 26.9 61.5 33.3 20.5 12.9 en-es cache 23.0 60.6 30.4 17.6 10.4 Table 1: Results on the WMT10 test set. In the adaptation experiments we applied exactly the same models using the feature weights from the baseline with the addition of the caching components in both, language models and translation models. Cache parameters are not particularly tuned for the task in our initial experiments which could be one reason for the disappointing results we obtained. Some of them can be integrated in the MERT procedure, for example, the interpolation weight of the translation cache. However, tuning these parameters with the standard procedures appears to be difficult as we will see in later experiments presented in section 3.2. Initially we used settings that appeared to be useful in previous experiments. In particular, we used a language model cache of 10,000 words with a decay of an interpolation weight of 0.001. A cache was used in all language models except the English News model for which caching was not available (because we did not implement this feature for RandLM). The translation cache size was set to 5,000 with a decay factor of 0.001. The weight for the translation cache was set to 0.001. Furthermore, we filtered items for the translation cache using a length constraint of 4 characters or more and a transition cost threshold (log score) of -4. The final results of the adaptive runs are shown in table 1. In all but one case the cache-based result is below the baseline which is, of course, quite disappointing. For German-English a small improvement can be observed. However, this may be rather accidental. In general, it seems that 191 the adaptive approach cannot cope with the noise added to the cache. 3.1 Discussion There are two important observations that should be mentioned here. First of all, the adaptive approach assumes coherent text input. However, the WMT test-set is composed of many short news headlines with various topics involved. We, therefore, also ran the adaptive approach on individual news segments. The results are illustrated in figure 2. Basically, the results do not change compared to the previous run. Still, cache-based models perform worse on average except for the German- English test-set for which we obtained a slight but insignificant improvement. Figure 2 plots the BLEU score differences between standard models and cached models for the individual news items. We can see a very blurred picture of these individual scores and the general conclusion is that caching failed. One problem is that the individual news items are very short (around 20 sentences each) which is probably too little for caching to show any positive effect. Surprising, however, is the negative influence of caching even on these small documents which is quite similar to the runs on the entire sets. The drop in performance for English-Spanish is especially striking. We have no explanation at this point for this exceptional behavior. A second observation is the variation in individual n-gram precision scores (see table 1). In all but one case the unigram precision goes up which indicates that the cache models often improve lexical choice at least in terms of individual words. The first example in figure 2 could be seen as a slight improvement due to a consistent lexical choice of “missile” (instead of “rocket”). The main problem, however, in the adaptive approach seems to appear in local contexts which might be due to the simplistic language modeling cache. It would be interesting to study possibilities of integrating local dependencies into the cache models. However, there are serious problems with data sparseness. Initial experiments with a bigram LM cache did not produce any improvements so far. Another crucial problem with the cache-based model is of course error propagation. An example which is probably due to this issue can be seen until the end of the journey , are , in turn , technical damage to the rocket. cache until the end of the journey , in turn , damage to the missile. reference but near the end of the flight there was technical damage to the missile. baseline iran has earlier criticism of its human rights record. cache iran rejected previous criticism of its rights record. reference iran has dismissed previous criticism of its human rights record. baseline facing conservationists is accused of extortion cache facing conservationists is accused of extortion reference Nature protection officers accused of blackmail baseline the leitmeritz-polizei accused the chairman the b¨urgervereinigung ” naturschutzgemeinschaft leitmeritz ” because of blackmail . cache the leitmeritz-polizei accused the chairman the b¨urgervereinigung ” naturschutzgemeinschaft leitmeritz ” because of extortion. reference The Litomerice police have accused the chairman of the Litomerice Nature Protection Society civil association of blackmail. Table 2: German to English example translations. in table 2 in the last two translations (propagation of the translation option “extortion”). This problem is difficult to get around especially in case of bad baseline translations. One possible idea would be to implement a two-pass procedure to run over the entire input first only to fill the cache and to identify reliable evidence for certain translation options (possibly focusing on simple translation tasks such as short sentences). Then, in the second pass the adaptive model can be applied to prefer repetition and consistency according to the parameters learned in the first pass. 3.2 Parameter Optimization Another question is if the cache parameters require careful optimization in order to make this approach effective. An attempt to investigate the influence of the cache components by simply varying the interpolation weights gave us the following results for English-German (see table 3). fixed cache TM parameters fixed cache LM parameters BLEU BLEU 0.1 14.12 0.1 12.75 0.01 14.39 0.01 13.04 0.005 14.40 0.005 13.57 0.001 14.44 0.001 14.42 0.0005 14.43 0.0005 14.57 Table 3: Results for English to German with varying mixture weights. Looking at these results the tendency of the scores 192 4 4 2 2 0 0 -2 -2 -4 -4 -6 -6 -8 -8 en-de de-en -10 -10 4 en-es es-en 4 2 2 0 0 -2 -2 -4 -4 -6 -6 -8 -8 -10 -10 Figure 2: BLEU score differences between a standard model and a cached model for individual news segments from the WMT test-set. seems to suggest that switching off caching is the right thing to do (as one might have expected already from the initial experimental results). We did not perform the same type of investigation for the other language pairs but we expect a similar behavior. Even though these results did not encourage us very much to investigate the possibilities of cache parameter optimization any further we still tried to look at the integration of the interpolation weights into the MERT procedure. The weight of the TM cache is especially suited for MERT as this component is implemented in terms of a separate feature function within the global log-linear model used in decoding. The LM mixture model, on the other hand, is implemented internally within SRILM and therefore not so straightforward to integrate into standard MERT. We, therefore, doubled the number of LM’s included in the SMT model using two standard LM’s and two LM’s with cache (one for Europarl and one for News in both cases). The latter are actually mixtures as using a fixed interpolation weight of the cached component and the background model. In this way the cached LM’s benefit from the smoothing with the static background model. Individual weights for all four LM’s are then learned in the global MERT procedure. Unfortunately, other cache parameters cannot be optimized in this way as they do not produce any particular values for individual translation hypotheses in decoding. We applied this tuning setup to the English- German translation task and ran MERT on the same development data as before. Actually, caching slows down translation quite substantially which makes MERT very slow. Due to the sequential caching procedure it is also not possible to parallelize tuning. Furthermore, the extra parameters seem to cause problems in convergence and we had to stop the optimization after 30 iterations when BLEU scores seemed to start stabilizing around 14.9 (in the standard setup only 12 iterations were required to complete tuning). Unfortunately, the result is again quite disappointing (see table 4). Actually, the final BLEU score after tuning is even lower than in our initial runs with fixed cache parameters taken from previous unrelated experiments. This is very surprising and it looks like that MERT just failed to find settings close to the global optimum because of some strong local suboptimal points in the search space. One would expect that it should be possible to obtain at least the</abstract>
<note confidence="0.576271">193 BLEU on dev-set (no caching) BLEU on dev-set (with caching) 15.2 14.9 Europarl LM 0.000417 News LM 0.057042 Europarl LM (with cache) 0.002429 News LM (with cache) -0.000604</note>
<date confidence="0.423467">0.000749</date>
<abstract confidence="0.959803285714286">BLEU on test-set (no caching) 15.6 BLEU on test-set (with caching) 12.7 Table 4: Tuning cache parameters. same score on the development set which was not the case in our experiment. However, as already mentioned, we had to interrupt tuning and there is still some chance that MERT would have improved in later iterations. At least intuitively, there seems to be some logic behind the tuned weights (shown in table 4). The out-of-domain LM (Europarl) obtains a higher weight with caching than without and the in-domain LM (News) is better without it and, therefore, the cached version obtains a negative weight. Furthermore, the TM cache weight is quite similar to the one we used in the initial experiments. However, applying these settings to the test-set did not work at all. 4 Conclusions In our WMT10 experiments cache-based adaptive models failed to improve translation quality. Previous experiments have shown that they can be useful in adapting SMT models to new domains. However, they seem to have their limitations in the general case with mixed topics involved. A general problem is error propagation and the corruption of local dependencies due to over-simplified cache models. Parameter optimization seems to be difficult as well. These issues should be investigated further in future research. References P.R. Clarkson and A. J. Robinson. 1997. Language model adaptation using mixtures and an exponendecaying cache. In Conference on Acoustics, Speech, and Signal Processing pages 799–802, Munich, Germany.</abstract>
<author confidence="0.8270524">Moses open</author>
<abstract confidence="0.643516285714286">source toolkit for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting the pages 177–180, Morristown, NJ, USA. Roland Kuhn and Renato De Mori. 1990. A cachebased natural language model for speech recogni- Transactions on Pattern Analysis and 12(6):570–583.</abstract>
<author confidence="0.533392">Laurent Nepveu</author>
<author confidence="0.533392">Guy Lapalme</author>
<author confidence="0.533392">Philippe Langlais</author>
<title confidence="0.59906625">and George Foster. 2004. Adaptive Language and Translation Models for Interactive Machine Trans- In of the 9th Conference on Empirical Methods in Natural Language Processing</title>
<note confidence="0.89418075">pages 190–197, Barcelona, Spain. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment 29(1):19–51. Franz Josef Och. 2003. Minimum error rate trainin statistical machine translation. In ’03: Proceedings of the 41st Annual Meeting on Associafor Computational pages 160–167,</note>
<address confidence="0.875757">Morristown, NJ, USA.</address>
<note confidence="0.814715166666667">Raab. 2007. Modeling for Machine VDM Verlag, Saarbr¨ucken, Germany. Andreas Stolcke. 2002. SRILM an extensible lanmodeling toolkit. In of the 7th international conference on spoken language pro- (ICSLP pages 901–904, Denver, CO,</note>
<address confidence="0.659157">USA.</address>
<author confidence="0.62263">Randomised</author>
<abstract confidence="0.803661230769231">language modelling for statistical machine transla- In ’07: Proceedings of the 45th Annual of the Prague, Czech Republic. Association for Computational Linguistics. J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and In Advances in Natural Language volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia. J¨org Tiedemann. 2010. Context adaptation in statistical machine translation using models with exponendecaying cache. In 2010 Workshop on Domain Adaptation for Natural Language Process-</abstract>
<intro confidence="0.289805">194</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>A J Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>799--802</pages>
<location>Munich, Germany.</location>
<contexts>
<context position="4944" citStr="Clarkson and Robinson, 1997" startWordPosition="784" endWordPosition="788">mented in the form of linear mixtures: P(wn|history) = (1 − A)Pbackground(wn|history) + APcache(wn|history) The background model is usually a standard ngram model taking limited amount of local context from the history into account and the cache model is often implemented as a simple (unsmoothed) unigram model using the elements stored in a fixed-size cache (100-5000 words) to estimate its parameters. Another improvement can be achieved by making the importance of cached elements a function of recency. This can be done by introducing a decaying factor in the estimation of cache probabilities (Clarkson and Robinson, 1997): Pcache(wn|wn−k..wn−1) � This is basically the model that we applied in our experiments as it showed the largest perplexity reduction in our previous experiments on domain adaptation. Similarly, translation models can be adapted as well. This is especially useful to account for translation consistency forcing the decoder to prefer identical translations for repeated terms. In our approach we try to model recency again using a decay factor to compute translation model scores from the cache in the following way (only for source language phrases fn for which a translation option exist in the cac</context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>P.R. Clarkson and A. J. Robinson. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 799–802, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="6031" citStr="Koehn et al., 2007" startWordPosition="965" endWordPosition="968">odel scores from the cache in the following way (only for source language phrases fn for which a translation option exist in the cache; we use a score of zero otherwise): The importance of a cached translation option exponentially decays and we normalize the sum of cached occurrences by the number of translation options with the same foreign language item that we condition on. Plugging this in into a standard phrase-based SMT engine is rather straightforward. The use of cachebased language models in SMT have been investigated before (Raab, 2007). In our case we used Moses as the base decoder (Koehn et al., 2007). The cache-based language model can be integrated in the decoder by simply adjusting the call to the language modeling toolkit appropriately. We implemented the exponentially decaying cache model within the standard SRILM toolkit (Stolcke, 2002) and added command line arguments to Moses to switch to that model and to set cache parameters such as interpolation, cache size and decay. Adding the translation model cache is a bit more tricky. For this we added a new feature function to the global log-linear model and implemented the decaying cache as explained above within the decoder. Again, simp</context>
<context position="10455" citStr="Koehn et al., 2007" startWordPosition="1698" endWordPosition="1701">lowing components: We trained two separate 5-gram language models for each language with the standard smoothing strategies (interpolation and KneserNey discounting), one for Europarl and one for the News data. All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. We also included two separate translation models, one for the combined Europarl and News data and one for the News data only. They were estimated using the standard tools GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007) applying default settings and lowercased training data. Lexicalized reordering was trained on the combined data set. All baseline models were then tuned on the News test data from 2008 using minimum error rate training (MERT) (Och, 2003). The results in terms of lower-case BLEU scores are listed in table 1. BLEU 1 n-gram scores 4 2 3 de-en baseline 21.3 57.4 27.8 15.1 8.6 de-en cache 21.5 58.1 28.1 15.2 8.7 en-de baseline 15.6 52.5 21.7 10.6 5.5 en-de cache 14.4 52.6 21.0 9.9 4.9 es-en baseline 26.7 61.7 32.7 19.9 12.6 es-en cache 26.1 62.6 32.7 19.8 12.5 en-es baseline 26.9 61.5 33.3 20.5 12</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL, pages 177–180, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato De Mori</author>
</authors>
<title>A cachebased natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>Roland Kuhn and Renato De Mori. 1990. A cachebased natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6):570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nepveu</author>
<author>Guy Lapalme</author>
<author>Philippe Langlais</author>
<author>George Foster</author>
</authors>
<title>Adaptive Language and Translation Models for Interactive Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>190--197</pages>
<location>Barcelona,</location>
<contexts>
<context position="8825" citStr="Nepveu et al., 2004" startWordPosition="1437" endWordPosition="1440">parameters. Therefore, we just added a simplistic approach filtering tokens by their length in characters instead. Assuming that longer items are more likely to be content words we simply set a threshold to decide whether to add a term to the cache or not. This threshold can be adjusted using command-line arguments. Finally, we also need to be careful about noise in the cache. This is essential as the caching approach is prone to error propagation. However, detecting noise is difficult. If there would be a notion of noise in translation hypotheses, the decoder would avoid it. In related work (Nepveu et al., 2004) have studied cache-based translation models in connection with interactive machine translation. In that case, one can assume correct input after post-editing the translation suggestions. One way to approach noise reduction in non-interactive MT is to make use of transition costs in the translation lattice. Assuming that this cost (which is estimated internally within the decoder during the expansion of translation hypotheses) refers to some kind of confidence we can discard translation options above a certain threshold, which is what we did in the implementation of our translation model cache</context>
</contexts>
<marker>Nepveu, Lapalme, Langlais, Foster, 2004</marker>
<rawString>Laurent Nepveu, Lapalme, Guy, Langlais, Philippe, and George Foster. 2004. Adaptive Language and Translation Models for Interactive Machine Translation. In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 190–197, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10424" citStr="Och and Ney, 2003" startWordPosition="1692" endWordPosition="1695">ne system incorporates the following components: We trained two separate 5-gram language models for each language with the standard smoothing strategies (interpolation and KneserNey discounting), one for Europarl and one for the News data. All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. We also included two separate translation models, one for the combined Europarl and News data and one for the News data only. They were estimated using the standard tools GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007) applying default settings and lowercased training data. Lexicalized reordering was trained on the combined data set. All baseline models were then tuned on the News test data from 2008 using minimum error rate training (MERT) (Och, 2003). The results in terms of lower-case BLEU scores are listed in table 1. BLEU 1 n-gram scores 4 2 3 de-en baseline 21.3 57.4 27.8 15.1 8.6 de-en cache 21.5 58.1 28.1 15.2 8.7 en-de baseline 15.6 52.5 21.7 10.6 5.5 en-de cache 14.4 52.6 21.0 9.9 4.9 es-en baseline 26.7 61.7 32.7 19.9 12.6 es-en cache 26.1 62.6 32.7 19.8 12.5 en-es </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10693" citStr="Och, 2003" startWordPosition="1738" endWordPosition="1739">SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. We also included two separate translation models, one for the combined Europarl and News data and one for the News data only. They were estimated using the standard tools GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007) applying default settings and lowercased training data. Lexicalized reordering was trained on the combined data set. All baseline models were then tuned on the News test data from 2008 using minimum error rate training (MERT) (Och, 2003). The results in terms of lower-case BLEU scores are listed in table 1. BLEU 1 n-gram scores 4 2 3 de-en baseline 21.3 57.4 27.8 15.1 8.6 de-en cache 21.5 58.1 28.1 15.2 8.7 en-de baseline 15.6 52.5 21.7 10.6 5.5 en-de cache 14.4 52.6 21.0 9.9 4.9 es-en baseline 26.7 61.7 32.7 19.9 12.6 es-en cache 26.1 62.6 32.7 19.8 12.5 en-es baseline 26.9 61.5 33.3 20.5 12.9 en-es cache 23.0 60.6 30.4 17.6 10.4 Table 1: Results on the WMT10 test set. In the adaptation experiments we applied exactly the same models using the feature weights from the baseline with the addition of the caching components in bo</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Raab</author>
</authors>
<title>Language Modeling for Machine Translation.</title>
<date>2007</date>
<publisher>VDM Verlag,</publisher>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="5963" citStr="Raab, 2007" startWordPosition="953" endWordPosition="954"> recency again using a decay factor to compute translation model scores from the cache in the following way (only for source language phrases fn for which a translation option exist in the cache; we use a score of zero otherwise): The importance of a cached translation option exponentially decays and we normalize the sum of cached occurrences by the number of translation options with the same foreign language item that we condition on. Plugging this in into a standard phrase-based SMT engine is rather straightforward. The use of cachebased language models in SMT have been investigated before (Raab, 2007). In our case we used Moses as the base decoder (Koehn et al., 2007). The cache-based language model can be integrated in the decoder by simply adjusting the call to the language modeling toolkit appropriately. We implemented the exponentially decaying cache model within the standard SRILM toolkit (Stolcke, 2002) and added command line arguments to Moses to switch to that model and to set cache parameters such as interpolation, cache size and decay. Adding the translation model cache is a bit more tricky. For this we added a new feature function to the global log-linear model and implemented t</context>
</contexts>
<marker>Raab, 2007</marker>
<rawString>Martin Raab. 2007. Language Modeling for Machine Translation. VDM Verlag, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th international conference on spoken language processing (ICSLP</booktitle>
<pages>901--904</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="6277" citStr="Stolcke, 2002" startWordPosition="1003" endWordPosition="1004">ize the sum of cached occurrences by the number of translation options with the same foreign language item that we condition on. Plugging this in into a standard phrase-based SMT engine is rather straightforward. The use of cachebased language models in SMT have been investigated before (Raab, 2007). In our case we used Moses as the base decoder (Koehn et al., 2007). The cache-based language model can be integrated in the decoder by simply adjusting the call to the language modeling toolkit appropriately. We implemented the exponentially decaying cache model within the standard SRILM toolkit (Stolcke, 2002) and added command line arguments to Moses to switch to that model and to set cache parameters such as interpolation, cache size and decay. Adding the translation model cache is a bit more tricky. For this we added a new feature function to the global log-linear model and implemented the decaying cache as explained above within the decoder. Again, simple command-line arguments can be used to switch caching on or off and to adjust cache parameters. One important issue is to decide when and what to cache. As we explore a lot of different options in decoding it is not feasible to adapt the cache </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the 7th international conference on spoken language processing (ICSLP 2002), pages 901–904, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10178" citStr="Talbot and Osborne, 2007" startWordPosition="1649" endWordPosition="1652">nglish (de-en) and EnglishGerman (en-de) using the constrained track, i.e. using the provided training and development data from Europarl and the News domain. Later we also added experiments for Spanish (es) and English using a similar setup. Our baseline system incorporates the following components: We trained two separate 5-gram language models for each language with the standard smoothing strategies (interpolation and KneserNey discounting), one for Europarl and one for the News data. All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. We also included two separate translation models, one for the combined Europarl and News data and one for the News data only. They were estimated using the standard tools GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007) applying default settings and lowercased training data. Lexicalized reordering was trained on the combined data set. All baseline models were then tuned on the News test data from 2008 using minimum error rate training (MERT) (Och, 2003). The results in terms of lower-case BLEU scores are listed in table 1. BLEU 1 n-gram</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<publisher>John Benjamins, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="2376" citStr="Tiedemann, 2009" startWordPosition="376" endWordPosition="377">s is related to the topic in the current discourse and the chance of using the same topicrelated expressions again in running text is higher than a mixed-topic model would predict. In translation another phenomenon can be observed, namely the consistency of translations. Polysemous terms are usually not ambiguous in their context and, hence, their translations become consistent according to the contextual sense. Even the choice between synonymous translations is rather consistent in translated texts as we can see in the example of subtitle translations in figure 1 (taken from the OPUS corpus (Tiedemann, 2009)). The 10 commandments Kerd ma lui To some land flowing with milk Mari honey ... and honey! Mari, gumman Till ett land fullt av mj¨olk och Sweetheart, honung. where are you going? ¨Alskling, var ska du? ... Who was that, honey? Vem var det, gumman? I’ve never tasted honey. Jag har aldrig smakat honung. ... But will sympathy lead us to this land flowing with milk and honey? Men kan sympati leda oss till detta mj¨olkens och honungens land? Figure 1: Repetition and translation consistency Ambiguous terms like “honey” are consistently translated into the Swedish counterpart “honung” (in the sense </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Context adaptation in statistical machine translation using models with exponentially decaying cache.</title>
<date>2010</date>
<booktitle>In ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP).</booktitle>
<contexts>
<context position="3650" citStr="Tiedemann, 2010" startWordPosition="583" endWordPosition="584">nse). Observe that this is true even in the latter case where synonymous translations such as “¨alskling” would be possible as well. In other words, deciding to stick to consistent lexical translations should be preferred in MT because the chance of alternative translations in repeated cases is low. Here again, common static translation models do not capture this property at all. In the following we explain our attempt to integrate contextual dependencies using cache-based adaptive models in a standard SMT setup. We have already successfully applied this technique to a domain-adaptation task (Tiedemann, 2010). 189 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 189–194, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Now we would like to investigate the robustness of this model in a more general case where some in-domain training data is available and input data is less repetitive. 2 Cache-based Adaptive Models The basic idea behind cache-based models is to mix a large static background model with a small local model that is dynamically estimated from recent items from the input stream. Dynamic cache language models</context>
</contexts>
<marker>Tiedemann, 2010</marker>
<rawString>J¨org Tiedemann. 2010. Context adaptation in statistical machine translation using models with exponentially decaying cache. In ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>