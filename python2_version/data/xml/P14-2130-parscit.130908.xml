<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003156">
<title confidence="0.995884">
Incremental Predictive Parsing with TurboParser
</title>
<author confidence="0.938954">
Arne K¨ohn and Wolfgang Menzel
</author>
<affiliation confidence="0.581322">
Fachbereich Informatik
Universit¨at Hamburg
</affiliation>
<email confidence="0.981754">
{koehn, menzel}@informatik.uni-hamburg.de
</email>
<sectionHeader confidence="0.993431" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921">
Most approaches to incremental parsing
either incur a degradation of accuracy or
they have to postpone decisions, yield-
ing underspecified intermediate output. We
present an incremental predictive depen-
dency parser that is fast, accurate, and
largely language independent. By extend-
ing a state-of-the-art dependency parser,
connected analyses for sentence prefixes
are obtained, which even predict properties
and the structural embedding of upcoming
words. In contrast to other approaches, ac-
curacy for complete sentence analyses does
not decrease.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919823529412">
When humans communicate by means of a natu-
ral language, utterances are not produced at once
but evolve over time. Human interaction benefits
from this property by processing yet unfinished
utterances and reacting on them. Computational
parsing on the other hand is mostly performed on
complete sentences, a processing mode which ren-
ders a responsive interaction based on incomplete
utterances impossible.
When spoken language is analyzed, a mismatch
between speech recognition and parsing occurs:
If parsing does not work incrementally, the over-
all system loses all the desirable properties made
possible by incremental processing. For speech di-
alogue systems, this leads to increased reaction
times and an unnatural ping-pong style of interac-
tion (Schlangen and Skantze, 2011).
</bodyText>
<subsectionHeader confidence="0.986093">
1.1 Desirable features of incremental parsers
</subsectionHeader>
<bodyText confidence="0.999945793103448">
Dependency parsing assigns a head and a depen-
dency label to each word form of an input sentence
and the resulting analysis of the sentence is usually
required to form a tree. An incremental dependency
parser processes a sentence word by word, building
analyses for sentence prefixes (partial dependency
analyses, PDA), which are extended and modified
in a piecemeal fashion as more words become avail-
able.
A PDA should come with three important (but
partly contradictory) properties: beyond being ac-
curate, it should also be as stable and informative as
possible. Stability can be measured as the amount
of structure (attachments and their labels) of a PDA
ai which is also part of the analysis an of the whole
sentence. To be maximally informative, at least all
available word forms should be integrated into the
prefix PDA. Even such a simple requirement cannot
easily be met without predicting a structural skele-
ton for the word forms in the upcoming part of the
sentence(bottom-up prediction). Other predictions
merely serve to satisfy completeness conditions
(i.e. valency requirements) in an anticipatory way
(top-down predictions). In fact, humans are able to
derive such predictions and they do so during sen-
tence comprehension (Sturt and Lombardo, 2005).
Without prediction, the sentence prefix “John
drives a” of “John drives a car” can only be parsed
as a disconnected structure:
</bodyText>
<subsectionHeader confidence="0.904437">
John drives a
</subsectionHeader>
<bodyText confidence="0.999987538461539">
The determiner remains unconnected to the rest of
the sentence, because a possible head is not yet
available. However, the determiner could be inte-
grated into the PDA if the connection is established
by means of a predicted word form, which has not
yet been observed. Beuck et al. (2011) propose to
use virtual nodes (VNs) for this purpose. Each VN
represents exactly one upcoming word. Its lexical
instantiation and its exact position remain unspeci-
fied. Using a VN, the prefix “John drives a” could
then be parsed as follows, creating a fully con-
nected analysis, which also satisfies the valency
requirements of the finite verb.
</bodyText>
<page confidence="0.982482">
803
</page>
<bodyText confidence="0.8256072">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 803–808,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
John drives a [VirtNoun]
This analysis is clearly more informative but still
restricted to the existence of a noun filling the ob-
ject role of ”drives” without predicting its position.
Although a VN does not specify the lexical identity
of the word form it represents, it can nonetheless
carry some information such as a coarse-grained
part-of-speech category.
</bodyText>
<sectionHeader confidence="0.90944" genericHeader="introduction">
1.2 Related work
</sectionHeader>
<bodyText confidence="0.999898628571429">
Parsers that produce incremental output are rel-
atively rare: PLTag (Demberg-Winterfors, 2010)
aims at psycholinguistic plausibility. It makes trade-
offs in the field of accuracy and coverage (they
report 6.2 percent of unparseable sentences on sen-
tences of the Penn Treebank with less than 40
words). Due to its use of beam search, the incre-
mental results are non-monotonic. Hassan et al.
(2009) present a CCG-based parser that can parse
in an incremental mode. The parser guarantees
that every parse of an increment extends the previ-
ous parse monotonically. However, using the incre-
mental mode without look-ahead, parsing accuracy
drops from 86.70% to 59.01%. Obviously, insist-
ing on strict monotonicity (ai C an) is too strong
a requirement, since it forces the parser to keep
attachments that later turn out to be clearly wrong
in light of new evidence.
Being a transition-based parser, Maltparser
(Nivre et al., 2007) does incremental parsing by
design. It is, however, not able to predict upcom-
ing structure and therefore its incremental output is
usually fragmented into several trees. In addition,
Maltparser needs a sufficiently large look-ahead to
achieve high accuracy (Beuck et al., 2011).
Beuck et al. (2011) introduced incremental and
predictive parsing using Weighted Constraint De-
pendency Grammar. While their approach does not
decrease in incremental mode, it is much slower
than most other parsers. Another disadvantage is
its hand-written grammar which prevents the parser
from being adapted to additional languages by sim-
ply training it on an annotated corpus and which
makes it difficult to derive empirically valid con-
clusions from the experimental results.
</bodyText>
<sectionHeader confidence="0.981306" genericHeader="method">
2 Challenges for predictive parsing
</sectionHeader>
<bodyText confidence="0.993809941176471">
Extending a dependency parser to incremental pars-
ing with VNs introduces a significant shift in the
problem to be solved: While originally the problem
was where to attach each word to (1), in the incre-
mental case the additional problem arises, which
VNs to include into the analysis (2). Problem (2),
however, depends on the syntactic structure of the
sentence prefix. Therefore, it is not possible to de-
termine the VNs before parsing commences, but
the decision has to be made while parsing is going
on. We can resolve this issue by transforming prob-
lem (2) into problem (1) by providing the parser
with an additional node, named unused. It is always
attached to the special node 0 (the root node of ev-
ery analysis) and it can only dominate VNs. unused
and every VN it dominates are not considered part
of the analysis. Using this idea, the problem of
whether a VN should be included into the analysis
is now reduced to the problem of where to attach
that VN:
John drives a [VirtNoun] [VirtVerb] [unused]
To enable the parser to include VNs into PDAs,
a set of VNs has to be provided. While this set
could include any number of VNs, we only in-
clude a set that covers most cases of prediction
since rare virtual nodes have a very low a-priori
probability of being included and additional VNs
make the parsing problem more complex. This set
is language-dependent and has to be determined in
advance. It can be obtained by generating PDAs
from a treebank and counting the occurrences of
VNs in them. Eventually, a set of VNs is used that
is a super-set of a large enough percentage (&gt; 90%)
of the observed sets.
</bodyText>
<sectionHeader confidence="0.872933" genericHeader="method">
3 Gold annotations for sentence prefixes
</sectionHeader>
<bodyText confidence="0.9999909375">
Annotating sentence prefixes by hand is pro-
hibitively costly because the number of increments
is a multitude of the number of sentences in the
corpus. Beuck and Menzel (2013) propose an ap-
proach to automatically generate predictive depen-
dency analyses from the annotation of full sen-
tences. Their method tries to generate upper bounds
for predictability which are relatively tight. There-
fore, not everything that is deemed predictable by
the algorithm is predictable in reality, but every-
thing that is predictable should be deemed as pre-
dictable: Let W be all tokens of the sentence and P
the set of tokens that lie in the prefix for which an
incremental analysis should be generated. A word
w E W \ P is assumed to be predictable (w E Pr) if
one of the following three criteria is met:
</bodyText>
<page confidence="0.989788">
804
</page>
<figure confidence="0.989949909090909">
German English
percentage 100 percentage 100
80 80
60 60
40 40
0 1 2 3 4 5 final 0 1 2 3 4 5 final
relative time point relative time point
wrong structural prediction
wrong
correct
correct structural prediction
</figure>
<figureCaption confidence="0.999976">
Figure 1: Results for TurboParser for German and English with gold standard PoS (labeled)
</figureCaption>
<bodyText confidence="0.987797333333333">
bottom-up prediction w lies on the path from
some w� E P to 0. E. g., given the sentence prefix
“The”, an upcoming noun and a verb is predicted:
The [VirtNoun] [VirtVerb]
top down prediction tc(w), the head of w, is in
P U Pr, and w fills a syntactic role – encoded by its
dependency label – that is structurally determined.
That means w can be predicted independently of
the lexical identity of tc(w). An example for this
is the subject label: If tc(w) is in Pr and w is its
subject, w is assumed to be predictable.
lexical top-down prediction tc(w) E P and w
fills a syntactic role that is determined by an already
observed lexical item, e.g. the object role: If tc(w)
is a known verb and w is its object, w E Pr because
it is required by a valency of the verb.
While this procedure is language-independent,
some language-specific transformations must be
applied nonetheless. For English, parts of gapping
coordinations can be predicted whereas others can
not. For German, the transformations described in
(Beuck and Menzel, 2013) have been used with-
out further changes. Both sets of structurally and
lexically determined roles are language dependent.
The label sets for German have been adopted from
(Beuck and Menzel, 2013), while the sets for En-
glish have been obtained by manually analyzing
the PTB (Marcus et al., 1994) for predictability.
For words marked as predictable their existence
and word class, but not their lexicalization and
position can be predicted. Therefore, we replace
the lexical item with “[virtual]” and generalize the
part-of-speech tag to a more coarse grained one.
</bodyText>
<sectionHeader confidence="0.981107" genericHeader="method">
4 Predictive parsing with TurboParser
</sectionHeader>
<bodyText confidence="0.99217452173913">
We adapt TurboParser (Martins et al., 2013) for
incremental parsing because it does not impose
structural constraints such as single-headedness in
its core algorithm. For each parsing problem, it
creates an integer linear program – in the form of a
factor graph – with the variables representing the
possible edges of the analyses.
Since well-formedness is enforced by factors,
additional constraints on the shape of analyses can
be imposed without changing the core algorithm of
the parser. We define three additional restrictions
with respect to VNs: 1) A VN that is attached to
unused may not have any dependents. 2) A VN
may not be attached to 0 if it has no dependents. 3)
Only VNs may be attached to the unused node.
For a given sentence prefix, let A be the set of
possible edges, V the set of all vertices, N C V
the VNs and u E V the unused node. Moreover, let
B C A be the set of edges building a well-formed
analysis and za °= I(a E B), where I(.) is the indica-
tor function. The three additional conditions can be
expressed as linear constraints which ensure that
every output is a valid PDA:
</bodyText>
<equation confidence="0.996972">
z(n, j) +z(u,n) &lt; 1, n E N, j E V (1)
z(0,n) &lt; ∑ z(n,j), n E N (2)
jEV
z(u,i) = 0, i E V \N (3)
</equation>
<bodyText confidence="0.999943375">
The current implementation is pseudo-incremen-
tal. It reinitializes the ILP for every increment with-
out passing intermediate results from one incremen-
tal processing step to the next, although this might
be an option for further optimization.
High quality incremental parsing results can not
be expected from models which have only been
trained on whole-sentence annotations. If a parser
is trained on gold-standard PDAs (generated as de-
scribed in section 3), it would include every VN
into every analysis because that data does not in-
clude any non-attached VNs. We therefore add non-
attached VNs to the generated PDAs until they
contain at least the set of VNs that is later used
during parsing. For instance, each German training
increment contains at least one virtual verb and
</bodyText>
<page confidence="0.996695">
805
</page>
<bodyText confidence="0.953944464285714">
two virtual nouns and each English one at least one
virtual verb and one virtual noun. This way, the per-
centage of VNs of a specific type being attached in
the training data resembles the a priori probability
that a VN of that type should be included by the
parser while parsing.
TurboParser is trained on these extended PDAs
and no adaptation of the training algorithm is
needed. The training data is heavily skewed be-
cause words at the beginning of the sentences are
more prevalent than the ones at the end. As a com-
parison with a version trained on non-incremental
data shows, this has no noticeable effect on the
parsing quality.
We define a word w as correctly attached (ignor-
ing the label) if π(φ(w)) = φ(π(w)). In an incre-
mental analysis, an attachment of a word w can be
classified into four cases:
correct π(φ(w)) = φ(π(w)), π(w) E V&apos;p
corr. pred. π(φ(w)) = φ(π(w)), π(w) E V&apos;v
wrong pred. π(φ(w)) =� φ(π(w)), π(w) E V&apos;v
wrong π(φ(w)) =� φ(π(w)), π(w) E V&apos;p
We can count the number of VNs that have been
correctly attached: Let T be the set of all analyses
produced by the parser and φt the best mapping as
defined above for each t E T. Furthermore, let vn(t)
be the set of VNs in t. The total number of correct
predictions of VNs is then defined as:
</bodyText>
<equation confidence="0.987974">
5 Evaluation corr = ∑ ∑ I(π(φt(v)) = φt(π(v)))
tET vEvn(t)
</equation>
<bodyText confidence="0.999975741935484">
The usual methods to determine the quality of a
dependency parser – labeled and unlabeled attach-
ment scores (AS) – are not sufficient for the evalu-
ation of incremental parsers. If the AS is computed
for whole sentences, all incremental output is dis-
carded and not considered at all. If every intermedi-
ate PDA is used, words at the start of a sentence are
counted more often than the ones at the end. No in-
formation becomes available on how the accuracy
of attachments evolves while parsing proceeds, and
the prediction quality (i.e. the VNs) is completely
ignored. Therefore, we adopt the enhanced mode
of evaluation proposed by Beuck et al. (2013): In
addition to the accuracy for whole sentences, the
accuracies of the n newest words of each analy-
sis are computed. This yields a curve that shows
how good a word can be assumed to be attached
depending on its distance to the most recent word.
Let (V,G) be the gold standard analysis of an
increment and (V&apos;,P) the corresponding parser out-
put. V and V&apos; are the vertices and G and P the
respective edges of the analyses. Let V&apos;p and V&apos;v be
the in-prefix and virtual subset of V&apos;, respectively.
To evaluate the prediction capabilities of a parser,
for each increment an optimal partial, surjective
mapping1 V&apos; —* V from the output produced by the
parser to the (automatically generated) gold stan-
dard is computed, where each non-virtual element
of V&apos; has to be mapped to the corresponding ele-
ment in V. Let M be the set of all such mappings.
Then the best mapping is defined as follows:
</bodyText>
<equation confidence="0.7285605">
φ = argmax ∑ I(π(m(w)) = m(π(w)))
mEM wEV&apos;
</equation>
<footnote confidence="0.98375">
1The mapping is partial because for some VNs in V&apos; there
might be no corresponding VN in the gold standard.
</footnote>
<bodyText confidence="0.9997282">
Precision and recall for the prediction with VNs
can be computed by dividing corr by the number
of predicted VNs and the number of VNs in the
gold standard, respectively.
Evaluation has been carried out on the PTB con-
verted to dependency structure using the LTH con-
verter (Johansson and Nugues, 2007) and on the
Hamburg Dependency Treebank (Foth et al., 2014).
From both corpora predictive PDAs padded with
unused virtual nodes have been created for training.
For English, the sentences of part 1-9 of the PTB
were used, for German the first 50,000 sentences
of the HDT have been selected. Testing was done
using one virtual noun and one virtual verb for En-
glish and two virtual nouns and one virtual verb for
German because these sets cover about 90% of the
prefixes in both training sets.
Figure 1 shows the evaluation results for pars-
ing German and English using TurboParser. For
both languages the attachment accuracy rises with
the amount of context available. The difference be-
tween the attachment accuracy of the most recent
word (relative time point 0, no word to the right
of it) and the second newest word (time point 1)
is strongest, especially for English. The word five
elements left of the newest word (time point 5) gets
attached with an accuracy that is nearly as high as
the accuracy for the whole sentence (final).
The types of errors made for German and En-
glish are similar. For both German and English the
unlabeled precision reaches more than 70% (see
Table 1). Even the correct dependency label of up-
coming words can be predicted with a fairly high
precision. TurboParser parses an increment in about
0.015 seconds, which is much faster than WCDG
</bodyText>
<page confidence="0.996216">
806
</page>
<table confidence="0.732665333333333">
TurboParser jwcdg
percentage 100 percentage 100
80 80
60 60
40 40
0 1 2 3 4 5 final 0 1 2 3 4 5 final
relative time point relative time point
Figure 2: Results for TurboParser and jwcdg for German with tagger (labeled).
English German German&amp;tagger German (jwcdg)
labeled unlabeled labeled unlabeled
precision 75.47% 78.55% 67.42% 75.90%
recall 57.92% 60,29% 46.77% 52.65%
labeled unlabeled labeled unlabeled
65.21% 73.39% 32.95% 42.23%
45.79% 51.54% 35.90% 46.00%
</table>
<tableCaption confidence="0.998325">
Table 1: Precision and recall for the prediction of virtual nodes
</tableCaption>
<table confidence="0.9976415">
time point 0 time point 5
unlabeled labeled unlabeled labeled
En 89.28% 84.92% 97.32% 97.11%
De 90.91% 88.96% 96.11% 95.65%
</table>
<tableCaption confidence="0.998288">
Table 2: Stability measures
</tableCaption>
<bodyText confidence="0.9999935">
where about eight seconds per word are needed to
achieve a good accuracy (K¨ohn and Menzel, 2013).
The prediction recall is higher for English than for
German which could be due to the differences in
gold-standard annotation.
Training TurboParser on the non-incremental
data sets results in a labeled whole-sentence accu-
racy of 93.02% for German.The whole-sentence
accuracy for parsing with VNs is 93.33%. This
shows that the additional mechanism of VNs has
no negative effects on the overall parsing quality.
To compare TurboParser and WCDG running
both in the predictive incremental mode, we use
jwcdg, the current implementation of this approach.
jwcdg differs from most other parsers in that it
does not act on pre-tagged data but runs an exter-
nal tagger itself in a multi-tag mode. To compare
both systems, TurboParser needs to be run in a
tagger-parser pipeline. We have chosen TurboTag-
ger without look-ahead for this purpose. Running
TurboParser in this pipeline leads to only slightly
worse results compared to the use of gold-standard
tags (see Figure 2). TurboParser’s attachment ac-
curacy is about ten percentage points better than
jwcdg’s across the board. In addition, its VN pre-
diction is considerably better.
To measure the stability, let Pi be a prefix of the
sentence P„ and ai and a„ be the corresponding
analyses produced by the parser. An attachment
of a word w ∈ Pi is stable if either w’s head is the
same in ai and a„ or w’s head is not part of Pi in
both ai and a„. The second part covers the case
where the parser predicts the head of w to lie in the
future and it really does, according to the final parse.
Table 2 shows the attachment stability of the newest
word at time point 0 compared to the word five
positions to the left of time point 0. TurboParser’s
stability turns out to be much higher than jwcdg’s:
For German Beuck et al. (2013) report a stability
of only 80% at the most recent word. Interestingly,
labeling the newest attachment for English seems
to be much harder than for German.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999797952380952">
Using a parser based on ILP, we were able to an-
alyze sentences incrementally and produce con-
nected dependency analyses at every point in time.
The intermediate structures produced by the parser
are highly informative, including predictions for
properties and structural embeddings of upcom-
ing words. In contrast to previous approaches, we
achieve state-of-the-art accuracy for whole sen-
tences by abandoning strong monotonicity and aim
at high stability instead, allowing the parser to im-
prove intermediate results in light of new evidence.
The parser is trained on treebank data for whole
sentences from which prefix annotations are de-
rived in a fully automatic manner. To guide this
process, a specification of structurally and lexically
determined dependency relations and some addi-
tional heuristics are needed. For parsing, only a set
of possible VNs has to be provided. These are the
only language specific components required. There-
fore, the approach can be ported to other languages
with quite modest effort.
</bodyText>
<page confidence="0.995627">
807
</page>
<sectionHeader confidence="0.989994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836117647059">
Niels Beuck and Wolfgang Menzel. 2013. Structural
prediction in incremental dependency parsing. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7816 of
Lecture Notes in Computer Science, pages 245–257.
Springer Berlin Heidelberg.
Niels Beuck, Arne K¨ohn, and Wolfgang Menzel. 2011.
Incremental parsing and the evaluation of partial de-
pendency analyses. In Proceedings of the 1st In-
ternational Conference on Dependency Linguistics.
Depling 2011.
Niels Beuck, Arne K¨ohn, and Wolfgang Menzel. 2013.
Predictive incremental parsing and its evaluation. In
Kim Gerdes, Eva Hajiˇcov´a, and Leo Wanner, editors,
Computational Dependency Theory, volume 258 of
Frontiers in Artificial Intelligence and Applications,
pages 186 – 206. IOS press.
Vera Demberg-Winterfors. 2010. A Broad-Coverage
Model of Prediction in Human Sentence Processing.
Ph.D. thesis, University of Edinburgh.
Kilian A. Foth, Niels Beuck, Arne K¨ohn, and Wolfgang
Menzel. 2014. The Hamburg Dependency Tree-
bank. In Proceedings of the Language Resources
and Evaluation Conference 2014. LREC, European
Language Resources Association (ELRA).
Hany Hassan, Khalil Sima’an, and Andy Way. 2009.
Lexicalized semi-incremental dependency parsing.
In Proceedings of the International Conference
RANLP-2009, pages 128–134, Borovets, Bulgaria,
September. Association for Computational Linguis-
tics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for En-
glish. In Proceedings of NODALIDA 2007, pages
105–112, Tartu, Estonia, May 25-26.
Arne K¨ohn and Wolfgang Menzel. 2013. Incremental
and predictive dependency parsing under real-time
conditions. In Proceedings of the International Con-
ference Recent Advances in Natural Language Pro-
cessing RANLP 2013, pages 373–381, Hissar, Bul-
garia, September. INCOMA Ltd. Shoumen, BUL-
GARIA.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate
argument structure. In Proceedings of the Workshop
on Human Language Technology, HLT ’94, pages
114–119, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617–622, Sofia, Bulgaria, August.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Davin Schlangen and Gabriel Skantze. 2011. A gen-
eral, abstract model of incremental dialogue process-
ing. Dialogue and Discourse, 2(1):83–111.
Patrick Sturt and Vincenzo Lombardo. 2005. Process-
ing coordinated structures: Incrementality and con-
nectedness. Cognitive Science, 29(2):291–305.
</reference>
<page confidence="0.997497">
808
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.222624">
<title confidence="0.996497">Incremental Predictive Parsing with TurboParser</title>
<author confidence="0.956682">Arne K¨ohn</author>
<author confidence="0.956682">Wolfgang</author>
<affiliation confidence="0.3239345">Fachbereich Universit¨at</affiliation>
<abstract confidence="0.998077">Most approaches to incremental parsing either incur a degradation of accuracy or they have to postpone decisions, yielding underspecified intermediate output. We present an incremental predictive dependency parser that is fast, accurate, and largely language independent. By extending a state-of-the-art dependency parser, connected analyses for sentence prefixes are obtained, which even predict properties and the structural embedding of upcoming words. In contrast to other approaches, accuracy for complete sentence analyses does not decrease.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Niels Beuck</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Structural prediction in incremental dependency parsing.</title>
<date>2013</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>7816</volume>
<pages>245--257</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="7729" citStr="Beuck and Menzel (2013)" startWordPosition="1225" endWordPosition="1228">e virtual nodes have a very low a-priori probability of being included and additional VNs make the parsing problem more complex. This set is language-dependent and has to be determined in advance. It can be obtained by generating PDAs from a treebank and counting the occurrences of VNs in them. Eventually, a set of VNs is used that is a super-set of a large enough percentage (&gt; 90%) of the observed sets. 3 Gold annotations for sentence prefixes Annotating sentence prefixes by hand is prohibitively costly because the number of increments is a multitude of the number of sentences in the corpus. Beuck and Menzel (2013) propose an approach to automatically generate predictive dependency analyses from the annotation of full sentences. Their method tries to generate upper bounds for predictability which are relatively tight. Therefore, not everything that is deemed predictable by the algorithm is predictable in reality, but everything that is predictable should be deemed as predictable: Let W be all tokens of the sentence and P the set of tokens that lie in the prefix for which an incremental analysis should be generated. A word w E W \ P is assumed to be predictable (w E Pr) if one of the following three crit</context>
<context position="9679" citStr="Beuck and Menzel, 2013" startWordPosition="1572" endWordPosition="1575">mple for this is the subject label: If tc(w) is in Pr and w is its subject, w is assumed to be predictable. lexical top-down prediction tc(w) E P and w fills a syntactic role that is determined by an already observed lexical item, e.g. the object role: If tc(w) is a known verb and w is its object, w E Pr because it is required by a valency of the verb. While this procedure is language-independent, some language-specific transformations must be applied nonetheless. For English, parts of gapping coordinations can be predicted whereas others can not. For German, the transformations described in (Beuck and Menzel, 2013) have been used without further changes. Both sets of structurally and lexically determined roles are language dependent. The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al., 1994) for predictability. For words marked as predictable their existence and word class, but not their lexicalization and position can be predicted. Therefore, we replace the lexical item with “[virtual]” and generalize the part-of-speech tag to a more coarse grained one. 4 Predictive parsing with TurboParser</context>
</contexts>
<marker>Beuck, Menzel, 2013</marker>
<rawString>Niels Beuck and Wolfgang Menzel. 2013. Structural prediction in incremental dependency parsing. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 7816 of Lecture Notes in Computer Science, pages 245–257. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Beuck</author>
<author>Arne K¨ohn</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Incremental parsing and the evaluation of partial dependency analyses.</title>
<date>2011</date>
<booktitle>In Proceedings of the 1st International Conference on Dependency Linguistics. Depling</booktitle>
<marker>Beuck, K¨ohn, Menzel, 2011</marker>
<rawString>Niels Beuck, Arne K¨ohn, and Wolfgang Menzel. 2011. Incremental parsing and the evaluation of partial dependency analyses. In Proceedings of the 1st International Conference on Dependency Linguistics. Depling 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Beuck</author>
<author>Arne K¨ohn</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Predictive incremental parsing and its evaluation.</title>
<date>2013</date>
<booktitle>Computational Dependency Theory,</booktitle>
<volume>258</volume>
<pages>186--206</pages>
<editor>In Kim Gerdes, Eva Hajiˇcov´a, and Leo Wanner, editors,</editor>
<publisher>IOS press.</publisher>
<marker>Beuck, K¨ohn, Menzel, 2013</marker>
<rawString>Niels Beuck, Arne K¨ohn, and Wolfgang Menzel. 2013. Predictive incremental parsing and its evaluation. In Kim Gerdes, Eva Hajiˇcov´a, and Leo Wanner, editors, Computational Dependency Theory, volume 258 of Frontiers in Artificial Intelligence and Applications, pages 186 – 206. IOS press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg-Winterfors</author>
</authors>
<title>A Broad-Coverage Model of Prediction in Human Sentence Processing.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4283" citStr="Demberg-Winterfors, 2010" startWordPosition="649" endWordPosition="650">n for Computational Linguistics (Short Papers), pages 803–808, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics John drives a [VirtNoun] This analysis is clearly more informative but still restricted to the existence of a noun filling the object role of ”drives” without predicting its position. Although a VN does not specify the lexical identity of the word form it represents, it can nonetheless carry some information such as a coarse-grained part-of-speech category. 1.2 Related work Parsers that produce incremental output are relatively rare: PLTag (Demberg-Winterfors, 2010) aims at psycholinguistic plausibility. It makes tradeoffs in the field of accuracy and coverage (they report 6.2 percent of unparseable sentences on sentences of the Penn Treebank with less than 40 words). Due to its use of beam search, the incremental results are non-monotonic. Hassan et al. (2009) present a CCG-based parser that can parse in an incremental mode. The parser guarantees that every parse of an increment extends the previous parse monotonically. However, using the incremental mode without look-ahead, parsing accuracy drops from 86.70% to 59.01%. Obviously, insisting on strict mo</context>
</contexts>
<marker>Demberg-Winterfors, 2010</marker>
<rawString>Vera Demberg-Winterfors. 2010. A Broad-Coverage Model of Prediction in Human Sentence Processing. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian A Foth</author>
<author>Niels Beuck</author>
<author>Arne K¨ohn</author>
<author>Wolfgang Menzel</author>
</authors>
<title>The Hamburg Dependency Treebank.</title>
<date>2014</date>
<journal>LREC, European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference</booktitle>
<marker>Foth, Beuck, K¨ohn, Menzel, 2014</marker>
<rawString>Kilian A. Foth, Niels Beuck, Arne K¨ohn, and Wolfgang Menzel. 2014. The Hamburg Dependency Treebank. In Proceedings of the Language Resources and Evaluation Conference 2014. LREC, European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Khalil Sima’an</author>
<author>Andy Way</author>
</authors>
<title>Lexicalized semi-incremental dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>128--134</pages>
<location>Borovets, Bulgaria,</location>
<marker>Hassan, Sima’an, Way, 2009</marker>
<rawString>Hany Hassan, Khalil Sima’an, and Andy Way. 2009. Lexicalized semi-incremental dependency parsing. In Proceedings of the International Conference RANLP-2009, pages 128–134, Borovets, Bulgaria, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia,</location>
<contexts>
<context position="15568" citStr="Johansson and Nugues, 2007" startWordPosition="2622" endWordPosition="2625">d, where each non-virtual element of V&apos; has to be mapped to the corresponding element in V. Let M be the set of all such mappings. Then the best mapping is defined as follows: φ = argmax ∑ I(π(m(w)) = m(π(w))) mEM wEV&apos; 1The mapping is partial because for some VNs in V&apos; there might be no corresponding VN in the gold standard. Precision and recall for the prediction with VNs can be computed by dividing corr by the number of predicted VNs and the number of VNs in the gold standard, respectively. Evaluation has been carried out on the PTB converted to dependency structure using the LTH converter (Johansson and Nugues, 2007) and on the Hamburg Dependency Treebank (Foth et al., 2014). From both corpora predictive PDAs padded with unused virtual nodes have been created for training. For English, the sentences of part 1-9 of the PTB were used, for German the first 50,000 sentences of the HDT have been selected. Testing was done using one virtual noun and one virtual verb for English and two virtual nouns and one virtual verb for German because these sets cover about 90% of the prefixes in both training sets. Figure 1 shows the evaluation results for parsing German and English using TurboParser. For both languages th</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia, May 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne K¨ohn</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Incremental and predictive dependency parsing under real-time conditions.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013,</booktitle>
<pages>373--381</pages>
<publisher>INCOMA Ltd. Shoumen, BULGARIA.</publisher>
<location>Hissar, Bulgaria,</location>
<marker>K¨ohn, Menzel, 2013</marker>
<rawString>Arne K¨ohn and Wolfgang Menzel. 2013. Incremental and predictive dependency parsing under real-time conditions. In Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013, pages 373–381, Hissar, Bulgaria, September. INCOMA Ltd. Shoumen, BULGARIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology, HLT ’94,</booktitle>
<pages>114--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9973" citStr="Marcus et al., 1994" startWordPosition="1621" endWordPosition="1624">, w E Pr because it is required by a valency of the verb. While this procedure is language-independent, some language-specific transformations must be applied nonetheless. For English, parts of gapping coordinations can be predicted whereas others can not. For German, the transformations described in (Beuck and Menzel, 2013) have been used without further changes. Both sets of structurally and lexically determined roles are language dependent. The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al., 1994) for predictability. For words marked as predictable their existence and word class, but not their lexicalization and position can be predicted. Therefore, we replace the lexical item with “[virtual]” and generalize the part-of-speech tag to a more coarse grained one. 4 Predictive parsing with TurboParser We adapt TurboParser (Martins et al., 2013) for incremental parsing because it does not impose structural constraints such as single-headedness in its core algorithm. For each parsing problem, it creates an integer linear program – in the form of a factor graph – with the variables representi</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the Workshop on Human Language Technology, HLT ’94, pages 114–119, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order nonprojective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>617--622</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="10323" citStr="Martins et al., 2013" startWordPosition="1673" endWordPosition="1676"> further changes. Both sets of structurally and lexically determined roles are language dependent. The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al., 1994) for predictability. For words marked as predictable their existence and word class, but not their lexicalization and position can be predicted. Therefore, we replace the lexical item with “[virtual]” and generalize the part-of-speech tag to a more coarse grained one. 4 Predictive parsing with TurboParser We adapt TurboParser (Martins et al., 2013) for incremental parsing because it does not impose structural constraints such as single-headedness in its core algorithm. For each parsing problem, it creates an integer linear program – in the form of a factor graph – with the variables representing the possible edges of the analyses. Since well-formedness is enforced by factors, additional constraints on the shape of analyses can be imposed without changing the core algorithm of the parser. We define three additional restrictions with respect to VNs: 1) A VN that is attached to unused may not have any dependents. 2) A VN may not be attache</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order nonprojective turbo parsers. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 617–622, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="5110" citStr="Nivre et al., 2007" startWordPosition="783" endWordPosition="786">ue to its use of beam search, the incremental results are non-monotonic. Hassan et al. (2009) present a CCG-based parser that can parse in an incremental mode. The parser guarantees that every parse of an increment extends the previous parse monotonically. However, using the incremental mode without look-ahead, parsing accuracy drops from 86.70% to 59.01%. Obviously, insisting on strict monotonicity (ai C an) is too strong a requirement, since it forces the parser to keep attachments that later turn out to be clearly wrong in light of new evidence. Being a transition-based parser, Maltparser (Nivre et al., 2007) does incremental parsing by design. It is, however, not able to predict upcoming structure and therefore its incremental output is usually fragmented into several trees. In addition, Maltparser needs a sufficiently large look-ahead to achieve high accuracy (Beuck et al., 2011). Beuck et al. (2011) introduced incremental and predictive parsing using Weighted Constraint Dependency Grammar. While their approach does not decrease in incremental mode, it is much slower than most other parsers. Another disadvantage is its hand-written grammar which prevents the parser from being adapted to addition</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davin Schlangen</author>
<author>Gabriel Skantze</author>
</authors>
<title>A general, abstract model of incremental dialogue processing.</title>
<date>2011</date>
<booktitle>Dialogue and Discourse,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="1514" citStr="Schlangen and Skantze, 2011" startWordPosition="209" endWordPosition="212">s property by processing yet unfinished utterances and reacting on them. Computational parsing on the other hand is mostly performed on complete sentences, a processing mode which renders a responsive interaction based on incomplete utterances impossible. When spoken language is analyzed, a mismatch between speech recognition and parsing occurs: If parsing does not work incrementally, the overall system loses all the desirable properties made possible by incremental processing. For speech dialogue systems, this leads to increased reaction times and an unnatural ping-pong style of interaction (Schlangen and Skantze, 2011). 1.1 Desirable features of incremental parsers Dependency parsing assigns a head and a dependency label to each word form of an input sentence and the resulting analysis of the sentence is usually required to form a tree. An incremental dependency parser processes a sentence word by word, building analyses for sentence prefixes (partial dependency analyses, PDA), which are extended and modified in a piecemeal fashion as more words become available. A PDA should come with three important (but partly contradictory) properties: beyond being accurate, it should also be as stable and informative a</context>
</contexts>
<marker>Schlangen, Skantze, 2011</marker>
<rawString>Davin Schlangen and Gabriel Skantze. 2011. A general, abstract model of incremental dialogue processing. Dialogue and Discourse, 2(1):83–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Sturt</author>
<author>Vincenzo Lombardo</author>
</authors>
<title>Processing coordinated structures: Incrementality and connectedness.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="2824" citStr="Sturt and Lombardo, 2005" startWordPosition="417" endWordPosition="420">ir labels) of a PDA ai which is also part of the analysis an of the whole sentence. To be maximally informative, at least all available word forms should be integrated into the prefix PDA. Even such a simple requirement cannot easily be met without predicting a structural skeleton for the word forms in the upcoming part of the sentence(bottom-up prediction). Other predictions merely serve to satisfy completeness conditions (i.e. valency requirements) in an anticipatory way (top-down predictions). In fact, humans are able to derive such predictions and they do so during sentence comprehension (Sturt and Lombardo, 2005). Without prediction, the sentence prefix “John drives a” of “John drives a car” can only be parsed as a disconnected structure: John drives a The determiner remains unconnected to the rest of the sentence, because a possible head is not yet available. However, the determiner could be integrated into the PDA if the connection is established by means of a predicted word form, which has not yet been observed. Beuck et al. (2011) propose to use virtual nodes (VNs) for this purpose. Each VN represents exactly one upcoming word. Its lexical instantiation and its exact position remain unspecified. U</context>
</contexts>
<marker>Sturt, Lombardo, 2005</marker>
<rawString>Patrick Sturt and Vincenzo Lombardo. 2005. Processing coordinated structures: Incrementality and connectedness. Cognitive Science, 29(2):291–305.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>