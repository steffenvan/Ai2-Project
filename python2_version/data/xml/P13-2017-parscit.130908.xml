<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007175">
<title confidence="0.996474">
Universal Dependency Annotation for Multilingual Parsing
</title>
<author confidence="0.919001333333333">
Ryan McDonald† Joakim Nivre†* Yvonne Quirmbach-Brundage$ Yoav Goldberg†*
Dipanjan Das† Kuzman Ganchev† Keith Hall† Slav Petrov† Hao Zhang†
Oscar T¨ackstr¨om†* Claudia Bedini$ N´uria Bertomeu Castell´o$ Jungmee Lee$
</author>
<affiliation confidence="0.98622">
Google, Inc.† Uppsala University* Appen-Butler-Hill$ Bar-Ilan University*
</affiliation>
<email confidence="0.996572">
Contact: ryanmcd@google.com
</email>
<sectionHeader confidence="0.993756" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999737272727273">
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ‘universal’ treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994719047619">
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (K¨ubler et
al., 2009). Research in dependency parsing – com-
putational methods to predict such representations
– has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al., 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
</bodyText>
<footnote confidence="0.991695">
1Downloadable at https://code.google.com/p/uni-dep-tb/.
</footnote>
<bodyText confidence="0.999775131578947">
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; K¨ubler et al., 2009; Zeman et al., 2012).
These data sets can be sufficient if one’s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al. (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al., 2005; Smith
and Eisner, 2009; Ganchev et al., 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al. (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajiˇc
et al., 2001; B¨ohmov´a et al., 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
</bodyText>
<page confidence="0.955954">
92
</page>
<bodyText confidence="0.955187322580645">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al., 2002; Helmreich et al., 2004;
Hovy et al., 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al. (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al., 2012; Gelling et al., 2012;
T¨ackstr¨om et al., 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al. (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
</bodyText>
<sectionHeader confidence="0.951035" genericHeader="method">
2 Towards A Universal Treebank
</sectionHeader>
<bodyText confidence="0.976897476190476">
The Stanford typed dependencies for English
(De Marneffe et al., 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
‘universal’ dependency representation, together
with the tag set of Petrov et al. (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al., 2009),
Finnish (Finno-Ugric) (Haverinen et al., 2010),
Persian (Indo-Iranian) (Seraji et al., 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world’s languages.
Alexandre r´eside avec sa famille a` Tinqueux .
</bodyText>
<note confidence="0.415399">
NOUN VERB ADP DET NOUN ADP NOUN P
</note>
<figureCaption confidence="0.999638">
Figure 1: A sample French sentence.
</figureCaption>
<bodyText confidence="0.999986384615385">
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al. (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al. (2012).
</bodyText>
<subsectionHeader confidence="0.98034">
2.1 Automatic Conversion
</subsectionHeader>
<bodyText confidence="0.99997756">
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al.,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al., 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al. (2012).
</bodyText>
<subsectionHeader confidence="0.996718">
2.2 Manual Annotation
</subsectionHeader>
<bodyText confidence="0.999457">
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
</bodyText>
<figure confidence="0.995423">
P
NSUBJ ADPMOD
ADPMOD
ADPOBJ
POSS
ADPOBJ
</figure>
<page confidence="0.994196">
93
</page>
<bodyText confidence="0.999951341463415">
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al., 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
</bodyText>
<subsectionHeader confidence="0.993321">
2.3 Harmonization
</subsectionHeader>
<bodyText confidence="0.999928285714286">
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
</bodyText>
<footnote confidence="0.986106">
2Available at http://ufal.mff.cuni.cz/tred/.
</footnote>
<bodyText confidence="0.999944261904762">
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al. (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
</bodyText>
<subsectionHeader confidence="0.991859">
2.4 Final Data Sets
</subsectionHeader>
<bodyText confidence="0.9990315">
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
</bodyText>
<footnote confidence="0.603170833333333">
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
</footnote>
<page confidence="0.999131">
94
</page>
<bodyText confidence="0.926277133333333">
Label Description Label Description Label Description
acomp adjectival complement compmod compound modifier nmod noun modifier
adp adposition conj conjunct nsubj nominal subject
adpcomp complement of adposition cop copula nsubjpass passive nominal subject
adpmod adpositional modifier csubj clausal subject num numeric modifier
adpobj object of adposition csubjpass passive clausal subject p punctuation
advcl adverbial clause modifier dep generic parataxis parataxis
advmod adverbial modifier det determiner partmod participial modifier
amod adjectival modifier dobj direct object poss possessive
appos appositive expl expletive prt verb particle
attr attribute infmod infinitival modifier rcmod relative clause modifier
aux auxiliary iobj indirect object rel relative
auxpass passive auxiliary mark marker xcomp open clausal complement
cc conjunction mwe multi-word expression
ccomp clausal complement neg negation
</bodyText>
<tableCaption confidence="0.999383">
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al., 2006).
</tableCaption>
<table confidence="0.999429571428571">
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB∗ 43,948 1,046,829
SV STB† 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
</table>
<tableCaption confidence="0.994595">
Table 2: Data set statistics. ∗Automatically con-
</tableCaption>
<bodyText confidence="0.931012636363636">
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. †Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
∼40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999521326530613">
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al. (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ‘delexical-
ized’ model that employs only ‘universal’ proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al. (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al. (2011) for the languages com-
</bodyText>
<footnote confidence="0.952213">
5These splits are included in the release of the data.
</footnote>
<page confidence="0.994012">
95
</page>
<table confidence="0.999850583333333">
Source Target Test Language
Training
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
Language
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
</table>
<tableCaption confidence="0.999778">
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
</tableCaption>
<bodyText confidence="0.999880571428571">
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38–68% range, as compared
to 55–75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
</bodyText>
<sectionHeader confidence="0.99837" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999989111111111">
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
</bodyText>
<sectionHeader confidence="0.991748" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998018333333334">
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al., 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and H´ector
Fern´andez Alcalde.
</bodyText>
<sectionHeader confidence="0.998764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.945261739130435">
Alena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora
Hladk´a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill´e,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103–127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
</reference>
<page confidence="0.99026">
96
</page>
<reference confidence="0.997553862745098">
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131–142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac¸a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajiˇc, Barbora Vidova Hladka, Jarmila Panevov´a,
Eva Hajiˇcov´a, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311–325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Be´ata Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Be´ata Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1–10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual f¨or grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan ˇStep´anek, Zdenˇek
ˇZabokrtsk`y, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings ofACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
</reference>
<page confidence="0.999693">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307132">
<title confidence="0.977195">Universal Dependency Annotation for Multilingual Parsing</title>
<author confidence="0.389648">Bertomeu</author>
<abstract confidence="0.985942545454546">We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This ‘universal’ treebank is made freely available in order to facilitate research on</abstract>
<intro confidence="0.859484">dependency</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alena B¨ohmov´a</author>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Barbora Hladk´a</author>
</authors>
<title>The Prague Dependency Treebank: A three-level annotation scenario.</title>
<date>2003</date>
<pages>103--127</pages>
<editor>In Anne Abeill´e, editor,</editor>
<publisher>Kluwer.</publisher>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2003</marker>
<rawString>Alena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora Hladk´a. 2003. The Prague Dependency Treebank: A three-level annotation scenario. In Anne Abeill´e, editor, Treebanks: Building and Using Parsed Corpora, pages 103–127. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories.</booktitle>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER Treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="1351" citStr="Buchholz and Marsi, 2006" startWordPosition="183" endWordPosition="187">ebank is made freely available in order to facilitate research on multilingual dependency parsing.1 1 Introduction In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest (K¨ubler et al., 2009). Research in dependency parsing – computational methods to predict such representations – has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages. In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format (Buchholz and Marsi, 2006; Nivre et al., 2007). While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1Downloadable at https://code.google.com/p/uni-dep-tb/. analysis of coordination, verb groups,</context>
<context position="15982" citStr="Buchholz and Marsi, 2006" startWordPosition="2471" endWordPosition="2475">nd testing, hence inducing a ‘delexicalized’ model that employs only ‘universal’ properties from source-side treebanks, such as part-ofspeech tags, labels, head-modifier distance, etc. We ran a number of experiments, which can be seen in Table 3. For these experiments we randomly split each data set into training, development and testing sets.5 The one exception is English, where we used the standard splits. Each row in Table 3 represents a source training language and each column a target evaluation language. We report both unlabeled attachment score (UAS) and labeled attachment score (LAS) (Buchholz and Marsi, 2006). This is likely the first reliable cross-lingual parsing evaluation. In particular, previous studies could not even report LAS due to differences in treebank annotations. We can make several interesting observations. Most notably, for the Germanic and Romance target languages, the best source language is from the same language group. This is in stark contrast to the results of McDonald et al. (2011), who observe that this is rarely the case with the heterogenous CoNLL treebanks. Among the Germanic languages, it is interesting to note that Swedish is the best source language for both German an</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Helge Dyvik</author>
<author>Tracy Holloway King</author>
<author>Hiroshi Masuichi</author>
<author>Christian Rohrer</author>
</authors>
<title>The parallel grammar project.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 workshop on Grammar engineering and evaluation-Volume 15.</booktitle>
<contexts>
<context position="4051" citStr="Butt et al., 2002" startWordPosition="593" endWordPosition="596"> (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common syn92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012;</context>
</contexts>
<marker>Butt, Dyvik, King, Masuichi, Rohrer, 2002</marker>
<rawString>Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and Christian Rohrer. 2002. The parallel grammar project. In Proceedings of the 2002 workshop on Grammar engineering and evaluation-Volume 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Discriminative reordering with Chinese grammatical relations features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT</booktitle>
<contexts>
<context position="5937" citStr="Chang et al., 2009" startWordPosition="881" endWordPosition="884">acies. 2 Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recently been adapted to several languages representing different (and typologically diverse) language groups, such as Chinese (Sino-Tibetan) (Chang et al., 2009), Finnish (Finno-Ugric) (Haverinen et al., 2010), Persian (Indo-Iranian) (Seraji et al., 2012), and Modern Hebrew (Semitic) (Tsarfaty, 2013). Its widespread use and proven adaptability makes it a natural choice for our endeavor, even though additional modifications will be needed to capture the full variety of grammatical structures in the world’s languages. Alexandre r´eside avec sa famille a` Tinqueux . NOUN VERB ADP DET NOUN ADP NOUN P Figure 1: A sample French sentence. We use the so-called basic dependencies (with punctuation included), where every dependency structure is a tree spanning </context>
</contexts>
<marker>Chang, Tseng, Jurafsky, Manning, 2009</marker>
<rawString>Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2009. Discriminative reordering with Chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="9777" citStr="Das and Petrov, 2011" startWordPosition="1487" endWordPosition="1490">aking fine-grained label distinctions was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2.</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation.</booktitle>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Chris D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Chris D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomaz Erjavec</author>
</authors>
<title>MULTEXT-East: Morphosyntactic resources for Central and Eastern European languages. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>46--131</pages>
<contexts>
<context position="4110" citStr="Erjavec, 2012" startWordPosition="605" endWordPosition="606">banks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common syn92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for synta</context>
</contexts>
<marker>Erjavec, 2012</marker>
<rawString>Tomaz Erjavec. 2012. MULTEXT-East: Morphosyntactic resources for Central and Eastern European languages. Language Resources and Evaluation, 46:131–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="3257" citStr="Ganchev et al., 2009" startWordPosition="470" endWordPosition="473">arsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common syn92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, Sofia, Bulgaria, Aug</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Gelling</author>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Joao Grac¸a</author>
</authors>
<title>The pascal challenge on grammar induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.</booktitle>
<marker>Gelling, Cohn, Blunsom, Grac¸a, 2012</marker>
<rawString>Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao Grac¸a. 2012. The pascal challenge on grammar induction. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Barbora Vidova Hladka, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, and Petr Pajas.</title>
<date>2001</date>
<marker>Hajiˇc, 2001</marker>
<rawString>Jan Hajiˇc, Barbora Vidova Hladka, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, and Petr Pajas. 2001. Prague Dependency Treebank 1.0. LDC, 2001T10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katri Haverinen</author>
<author>Timo Viljanen</author>
<author>Veronika Laippala</author>
<author>Samuel Kohonen</author>
<author>Filip Ginter</author>
<author>Tapio Salakoski</author>
</authors>
<title>Treebanking finnish.</title>
<date>2010</date>
<booktitle>In Proceedings of The Ninth International Workshop on Treebanks and Linguistic Theories (TLT9).</booktitle>
<contexts>
<context position="5985" citStr="Haverinen et al., 2010" startWordPosition="887" endWordPosition="890">anford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recently been adapted to several languages representing different (and typologically diverse) language groups, such as Chinese (Sino-Tibetan) (Chang et al., 2009), Finnish (Finno-Ugric) (Haverinen et al., 2010), Persian (Indo-Iranian) (Seraji et al., 2012), and Modern Hebrew (Semitic) (Tsarfaty, 2013). Its widespread use and proven adaptability makes it a natural choice for our endeavor, even though additional modifications will be needed to capture the full variety of grammatical structures in the world’s languages. Alexandre r´eside avec sa famille a` Tinqueux . NOUN VERB ADP DET NOUN ADP NOUN P Figure 1: A sample French sentence. We use the so-called basic dependencies (with punctuation included), where every dependency structure is a tree spanning all the input tokens, because this is the kind o</context>
</contexts>
<marker>Haverinen, Viljanen, Laippala, Kohonen, Ginter, Salakoski, 2010</marker>
<rawString>Katri Haverinen, Timo Viljanen, Veronika Laippala, Samuel Kohonen, Filip Ginter, and Tapio Salakoski. 2010. Treebanking finnish. In Proceedings of The Ninth International Workshop on Treebanks and Linguistic Theories (TLT9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Helmreich</author>
<author>David Farwell</author>
<author>Bonnie Dorr</author>
<author>Nizar Habash</author>
<author>Lori Levin</author>
<author>Teruko Mitamura</author>
<author>Florence Reeder</author>
<author>Keith Miller</author>
<author>Eduard Hovy</author>
<author>Owen Rambow</author>
<author>Advaith Siddharthan</author>
</authors>
<title>Interlingual annotation of multilingual text corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-EACL Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="4075" citStr="Helmreich et al., 2004" startWordPosition="597" endWordPosition="600">harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common syn92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 201</context>
<context position="6854" citStr="Helmreich et al. (2004)" startWordPosition="1025" endWordPosition="1028">e full variety of grammatical structures in the world’s languages. Alexandre r´eside avec sa famille a` Tinqueux . NOUN VERB ADP DET NOUN ADP NOUN P Figure 1: A sample French sentence. We use the so-called basic dependencies (with punctuation included), where every dependency structure is a tree spanning all the input tokens, because this is the kind of representation that most available dependency parsers require. A sample dependency tree from the French data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and wit</context>
</contexts>
<marker>Helmreich, Farwell, Dorr, Habash, Levin, Mitamura, Reeder, Miller, Hovy, Rambow, Siddharthan, 2004</marker>
<rawString>Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar Habash, Lori Levin, Teruko Mitamura, Florence Reeder, Keith Miller, Eduard Hovy, Owen Rambow, and Advaith Siddharthan. 2004. Interlingual annotation of multilingual text corpora. In Proceedings of the HLT-EACL Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="4094" citStr="Hovy et al., 2006" startWordPosition="601" endWordPosition="604"> of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common syn92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do th</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>03</issue>
<contexts>
<context position="2661" citStr="Hwa et al., 2005" startWordPosition="375" endWordPosition="378"> Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), bu</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11(03):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7311" citStr="Klein and Manning, 2003" startWordPosition="1095" endWordPosition="1098">nch data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2607" citStr="Klein and Manning, 2004" startWordPosition="367" endWordPosition="370">word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Chris D. Manning. 2004. Corpus-based induction of syntactic structure: models of dependency and constituency. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<publisher>Morgan</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Morgan and Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7397" citStr="Marcus et al., 1993" startWordPosition="1111" endWordPosition="1114"> traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coo</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2715" citStr="McDonald et al. (2011)" startWordPosition="384" endWordPosition="387">icient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to cr</context>
<context position="5017" citStr="McDonald et al. (2011)" startWordPosition="744" endWordPosition="748">ore data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-lingual studies, we can report full labeled accuracies and not just unlabeled structural accuracies. 2 Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-spee</context>
<context position="9813" citStr="McDonald et al., 2011" startWordPosition="1493" endWordPosition="1496">ns was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2.3 Harmonization After producing the </context>
<context position="15140" citStr="McDonald et al. (2011)" startWordPosition="2333" endWordPosition="2336"> the data itself. †Automatically converted Talbanken section of the Swedish Treebank. N=News, B=Blogs, R=Consumer Reviews. due to the source and tokenization. For example, Korean has 50% more sentences than Spanish, but ∼40k less tokens due to a more coarse-grained tokenization. In addition to the data itself, annotation guidelines and harmonization rules are included so that the data can be regenerated. 3 Experiments One of the motivating factors in creating such a data set was improved cross-lingual transfer evaluation. To test this, we use a cross-lingual transfer parser similar to that of McDonald et al. (2011). In particular, it is a perceptron-trained shift-reduce parser with a beam of size 8. We use the features of Zhang and Nivre (2011), except that all lexical identities are dropped from the templates during training and testing, hence inducing a ‘delexicalized’ model that employs only ‘universal’ properties from source-side treebanks, such as part-ofspeech tags, labels, head-modifier distance, etc. We ran a number of experiments, which can be seen in Table 3. For these experiments we randomly split each data set into training, development and testing sets.5 The one exception is English, where </context>
<context position="16385" citStr="McDonald et al. (2011)" startWordPosition="2537" endWordPosition="2540">lits. Each row in Table 3 represents a source training language and each column a target evaluation language. We report both unlabeled attachment score (UAS) and labeled attachment score (LAS) (Buchholz and Marsi, 2006). This is likely the first reliable cross-lingual parsing evaluation. In particular, previous studies could not even report LAS due to differences in treebank annotations. We can make several interesting observations. Most notably, for the Germanic and Romance target languages, the best source language is from the same language group. This is in stark contrast to the results of McDonald et al. (2011), who observe that this is rarely the case with the heterogenous CoNLL treebanks. Among the Germanic languages, it is interesting to note that Swedish is the best source language for both German and English, which makes sense from a typological point of view, because Swedish is intermediate between German and English in terms of word order properties. For Romance languages, the crosslingual parser is approaching the accuracy of the supervised setting, confirming that for these languages much of the divergence is lexical and not structural, which is not true for the Germanic languages. Finally,</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
</authors>
<title>Generalizing tree transformations for inductive dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2021" citStr="Nilsson et al., 2007" startWordPosition="280" endWordPosition="283">tandardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1Downloadable at https://code.google.com/p/uni-dep-tb/. analysis of coordination, verb groups, subordinate clauses, and multi-word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-ling</context>
<context position="8093" citStr="Nilsson et al., 2007" startWordPosition="1223" endWordPosition="1226">as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). 2.2 Manual Annotation For the remaining four languages, annotators were given three resources: 1) the English Stanford P NSUBJ ADPMOD ADPMOD ADPOBJ POSS ADPOBJ 93 guidelines; 2) a set of English sentences with Stanford dependencies and universal tags (as above); and 3) a large collection of unlabeled sentences randomly drawn from newswire, weblogs and/or consumer reviews, automatically tokenized with a rule-based system. For German, French and Spanish</context>
</contexts>
<marker>Nilsson, Nivre, Hall, 2007</marker>
<rawString>Jens Nilsson, Joakim Nivre, and Johan Hall. 2007. Generalizing tree transformations for inductive dependency parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Be´ata Megyesi</author>
</authors>
<title>Bootstrapping a Swedish treebank using cross-corpus harmonization and annotation projection.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Workshop on Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="7643" citStr="Nivre and Megyesi, 2007" startWordPosition="1152" endWordPosition="1155">(2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). 2.2 M</context>
</contexts>
<marker>Nivre, Megyesi, 2007</marker>
<rawString>Joakim Nivre and Be´ata Megyesi. 2007. Bootstrapping a Swedish treebank using cross-corpus harmonization and annotation projection. In Proceedings of the 6th International Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="4524" citStr="Petrov et al. (2012)" startWordPosition="669" endWordPosition="672">nal Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, su</context>
<context position="8236" citStr="Petrov et al. (2012)" startWordPosition="1247" endWordPosition="1250">k (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). 2.2 Manual Annotation For the remaining four languages, annotators were given three resources: 1) the English Stanford P NSUBJ ADPMOD ADPMOD ADPOBJ POSS ADPOBJ 93 guidelines; 2) a set of English sentences with Stanford dependencies and universal tags (as above); and 3) a large collection of unlabeled sentences randomly drawn from newswire, weblogs and/or consumer reviews, automatically tokenized with a rule-based system. For German, French and Spanish, contractions were split, except in the case of clitics. For Korean, tokenization was more coarse and included particles within token units. A</context>
<context position="12222" citStr="Petrov et al. (2012)" startWordPosition="1878" endWordPosition="1881">ld be misleading in the universal context (although quite appropriate for English). For example, the label prep (for a modifier headed by a preposition) was renamed adpmod, to make clear the relation to other modifier labels and to allow postpositions as well as prepositions.3 We also eliminated a few distinctions in the original Stanford scheme that were not annotated consistently across languages (e.g., merging complm with mark, number with num, and purpcl with advcl). The final set of labels is listed with explanations in Table 1. Note that relative to the universal partof-speech tagset of Petrov et al. (2012) our final label set is quite rich (40 versus 12). This is due mainly to the fact that the the former is based on deterministic mappings from a large set of annotation schemes and therefore reduced to the granularity of the greatest common denominator. Such a reduction may ultimately be necessary also in the case of dependency relations, but since most of our data sets were created through manual annotation, we could afford to retain a fine-grained analysis, knowing that it is always possible to map from finer to coarser distinctions, but not vice versa.4 2.4 Final Data Sets Table 2 presents t</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mojgan Seraji</author>
<author>Be´ata Megyesi</author>
<author>Nivre Joakim</author>
</authors>
<title>Bootstrapping a Persian dependency treebank.</title>
<date>2012</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>7</volume>
<issue>18</issue>
<contexts>
<context position="6031" citStr="Seraji et al., 2012" startWordPosition="893" endWordPosition="896">e et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recently been adapted to several languages representing different (and typologically diverse) language groups, such as Chinese (Sino-Tibetan) (Chang et al., 2009), Finnish (Finno-Ugric) (Haverinen et al., 2010), Persian (Indo-Iranian) (Seraji et al., 2012), and Modern Hebrew (Semitic) (Tsarfaty, 2013). Its widespread use and proven adaptability makes it a natural choice for our endeavor, even though additional modifications will be needed to capture the full variety of grammatical structures in the world’s languages. Alexandre r´eside avec sa famille a` Tinqueux . NOUN VERB ADP DET NOUN ADP NOUN P Figure 1: A sample French sentence. We use the so-called basic dependencies (with punctuation included), where every dependency structure is a tree spanning all the input tokens, because this is the kind of representation that most available dependenc</context>
</contexts>
<marker>Seraji, Megyesi, Joakim, 2012</marker>
<rawString>Mojgan Seraji, Be´ata Megyesi, and Nivre Joakim. 2012. Bootstrapping a Persian dependency treebank. Linguistic Issues in Language Technology, 7(18):1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3234" citStr="Smith and Eisner, 2009" startWordPosition="466" endWordPosition="469">ross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common syn92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–9</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the ACL.</journal>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Teleman</author>
</authors>
<title>Manual f¨or grammatisk beskrivning av talad och skriven svenska.</title>
<date>1974</date>
<journal>Studentlitteratur.</journal>
<contexts>
<context position="7867" citStr="Teleman, 1974" startWordPosition="1189" endWordPosition="1190">sed the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). 2.2 Manual Annotation For the remaining four languages, annotators were given three resources: 1) the English Stanford P NSUBJ ADPMOD ADPMOD ADPOBJ POSS ADPOBJ 93 guidelines; 2) a set of English sentences with Stanford dependenci</context>
</contexts>
<marker>Teleman, 1974</marker>
<rawString>Ulf Teleman. 1974. Manual f¨or grammatisk beskrivning av talad och skriven svenska. Studentlitteratur.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>A unified morpho-syntactic scheme of stanford dependencies.</title>
<date>2013</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="6077" citStr="Tsarfaty, 2013" startWordPosition="901" endWordPosition="902">e as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recently been adapted to several languages representing different (and typologically diverse) language groups, such as Chinese (Sino-Tibetan) (Chang et al., 2009), Finnish (Finno-Ugric) (Haverinen et al., 2010), Persian (Indo-Iranian) (Seraji et al., 2012), and Modern Hebrew (Semitic) (Tsarfaty, 2013). Its widespread use and proven adaptability makes it a natural choice for our endeavor, even though additional modifications will be needed to capture the full variety of grammatical structures in the world’s languages. Alexandre r´eside avec sa famille a` Tinqueux . NOUN VERB ADP DET NOUN ADP NOUN P Figure 1: A sample French sentence. We use the so-called basic dependencies (with punctuation included), where every dependency structure is a tree spanning all the input tokens, because this is the kind of representation that most available dependency parsers require. A sample dependency tree fr</context>
</contexts>
<marker>Tsarfaty, 2013</marker>
<rawString>Reut Tsarfaty. 2013. A unified morpho-syntactic scheme of stanford dependencies. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>David Marecek</author>
<author>Martin Popel</author>
<author>Loganathan Ramasamy</author>
<author>Jan ˇStep´anek</author>
<author>Zdenˇek ˇZabokrtsk`y</author>
<author>Jan Hajic</author>
</authors>
<title>Hamledt: To parse or not to parse.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Zeman, Marecek, Popel, Ramasamy, ˇStep´anek, ˇZabokrtsk`y, Hajic, 2012</marker>
<rawString>Daniel Zeman, David Marecek, Martin Popel, Loganathan Ramasamy, Jan ˇStep´anek, Zdenˇek ˇZabokrtsk`y, and Jan Hajic. 2012. Hamledt: To parse or not to parse. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT.</booktitle>
<contexts>
<context position="15272" citStr="Zhang and Nivre (2011)" startWordPosition="2356" endWordPosition="2359">e source and tokenization. For example, Korean has 50% more sentences than Spanish, but ∼40k less tokens due to a more coarse-grained tokenization. In addition to the data itself, annotation guidelines and harmonization rules are included so that the data can be regenerated. 3 Experiments One of the motivating factors in creating such a data set was improved cross-lingual transfer evaluation. To test this, we use a cross-lingual transfer parser similar to that of McDonald et al. (2011). In particular, it is a perceptron-trained shift-reduce parser with a beam of size 8. We use the features of Zhang and Nivre (2011), except that all lexical identities are dropped from the templates during training and testing, hence inducing a ‘delexicalized’ model that employs only ‘universal’ properties from source-side treebanks, such as part-ofspeech tags, labels, head-modifier distance, etc. We ran a number of experiments, which can be seen in Table 3. For these experiments we randomly split each data set into training, development and testing sets.5 The one exception is English, where we used the standard splits. Each row in Table 3 represents a source training language and each column a target evaluation language.</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Zhang</author>
<author>Roi Reichart</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Learning to map into a universal pos tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4628" citStr="Zhang et al., 2012" startWordPosition="684" endWordPosition="687">for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-ling</context>
</contexts>
<marker>Zhang, Reichart, Barzilay, Globerson, 2012</marker>
<rawString>Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir Globerson. 2012. Learning to map into a universal pos tagset. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>