<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002546">
<note confidence="0.538943333333333">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.998719">
Semantic Role Labelling with Similarity-Based Generalization Using
EM-based Clustering
</title>
<author confidence="0.998993">
Ulrike Baldewein, Katrin Erk, Sebastian Padó Detlef Prescher
</author>
<affiliation confidence="0.889994">
Saarland University University of Amsterdam
Saarbrücken, Germany Amsterdam, The Netherlands
</affiliation>
<email confidence="0.997765">
{ulrike,erk,pado}@coli.uni-sb.de prescher@science.uva.nl
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998737875">
We describe a system for semantic role assignment
built as part of the Senseval III task, based on an
off-the-shelf parser and Maxent and Memory-Based
learners. We focus on generalisation using several
similarity measures to increase the amount of train-
ing data available and on the use of EM-based clus-
tering to improve role assignment. Our final score
is Precision=73.6%, Recall=59.4% (F=65.7).
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946833333333">
This paper describes a study in semantic role la-
belling in the context of the Senseval III task, for
which the training and test data were both drawn
from the current FrameNet release (Johnson et al.,
2002). We concentrated on two questions: first,
whether role assignment can be improved by gener-
alisation over training instances using different sim-
ilarity measures; and second, the impact of EM-
based clustering, both in deriving more informative
selectional preference features and in the generali-
sations mentioned above. The basis of our experi-
ments was formed by off-the-shelf statistical tools
for data processing and modelling.
After listing our data preparation steps (Sec. 2)
and features (Sec. 3), we describe our classification
procedure and the learners we used (Sec. 4). Sec. 5
outlines our experiments in similarity-based gener-
alisations, and Section 6 discusses our results.
</bodyText>
<sectionHeader confidence="0.943601" genericHeader="introduction">
2 Data and Instances
</sectionHeader>
<bodyText confidence="0.99923655">
Parsing. To tag and parse the data, we used
LoPar (Schmid, 2000), a probabilistic context-
free parser, which comes with a Head-Lexicalised
Grammar for English (Carroll and Rooth, 1998).
We considered only the most probable parse for
each sentence and simplified parse trees by elim-
inating unary nodes. The resulting nodes form
the instances of our classification. We used the
Stuttgart TreeTagger (Schmid, 1994) to lemmatise
constituent heads.
Projection of role labels. FrameNet provides se-
mantic roles as character offsets. We labelled
those instances (i.e. nodes in the parse tree) with
gold standard semantic roles which corresponded to
roles’ maximal projections. 13.95% of roles in the
training corpus spanned more than one parse tree
node. Figure 1 shows an example sentence for the
AWARENESS frame. The nodes’ respective seman-
tic role labels are given in small caps, and the target
predicate is marked in boldface.
</bodyText>
<figure confidence="0.826776">
S (NONE)
</figure>
<figureCaption confidence="0.999729">
Figure 1: Example parse tree with role labels
</figureCaption>
<bodyText confidence="0.997624857142857">
Semantic clustering. We used clustering to gen-
eralise over possible fillers of roles. In a first model,
we derived a probability distribution for pairs
, where is a target:role combination
and is the head lemma of a role filler. The key
idea is that and are mutually independent, but
conditioned on an unobserved class . In this
manner, we define the probability of
as:
Estimation was performed using a variant of the
expectation-maximisation algorithm (Prescher et
al., 2000). We used this model both as a feature and
in the generalisation described in Sec. 5. In a sec-
ond model, we clustered pairs of target:role and the
</bodyText>
<figure confidence="0.728751125">
NP (COGNIZER)
VP (NONE)
V (NONE)
does not
VP (NONE)
know NP (CONTENT)
the answer
Peter
</figure>
<bodyText confidence="0.9914955">
syntactic properties of the role fillers; the resulting
model was only used for generalisation.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999934625">
Constituent features. The first group of fea-
tures represents properties of instances (i.e. con-
stituents). We used the phrase type and head lemma
of each constituent, its preposition, if any (otherwise
NONE), its relative position with respect to the tar-
get (left, right, overlapping), the phrase type of its
mother node, and the simplified path from the tar-
get to the constituent: all phrase types encountered
on the way, and whether each step was up or down.
Two further features stated whether this path had
been seen as a frame element in the training data,
and whether the constituent was subcategorised for
(determined heuristically).
Sentence level features. The second type of fea-
ture described the context of the current instance:
The target word was characterised by its lemma,
POS, voice, subcat frame (determined heuristi-
cally), and its governing verb; we also compiled a
list of all prepositions in the sentence.
Semantic features. The third type of features
made use of EM-based clustering, stating the most
probable label assigned to the constituent by the
clustering model as well as a confidence score for
this decision.
</bodyText>
<sectionHeader confidence="0.997792" genericHeader="method">
4 Classification
</sectionHeader>
<bodyText confidence="0.9999245">
We first describe our general procedure, then the
two different machine learning systems we used.
Classification Procedure. As the semantic role
labels of FrameNet are frame-specific, we decided
to train one classifier for each frame. To cope with
the large amount of constituents bearing no role la-
bel, we divided the procedure into two steps, distin-
guishing argument identification and argument la-
belling. First, argument identification decides for
all constituents whether they are role-bearers or not.
Then, argument labelling assigns semantic roles to
those sequences classified as role-bearing. In our
example (Fig. 1), the first step of classification ide-
ally would single out the two NPs as possible role
fillers, while the second step would assign the COG-
NIZER and CONTENT roles.
Maximum Entropy Learning. Our first classifier
was a log-linear model, where the probability of a
class given an feature vector is defined as
where is a normalisation constant, the
value of feature for class, and the weight
assigned to. The model is trained by optimising
the weights subject to the maximum entropy con-
straint which ensures that the least committal opti-
mal model is learnt. Maximum Entropy (Maxent)
models have been successfully applied to semantic
role labelling (Fleischman et al., 2003). We used
the estimate software for estimation, which im-
plements the LMVM algorithm (Malouf, 2002) and
was kindly provided by Rob Malouf.
Memory-based Learning. Our second learner
implements an instance of a memory-based learn-
ing (MBL) algorithm, namely the -nearest neigh-
bour algorithm. This algorithm classifies test in-
stances by assigning them the label of the most sim-
ilar examples from the training set. Its parameters
are the number of training examples to be consid-
ered, the similarity metric, and the feature weight-
ing scheme. We used the implementation provided
by TiMBL (Daelemans et al., 2003) with the default
parameters, i.e. =1 and the weighted overlap simi-
larity metric with gain ratio feature weighting.
</bodyText>
<sectionHeader confidence="0.830938" genericHeader="method">
5 Similarity-based Generalisation over
Training Instances
</sectionHeader>
<bodyText confidence="0.991775925">
FrameNet role labels are frame-specific. This
makes it necessary to either train individual clas-
sifiers with little training data per frame, or train a
large classifier with many sparse classes. So one im-
portant question is whether we can generalise, i.e.
exploit similarities between frame elements, to gain
more training data.
We experimented with different generalisation
methods, all following the same basic idea: If frame
element A1 of frame A and frame element B1 of
frame B are similar, we re-use A1 training data as
B1 instances. In this process, we mask out features
which might harm learning for A1, such as targets or
sentence level features, or semantic features in case
of syntactic similarities (and vice versa). We ex-
plored three types of role similarities, two based on
symbolic information from the FrameNet database,
and one statistical.
Frame Hierarchy. FrameNet specifies frame-to-
frame relations, among them three that order frames
hierarchically: Inheritance, the Uses relation of par-
tial inheritance, and the Subframe relation linking
larger situation frames to their individual stages. All
three indicate semantic similarity between (at least
some) frame elements; in some cases corresponding
frame elements are also syntactically similar, e.g.
the Victim role of Cause_harm and the Evaluee role
of Corporal_punishment are both typically realised
as direct objects.
Peripheral frame elements. FrameNet distin-
guishes core, extrathematic, and peripheral frame
elements. Peripheral frame elements are frame-
independent adjuncts; however the same frame el-
ement may be peripheral to one frame and core to
another. So we took a peripheral frame element
as similar to the same peripheral frame element in
other frames: Given an instance of a peripheral
frame element, we used it as training instance for
all frames for which it was marked as peripheral in
the FrameNet database.
</bodyText>
<construct confidence="0.577472666666667">
Group 6: puzzle:Experiencer_obj.Stimulus, increase:Change_posi-
tion_on_a_scale.Item, praise:Judgment_communication.Communi-
cator, travel:Travel. Traveler, ...
Group 11: lodge:Residence.Location, scoff:Judgment_communi-
cation.Evaluee, chug:Motion_noise.Path, emerge:Departing.Source,
. . .
</construct>
<figureCaption confidence="0.9985345">
Figure 2: EM-based syntactic clustering: excerpts
of 2 clusters
</figureCaption>
<bodyText confidence="0.9486106">
EM-based clustering. The EM-based clustering
methods introduced in Sec. 2 measure the “good-
ness of fit” between a target word and a potential
role filler. We now say that two frame elements
are similar if they are appropriate for some com-
mon cluster. For the head lemma clustering model,
we define the appropriateness of a target:role
pair for a cluster as follows:
where is the total frequency of all head lem-
mas that have been seen with , weighted by the
class-membership probability of in . This ap-
propriateness measure is built on top of the
class-based frequencies rather than on
the frequencies or the class-membership prob-
abilities in isolation: For some tasks the com-
bination of lexical and semantic information has
been shown to outperform each of the single infor-
mation sources (Prescher et al., 2000). Our simi-
larity notion is now formalised as follows: With a
threshold as a parameter, two frame elements ,
count as similar if for some class,
and .
In the syntactic clustering model, a role filler was
described as a combination of the path from in-
stance to target, the instance’s preposition, and the
target voice. The appropriateness of a target:role
pair is defined as for the above model. For time rea-
sons, only verbal targets were considered.
Figure 2 shows excerpts of two “syntactic” clus-
ters in the form of target:frame.role members.
Group 6 is a very homogeneous group, consisting
of roles that are usually realised as subjects. Group
11 contains roles realised as prepositional phrases,
but with very diverse prepositions, including in, at,
along, and from.
</bodyText>
<sectionHeader confidence="0.999696" genericHeader="conclusions">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.99999825">
We first give the final results of our systems on the
test set according to the official evaluation software.
Then we discuss detailed results on a development
set we randomly extracted from the training data.
</bodyText>
<subsectionHeader confidence="0.99311">
6.1 Final Results
</subsectionHeader>
<bodyText confidence="0.99996575">
We submitted the results of two models. One was
produced using the maximum entropy learner, in-
cluding all features of Sec. 3 and with the three most
helpful generalisation techniques (EM head lemma,
EM path, and Peripherals). For the second model
we used the MBL learner trained on all features,
with no additional training data1. The performance
of the two models is shown in Table 1.
</bodyText>
<table confidence="0.937262833333333">
Maxent MBL
Precision 73.6% 65.4%
Recall 59.4% 47.1%
F-score 65.7 54.8
Coverage 80.7% 72.0%
Overlap 67.5% 60.2%
</table>
<tableCaption confidence="0.99991">
Table 1: Test set results (official scoring scheme)
</tableCaption>
<subsectionHeader confidence="0.998166">
6.2 Detailed Results
</subsectionHeader>
<bodyText confidence="0.999058555555556">
For a detailed evaluation, we randomly split off 10%
of the training data to form development sets. In this
section, we report results of two such splits to take
chance variation into account.
For time reasons, this detailed evaluation was per-
formed using our own evaluation software, which is
based on our internal constituent-based representa-
tion. This software gives the same tendencies (im-
provements / deteriorations) as the official software,
but absolute values differ; so we restrict ourselves
to reporting relative figures.
Basis for Comparison. All following models are
compared against a set of basic models trained on
all features of Sec. 3. Table 2 gives the results for
these models, using our own scoring software.
Contribution of Features. We computed the con-
tribution of individual features by leaving out each
feature in turn. Table 3 shows the results, averaging
</bodyText>
<footnote confidence="0.9965365">
1For time reasons, we were not able to test generalisation in
the Memory-Based Learning paradigm.
</footnote>
<table confidence="0.921422434782609">
1st split 2nd split
Maxent F=80.02 F=80.86
MBL F=86.43 F=85.66
Table 2: Devel set results (own scoring scheme)
F-score
Feature MBL Maxent
head lemma 0 0.6
emmc label 3.9 3.9
emmc prob -0.3 1.8
mother phrase type -0.7 -0.3
governing verb -0.1 -0.5
is subcategorized -0.1 -0.5
path 0.2 0.5
path length -0.5 -0.5
path seen 1.6 3.4
preposition 0 -0.3
all preps -0.2 -0.7
phrase type 1.2 2.2
position 0.5 0.3
sc frame 0.1 -0.2
target lemma 0 -0.6
target POS 0.1 -0.3
voice 0.1 -0.3
</table>
<tableCaption confidence="0.999928">
Table 3: Contribution of each feature
</tableCaption>
<bodyText confidence="0.991148038461539">
over the two splits. The features that contributed
most to the performance were the same for both
learners: the label assigned by the EM-based model,
the phrase type, and whether the path had been seen
to lead to a frame element. The relative position
to the target helped in one MBL and one Maxent
run. Interestingly, the Maxent learner profits from
the probability with which the EM-based model as-
signs its label, while MBL does not.
Generalisation. To measure the effect of each
of the similarity measures listed in Sec. 5, we
tested them individually using the Maximum En-
tropy learner with all features.
As mentioned above, training instances of one
frame were generalised and then added to the train-
ing instances of another, retaining only part of the
features in the generalisation. Table 4 shows the
features retained for each similarity measure, as
well as the number of additional instances gener-
ated, summed over all frames. We empirically de-
termined the optimal parameter values as: For FN-h
(sem) and FN-h (syn), 1 level in the hierarchy; for
EMhead, a weight threshold of , and for EM
path, a weight threshold of .
Table 5 gives the improvements made over
the baseline through adding data gained by each
</bodyText>
<table confidence="0.998599857142857">
FN hierarchy (sem): 10,000 instances
head lemma
FN hierarchy (syn): 10,000 instances
phrase type, path, prep., path seen, is subcat-
egorised, voice, target POS
Peripherals: 55,000 instances
head lemma, phrase type, path, prep., path
seen, is subcategorised, voice, target POS
EM head: 1,000,000 instances
head lemma
EM path: 433,000 instances
phrase type, mother phrase type, path, path
length, prep., path seen, is subcategorised,
voice, target POS
</table>
<tableCaption confidence="0.99317">
Table 4: Similarity-based generalisation: Features
retained and number of generated instances
</tableCaption>
<table confidence="0.999877428571429">
F-score
Strategy Split 1 Split 2
FN hierarchy (sem) 0.3 -0.5
FN hierarchy (syn) -0.2 -0.4
Peripherals 0.2 -0.1
EM head 0.4 0.5
EM path 1.0 0.2
</table>
<tableCaption confidence="0.999771">
Table 5: Contribution of generalization strategies
</tableCaption>
<bodyText confidence="0.977650181818182">
generalisation strategy. Results are shown in
points F-score and individually for both train-
ing/development splits. EM-based clustering
proved to be helpful, showing both the highest sin-
gle improvement (EMpath) and the highest consis-
tent improvement (EM head), while all other gener-
alisations show mixed results.
Combining the three most promising generali-
sation techniques (Peripherals, EM head, and EM
path) led to an improvement of 0.7 points F-score
for split 1 and 1.1 points F-score for split 2.
</bodyText>
<subsectionHeader confidence="0.886844">
6.3 Discussion.
</subsectionHeader>
<bodyText confidence="0.999981118421053">
Feature quality. The features that improved the
learners’ performance most are EM-based label,
phrase type and the “path seen as FE”. The other
features did not show much impact for us. The
Maxent learner was negatively affected by sentence-
level features such as the subcat frame and “is sub-
categorised”.
Comparing the learners. In a comparable ba-
sic setting (all features, no generalisation), the
Memory-Based learner easily outperforms the Max-
ent learner, according to our scoring scheme. How-
ever, the official scoring scheme determines the
Memory-based learner’s performance at more than
10 points F-score below the Maxent learner. We in-
tend to run the Memory-based learner with general-
isation data for a more comprehensive comparison.
Generalisation. Gildea and Jurafsky (2002) re-
port an improvement of 1.6% through generali-
sation, which is roughly comparable to our fig-
ures. The two strategies share the common idea
of exploiting role similarities, but the realisations
are converse: Gildea and Jurafsky manually com-
pact similar frame elements into 18 abstract, frame-
independent roles, whereas we keep the roles frame-
specific but augment the training data for each by
automatically discovered similarities.
One reason for the disappointing performance of
the FrameNet hierarchy-based generalisation strate-
gies may be simply the amount of data, as shown
by Table 4: FN-h (sem) and FN-h (syn) each only
yield 10,000 additional instances as compared to
around 1,000,000 for EM head. That the reliabil-
ity of the results roughly seems to go up with the
number of additional instances generated (Periph-
erals: ca. 50,000, EM-Path: ca. 400,000) fits this
argumentation well.
The input to the EM path clusters is a tuple of
the path, target voice and preposition information.
In the resulting model, generalisation over voice
worked well, yielding clusters containing both ac-
tive and passive alternations of similar frame el-
ements. However, prepositions were distributed
more arbitrarily. While this may indicate problems
of clustering with more structured forms of input, it
may also just be a consequence of noisy input, as the
preposition feature has not had much impact either
on the learners’ performance.
The EM head strategy adds large amounts of
head lemma instances, which probably alleviates
the sparse data problem that makes the head lemma
feature virtually useless. Another way of capitalis-
ing on this type of information would be to use the
FN hierarchy generalisation to derive more input for
EM-based clustering and see if this indirect use of
generalisation still improves semantic role assign-
ment. Interestingly, the EM head strategy and the
EM-based clustering feature, both geared at solving
the same sparse data problem, do not cancel each
other out. In future work, we will try to combine the
EM head strategy with the FrameNet hierarchy to
derive more input for the clustering model to see if
this can improve the present generalisation results.
Comparison with CoNLL. We recently studied
semantic role labelling in the context of the CoNLL
shared task (Baldewein et al., 2004). The two key
differences to this study were that the semantic roles
in question were PropBank roles and that only shal-
low information was available. Our system there
showed two main differences to the current sys-
tem: the overall level of accuracy was lower, and
EM-based clustering did not improve the perfor-
mance. While the performance difference is evi-
dently a consequence of only shallow information
being available, it remains an interesting open ques-
tion why EM-based clustering could improve one
system, but not the other.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999814783783784">
U. Baldewein, K. Erk, S. Pado, and D. Prescher.
2004. Semantic role labelling with chunk se-
quences. In Proceedings of CoNLL-2004.
G. Carroll and M. Rooth. 1998. Valence induction
with a head-lexicalized PCFG. In Proceedings of
EMNLP-1998.
W. Daelemans, J. Zavrel, K. van der Sloot,
and A. van den Bosch. 2003. Timbl:
Tilburg memory based learner, version 5.0,
reference guide. Technical Report ILK
03-10, Tilburg University. Available from
http://ilk.uvt.nl/downloads/pub/
papers/ilk0310.ps.gz.
M. Fleischman, N. Kwon, and E. Hovy. 2003.
Maximum entropy models for FrameNet classi-
fication. In Proceedings ofEMNLP-2003.
D. Gildea and D. Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
C. R. Johnson, C. J. Fillmore, M. R. L. Petruck,
C. F. Baker, M. J. Ellsworth, J. Ruppenhofer, and
E. J. Wood. 2002. FrameNet: Theory and Prac-
tice. http://www.icsi.berkeley.edu/
~framenet/book/book.html.
R. Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Pro-
ceedings of CoNLL-2002.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In Proceeedings of COLING-
2000.
H. Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of
NeMLP-1994.
H. Schmid, 2000. LoPar – Design und Implemen-
tation. Institute for Computational Linguistics,
University of Stuttgart.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.113344">
<note confidence="0.7440955">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.857905333333333">Association for Computational Linguistics Semantic Role Labelling with Similarity-Based Generalization EM-based Clustering</title>
<author confidence="0.926656">Ulrike Baldewein</author>
<author confidence="0.926656">Katrin Erk</author>
<author confidence="0.926656">Sebastian Padó Detlef Prescher</author>
<affiliation confidence="0.999571">Saarland University University of Amsterdam</affiliation>
<address confidence="0.9961">Saarbrücken, Germany Amsterdam, The Netherlands</address>
<email confidence="0.963422">ulrike@coli.uni-sb.deprescher@science.uva.nl</email>
<email confidence="0.963422">erk@coli.uni-sb.deprescher@science.uva.nl</email>
<email confidence="0.963422">pado@coli.uni-sb.deprescher@science.uva.nl</email>
<abstract confidence="0.971425625">We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of training data available and on the use of EM-based clustering to improve role assignment. Our final score</abstract>
<note confidence="0.552985">is Precision=73.6%, Recall=59.4% (F=65.7).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>U Baldewein</author>
<author>K Erk</author>
<author>S Pado</author>
<author>D Prescher</author>
</authors>
<title>Semantic role labelling with chunk sequences.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<marker>Baldewein, Erk, Pado, Prescher, 2004</marker>
<rawString>U. Baldewein, K. Erk, S. Pado, and D. Prescher. 2004. Semantic role labelling with chunk sequences. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of EMNLP-1998.</booktitle>
<contexts>
<context position="1986" citStr="Carroll and Rooth, 1998" startWordPosition="285" endWordPosition="288">e features and in the generalisations mentioned above. The basis of our experiments was formed by off-the-shelf statistical tools for data processing and modelling. After listing our data preparation steps (Sec. 2) and features (Sec. 3), we describe our classification procedure and the learners we used (Sec. 4). Sec. 5 outlines our experiments in similarity-based generalisations, and Section 6 discusses our results. 2 Data and Instances Parsing. To tag and parse the data, we used LoPar (Schmid, 2000), a probabilistic contextfree parser, which comes with a Head-Lexicalised Grammar for English (Carroll and Rooth, 1998). We considered only the most probable parse for each sentence and simplified parse trees by eliminating unary nodes. The resulting nodes form the instances of our classification. We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads. Projection of role labels. FrameNet provides semantic roles as character offsets. We labelled those instances (i.e. nodes in the parse tree) with gold standard semantic roles which corresponded to roles’ maximal projections. 13.95% of roles in the training corpus spanned more than one parse tree node. Figure 1 shows an example sentence fo</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>G. Carroll and M. Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of EMNLP-1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 5.0, reference guide.</title>
<date>2003</date>
<tech>Technical Report ILK 03-10,</tech>
<institution>Tilburg University.</institution>
<note>Available from http://ilk.uvt.nl/downloads/pub/ papers/ilk0310.ps.gz.</note>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2003. Timbl: Tilburg memory based learner, version 5.0, reference guide. Technical Report ILK 03-10, Tilburg University. Available from http://ilk.uvt.nl/downloads/pub/ papers/ilk0310.ps.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>N Kwon</author>
<author>E Hovy</author>
</authors>
<title>Maximum entropy models for FrameNet classification.</title>
<date>2003</date>
<booktitle>In Proceedings ofEMNLP-2003.</booktitle>
<contexts>
<context position="6045" citStr="Fleischman et al., 2003" startWordPosition="938" endWordPosition="941">ld single out the two NPs as possible role fillers, while the second step would assign the COGNIZER and CONTENT roles. Maximum Entropy Learning. Our first classifier was a log-linear model, where the probability of a class given an feature vector is defined as where is a normalisation constant, the value of feature for class, and the weight assigned to. The model is trained by optimising the weights subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt. Maximum Entropy (Maxent) models have been successfully applied to semantic role labelling (Fleischman et al., 2003). We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. Memory-based Learning. Our second learner implements an instance of a memory-based learning (MBL) algorithm, namely the -nearest neighbour algorithm. This algorithm classifies test instances by assigning them the label of the most similar examples from the training set. Its parameters are the number of training examples to be considered, the similarity metric, and the feature weighting scheme. We used the implementation provided by TiMBL (Daelemans et al., 2</context>
</contexts>
<marker>Fleischman, Kwon, Hovy, 2003</marker>
<rawString>M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum entropy models for FrameNet classification. In Proceedings ofEMNLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="16218" citStr="Gildea and Jurafsky (2002)" startWordPosition="2546" endWordPosition="2549">d not show much impact for us. The Maxent learner was negatively affected by sentencelevel features such as the subcat frame and “is subcategorised”. Comparing the learners. In a comparable basic setting (all features, no generalisation), the Memory-Based learner easily outperforms the Maxent learner, according to our scoring scheme. However, the official scoring scheme determines the Memory-based learner’s performance at more than 10 points F-score below the Maxent learner. We intend to run the Memory-based learner with generalisation data for a more comprehensive comparison. Generalisation. Gildea and Jurafsky (2002) report an improvement of 1.6% through generalisation, which is roughly comparable to our figures. The two strategies share the common idea of exploiting role similarities, but the realisations are converse: Gildea and Jurafsky manually compact similar frame elements into 18 abstract, frameindependent roles, whereas we keep the roles framespecific but augment the training data for each by automatically discovered similarities. One reason for the disappointing performance of the FrameNet hierarchy-based generalisation strategies may be simply the amount of data, as shown by Table 4: FN-h (sem) </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Johnson</author>
<author>C J Fillmore</author>
<author>M R L Petruck</author>
<author>C F Baker</author>
<author>M J Ellsworth</author>
<author>J Ruppenhofer</author>
<author>E J Wood</author>
</authors>
<title>FrameNet: Theory and Practice.</title>
<date>2002</date>
<note>http://www.icsi.berkeley.edu/ ~framenet/book/book.html.</note>
<contexts>
<context position="1099" citStr="Johnson et al., 2002" startWordPosition="150" endWordPosition="153">ct We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of training data available and on the use of EM-based clustering to improve role assignment. Our final score is Precision=73.6%, Recall=59.4% (F=65.7). 1 Introduction This paper describes a study in semantic role labelling in the context of the Senseval III task, for which the training and test data were both drawn from the current FrameNet release (Johnson et al., 2002). We concentrated on two questions: first, whether role assignment can be improved by generalisation over training instances using different similarity measures; and second, the impact of EMbased clustering, both in deriving more informative selectional preference features and in the generalisations mentioned above. The basis of our experiments was formed by off-the-shelf statistical tools for data processing and modelling. After listing our data preparation steps (Sec. 2) and features (Sec. 3), we describe our classification procedure and the learners we used (Sec. 4). Sec. 5 outlines our exp</context>
</contexts>
<marker>Johnson, Fillmore, Petruck, Baker, Ellsworth, Ruppenhofer, Wood, 2002</marker>
<rawString>C. R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F. Baker, M. J. Ellsworth, J. Ruppenhofer, and E. J. Wood. 2002. FrameNet: Theory and Practice. http://www.icsi.berkeley.edu/ ~framenet/book/book.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<contexts>
<context position="6143" citStr="Malouf, 2002" startWordPosition="955" endWordPosition="956">NT roles. Maximum Entropy Learning. Our first classifier was a log-linear model, where the probability of a class given an feature vector is defined as where is a normalisation constant, the value of feature for class, and the weight assigned to. The model is trained by optimising the weights subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt. Maximum Entropy (Maxent) models have been successfully applied to semantic role labelling (Fleischman et al., 2003). We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf. Memory-based Learning. Our second learner implements an instance of a memory-based learning (MBL) algorithm, namely the -nearest neighbour algorithm. This algorithm classifies test instances by assigning them the label of the most similar examples from the training set. Its parameters are the number of training examples to be considered, the similarity metric, and the feature weighting scheme. We used the implementation provided by TiMBL (Daelemans et al., 2003) with the default parameters, i.e. =1 and the weighted overlap similarity metric with gain rat</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>R. Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of CoNLL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Prescher</author>
<author>S Riezler</author>
<author>M Rooth</author>
</authors>
<title>Using a probabilistic class-based lexicon for lexical ambiguity resolution.</title>
<date>2000</date>
<booktitle>In Proceeedings of COLING2000.</booktitle>
<contexts>
<context position="3256" citStr="Prescher et al., 2000" startWordPosition="489" endWordPosition="492">antic role labels are given in small caps, and the target predicate is marked in boldface. S (NONE) Figure 1: Example parse tree with role labels Semantic clustering. We used clustering to generalise over possible fillers of roles. In a first model, we derived a probability distribution for pairs , where is a target:role combination and is the head lemma of a role filler. The key idea is that and are mutually independent, but conditioned on an unobserved class . In this manner, we define the probability of as: Estimation was performed using a variant of the expectation-maximisation algorithm (Prescher et al., 2000). We used this model both as a feature and in the generalisation described in Sec. 5. In a second model, we clustered pairs of target:role and the NP (COGNIZER) VP (NONE) V (NONE) does not VP (NONE) know NP (CONTENT) the answer Peter syntactic properties of the role fillers; the resulting model was only used for generalisation. 3 Features Constituent features. The first group of features represents properties of instances (i.e. constituents). We used the phrase type and head lemma of each constituent, its preposition, if any (otherwise NONE), its relative position with respect to the target (l</context>
<context position="9879" citStr="Prescher et al., 2000" startWordPosition="1517" endWordPosition="1520"> are similar if they are appropriate for some common cluster. For the head lemma clustering model, we define the appropriateness of a target:role pair for a cluster as follows: where is the total frequency of all head lemmas that have been seen with , weighted by the class-membership probability of in . This appropriateness measure is built on top of the class-based frequencies rather than on the frequencies or the class-membership probabilities in isolation: For some tasks the combination of lexical and semantic information has been shown to outperform each of the single information sources (Prescher et al., 2000). Our similarity notion is now formalised as follows: With a threshold as a parameter, two frame elements , count as similar if for some class, and . In the syntactic clustering model, a role filler was described as a combination of the path from instance to target, the instance’s preposition, and the target voice. The appropriateness of a target:role pair is defined as for the above model. For time reasons, only verbal targets were considered. Figure 2 shows excerpts of two “syntactic” clusters in the form of target:frame.role members. Group 6 is a very homogeneous group, consisting of roles </context>
</contexts>
<marker>Prescher, Riezler, Rooth, 2000</marker>
<rawString>D. Prescher, S. Riezler, and M. Rooth. 2000. Using a probabilistic class-based lexicon for lexical ambiguity resolution. In Proceeedings of COLING2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of NeMLP-1994.</booktitle>
<contexts>
<context position="2213" citStr="Schmid, 1994" startWordPosition="322" endWordPosition="323">we describe our classification procedure and the learners we used (Sec. 4). Sec. 5 outlines our experiments in similarity-based generalisations, and Section 6 discusses our results. 2 Data and Instances Parsing. To tag and parse the data, we used LoPar (Schmid, 2000), a probabilistic contextfree parser, which comes with a Head-Lexicalised Grammar for English (Carroll and Rooth, 1998). We considered only the most probable parse for each sentence and simplified parse trees by eliminating unary nodes. The resulting nodes form the instances of our classification. We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads. Projection of role labels. FrameNet provides semantic roles as character offsets. We labelled those instances (i.e. nodes in the parse tree) with gold standard semantic roles which corresponded to roles’ maximal projections. 13.95% of roles in the training corpus spanned more than one parse tree node. Figure 1 shows an example sentence for the AWARENESS frame. The nodes’ respective semantic role labels are given in small caps, and the target predicate is marked in boldface. S (NONE) Figure 1: Example parse tree with role labels Semantic clustering. We used clus</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of NeMLP-1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<date>2000</date>
<booktitle>LoPar – Design und Implementation.</booktitle>
<institution>Institute for Computational Linguistics, University of Stuttgart.</institution>
<contexts>
<context position="1867" citStr="Schmid, 2000" startWordPosition="270" endWordPosition="271">sures; and second, the impact of EMbased clustering, both in deriving more informative selectional preference features and in the generalisations mentioned above. The basis of our experiments was formed by off-the-shelf statistical tools for data processing and modelling. After listing our data preparation steps (Sec. 2) and features (Sec. 3), we describe our classification procedure and the learners we used (Sec. 4). Sec. 5 outlines our experiments in similarity-based generalisations, and Section 6 discusses our results. 2 Data and Instances Parsing. To tag and parse the data, we used LoPar (Schmid, 2000), a probabilistic contextfree parser, which comes with a Head-Lexicalised Grammar for English (Carroll and Rooth, 1998). We considered only the most probable parse for each sentence and simplified parse trees by eliminating unary nodes. The resulting nodes form the instances of our classification. We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads. Projection of role labels. FrameNet provides semantic roles as character offsets. We labelled those instances (i.e. nodes in the parse tree) with gold standard semantic roles which corresponded to roles’ maximal projectio</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>H. Schmid, 2000. LoPar – Design und Implementation. Institute for Computational Linguistics, University of Stuttgart.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>