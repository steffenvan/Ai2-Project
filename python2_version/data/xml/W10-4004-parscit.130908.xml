<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022072">
<title confidence="0.9877485">
How to Get the Same News from Different Language News
Papers
</title>
<author confidence="0.959849">
T. Pattabhi R. K Rao Sobha Lalitha Devi
</author>
<affiliation confidence="0.969042">
AU-KBC Research Centre AU-KBC Research Centre
Anna University Chennai Anna University Chennai
</affiliation>
<email confidence="0.850914">
sobha@au-kbc.org
</email>
<sectionHeader confidence="0.99003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941842105263">
This paper presents an ongoing work on
identifying similarity between documents
across News papers in different
languages. Our aim is to identify similar
documents for a given News or event as
a query, across languages and make cross
lingual search more accurate and easy.
For example given an event or News in
English, all the English news documents
related to the query are retrieved as well
as in other languages such as Hindi,
Bengali, Tamil, Telugu, Malayalam,
Spanish. We use Vector Space Model, a
known method for similarity calculation,
but the novelty is in identification of
terms for VSM calculation. Here a robust
translation system is not used for
translating the documents. The system is
working with good recall and precision.
</bodyText>
<sectionHeader confidence="0.999" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999486666666667">
In this paper we present a novel method for
identifying similar News documents from
various language families such as Indo-
European, Indo- Aryan and Dravidian. The
languages considered from the above language
families are English, Hindi, Bengali, Tamil,
Telugu, Malayalam and Spanish. The News
documents in various languages are obtained
using a crawler. The documents are represented
as vector of terms.
Given a query in any of the language mentioned
above, the documents relevant to the query are
retrieved. The first two document retrieved in the
language of the query is taken as base for the
identification of similar documents. The
documents are converted into terms and the
terms are translated to other languages using
bilingual dictionaries. The terms thus obtained is
used for similarity calculation. The paper is
further organized as follows. In the following
section 2, related work is described. In section 3,
the algorithm is discussed. Section 4 describes
experiments and results. The paper concludes
with section 5.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999921681818182">
In the past decade there has been significant
amount of work done on finding similarity of
documents and organizing the documents
according to their content. Similarity of
documents are identified using different methods
such as Self-Organizing Maps (SOMs)
(Kohonen et al, 2000; Rauber, 1999), based on
Ontologies and taxanomy (Gruber, 1993; Resnik,
1995), Vector Space Model (VSM) with
similarity measures like Dice similarity,
Jaccard’s similarity, cosine similarity (Salton,
1989).
Many similarity measures were developed,
such as information content (Resnik, 1995)
mutual information (Hindle, 1990), Dice
coefficient (Frakes and Baeza-Yates, 1992),
cosine coefficient (Frakes and Baeza-Yates,
1992), distance-based measurements (Lee et al.,
1989; Rada et al., 1989), and feature contrast
model (Tversky, 1977). McGill etc. surveyed
and compared 67 similarity measures used in
information retrieval (McGill et al., 1979).
</bodyText>
<page confidence="0.990588">
11
</page>
<note confidence="0.429949">
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15,
</note>
<keyword confidence="0.436903">
Beijing, August 2010
</keyword>
<sectionHeader confidence="0.997446" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999940807692308">
Similarity is a fundamental concept. Two
documents can be said to be similar if both the
documents have same content, describing a topic
or an event or an entity. Similarity is a measure
of degree of resemblance, or commonality
between the documents.
In this work we have used Vector Space
Model (VSM) for document representation. In
VSM the documents are represented as vectors
of unique terms. Here we have performed
experiments by creating three types of document
vector space models. In the first case we have
taken all unique words in the document
collection for vector of terms. In the second case
we take the terms after removing all stop words.
In the third case we have taken a sequence of
words as terms. After the document model is
built we use cosine similarity measure to identify
the degree of similarity between documents.
In this work we have taken documents from
the languages mentioned in the previous section.
For the purpose of identifying similar documents
across the languages we use map of term vectors
of documents from English to other languages.
Using the term vector map we can identify
similar documents for various languages.
</bodyText>
<subsectionHeader confidence="0.999968">
3.1 Similarity analyser
</subsectionHeader>
<bodyText confidence="0.999751430555556">
The main modules are i) Document vector
creator ii) Translator and iii) Similarity
identifier.
a) Document Vector Creator: Each document
is represented as vector of terms. Here we take
three types of term vectors. In the first type a
single word is taken as a term which is the
standard implementation of VSM. In the second
type single words are taken but the stop words
are removed.
In the third type each term is a sequence of
words, where we define the number of words in
the sequence as 4. This moving window of 4 is
obtained by performing many experiments using
different combinations of words. So our term of
vector is defined as a set of four consecutive
words, where the last three words in the
preceding sequence is considered as the first
three words in the following sequence. For
example if a sentence has 10 words (w), the
vector of terms for this sentence is w1w2w3w4,
w2w3w4w5, w3w4w5w6, w4w5w6w7,
w5w6w7w8, w6w7w8w9, w7w8w9w10. The
weights of the terms in the vector are the term
frequency and inverse document frequency (tf-
idf). While creating document vectors, for Indian
languages which are highly agglutinative and
morphologically rich we use morphological
analyzer to reduce the word into its root and it is
used for document vector creation.
The first two experiments are the standard
VSM implementation. The third experiment
differs in the way the terms are taken for
building the VSM. For building the VSM model
which is common for all language document
texts, it is essential that there should be
translation/transliteration tool. First the terms are
collected from individual language documents
and a unique list is formed. The unique list of
words is then translated using the translator
module.
b) Word by Word Translator: In this module,
the terms from English documents are taken and
are translated to different languages. The
translation is done word by word with the use of
bilingual and multilingual synset dictionaries.
This translation creates a map of terms from
English to different languages. We have used
bilingual dictionaries from English to Spanish,
Hindi, Tamil, Telugu, and Malayalam
dictionaries. Also we have used multilingual
synset dictionaries for English, Tamil, Telugu,
Hindi, and Malayalam. For each pair of bilingual
dictionaries there are more than 100K root
words. Since in this work we do not require
syntactically and semantically correct translation
of the sentences we adopted word to word
translation. Hence we did not use any other
system such as SMT for English to Indian
languages. Named entities require transliteration.
Here we have used a transliteration tool. This
tool uses rule based approach, based on the
phoneme match. The transliteration tool
produces all possible transliteration outputs.
Here we take into consideration the top five best
possible outputs. For example the name “Lal
Krishna Advani” would get transliterations in
Indian languages as “laala krishna athvaani”,
“laala krishna advaani”.
c) Similarity Identifier: The similarity
identifier module takes the query in the form
document as input and identifies all relevant
</bodyText>
<page confidence="0.994014">
12
</page>
<bodyText confidence="0.999958571428571">
documents. The similarity identifier uses cosine
similarity measure over documents vector
creator. The cosine similarity measure is the dot
product of two vectors and is between 0 and 1
value. The more it is closer to 1, the similarity is
more. The formula of cosine similarity is as
follows:
</bodyText>
<equation confidence="0.983738">
Sim(S1,S2)tj = Σ (W1j x W2j ) -- (1)
</equation>
<bodyText confidence="0.963832285714286">
Where,
tj is a term present in both vectors S1and S2.
W1j is the weight of term tj in S1 and
W2j is the weight of term tj in S2.
The weight of term tj in the vector S1 is
calculated by the formula given by equation (2),
below.
</bodyText>
<equation confidence="0.8332065">
Wij=(tf*log(N/df))/[sqrt(Si12+Si22+...+Sin2)]
--(2)
</equation>
<bodyText confidence="0.963788230769231">
Where,
tf = term frequency of term tj
N=total number of documents in the collection
df = number of documents in the collection that
the term tj occurs in.
sqrt represents square root
The denominator
[sqrt(Si12+Si22+ +Sin2)] is the cosine
normalization factor. This cosine normalization
factor is the Euclidean length of the vector Si,
where ‘i’ is the document number in the
collection and Sin2 is the square of the product
of (tf*log(N/df)) for term in the vector Si.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999720055555556">
We have performed three experiments with two
different data sets. The first data set was
collected by crawling the web for a single day’s
news articles and obtained 1000 documents from
various online news magazines in various
languages. The test set was taken from Times of
India, The Hindu for English, BBC, Dinamani,
Dinamalar for Tamil, Yahoo for Telugu,
Matrubhumi for Malayalam, BBC and Dainik
Jagran for Hindi and BBC for Spanish. The
distribution of documents in the first set for
various languages is as follows: 300 English,
200 Tamil, 150 Telugu, 125 Hindi, 125
Malayalam, 50 Spanish. The figure 1 given
below shows the language distribution in this
first set.
The number of similar documents were 600 in
this set.
</bodyText>
<figureCaption confidence="0.996875">
Figure 1. Data Distribution of Set 1
</figureCaption>
<bodyText confidence="0.998823833333333">
In the second data set we have taken news
documents of one week time duration. This
consisted of 9750 documents. The language
distribution for this data set is shown in figure 2.
This second data set consisted of 5350 similar
documents.
</bodyText>
<figureCaption confidence="0.997788">
Figure 2. Data Distribution of Set 2
</figureCaption>
<bodyText confidence="0.993966">
In the first experiment we took all the unique
words (separated by white space) as terms for
building the document vector. In the second
experiment the terms taken were same as the
first experiment, except that all the stop words
were removed. In the third experiment, the terms
taken for document vector creation were four
consecutive words. The results obtained for
</bodyText>
<tableCaption confidence="0.8686785">
three experiments for data set 1 is shown in
Table 1. And results for data set 2 are shown in
Table 2. Table 3 shows the similarity
identification for various languages.
</tableCaption>
<bodyText confidence="0.999527833333333">
Here we take a news story document as a
query and perform similarity analysis across all
documents in the document collection to identify
similarly occurring news stories. In the first data
set in the gold standard there are 600 similar
pairs of documents. And in the second data set
there are 5350 similar pairs of documents in the
gold standard.
It is observed that even though there were
more similar documents which could have been
identified, but the system could not identify
those documents. The cosine measure for those
</bodyText>
<figure confidence="0.99854575">
English
Tamil
Telugu
Hindi
Malyalam
Spanish
English
Tamil
Telugu
Hindi
Malayalam
Spanish
</figure>
<page confidence="0.997319">
13
</page>
<bodyText confidence="0.999755620689655">
unidentified documents was found to be lower
than 0.8. We have taken 0.8 as the threshold for
documents to be considered similar. In the
documents which were not identified by the
system, the content described consisted of less
number of words. These were mostly two
paragraph documents; hence the similarity score
obtained was less than the threshold. In
experiment three, we find that the number of
false positives is decreased and also the number
of documents identified similar is increased. This
is because, in this case the system sees for terms
of four words and hence single word matches are
reduced. This reduces false positives. The other
advantage of this is the words get the context, in
a sense that the words in each sequence are not
independent. The words get an order and are
sensitive to that order. This solves sense
disambiguation. Hence we find that it is solving
the polysemy problem to some extent. The
system can be further improved by creating
robust map files between terms in different
languages. The bilingual dictionaries also need
to be improved.
In our work, since we are using a sequence of
words as terms for document vectors, we do not
require proper, sophisticated translation systems.
A word by word translation would suffice to get
the desired results.
</bodyText>
<table confidence="0.998632833333333">
Exp Gold std System System Pre Rec
No Similari Identified Identified c %
ty Correct Wrong %
1 600 534 50 91.4 89.0
2 600 547 44 92.5 91.2
3 600 565 10 98.3 94.2
</table>
<tableCaption confidence="0.997313">
Table 1. Similarity Results on Data Set 1
</tableCaption>
<table confidence="0.999730142857143">
Exp Gold System System Prec Rec
No Standard Identified Identifi % %
Similarity Correct ed
Wrong
1 5350 4820 476 91.0 90.0
2 5350 4903 410 92.3 91.6
3 5350 5043 114 97.8 94.3
</table>
<tableCaption confidence="0.996184">
Table 2. Similarity Results on Data Set 2
</tableCaption>
<table confidence="0.999817454545455">
Lang Gold System System Prec Rec
Std Identifi Identifi % %
similar ed ed
docs correct wrong
Eng 1461 1377 30 97.86 94.25
Span 732 690 15 97.87 94.26
Hin 588 554 11 98.05 94.22
Mal 892 839 19 97.78 94.05
Tam 932 880 22 97.56 94.42
Tel 745 703 17 97.63 94.36
AVG 97.79 94.26
</table>
<tableCaption confidence="0.998425">
Table 3.Similarity Results Data Set with Ex:3
</tableCaption>
<sectionHeader confidence="0.98938" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999902375">
Here we have shown how we can identify
similar News document in various languages.
The results obtained are encouraging; we obtain
an average precision of 97.8% and recall of
94.3%. This work differs from previous works in
two aspects: 1) no language preprocessing of the
documents is required and 2) terms taken for
VSM are a sequence of four words.
</bodyText>
<sectionHeader confidence="0.999008" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999508444444444">
Frakes, W. B. and Baeza-Yates, R., editors 1992.
Information Retrieval, Data Structure and
Algorithms. Prentice Hall.
T. R. Gruber. 1993. A translation approach to
portable ontologies, Knowledge Acquisition,
5(2):199–220.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
pages 268–275, Pittsburg, Pennsylvania.
Kohonen, Teuvo Kaski, Samuel Lagus, Krista
Salojarvi, Jarkko Honkela, Jukka Paatero,Vesa
Saarela, Anti. 2000. Self organisation of a massive
document collection, IEEE Transactions on Neural
Networks, 11(3): 574-585.
Lee, J. H., Kim, M. H., and Lee, Y. J. 1989.
Information retrieval based on conceptual distance
in is-a hierarchies. Journal of Documentation,
49(2):188–207.
McGill et al., M. 1979. An evaluation of factors
affecting document ranking by information
retrieval systems. Project report, Syracuse
University School of Information Studies.
Rauber, Andreas Merkl, Dieter. 1999. The SOMLib
digital library system, In the Proceedings of the
3rd European Conference on Research and
Advanced Technology for Digital Libraries
(ECDL&apos;99), Paris, France. Berlin: 323-341.
</reference>
<page confidence="0.984232">
14
</page>
<reference confidence="0.999692461538462">
Rada, R., Mili, H., Bicknell, E., and Blettner, M.
1989. Development and application of a metric on
semantic nets. IEEE Transaction on Systems, Man,
and Cybernetics, 19(1):17–30.
P. Resnik. 1995. Using information content to
evaluate semantic similarity in taxonomy,
Proceedings of IJCAI: 448–453.
Salton, Gerald. 1989. Automatic Text Processing: The
Transformation, Analysis and Retrieval of
Information by Computer, Reading, MA: Addison
Wesley
Tversky, A. 1977. Features of similarity.
Pychological Review, 84:327–352.
</reference>
<page confidence="0.997931">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685704">
<title confidence="0.999083">How to Get the Same News from Different Language News Papers</title>
<author confidence="0.997249">T Pattabhi R K Rao Sobha Lalitha Devi</author>
<affiliation confidence="0.9979285">AU-KBC Research Centre AU-KBC Research Centre Anna University Chennai Anna University Chennai</affiliation>
<email confidence="0.992664">sobha@au-kbc.org</email>
<abstract confidence="0.96741495">This paper presents an ongoing work on identifying similarity between documents across News papers in different languages. Our aim is to identify similar documents for a given News or event as a query, across languages and make cross lingual search more accurate and easy. For example given an event or News in English, all the English news documents related to the query are retrieved as well as in other languages such as Hindi, Bengali, Tamil, Telugu, Malayalam, Spanish. We use Vector Space Model, a known method for similarity calculation, but the novelty is in identification of terms for VSM calculation. Here a robust translation system is not used for translating the documents. The system is working with good recall and precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R Baeza-Yates</author>
</authors>
<date>1992</date>
<booktitle>Information Retrieval, Data Structure and Algorithms.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="2675" citStr="Frakes and Baeza-Yates, 1992" startWordPosition="406" endWordPosition="409">re has been significant amount of work done on finding similarity of documents and organizing the documents according to their content. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15, Beijing, August 2010 3 Methodology Similarity is a fundamental concept. Two documents can be said to be similar if both the documents have same content, describing a topic or an event or an entity. Similarity is </context>
</contexts>
<marker>Frakes, Baeza-Yates, 1992</marker>
<rawString>Frakes, W. B. and Baeza-Yates, R., editors 1992. Information Retrieval, Data Structure and Algorithms. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Gruber</author>
</authors>
<title>A translation approach to portable ontologies,</title>
<date>1993</date>
<journal>Knowledge Acquisition,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="2364" citStr="Gruber, 1993" startWordPosition="369" endWordPosition="370">is used for similarity calculation. The paper is further organized as follows. In the following section 2, related work is described. In section 3, the algorithm is discussed. Section 4 describes experiments and results. The paper concludes with section 5. 2 Related Work In the past decade there has been significant amount of work done on finding similarity of documents and organizing the documents according to their content. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings </context>
</contexts>
<marker>Gruber, 1993</marker>
<rawString>T. R. Gruber. 1993. A translation approach to portable ontologies, Knowledge Acquisition, 5(2):199–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90,</booktitle>
<pages>268--275</pages>
<location>Pittsburg, Pennsylvania.</location>
<contexts>
<context position="2626" citStr="Hindle, 1990" startWordPosition="402" endWordPosition="403">lated Work In the past decade there has been significant amount of work done on finding similarity of documents and organizing the documents according to their content. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15, Beijing, August 2010 3 Methodology Similarity is a fundamental concept. Two documents can be said to be similar if both the documents have same content, describing</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. 1990. Noun classification from predicateargument structures. In Proceedings of ACL-90, pages 268–275, Pittsburg, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teuvo Kaski Kohonen</author>
<author>Samuel Lagus</author>
</authors>
<title>Krista Salojarvi, Jarkko Honkela, Jukka Paatero,Vesa Saarela, Anti.</title>
<date>2000</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>11</volume>
<issue>3</issue>
<pages>574--585</pages>
<marker>Kohonen, Lagus, 2000</marker>
<rawString>Kohonen, Teuvo Kaski, Samuel Lagus, Krista Salojarvi, Jarkko Honkela, Jukka Paatero,Vesa Saarela, Anti. 2000. Self organisation of a massive document collection, IEEE Transactions on Neural Networks, 11(3): 574-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lee</author>
<author>M H Kim</author>
<author>Y J Lee</author>
</authors>
<title>Information retrieval based on conceptual distance in is-a hierarchies.</title>
<date>1989</date>
<journal>Journal of Documentation,</journal>
<volume>49</volume>
<issue>2</issue>
<contexts>
<context position="2773" citStr="Lee et al., 1989" startWordPosition="418" endWordPosition="421">ording to their content. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15, Beijing, August 2010 3 Methodology Similarity is a fundamental concept. Two documents can be said to be similar if both the documents have same content, describing a topic or an event or an entity. Similarity is a measure of degree of resemblance, or commonality between the documents. In this work we have use</context>
</contexts>
<marker>Lee, Kim, Lee, 1989</marker>
<rawString>Lee, J. H., Kim, M. H., and Lee, Y. J. 1989. Information retrieval based on conceptual distance in is-a hierarchies. Journal of Documentation, 49(2):188–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McGill</author>
</authors>
<title>An evaluation of factors affecting document ranking by information retrieval systems. Project report,</title>
<date>1979</date>
<institution>Syracuse University School of Information Studies.</institution>
<marker>McGill, 1979</marker>
<rawString>McGill et al., M. 1979. An evaluation of factors affecting document ranking by information retrieval systems. Project report, Syracuse University School of Information Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Merkl Rauber</author>
<author>Dieter</author>
</authors>
<title>The SOMLib digital library system,</title>
<date>1999</date>
<booktitle>In the Proceedings of the 3rd European Conference on Research and Advanced Technology for Digital Libraries (ECDL&apos;99),</booktitle>
<pages>323--341</pages>
<location>Paris, France. Berlin:</location>
<marker>Rauber, Dieter, 1999</marker>
<rawString>Rauber, Andreas Merkl, Dieter. 1999. The SOMLib digital library system, In the Proceedings of the 3rd European Conference on Research and Advanced Technology for Digital Libraries (ECDL&apos;99), Paris, France. Berlin: 323-341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rada</author>
<author>H Mili</author>
<author>E Bicknell</author>
<author>M Blettner</author>
</authors>
<title>Development and application of a metric on semantic nets.</title>
<date>1989</date>
<journal>IEEE Transaction on Systems, Man, and Cybernetics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="2793" citStr="Rada et al., 1989" startWordPosition="422" endWordPosition="425">ntent. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15, Beijing, August 2010 3 Methodology Similarity is a fundamental concept. Two documents can be said to be similar if both the documents have same content, describing a topic or an event or an entity. Similarity is a measure of degree of resemblance, or commonality between the documents. In this work we have used Vector Space Model</context>
</contexts>
<marker>Rada, Mili, Bicknell, Blettner, 1989</marker>
<rawString>Rada, R., Mili, H., Bicknell, E., and Blettner, M. 1989. Development and application of a metric on semantic nets. IEEE Transaction on Systems, Man, and Cybernetics, 19(1):17–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in taxonomy,</title>
<date>1995</date>
<booktitle>Proceedings of IJCAI:</booktitle>
<pages>448--453</pages>
<contexts>
<context position="2379" citStr="Resnik, 1995" startWordPosition="371" endWordPosition="372">milarity calculation. The paper is further organized as follows. In the following section 2, related work is described. In section 3, the algorithm is discussed. Section 4 describes experiments and results. The paper concludes with section 5. 2 Related Work In the past decade there has been significant amount of work done on finding similarity of documents and organizing the documents according to their content. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th Inte</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in taxonomy, Proceedings of IJCAI: 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer,</title>
<date>1989</date>
<publisher>Addison Wesley</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="2507" citStr="Salton, 1989" startWordPosition="387" endWordPosition="388">on 3, the algorithm is discussed. Section 4 describes experiments and results. The paper concludes with section 5. 2 Related Work In the past decade there has been significant amount of work done on finding similarity of documents and organizing the documents according to their content. Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15, Beijing, August 2010 3 Methodology Similarit</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, Gerald. 1989. Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer, Reading, MA: Addison Wesley</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Pychological Review,</journal>
<pages>84--327</pages>
<contexts>
<context position="2837" citStr="Tversky, 1977" startWordPosition="430" endWordPosition="431">ing different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccard’s similarity, cosine similarity (Salton, 1989). Many similarity measures were developed, such as information content (Resnik, 1995) mutual information (Hindle, 1990), Dice coefficient (Frakes and Baeza-Yates, 1992), cosine coefficient (Frakes and Baeza-Yates, 1992), distance-based measurements (Lee et al., 1989; Rada et al., 1989), and feature contrast model (Tversky, 1977). McGill etc. surveyed and compared 67 similarity measures used in information retrieval (McGill et al., 1979). 11 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 11–15, Beijing, August 2010 3 Methodology Similarity is a fundamental concept. Two documents can be said to be similar if both the documents have same content, describing a topic or an event or an entity. Similarity is a measure of degree of resemblance, or commonality between the documents. In this work we have used Vector Space Model (VSM) for document representation. In VSM t</context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Tversky, A. 1977. Features of similarity. Pychological Review, 84:327–352.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>