<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.997446">
Vector-space calculation of semantic surprisal for predicting word
pronunciation duration
</title>
<author confidence="0.991945">
Asad Sayeed, Stefan Fischer, and Vera Demberg
</author>
<affiliation confidence="0.983362">
Computational Linguistics and Phonetics/M2CI Cluster of Excellence
Saarland University
</affiliation>
<address confidence="0.781644">
66123 Saarbr¨ucken, Germany
</address>
<email confidence="0.997604">
{asayeed,sfischer,vera}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.997367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951740740741">
In order to build psycholinguistic mod-
els of processing difficulty and evaluate
these models against human data, we need
highly accurate language models. Here we
specifically consider surprisal, a word’s
predictability in context. Existing ap-
proaches have mostly used n-gram models
or more sophisticated syntax-based pars-
ing models; this largely does not account
for effects specific to semantics. We build
on the work by Mitchell et al. (2010) and
show that the semantic prediction model
suggested there can successfully predict
spoken word durations in naturalistic con-
versational data.
An interesting finding is that the training
data for the semantic model also plays
a strong role: the model trained on in-
domain data, even though a better lan-
guage model for our data, is not able to
predict word durations, while the out-of-
domain trained language model does pre-
dict word durations. We argue that this at
first counter-intuitive result is due to the
out-of-domain model better matching the
“language models” of the speakers in our
data.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853125">
The Uniform Information Density (UID) hypothe-
sis holds that speakers tend to maintain a relatively
constant rate of information transfer during speech
production (e.g., Jurafsky et al., 2001; Aylett and
Turk, 2006; Frank and Jaeger, 2008). The rate
of information transfer is thereby quantified using
as each words’ Surprisal (Hale, 2001), that is, a
word’s negative log probability in context.
</bodyText>
<equation confidence="0.931556">
Surprisal(wi) = −log P(wi|w1..wi−1)
</equation>
<bodyText confidence="0.999932609756098">
This work makes use of an existing measure of
semantic surprisal calculated from a distributional
space in order to test whether this measure ac-
counts for an effect of UID on speech production.
Our hypothesis is that a word in a semantically
surprising context is pronounced with a slightly
longer duration than the same word in a seman-
tically less-expected context. In this way, a more
uniform rate of information transfer is achieved,
because the higher information content of the un-
expected word is stretched over a slightly longer
time. To our knowledge, the use of this form of
surprisal as a pronunciation predictor has never
been investigated.
The intuition is thus: in a sentence like the
sheep ate the long grass, the word grass will have
relatively high surprisal if the context only con-
sists of the long. However, a distributional repre-
sentation that retains the other content words in the
sentence, thus representing the contextual similar-
ity of grass to sheep ate, would able to capture the
relevant context for content word prediction more
easily. In the approach taken here, both types of
models are combined: a standard language model
is reweighted with semantic similarities in order to
capture both short- and more long-distance depen-
dency effects within the sentence.
The semantic surprisal model, a re-
implementation of Mitchell (2011), uses a
word vector w and a history or context vector h to
calculate the language model p(w|h), defining this
probability in vector space via cosine similarity.
Words that have a higher distributional similarity
to their context are thus represented as having a
higher probability than words that do not. Thus,
we calculate probabilities for words in the context
of a sentence in a framework of distributional
semantics.
Regarding our main hypothesis—that speakers
adapt their speech rate as a function of a word’s in-
formation content—it is particularly important to
</bodyText>
<page confidence="0.977353">
763
</page>
<note confidence="0.978068">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 763–773,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991601875">
us to test this hypothesis on fully “natural” conver-
sational data. Therefore, we use the AMI corpus,
which contains transcripts of English-language
conversations with orthographically correct tran-
scriptions and precise word pronunciation bound-
aries in terms of time.
We will explain the calculation of semantic sur-
prisal in section 4 (this is so far only described in
Mitchell’s 2011 PhD thesis), and then evaluate the
effect of an in-domain semantic surprisal model in
section 7. Next, we will compare this to the ef-
fect of an out-of-domain semantic surprisal model
in section 8. The hypothesis is only confirmed for
the out-of-domain model, which we argue is due
to this model being more similar to the speaker’s
internal “model” than the in-domain model.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.994786">
2.1 Surprisal and UID
</subsectionHeader>
<bodyText confidence="0.999919982142857">
Surprisal is defined in terms of the negative
logarithm of the probability of a word in con-
text: S(w) _ −log P(w|context), where
P(w|context) is the probability of a word given
its previous (linguistic) context. It is a measure
of information content in which a high surprisal
implies low predictability. The use of surprisal
in psycholinguistic research goes back to Hale
(2001), who used a probabilistic Earley Parser to
model the difficulty in parsing so-called garden
path sentences (e.g. “The horse raced past the barn
fell”), wherein the unexpectedness of an upcom-
ing word or structure influences the language pro-
cessor’s difficulty. Recent work in psycholinguis-
tics has provided increasing support (e.g., Levy
(2008); Demberg and Keller (2008); Smith and
Levy (2013); Frank et al. (2013)) for the hypoth-
esis that the surprisal of a word is proportional
to the processing difficulty (measured in terms of
reading times and EEG event-related potentials) it
causes to a human.
The Uniform Information Density (UID) hy-
pothesis (Frank and Jaeger, 2008) holds that
speakers tend distribute information uniformly
across an utterance (in the limits of grammatical-
ity). Information density is quantified in terms of
the surprisal of each word (or other linguistic unit)
in the utterance. These notions go back to Shan-
non (1948), who showed that conveying informa-
tion uniformly close to channel capacity is optimal
for communication through a (noisy) communica-
tion channel.
Frank and Jaeger (2008) investigated UID ef-
fects in the SWITCHBOARD corpus at a mor-
phosyntactic level wherein speakers avoid using
English contracted forms (“you are” vs. “you’re”)
when the contractible phrase is also transmitting
a high degree of information in context. In this
case, n-gram surprisal was used as the information
density measure. Related hypotheses have been
suggested by Jurafsky et al. (2001), who related
speech durations to bigram probabilities on the
Switchboard corpus, and Aylett and Turk (2006),
who investigated information density effects at the
syllable level. They used a read-aloud English
speech synthesis corpus, and they found that there
is an inverse relationship between the pronuncia-
tion duration and the N-gram predictability. Dem-
berg et al. (2012) also use the AMI corpus used
in this work, and show that syntactic surprisal
(i.e., the surprisal estimated from Roark’s (2009)
PCFG parser) can predict word durations in natu-
ral speech.
Our work expands upon the existing efforts in
demonstrating the UID hypothesis by applying
surprisal to the level of lexical semantics.
</bodyText>
<subsectionHeader confidence="0.999335">
2.2 Distributional semantics
</subsectionHeader>
<bodyText confidence="0.995433826086957">
Given a means of evaluating the similarity of lin-
guistic units (e.g., words, sentences, texts) in some
numerical space that represents the contexts in
which they appear, it is possible to approximate
the semantics in distributional terms. This is usu-
ally done by collecting statistics from a corpus us-
ing techniques developed for information retrieval.
Using these statistics as a model of semantics is
justified in terms of the “distributional hypothe-
sis”, which holds that words used in similar con-
texts have similar meanings (Harris, 1954).
A simple and widely-used type of distributional
semantic model is the vector space model (Tur-
ney and Pantel, 2010). In such a model, all words
are represented each in terms of vectors in a sin-
gle high-dimensional space. The semantic simi-
larity of words can then be calculated via the co-
sine of the angle between the vectors in this man-
are usually excluded from this calculation. Until
relatively recently (Erk, 2012), distributional se-
mantic models did not take into account the fine-
grained details of syntactic and semantic structure
construed in formal terms.
</bodyText>
<figure confidence="0.9883018">
ner: cos(ϕ) _
�a·�b Closed-class function words
a
bi
||
</figure>
<page confidence="0.868009">
764
</page>
<note confidence="0.646574">
3 Corpus is calculated from the composition of individ-
</note>
<bodyText confidence="0.999475833333333">
The AMI Meeting Corpus (Carletta, 2007) is a
multimodal English-language corpus. It contains
videos and transcripts of simulated workgroup
meetings accompanied by various kinds of anno-
tations. The corpus is available along with its an-
notations under a free license1.
Two-thirds of the videos contain simulated
meetings of 4-person design teams assigned to talk
about the development of a fictional television re-
mote control. The remaining meetings discuss var-
ious other topics. The majority of speakers were
non-native speakers of English, although all the
conversations were held in English. The corpus
contains about 100 hours of material.
An important characteristic of this corpus for
our work is that the transcripts make use of con-
sistent English orthography (as opposed to being
phonetic transcripts). This enables the use of nat-
ural language processing techniques that require
the reliable identification of words. Grammatical
errors, however, remain in the corpus. The corpus
includes other annotations such as gesture and dia-
log acts. Most important for our work are the time
spans of word pronunciation, which are precise to
the hundredth of a second.
We removed interjections, incomplete words,
and transcriptions that were still misspelled from
the corpus, and we took out all incomplete sen-
tences. This left 951,769 tokens (15,403 types) re-
maining in the corpus.
</bodyText>
<sectionHeader confidence="0.99062" genericHeader="method">
4 Semantic surprisal model
</sectionHeader>
<bodyText confidence="0.999953">
We make use of a re-implementation of the se-
mantic surprisal model presented in Mitchell et al.
(2010). As this paper does not provide a detailed
description of how to calculate semantic surprisal,
our re-implementation is based on the description
in Mitchell’s PhD thesis (2011).
In order to calculate surprisal, we need to be
able to obtain a good estimate of a word given
previous context. Mitchell uses the following con-
cepts in his model:
</bodyText>
<listItem confidence="0.912025">
• hn−1 is the history and represents all the pre-
vious words in the sentence. If wn is the cur-
rent word, then hn−1 = w1 ... wn−1. The
vector-space semantic representation of hn−1
</listItem>
<footnote confidence="0.956608">
1http://groups.inf.ed.ac.uk/ami/
download/
</footnote>
<listItem confidence="0.943143777777778">
ual word vectors, which we call �hn−1.
• context words represent the dimensions of the
word vectors. The value of a word vector’s
component is the co-occurrence of that word
with a context word. The context words con-
sist of the most frequent words in the corpus.
• we use word class and distinguish between
content words and function words, for which
we use open and closed classes as a proxy.
</listItem>
<subsectionHeader confidence="0.998938">
4.1 Computing the vector components
</subsectionHeader>
<bodyText confidence="0.940726428571429">
The proportion between two probabilities p(ci|w)
p(ci)
is used for calculating vector components, where
ci is the ith context dimension and w is the given
word in the current position. We can calculate
each vector component vi for a word vector v� ac-
cording to the following equation:
</bodyText>
<equation confidence="0.9649045">
fciwftotal (1)
fwfci
</equation>
<bodyText confidence="0.9999932">
where fciw is the cooccurrence frequency of w and
ci together, ftotal is the total corpus size, and ci
represents the unigram frequencies of w. All fu-
ture steps in calculating our language model rely
on this definition of vi.
</bodyText>
<subsectionHeader confidence="0.997621">
4.2 Semantic probabilities
</subsectionHeader>
<bodyText confidence="0.999828875">
For the goal of computing p(w|h), we use the ba-
sic idea that the more “semantically coherent” a
word is with its history, the more likely it is. Co-
sine similarity is a common way to define this
similarity mathematically in a distributional space,
producing a value in the interval [−1, 1]. We use
the following definitions, wherein cp is the angle
between w� and h:
</bodyText>
<equation confidence="0.988927333333333">
w� · h� (2)
w�· h� = � wihi (3)
i
</equation>
<bodyText confidence="0.999902222222222">
Mitchell notes that there are at least three prob-
lems with using cosine similarity in connection
with the construction of a probabilistic model:
(a) the sum of all cosine values is not unity, (b)
word frequency does not pay a role in the cal-
culation, such that a rare synonym of a frequent
word might get a high similarity rating, despite
low predictability, and (c) the calculation can re-
sult in negative values.
</bodyText>
<equation confidence="0.978139333333333">
p(ci|w)
p(ci)
vi =
cos(cp) =
h|
|79||
</equation>
<page confidence="0.978442">
765
</page>
<bodyText confidence="0.996077666666667">
This problem is addressed by two changes to the
notion of dot product used in the calculation of the
cosine:
</bodyText>
<equation confidence="0.9799975">
p(ci|h) (4)
p(ci)
</equation>
<bodyText confidence="0.996893">
The influence of word frequencies is then restored
using p(w) and p(ci):
</bodyText>
<equation confidence="0.947894">
p(ci |h)p(ci) (5)
p(ci)
</equation>
<bodyText confidence="0.999920625">
This expression reweights the new scalar product
with the likelihood of the given words and the con-
text words. We refer the reader to Mitchell (2011)
in order to see that this is a true probability. The
application of Bayes’ Rule allows us to rewrite the
formula as p(w|h) = Pi p(w|ci)p(ci|h). Never-
theless, equation (5) is better suited to our task, as
it operates directly over our word vectors.
</bodyText>
<subsectionHeader confidence="0.998799">
4.3 Incremental processing
</subsectionHeader>
<bodyText confidence="0.950819333333333">
Equation (5) provides a conditional probability for
a word w and its history h. To calculate the prod-
uct p(ci|w) p(ci|h), we need the components of the
p(ci)p(ci)
vectors for w and h at the current position in the
sentence. We can get w1 from directly from the
vector space of words. However, h1 does not have
a direct representation in that space, and it must be
constructed compositionally:
</bodyText>
<equation confidence="0.9804055">
=
1h1
1w1 Initialization (6)
1hn = f(1hn−1, 1wn) Composition (7)
</equation>
<bodyText confidence="0.911516923076923">
f is a vector composition function that can be cho-
sen independently from the model. The history is
initialized using the vector of the first word and
combined step-by-step with the vectors of the fol-
lowing words. History vectors that arise from the
composition step are normalized2:
Normalization (8)
The equations (5), (6), (7), and (8) represent a sim-
ple language model, assuming calculation of vec-
tor components with equation (1).
2This equation is slightly different from what appears in
Mitchell (2011). We present here a corrected formula based
on private communication with the author.
</bodyText>
<subsectionHeader confidence="0.991808">
4.4 Accounting for word order
</subsectionHeader>
<bodyText confidence="0.999989125">
The model described so far is based on semantic
coherence and mostly ignores word order. Conse-
quently, it has poor predictive power. In this sec-
tion, we describe how a notion of word order is
included in the model through the integration of
an n-gram language model.
Specifically, equation (5) can be represented as
the product of two factors:
</bodyText>
<equation confidence="0.986943">
p(w|h) = p(w)Δ(w, h) (9)
p(ci |h)p(ci) (10)
p(ci)
</equation>
<bodyText confidence="0.999754714285714">
where Δ is the semantic component that scales
p(w) in function of the context. A word w that has
a close semantic similarity to a history h should
receive higher or lower probability depending on
whether Δ is higher or lower than 1. In order to
make this into a prediction, p(w) is replaced with
a trigram probability.
</bodyText>
<equation confidence="0.809551666666667">
ˆp(wn, hn−1, wn−1
n−2) = p(wn|wn−1 n−2)Δ(wn, hn−1)
(11)
</equation>
<bodyText confidence="0.99977">
However, this change means that the result is no
longer a true probability. Instead, equation 11 can
be seen as an estimate of semantic similarity. In
order to restore its status as a probability, Mitchell
includes another normalization step:
</bodyText>
<equation confidence="0.920722454545454">
p(wn|wn−1
n−2)
Function words
n−1
ˆp(wn ,hn−3 ,wn−2 )
n−1
ˆp(wc,hn−3 ,wn−2 )
p(wc|wn−1
n−2)
Content words
(12)
</equation>
<bodyText confidence="0.946048454545454">
The model hence simply uses the trigram model
probability for function words, making the as-
sumption that the distributional representation of
such words does not include useful information.
On the other hand, content words obtain a por-
tion of the probability mass whose size depends
on its similarity estimate ˆp(wn, hn−3, wn−1
n−2) rel-
ative to the similarity estimates of all other
words Pwc ˆp(wc, hn−3, wn−1
n−2). The factor
</bodyText>
<footnote confidence="0.788027">
Pwc p(wc|wn−1
n−2) ensures that not all of the proba-
bility mass is divided up among the content words
wc; rather, only the mass assigned by the n-gram
model at position wn−1
n−2 is re-distributed. The
</footnote>
<equation confidence="0.511974">
w1· h1 = X
i
p(ci|w)
p(ci)
X p(ci|w)
p(w|h) = p(w) p(ci)
i
ˆhi
hi =
ˆhjp(cj)
P
j
X p(ci|w)
Δ(w, h) = p(ci)
i
n−1
p(wn  |hn−3, wn−2) =
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
E
wc
P
wc
</equation>
<page confidence="0.986514">
766
</page>
<bodyText confidence="0.999176818181818">
probability mass of the function words remains
unchanged.
Mitchell (2011) restricts the history so that only
words outside the trigram window are taken into
account in order to keep the n-gram model and the
semantic similarity model independent. Thus, the
n-gram model represents local dependencies, and
the semantic model represents longer-distance de-
pendencies.
The final model that we use in our experiment
consists of equations (1), (6), (7), (8) and (12).
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="method">
5 Evaluation Methods
</sectionHeader>
<bodyText confidence="0.999847904761905">
Our goal is to test whether semantically
reweighted surprisal can explain spoken word
durations over and above more simple factors that
are known to influence word durations, such as
word length, frequency and predictability using
a simpler language model. Our first experiment
tests whether semantic surprisal based on a model
trained using in-domain data is predictive of
word pronunciation duration, considering the
UID hypothesis. For our in-domain model, we
estimate surprisal using 10-fold cross-validation
over the AMI corpus: we divide the corpus into
ten equally-sized segments and produce surprisal
values for each word in each segment based on a
model trained from the other nine segments. We
then use linear mixed effects modeling (LME) via
the lme4 package in R (Pinheiro and Bates, 2000;
Bates et al., 2014) in order to account for word
pronunciation length. We follow the approach of
Demberg et al. (2012).
Linear mixed effects modelling is a generaliza-
tion of linear regression modeling and includes
both fixed effects and random effects. This is par-
ticularly useful when we have a statistical units
(e.g., speakers) each with their own set of repeated
measures (e.g., word duration), but each such unit
has its own particular characteristics (e.g., some
speakers naturally speak more slowly than others).
These are the random effects. The fixed effects are
those characteristics that are expected not to vary
across such units. LME modeling learns coeffi-
cients for all of the predictors, defining a regres-
sion equation that should account for the data in
the dependent variable (in our case, word pronun-
ciation duration). The variance in the data that a
model cannot explain is referred to as the residual.
We denote statistical significances in the following
way: *** means a p-value &lt; 0.001, ** means p &lt;
0.01, * means p &lt; 0.05, and no stars means that
the predictor is not significant (p &gt; 0.05).
In our regression models, all the variables are
centered and scaled to reduce effects of correla-
tions between predictors. Furthermore, we log-
transformed the response variable (actual spoken
word durations from the corpus) as well as the du-
ration estimates from the MARY speech synthesis
system to obtain more normal distributions, which
are prerequisite for applying the LME models. All
conclusions drawn here also hold for versions of
the model where no log transformation is used.
From the AMI corpus, we filter out data points
(words) that have a pronunciation duration of zero
or those that are longer than two seconds, the latter
in order to avoid including such things as pauses
for thought. We also remove items that are not
represented in Gigaword. That leaves us with
790,061 data points for further analysis. How-
ever, in our semantic model, function words are
not affected by the Δ semantic similarity adjust-
ment and are therefore not analyzable for the ef-
fect of semantically-weighted trigram predictabil-
ity. That leaves 260k data points for analysis in the
models.
</bodyText>
<sectionHeader confidence="0.999057" genericHeader="method">
6 Baseline model
</sectionHeader>
<bodyText confidence="0.999958458333333">
As a first step, we estimate a baseline model
which does not include the in-domain semantic
surprisal. The response variable in this model
are the word durations observed in the corpus.
Predictor variables include DMARY (the context-
dependent spoken word duration as estimated by
the MARY speech synthesis system), word fre-
quency estimates from the same domain as well
as the GigaWord corpus (FAMI and FGiga, both
as log relative frequencies), the interaction be-
tween estimated word durations and in-domain
frequency, (DMARY:FAMI) and a domain-general
trigram model (SAMI-3). Our model also includes a
random intercept for each speaker, as well as ran-
dom slopes under speaker for DMARY and SAMI-3.
The baseline model is shown in Table 1.
All predictors in the baseline model shown in
Table 1 significantly improve model fit. We can
see that the MARY-TTS estimated word durations
are a positive highly significant predictor in the
model. Furthermore, the word frequency esti-
mates from the domain general corpus as well as
the in-domain frequency estimates are significant
negative predictors of word durations, this means
</bodyText>
<page confidence="0.992673">
767
</page>
<table confidence="0.999552428571429">
Predictor Coefficient t-value Sig.
(Intercept) 0.034 4.90 ***
DMARY 0.427 143.97 ***
FAMI -0.137 -60.26 ***
FGiga -0.051 -18.92 ***
SGiga-3gram 0.032 10.94 ***
DMARY:FAMI -0.003 -2.12 *
</table>
<tableCaption confidence="0.805398666666667">
Table 1: Fixed effects of a baseline model includ-
ing the data points for which we could calculate
semantic surprisal.
</tableCaption>
<bodyText confidence="0.9993559">
that as expected, words durations are shorter for
more frequent words. We can furthermore see
that n-gram surprisal is a significant positive pre-
dictor of spoken word durations; i.e., more unex-
pected words have longer durations than otherwise
predicted. Finally, there is also a significant in-
teraction between estimated word durations and
in-domain word frequency, which means that the
duration of long and frequent words is corrected
slightly downward.
</bodyText>
<sectionHeader confidence="0.96226" genericHeader="method">
7 Experiment 1: in-domain model
</sectionHeader>
<bodyText confidence="0.999968826086957">
The AMI corpus contains spoken conversations,
and is thus quite different from the written cor-
pora we have available. When we train an n-
gram model in domain (using 10-fold cross valida-
tion), perplexities for the in-domain model (67.9)
are much lower than for a language model trained
on gigaword (359.7), showing that the in-domain
model is a better language model for the data3.
In order to see the effect of semantic surprisal
estimated based on the in-domain language model
and reweighted for semantic similarity within the
same sentence as described in Section 3, we then
expand the baseline model, adding SSemantics as
a predictor. Table 2 shows the fixed effects of
this expanded model. The predictor for semantic
surprisal is significant, but the coefficient is neg-
ative. This apparently contradicts our hypothesis
that semantic surprisal has a UID effect on pronun-
ciation duration, so that higher SSemantics means
higher DAMI. We found that these results are very
stable—in particular, the same results also hold if
we estimate a separate model with SSemantics as a
predictor and residuals of the baseline model as a
</bodyText>
<footnote confidence="0.8799205">
3Low perplexity estimates are reflective of the spoken
conversational domain. Perplexities on content words are
much higher: 357.3 for the in-domain model and 2169.8 for
the out of domain model.
</footnote>
<table confidence="0.99941575">
Predictor Coefficient t-value Sig.
(Intercept) 0.031 4.53 **
DMARY 0.428 144.06 ***
FAMI -0.148 -59.15 ***
FGiga -0.043 -15.10 ***
SGiga-3gram 0.047 14.60 ***
SSemantics -0.028 -9.78 ***
DMARY:FAMI -0.003 -2.27 *
</table>
<tableCaption confidence="0.883544">
Table 2: Fixed effects of the baseline model with
semantic surprisal (including also a random slope
for semantic surprisal under subject).
</tableCaption>
<figureCaption confidence="0.993239">
Figure 1: GAM-calculated spline for SSemantics for
the in-domain model.
</figureCaption>
<bodyText confidence="0.99956105882353">
response variable, and when we include in-domain
semantic surprisal in a model where there ngram
surprisal on the out of domain corpus is not in-
cluded as a predictor variable.
In order to understand the unexpected behaviour
of SSemantics, we make use of a generalized additive
model (GAM) with the R package mgcv. Com-
pared to LME models, GAMs are parameter-free
and do not assume a linear form of the predic-
tors. Instead, for every predictor, GAMs can fit a
spline. We learn a GAM using the residuals of the
baseline model as a response variable and fitting
semantic surprisal based on the in-domain model;
see Table 2.
In figure 1, we see that SSemantics is poorly fit
by a linear function. In particular, there are two
intervals in the curve. Between surprisal values 0
</bodyText>
<page confidence="0.994247">
768
</page>
<bodyText confidence="0.9997509">
and 1.5, the curve falls, but between 1.5 and 4, it
rises. (For high surprisal values, there are too few
data points from which to draw conclusions.)
Therefore, we decided to divide the data up into
datapoints with SSemantics above 1.5 and below 1.5.
We then modelled the effect of SSemantics on the
residuals of the baseline model, with SSemantics as
a random effect. This is to remove a possible effect
of collinearity between SSemantics and the other
predictors.
</bodyText>
<equation confidence="0.723064">
[1.5, ∞[ (Intercept) 0 0
SSemantics 0.013 5.50 ***
</equation>
<bodyText confidence="0.980167153846153">
Table 3: Three models of SSemantics as a random ef-
fect over the residuals of baseline models learned
from the remaining fixed effects. The first model
is over the entire range.
Table 3 shows that the random effect of se-
mantic surprisal is positive and significant in the
range of semantic surprisal above 1.5. That low
surprisals have the opposite effect compared to
what we expect suggests to us that using the
AMI corpus as an in-domain source of training
data presents a problem. The observed result
for the relationship between semantic surprisal
and spoken word durations does not only hold
for the semantic surprisal model, but also for the
standard non-weight-adjusted in-domain trigram
model. We therefore hypothesize that our seman-
tic surprisal model is producing surprisal values
that are low because they are common in this do-
main (both higher frequency and higher similari-
ties), but speakers are coming to the AMI task with
“models” trained on out-of-domain data. Thus,
words that are apparently very low-surprisal dis-
play longer pronunciation durations as an artifact
of the model. To test this, we conducted a second
experiment, for which we built a model with out-
of-domain data.
</bodyText>
<sectionHeader confidence="0.928552" genericHeader="method">
8 Experiment 2: out-of-domain training
</sectionHeader>
<bodyText confidence="0.999892105263158">
In order to test for the effect of possible under-
estimation of surprisal due to in-domain training,
we also tested the semantic surprisal model when
trained on more domain-general text. As train-
ing data for our semantic model, we use a ran-
domly selected 1% (by sentence) of the English
Gigaword 5.0 corpus. This is lowercased, with ha-
pax legomena treated as unknown words. We test
the model against the entire AMI corpus. Further-
more, we also compare our semantic surprisal val-
ues to the syntactic surprisal values calculated by
Demberg et al. (2012) for the AMI corpus, which
we obtained from the authors. As noted above,
the out-of-domain language model has higher per-
plexity on the AMI corpus—that is, it is a lower-
performing language model. On the other hand, it
may represent overall speaker experience more ac-
curately than the in-domain model; in other words,
it may be a better model of the speaker.
</bodyText>
<subsectionHeader confidence="0.753404">
8.1 Results
</subsectionHeader>
<bodyText confidence="0.988615515151515">
Once again, the semantic surprisal model is only
different from a general n-gram model on content
words. We therefore first compare whether the
model that is reweighted for semantic surprisal can
explain more of the variance than the same model
without semantic reweighting.
We again use the same baseline model as for the
in-domain experiment, see table 1. As the seman-
tic surprisal model represents a reweighted trigram
model, there is a high correlation between the
trigram model and the semantic surprisal model.
We thus need to know whether the semantically
reweighted model is better than the simple tri-
gram model. When we compare a model that con-
tains both trigram surprisal and semantic surprisal
as a predictor, we find that this model is signifi-
cantly better than the model including only trigram
surprisal (AIC of baseline model: 618427; AIC
of model with semantic surprisal: 618394; x2 =
35.8; p &lt; 0.00001). On the other hand, the model
including both predictors is only marginally better
than the model including semantic surprsial (AIC
of semantic surprisal model: 618398). This means
that the simpler trigram surprisal model does not
contribute anything over the semantic model, and
that the semantic model fits the word duration data
better. Table 4 shows the model with semantic sur-
prisal as a predictor.
Furthermore, we wanted to check whether our
hypothesis about the negative result for the in-
domain model was indeed due to an under-
estimation of surprisal of in-domain words for the
Interval of Predictor Coef. t-value Sig.
</bodyText>
<equation confidence="0.9307382">
[0,∞[ (Intercept) 0 0
SSemantics -0.013 -7.01 ***
[0,1.5[ (Intercept) 0 0
SSemantics -0.06 -18.56 ***
SSemantics
</equation>
<page confidence="0.973825">
769
</page>
<table confidence="0.999508714285714">
Predictor Coefficient t-value Sig.
(Intercept) 0.034 4.90 ***
DMARY 0.427 144.36 ***
FAMI -0.135 -58.76 ***
FGiga -0.053 -19.99 ***
SSemantics 0.034 11.70 ***
DMARY:FAMI -0.003 -2.09 *
</table>
<tableCaption confidence="0.924230333333333">
Table 4: Model of spoken word durations,
with random intercept and random slopes for
DMARY and SSemantics under speaker.
</tableCaption>
<figureCaption confidence="0.9895865">
Figure 2: GAM-calculated spline for SSemantics for
the ouf-of-domain model.
</figureCaption>
<bodyText confidence="0.999308666666667">
in-domain model. We again calculate a GAM
model showing the effect of out-of-domain seman-
tic surprisal in a model containing also the base-
line predictors, see figure 2.
We can see that word durations increase with
increasing semantic surprisal, and that there is in
particular no effect of longer word durations for
low surprisal words. This result is also confirmed
by LME models splitting up the data in small and
large surprisal values, as done for the in-domain
model in Table 3; semantic surprisal based on the
out-of-domain model is a significant positive pre-
dictor in both data ranges.
Next, we tested whether the semantic similarity
model improves model fit over and above a model
also containing syntactic surprisal as a predictor.
We find that syntactic surprisal improves model fit
over and above the model including semantic sur-
</bodyText>
<table confidence="0.999257375">
Predictor Coefficient t-value Sig.
(Intercept) -0.058 -6.58 ***
DMARY 0.425 144.04 ***
FAMI -0.131 -57.04 ***
FGiga -0.051 -19.41 ***
SSyntax 0.011 17.61 ***
SSemantics 0.015 4.99 ***
DMARY:FAMI -0.007 -4.44 ***
</table>
<tableCaption confidence="0.994479">
Table 5: Linear mixed effects model for spoken
</tableCaption>
<bodyText confidence="0.986245615384615">
word durations in the AMI corpus, for a model in-
cluding both syntactic and semantic surprisal as a
predictor as well as a random intercept and slope
for DMARY and SSemantics under speaker.
prisal (x2 = 309.5; p &lt; 0.00001), and that seman-
tic surprisal improves model fit over and above
a model including syntactic surprisal and trigram
surprisal (x2 = 28.5;p &lt; 0.00001). Table 5
shows the model containing both syntactic based
on the Roark parser ((Roark et al., 2009); see also
Demberg et al. (2012) for use of syntactic surprisal
for estimating spoken word durations) and seman-
tic surprisal.
Finally, we split our dataset into data from na-
tive and non-native speakers of English (305 na-
tive speakers, vs. 376 non-native speakers). Ta-
ble 6 shows generally larger effects for native than
non-native speakers. In particular, the interac-
tion between duration estimates and word frequen-
cies, and semantic surprisal were not significant
predictors in the non-native speaker model (how-
ever, random slopes for semantic surprisal un-
der speaker still improved model fit very strongly,
showing that non-native speakers differ in whether
and how they take into account semantic surprisal
during language production).
</bodyText>
<sectionHeader confidence="0.999032" genericHeader="evaluation">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999985636363636">
Our analysis shows that high information density
at one linguistic level of description (for exam-
ple, syntax or semantics) can lead to a compen-
satory effect at a different linguistic level (here,
spoken word durations). Our data also shows how-
ever, that the choice of training data for the mod-
els is important. A language model trained exclu-
sively in a specific domain, while a good language
model, may not be representative of speaker’s
overall language experience. This is particularly
relevant for the AMI corpus, in which groups of
</bodyText>
<page confidence="0.986382">
770
</page>
<table confidence="0.999365666666667">
Native Speaker Non-native Speaker
Predictor Coefficient t-value Sig. Coefficient t-value Sig.
(Intercept) -0.1706 -13.76 *** 0.035 3.42 ***
DMARY 0.4367 105.43 *** 0.415 104.09 ***
FAMI -0.1407 -42.54 *** -0.122 -38.66 ***
FGiga -0.0421 -11.07 *** -0.063 -18.70 ***
SSyntax 0.0132 14.22 *** 0.009 11.96 ***
SSemantics 0.0246 5.89 *** ***
DMARY:FAMI -0.0139 -6.12 *** ***
</table>
<tableCaption confidence="0.963885333333333">
Table 6: Linear mixed effects models for spoken word durations in the AMI corpus, for native as well as
non-native speakers of English separately. The models include both syntactic and semantic surprisal as
a fixed effect, and a random intercept and slope for DMARY and SSemantics under speaker.
</tableCaption>
<bodyText confidence="0.999841620689655">
researchers are discussing the design of a remote
control, but where it is not necessarily the case
that these people discuss remote controls very fre-
quently. Furthermore, none of the speakers were
present in the whole corpus, and most of the &gt; 600
speakers participated only in very few meetings.
This means that the in-domain language model
strongly over-estimates people’s familiarity with
the domain.
Words that are highly predictable for the in-
domain model (but which are not highly pre-
dictable in general) were not pronounced faster,
as evident in our first analysis. When seman-
tic surprisal is however estimated based on a
more domain-general text like Gigaword, we find
a significant positive effect of semantic surprisal
on spoken word durations across the complete
spectrum from very predictable to unpredictable
words.
These results also point to an interesting sci-
entific question: to what extent to people use
their domain-general model for adapting their lan-
guage and speech production in a specific situa-
tion, and to what extent do they use a domain-
specific model for adaptation? Do people adapt
during a conversation, such that in-domain mod-
els would be more relevant for language produc-
tion in situations where speakers are more versed
in the domain?
</bodyText>
<sectionHeader confidence="0.984624" genericHeader="conclusions">
10 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999990242424242">
We have described a method by which it is pos-
sible to connect a semantic level of representation
(estimated using a distributional model) to obser-
vations about speech patterns at the word level.
From a language science or psycholinguistic per-
spective, we have shown that semantic surprisal
affects spoken word durations in natural conversa-
tional speech, thus providing additional supportive
evidence for the uniform information density hy-
pothesis. In particular, we find evidence that UID
effects connect linguistic levels of representation,
providing more information about the architecture
of the human processor or generator.
This work also has implications for designers
of speech synthesis systems: our results point to-
wards using high-level information about the rate
of information transfer measured in terms of sur-
prisal for estimating word durations in order to
make artificial word pronunciation systems sound
more natural.
Finally, the strong effect of training data domain
raises scientific questions about how speakers use
domain-general and -specific knowledge in com-
municative cooperation with listeners at the word
pronunciation level.
One possible next step would be to expand this
work to more complex semantic spaces which in-
clude stronger notions of compositionality, seman-
tic roles, and so on, such as the distributional ap-
proaches of Baroni and Lenci (2010), Sayeed and
Demberg (2014), and Greenberg et al. (2015) that
contain grammatical information but rely on vec-
tor operations.
</bodyText>
<sectionHeader confidence="0.998716" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.958274">
This research was funded by the German Research
Foundation (DFG) as part of SFB 1102 “Informa-
tion Density and Linguistic Encoding”.
</bodyText>
<page confidence="0.996741">
771
</page>
<sectionHeader confidence="0.997755" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993507226804124">
Aylett, M. and Turk, A. (2006). Language redun-
dancy predicts syllabic duration and the spectral
characteristics of vocalic syllable nuclei. The
Journal of the Acoustical Society of America,
119(5):3048–3058.
Baroni, M. and Lenci, A. (2010). Distributional
memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673–
721.
Bates, D., M¨achler, M., Bolker, B. M., and Walker,
S. C. (2014). Fitting linear mixed-effects mod-
els using lme4. ArXiv e-print; submitted to
Journal of Statistical Software.
Carletta, J. (2007). Unleashing the killer corpus:
experiences in creating the multi-everything
AMI meeting corpus. Language Resources and
Evaluation, 41(2):181–190.
Demberg, V. and Keller, F. (2008). Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
109(2):193–210.
Demberg, V., Sayeed, A., Gorinski, P., and En-
gonopoulos, N. (2012). Syntactic surprisal af-
fects spoken word duration in conversational
contexts. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Nat-
ural Language Learning, pages 356–367, Jeju
Island, Korea. Association for Computational
Linguistics.
Erk, K. (2012). Vector space models of word
meaning and phrase meaning: A survey. Lan-
guage and Linguistics Compass, 6(10):635–
653.
Frank, A. F. and Jaeger, T. F. (2008). Speaking ra-
tionally: Uniform information density as an op-
timal strategy for language production. In Love,
B. C., McRae, K., and Sloutsky, V. M., editors,
Proceedings of the 30th Annual Conference of
the Cognitive Science Society, pages 939–944.
Cognitive Science Society.
Frank, S. L., Otten, L. J., Galli, G., and Vigliocco,
G. (2013). Word surprisal predicts n400 ampli-
tude during reading. In ACL (2), pages 878–
883.
Greenberg, C., Sayeed, A., and Demberg, V.
(2015). Improving unsupervised vector-space
thematic fit evaluation via role-filler prototype
clustering. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics Human
Language Technologies (NAACL HLT).
Hale, J. (2001). A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the
Second Meeting of the North American Chapter
of the Association for Computational Linguis-
tics on Language Technologies, NAACL ’01,
pages 1–8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Harris, Z. S. (1954). Distributional structure.
Word, 10(2-3):146–162.
Jurafsky, D., Bell, A., Gregory, M., and Ray-
mond, W. D. (2001). Probabilistic relations be-
tween words: Evidence from reduction in lexi-
cal production. Typological studies in language,
45:229–254.
Levy, R. (2008). Expectation-based syntactic
comprehension. Cognition, 106(3):1126–1177.
Mitchell, J., Lapata, M., Demberg, V., and Keller,
F. (2010). Syntactic and semantic factors in
processing difficulty: An integrated measure.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 196–206. Association for Computational
Linguistics.
Mitchell, J. J. (2011). Composition in distribu-
tional models of semantics. PhD thesis, The
University of Edinburgh.
Pinheiro, J. C. and Bates, D. M. (2000). Mixed-
Effects Models in S and S-PLUS. Statistics and
Computing. Springer.
Roark, B., Bachrach, A., Cardenas, C., and Pal-
lier, C. (2009). Deriving lexical and syntactic
expectation-based measures for psycholinguis-
tic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 324–333, Singapore. Association for
Computational Linguistics.
Sayeed, A. and Demberg, V. (2014). Combin-
ing unsupervised syntactic and semantic mod-
els of thematic fit. In Proceedings of the first
Italian Conference on Computational Linguis-
tics (CLiC-it 2014).
Shannon, C. E. (1948). A mathematical theory of
communication. Bell System Technical Journal,
27(379-423):623–656.
</reference>
<page confidence="0.970431">
772
</page>
<reference confidence="0.981128857142857">
Smith, N. J. and Levy, R. (2013). The effect of
word predictability on reading time is logarith-
mic. Cognition, 128(3):302–319.
Turney, P. D. and Pantel, P. (2010). From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Re-
search, 37:141–188.
</reference>
<page confidence="0.998962">
773
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.181049">
<title confidence="0.983187">Vector-space calculation of semantic surprisal for predicting pronunciation duration</title>
<author confidence="0.711776">Asad Sayeed</author>
<author confidence="0.711776">Stefan Fischer</author>
<author confidence="0.711776">Vera</author>
<affiliation confidence="0.4578605">Linguistics and Cluster of Saarland</affiliation>
<address confidence="0.757687">66123 Saarbr¨ucken,</address>
<abstract confidence="0.998268178571428">In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we consider a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Aylett</author>
<author>A Turk</author>
</authors>
<title>Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei.</title>
<date>2006</date>
<journal>The Journal of the Acoustical Society of America,</journal>
<volume>119</volume>
<issue>5</issue>
<contexts>
<context position="1574" citStr="Aylett and Turk, 2006" startWordPosition="227" endWordPosition="230">mantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g., Jurafsky et al., 2001; Aylett and Turk, 2006; Frank and Jaeger, 2008). The rate of information transfer is thereby quantified using as each words’ Surprisal (Hale, 2001), that is, a word’s negative log probability in context. Surprisal(wi) = −log P(wi|w1..wi−1) This work makes use of an existing measure of semantic surprisal calculated from a distributional space in order to test whether this measure accounts for an effect of UID on speech production. Our hypothesis is that a word in a semantically surprising context is pronounced with a slightly longer duration than the same word in a semantically less-expected context. In this way, a </context>
<context position="6763" citStr="Aylett and Turk (2006)" startWordPosition="1046" endWordPosition="1049">ormly close to channel capacity is optimal for communication through a (noisy) communication channel. Frank and Jaeger (2008) investigated UID effects in the SWITCHBOARD corpus at a morphosyntactic level wherein speakers avoid using English contracted forms (“you are” vs. “you’re”) when the contractible phrase is also transmitting a high degree of information in context. In this case, n-gram surprisal was used as the information density measure. Related hypotheses have been suggested by Jurafsky et al. (2001), who related speech durations to bigram probabilities on the Switchboard corpus, and Aylett and Turk (2006), who investigated information density effects at the syllable level. They used a read-aloud English speech synthesis corpus, and they found that there is an inverse relationship between the pronunciation duration and the N-gram predictability. Demberg et al. (2012) also use the AMI corpus used in this work, and show that syntactic surprisal (i.e., the surprisal estimated from Roark’s (2009) PCFG parser) can predict word durations in natural speech. Our work expands upon the existing efforts in demonstrating the UID hypothesis by applying surprisal to the level of lexical semantics. 2.2 Distri</context>
</contexts>
<marker>Aylett, Turk, 2006</marker>
<rawString>Aylett, M. and Turk, A. (2006). Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei. The Journal of the Acoustical Society of America, 119(5):3048–3058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpusbased semantics.</title>
<date>2010</date>
<journal>Comput. Linguist.,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Baroni, M. and Lenci, A. (2010). Distributional memory: A general framework for corpusbased semantics. Comput. Linguist., 36(4):673– 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bates</author>
<author>M M¨achler</author>
<author>B M Bolker</author>
<author>S C Walker</author>
</authors>
<title>Fitting linear mixed-effects models using lme4. ArXiv e-print; submitted to</title>
<date>2014</date>
<journal>Journal of Statistical Software.</journal>
<marker>Bates, M¨achler, Bolker, Walker, 2014</marker>
<rawString>Bates, D., M¨achler, M., Bolker, B. M., and Walker, S. C. (2014). Fitting linear mixed-effects models using lme4. ArXiv e-print; submitted to Journal of Statistical Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Unleashing the killer corpus: experiences in creating the multi-everything AMI meeting corpus. Language Resources and Evaluation,</title>
<date>2007</date>
<pages>41--2</pages>
<contexts>
<context position="8641" citStr="Carletta, 2007" startWordPosition="1349" endWordPosition="1350"> Pantel, 2010). In such a model, all words are represented each in terms of vectors in a single high-dimensional space. The semantic similarity of words can then be calculated via the cosine of the angle between the vectors in this manare usually excluded from this calculation. Until relatively recently (Erk, 2012), distributional semantic models did not take into account the finegrained details of syntactic and semantic structure construed in formal terms. ner: cos(ϕ) _ �a·�b Closed-class function words a bi || 764 3 Corpus is calculated from the composition of individThe AMI Meeting Corpus (Carletta, 2007) is a multimodal English-language corpus. It contains videos and transcripts of simulated workgroup meetings accompanied by various kinds of annotations. The corpus is available along with its annotations under a free license1. Two-thirds of the videos contain simulated meetings of 4-person design teams assigned to talk about the development of a fictional television remote control. The remaining meetings discuss various other topics. The majority of speakers were non-native speakers of English, although all the conversations were held in English. The corpus contains about 100 hours of materia</context>
</contexts>
<marker>Carletta, 2007</marker>
<rawString>Carletta, J. (2007). Unleashing the killer corpus: experiences in creating the multi-everything AMI meeting corpus. Language Resources and Evaluation, 41(2):181–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="5522" citStr="Demberg and Keller (2008)" startWordPosition="852" endWordPosition="855">ere P(w|context) is the probability of a word given its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to Hale (2001), who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (1948), who showed that conveyi</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Demberg, V. and Keller, F. (2008). Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>A Sayeed</author>
<author>P Gorinski</author>
<author>N Engonopoulos</author>
</authors>
<title>Syntactic surprisal affects spoken word duration in conversational contexts.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>356--367</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="7029" citStr="Demberg et al. (2012)" startWordPosition="1085" endWordPosition="1089">” vs. “you’re”) when the contractible phrase is also transmitting a high degree of information in context. In this case, n-gram surprisal was used as the information density measure. Related hypotheses have been suggested by Jurafsky et al. (2001), who related speech durations to bigram probabilities on the Switchboard corpus, and Aylett and Turk (2006), who investigated information density effects at the syllable level. They used a read-aloud English speech synthesis corpus, and they found that there is an inverse relationship between the pronunciation duration and the N-gram predictability. Demberg et al. (2012) also use the AMI corpus used in this work, and show that syntactic surprisal (i.e., the surprisal estimated from Roark’s (2009) PCFG parser) can predict word durations in natural speech. Our work expands upon the existing efforts in demonstrating the UID hypothesis by applying surprisal to the level of lexical semantics. 2.2 Distributional semantics Given a means of evaluating the similarity of linguistic units (e.g., words, sentences, texts) in some numerical space that represents the contexts in which they appear, it is possible to approximate the semantics in distributional terms. This is </context>
<context position="17571" citStr="Demberg et al. (2012)" startWordPosition="2847" endWordPosition="2850">c surprisal based on a model trained using in-domain data is predictive of word pronunciation duration, considering the UID hypothesis. For our in-domain model, we estimate surprisal using 10-fold cross-validation over the AMI corpus: we divide the corpus into ten equally-sized segments and produce surprisal values for each word in each segment based on a model trained from the other nine segments. We then use linear mixed effects modeling (LME) via the lme4 package in R (Pinheiro and Bates, 2000; Bates et al., 2014) in order to account for word pronunciation length. We follow the approach of Demberg et al. (2012). Linear mixed effects modelling is a generalization of linear regression modeling and includes both fixed effects and random effects. This is particularly useful when we have a statistical units (e.g., speakers) each with their own set of repeated measures (e.g., word duration), but each such unit has its own particular characteristics (e.g., some speakers naturally speak more slowly than others). These are the random effects. The fixed effects are those characteristics that are expected not to vary across such units. LME modeling learns coefficients for all of the predictors, defining a regr</context>
<context position="26397" citStr="Demberg et al. (2012)" startWordPosition="4289" endWordPosition="4292">a model with outof-domain data. 8 Experiment 2: out-of-domain training In order to test for the effect of possible underestimation of surprisal due to in-domain training, we also tested the semantic surprisal model when trained on more domain-general text. As training data for our semantic model, we use a randomly selected 1% (by sentence) of the English Gigaword 5.0 corpus. This is lowercased, with hapax legomena treated as unknown words. We test the model against the entire AMI corpus. Furthermore, we also compare our semantic surprisal values to the syntactic surprisal values calculated by Demberg et al. (2012) for the AMI corpus, which we obtained from the authors. As noted above, the out-of-domain language model has higher perplexity on the AMI corpus—that is, it is a lowerperforming language model. On the other hand, it may represent overall speaker experience more accurately than the in-domain model; in other words, it may be a better model of the speaker. 8.1 Results Once again, the semantic surprisal model is only different from a general n-gram model on content words. We therefore first compare whether the model that is reweighted for semantic surprisal can explain more of the variance than t</context>
<context position="30407" citStr="Demberg et al. (2012)" startWordPosition="4946" endWordPosition="4949">ntax 0.011 17.61 *** SSemantics 0.015 4.99 *** DMARY:FAMI -0.007 -4.44 *** Table 5: Linear mixed effects model for spoken word durations in the AMI corpus, for a model including both syntactic and semantic surprisal as a predictor as well as a random intercept and slope for DMARY and SSemantics under speaker. prisal (x2 = 309.5; p &lt; 0.00001), and that semantic surprisal improves model fit over and above a model including syntactic surprisal and trigram surprisal (x2 = 28.5;p &lt; 0.00001). Table 5 shows the model containing both syntactic based on the Roark parser ((Roark et al., 2009); see also Demberg et al. (2012) for use of syntactic surprisal for estimating spoken word durations) and semantic surprisal. Finally, we split our dataset into data from native and non-native speakers of English (305 native speakers, vs. 376 non-native speakers). Table 6 shows generally larger effects for native than non-native speakers. In particular, the interaction between duration estimates and word frequencies, and semantic surprisal were not significant predictors in the non-native speaker model (however, random slopes for semantic surprisal under speaker still improved model fit very strongly, showing that non-native</context>
</contexts>
<marker>Demberg, Sayeed, Gorinski, Engonopoulos, 2012</marker>
<rawString>Demberg, V., Sayeed, A., Gorinski, P., and Engonopoulos, N. (2012). Syntactic surprisal affects spoken word duration in conversational contexts. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 356–367, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<pages>653</pages>
<contexts>
<context position="8342" citStr="Erk, 2012" startWordPosition="1301" endWordPosition="1302">l. Using these statistics as a model of semantics is justified in terms of the “distributional hypothesis”, which holds that words used in similar contexts have similar meanings (Harris, 1954). A simple and widely-used type of distributional semantic model is the vector space model (Turney and Pantel, 2010). In such a model, all words are represented each in terms of vectors in a single high-dimensional space. The semantic similarity of words can then be calculated via the cosine of the angle between the vectors in this manare usually excluded from this calculation. Until relatively recently (Erk, 2012), distributional semantic models did not take into account the finegrained details of syntactic and semantic structure construed in formal terms. ner: cos(ϕ) _ �a·�b Closed-class function words a bi || 764 3 Corpus is calculated from the composition of individThe AMI Meeting Corpus (Carletta, 2007) is a multimodal English-language corpus. It contains videos and transcripts of simulated workgroup meetings accompanied by various kinds of annotations. The corpus is available along with its annotations under a free license1. Two-thirds of the videos contain simulated meetings of 4-person design te</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Erk, K. (2012). Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635– 653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Frank</author>
<author>T F Jaeger</author>
</authors>
<title>Speaking rationally: Uniform information density as an optimal strategy for language production.</title>
<date>2008</date>
<booktitle>Proceedings of the 30th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>939--944</pages>
<editor>In Love, B. C., McRae, K., and Sloutsky, V. M., editors,</editor>
<publisher>Cognitive Science Society.</publisher>
<contexts>
<context position="1599" citStr="Frank and Jaeger, 2008" startWordPosition="231" endWordPosition="234"> a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g., Jurafsky et al., 2001; Aylett and Turk, 2006; Frank and Jaeger, 2008). The rate of information transfer is thereby quantified using as each words’ Surprisal (Hale, 2001), that is, a word’s negative log probability in context. Surprisal(wi) = −log P(wi|w1..wi−1) This work makes use of an existing measure of semantic surprisal calculated from a distributional space in order to test whether this measure accounts for an effect of UID on speech production. Our hypothesis is that a word in a semantically surprising context is pronounced with a slightly longer duration than the same word in a semantically less-expected context. In this way, a more uniform rate of info</context>
<context position="5826" citStr="Frank and Jaeger, 2008" startWordPosition="902" endWordPosition="905">e difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (1948), who showed that conveying information uniformly close to channel capacity is optimal for communication through a (noisy) communication channel. Frank and Jaeger (2008) investigated UID effects in the SWITCHBOARD corpus at a morphosyntactic level wherein speakers avoid using English contracted forms (“you are” vs. “you’re”) wh</context>
</contexts>
<marker>Frank, Jaeger, 2008</marker>
<rawString>Frank, A. F. and Jaeger, T. F. (2008). Speaking rationally: Uniform information density as an optimal strategy for language production. In Love, B. C., McRae, K., and Sloutsky, V. M., editors, Proceedings of the 30th Annual Conference of the Cognitive Science Society, pages 939–944. Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
<author>L J Otten</author>
<author>G Galli</author>
<author>G Vigliocco</author>
</authors>
<title>Word surprisal predicts n400 amplitude during reading.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>878--883</pages>
<contexts>
<context position="5566" citStr="Frank et al. (2013)" startWordPosition="860" endWordPosition="863">n its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to Hale (2001), who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (1948), who showed that conveying information uniformly close to channel ca</context>
</contexts>
<marker>Frank, Otten, Galli, Vigliocco, 2013</marker>
<rawString>Frank, S. L., Otten, L. J., Galli, G., and Vigliocco, G. (2013). Word surprisal predicts n400 amplitude during reading. In ACL (2), pages 878– 883.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Greenberg</author>
<author>A Sayeed</author>
<author>V Demberg</author>
</authors>
<title>Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT).</booktitle>
<marker>Greenberg, Sayeed, Demberg, 2015</marker>
<rawString>Greenberg, C., Sayeed, A., and Demberg, V. (2015). Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1699" citStr="Hale, 2001" startWordPosition="248" endWordPosition="249">e to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g., Jurafsky et al., 2001; Aylett and Turk, 2006; Frank and Jaeger, 2008). The rate of information transfer is thereby quantified using as each words’ Surprisal (Hale, 2001), that is, a word’s negative log probability in context. Surprisal(wi) = −log P(wi|w1..wi−1) This work makes use of an existing measure of semantic surprisal calculated from a distributional space in order to test whether this measure accounts for an effect of UID on speech production. Our hypothesis is that a word in a semantically surprising context is pronounced with a slightly longer duration than the same word in a semantically less-expected context. In this way, a more uniform rate of information transfer is achieved, because the higher information content of the unexpected word is stret</context>
<context position="5151" citStr="Hale (2001)" startWordPosition="798" endWordPosition="799">model in section 8. The hypothesis is only confirmed for the out-of-domain model, which we argue is due to this model being more similar to the speaker’s internal “model” than the in-domain model. 2 Background 2.1 Surprisal and UID Surprisal is defined in terms of the negative logarithm of the probability of a word in context: S(w) _ −log P(w|context), where P(w|context) is the probability of a word given its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to Hale (2001), who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="7924" citStr="Harris, 1954" startWordPosition="1229" endWordPosition="1230">prisal to the level of lexical semantics. 2.2 Distributional semantics Given a means of evaluating the similarity of linguistic units (e.g., words, sentences, texts) in some numerical space that represents the contexts in which they appear, it is possible to approximate the semantics in distributional terms. This is usually done by collecting statistics from a corpus using techniques developed for information retrieval. Using these statistics as a model of semantics is justified in terms of the “distributional hypothesis”, which holds that words used in similar contexts have similar meanings (Harris, 1954). A simple and widely-used type of distributional semantic model is the vector space model (Turney and Pantel, 2010). In such a model, all words are represented each in terms of vectors in a single high-dimensional space. The semantic similarity of words can then be calculated via the cosine of the angle between the vectors in this manare usually excluded from this calculation. Until relatively recently (Erk, 2012), distributional semantic models did not take into account the finegrained details of syntactic and semantic structure construed in formal terms. ner: cos(ϕ) _ �a·�b Closed-class fun</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Harris, Z. S. (1954). Distributional structure. Word, 10(2-3):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>A Bell</author>
<author>M Gregory</author>
<author>W D Raymond</author>
</authors>
<title>Probabilistic relations between words: Evidence from reduction in lexical production. Typological studies in language,</title>
<date>2001</date>
<pages>45--229</pages>
<contexts>
<context position="1551" citStr="Jurafsky et al., 2001" startWordPosition="223" endWordPosition="226">raining data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively constant rate of information transfer during speech production (e.g., Jurafsky et al., 2001; Aylett and Turk, 2006; Frank and Jaeger, 2008). The rate of information transfer is thereby quantified using as each words’ Surprisal (Hale, 2001), that is, a word’s negative log probability in context. Surprisal(wi) = −log P(wi|w1..wi−1) This work makes use of an existing measure of semantic surprisal calculated from a distributional space in order to test whether this measure accounts for an effect of UID on speech production. Our hypothesis is that a word in a semantically surprising context is pronounced with a slightly longer duration than the same word in a semantically less-expected c</context>
<context position="6655" citStr="Jurafsky et al. (2001)" startWordPosition="1030" endWordPosition="1033"> unit) in the utterance. These notions go back to Shannon (1948), who showed that conveying information uniformly close to channel capacity is optimal for communication through a (noisy) communication channel. Frank and Jaeger (2008) investigated UID effects in the SWITCHBOARD corpus at a morphosyntactic level wherein speakers avoid using English contracted forms (“you are” vs. “you’re”) when the contractible phrase is also transmitting a high degree of information in context. In this case, n-gram surprisal was used as the information density measure. Related hypotheses have been suggested by Jurafsky et al. (2001), who related speech durations to bigram probabilities on the Switchboard corpus, and Aylett and Turk (2006), who investigated information density effects at the syllable level. They used a read-aloud English speech synthesis corpus, and they found that there is an inverse relationship between the pronunciation duration and the N-gram predictability. Demberg et al. (2012) also use the AMI corpus used in this work, and show that syntactic surprisal (i.e., the surprisal estimated from Roark’s (2009) PCFG parser) can predict word durations in natural speech. Our work expands upon the existing eff</context>
</contexts>
<marker>Jurafsky, Bell, Gregory, Raymond, 2001</marker>
<rawString>Jurafsky, D., Bell, A., Gregory, M., and Raymond, W. D. (2001). Probabilistic relations between words: Evidence from reduction in lexical production. Typological studies in language, 45:229–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="5495" citStr="Levy (2008)" startWordPosition="850" endWordPosition="851">|context), where P(w|context) is the probability of a word given its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to Hale (2001), who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (194</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, R. (2008). Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Syntactic and semantic factors in processing difficulty: An integrated measure.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>196--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="749" citStr="Mitchell et al. (2010)" startWordPosition="95" endWordPosition="98"> Demberg Computational Linguistics and Phonetics/M2CI Cluster of Excellence Saarland University 66123 Saarbr¨ucken, Germany {asayeed,sfischer,vera}@coli.uni-saarland.de Abstract In order to build psycholinguistic models of processing difficulty and evaluate these models against human data, we need highly accurate language models. Here we specifically consider surprisal, a word’s predictability in context. Existing approaches have mostly used n-gram models or more sophisticated syntax-based parsing models; this largely does not account for effects specific to semantics. We build on the work by Mitchell et al. (2010) and show that the semantic prediction model suggested there can successfully predict spoken word durations in naturalistic conversational data. An interesting finding is that the training data for the semantic model also plays a strong role: the model trained on indomain data, even though a better language model for our data, is not able to predict word durations, while the out-ofdomain trained language model does predict word durations. We argue that this at first counter-intuitive result is due to the out-of-domain model better matching the “language models” of the speakers in our data. 1 I</context>
<context position="10108" citStr="Mitchell et al. (2010)" startWordPosition="1576" endWordPosition="1579"> the reliable identification of words. Grammatical errors, however, remain in the corpus. The corpus includes other annotations such as gesture and dialog acts. Most important for our work are the time spans of word pronunciation, which are precise to the hundredth of a second. We removed interjections, incomplete words, and transcriptions that were still misspelled from the corpus, and we took out all incomplete sentences. This left 951,769 tokens (15,403 types) remaining in the corpus. 4 Semantic surprisal model We make use of a re-implementation of the semantic surprisal model presented in Mitchell et al. (2010). As this paper does not provide a detailed description of how to calculate semantic surprisal, our re-implementation is based on the description in Mitchell’s PhD thesis (2011). In order to calculate surprisal, we need to be able to obtain a good estimate of a word given previous context. Mitchell uses the following concepts in his model: • hn−1 is the history and represents all the previous words in the sentence. If wn is the current word, then hn−1 = w1 ... wn−1. The vector-space semantic representation of hn−1 1http://groups.inf.ed.ac.uk/ami/ download/ ual word vectors, which we call �hn−1</context>
</contexts>
<marker>Mitchell, Lapata, Demberg, Keller, 2010</marker>
<rawString>Mitchell, J., Lapata, M., Demberg, V., and Keller, F. (2010). Syntactic and semantic factors in processing difficulty: An integrated measure. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196–206. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Mitchell</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<institution>The University of Edinburgh.</institution>
<contexts>
<context position="3146" citStr="Mitchell (2011)" startWordPosition="483" endWordPosition="484">ill have relatively high surprisal if the context only consists of the long. However, a distributional representation that retains the other content words in the sentence, thus representing the contextual similarity of grass to sheep ate, would able to capture the relevant context for content word prediction more easily. In the approach taken here, both types of models are combined: a standard language model is reweighted with semantic similarities in order to capture both short- and more long-distance dependency effects within the sentence. The semantic surprisal model, a reimplementation of Mitchell (2011), uses a word vector w and a history or context vector h to calculate the language model p(w|h), defining this probability in vector space via cosine similarity. Words that have a higher distributional similarity to their context are thus represented as having a higher probability than words that do not. Thus, we calculate probabilities for words in the context of a sentence in a framework of distributional semantics. Regarding our main hypothesis—that speakers adapt their speech rate as a function of a word’s information content—it is particularly important to 763 Proceedings of the 53rd Annu</context>
<context position="12889" citStr="Mitchell (2011)" startWordPosition="2065" endWordPosition="2066">s not pay a role in the calculation, such that a rare synonym of a frequent word might get a high similarity rating, despite low predictability, and (c) the calculation can result in negative values. p(ci|w) p(ci) vi = cos(cp) = h| |79|| 765 This problem is addressed by two changes to the notion of dot product used in the calculation of the cosine: p(ci|h) (4) p(ci) The influence of word frequencies is then restored using p(w) and p(ci): p(ci |h)p(ci) (5) p(ci) This expression reweights the new scalar product with the likelihood of the given words and the context words. We refer the reader to Mitchell (2011) in order to see that this is a true probability. The application of Bayes’ Rule allows us to rewrite the formula as p(w|h) = Pi p(w|ci)p(ci|h). Nevertheless, equation (5) is better suited to our task, as it operates directly over our word vectors. 4.3 Incremental processing Equation (5) provides a conditional probability for a word w and its history h. To calculate the product p(ci|w) p(ci|h), we need the components of the p(ci)p(ci) vectors for w and h at the current position in the sentence. We can get w1 from directly from the vector space of words. However, h1 does not have a direct repre</context>
<context position="14130" citStr="Mitchell (2011)" startWordPosition="2274" endWordPosition="2275">nd it must be constructed compositionally: = 1h1 1w1 Initialization (6) 1hn = f(1hn−1, 1wn) Composition (7) f is a vector composition function that can be chosen independently from the model. The history is initialized using the vector of the first word and combined step-by-step with the vectors of the following words. History vectors that arise from the composition step are normalized2: Normalization (8) The equations (5), (6), (7), and (8) represent a simple language model, assuming calculation of vector components with equation (1). 2This equation is slightly different from what appears in Mitchell (2011). We present here a corrected formula based on private communication with the author. 4.4 Accounting for word order The model described so far is based on semantic coherence and mostly ignores word order. Consequently, it has poor predictive power. In this section, we describe how a notion of word order is included in the model through the integration of an n-gram language model. Specifically, equation (5) can be represented as the product of two factors: p(w|h) = p(w)Δ(w, h) (9) p(ci |h)p(ci) (10) p(ci) where Δ is the semantic component that scales p(w) in function of the context. A word w th</context>
<context position="16242" citStr="Mitchell (2011)" startWordPosition="2640" endWordPosition="2641"> mass whose size depends on its similarity estimate ˆp(wn, hn−3, wn−1 n−2) relative to the similarity estimates of all other words Pwc ˆp(wc, hn−3, wn−1 n−2). The factor Pwc p(wc|wn−1 n−2) ensures that not all of the probability mass is divided up among the content words wc; rather, only the mass assigned by the n-gram model at position wn−1 n−2 is re-distributed. The w1· h1 = X i p(ci|w) p(ci) X p(ci|w) p(w|h) = p(w) p(ci) i ˆhi hi = ˆhjp(cj) P j X p(ci|w) Δ(w, h) = p(ci) i n−1 p(wn |hn−3, wn−2) = ⎧ ⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ E wc P wc 766 probability mass of the function words remains unchanged. Mitchell (2011) restricts the history so that only words outside the trigram window are taken into account in order to keep the n-gram model and the semantic similarity model independent. Thus, the n-gram model represents local dependencies, and the semantic model represents longer-distance dependencies. The final model that we use in our experiment consists of equations (1), (6), (7), (8) and (12). 5 Evaluation Methods Our goal is to test whether semantically reweighted surprisal can explain spoken word durations over and above more simple factors that are known to influence word durations, such as word len</context>
</contexts>
<marker>Mitchell, 2011</marker>
<rawString>Mitchell, J. J. (2011). Composition in distributional models of semantics. PhD thesis, The University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Pinheiro</author>
<author>D M Bates</author>
</authors>
<date>2000</date>
<booktitle>MixedEffects Models in S and S-PLUS. Statistics and Computing.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="17451" citStr="Pinheiro and Bates, 2000" startWordPosition="2826" endWordPosition="2829">uch as word length, frequency and predictability using a simpler language model. Our first experiment tests whether semantic surprisal based on a model trained using in-domain data is predictive of word pronunciation duration, considering the UID hypothesis. For our in-domain model, we estimate surprisal using 10-fold cross-validation over the AMI corpus: we divide the corpus into ten equally-sized segments and produce surprisal values for each word in each segment based on a model trained from the other nine segments. We then use linear mixed effects modeling (LME) via the lme4 package in R (Pinheiro and Bates, 2000; Bates et al., 2014) in order to account for word pronunciation length. We follow the approach of Demberg et al. (2012). Linear mixed effects modelling is a generalization of linear regression modeling and includes both fixed effects and random effects. This is particularly useful when we have a statistical units (e.g., speakers) each with their own set of repeated measures (e.g., word duration), but each such unit has its own particular characteristics (e.g., some speakers naturally speak more slowly than others). These are the random effects. The fixed effects are those characteristics that</context>
</contexts>
<marker>Pinheiro, Bates, 2000</marker>
<rawString>Pinheiro, J. C. and Bates, D. M. (2000). MixedEffects Models in S and S-PLUS. Statistics and Computing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>A Bachrach</author>
<author>C Cardenas</author>
<author>C Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>324--333</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="30375" citStr="Roark et al., 2009" startWordPosition="4940" endWordPosition="4943">** FGiga -0.051 -19.41 *** SSyntax 0.011 17.61 *** SSemantics 0.015 4.99 *** DMARY:FAMI -0.007 -4.44 *** Table 5: Linear mixed effects model for spoken word durations in the AMI corpus, for a model including both syntactic and semantic surprisal as a predictor as well as a random intercept and slope for DMARY and SSemantics under speaker. prisal (x2 = 309.5; p &lt; 0.00001), and that semantic surprisal improves model fit over and above a model including syntactic surprisal and trigram surprisal (x2 = 28.5;p &lt; 0.00001). Table 5 shows the model containing both syntactic based on the Roark parser ((Roark et al., 2009); see also Demberg et al. (2012) for use of syntactic surprisal for estimating spoken word durations) and semantic surprisal. Finally, we split our dataset into data from native and non-native speakers of English (305 native speakers, vs. 376 non-native speakers). Table 6 shows generally larger effects for native than non-native speakers. In particular, the interaction between duration estimates and word frequencies, and semantic surprisal were not significant predictors in the non-native speaker model (however, random slopes for semantic surprisal under speaker still improved model fit very s</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Roark, B., Bachrach, A., Cardenas, C., and Pallier, C. (2009). Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sayeed</author>
<author>V Demberg</author>
</authors>
<title>Combining unsupervised syntactic and semantic models of thematic fit.</title>
<date>2014</date>
<booktitle>In Proceedings of the first Italian Conference on Computational Linguistics (CLiC-it</booktitle>
<marker>Sayeed, Demberg, 2014</marker>
<rawString>Sayeed, A. and Demberg, V. (2014). Combining unsupervised syntactic and semantic models of thematic fit. In Proceedings of the first Italian Conference on Computational Linguistics (CLiC-it 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<pages>27--379</pages>
<contexts>
<context position="6097" citStr="Shannon (1948)" startWordPosition="946" endWordPosition="948"> Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (1948), who showed that conveying information uniformly close to channel capacity is optimal for communication through a (noisy) communication channel. Frank and Jaeger (2008) investigated UID effects in the SWITCHBOARD corpus at a morphosyntactic level wherein speakers avoid using English contracted forms (“you are” vs. “you’re”) when the contractible phrase is also transmitting a high degree of information in context. In this case, n-gram surprisal was used as the information density measure. Related hypotheses have been suggested by Jurafsky et al. (2001), who related speech durations to bigram p</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Shannon, C. E. (1948). A mathematical theory of communication. Bell System Technical Journal, 27(379-423):623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Smith</author>
<author>R Levy</author>
</authors>
<title>The effect of word predictability on reading time is logarithmic.</title>
<date>2013</date>
<journal>Cognition,</journal>
<volume>128</volume>
<issue>3</issue>
<contexts>
<context position="5545" citStr="Smith and Levy (2013)" startWordPosition="856" endWordPosition="859">bability of a word given its previous (linguistic) context. It is a measure of information content in which a high surprisal implies low predictability. The use of surprisal in psycholinguistic research goes back to Hale (2001), who used a probabilistic Earley Parser to model the difficulty in parsing so-called garden path sentences (e.g. “The horse raced past the barn fell”), wherein the unexpectedness of an upcoming word or structure influences the language processor’s difficulty. Recent work in psycholinguistics has provided increasing support (e.g., Levy (2008); Demberg and Keller (2008); Smith and Levy (2013); Frank et al. (2013)) for the hypothesis that the surprisal of a word is proportional to the processing difficulty (measured in terms of reading times and EEG event-related potentials) it causes to a human. The Uniform Information Density (UID) hypothesis (Frank and Jaeger, 2008) holds that speakers tend distribute information uniformly across an utterance (in the limits of grammaticality). Information density is quantified in terms of the surprisal of each word (or other linguistic unit) in the utterance. These notions go back to Shannon (1948), who showed that conveying information uniforml</context>
</contexts>
<marker>Smith, Levy, 2013</marker>
<rawString>Smith, N. J. and Levy, R. (2013). The effect of word predictability on reading time is logarithmic. Cognition, 128(3):302–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="8040" citStr="Turney and Pantel, 2010" startWordPosition="1245" endWordPosition="1249">milarity of linguistic units (e.g., words, sentences, texts) in some numerical space that represents the contexts in which they appear, it is possible to approximate the semantics in distributional terms. This is usually done by collecting statistics from a corpus using techniques developed for information retrieval. Using these statistics as a model of semantics is justified in terms of the “distributional hypothesis”, which holds that words used in similar contexts have similar meanings (Harris, 1954). A simple and widely-used type of distributional semantic model is the vector space model (Turney and Pantel, 2010). In such a model, all words are represented each in terms of vectors in a single high-dimensional space. The semantic similarity of words can then be calculated via the cosine of the angle between the vectors in this manare usually excluded from this calculation. Until relatively recently (Erk, 2012), distributional semantic models did not take into account the finegrained details of syntactic and semantic structure construed in formal terms. ner: cos(ϕ) _ �a·�b Closed-class function words a bi || 764 3 Corpus is calculated from the composition of individThe AMI Meeting Corpus (Carletta, 2007</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, P. D. and Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>