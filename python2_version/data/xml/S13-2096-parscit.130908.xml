<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007439">
<title confidence="0.99811">
UNITOR-HMM-TK: Structured Kernel-based Learning
for Spatial Role Labeling
</title>
<author confidence="0.9551055">
Emanuele Bastianelli(†)(*), Danilo Croce(‡), Daniele Nardi(*), Roberto Basili(‡)
(†) DICII (�) DII (*) DIAG
</author>
<affiliation confidence="0.8970995">
University of Roma Tor Vergata University of Roma Tor Vergata University of Roma La Sapienza
Rome, Italy Rome, Italy Rome, Italy
</affiliation>
<email confidence="0.985398">
{bastianelli}@ing.uniroma2.it {croce,basili}@info.uniroma2.it {nardi}@dis.uniroma1.it
</email>
<sectionHeader confidence="0.975257" genericHeader="abstract">
Abstract
</sectionHeader>
<figureCaption confidence="0.961253888888889">
In this paper the UNITOR-HMM-TK system
participating in the Spatial Role Labeling
task at SemEval 2013 is presented. The
spatial roles classification is addressed as a
sequence-based word classification problem:
the SVM&amp;quot;&amp;quot; learning algorithm is applied,
based on a simple feature modeling and a ro-
bust lexical generalization achieved through a
Distributional Model of Lexical Semantics. In
the identification of spatial relations, roles are
combined to generate candidate relations, later
verified by a SVM classifier. The Smoothed
Partial Tree Kernel is applied, i.e. a con-
volution kernel that enhances both syntactic
and lexical properties of the examples, avoid-
ing the need of a manual feature engineering
phase. Finally, results on three of the five tasks
of the challenge are reported.
</figureCaption>
<sectionHeader confidence="0.996945" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993661">
Referring to objects or entities in the space, as well
as to relations holding among them, is one of the
most important functionalities in natural language
understanding. The detection of spatial utterances
thus finds many applications, such as in GPS navi-
gation systems, or Human-Robot Interaction (HRI).
In Computational Linguistics, the task of recog-
nizing spatial information is known as Spatial Role
Labeling (SpRL), as discussed in (KordJamshidi et
al., 2010). Let us consider the sentence:
</bodyText>
<equation confidence="0.7089">
[A man]TRAJECTOR is sitting [on]SPATIAL INDICATOR
</equation>
<bodyText confidence="0.983753">
[a chair]LANDMARK and talking on the phone. (1)
where three roles are labeled: the phrase “A man”
refers to a TRAJECTOR, “a chair” to a LAND-
MARK and they are related by the spatial expres-
sion “on” denoted as SPATIAL INDICATOR. The
last role establishes the type of the spatial relation,
e.g. Regional. The ambiguity of natural language
makes this task very challenging. For example, in
the same Example 1, another preposition “on” can
be considered, but the phrase “the phone” is not a
spatial role, as it refers to a communication mean.
This mainly depends on the semantics of the gram-
matical headwords, i.e. chair and phone. Such phe-
nomena are crucial in many learning frameworks,
as in kernel-based learning (Shawe-Taylor and Cris-
tianini, 2004), where the decision is based on the
similarity between training and testing data.
This paper describes the UNITOR-HMM-TK sys-
tem participating in the Semeval 2013 Spatial Role
Labeling Task (Kolomiyets et al., 2013), addressing
three of the five defined sub-tasks:
</bodyText>
<listItem confidence="0.977925142857143">
• Task A: Spatial Role Classification. It con-
sists in labeling short sentences with spatial
roles among SPATIAL INDICATOR, TRAJEC-
TOR and LANDMARK.
• Task B: Relation Identification. It consists in
the identification of relations among roles iden-
tified in Task A. This task does not involve the
semantic relation classification.
• Task C: Spatial Role Classification. It con-
sists in labeling short documents with spa-
tial roles among the extended role set: TRA-
JECTOR, LANDMARK, SPATIAL INDICATOR,
MOTION INDICATOR, PATH, DIRECTION and
DISTANCE.
</listItem>
<bodyText confidence="0.997976">
The UNITOR-HMM-TK system addresses both the
problems of identifying spatial roles and relations as
a sequence of two main classification steps.
</bodyText>
<page confidence="0.975579">
573
</page>
<bodyText confidence="0.99699002">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 573–579, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
In the first step, each word in the sentence is
classified by a sequence-based classifier with re-
spect to the possible spatial roles. It is in line
with other methods based on sequence-based clas-
sifier for SpRL (Kordjamshidi et al., 2011; Kord-
jamshidi et al., 2012b). Our labeling has been in-
spired by the work in (Croce et al., 2012), where
the SVMhmm learning algorithm, formulated in (Al-
tun et al., 2003), has been applied to the classi-
cal FrameNet-based Semantic Role Labeling. The
main contribution in (Croce et al., 2012) is the adop-
tion of shallow grammatical features (e.g. POS-
tag sequences) instead of the full syntax of the sen-
tence, in order to avoid over-fitting over training
data. Moreover, lexical information has been gen-
eralized through the use of a Word Space, in line
with (Schutze, 1998; Sahlgren, 2006): it consists
in a Distributional Model of Lexical Semantics de-
rived from the unsupervised analysis of an unla-
beled large-scale corpus. The result is a geometri-
cal space where words with similar meaning, e.g.
involved in a paradigmatic or almost-synonymic re-
lations, will be projected in similar vectors. As an
example, we expect that a word like “table”, maybe
a LANDMARK in a training example, is more similar
to “chair” as compared with “phone”.
In the second step, all roles found in a sentence are
combined to generate candidate relations, which are
then verified by a Support Vector Machine (SVM)
classifier. As the entire sentence is informative to de-
termine the proper conjunction of all roles, we apply
a kernel function within the classifier, that enhances
both syntactic and lexical information of the exam-
ples. We adopted the Smoothed Partial Tree Kernel
(SPTK), defined in (Croce et al., 2011): it is con-
volution kernel that allows to measure the similar-
ity between syntactic structures, which are partially
similar and whose nodes can differ, but are semanti-
cally related. Each example is represented as a tree
structure directly derived from the sentence depen-
dency parse, thus avoiding the manual definition of
features. Similarity between lexical nodes is mea-
sured in the same Word Space mentioned above.
In the rest of the paper, Section 2 discusses the
SVMhmm based approach. The SPTK-based learn-
ing algorithm will be presented in Section 3. Finally,
results obtained in the competition are discussed in
Section 4.
</bodyText>
<sectionHeader confidence="0.8106435" genericHeader="method">
2 Sequential Tagging for Spatial Role
Classification
</sectionHeader>
<bodyText confidence="0.999049066666666">
The system proposed for the Spatial Role Classi-
fication task is based on the SVMhmm formula-
tion discussed in (Altun et al., 2003). It extends
classical SVMs by learning a discriminative model
isomorphic to a k-order Hidden Markov Model
through the Structural SVM formulation (Tsochan-
taridis et al., 2005). In the discriminative view
of SVMhmm, given an observed input word se-
quence x = (xi ... xl) E X of feature vectors
xI ... xl, the model predicts a sequence of labels
y = (yI ... yl) E Y after learning a linear discrim-
inant function F : X x Y —* R over input/output
pairs. Each word is then modeled as a set of linear
features that express lexical information as well as
syntactic information surrogated by POS n-grams.
With respect to other works using SVMhmm for
SpRL, such as (Kordjamshidi et al., 2012b), we in-
vestigate another set of possible features, as the ones
proposed in (Croce et al., 2012): the aim is to pro-
vide an agile system that takes advantages in adopt-
ing only shallow grammatical features, thus ignoring
the full syntactic information of a sentence. The syn-
tactic features derived from a dependency parse pro-
cess are surrogated by POS n-grams. According to
this, our feature modeling adopts the IOB notation
discussed in (Croce et al., 2012). It provides a class
label for each token, mapping them into artificial
classes representing the beginning (B), the inside (I)
or ending (O) of a spatial role, plus the label of the
classified role (i.e. BSPIND for the starting token of a
SPATIAL INDICATOR); words external to every role
are labeled with the special class ( ). According to
this notation, the labeling of Example 1 can be ex-
pressed as follows: “A/BTRAJ man/OTRAJ is/ sitting/
on/BSPIND a/BLAND chair/OLAND and/ . . . ”.
In order to reduce the complexity of the entire
classification task, two phases are applied. In Task
A, as in (Kordjamshidi et al., 2011), the first phase
aims at labeling only SPATIAL INDICATOR, as they
should relate remaining spatial expressions. For the
same reason, in Task C we first label only SPA-
TIAL INDICATOR and MOTION INDICATOR. Roles
classified in this step are considered pivot and they
can be used as features for the classification of the
other roles: TRAJECTORS and LANDMARKS for
</bodyText>
<page confidence="0.988813">
574
</page>
<bodyText confidence="0.999665983333333">
Task A while TRAJECTORS, LANDMARKS, PATHS,
DISTANCES and DIRECTIONS for Task C.
For the classification of SPATIAL and MOTION
INDICATOR, each word, such as the first “on” occur-
rence in the Example 1, is modeled through the fol-
lowing features: its lemma (on) and POS tag (IN);
the left and right lexical contexts, represented by the
n words before (man::NN is::VBZ sitting::VBG) and
after (a::DT chair::NN and::CC); the left and right
syntactic contexts as the POS n-grams occurring be-
fore (i.e. NN VBZ VBZ VBG NN VBZ VBG) and
after (i.e. DT NN NN CC DT NN CC) the word.
For the TRAJECTOR and LANDMARK classifica-
tion in Task A, each word is represented by the same
features described above, plus the following ones
(with respect to Example 1, the token relative to
the word man): lemma of the SPATIAL INDICATOR
(on); Positional Feature: distance from the SPATIAL
INDICATOR in terms of number of tokens (-3); rel-
ative position with respect to the SPATIAL INDICA-
TOR, that is before or after (before); a boolean fea-
ture that indicates whether or not the current token is
a SPATIAL INDICATOR; the number of words com-
posing the SPATIAL INDICATOR (here 1).
In Task C, for the classification with respect to
the complete set of roles, each word is modeled by
the previous features together with the following:
distance from the MOTION INDICATOR in terms
of number of tokens; relative position with respect
to the MOTION INDICATOR (before and after); a
boolean feature that indicates whether or not the cur-
rent token is a MOTION INDICATOR; the number of
words that composes the MOTION INDICATOR. In
both Tasks A and C the symbols SI and MI to rep-
resent a SPATIAL INDICATOR or a MOTION INDI-
CATOR are used respectively to represent the target
pivot role within any n-gram.
In order to increase the robustness of our model-
ing, we extended the lexical information with fea-
tures derived from a distributional analysis over
large texts. In essence, we represent the lexical se-
mantic similarity between different words with sim-
ilar meaning. We extend a supervised approach
through the adoption of vector based models of lex-
ical meaning: a large-scale corpus is statistically an-
alyzed and a Word Space, (Sahlgren, 2006), is ac-
quired as follows. A word-by-context matrix M
is obtained through a large scale corpus analysis.
Then the Latent Semantic Analysis (Landauer and
Dumais, 1997) technique is applied to reduce the
space dimensionality. Moreover it provides a way
to project a generic word wi into a k-dimensional
space where each row corresponds to the representa-
tion vector wi. In such a space, the distance between
vectors reflects the similarity between correspond-
ing words. The resulting feature vector representing
wi is then augmented with wi, as in (Croce et al.,
2010), where the benefits of such information have
been reported in the FrameNet-based Semantic Role
Labeling task.
</bodyText>
<sectionHeader confidence="0.997204" genericHeader="method">
3 Relation identification
</sectionHeader>
<bodyText confidence="0.999977230769231">
The UNITOR-HMM-TK system tackles Relation Iden-
tification task by determining which spatial roles,
discovered in the previous classification phase, can
be combined to determine valid spatial relations.
Our method is inspired by the work of (Roberts and
Harabagiu, 2012), where all possible spatial roles
are first generated through heuristics and then com-
binatorially combined to acquire candidate relations;
valid spatial relations are finally determined using a
SVM classifier. We aim at reducing the potentially
huge search space, by considering only spatial roles
proposed by our sequential tagging approach, de-
scribed in Section 2. Most importantly, we avoid the
manual feature engineering phase of (Roberts and
Harabagiu, 2012). Candidate relations are not rep-
resented as vectors, whose dimensions are manually
defined features useful for the target classification.
We directly apply the Smoothed Partial Tree-Kernel
(SPTK), proposed in (Croce et al., 2011), to estimate
the similarity among a specific tree representation.
Tree kernels exploit syntactic similarity through
the idea of convolutions among substructures.
Any tree kernel computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
general equation is reported hereafter:
</bodyText>
<equation confidence="0.936454">
�
TK(T1, T2) =
n1ENT1
</equation>
<bodyText confidence="0.999922">
where NT1 and NT2 are the sets of the T1’s and
T2’s nodes respectively, and A(n1, n2) is equal to
the number of common fragments rooted in the n1
and n2 nodes1. The SVM classifier is thus trained in
</bodyText>
<footnote confidence="0.884357">
1To have a similarity score between 0 and 1, a normalization
</footnote>
<equation confidence="0.838746">
E A(n1, n2)
n2ENT2
</equation>
<page confidence="0.986711">
575
</page>
<bodyText confidence="0.996775466666667">
a implicit very high-dimensional space, where each
dimension reflects a possible tree sub-structure, thus
avoiding the need of an explicit feature definition.
The function A determines the nature of such space.
For example, Syntactic Tree Kernel (STK) are used
to model complete context free rules as in (Collins
and Duffy, 2001).
The algorithm for SPTK (Croce et al., 2011)
pushes for more emphasis on lexical nodes. The
A function allows to recursively matches tree struc-
tures and lexical nodes: this allows to match frag-
ments having same structure but different lexical
nodes, by assigning a score proportional to the
product of the lexical similarities, thus generalizing
grammatical and lexical information in training data.
While similarity can be modeled directly over lexi-
cal resources, e.g. WordNet as discussed in (Peder-
sen et al., 2004), their development can be very ex-
pensive, thus limiting the coverage of the resulting
convolution kernel, especially in specific application
domains. Again, a Word Space model is adopted:
given two words, the term similarity function Q is
estimated as the cosine similarity between the corre-
sponding projections.
As proposed in (Croce et al., 2011), the SPTK is
applied to examples modeled according the Gram-
matical Relation Centered Tree (GRCT) representa-
tion, which is derived from the original dependency
parse structure. Figure 1 shows the GRCT for Exam-
ple 1: non-terminal nodes reflect syntactic relations,
such as subject (NSUBJ); pre-terminals are the POS,
such as nouns (NN), and leaves are lexemes, such as
man::n2. Non-terminal nodes associated with a role
are enriched with the role name, e.g. NSUBJTRAJ.
All nodes not covering any role are pruned out, so
that all information not concerning spatial aspects
that would introduce noise is ignored.
In this setting, positive examples are provided by
considering sentences labeled by roles involved in
a valid relation. The definition of negative exam-
ples is more difficult. We considered all roles la-
belled by the SVMhmm based system, discussed in
Section 2. For each incorrect labeling over the an-
cha
in the kernel space, i.e. TK(T1,T2) is applied.
</bodyText>
<equation confidence="0.893899">
&apos;I// TK(T1,T1) X TK(T2,T2)
</equation>
<footnote confidence="0.958157333333333">
2Each word is lemmatized to reduce data sparseness, but
they are enriched with POS tags to avoid confusing words from
different grammatical categories.
</footnote>
<page confidence="0.986214">
576
</page>
<figure confidence="0.7183046">
ROOT
PREPSPIND
POBJ
NN
LA ND
</figure>
<bodyText confidence="0.998370970588235">
of the same training dataset employed in (Kord-
jamshidi et al., 2012a)3. The dataset for Task C was
part of the Confluence corpus4. More details about
the dataset are provided in (Kolomiyets et al., 2013).
In all experiments, sentences are processed with the
Stanford CoreNLP5, for Part-of-Speech tagging,
lemmatization (Task A and C) and dependency
parsing (Task B).
The sequential labeling system described in Sec-
tion 2 has been made available by the SVMhmm
software6. The estimation of the semantically
Smoothed Partial Tree Kernel (SPTK), described in
Section 3 is made available by an extended ver-
sion of SVM-LightTK software7 (Moschitti, 2006),
implementing the smooth matching between tree
nodes. Similarity between lexical nodes is estimated
as the cosine similarity in the co-occurrence Word
Space described above, as in (Croce et al., 2011).
The co-occurrence Word Space is acquired
through the distributional analysis of the UkWaC
corpus (Baroni et al., 2009). First, all words oc-
curring more than 100 times (i.e. the targets) are
represented through vectors. The original space di-
mensions are generated from the set of the 20,000
most frequent words (i.e. features) in the UkWaC
corpus. One dimension describes the Pointwise Mu-
tual Information score between one feature, as it oc-
curs on a left or right window of 3 tokens around a
target. Left contexts of targets are treated differently
from the right ones, in order to capture asymmetric
syntactic behaviors (e.g., useful for verbs): 40,000
dimensional vectors are thus derived for each tar-
get. The Singular Value Decomposition is applied
and the space dimensionality is reduced to k = 100.
</bodyText>
<subsectionHeader confidence="0.7959">
4.1 Results in Task A
</subsectionHeader>
<bodyText confidence="0.9999745">
Two different runs were submitted for Task A. The
first takes into account all roles labeled accordingly
to the approach described in Section 2. Results, in
term of precision, recall and F-measure for each spa-
tial role are shown in Table 1. The second run con-
siders only those roles composing the relations that
</bodyText>
<footnote confidence="0.9911676">
3The initial number of sentences was of 600, but it decreased
after the elimination of 21 duplicated sentences.
4Three of the original 95 files were ignored because of some
issues with their format. See http://confluence.org
5http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<page confidence="0.609629">
6
</page>
<footnote confidence="0.9895965">
http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
7http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<bodyText confidence="0.99697425">
are positively classified in Task B and it will be dis-
cussed in Section 4.2.
A tuning phase has been carried out through a 10-
fold cross validation: it allowed to find the best clas-
sifier parameters. The evaluation of the system per-
formances is measured using a character based mea-
sure, i.e. considering the number of characters in the
span that overlap a role in the gold-standard test.
</bodyText>
<table confidence="0.9991205">
Spatial Role Precision Recall F-Measure
SPATIAL INDICATOR 0.967 0.889 0.926
TRAJECTOR 0.684 0.681 0.682
LANDMARK 0.741 0.835 0.785
</table>
<tableCaption confidence="0.99988">
Table 1: Task A results (first run)
</tableCaption>
<bodyText confidence="0.999678212121212">
The overall performances of the first run are
very promising in terms of both precision and re-
call. In particular, the SPATIAL INDICATOR label-
ing achieves a significant F-Measure of 0.926 with a
precision of 0.967. The sequence labeling approach
provides good results for the LANDMARK and the
TRAJECTOR roles too. Unfortunately, these results
are not comparable with the performances obtained
the last year edition of the SpRL task, where a gram-
matical head word-based measure has been applied.
The main difficulty in the SPATIAL INDICATOR
classification concerns the tagging of a larger or
smaller span for the roles, as for “at the back” that is
tagged as “at the back of”. On the contrary, for roles
like “to the left and the right” the system produces a
tag covering just the first three words, “to the left”,
because this shortest sequence was far more repre-
sented within the training set. Some roles corre-
sponding to unknown word sequences, such as “on
the very right”, were not labeled, leading to the little
drop in terms of recall for the SPATIAL INDICATOR.
Another issue in the TRAJECTOR and LAND-
MARK labeling is due to the absence of specific role
sequences in the training set, such as LANDMARK-
TRAJECTOR-SPATIAL INDICATOR labeled in the
test sentence “there is a [coffee table]LANDMARK with
a [sofa]TRAJECTOR [around]SP.IND”: the SVMhmm
classifier in fact tends to discard any sequence un-
seen during training. Another issue concerns the
difficulty in assigning the TRAJECTOR role to the
proper SPATIAL INDICATOR: in the sentence “a
bench with a person lying on it” where both “a
bench” and “a person” are tagged as TRAJECTOR.
</bodyText>
<page confidence="0.991886">
577
</page>
<subsectionHeader confidence="0.699039">
4.2 Results in Task B
</subsectionHeader>
<bodyText confidence="0.999609454545455">
Task B has been tackled using the SPTK-based Re-
lation Identification approach, described in Section
3. In particular, the SVM classifier is fed with 741
positive examples, corresponding to the number of
gold relations, while the negative examples genera-
tion process, described in Section 3, yielded 2,256
examples. The same Word Space described in the
previous section has been used to compute the se-
mantic similarity within the SPTK. For the tuning
phase, a 80-20 fixed split has been applied.
For this task, two different measures are pre-
sented. The Relaxed measure considers a relation
correct if each role composing it has at least one
character overlapping the corresponding gold role.
The Strict measure considers a relation correct only
if each role in it has all the characters overlapping
with the gold role. The first measure is more com-
parable with the one used in (Kordjamshidi et al.,
2012a), where a relation is considered correct only
if each grammatical head word of the involved roles
were correctly labeled. The results achieved in this
task by our system are reported in Table 2.
</bodyText>
<table confidence="0.997949666666667">
Spatial Role Precision Recall F-Measure
RELAXED 0.551 0.391 0.458
STRICT 0.431 0.306 0.358
</table>
<tableCaption confidence="0.998964">
Table 2: Task B results
</tableCaption>
<bodyText confidence="0.999778090909091">
The problem for this task is more challenging. In
fact, the overall task is strictly biased by the quality
of the SVMhmm based classifier and inherits all the
limitations underlined in Section 4.2. This mostly
affects the recall, because every error generated dur-
ing the role classification is cumulative and losing
only one role in Task A implies a misclassification
of the whole relation. However, it is important to
notice that these results have been achieved without
any manual feature engineering nor any heuristics or
hand coded lexical resource.
</bodyText>
<table confidence="0.996193">
Spatial Role Precision Recall F-Measure
SPATIAL INDICATOR 0.968 0.585 0.729
TRAJECTOR 0.682 0.493 0.572
LANDMARK 0.801 0.560 0.659
</table>
<tableCaption confidence="0.99969">
Table 3: Task A results (second run)
</tableCaption>
<bodyText confidence="0.9991298">
In the second run of Task A, we evaluate the con-
tribution of this syntactic information to filter out
roles. In Table 3 results of the second run for Task
A are reported (see previous Section). As expected,
the recall measure shows a performance drop with
respect to results shown in Table 1: the results pro-
posed in the first run represents an upperbound to the
recall as any novel role is added here. However, the
precision measure for the LANDMARK role classifi-
cation is improved of about 10%.
</bodyText>
<table confidence="0.999307125">
Spatial Role Precision Recall F-Measure
SPATIAL INDICATOR 0.609 0.479 0.536
MOTION INDICATOR 0.892 0.294 0.443
TRAJECTOR 0.565 0.317 0.406
LANDMARK 0.662 0.476 0.554
PATH 0.775 0.295 0.427
DIRECTION 0.312 0.229 0.264
DISTANCE 0.946 0.331 0.490
</table>
<tableCaption confidence="0.999036">
Table 4: Task C results
</tableCaption>
<subsectionHeader confidence="0.703685">
4.3 Results in Task C
</subsectionHeader>
<bodyText confidence="0.999894214285714">
In Task C the extended set of roles is considered.
According to this, the number of possible labels to
be learnt by the system increases, thus making the
problem more challenging. As for Task A, here the
SVMhmm has been trained over the whole training
set, using a 10-fold cross validation in the tuning
phase. Moreover, the sentences of the Confluence
corpus are far more complex than the ones from the
CLEF corpus. Confluence sentences have a more
narrative nature with respect to the CLEF sentences,
that are simple description of images. The combina-
tion of these two factors resulted in a large drop in
the performance, especially for the recall.
As shown by the results in Table 4, DIRECTION
is the most difficult role to be classified, probably
because it is represented by many different word se-
quences. Other roles are found in few instances, but
almost all correct, as for DISTANCE and MOTION
INDICATOR. The high value of Precision for the
DISTANCE role is justified by the fact that when this
role is composed by a number, (i.e. “530 meters”),
the system identified and classified it well, while for
a representation with only words (i.e. “very close”)
the system did not retrieved it at all.
Acknowledgements This work has been partially
funded by European Union VII Framework Pro-
gramme under the project Speaky for Robot within
the framework of the ECHORD Project.
</bodyText>
<page confidence="0.996632">
578
</page>
<sectionHeader confidence="0.990098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982424">
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In Proceedings
of the International Conference on Machine Learning.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS’2001), pages
625–632.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain semantic
role labeling. In ACL, pages 237–246.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-
tianelli. 2012. Structured learning for semantic role
labeling. In Intelligenza Artificiale, 6(2):163–176,
January.
Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven
Bethard, and Marie-Francine Moens. 2013. Semeval-
2013 task 2: Spatial role labeling. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion. Association for Computational Linguistics.
Parisa KordJamshidi, Martijn van Otterlo, and Marie-
Francine Moens. 2010. Spatial role labeling: Task
definition and annotation scheme. In LREC.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial role labeling: To-
wards extraction of spatial relations from natural lan-
guage. ACM Trans. Speech Lang. Process., 8(3):4:1–
4:36, December.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012a. Semeval-2012 task 3: Spa-
tial role labeling. In SemEval 2012, pages 365–373,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
2012b. Relational learning for spatial relation extrac-
tion from natural language. In Proceedings of the
21st international conference on Inductive Logic Pro-
gramming, ILP’11, pages 204–220, Berlin, Heidel-
berg. Springer-Verlag.
T. Landauer and S. Dumais. 1997. A solution to plato’s
problem: The latent semantic analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104(2):211–240.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML, pages 318–329, Berlin, Germany, September.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Kirk Roberts and Sanda Harabagiu. 2012. Utd-sprl: A
joint approach to spatial role labeling. In SemEval
2012), pages 419–424, Montr´eal, Canada, 7-8 June.
Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97–
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
J. Machine Learning Reserach., 6, December.
</reference>
<page confidence="0.998576">
579
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941833">
<title confidence="0.999333">UNITOR-HMM-TK: Structured Kernel-based for Spatial Role Labeling</title>
<author confidence="0.996807">Danilo Daniele Roberto</author>
<affiliation confidence="0.994835">DICII DII DIAG University of Roma Tor Vergata University of Roma Tor Vergata University of Roma La Sapienza</affiliation>
<address confidence="0.964483">Rome, Italy Rome, Italy Rome, Italy</address>
<abstract confidence="0.999398842105263">this paper the participating in the Spatial Role Labeling task at SemEval 2013 is presented. The spatial roles classification is addressed as a sequence-based word classification problem: the SVM&amp;quot;&amp;quot; learning algorithm is applied, based on a simple feature modeling and a robust lexical generalization achieved through a Distributional Model of Lexical Semantics. In the identification of spatial relations, roles are combined to generate candidate relations, later verified by a SVM classifier. The Smoothed Partial Tree Kernel is applied, i.e. a convolution kernel that enhances both syntactic and lexical properties of the examples, avoiding the need of a manual feature engineering phase. Finally, results on three of the five tasks of the challenge are reported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
</authors>
<title>Hidden Markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4119" citStr="Altun et al., 2003" startWordPosition="621" endWordPosition="625">Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 573–579, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics In the first step, each word in the sentence is classified by a sequence-based classifier with respect to the possible spatial roles. It is in line with other methods based on sequence-based classifier for SpRL (Kordjamshidi et al., 2011; Kordjamshidi et al., 2012b). Our labeling has been inspired by the work in (Croce et al., 2012), where the SVMhmm learning algorithm, formulated in (Altun et al., 2003), has been applied to the classical FrameNet-based Semantic Role Labeling. The main contribution in (Croce et al., 2012) is the adoption of shallow grammatical features (e.g. POStag sequences) instead of the full syntax of the sentence, in order to avoid over-fitting over training data. Moreover, lexical information has been generalized through the use of a Word Space, in line with (Schutze, 1998; Sahlgren, 2006): it consists in a Distributional Model of Lexical Semantics derived from the unsupervised analysis of an unlabeled large-scale corpus. The result is a geometrical space where words wi</context>
<context position="6251" citStr="Altun et al., 2003" startWordPosition="971" endWordPosition="974">le is represented as a tree structure directly derived from the sentence dependency parse, thus avoiding the manual definition of features. Similarity between lexical nodes is measured in the same Word Space mentioned above. In the rest of the paper, Section 2 discusses the SVMhmm based approach. The SPTK-based learning algorithm will be presented in Section 3. Finally, results obtained in the competition are discussed in Section 4. 2 Sequential Tagging for Spatial Role Classification The system proposed for the Spatial Role Classification task is based on the SVMhmm formulation discussed in (Altun et al., 2003). It extends classical SVMs by learning a discriminative model isomorphic to a k-order Hidden Markov Model through the Structural SVM formulation (Tsochantaridis et al., 2005). In the discriminative view of SVMhmm, given an observed input word sequence x = (xi ... xl) E X of feature vectors xI ... xl, the model predicts a sequence of labels y = (yI ... yl) E Y after learning a linear discriminant function F : X x Y —* R over input/output pairs. Each word is then modeled as a set of linear features that express lexical information as well as syntactic information surrogated by POS n-grams. With</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hidden Markov support vector machines. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="16235" citStr="Baroni et al., 2009" startWordPosition="2599" endWordPosition="2602">parsing (Task B). The sequential labeling system described in Section 2 has been made available by the SVMhmm software6. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK), described in Section 3 is made available by an extended version of SVM-LightTK software7 (Moschitti, 2006), implementing the smooth matching between tree nodes. Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space described above, as in (Croce et al., 2011). The co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009). First, all words occurring more than 100 times (i.e. the targets) are represented through vectors. The original space dimensions are generated from the set of the 20,000 most frequent words (i.e. features) in the UkWaC corpus. One dimension describes the Pointwise Mutual Information score between one feature, as it occurs on a left or right window of 3 tokens around a target. Left contexts of targets are treated differently from the right ones, in order to capture asymmetric syntactic behaviors (e.g., useful for verbs): 40,000 dimensional vectors are thus derived for each target. The Singula</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS’2001),</booktitle>
<pages>625--632</pages>
<contexts>
<context position="13226" citStr="Collins and Duffy, 2001" startWordPosition="2125" endWordPosition="2128">2) = n1ENT1 where NT1 and NT2 are the sets of the T1’s and T2’s nodes respectively, and A(n1, n2) is equal to the number of common fragments rooted in the n1 and n2 nodes1. The SVM classifier is thus trained in 1To have a similarity score between 0 and 1, a normalization E A(n1, n2) n2ENT2 575 a implicit very high-dimensional space, where each dimension reflects a possible tree sub-structure, thus avoiding the need of an explicit feature definition. The function A determines the nature of such space. For example, Syntactic Tree Kernel (STK) are used to model complete context free rules as in (Collins and Duffy, 2001). The algorithm for SPTK (Croce et al., 2011) pushes for more emphasis on lexical nodes. The A function allows to recursively matches tree structures and lexical nodes: this allows to match fragments having same structure but different lexical nodes, by assigning a score proportional to the product of the lexical similarities, thus generalizing grammatical and lexical information in training data. While similarity can be modeled directly over lexical resources, e.g. WordNet as discussed in (Pedersen et al., 2004), their development can be very expensive, thus limiting the coverage of the resul</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of Neural Information Processing Systems (NIPS’2001), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Cristina Giannone</author>
<author>Paolo Annesi</author>
<author>Roberto Basili</author>
</authors>
<title>Towards open-domain semantic role labeling.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>237--246</pages>
<contexts>
<context position="11141" citStr="Croce et al., 2010" startWordPosition="1806" endWordPosition="1809">tatistically analyzed and a Word Space, (Sahlgren, 2006), is acquired as follows. A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied to reduce the space dimensionality. Moreover it provides a way to project a generic word wi into a k-dimensional space where each row corresponds to the representation vector wi. In such a space, the distance between vectors reflects the similarity between corresponding words. The resulting feature vector representing wi is then augmented with wi, as in (Croce et al., 2010), where the benefits of such information have been reported in the FrameNet-based Semantic Role Labeling task. 3 Relation identification The UNITOR-HMM-TK system tackles Relation Identification task by determining which spatial roles, discovered in the previous classification phase, can be combined to determine valid spatial relations. Our method is inspired by the work of (Roberts and Harabagiu, 2012), where all possible spatial roles are first generated through heuristics and then combinatorially combined to acquire candidate relations; valid spatial relations are finally determined using a </context>
</contexts>
<marker>Croce, Giannone, Annesi, Basili, 2010</marker>
<rawString>Danilo Croce, Cristina Giannone, Paolo Annesi, and Roberto Basili. 2010. Towards open-domain semantic role labeling. In ACL, pages 237–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="5441" citStr="Croce et al., 2011" startWordPosition="840" endWordPosition="843"> similar vectors. As an example, we expect that a word like “table”, maybe a LANDMARK in a training example, is more similar to “chair” as compared with “phone”. In the second step, all roles found in a sentence are combined to generate candidate relations, which are then verified by a Support Vector Machine (SVM) classifier. As the entire sentence is informative to determine the proper conjunction of all roles, we apply a kernel function within the classifier, that enhances both syntactic and lexical information of the examples. We adopted the Smoothed Partial Tree Kernel (SPTK), defined in (Croce et al., 2011): it is convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ, but are semantically related. Each example is represented as a tree structure directly derived from the sentence dependency parse, thus avoiding the manual definition of features. Similarity between lexical nodes is measured in the same Word Space mentioned above. In the rest of the paper, Section 2 discusses the SVMhmm based approach. The SPTK-based learning algorithm will be presented in Section 3. Finally, results obtained in the competition </context>
<context position="12240" citStr="Croce et al., 2011" startWordPosition="1965" endWordPosition="1968">nd then combinatorially combined to acquire candidate relations; valid spatial relations are finally determined using a SVM classifier. We aim at reducing the potentially huge search space, by considering only spatial roles proposed by our sequential tagging approach, described in Section 2. Most importantly, we avoid the manual feature engineering phase of (Roberts and Harabagiu, 2012). Candidate relations are not represented as vectors, whose dimensions are manually defined features useful for the target classification. We directly apply the Smoothed Partial Tree-Kernel (SPTK), proposed in (Croce et al., 2011), to estimate the similarity among a specific tree representation. Tree kernels exploit syntactic similarity through the idea of convolutions among substructures. Any tree kernel computes the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its general equation is reported hereafter: � TK(T1, T2) = n1ENT1 where NT1 and NT2 are the sets of the T1’s and T2’s nodes respectively, and A(n1, n2) is equal to the number of common fragments rooted in the n1 and n2 nodes1. The SVM classifier is thus trained in 1To have a similarity score</context>
<context position="14093" citStr="Croce et al., 2011" startWordPosition="2261" endWordPosition="2264">s, by assigning a score proportional to the product of the lexical similarities, thus generalizing grammatical and lexical information in training data. While similarity can be modeled directly over lexical resources, e.g. WordNet as discussed in (Pedersen et al., 2004), their development can be very expensive, thus limiting the coverage of the resulting convolution kernel, especially in specific application domains. Again, a Word Space model is adopted: given two words, the term similarity function Q is estimated as the cosine similarity between the corresponding projections. As proposed in (Croce et al., 2011), the SPTK is applied to examples modeled according the Grammatical Relation Centered Tree (GRCT) representation, which is derived from the original dependency parse structure. Figure 1 shows the GRCT for Example 1: non-terminal nodes reflect syntactic relations, such as subject (NSUBJ); pre-terminals are the POS, such as nouns (NN), and leaves are lexemes, such as man::n2. Non-terminal nodes associated with a role are enriched with the role name, e.g. NSUBJTRAJ. All nodes not covering any role are pruned out, so that all information not concerning spatial aspects that would introduce noise is</context>
<context position="16115" citStr="Croce et al., 2011" startWordPosition="2581" endWordPosition="2584">nces are processed with the Stanford CoreNLP5, for Part-of-Speech tagging, lemmatization (Task A and C) and dependency parsing (Task B). The sequential labeling system described in Section 2 has been made available by the SVMhmm software6. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK), described in Section 3 is made available by an extended version of SVM-LightTK software7 (Moschitti, 2006), implementing the smooth matching between tree nodes. Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space described above, as in (Croce et al., 2011). The co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009). First, all words occurring more than 100 times (i.e. the targets) are represented through vectors. The original space dimensions are generated from the set of the 20,000 most frequent words (i.e. features) in the UkWaC corpus. One dimension describes the Pointwise Mutual Information score between one feature, as it occurs on a left or right window of 3 tokens around a target. Left contexts of targets are treated differently from the right ones, in order to capture asymmetri</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of EMNLP, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Giuseppe Castellucci</author>
<author>Emanuele Bastianelli</author>
</authors>
<title>Structured learning for semantic role labeling.</title>
<date>2012</date>
<journal>In Intelligenza Artificiale,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="4046" citStr="Croce et al., 2012" startWordPosition="610" endWordPosition="613">wo main classification steps. 573 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 573–579, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics In the first step, each word in the sentence is classified by a sequence-based classifier with respect to the possible spatial roles. It is in line with other methods based on sequence-based classifier for SpRL (Kordjamshidi et al., 2011; Kordjamshidi et al., 2012b). Our labeling has been inspired by the work in (Croce et al., 2012), where the SVMhmm learning algorithm, formulated in (Altun et al., 2003), has been applied to the classical FrameNet-based Semantic Role Labeling. The main contribution in (Croce et al., 2012) is the adoption of shallow grammatical features (e.g. POStag sequences) instead of the full syntax of the sentence, in order to avoid over-fitting over training data. Moreover, lexical information has been generalized through the use of a Word Space, in line with (Schutze, 1998; Sahlgren, 2006): it consists in a Distributional Model of Lexical Semantics derived from the unsupervised analysis of an unlab</context>
<context position="7029" citStr="Croce et al., 2012" startWordPosition="1110" endWordPosition="1113">is et al., 2005). In the discriminative view of SVMhmm, given an observed input word sequence x = (xi ... xl) E X of feature vectors xI ... xl, the model predicts a sequence of labels y = (yI ... yl) E Y after learning a linear discriminant function F : X x Y —* R over input/output pairs. Each word is then modeled as a set of linear features that express lexical information as well as syntactic information surrogated by POS n-grams. With respect to other works using SVMhmm for SpRL, such as (Kordjamshidi et al., 2012b), we investigate another set of possible features, as the ones proposed in (Croce et al., 2012): the aim is to provide an agile system that takes advantages in adopting only shallow grammatical features, thus ignoring the full syntactic information of a sentence. The syntactic features derived from a dependency parse process are surrogated by POS n-grams. According to this, our feature modeling adopts the IOB notation discussed in (Croce et al., 2012). It provides a class label for each token, mapping them into artificial classes representing the beginning (B), the inside (I) or ending (O) of a spatial role, plus the label of the classified role (i.e. BSPIND for the starting token of a </context>
</contexts>
<marker>Croce, Castellucci, Bastianelli, 2012</marker>
<rawString>Danilo Croce, Giuseppe Castellucci, and Emanuele Bastianelli. 2012. Structured learning for semantic role labeling. In Intelligenza Artificiale, 6(2):163–176, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Semeval2013 task 2: Spatial role labeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2720" citStr="Kolomiyets et al., 2013" startWordPosition="406" endWordPosition="409">kes this task very challenging. For example, in the same Example 1, another preposition “on” can be considered, but the phrase “the phone” is not a spatial role, as it refers to a communication mean. This mainly depends on the semantics of the grammatical headwords, i.e. chair and phone. Such phenomena are crucial in many learning frameworks, as in kernel-based learning (Shawe-Taylor and Cristianini, 2004), where the decision is based on the similarity between training and testing data. This paper describes the UNITOR-HMM-TK system participating in the Semeval 2013 Spatial Role Labeling Task (Kolomiyets et al., 2013), addressing three of the five defined sub-tasks: • Task A: Spatial Role Classification. It consists in labeling short sentences with spatial roles among SPATIAL INDICATOR, TRAJECTOR and LANDMARK. • Task B: Relation Identification. It consists in the identification of relations among roles identified in Task A. This task does not involve the semantic relation classification. • Task C: Spatial Role Classification. It consists in labeling short documents with spatial roles among the extended role set: TRAJECTOR, LANDMARK, SPATIAL INDICATOR, MOTION INDICATOR, PATH, DIRECTION and DISTANCE. The UNI</context>
<context position="15469" citStr="Kolomiyets et al., 2013" startWordPosition="2484" endWordPosition="2487">tive examples is more difficult. We considered all roles labelled by the SVMhmm based system, discussed in Section 2. For each incorrect labeling over the ancha in the kernel space, i.e. TK(T1,T2) is applied. &apos;I// TK(T1,T1) X TK(T2,T2) 2Each word is lemmatized to reduce data sparseness, but they are enriched with POS tags to avoid confusing words from different grammatical categories. 576 ROOT PREPSPIND POBJ NN LA ND of the same training dataset employed in (Kordjamshidi et al., 2012a)3. The dataset for Task C was part of the Confluence corpus4. More details about the dataset are provided in (Kolomiyets et al., 2013). In all experiments, sentences are processed with the Stanford CoreNLP5, for Part-of-Speech tagging, lemmatization (Task A and C) and dependency parsing (Task B). The sequential labeling system described in Section 2 has been made available by the SVMhmm software6. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK), described in Section 3 is made available by an extended version of SVM-LightTK software7 (Moschitti, 2006), implementing the smooth matching between tree nodes. Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Spa</context>
</contexts>
<marker>Kolomiyets, Kordjamshidi, Bethard, Moens, 2013</marker>
<rawString>Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens. 2013. Semeval2013 task 2: Spatial role labeling. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa KordJamshidi</author>
<author>Martijn van Otterlo</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Spatial role labeling: Task definition and annotation scheme.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<marker>KordJamshidi, van Otterlo, Moens, 2010</marker>
<rawString>Parisa KordJamshidi, Martijn van Otterlo, and MarieFrancine Moens. 2010. Spatial role labeling: Task definition and annotation scheme. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Martijn Van Otterlo</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Spatial role labeling: Towards extraction of spatial relations from natural language.</title>
<date>2011</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>4--36</pages>
<marker>Kordjamshidi, Van Otterlo, Moens, 2011</marker>
<rawString>Parisa Kordjamshidi, Martijn Van Otterlo, and MarieFrancine Moens. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Trans. Speech Lang. Process., 8(3):4:1– 4:36, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Semeval-2012 task 3: Spatial role labeling. In SemEval</title>
<date>2012</date>
<pages>365--373</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="3976" citStr="Kordjamshidi et al., 2012" startWordPosition="595" endWordPosition="599">h the problems of identifying spatial roles and relations as a sequence of two main classification steps. 573 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 573–579, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics In the first step, each word in the sentence is classified by a sequence-based classifier with respect to the possible spatial roles. It is in line with other methods based on sequence-based classifier for SpRL (Kordjamshidi et al., 2011; Kordjamshidi et al., 2012b). Our labeling has been inspired by the work in (Croce et al., 2012), where the SVMhmm learning algorithm, formulated in (Altun et al., 2003), has been applied to the classical FrameNet-based Semantic Role Labeling. The main contribution in (Croce et al., 2012) is the adoption of shallow grammatical features (e.g. POStag sequences) instead of the full syntax of the sentence, in order to avoid over-fitting over training data. Moreover, lexical information has been generalized through the use of a Word Space, in line with (Schutze, 1998; Sahlgren, 2006): it consists in a Distributional Model o</context>
<context position="6932" citStr="Kordjamshidi et al., 2012" startWordPosition="1093" endWordPosition="1096"> model isomorphic to a k-order Hidden Markov Model through the Structural SVM formulation (Tsochantaridis et al., 2005). In the discriminative view of SVMhmm, given an observed input word sequence x = (xi ... xl) E X of feature vectors xI ... xl, the model predicts a sequence of labels y = (yI ... yl) E Y after learning a linear discriminant function F : X x Y —* R over input/output pairs. Each word is then modeled as a set of linear features that express lexical information as well as syntactic information surrogated by POS n-grams. With respect to other works using SVMhmm for SpRL, such as (Kordjamshidi et al., 2012b), we investigate another set of possible features, as the ones proposed in (Croce et al., 2012): the aim is to provide an agile system that takes advantages in adopting only shallow grammatical features, thus ignoring the full syntactic information of a sentence. The syntactic features derived from a dependency parse process are surrogated by POS n-grams. According to this, our feature modeling adopts the IOB notation discussed in (Croce et al., 2012). It provides a class label for each token, mapping them into artificial classes representing the beginning (B), the inside (I) or ending (O) o</context>
<context position="15333" citStr="Kordjamshidi et al., 2012" startWordPosition="2460" endWordPosition="2464">is setting, positive examples are provided by considering sentences labeled by roles involved in a valid relation. The definition of negative examples is more difficult. We considered all roles labelled by the SVMhmm based system, discussed in Section 2. For each incorrect labeling over the ancha in the kernel space, i.e. TK(T1,T2) is applied. &apos;I// TK(T1,T1) X TK(T2,T2) 2Each word is lemmatized to reduce data sparseness, but they are enriched with POS tags to avoid confusing words from different grammatical categories. 576 ROOT PREPSPIND POBJ NN LA ND of the same training dataset employed in (Kordjamshidi et al., 2012a)3. The dataset for Task C was part of the Confluence corpus4. More details about the dataset are provided in (Kolomiyets et al., 2013). In all experiments, sentences are processed with the Stanford CoreNLP5, for Part-of-Speech tagging, lemmatization (Task A and C) and dependency parsing (Task B). The sequential labeling system described in Section 2 has been made available by the SVMhmm software6. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK), described in Section 3 is made available by an extended version of SVM-LightTK software7 (Moschitti, 2006), implementing the </context>
<context position="20758" citStr="Kordjamshidi et al., 2012" startWordPosition="3335" endWordPosition="3338">in Section 3, yielded 2,256 examples. The same Word Space described in the previous section has been used to compute the semantic similarity within the SPTK. For the tuning phase, a 80-20 fixed split has been applied. For this task, two different measures are presented. The Relaxed measure considers a relation correct if each role composing it has at least one character overlapping the corresponding gold role. The Strict measure considers a relation correct only if each role in it has all the characters overlapping with the gold role. The first measure is more comparable with the one used in (Kordjamshidi et al., 2012a), where a relation is considered correct only if each grammatical head word of the involved roles were correctly labeled. The results achieved in this task by our system are reported in Table 2. Spatial Role Precision Recall F-Measure RELAXED 0.551 0.391 0.458 STRICT 0.431 0.306 0.358 Table 2: Task B results The problem for this task is more challenging. In fact, the overall task is strictly biased by the quality of the SVMhmm based classifier and inherits all the limitations underlined in Section 4.2. This mostly affects the recall, because every error generated during the role classificati</context>
</contexts>
<marker>Kordjamshidi, Bethard, Moens, 2012</marker>
<rawString>Parisa Kordjamshidi, Steven Bethard, and MarieFrancine Moens. 2012a. Semeval-2012 task 3: Spatial role labeling. In SemEval 2012, pages 365–373, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Paolo Frasconi</author>
<author>Martijn Van Otterlo</author>
</authors>
<title>Marie-Francine Moens, and Luc De Raedt. 2012b. Relational learning for spatial relation extraction from natural language.</title>
<booktitle>In Proceedings of the 21st international conference on Inductive Logic Programming, ILP’11,</booktitle>
<pages>204--220</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Kordjamshidi, Frasconi, Van Otterlo, </marker>
<rawString>Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Otterlo, Marie-Francine Moens, and Luc De Raedt. 2012b. Relational learning for spatial relation extraction from natural language. In Proceedings of the 21st international conference on Inductive Logic Programming, ILP’11, pages 204–220, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="10743" citStr="Landauer and Dumais, 1997" startWordPosition="1741" endWordPosition="1744">e within any n-gram. In order to increase the robustness of our modeling, we extended the lexical information with features derived from a distributional analysis over large texts. In essence, we represent the lexical semantic similarity between different words with similar meaning. We extend a supervised approach through the adoption of vector based models of lexical meaning: a large-scale corpus is statistically analyzed and a Word Space, (Sahlgren, 2006), is acquired as follows. A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied to reduce the space dimensionality. Moreover it provides a way to project a generic word wi into a k-dimensional space where each row corresponds to the representation vector wi. In such a space, the distance between vectors reflects the similarity between corresponding words. The resulting feature vector representing wi is then augmented with wi, as in (Croce et al., 2010), where the benefits of such information have been reported in the FrameNet-based Semantic Role Labeling task. 3 Relation identification The UNITOR-HMM-TK system tackles Relation Identification task by </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML,</booktitle>
<pages>318--329</pages>
<location>Berlin, Germany,</location>
<contexts>
<context position="15914" citStr="Moschitti, 2006" startWordPosition="2553" endWordPosition="2554">loyed in (Kordjamshidi et al., 2012a)3. The dataset for Task C was part of the Confluence corpus4. More details about the dataset are provided in (Kolomiyets et al., 2013). In all experiments, sentences are processed with the Stanford CoreNLP5, for Part-of-Speech tagging, lemmatization (Task A and C) and dependency parsing (Task B). The sequential labeling system described in Section 2 has been made available by the SVMhmm software6. The estimation of the semantically Smoothed Partial Tree Kernel (SPTK), described in Section 3 is made available by an extended version of SVM-LightTK software7 (Moschitti, 2006), implementing the smooth matching between tree nodes. Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space described above, as in (Croce et al., 2011). The co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (Baroni et al., 2009). First, all words occurring more than 100 times (i.e. the targets) are represented through vectors. The original space dimensions are generated from the set of the 20,000 most frequent words (i.e. features) in the UkWaC corpus. One dimension describes the Pointwise Mutual Infor</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML, pages 318–329, Berlin, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concept.</title>
<date>2004</date>
<booktitle>In Proc. of 5th NAACL,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="13744" citStr="Pedersen et al., 2004" startWordPosition="2206" endWordPosition="2210">yntactic Tree Kernel (STK) are used to model complete context free rules as in (Collins and Duffy, 2001). The algorithm for SPTK (Croce et al., 2011) pushes for more emphasis on lexical nodes. The A function allows to recursively matches tree structures and lexical nodes: this allows to match fragments having same structure but different lexical nodes, by assigning a score proportional to the product of the lexical similarities, thus generalizing grammatical and lexical information in training data. While similarity can be modeled directly over lexical resources, e.g. WordNet as discussed in (Pedersen et al., 2004), their development can be very expensive, thus limiting the coverage of the resulting convolution kernel, especially in specific application domains. Again, a Word Space model is adopted: given two words, the term similarity function Q is estimated as the cosine similarity between the corresponding projections. As proposed in (Croce et al., 2011), the SPTK is applied to examples modeled according the Grammatical Relation Centered Tree (GRCT) representation, which is derived from the original dependency parse structure. Figure 1 shows the GRCT for Example 1: non-terminal nodes reflect syntacti</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concept. In Proc. of 5th NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirk Roberts</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Utd-sprl: A joint approach to spatial role labeling. In SemEval</title>
<date>2012</date>
<pages>419--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="11546" citStr="Roberts and Harabagiu, 2012" startWordPosition="1864" endWordPosition="1867">representation vector wi. In such a space, the distance between vectors reflects the similarity between corresponding words. The resulting feature vector representing wi is then augmented with wi, as in (Croce et al., 2010), where the benefits of such information have been reported in the FrameNet-based Semantic Role Labeling task. 3 Relation identification The UNITOR-HMM-TK system tackles Relation Identification task by determining which spatial roles, discovered in the previous classification phase, can be combined to determine valid spatial relations. Our method is inspired by the work of (Roberts and Harabagiu, 2012), where all possible spatial roles are first generated through heuristics and then combinatorially combined to acquire candidate relations; valid spatial relations are finally determined using a SVM classifier. We aim at reducing the potentially huge search space, by considering only spatial roles proposed by our sequential tagging approach, described in Section 2. Most importantly, we avoid the manual feature engineering phase of (Roberts and Harabagiu, 2012). Candidate relations are not represented as vectors, whose dimensions are manually defined features useful for the target classificatio</context>
</contexts>
<marker>Roberts, Harabagiu, 2012</marker>
<rawString>Kirk Roberts and Sanda Harabagiu. 2012. Utd-sprl: A joint approach to spatial role labeling. In SemEval 2012), pages 419–424, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="4535" citStr="Sahlgren, 2006" startWordPosition="693" endWordPosition="694">RL (Kordjamshidi et al., 2011; Kordjamshidi et al., 2012b). Our labeling has been inspired by the work in (Croce et al., 2012), where the SVMhmm learning algorithm, formulated in (Altun et al., 2003), has been applied to the classical FrameNet-based Semantic Role Labeling. The main contribution in (Croce et al., 2012) is the adoption of shallow grammatical features (e.g. POStag sequences) instead of the full syntax of the sentence, in order to avoid over-fitting over training data. Moreover, lexical information has been generalized through the use of a Word Space, in line with (Schutze, 1998; Sahlgren, 2006): it consists in a Distributional Model of Lexical Semantics derived from the unsupervised analysis of an unlabeled large-scale corpus. The result is a geometrical space where words with similar meaning, e.g. involved in a paradigmatic or almost-synonymic relations, will be projected in similar vectors. As an example, we expect that a word like “table”, maybe a LANDMARK in a training example, is more similar to “chair” as compared with “phone”. In the second step, all roles found in a sentence are combined to generate candidate relations, which are then verified by a Support Vector Machine (SV</context>
<context position="10578" citStr="Sahlgren, 2006" startWordPosition="1717" endWordPosition="1718">In both Tasks A and C the symbols SI and MI to represent a SPATIAL INDICATOR or a MOTION INDICATOR are used respectively to represent the target pivot role within any n-gram. In order to increase the robustness of our modeling, we extended the lexical information with features derived from a distributional analysis over large texts. In essence, we represent the lexical semantic similarity between different words with similar meaning. We extend a supervised approach through the adoption of vector based models of lexical meaning: a large-scale corpus is statistically analyzed and a Word Space, (Sahlgren, 2006), is acquired as follows. A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Landauer and Dumais, 1997) technique is applied to reduce the space dimensionality. Moreover it provides a way to project a generic word wi into a k-dimensional space where each row corresponds to the representation vector wi. In such a space, the distance between vectors reflects the similarity between corresponding words. The resulting feature vector representing wi is then augmented with wi, as in (Croce et al., 2010), where the benefits of such informat</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Journal of Computational Linguistics,</journal>
<volume>24</volume>
<pages>123</pages>
<contexts>
<context position="4518" citStr="Schutze, 1998" startWordPosition="691" endWordPosition="692">assifier for SpRL (Kordjamshidi et al., 2011; Kordjamshidi et al., 2012b). Our labeling has been inspired by the work in (Croce et al., 2012), where the SVMhmm learning algorithm, formulated in (Altun et al., 2003), has been applied to the classical FrameNet-based Semantic Role Labeling. The main contribution in (Croce et al., 2012) is the adoption of shallow grammatical features (e.g. POStag sequences) instead of the full syntax of the sentence, in order to avoid over-fitting over training data. Moreover, lexical information has been generalized through the use of a Word Space, in line with (Schutze, 1998; Sahlgren, 2006): it consists in a Distributional Model of Lexical Semantics derived from the unsupervised analysis of an unlabeled large-scale corpus. The result is a geometrical space where words with similar meaning, e.g. involved in a paradigmatic or almost-synonymic relations, will be projected in similar vectors. As an example, we expect that a word like “table”, maybe a LANDMARK in a training example, is more similar to “chair” as compared with “phone”. In the second step, all roles found in a sentence are combined to generate candidate relations, which are then verified by a Support V</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Journal of Computational Linguistics, 24:97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2505" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="372" endWordPosition="376">AJECTOR, “a chair” to a LANDMARK and they are related by the spatial expression “on” denoted as SPATIAL INDICATOR. The last role establishes the type of the spatial relation, e.g. Regional. The ambiguity of natural language makes this task very challenging. For example, in the same Example 1, another preposition “on” can be considered, but the phrase “the phone” is not a spatial role, as it refers to a communication mean. This mainly depends on the semantics of the grammatical headwords, i.e. chair and phone. Such phenomena are crucial in many learning frameworks, as in kernel-based learning (Shawe-Taylor and Cristianini, 2004), where the decision is based on the similarity between training and testing data. This paper describes the UNITOR-HMM-TK system participating in the Semeval 2013 Spatial Role Labeling Task (Kolomiyets et al., 2013), addressing three of the five defined sub-tasks: • Task A: Spatial Role Classification. It consists in labeling short sentences with spatial roles among SPATIAL INDICATOR, TRAJECTOR and LANDMARK. • Task B: Relation Identification. It consists in the identification of relations among roles identified in Task A. This task does not involve the semantic relation classification. • Task </context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>J. Machine Learning Reserach.,</journal>
<volume>6</volume>
<contexts>
<context position="6426" citStr="Tsochantaridis et al., 2005" startWordPosition="996" endWordPosition="1000"> nodes is measured in the same Word Space mentioned above. In the rest of the paper, Section 2 discusses the SVMhmm based approach. The SPTK-based learning algorithm will be presented in Section 3. Finally, results obtained in the competition are discussed in Section 4. 2 Sequential Tagging for Spatial Role Classification The system proposed for the Spatial Role Classification task is based on the SVMhmm formulation discussed in (Altun et al., 2003). It extends classical SVMs by learning a discriminative model isomorphic to a k-order Hidden Markov Model through the Structural SVM formulation (Tsochantaridis et al., 2005). In the discriminative view of SVMhmm, given an observed input word sequence x = (xi ... xl) E X of feature vectors xI ... xl, the model predicts a sequence of labels y = (yI ... yl) E Y after learning a linear discriminant function F : X x Y —* R over input/output pairs. Each word is then modeled as a set of linear features that express lexical information as well as syntactic information surrogated by POS n-grams. With respect to other works using SVMhmm for SpRL, such as (Kordjamshidi et al., 2012b), we investigate another set of possible features, as the ones proposed in (Croce et al., 20</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. J. Machine Learning Reserach., 6, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>