<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000377">
<note confidence="0.509206">
Accurate and Robust LFG-Based Generation for Chinese
</note>
<author confidence="0.992178">
Yuqing Guo Haifeng Wang Josef van Genabith
</author>
<affiliation confidence="0.9839235">
NCLT, School of Computing Toshiba (China) NCLT, School of Computing
Dublin City University Research and Development Center Dublin City University
</affiliation>
<address confidence="0.802809">
Dublin 9, Ireland Beijing, 100738, China IBM CAS, Dublin, Ireland
</address>
<email confidence="0.995098">
yguo@computing.dcu.ie wanghaifeng@rdc.toshiba.com.cn josef@computing.dcu.ie
</email>
<sectionHeader confidence="0.996597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836684210527">
We describe three PCFG-based models for
Chinese sentence realisation from Lexical-
Functional Grammar (LFG) f-structures. Both
the lexicalised model and the history-based
model improve on the accuracy of a simple
wide-coverage PCFG model by adding lexical
and contextual information to weaken inap-
propriate independence assumptions implicit
in the PCFG models. In addition, we pro-
vide techniques for lexical smoothing and rule
smoothing to increase the generation cover-
age. Trained on 15,663 automatically LFG f-
structure annotated sentences of the Penn Chi-
nese treebank and tested on 500 sentences ran-
domly selected from the treebank test set, the
lexicalised model achieves a BLEU score of
0.7265 at 100% coverage, while the history-
based model achieves a BLEU score of 0.7245
also at 100% coverage.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999745808510638">
Sentence generation, or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given abstract semantic /
logical representation according to some linguistic
theory, e.g. Lexical Functional Grammar (LFG),
Head-Driven Phrase Structure Grammar (HPSG),
Combinatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. Grammars, such as
these, are declarative formulations of the correspon-
dences between semantic and syntactic representa-
tions. Traditionally, grammar rules have been care-
fully handcrafted, such as those used in LinGo (Car-
roll et al., 1999), OpenCCG (White, 2004) and
XLE (Crouch et al., 2007). As handcrafting gram-
mar rules is time-consuming, language-dependent
and domain-specific, recent years have witnessed re-
search on extracting wide-coverage grammars auto-
matically from annotated corpora, for both parsing
and generation. FERGUS (Bangalore and Rambow,
2000) took dependency structures as inputs, and pro-
duced XTAG derivations by a stochastic tree model
automatically acquired from an annotated corpus.
Nakanishi et al. (2005) presented log-linear models
for a chart generator using a HPSG grammar ac-
quired from the Penn-II Treebank. From the same
treebank, Cahill and van Genabith (2006) automati-
cally extracted wide-coverage LFG approximations
for a PCFG-based generation model.
In addition to applying statistical techniques to
automatically acquire generation grammars, over the
last decade, there has been a lot of interest in a
generate-and-select paradigm for surface realisation.
The paradigm is characterised by a separation be-
tween generation and selection, in which symbolic
or rule-based methods are used to generate a space
of possible paraphrases, and statistical methods are
used to select one or more outputs from the space.
Starting from Langkilde (2002) who used a n-gram
language model to rank generated output strings, a
substantial number of traditional handcrafted sur-
face realisers have been augmented with sophisti-
cated stochastic rankers (Velldal and Oepen, 2005;
White et al., 2007; Cahill et al., 2007).
It is interesting to note that, while the study of
how the granularity of context-free grammars (CFG)
affects the performance of a parser (e.g. in the form
</bodyText>
<page confidence="0.987729">
86
</page>
<figure confidence="0.959151038461538">
n2:NP
[↑SUBJ=↓]
n4:NR
[↑=↓]
M*K
JiangZemin
n5:VV
[↑=↓]
rJni.
interview
n1:IP
[↑=↓]
n7:NR
[↓∈↑ADJUNCT]
pj_n
Thai
n3:VP
[↑=↓]
n6:NP
[↑OBJ=↓]
n8:NN
[↑=↓]
,01It
president
f1
�
</figure>
<equation confidence="0.999387181818182">
� � � � � � �
� � � �
� �
� �
� �
� �
��� � � �
��� �
� � �
� �
� �
�� ��
� �
�PRED ‘��’
� � �
�PRED ‘���’
� � � �
�SUBJ f2LNTYPE proper �
� �NUM sg
� � � ‘��’
�PRED
� � �NTYPE common
� �
� �
� �NUM � �sg �
� OBJ f3 �
� �PRED ‘��’
� � ��
� �
� �ADJUNCT f4 NTYPE proper
L L I NUM sg
0 : N//→F
0(n1)=0(n3)=0(n5)=f1 0(n2)=0(n4)=f2 0(n6)=0(n8)=f3 0(n7)=f4
</equation>
<figureCaption confidence="0.999159">
Figure 1: C- and f-structures with 0 links for the sentence “� 7N;)FLT�Q,ultV”
</figureCaption>
<bodyText confidence="0.999980791666667">
of grammar transforms (Johnson, 1998) and lexical-
isation (Collins, 1997)) has attracted substantial at-
tention, to our knowledge, there has been a lot less
research on this subject for surface realisation, a pro-
cess that is generally regarded as the reverse pro-
cess of parsing. Moreover, while most of the re-
search so far has concentrated on English or Euro-
pean languages, we are also interested in generation
for other languages with diverse properties, such as
Chinese which is currently a focus language in pars-
ing (Bikel, 2004; Cao et al., 2007).
In this paper, we investigate three generative
PCFG models for Chinese generation based on
wide-coverage LFG grammars automatically ex-
tracted from the Penn Chinese Treebank (CTB). Our
work is couched in the framework of Lexical Func-
tional Grammar and is implemented in a chart-style
generator. We briefly describe LFG and the basic
generation model in Section 2. We improve the
baseline PCFG model by weakening the indepen-
dence assumptions in two disambiguation models in
Section 3. Section 4 describes the smoothing algo-
rithms adopted for the chart generator and Section 5
gives the experimental details and results.
</bodyText>
<sectionHeader confidence="0.99688" genericHeader="method">
2 LFG-Based Generation
</sectionHeader>
<subsectionHeader confidence="0.999346">
2.1 Lexical Functional Grammar
</subsectionHeader>
<bodyText confidence="0.999909421052632">
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar formal-
ism which postulates (minimally) two levels of rep-
resentation: c(onstituent)-structure and f(unctional)-
structure. C-structure takes the form of phrase struc-
ture trees and captures surface grammatical config-
urations. F-structure encodes more abstract gram-
matical functions (GFs) such as SUBJ(ect), OBJ(ect),
ADJUNCT and TOPIC etc., in the form of hierar-
chical attribute-value matrices. C-structures and
f-structures are related by a piecewise correspon-
dence function 0 that goes from the nodes of a c-
structure tree into units of f-structure spaces (Ka-
plan, 1995). As illustrated in Figure 1, given a
c-structure node nz, the corresponding f-structure
component fj is 0(nz). Admissible c-structures
are specified by a context-free grammar. The cor-
responding f-structures are derived from functional
annotations attached to the CFG rewriting rules.
</bodyText>
<equation confidence="0.9178995">
(1)
[↑=↓]
</equation>
<bodyText confidence="0.9591152">
(1) shows a miniature set of annotated CFG rules
(lexical entries omitted) which generates the c- and
f-structure in Figure 1. In the functional annotations,
(↓) refers to the f-structure associated with the local
c-structure node nz, i.e. 0(nz), and (↑) refers to the
</bodyText>
<figure confidence="0.836593857142857">
IP −→ NP VP
[↑SUBJ=↓] [↑=↓]
VP −→ VV NP
[↑=↓] [↑OBJ=↓]
NP −→ NR NN
[↑ADJ=↓] [↑=↓]
NP −→ NR
</figure>
<page confidence="0.955519">
87
</page>
<table confidence="0.998597">
Model Grammar Rule Conditions
PCFG VP[T=].] → VV[T=].] NP[TOBJ=].] VP[T=].], {PRED, SUBJ, OBJ}
HB-PCFG VP[T=].] → VV[T=].] NP[TOBJ=].] VP[T=].], {PRED, SUBJ, OBJ}, TOP
LEX-PCFG VP(��)[T=].] → VV(7�-N-;JFL)[T=].] NP(,O,tV)[TOBJ=].] VP(7�-N-;JFL)[T=].], {PRED, SUBJ, OBJ}
</table>
<tableCaption confidence="0.999912">
Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models
</tableCaption>
<bodyText confidence="0.9983805">
f-structure associated with the mother (M) node of
ni, i.e. O(M(ni)).
</bodyText>
<subsectionHeader confidence="0.999387">
2.2 Generation from f-Structures
</subsectionHeader>
<bodyText confidence="0.999820870967742">
The generation task in LFG is to determine which
sentences correspond to a specified f-structure,
given a particular grammar, such as (1). Kaplan
and Wedekind (2000) proved that the set of strings
generated by an LFG grammar from fully speci-
fied f-structures is a context-free language. Based
on this theoretical cornerstone, Cahill and van Gen-
abith (2006) presented a PCFG-based chart genera-
tor using wide-coverage LFG approximations auto-
matically extracted from the Penn-II treebank. The
LFG-based statistical generation model defines the
conditional probability P(T |F), for each candidate
functionally annotated c-structure tree T (which
fully specifies a surface realisation) given an f-
structure F. The generation model searches for the
Tbest that maximises P(T|F) (Eq. 1). P(T|F) is
then decomposed as the product of the probabilities
of all the functionally annotated CFG rewriting rules
X → Y (conditioned on the left hand side (LHS) X
and local features of the corresponding f-structure
O(X)) contributing to the tree T (Eq. 2). The first
line (PCFG) of Table 1 shows the f-structure anno-
tated CFG rule to expand node n3 in Figure 1.
phenomena. Methodologies such as lexicalisa-
tion (Collins, 1997; Charniak, 2000) and tree trans-
formations (Johnson, 1998), weaken the indepen-
dence assumptions and have been applied success-
fully to parsing and shown significant improvements
over simple PCFGs. In this section we study the ef-
fect of such methods in LFG-based generation for
Chinese.
</bodyText>
<subsectionHeader confidence="0.997388">
3.1 A History-Based Model
</subsectionHeader>
<bodyText confidence="0.999994533333333">
The history-based (HB) approach which incorpo-
rates more context information has worked well
in parsing (Collins, 1997; Charniak, 2000). Re-
sembling history-based models for parsing, Hogan
et al. (2007) presented a history-based generation
model to overcome some of the inappropriate inde-
pendence assumptions in the basic generation model
of (Cahill and van Genabith, 2006). The history-
based model increases the context by simply includ-
ing the parent grammatical function GF of the f-
structure in addition to the local O-linked feature set
in the conditioning context (Eq. 3). The f-structure
annotated CFG rule expanding n3 in the history-
based model is shown in the second line (HB-PCFG)
of Table 1.1
</bodyText>
<equation confidence="0.99400175">
P(T|F) = � P(X → Y |X, Feats, GF) (3)
Tbest = argmax P(T|F) (1) X → Yin T
T Feats = {ai|ai E O(X)}
∃f (f GF) = O(X)
</equation>
<sectionHeader confidence="0.996204" genericHeader="method">
3 Disambiguation Models
</sectionHeader>
<bodyText confidence="0.999937642857143">
The basic generation model presented in (Cahill
and van Genabith, 2006) used simple probabilis-
tic context-free grammars. However, the indepen-
dence assumptions implicit in PCFG models may
not be appropriate to best capture natural language
The history-based model is motivated by English
data, for example, to generate the appropriate case
for pronouns in subject position and object position,
respectively. Though Chinese does not distinguish
cases, we expect the f-structure parent GF to help
predict grammar rule expansions more accurately in
the tree derivation than the simple PCFG model. We
will investigate how the HB model performs while
migrating it from English to Chinese data.
</bodyText>
<footnote confidence="0.522593">
1The parent grammatical function of the outermost f-
structure is assumed to be a dummy GF TOP.
</footnote>
<equation confidence="0.995872">
P(T|F) = � P(X → Y |X, Feats) (2)
X → Yin T
Feats = {ai|ai E O(X)}
</equation>
<page confidence="0.986905">
88
</page>
<subsectionHeader confidence="0.996547">
3.2 A Lexicalised Model
</subsectionHeader>
<bodyText confidence="0.997683764705882">
Compared to the HB model which includes the par-
ent grammatical function in the conditioning con-
text, lexicalised grammar rules contain more fine-
grained categorial information. To the best of our
knowledge, lexicalised parsers (Bikel, 2004) outper-
form unlexicalised parsers for Chinese. The expec-
tation is that a lexicalised PCFG model also works
better than a simple PCFG model in Chinese gen-
eration, considering e.g. prepositional phrase (PP)
modification in Chinese. Some prepositions indicat-
ing directions can occur either before or after the
main verbs, for instance both (2a) and (2b) are ac-
ceptable in Chinese. However, most PP modifiers
only act as adverbial adjuncts between the subjects
and verbal predicates. For instance “�/to” never
follows a verb as exemplified in the ungrammatical
sentence (3b).
</bodyText>
<figure confidence="0.913137">
(2) a.1A!3 11T-3f
this CLS train run to Beijing
‘The train is bound for Beijing.’
b.1A!3 11� LLI,3f
this CLS train to Beijing run
(3) a.*I WAl+I11 T�lR
Thai president to China make visit
‘The Thai president paid a visit to China.’
b. **I W11TTihn,XII+I
Thai president make visit to China
</figure>
<bodyText confidence="0.970249041666667">
In order to model phenomena such as these, we
head-lexicalise our grammar by associating each
non-terminal node with the head word2 in the c-
structure tree along the head-projection line. A non-
terminal node is written as X(x), where x is the lex-
ical head of X. The example generation grammar
rule in the lexicalised model is shown in the last line
(LEX-PCFG) of Table 1.
As in CKY chart parsing, generation grammars
are binarised in our chart generator. Thus all gram-
mar rules are either unary of the form X —* H or
binary X —* Y H (or X —* HY ), where H is the
head constituent and Y is the modifier. To handle the
problem of sparse data while estimating rule proba-
bilities, a back-off to baseline model is employed.
As, from a linguistic perspective, it is the modifier
2We use a mechanism similar to (Collins, 1997) but adapted
to Chinese data to find lexical heads in the treebank data.
rather than the head word which plays the main role
in determining word order, a back-off to partial lexi-
calisation on the modifier only is also used for bi-
nary rules. As a result, the probabilities of lexi-
calised unary and binary CFG rules are calculated
as in Eq. (4) and Eq. (5), respectively.
</bodyText>
<equation confidence="0.999819">
Pbk(H(h)|X(h)) = A1P(H(h)|X(h))
+A2P(H|X) (4)
Pbk(Y (y)H(h)|X(h)) = A1P(Y (y)H(h)|X(h))
+A2P(Y (y)H|X) + A3P(Y H|X) (5)
�where Ai = 1
i=1
</equation>
<bodyText confidence="0.999896785714286">
In principle, grammars binarisation from left-to-
right (left-) or from right-to-left (right-) are equiva-
lent to represent the original grammar and the prob-
ability distributions. However the head word is the
final constituent for most phrasal categories in Chi-
nese.3 In lexicalised model, the head word imme-
diately projects to the top level in a left-binary tree,
and as a result, the intermediate NP nodes cannot
be lexicalised with the head word as illustrated in
Figure (2b). By contrast, right-binary rules are lex-
icalised and the head word is percolated from the
bottom of the tree (Figure (2c)). Therefore we adopt
the right binarisation method in our generation algo-
rithm.
</bodyText>
<sectionHeader confidence="0.997793" genericHeader="method">
4 Chart Generation and Smoothing
Algorithms
</sectionHeader>
<subsectionHeader confidence="0.999614">
4.1 Chart Generation Algorithm
</subsectionHeader>
<bodyText confidence="0.9906945">
The PCFG-based generation algorithms are imple-
mented in terms of a chart generator (Kay, 1996).
In the generation algorithm, each (sub-)f-structure
indexes a (sub-)chart. Each local chart generates
the most probable trees for the local f-structure in
a bottom-up manner:
</bodyText>
<listItem confidence="0.8617">
• generating lexical edges from the the local GF
PRED and some atomic features representing
function words, mood or aspect etc.
</listItem>
<footnote confidence="0.6003695">
3Except for prepositional phrases, localiser and some verbal
phrases.
</footnote>
<page confidence="0.99544">
89
</page>
<figure confidence="0.994463333333333">
NP()
NN
[T=1]
�
cup
NR
[1ETADJUNCT]
f
Shanghai
NN
[1ETADJUNCT]
KA�*
tennis
NN
[1ETADJUNCT]
)�&apos;Ji �
masters
(a.) the original tree
</figure>
<figureCaption confidence="0.739665">
Figure 2: Lexicalised binary trees
</figureCaption>
<figure confidence="0.999889219512195">
NP() NP()
(b.) left-binarisation (c.) right-binarisation
NN
[T=1]
�
cup
NN
[1ETADJUNCT]
)�&apos;Ji �
masters
NR
[1ETADJUNCT]
f
Shanghai
NN
[1ETADJUNCT]
�Ji �
NN masters
NN
[T=1]
�
cup
KA�*
tennis
NN
[1ETADJUNCT]
NP(null)
[T=1]
NR
[1ETADJUNCT]
f
Shanghai
NP(null)
[T=1]
[1ETADJUNCT]
Kx�*
tennis
NP()
[T=1]
NP()
[T=1]
</figure>
<listItem confidence="0.958333">
• applying unary rules and binary rules to gener-
ate new edges until no any new edges can be
generated in the current local chart.
• propagating compatible edges to the upper-
</listItem>
<bodyText confidence="0.9798737">
level chart.
For efficiency, the generation algorithm does
Viterbi-pruning for each local chart, viz. if two
edges have equivalent categories and lexical cover-
age, only the most probable one is kept.
The generation coverage is impacted on by un-
known words4 and unmatched grammar rules in
chart generation. We present a lexical smoothing
and a rule smoothing strategy in the following sub-
sections.
</bodyText>
<subsectionHeader confidence="0.996163">
4.2 Lexical Smoothing
</subsectionHeader>
<bodyText confidence="0.999855">
In LFG f-structure, the surface form of the lemma
is represented via lexical rules involving a particular
set of features, e.g. the lemma “,0, n/president” is
represented as {TPRED=‘,0, JT’, TNTYPE=common,
TNUM=sg}. Particular lexical rules can be cap-
tured in general lexical macros abstracting away
</bodyText>
<footnote confidence="0.8021525">
4We use unknown words as a cover term to refer to all words
occurring in the test set but not in the training set.
</footnote>
<bodyText confidence="0.999869615384615">
from particular surface forms to lemmas, e.g. the
lexical macro encapsuling the above lexical rule is
{TPRED=$LEMMA, TNTYPE=common, TNUM=sg},
which generally associates to common nouns NN in
the CTB. According to the assumption that unknown
words have a probability distribution similar to ha-
pax legomenon (Baayen and Sproat, 1996), we pre-
dict the part-of-speech of unknown words from in-
frequent words in the training set by automatically
extracting lexical macros corresponding to the par-
ticular set of f-structure features. The probability of
the potential POS tag t associated to a feature set f
is estimated according to Eq. (6).
</bodyText>
<equation confidence="0.999763">
count(t, f) (6)
P(t|f) = �Z=1 count(ti, f)
</equation>
<subsectionHeader confidence="0.998076">
4.3 Rule Smoothing
</subsectionHeader>
<bodyText confidence="0.999883714285714">
The coverage of grammar rules increases with the
size of training data and in theory all the rules can
be fully covered by a training set, if it is big enough.
With limited training resources we have to resort to
fuzzy matching of grammar rules. Two smoothing
strategies are carried out at the level of grammar
rules.
</bodyText>
<page confidence="0.994717">
90
</page>
<table confidence="0.998929">
Mathched Grammar Rule
Nonsmooth VP[T=t] → VV[T=t] NP[TOBJ=t], {SUBJ, OBJ, PRED}
Feature smooth VP[T=t] → VV[T=t] NP[TOBJ=t]
Partial match VP → VV [TOBJ=t], {SUBJ, OBJ, PRED}
</table>
<tableCaption confidence="0.998432">
Table 2: Smoothing of CFG rules
</tableCaption>
<listItem confidence="0.999804">
• Reducing the conditioning f-structure features
during rule matching;
• Applying partial match during rule application.
</listItem>
<bodyText confidence="0.999740833333333">
A node in each unlexicalised grammar rule X →
Y H includes two parts: constituent category c, such
as IP, NP, VP etc.; functional f-structure annotation
a, such as [TSUBJ=t], [T=t] etc. As a heuristic based
on linguistic experience, we define the order of im-
portance of these elements as follows:
</bodyText>
<equation confidence="0.9850715">
X(c) &gt; H(c) &gt; Y (a) &gt; Y (c) &gt; X(a) &gt; H(a)
(4) IP[TCOMP=t] → NP[TSUBJ=t] VP[T=t]
</equation>
<bodyText confidence="0.9504375">
For the above example rule (4), the importance of
the elements is:
</bodyText>
<equation confidence="0.895692">
IP &gt; VP &gt; [TSUBJ=t] &gt; NP &gt; [TCOMP=t] &gt; [T=t]
</equation>
<bodyText confidence="0.999917125">
The elements can be deleted from the rules in an im-
portance order from low to high.5 The partial rules
adopted in our system ignore the least important 3
elements, viz. the functional annotation of the head
node H(a), the functional annotation on LHS X(a)
and constituent category of the modifier node Y (c).
Examples of the two types of smoothed rules are
shown in Table 2.
</bodyText>
<sectionHeader confidence="0.982454" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.997100333333333">
Our experiments are carried out on the newly
released Penn Chinese treebank version 6.0
(CTB6) (Xue et al., 2005), excluding the portion of
ACE broadcast news. We follow the recommended
splits (in the list-of-file of CTB6) to divide the
data into test set, development set and training set.
The training set includes 756 files with a total of
15,663 sentences. The CTB trees of the training set
were automatically annotated with LFG f-structure
equations following (Guo et al., 2007). Table 3
shows the number of different grammar rule types
extracted from the training set. From the test files,
</bodyText>
<footnote confidence="0.873437">
5However c and a on the same node can’t be deleted at the
same time.
</footnote>
<bodyText confidence="0.999754555555556">
we randomly select 500 sentences as test data
with minimal sentence length 5 words, maximal
length 80 words, and average length 28.84 words.
The development set also includes 500 sentences
randomly selected from the development files with
sentence length between 5 and 80 words. The
c-structure trees of the test and development data
were also automatically converted to f-structures as
input to the generator.
</bodyText>
<table confidence="0.85722625">
Type with features without features
PCFG 22,372 8,548
HB-PCFG 28,487 11,969
LEX-PCFG 325,094 286,468
</table>
<tableCaption confidence="0.998096">
Table 3: Number of rules in the training set
</tableCaption>
<bodyText confidence="0.999971857142857">
The generation system is evaluated against the
raw text of the test data in terms of accuracy and cov-
erage. Following (Langkilde, 2002) and other work
on general-purpose generators, we adopt BLEU
score (Papineni et al., 2002), average simple string
accuracy (SSA) and percentage of exactly matched
sentences for accuracy evaluation.6 For coverage
evaluation, we measure the percentage of input f-
structures that generate a sentence.
Table 4 reports the initial experiments on the sim-
ple PCFG, HB-based PCFG and lexicalised PCFG
models. The results in the left column evaluate all
input f-structures, the right column evaluate only
those f-structures which yield a complete sentence.
The results show that the lexicalised model outper-
forms the baseline PCFG model. The HB model is
the most accurate for complete sentences, but with
reduced coverage compared to the other two mod-
els. However the low coverage of sentences com-
pletely generated due to unknown words and un-
matched rules makes the results unusable in prac-
</bodyText>
<footnote confidence="0.999937">
6We are aware of the limitations in fully automatic evalua-
tion metrics, and in an ideal scenario, we would complement the
BLEU and SSA scores by a human evaluation. Unfortunately,
this is beyond the scope of the current paper.
</footnote>
<page confidence="0.99453">
91
</page>
<table confidence="0.9996762">
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 7.2% 0.5401 0.6261 36.40% 19.78% 0.7101 0.7687
HB-PCFG 100% 8.60% 0.5474 0.6281 34.80% 24.71% 0.7513 0.8092
LEX-PCFG 100% 9.40% 0.5687 0.6537 37.00% 25.41% 0.7431 0.8024
</table>
<tableCaption confidence="0.998591">
Table 4: Results without smoothing
</tableCaption>
<table confidence="0.9998672">
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 11.00% 0.6894 0.7240 94.20% 11.68% 0.7047 0.7388
HB-PCFG 100% 11.80% 0.7108 0.7348 94.00% 12.55% 0.7284 0.7506
LEX-PCFG 100% 14.00% 0.7152 0.7595 94.40% 14.83% 0.7302 0.7754
</table>
<tableCaption confidence="0.998761">
Table 5: Results with lexical smoothing
</tableCaption>
<table confidence="0.9999506">
Partial match Feature smooth
Complete Sentences Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 97.20% 11.32% 0.7022 0.7356 100% 11.20% 0.7021 0.7330
HB-PCFG 96.20% 12.27% 0.7263 0.7458 100% 12.00% 0.7245 0.7413
LEX-PCFG 97.80% 14.31% 0.7265 0.7696 100% 14.20% 0.7265 0.7675
</table>
<tableCaption confidence="0.999888">
Table 6: Results with lexical and rule smoothing
</tableCaption>
<bodyText confidence="0.99014926">
tice. 6 Conclusion and Further Work
Table 5 gives the results with lexical smoothing.
The coverage for complete sentences increases by
nearly 60% absolute for all models. The increased
coverage also improves the overall results evaluated
against all sentences. The HB model performs better
than the simple PCFG model in nearly all respects
and in turn the lexicalised model comprehensively
outperforms the HB model.
The final results with both lexical smoothing and
rule smoothing by two different strategies are tabu-
lated in Table 6. The left column provides the results
of smoothing by partial match and the right column
the results by reducing conditioning f-structure fea-
tures. All results are evaluated for completely gen-
erated sentences only. The feature smoothing re-
sults in a full coverage of 100%, while slightly de-
grading the quality of sentences generated compared
with partial match smoothing. We feel the tradeoff
at the cost of a small decrease in quality is still worth
the full coverage. Throughout the experiments, the
lexicalised model exhibits consistently better perfor-
mance than the unlexicalised models, which proves
our intuition that successful techniques in parsing
also work well in generation.
We have presented an accurate, robust chart genera-
tor for Chinese based on treebank-based, automati-
cally acquired LFG resources. Our model improves
the baseline provided by (Cahill and van Genabith,
2006): (i) accuracy is increased by creating a lexi-
calised PCFG grammar and enriching conditioning
context with parent f-structure features; and (ii) cov-
erage is increased by providing lexical smoothing
and fuzzy matching techniques for rule smoothing.
The combinational explosion of grammar rules
encountered in the chart generator is similar to that
in parsing. In the current system, we only keep the
most probable realisation for each input f-structure.
An alternative model in line with the generate-and-
select paradigm, would pack all the locally equiva-
lent edges in a forest and re-rank all the realisations
by a separate language model. This might help us to
reduce some errors caused in our current model, for
instance, the generation of function words in fixed
phrases. As shown in ex. (5), the function word
is incorrectly generated as “n, ”. This is be-
cause they share the same part-of-speech DEG in
CTB, however “n, ” has a much higher frequency
than “Z” in Chinese text and thus has a higher prob-
ability to be generated.
</bodyText>
<page confidence="0.897923">
92
</page>
<figure confidence="0.88598075">
(5) a.�4t LT
all things DE in
‘among all things’
b. *��all things DE in
</figure>
<sectionHeader confidence="0.991383" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9983538">
The research reported in this paper is supported by
Science Foundation Ireland grant 04/IN/I527. Also,
we would like to thank Aoife Cahill for many help-
ful and insightful discussions on the work. And we
gratefully acknowledge the anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.998323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999750264367816">
Harald Baayen and Richard Sproat. 1996. Estimat-
ing Lexical Priors for Low-Frequency Morphologi-
cally Ambiguous Forms. Computational Linguistics,
22(2): 155–166.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 42–48.
Saarbr¨ucken, Germany.
Daniel M. Bikel. 2004. On the Parameter Space of Gen-
erative Lexicalized Statistical Parsing Models. Ph.D.
Thesis of Department of Computer &amp; Information Sci-
ence, University of Pennsylvania.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1033–1040. Syd-
ney, Australia.
Aoife Cahill and Martin Forst and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free Word
Order Language. Proceedings of the 11th European
Workshop on Natural Language Generation, pages
17–24. Schloss Dagstuhl, Germany.
John Carroll and Ann Copestake and Dan Flickinger and
Victor Poznanski. 1999. An efficient chart genera-
tor for (semi-)lexicalist grammars. Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86–95. Toulouse, France.
Hailong Cao and Yujie Zhang and Hitoshi Isahara. 2007.
Empirical study on Parsing Chinese Based on Collins’
Model. Proceedings of the 10th Conference of the Pa-
cific Association for Computational Linguistics, pages
113–119. Melbourne, Australia.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132–139. Seattle, WA.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16–23. Madrid, Spain.
Dick Crouch and Mary Dalrymple and Ron Kaplan and
Tracy King and John Maxwell and Paula Newman.
2007. XLE Documentation. Palo Alto Research Cen-
ter, CA.
Yuqing Guo and Josef van Genabith and Haifeng Wang.
2007. Treebank-based Acquisition of LFG Resources
for Chinese. Proceedings of LFG07 Conference,
pages 214–232. Stanford, CA, USA.
Deirdre Hogan and Conor Cafferkey and Aoife Cahill
and Josef van Genabith. 2007. Exploiting Multi-Word
Units in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 267–276.
Prague, Czech Republic.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4):
613–632. MIT Press, Cambridge, MA,
Ronald M. Kaplan. 1995. The formal architecture of
lexical-functional grammar. Formal Issues in Lexical-
Functional Grammar, pages 7–27. CSLI Publications,
Standford, USA.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical
Functional Grammar: a Formal System for Grammat-
ical Representation. The Mental Representation of
Grammatical Relations, pages 173-282. MIT Press,
Cambridge, MA.
Ronald M. Kaplan and Jurgen Wedekind. 2000.
LFG Generation Produces Context-free Languages.
Proceedings of the 18th International Conference
on Computational Linguistics, pages 425–431.
Saarbr¨ucken, Germany.
Martin Kay. 1996. Chart Generation. Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, pages 200–204. Santa Cruz, USA.
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 170–177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification of
Coverage and Correctness for a General-Purpose Sen-
tence Generator. Proceedings of the Second Interna-
tional Conference on Natural Language Generation,
17–24. New York, USA.
</reference>
<page confidence="0.982984">
93
</page>
<reference confidence="0.999608846153846">
Hiroko Nakanishi and Yusuke Nakanishi and Jun’ichi
Tsujii. 2005. Probabilistic Models for Disambigua-
tion of an HPSG-Based Chart Generator. Proceedings
of the 9th International Workshop on Parsing Technol-
ogy, pages 93–102. Vancouver, British Columbia.
Kishore Papineni and Salim Roukos and Todd Ward and
Wei-Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318. Philadelphia,
USA.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. Proceedings of
the MTSummit ’05.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
Michael White and Rajakrishnan Rajkumar and Scott
Martin. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop on Language Generation and Machine
Translation, pages 22–30. Copenhagen, Danmark.
Nianwen Xue and Fei Xia and Fu dong Chiou and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2): 207–238.
</reference>
<page confidence="0.999551">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.610918">
<title confidence="0.999735">Accurate and Robust LFG-Based Generation for Chinese</title>
<author confidence="0.999156">Yuqing Guo Haifeng Wang Josef van_Genabith</author>
<affiliation confidence="0.9834445">NCLT, School of Computing Toshiba (China) NCLT, School of Dublin City University Research and Development Center Dublin City University</affiliation>
<address confidence="0.98882">Dublin 9, Ireland Beijing, 100738, China IBM CAS, Dublin,</address>
<email confidence="0.97129">yguo@computing.dcu.iewanghaifeng@rdc.toshiba.com.cnjosef@computing.dcu.ie</email>
<abstract confidence="0.97651205">We describe three PCFG-based models for Chinese sentence realisation from Lexical- Functional Grammar (LFG) f-structures. Both the lexicalised model and the history-based model improve on the accuracy of a simple wide-coverage PCFG model by adding lexical and contextual information to weaken inappropriate independence assumptions implicit in the PCFG models. In addition, we provide techniques for lexical smoothing and rule smoothing to increase the generation coverage. Trained on 15,663 automatically LFG fstructure annotated sentences of the Penn Chinese treebank and tested on 500 sentences randomly selected from the treebank test set, the lexicalised model achieves a BLEU score of 0.7265 at 100% coverage, while the historybased model achieves a BLEU score of 0.7245 also at 100% coverage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Richard Sproat</author>
</authors>
<title>Estimating Lexical Priors for Low-Frequency Morphologically Ambiguous Forms.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>155--166</pages>
<contexts>
<context position="15800" citStr="Baayen and Sproat, 1996" startWordPosition="2528" endWordPosition="2531"> e.g. the lemma “,0, n/president” is represented as {TPRED=‘,0, JT’, TNTYPE=common, TNUM=sg}. Particular lexical rules can be captured in general lexical macros abstracting away 4We use unknown words as a cover term to refer to all words occurring in the test set but not in the training set. from particular surface forms to lemmas, e.g. the lexical macro encapsuling the above lexical rule is {TPRED=$LEMMA, TNTYPE=common, TNUM=sg}, which generally associates to common nouns NN in the CTB. According to the assumption that unknown words have a probability distribution similar to hapax legomenon (Baayen and Sproat, 1996), we predict the part-of-speech of unknown words from infrequent words in the training set by automatically extracting lexical macros corresponding to the particular set of f-structure features. The probability of the potential POS tag t associated to a feature set f is estimated according to Eq. (6). count(t, f) (6) P(t|f) = �Z=1 count(ti, f) 4.3 Rule Smoothing The coverage of grammar rules increases with the size of training data and in theory all the rules can be fully covered by a training set, if it is big enough. With limited training resources we have to resort to fuzzy matching of gram</context>
</contexts>
<marker>Baayen, Sproat, 1996</marker>
<rawString>Harald Baayen and Richard Sproat. 1996. Estimating Lexical Priors for Low-Frequency Morphologically Ambiguous Forms. Computational Linguistics, 22(2): 155–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a Probabilistic Hierarchical Model for Generation.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>42--48</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="2176" citStr="Bangalore and Rambow, 2000" startWordPosition="303" endWordPosition="306">atory Categorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. Grammars, such as these, are declarative formulations of the correspondences between semantic and syntactic representations. Traditionally, grammar rules have been carefully handcrafted, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007). As handcrafting grammar rules is time-consuming, language-dependent and domain-specific, recent years have witnessed research on extracting wide-coverage grammars automatically from annotated corpora, for both parsing and generation. FERGUS (Bangalore and Rambow, 2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. Nakanishi et al. (2005) presented log-linear models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank. From the same treebank, Cahill and van Genabith (2006) automatically extracted wide-coverage LFG approximations for a PCFG-based generation model. In addition to applying statistical techniques to automatically acquire generation grammars, over the last decade, there has been a lot of interest in a generate-and-select para</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a Probabilistic Hierarchical Model for Generation. Proceedings of the 18th International Conference on Computational Linguistics, pages 42–48. Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>On the Parameter Space of Generative Lexicalized Statistical Parsing Models.</title>
<date>2004</date>
<tech>Ph.D. Thesis</tech>
<institution>of Department of Computer &amp; Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="4640" citStr="Bikel, 2004" startWordPosition="737" endWordPosition="738">f4 Figure 1: C- and f-structures with 0 links for the sentence “� 7N;)FLT�Q,ultV” of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based on wide-coverage LFG grammars automatically extracted from the Penn Chinese Treebank (CTB). Our work is couched in the framework of Lexical Functional Grammar and is implemented in a chart-style generator. We briefly describe LFG and the basic generation model in Section 2. We improve the baseline PCFG model by weakening the independence assumptions in two disambiguation models in Section 3. Section 4 describes the smoothing algorithms adopted for the chart generator and Section 5 gives</context>
<context position="10576" citStr="Bikel, 2004" startWordPosition="1669" endWordPosition="1670">rammar rule expansions more accurately in the tree derivation than the simple PCFG model. We will investigate how the HB model performs while migrating it from English to Chinese data. 1The parent grammatical function of the outermost fstructure is assumed to be a dummy GF TOP. P(T|F) = � P(X → Y |X, Feats) (2) X → Yin T Feats = {ai|ai E O(X)} 88 3.2 A Lexicalised Model Compared to the HB model which includes the parent grammatical function in the conditioning context, lexicalised grammar rules contain more finegrained categorial information. To the best of our knowledge, lexicalised parsers (Bikel, 2004) outperform unlexicalised parsers for Chinese. The expectation is that a lexicalised PCFG model also works better than a simple PCFG model in Chinese generation, considering e.g. prepositional phrase (PP) modification in Chinese. Some prepositions indicating directions can occur either before or after the main verbs, for instance both (2a) and (2b) are acceptable in Chinese. However, most PP modifiers only act as adverbial adjuncts between the subjects and verbal predicates. For instance “�/to” never follows a verb as exemplified in the ungrammatical sentence (3b). (2) a.1A!3 11T-3f this CLS t</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. On the Parameter Space of Generative Lexicalized Statistical Parsing Models. Ph.D. Thesis of Department of Computer &amp; Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations.</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1033--1040</pages>
<location>Sydney, Australia.</location>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>Aoife Cahill and Josef van Genabith. 2006. Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations. Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1033–1040. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic Realisation Ranking for a Free Word Order Language.</title>
<date>2007</date>
<booktitle>Proceedings of the 11th European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<location>Schloss Dagstuhl, Germany.</location>
<contexts>
<context position="3347" citStr="Cahill et al., 2007" startWordPosition="479" endWordPosition="482">n a lot of interest in a generate-and-select paradigm for surface realisation. The paradigm is characterised by a separation between generation and selection, in which symbolic or rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select one or more outputs from the space. Starting from Langkilde (2002) who used a n-gram language model to rank generated output strings, a substantial number of traditional handcrafted surface realisers have been augmented with sophisticated stochastic rankers (Velldal and Oepen, 2005; White et al., 2007; Cahill et al., 2007). It is interesting to note that, while the study of how the granularity of context-free grammars (CFG) affects the performance of a parser (e.g. in the form 86 n2:NP [↑SUBJ=↓] n4:NR [↑=↓] M*K JiangZemin n5:VV [↑=↓] rJni. interview n1:IP [↑=↓] n7:NR [↓∈↑ADJUNCT] pj_n Thai n3:VP [↑=↓] n6:NP [↑OBJ=↓] n8:NN [↑=↓] ,01It president f1 � � � � � � � � � � � � � � � � � � � � ��� � � � ��� � � � � � � � � �� �� � � �PRED ‘��’ � � � �PRED ‘���’ � � � � �SUBJ f2LNTYPE proper � � �NUM sg � � � ‘��’ �PRED � � �NTYPE common � � � � � �NUM � �sg � � OBJ f3 � � �PRED ‘��’ � � �� � � � �ADJUNCT f4 NTYPE prope</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill and Martin Forst and Christian Rohrer. 2007. Stochastic Realisation Ranking for a Free Word Order Language. Proceedings of the 11th European Workshop on Natural Language Generation, pages 17–24. Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Poznanski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>Proceedings of the 7th European Workshop on Natural Language Generation,</booktitle>
<pages>86--95</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1852" citStr="Carroll et al., 1999" startWordPosition="258" endWordPosition="262">urface realisation can be described as the problem of producing syntactically, morphologically, and orthographically correct sentences from a given abstract semantic / logical representation according to some linguistic theory, e.g. Lexical Functional Grammar (LFG), Head-Driven Phrase Structure Grammar (HPSG), Combinatory Categorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. Grammars, such as these, are declarative formulations of the correspondences between semantic and syntactic representations. Traditionally, grammar rules have been carefully handcrafted, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007). As handcrafting grammar rules is time-consuming, language-dependent and domain-specific, recent years have witnessed research on extracting wide-coverage grammars automatically from annotated corpora, for both parsing and generation. FERGUS (Bangalore and Rambow, 2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. Nakanishi et al. (2005) presented log-linear models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank. </context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>John Carroll and Ann Copestake and Dan Flickinger and Victor Poznanski. 1999. An efficient chart generator for (semi-)lexicalist grammars. Proceedings of the 7th European Workshop on Natural Language Generation, pages 86–95. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hailong Cao</author>
<author>Yujie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Empirical study on Parsing Chinese Based on Collins’ Model.</title>
<date>2007</date>
<booktitle>Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>113--119</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="4659" citStr="Cao et al., 2007" startWordPosition="739" endWordPosition="742">C- and f-structures with 0 links for the sentence “� 7N;)FLT�Q,ultV” of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based on wide-coverage LFG grammars automatically extracted from the Penn Chinese Treebank (CTB). Our work is couched in the framework of Lexical Functional Grammar and is implemented in a chart-style generator. We briefly describe LFG and the basic generation model in Section 2. We improve the baseline PCFG model by weakening the independence assumptions in two disambiguation models in Section 3. Section 4 describes the smoothing algorithms adopted for the chart generator and Section 5 gives the experimental d</context>
</contexts>
<marker>Cao, Zhang, Isahara, 2007</marker>
<rawString>Hailong Cao and Yujie Zhang and Hitoshi Isahara. 2007. Empirical study on Parsing Chinese Based on Collins’ Model. Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 113–119. Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="8328" citStr="Charniak, 2000" startWordPosition="1300" endWordPosition="1301">nnotated c-structure tree T (which fully specifies a surface realisation) given an fstructure F. The generation model searches for the Tbest that maximises P(T|F) (Eq. 1). P(T|F) is then decomposed as the product of the probabilities of all the functionally annotated CFG rewriting rules X → Y (conditioned on the left hand side (LHS) X and local features of the corresponding f-structure O(X)) contributing to the tree T (Eq. 2). The first line (PCFG) of Table 1 shows the f-structure annotated CFG rule to expand node n3 in Figure 1. phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. 3.1 A History-Based Model The history-based (HB) approach which incorporates more context information has worked well in parsing (Collins, 1997; Charniak, 2000). Resembling history-based models for parsing, Hogan et al. (2007) presented a history-based generation model to overcome some of the inappropriate independence assumptions</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalized Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="4183" citStr="Collins, 1997" startWordPosition="659" endWordPosition="660">interview n1:IP [↑=↓] n7:NR [↓∈↑ADJUNCT] pj_n Thai n3:VP [↑=↓] n6:NP [↑OBJ=↓] n8:NN [↑=↓] ,01It president f1 � � � � � � � � � � � � � � � � � � � � ��� � � � ��� � � � � � � � � �� �� � � �PRED ‘��’ � � � �PRED ‘���’ � � � � �SUBJ f2LNTYPE proper � � �NUM sg � � � ‘��’ �PRED � � �NTYPE common � � � � � �NUM � �sg � � OBJ f3 � � �PRED ‘��’ � � �� � � � �ADJUNCT f4 NTYPE proper L L I NUM sg 0 : N//→F 0(n1)=0(n3)=0(n5)=f1 0(n2)=0(n4)=f2 0(n6)=0(n8)=f3 0(n7)=f4 Figure 1: C- and f-structures with 0 links for the sentence “� 7N;)FLT�Q,ultV” of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based on wide-coverage LFG grammars auto</context>
<context position="8311" citStr="Collins, 1997" startWordPosition="1298" endWordPosition="1299"> functionally annotated c-structure tree T (which fully specifies a surface realisation) given an fstructure F. The generation model searches for the Tbest that maximises P(T|F) (Eq. 1). P(T|F) is then decomposed as the product of the probabilities of all the functionally annotated CFG rewriting rules X → Y (conditioned on the left hand side (LHS) X and local features of the corresponding f-structure O(X)) contributing to the tree T (Eq. 2). The first line (PCFG) of Table 1 shows the f-structure annotated CFG rule to expand node n3 in Figure 1. phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. 3.1 A History-Based Model The history-based (HB) approach which incorporates more context information has worked well in parsing (Collins, 1997; Charniak, 2000). Resembling history-based models for parsing, Hogan et al. (2007) presented a history-based generation model to overcome some of the inappropriate indepen</context>
<context position="12253" citStr="Collins, 1997" startWordPosition="1960" endWordPosition="1961">de is written as X(x), where x is the lexical head of X. The example generation grammar rule in the lexicalised model is shown in the last line (LEX-PCFG) of Table 1. As in CKY chart parsing, generation grammars are binarised in our chart generator. Thus all grammar rules are either unary of the form X —* H or binary X —* Y H (or X —* HY ), where H is the head constituent and Y is the modifier. To handle the problem of sparse data while estimating rule probabilities, a back-off to baseline model is employed. As, from a linguistic perspective, it is the modifier 2We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the treebank data. rather than the head word which plays the main role in determining word order, a back-off to partial lexicalisation on the modifier only is also used for binary rules. As a result, the probabilities of lexicalised unary and binary CFG rules are calculated as in Eq. (4) and Eq. (5), respectively. Pbk(H(h)|X(h)) = A1P(H(h)|X(h)) +A2P(H|X) (4) Pbk(Y (y)H(h)|X(h)) = A1P(Y (y)H(h)|X(h)) +A2P(Y (y)H|X) + A3P(Y H|X) (5) �where Ai = 1 i=1 In principle, grammars binarisation from left-toright (left-) or from right-to-left (right-)</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalized Models for Statistical Parsing. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23. Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE Documentation. Palo Alto Research</title>
<date>2007</date>
<location>Center, CA.</location>
<contexts>
<context position="1905" citStr="Crouch et al., 2007" startWordPosition="268" endWordPosition="271">producing syntactically, morphologically, and orthographically correct sentences from a given abstract semantic / logical representation according to some linguistic theory, e.g. Lexical Functional Grammar (LFG), Head-Driven Phrase Structure Grammar (HPSG), Combinatory Categorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. Grammars, such as these, are declarative formulations of the correspondences between semantic and syntactic representations. Traditionally, grammar rules have been carefully handcrafted, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007). As handcrafting grammar rules is time-consuming, language-dependent and domain-specific, recent years have witnessed research on extracting wide-coverage grammars automatically from annotated corpora, for both parsing and generation. FERGUS (Bangalore and Rambow, 2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. Nakanishi et al. (2005) presented log-linear models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank. From the same treebank, Cahill and van Genabith (2006</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2007</marker>
<rawString>Dick Crouch and Mary Dalrymple and Ron Kaplan and Tracy King and John Maxwell and Paula Newman. 2007. XLE Documentation. Palo Alto Research Center, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Josef van Genabith</author>
<author>Haifeng Wang</author>
</authors>
<title>Treebank-based Acquisition of LFG Resources for Chinese.</title>
<date>2007</date>
<booktitle>Proceedings of LFG07 Conference,</booktitle>
<pages>214--232</pages>
<location>Stanford, CA, USA.</location>
<marker>Guo, van Genabith, Wang, 2007</marker>
<rawString>Yuqing Guo and Josef van Genabith and Haifeng Wang. 2007. Treebank-based Acquisition of LFG Resources for Chinese. Proceedings of LFG07 Conference, pages 214–232. Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
<author>Conor Cafferkey</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Exploiting Multi-Word Units in History-Based Probabilistic Generation.</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>267--276</pages>
<location>Prague, Czech Republic.</location>
<marker>Hogan, Cafferkey, Cahill, van Genabith, 2007</marker>
<rawString>Deirdre Hogan and Conor Cafferkey and Aoife Cahill and Josef van Genabith. 2007. Exploiting Multi-Word Units in History-Based Probabilistic Generation. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 267–276. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>613--632</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="4148" citStr="Johnson, 1998" startWordPosition="654" endWordPosition="655">] M*K JiangZemin n5:VV [↑=↓] rJni. interview n1:IP [↑=↓] n7:NR [↓∈↑ADJUNCT] pj_n Thai n3:VP [↑=↓] n6:NP [↑OBJ=↓] n8:NN [↑=↓] ,01It president f1 � � � � � � � � � � � � � � � � � � � � ��� � � � ��� � � � � � � � � �� �� � � �PRED ‘��’ � � � �PRED ‘���’ � � � � �SUBJ f2LNTYPE proper � � �NUM sg � � � ‘��’ �PRED � � �NTYPE common � � � � � �NUM � �sg � � OBJ f3 � � �PRED ‘��’ � � �� � � � �ADJUNCT f4 NTYPE proper L L I NUM sg 0 : N//→F 0(n1)=0(n3)=0(n5)=f1 0(n2)=0(n4)=f2 0(n6)=0(n8)=f3 0(n7)=f4 Figure 1: C- and f-structures with 0 links for the sentence “� 7N;)FLT�Q,ultV” of grammar transforms (Johnson, 1998) and lexicalisation (Collins, 1997)) has attracted substantial attention, to our knowledge, there has been a lot less research on this subject for surface realisation, a process that is generally regarded as the reverse process of parsing. Moreover, while most of the research so far has concentrated on English or European languages, we are also interested in generation for other languages with diverse properties, such as Chinese which is currently a focus language in parsing (Bikel, 2004; Cao et al., 2007). In this paper, we investigate three generative PCFG models for Chinese generation based</context>
<context position="8369" citStr="Johnson, 1998" startWordPosition="1306" endWordPosition="1307">pecifies a surface realisation) given an fstructure F. The generation model searches for the Tbest that maximises P(T|F) (Eq. 1). P(T|F) is then decomposed as the product of the probabilities of all the functionally annotated CFG rewriting rules X → Y (conditioned on the left hand side (LHS) X and local features of the corresponding f-structure O(X)) contributing to the tree T (Eq. 2). The first line (PCFG) of Table 1 shows the f-structure annotated CFG rule to expand node n3 in Figure 1. phenomena. Methodologies such as lexicalisation (Collins, 1997; Charniak, 2000) and tree transformations (Johnson, 1998), weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple PCFGs. In this section we study the effect of such methods in LFG-based generation for Chinese. 3.1 A History-Based Model The history-based (HB) approach which incorporates more context information has worked well in parsing (Collins, 1997; Charniak, 2000). Resembling history-based models for parsing, Hogan et al. (2007) presented a history-based generation model to overcome some of the inappropriate independence assumptions in the basic generation model of (Cahill</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24(4): 613–632. MIT Press, Cambridge, MA,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
</authors>
<title>The formal architecture of lexical-functional grammar. Formal Issues in LexicalFunctional Grammar,</title>
<date>1995</date>
<pages>7--27</pages>
<publisher>CSLI Publications,</publisher>
<location>Standford, USA.</location>
<contexts>
<context position="5986" citStr="Kaplan, 1995" startWordPosition="939" endWordPosition="941">resnan, 1982) is a constraint-based grammar formalism which postulates (minimally) two levels of representation: c(onstituent)-structure and f(unctional)- structure. C-structure takes the form of phrase structure trees and captures surface grammatical configurations. F-structure encodes more abstract grammatical functions (GFs) such as SUBJ(ect), OBJ(ect), ADJUNCT and TOPIC etc., in the form of hierarchical attribute-value matrices. C-structures and f-structures are related by a piecewise correspondence function 0 that goes from the nodes of a cstructure tree into units of f-structure spaces (Kaplan, 1995). As illustrated in Figure 1, given a c-structure node nz, the corresponding f-structure component fj is 0(nz). Admissible c-structures are specified by a context-free grammar. The corresponding f-structures are derived from functional annotations attached to the CFG rewriting rules. (1) [↑=↓] (1) shows a miniature set of annotated CFG rules (lexical entries omitted) which generates the c- and f-structure in Figure 1. In the functional annotations, (↓) refers to the f-structure associated with the local c-structure node nz, i.e. 0(nz), and (↑) refers to the IP −→ NP VP [↑SUBJ=↓] [↑=↓] VP −→ VV</context>
</contexts>
<marker>Kaplan, 1995</marker>
<rawString>Ronald M. Kaplan. 1995. The formal architecture of lexical-functional grammar. Formal Issues in LexicalFunctional Grammar, pages 7–27. CSLI Publications, Standford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical Functional Grammar: a Formal System for Grammatical Representation. The Mental Representation of Grammatical Relations,</title>
<date>1982</date>
<pages>173--282</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5386" citStr="Kaplan and Bresnan, 1982" startWordPosition="850" endWordPosition="854">rage LFG grammars automatically extracted from the Penn Chinese Treebank (CTB). Our work is couched in the framework of Lexical Functional Grammar and is implemented in a chart-style generator. We briefly describe LFG and the basic generation model in Section 2. We improve the baseline PCFG model by weakening the independence assumptions in two disambiguation models in Section 3. Section 4 describes the smoothing algorithms adopted for the chart generator and Section 5 gives the experimental details and results. 2 LFG-Based Generation 2.1 Lexical Functional Grammar Lexical Functional Grammar (Kaplan and Bresnan, 1982) is a constraint-based grammar formalism which postulates (minimally) two levels of representation: c(onstituent)-structure and f(unctional)- structure. C-structure takes the form of phrase structure trees and captures surface grammatical configurations. F-structure encodes more abstract grammatical functions (GFs) such as SUBJ(ect), OBJ(ect), ADJUNCT and TOPIC etc., in the form of hierarchical attribute-value matrices. C-structures and f-structures are related by a piecewise correspondence function 0 that goes from the nodes of a cstructure tree into units of f-structure spaces (Kaplan, 1995)</context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1982. Lexical Functional Grammar: a Formal System for Grammatical Representation. The Mental Representation of Grammatical Relations, pages 173-282. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Jurgen Wedekind</author>
</authors>
<title>LFG Generation Produces Context-free Languages.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>425--431</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="7271" citStr="Kaplan and Wedekind (2000)" startWordPosition="1133" endWordPosition="1136">odel Grammar Rule Conditions PCFG VP[T=].] → VV[T=].] NP[TOBJ=].] VP[T=].], {PRED, SUBJ, OBJ} HB-PCFG VP[T=].] → VV[T=].] NP[TOBJ=].] VP[T=].], {PRED, SUBJ, OBJ}, TOP LEX-PCFG VP(��)[T=].] → VV(7�-N-;JFL)[T=].] NP(,O,tV)[TOBJ=].] VP(7�-N-;JFL)[T=].], {PRED, SUBJ, OBJ} Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models f-structure associated with the mother (M) node of ni, i.e. O(M(ni)). 2.2 Generation from f-Structures The generation task in LFG is to determine which sentences correspond to a specified f-structure, given a particular grammar, such as (1). Kaplan and Wedekind (2000) proved that the set of strings generated by an LFG grammar from fully specified f-structures is a context-free language. Based on this theoretical cornerstone, Cahill and van Genabith (2006) presented a PCFG-based chart generator using wide-coverage LFG approximations automatically extracted from the Penn-II treebank. The LFG-based statistical generation model defines the conditional probability P(T |F), for each candidate functionally annotated c-structure tree T (which fully specifies a surface realisation) given an fstructure F. The generation model searches for the Tbest that maximises P(</context>
</contexts>
<marker>Kaplan, Wedekind, 2000</marker>
<rawString>Ronald M. Kaplan and Jurgen Wedekind. 2000. LFG Generation Produces Context-free Languages. Proceedings of the 18th International Conference on Computational Linguistics, pages 425–431. Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart Generation.</title>
<date>1996</date>
<booktitle>Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>200--204</pages>
<location>Santa Cruz, USA.</location>
<contexts>
<context position="13608" citStr="Kay, 1996" startWordPosition="2183" endWordPosition="2184">sal categories in Chinese.3 In lexicalised model, the head word immediately projects to the top level in a left-binary tree, and as a result, the intermediate NP nodes cannot be lexicalised with the head word as illustrated in Figure (2b). By contrast, right-binary rules are lexicalised and the head word is percolated from the bottom of the tree (Figure (2c)). Therefore we adopt the right binarisation method in our generation algorithm. 4 Chart Generation and Smoothing Algorithms 4.1 Chart Generation Algorithm The PCFG-based generation algorithms are implemented in terms of a chart generator (Kay, 1996). In the generation algorithm, each (sub-)f-structure indexes a (sub-)chart. Each local chart generates the most probable trees for the local f-structure in a bottom-up manner: • generating lexical edges from the the local GF PRED and some atomic features representing function words, mood or aspect etc. 3Except for prepositional phrases, localiser and some verbal phrases. 89 NP() NN [T=1] � cup NR [1ETADJUNCT] f Shanghai NN [1ETADJUNCT] KA�* tennis NN [1ETADJUNCT] )�&apos;Ji � masters (a.) the original tree Figure 2: Lexicalised binary trees NP() NP() (b.) left-binarisation (c.) right-binarisation </context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart Generation. Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 200–204. Santa Cruz, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-Based Statistical Sentence Generation.</title>
<date>2000</date>
<booktitle>Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>170--177</pages>
<location>Seattle, WA.</location>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-Based Statistical Sentence Generation. Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 170–177. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator.</title>
<date>2002</date>
<booktitle>Proceedings of the Second International Conference on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<location>New York, USA.</location>
<contexts>
<context position="3089" citStr="Langkilde (2002)" startWordPosition="441" endWordPosition="442">k, Cahill and van Genabith (2006) automatically extracted wide-coverage LFG approximations for a PCFG-based generation model. In addition to applying statistical techniques to automatically acquire generation grammars, over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realisation. The paradigm is characterised by a separation between generation and selection, in which symbolic or rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select one or more outputs from the space. Starting from Langkilde (2002) who used a n-gram language model to rank generated output strings, a substantial number of traditional handcrafted surface realisers have been augmented with sophisticated stochastic rankers (Velldal and Oepen, 2005; White et al., 2007; Cahill et al., 2007). It is interesting to note that, while the study of how the granularity of context-free grammars (CFG) affects the performance of a parser (e.g. in the form 86 n2:NP [↑SUBJ=↓] n4:NR [↑=↓] M*K JiangZemin n5:VV [↑=↓] rJni. interview n1:IP [↑=↓] n7:NR [↓∈↑ADJUNCT] pj_n Thai n3:VP [↑=↓] n6:NP [↑OBJ=↓] n8:NN [↑=↓] ,01It president f1 � � � � � �</context>
<context position="19057" citStr="Langkilde, 2002" startWordPosition="3082" endWordPosition="3083"> words, maximal length 80 words, and average length 28.84 words. The development set also includes 500 sentences randomly selected from the development files with sentence length between 5 and 80 words. The c-structure trees of the test and development data were also automatically converted to f-structures as input to the generator. Type with features without features PCFG 22,372 8,548 HB-PCFG 28,487 11,969 LEX-PCFG 325,094 286,468 Table 3: Number of rules in the training set The generation system is evaluated against the raw text of the test data in terms of accuracy and coverage. Following (Langkilde, 2002) and other work on general-purpose generators, we adopt BLEU score (Papineni et al., 2002), average simple string accuracy (SSA) and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence. Table 4 reports the initial experiments on the simple PCFG, HB-based PCFG and lexicalised PCFG models. The results in the left column evaluate all input f-structures, the right column evaluate only those f-structures which yield a complete sentence. The results show that the lexicalised model outperfor</context>
</contexts>
<marker>Langkilde, 2002</marker>
<rawString>Langkilde, Irene. 2002. An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator. Proceedings of the Second International Conference on Natural Language Generation, 17–24. New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Nakanishi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic Models for Disambiguation of an HPSG-Based Chart Generator.</title>
<date>2005</date>
<booktitle>Proceedings of the 9th International Workshop on Parsing Technology,</booktitle>
<pages>93--102</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="2344" citStr="Nakanishi et al. (2005)" startWordPosition="328" endWordPosition="331">c representations. Traditionally, grammar rules have been carefully handcrafted, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007). As handcrafting grammar rules is time-consuming, language-dependent and domain-specific, recent years have witnessed research on extracting wide-coverage grammars automatically from annotated corpora, for both parsing and generation. FERGUS (Bangalore and Rambow, 2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. Nakanishi et al. (2005) presented log-linear models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank. From the same treebank, Cahill and van Genabith (2006) automatically extracted wide-coverage LFG approximations for a PCFG-based generation model. In addition to applying statistical techniques to automatically acquire generation grammars, over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realisation. The paradigm is characterised by a separation between generation and selection, in which symbolic or rule-based methods are used to genera</context>
</contexts>
<marker>Nakanishi, Nakanishi, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi and Yusuke Nakanishi and Jun’ichi Tsujii. 2005. Probabilistic Models for Disambiguation of an HPSG-Based Chart Generator. Proceedings of the 9th International Workshop on Parsing Technology, pages 93–102. Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="19147" citStr="Papineni et al., 2002" startWordPosition="3094" endWordPosition="3097"> also includes 500 sentences randomly selected from the development files with sentence length between 5 and 80 words. The c-structure trees of the test and development data were also automatically converted to f-structures as input to the generator. Type with features without features PCFG 22,372 8,548 HB-PCFG 28,487 11,969 LEX-PCFG 325,094 286,468 Table 3: Number of rules in the training set The generation system is evaluated against the raw text of the test data in terms of accuracy and coverage. Following (Langkilde, 2002) and other work on general-purpose generators, we adopt BLEU score (Papineni et al., 2002), average simple string accuracy (SSA) and percentage of exactly matched sentences for accuracy evaluation.6 For coverage evaluation, we measure the percentage of input fstructures that generate a sentence. Table 4 reports the initial experiments on the simple PCFG, HB-based PCFG and lexicalised PCFG models. The results in the left column evaluate all input f-structures, the right column evaluate only those f-structures which yield a complete sentence. The results show that the lexicalised model outperforms the baseline PCFG model. The HB model is the most accurate for complete sentences, but </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318. Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum entropy models for realization ranking.</title>
<date>2005</date>
<booktitle>Proceedings of the MTSummit ’05.</booktitle>
<contexts>
<context position="3305" citStr="Velldal and Oepen, 2005" startWordPosition="471" endWordPosition="474">grammars, over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realisation. The paradigm is characterised by a separation between generation and selection, in which symbolic or rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select one or more outputs from the space. Starting from Langkilde (2002) who used a n-gram language model to rank generated output strings, a substantial number of traditional handcrafted surface realisers have been augmented with sophisticated stochastic rankers (Velldal and Oepen, 2005; White et al., 2007; Cahill et al., 2007). It is interesting to note that, while the study of how the granularity of context-free grammars (CFG) affects the performance of a parser (e.g. in the form 86 n2:NP [↑SUBJ=↓] n4:NR [↑=↓] M*K JiangZemin n5:VV [↑=↓] rJni. interview n1:IP [↑=↓] n7:NR [↓∈↑ADJUNCT] pj_n Thai n3:VP [↑=↓] n6:NP [↑OBJ=↓] n8:NN [↑=↓] ,01It president f1 � � � � � � � � � � � � � � � � � � � � ��� � � � ��� � � � � � � � � �� �� � � �PRED ‘��’ � � � �PRED ‘���’ � � � � �SUBJ f2LNTYPE proper � � �NUM sg � � � ‘��’ �PRED � � �NTYPE common � � � � � �NUM � �sg � � OBJ f3 � � �PRED</context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>Erik Velldal and Stephan Oepen. 2005. Maximum entropy models for realization ranking. Proceedings of the MTSummit ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG Chart Realization.</title>
<date>2004</date>
<booktitle>Proceedings of the third International Natural Language Generation Conference.</booktitle>
<publisher>Hampshire, UK.</publisher>
<contexts>
<context position="1875" citStr="White, 2004" startWordPosition="264" endWordPosition="265">bed as the problem of producing syntactically, morphologically, and orthographically correct sentences from a given abstract semantic / logical representation according to some linguistic theory, e.g. Lexical Functional Grammar (LFG), Head-Driven Phrase Structure Grammar (HPSG), Combinatory Categorial Grammar (CCG), Tree Adjoining Grammar (TAG) etc. Grammars, such as these, are declarative formulations of the correspondences between semantic and syntactic representations. Traditionally, grammar rules have been carefully handcrafted, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007). As handcrafting grammar rules is time-consuming, language-dependent and domain-specific, recent years have witnessed research on extracting wide-coverage grammars automatically from annotated corpora, for both parsing and generation. FERGUS (Bangalore and Rambow, 2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. Nakanishi et al. (2005) presented log-linear models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank. From the same treebank,</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG Chart Realization. Proceedings of the third International Natural Language Generation Conference. Hampshire, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards Broad Coverage Surface Realization with CCG.</title>
<date>2007</date>
<booktitle>Proceedings of the MT Summit XI Workshop on Language Generation and Machine Translation,</booktitle>
<pages>22--30</pages>
<location>Copenhagen, Danmark.</location>
<contexts>
<context position="3325" citStr="White et al., 2007" startWordPosition="475" endWordPosition="478">ecade, there has been a lot of interest in a generate-and-select paradigm for surface realisation. The paradigm is characterised by a separation between generation and selection, in which symbolic or rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select one or more outputs from the space. Starting from Langkilde (2002) who used a n-gram language model to rank generated output strings, a substantial number of traditional handcrafted surface realisers have been augmented with sophisticated stochastic rankers (Velldal and Oepen, 2005; White et al., 2007; Cahill et al., 2007). It is interesting to note that, while the study of how the granularity of context-free grammars (CFG) affects the performance of a parser (e.g. in the form 86 n2:NP [↑SUBJ=↓] n4:NR [↑=↓] M*K JiangZemin n5:VV [↑=↓] rJni. interview n1:IP [↑=↓] n7:NR [↓∈↑ADJUNCT] pj_n Thai n3:VP [↑=↓] n6:NP [↑OBJ=↓] n8:NN [↑=↓] ,01It president f1 � � � � � � � � � � � � � � � � � � � � ��� � � � ��� � � � � � � � � �� �� � � �PRED ‘��’ � � � �PRED ‘���’ � � � � �SUBJ f2LNTYPE proper � � �NUM sg � � � ‘��’ �PRED � � �NTYPE common � � � � � �NUM � �sg � � OBJ f3 � � �PRED ‘��’ � � �� � � � �</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White and Rajakrishnan Rajkumar and Scott Martin. 2007. Towards Broad Coverage Surface Realization with CCG. Proceedings of the MT Summit XI Workshop on Language Generation and Machine Translation, pages 22–30. Copenhagen, Danmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<pages>207--238</pages>
<contexts>
<context position="17813" citStr="Xue et al., 2005" startWordPosition="2876" endWordPosition="2879">e above example rule (4), the importance of the elements is: IP &gt; VP &gt; [TSUBJ=t] &gt; NP &gt; [TCOMP=t] &gt; [T=t] The elements can be deleted from the rules in an importance order from low to high.5 The partial rules adopted in our system ignore the least important 3 elements, viz. the functional annotation of the head node H(a), the functional annotation on LHS X(a) and constituent category of the modifier node Y (c). Examples of the two types of smoothed rules are shown in Table 2. 5 Experimental Results Our experiments are carried out on the newly released Penn Chinese treebank version 6.0 (CTB6) (Xue et al., 2005), excluding the portion of ACE broadcast news. We follow the recommended splits (in the list-of-file of CTB6) to divide the data into test set, development set and training set. The training set includes 756 files with a total of 15,663 sentences. The CTB trees of the training set were automatically annotated with LFG f-structure equations following (Guo et al., 2007). Table 3 shows the number of different grammar rule types extracted from the training set. From the test files, 5However c and a on the same node can’t be deleted at the same time. we randomly select 500 sentences as test data wi</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue and Fei Xia and Fu dong Chiou and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2): 207–238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>