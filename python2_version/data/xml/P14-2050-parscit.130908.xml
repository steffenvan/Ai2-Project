<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003713">
<title confidence="0.965757">
Dependency-Based Word Embeddings
</title>
<author confidence="0.957029">
Omer Levy∗ and Yoav Goldberg
</author>
<affiliation confidence="0.974975">
Computer Science Department
Bar-Ilan University
</affiliation>
<address confidence="0.738782">
Ramat-Gan, Israel
</address>
<email confidence="0.997234">
{omerlevy,yoav.goldberg}@gmail.com
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998941230769231">
While continuous word embeddings are
gaining popularity, current models are
based solely on linear contexts. In this
work, we generalize the skip-gram model
with negative sampling introduced by
Mikolov et al. to include arbitrary con-
texts. In particular, we perform exper-
iments with dependency-based contexts,
and show that they produce markedly
different embeddings. The dependency-
based embeddings are less topical and ex-
hibit more functional similarity than the
original skip-gram embeddings.
</bodyText>
<sectionHeader confidence="0.999397" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998764421875">
Word representation is central to natural language
processing. The default approach of represent-
ing words as discrete and distinct symbols is in-
sufficient for many tasks, and suffers from poor
generalization. For example, the symbolic repre-
sentation of the words “pizza” and “hamburger”
are completely unrelated: even if we know that
the word “pizza” is a good argument for the verb
“eat”, we cannot infer that “hamburger” is also
a good argument. We thus seek a representation
that captures semantic and syntactic similarities
between words. A very common paradigm for ac-
quiring such representations is based on the distri-
butional hypothesis of Harris (1954), stating that
words in similar contexts have similar meanings.
Based on the distributional hypothesis, many
methods of deriving word representations were ex-
plored in the NLP community. On one end of the
spectrum, words are grouped into clusters based
on their contexts (Brown et al., 1992; Uszkor-
eit and Brants, 2008). On the other end, words
∗ Supported by the European Community’s Seventh
Framework Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
are represented as a very high dimensional but
sparse vectors in which each entry is a measure
of the association between the word and a particu-
lar context (see (Turney and Pantel, 2010; Baroni
and Lenci, 2010) for a comprehensive survey).
In some works, the dimensionality of the sparse
word-context vectors is reduced, using techniques
such as SVD (Bullinaria and Levy, 2007) or LDA
(Ritter et al., 2010; S´eaghdha, 2010; Cohen et
al., 2012). Most recently, it has been proposed
to represent words as dense vectors that are de-
rived by various training methods inspired from
neural-network language modeling (Bengio et al.,
2003; Collobert and Weston, 2008; Mnih and
Hinton, 2008; Mikolov et al., 2011; Mikolov et
al., 2013b). These representations, referred to as
“neural embeddings” or “word embeddings”, have
been shown to perform well across a variety of
tasks (Turian et al., 2010; Collobert et al., 2011;
Socher et al., 2011; Al-Rfou et al., 2013).
Word embeddings are easy to work with be-
cause they enable efficient computation of word
similarities through low-dimensional matrix op-
erations. Among the state-of-the-art word-
embedding methods is the skip-gram with nega-
tive sampling model (SKIPGRAM), introduced by
Mikolov et al. (2013b) and implemented in the
word2vec software.1 Not only does it produce
useful word representations, but it is also very ef-
ficient to train, works in an online fashion, and
scales well to huge copora (billions of words) as
well as very large word and context vocabularies.
Previous work on neural word embeddings take
the contexts of a word to be its linear context –
words that precede and follow the target word, typ-
ically in a window of k tokens to each side. How-
ever, other types of contexts can be explored too.
In this work, we generalize the SKIP-
GRAM model, and move from linear bag-of-words
contexts to arbitrary word contexts. Specifically,
</bodyText>
<footnote confidence="0.52283">
1code.google.com/p/word2vec/
</footnote>
<page confidence="0.984936">
302
</page>
<bodyText confidence="0.987399172413793">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
following work in sparse vector-space models
(Lin, 1998; Pad´o and Lapata, 2007; Baroni and
Lenci, 2010), we experiment with syntactic con-
texts that are derived from automatically produced
dependency parse-trees.
The different kinds of contexts produce no-
ticeably different embeddings, and induce differ-
ent word similarities. In particular, the bag-of-
words nature of the contexts in the “original”
SKIPGRAM model yield broad topical similari-
ties, while the dependency-based contexts yield
more functional similarities of a cohyponym na-
ture. This effect is demonstrated using both quali-
tative and quantitative analysis (Section 4).
The neural word-embeddings are considered
opaque, in the sense that it is hard to assign mean-
ings to the dimensions of the induced represen-
tation. In Section 5 we show that the SKIP-
GRAM model does allow for some introspection
by querying it for contexts that are “activated by” a
target word. This allows us to peek into the learned
representation and explore the contexts that are
found by the learning process to be most discrim-
inative of particular words (or groups of words).
To the best of our knowledge, this is the first work
to suggest such an analysis of discriminatively-
trained word-embedding models.
</bodyText>
<sectionHeader confidence="0.98314" genericHeader="method">
2 The Skip-Gram Model
</sectionHeader>
<bodyText confidence="0.991277607142857">
Our departure point is the skip-gram neural em-
bedding model introduced in (Mikolov et al.,
2013a) trained using the negative-sampling pro-
cedure presented in (Mikolov et al., 2013b). In
this section we summarize the model and train-
ing objective following the derivation presented by
Goldberg and Levy (2014), and highlight the ease
of incorporating arbitrary contexts in the model.
In the skip-gram model, each word w E W is
associated with a vector vw E Rd and similarly
each context c E C is represented as a vector
vc E Rd, where W is the words vocabulary, C
is the contexts vocabulary, and d is the embed-
ding dimensionality. The entries in the vectors
are latent, and treated as parameters to be learned.
Loosely speaking, we seek parameter values (that
is, vector representations for both words and con-
texts) such that the dot product vw · vc associated
with “good” word-context pairs is maximized.
More specifically, the negative-sampling objec-
tive assumes a dataset D of observed (w, c) pairs
of words w and the contexts c, which appeared in
a large body of text. Consider a word-context pair
(w, c). Did this pair come from the data? We de-
note by p(D = 1|w, c) the probability that (w, c)
came from the data, and by p(D = 0|w, c) =
1 − p(D = 1|w, c) the probability that (w, c) did
not. The distribution is modeled as:
</bodyText>
<equation confidence="0.9987235">
1
p(D = 1|w, c) = 1+e−vw·vc
</equation>
<bodyText confidence="0.99997875">
where vw and vc (each a d-dimensional vector) are
the model parameters to be learned. We seek to
maximize the log-probability of the observed pairs
belonging to the data, leading to the objective:
</bodyText>
<equation confidence="0.948998857142857">
E
arg maxvw,vc (w,c)ED log 1
1+e−vc·vw
This objective admits a trivial solution in which
p(D = 1|w, c) = 1 for every pair (w, c). This can
be easily achieved by setting vc = vw and vc·vw =
K for all c, w, where K is large enough number.
</equation>
<bodyText confidence="0.97908672">
In order to prevent the trivial solution, the ob-
jective is extended with (w, c) pairs for which
p(D = 1|w, c) must be low, i.e. pairs which are
not in the data, by generating the set D&apos; of ran-
dom (w, c) pairs (assuming they are all incorrect),
yielding the negative-sampling training objective:
arg maxvw,vc (Il(w,c)ED p(D = 1|c, w) Il(w,c)ED0 p(D = 0|c, w))
which can be rewritten as:
arg maxvw,vc (E (w,c) ED log a(vc · vw) + E (w,c) ED0 log a(−vc · vw) )
where a(x) = 1/(1+ex). The objective is trained
in an online fashion using stochastic-gradient up-
dates over the corpus D U D&apos;.
The negative samples D&apos; can be constructed in
various ways. We follow the method proposed by
Mikolov et al.: for each (w, c) E D we construct
n samples (w, c1), ... , (w, cn), where n is a hy-
perparameter and each cj is drawn according to its
unigram distribution raised to the 3/4 power.
Optimizing this objective makes observed
word-context pairs have similar embeddings,
while scattering unobserved pairs. Intuitively,
words that appear in similar contexts should have
similar embeddings, though we have not yet found
a formal proof that SKIPGRAM does indeed max-
imize the dot product of similar words.
</bodyText>
<sectionHeader confidence="0.93069" genericHeader="method">
3 Embedding with Arbitrary Contexts
</sectionHeader>
<bodyText confidence="0.9979515">
In the SKIPGRAM embedding algorithm, the con-
texts of a word w are the words surrounding it
</bodyText>
<page confidence="0.996476">
303
</page>
<bodyText confidence="0.980615907407408">
in the text. The context vocabulary C is thus
identical to the word vocabulary W. However,
this restriction is not required by the model; con-
texts need not correspond to words, and the num-
ber of context-types can be substantially larger
than the number of word-types. We generalize
SKIPGRAM by replacing the bag-of-words con-
texts with arbitrary contexts.
In this paper we experiment with dependency-
based syntactic contexts. Syntactic contexts cap-
ture different information than bag-of-word con-
texts, as we demonstrate using the sentence “Aus-
tralian scientist discovers star with telescope”.
Linear Bag-of-Words Contexts This is the
context used by word2vec and many other neu-
ral embeddings. Using a window of size k around
the target word w, 2k contexts are produced: the
k words before and the k words after w. For
k = 2, the contexts of the target word w are
w−2, w−1, w+1, w+2. In our example, the contexts
of discovers are Australian, scientist, star, with.2
Note that a context window of size 2 may miss
some important contexts (telescope is not a con-
text of discovers), while including some acciden-
tal ones (Australian is a context discovers). More-
over, the contexts are unmarked, resulting in dis-
covers being a context of both stars and scientist,
which may result in stars and scientists ending
up as neighbours in the embedded space. A win-
dow size of 5 is commonly used to capture broad
topical content, whereas smaller windows contain
more focused information about the target word.
Dependency-Based Contexts An alternative to
the bag-of-words approach is to derive contexts
based on the syntactic relations the word partic-
ipates in. This is facilitated by recent advances
in parsing technology (Goldberg and Nivre, 2012;
Goldberg and Nivre, 2013) that allow parsing to
syntactic dependencies with very high speed and
near state-of-the-art accuracy.
After parsing each sentence, we derive word
contexts as follows: for a target word w with
modifiers m1, ... , mk and a head h, we consider
the contexts (m1, lbl1), ... , (mk, lblk), (h, lbl−1
h ),
2word2vec’s implementation is slightly more compli-
cated. The software defaults to prune rare words based on
their frequency, and has an option for sub-sampling the fre-
quent words. These pruning and sub-sampling happen before
the context extraction, leading to a dynamic window size. In
addition, the window size is not fixed to k but is sampled
uniformly in the range [1, k] for each word.
Australian scientist discovers star with telescope
Australian scientist discovers star telescope
</bodyText>
<figure confidence="0.735933333333333">
WORD CONTEXTS
australian scientist/amod−1
scientist australian/amod, discovers/nsubj−1
discovers scientist/nsubj, star/dobj, telescope/prep with
star discovers/dobj−1
telescope discovers/prep with−1
</figure>
<figureCaption confidence="0.9163095">
Figure 1: Dependency-based context extraction example.
Top: preposition relations are collapsed into single arcs,
making telescope a direct modifier of discovers. Bottom: the
contexts extracted for each word in the sentence.
</figureCaption>
<bodyText confidence="0.999993304347826">
where lbl is the type of the dependency relation be-
tween the head and the modifier (e.g. nsubj, dobj,
prep with, amod) and lbl−1 is used to mark the
inverse-relation. Relations that include a preposi-
tion are “collapsed” prior to context extraction, by
directly connecting the head and the object of the
preposition, and subsuming the preposition itself
into the dependency label. An example of the de-
pendency context extraction is given in Figure 1.
Notice that syntactic dependencies are both
more inclusive and more focused than bag-of-
words. They capture relations to words that are
far apart and thus “out-of-reach” with small win-
dow bag-of-words (e.g. the instrument of discover
is telescope/prep with), and also filter out “coinci-
dental” contexts which are within the window but
not directly related to the target word (e.g. Aus-
tralian is not used as the context for discovers). In
addition, the contexts are typed, indicating, for ex-
ample, that stars are objects of discovery and sci-
entists are subjects. We thus expect the syntactic
contexts to yield more focused embeddings, cap-
turing more functional and less topical similarity.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="method">
4 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999959">
We experiment with 3 training conditions: BOW5
(bag-of-words contexts with k = 5), BOW2
(same, with k = 2) and DEPS (dependency-based
syntactic contexts). We modified word2vec to
support arbitrary contexts, and to output the con-
text embeddings in addition to the word embed-
dings. For bag-of-words contexts we used the
original word2vec implementation, and for syn-
tactic contexts, we used our modified version. The
negative-sampling parameter (how many negative
contexts to sample for every correct one) was 15.
</bodyText>
<figure confidence="0.834487">
prep with
amod nsubj dobj
prep
amod nsubj dobj pobj
</figure>
<page confidence="0.993729">
304
</page>
<bodyText confidence="0.999958785714286">
All embeddings were trained on English
Wikipedia. For DEPS, the corpus was tagged
with parts-of-speech using the Stanford tagger
(Toutanova et al., 2003) and parsed into labeled
Stanford dependencies (de Marneffe and Man-
ning, 2008) using an implementation of the parser
described in (Goldberg and Nivre, 2012). All to-
kens were converted to lowercase, and words and
contexts that appeared less than 100 times were
filtered. This resulted in a vocabulary of about
175,000 words, with over 900,000 distinct syntac-
tic contexts. We report results for 300 dimension
embeddings, though similar trends were also ob-
served with 600 dimensions.
</bodyText>
<subsectionHeader confidence="0.989917">
4.1 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999963032258065">
Our first evaluation is qualitative: we manually in-
spect the 5 most similar words (by cosine similar-
ity) to a given set of target words (Table 1).
The first target word, Batman, results in similar
sets across the different setups. This is the case for
many target words. However, other target words
show clear differences between embeddings.
In Hogwarts - the school of magic from the
fictional Harry Potter series - it is evident that
BOW contexts reflect the domain aspect, whereas
DEPS yield a list of famous schools, capturing
the semantic type of the target word. This ob-
servation holds for Turing3 and many other nouns
as well; BOW find words that associate with w,
while DEPS find words that behave like w. Turney
(2012) described this distinction as domain simi-
larity versus functional similarity.
The Florida example presents an ontologi-
cal difference; bag-of-words contexts generate
meronyms (counties or cities within Florida),
while dependency-based contexts provide cohy-
ponyms (other US states). We observed the same
behavior with other geographical locations, partic-
ularly with countries (though not all of them).
The next two examples demonstrate that simi-
larities induced from DEPS share a syntactic func-
tion (adjectives and gerunds), while similarities
based on BOW are more diverse. Finally, we ob-
serve that while both BOW5 and BOW2 yield top-
ical similarities, the larger window size result in
more topicality, as expected.
</bodyText>
<footnote confidence="0.9962126">
3DEPS generated a list of scientists whose name ends with
“ing”. This is may be a result of occasional POS-tagging
errors. Still, the embedding does a remarkable job and re-
trieves scientists, despite the noisy POS. The list contains
more mathematicians without “ing” further down.
</footnote>
<figureCaption confidence="0.993372903225806">
Target Word BOW5 BOW2 DEPS
batman nightwing superman superman
aquaman superboy superboy
catwoman aquaman supergirl
superman catwoman catwoman
manhunter batgirl aquaman
hogwarts dumbledore evernight sunnydale
hallows sunnydale collinwood
half-blood garderobe calarts
malfoy blandings greendale
snape collinwood millfield
turing nondeterministic non-deterministic pauling
non-deterministic finite-state hotelling
computability nondeterministic heting
deterministic buchi lessing
finite-state primality hamming
florida gainesville fla texas
fla alabama louisiana
jacksonville gainesville georgia
tampa tallahassee california
lauderdale texas carolina
object-oriented aspect-oriented aspect-oriented event-driven
smalltalk event-driven domain-specific
event-driven objective-c rule-based
prolog dataflow data-driven
domain-specific 4gl human-centered
dancing singing singing singing
dance dance rapping
dances dances breakdancing
dancers breakdancing miming
tap-dancing clowning busking
</figureCaption>
<tableCaption confidence="0.98718">
Table 1: Target words and their 5 most similar words, as in-
duced by different embeddings.
</tableCaption>
<bodyText confidence="0.999803833333333">
We also tried using the subsampling option
(Mikolov et al., 2013b) with BOW contexts (not
shown). Since word2vec removes the subsam-
pled words from the corpus before creating the
window contexts, this option effectively increases
the window size, resulting in greater topicality.
</bodyText>
<subsectionHeader confidence="0.994645">
4.2 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999984111111111">
We supplement the examples in Table 1 with
quantitative evaluation to show that the qualita-
tive differences pointed out in the previous sec-
tion are indeed widespread. To that end, we use
the WordSim353 dataset (Finkelstein et al., 2002;
Agirre et al., 2009). This dataset contains pairs of
similar words that reflect either relatedness (top-
ical similarity) or similarity (functional similar-
ity) relations.4 We use the embeddings in a re-
trieval/ranking setup, where the task is to rank the
similar pairs in the dataset above the related ones.
The pairs are ranked according to cosine sim-
ilarities between the embedded words. We then
draw a recall-precision curve that describes the
embedding’s affinity towards one subset (“sim-
ilarity”) over another (“relatedness”). We ex-
pect DEPS’s curve to be higher than BOW2’s
curve, which in turn is expected to be higher than
</bodyText>
<footnote confidence="0.4969915">
4Some word pairs are judged to exhibit both types of sim-
ilarity, and were ignored in this experiment.
</footnote>
<page confidence="0.998359">
305
</page>
<figureCaption confidence="0.996521666666667">
Figure 2: Recall-precision curve when attempting to rank the
similar words above the related ones. (a) is based on the
WordSim353 dataset, and (b) on the Chiarello et al. dataset.
</figureCaption>
<bodyText confidence="0.999703888888889">
BOW5’s. The graph in Figure 2a shows this is in-
deed the case. We repeated the experiment with a
different dataset (Chiarello et al., 1990) that was
used by Turney (2012) to distinguish between do-
main and functional similarities. The results show
a similar trend (Figure 2b). When reversing the
task such that the goal is to rank the related terms
above the similar ones, the results are reversed, as
expected (not shown).5
</bodyText>
<sectionHeader confidence="0.97784" genericHeader="method">
5 Model Introspection
</sectionHeader>
<bodyText confidence="0.999962125">
Neural word embeddings are often considered
opaque and uninterpretable, unlike sparse vec-
tor space representations in which each dimen-
sion corresponds to a particular known context, or
LDA models where dimensions correspond to la-
tent topics. While this is true to a large extent, we
observe that SKIPGRAM does allow a non-trivial
amount of introspection. Although we cannot as-
sign a meaning to any particular dimension, we
can indeed get a glimpse at the kind of informa-
tion being captured by the model, by examining
which contexts are “activated” by a target word.
Recall that the learning procedure is attempting
to maximize the dot product v, ·v,,, for good (w, c)
pairs and minimize it for bad ones. If we keep the
context embeddings, we can query the model for
the contexts that are most activated by (have the
highest dot product with) a given target word. By
doing so, we can see what the model learned to be
a good discriminative context for the word.
To demonstrate, we list the 5 most activated
contexts for our example words with DEPS em-
beddings in Table 2. Interestingly, the most dis-
criminative syntactic contexts in these cases are
</bodyText>
<footnote confidence="0.995405">
5Additional experiments (not presented in this paper) re-
inforce our conclusion. In particular, we found that DEPS
perform dramatically worse than BOW contexts on analogy
tasks as in (Mikolov et al., 2013c; Levy and Goldberg, 2014).
</footnote>
<table confidence="0.8470995">
batman hogwarts turing
superman/conj−1 students/prep at−1 machine/nn−1
spider-man/conj−1 educated/prep at−1 test/nn−1
superman/conj student/prep at−1 theorem/poss−1
spider-man/conj stay/prep at−1 machines/nn−1
robin/conj learned/prep at−1 tests/nn−1
florida object-oriented dancing
marlins/nn−1 programming/amod−1 dancing/conj
beach/appos−1 language/amod−1 dancing/conj−1
jacksonville/appos−1 framework/amod−1 singing/conj−1
tampa/appos−1 interface/amod−1 singing/conj
florida/conj−1 software/amod−1 ballroom/nn
</table>
<tableCaption confidence="0.9968">
Table 2: Words and their top syntactic contexts.
</tableCaption>
<bodyText confidence="0.999952944444444">
not associated with subjects or objects of verbs
(or their inverse), but rather with conjunctions, ap-
positions, noun-compounds and adjectivial modi-
fiers. Additionally, the collapsed preposition rela-
tion is very useful (e.g. for capturing the school
aspect of hogwarts). The presence of many con-
junction contexts, such as superman/conj for
batman and singing/conj for dancing, may
explain the functional similarity observed in Sec-
tion 4; conjunctions in natural language tend to en-
force their conjuncts to share the same semantic
types and inflections.
In the future, we hope that insights from such
model introspection will allow us to develop better
contexts, by focusing on conjunctions and prepo-
sitions for example, or by trying to figure out why
the subject and object relations are absent and
finding ways of increasing their contributions.
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99996145">
We presented a generalization of the SKIP-
GRAM embedding model in which the linear bag-
of-words contexts are replaced with arbitrary ones,
and experimented with dependency-based con-
texts, showing that they produce markedly differ-
ent kinds of similarities. These results are ex-
pected, and follow similar findings in the distri-
butional semantics literature. We also demon-
strated how the resulting embedding model can be
queried for the discriminative contexts for a given
word, and observed that the learning procedure
seems to favor relatively local syntactic contexts,
as well as conjunctions and objects of preposition.
We hope these insights will facilitate further re-
search into improved context modeling and better,
possibly task-specific, embedded representations.
Our software, allowing for experimentation with
arbitrary contexts, together with the embeddings
described in this paper, are available for download
at the authors’ websites.
</bodyText>
<page confidence="0.998411">
306
</page>
<sectionHeader confidence="0.996007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998467348214285">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19–27, Boulder, Colorado, June. Association
for Computational Linguistics.
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proc. of CoNLL 2013.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Peter F Brown, Robert L Mercer, Vincent J
Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural. Computational Linguis-
tics, 18(4).
John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510–526.
Christine Chiarello, Curt Burgess, Lorie Richards, and
Alma Pollock. 1990. Semantic and associative
priming in the cerebral hemispheres: Some words
do, some words don’t... sometimes, some places.
Brain and Language, 38(1):75–104.
Raphael Cohen, Yoav Goldberg, and Michael Elhadad.
2012. Domain adaptation of a dependency parser
with a class-class selectional preference model. In
Proceedings of ACL 2012 Student Research Work-
shop, pages 43–48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for the arc-eager system. In Proc. of COLING
2012.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
Omer Levy and Yoav Goldberg. 2014. Linguistic
regularities in sparse and explicit word representa-
tions. In Proceedings of the Eighteenth Conference
on Computational Natural Language Learning, Bal-
timore, Maryland, USA, June. Association for Com-
putational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ’98, pages
768–774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528–5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111–
3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.983732">
307
</page>
<reference confidence="0.993632170731708">
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
pages 1081–1088.
Sebastian Pad´o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In ACL, pages 424–434.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In ACL, pages 435–444.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
P.D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141–
188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Proc.
of ACL, pages 755–762.
</reference>
<page confidence="0.998545">
308
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.186507">
<title confidence="0.989491">Dependency-Based Word Embeddings</title>
<affiliation confidence="0.7733005">Computer Science Bar-Ilan</affiliation>
<address confidence="0.225681">Ramat-Gan,</address>
<abstract confidence="0.999438714285714">While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="17045" citStr="Agirre et al., 2009" startWordPosition="2696" endWordPosition="2699">most similar words, as induced by different embeddings. We also tried using the subsampling option (Mikolov et al., 2013b) with BOW contexts (not shown). Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality. 4.2 Quantitative Evaluation We supplement the examples in Table 1 with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread. To that end, we use the WordSim353 dataset (Finkelstein et al., 2002; Agirre et al., 2009). This dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations.4 We use the embeddings in a retrieval/ranking setup, where the task is to rank the similar pairs in the dataset above the related ones. The pairs are ranked according to cosine similarities between the embedded words. We then draw a recall-precision curve that describes the embedding’s affinity towards one subset (“similarity”) over another (“relatedness”). We expect DEPS’s curve to be higher than BOW2’s curve, which in turn is expected to be high</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Al-Rfou</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed word representations for multilingual nlp.</title>
<date>2013</date>
<booktitle>In Proc. of CoNLL</booktitle>
<contexts>
<context position="2770" citStr="Al-Rfou et al., 2013" startWordPosition="419" endWordPosition="422">ques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies. Previous work on neural word embeddings t</context>
</contexts>
<marker>Al-Rfou, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed word representations for multilingual nlp. In Proc. of CoNLL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="2026" citStr="Baroni and Lenci, 2010" startWordPosition="300" endWordPosition="303">. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, ha</context>
<context position="4054" citStr="Baroni and Lenci, 2010" startWordPosition="619" endWordPosition="622">hat precede and follow the target word, typically in a window of k tokens to each side. However, other types of contexts can be explored too. In this work, we generalize the SKIPGRAM model, and move from linear bag-of-words contexts to arbitrary word contexts. Specifically, 1code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics following work in sparse vector-space models (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees. The different kinds of contexts produce noticeably different embeddings, and induce different word similarities. In particular, the bag-ofwords nature of the contexts in the “original” SKIPGRAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature. This effect is demonstrated using both qualitative and quantitative analysis (Section 4). The neural word-embeddings are considered opaque, in the sense that it is</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2445" citStr="Bengio et al., 2003" startWordPosition="366" endWordPosition="369">ented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1630" citStr="Brown et al., 1992" startWordPosition="236" endWordPosition="239">hat the word “pizza” is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghd</context>
</contexts>
<marker>Brown, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural. Computational Linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="2193" citStr="Bullinaria and Levy, 2007" startWordPosition="325" endWordPosition="328">ouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are e</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39(3):510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Chiarello</author>
<author>Curt Burgess</author>
<author>Lorie Richards</author>
<author>Alma Pollock</author>
</authors>
<title>Semantic and associative priming in the cerebral hemispheres: Some words do, some words don’t... sometimes, some places.</title>
<date>1990</date>
<journal>Brain and Language,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="18077" citStr="Chiarello et al., 1990" startWordPosition="2867" endWordPosition="2870"> describes the embedding’s affinity towards one subset (“similarity”) over another (“relatedness”). We expect DEPS’s curve to be higher than BOW2’s curve, which in turn is expected to be higher than 4Some word pairs are judged to exhibit both types of similarity, and were ignored in this experiment. 305 Figure 2: Recall-precision curve when attempting to rank the similar words above the related ones. (a) is based on the WordSim353 dataset, and (b) on the Chiarello et al. dataset. BOW5’s. The graph in Figure 2a shows this is indeed the case. We repeated the experiment with a different dataset (Chiarello et al., 1990) that was used by Turney (2012) to distinguish between domain and functional similarities. The results show a similar trend (Figure 2b). When reversing the task such that the goal is to rank the related terms above the similar ones, the results are reversed, as expected (not shown).5 5 Model Introspection Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics. While this is true to a large extent, we observe th</context>
</contexts>
<marker>Chiarello, Burgess, Richards, Pollock, 1990</marker>
<rawString>Christine Chiarello, Curt Burgess, Lorie Richards, and Alma Pollock. 1990. Semantic and associative priming in the cerebral hemispheres: Some words do, some words don’t... sometimes, some places. Brain and Language, 38(1):75–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Cohen</author>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Domain adaptation of a dependency parser with a class-class selectional preference model.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012 Student Research Workshop,</booktitle>
<pages>43--48</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2259" citStr="Cohen et al., 2012" startWordPosition="337" endWordPosition="340">t and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word</context>
</contexts>
<marker>Cohen, Goldberg, Elhadad, 2012</marker>
<rawString>Raphael Cohen, Yoav Goldberg, and Michael Elhadad. 2012. Domain adaptation of a dependency parser with a class-class selectional preference model. In Proceedings of ACL 2012 Student Research Workshop, pages 43–48, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2473" citStr="Collobert and Weston, 2008" startWordPosition="370" endWordPosition="373">dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implement</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2726" citStr="Collobert et al., 2011" startWordPosition="411" endWordPosition="414">word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularie</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<location>Manchester, UK,</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="17023" citStr="Finkelstein et al., 2002" startWordPosition="2692" endWordPosition="2695"> Target words and their 5 most similar words, as induced by different embeddings. We also tried using the subsampling option (Mikolov et al., 2013b) with BOW contexts (not shown). Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality. 4.2 Quantitative Evaluation We supplement the examples in Table 1 with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread. To that end, we use the WordSim353 dataset (Finkelstein et al., 2002; Agirre et al., 2009). This dataset contains pairs of similar words that reflect either relatedness (topical similarity) or similarity (functional similarity) relations.4 We use the embeddings in a retrieval/ranking setup, where the task is to rank the similar pairs in the dataset above the related ones. The pairs are ranked according to cosine similarities between the embedded words. We then draw a recall-precision curve that describes the embedding’s affinity towards one subset (“similarity”) over another (“relatedness”). We expect DEPS’s curve to be higher than BOW2’s curve, which in turn </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="5520" citStr="Goldberg and Levy (2014)" startWordPosition="851" endWordPosition="854"> into the learned representation and explore the contexts that are found by the learning process to be most discriminative of particular words (or groups of words). To the best of our knowledge, this is the first work to suggest such an analysis of discriminativelytrained word-embedding models. 2 The Skip-Gram Model Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al., 2013a) trained using the negative-sampling procedure presented in (Mikolov et al., 2013b). In this section we summarize the model and training objective following the derivation presented by Goldberg and Levy (2014), and highlight the ease of incorporating arbitrary contexts in the model. In the skip-gram model, each word w E W is associated with a vector vw E Rd and similarly each context c E C is represented as a vector vc E Rd, where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality. The entries in the vectors are latent, and treated as parameters to be learned. Loosely speaking, we seek parameter values (that is, vector representations for both words and contexts) such that the dot product vw · vc associated with “good” word-context pairs is maximized. Mor</context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A dynamic oracle for the arc-eager system.</title>
<date>2012</date>
<booktitle>In Proc. of COLING</booktitle>
<contexts>
<context position="10052" citStr="Goldberg and Nivre, 2012" startWordPosition="1653" endWordPosition="1656">l ones (Australian is a context discovers). Moreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist, which may result in stars and scientists ending up as neighbours in the embedded space. A window size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word. Dependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This is facilitated by recent advances in parsing technology (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy. After parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m1, ... , mk and a head h, we consider the contexts (m1, lbl1), ... , (mk, lblk), (h, lbl−1 h ), 2word2vec’s implementation is slightly more complicated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic win</context>
<context position="13333" citStr="Goldberg and Nivre, 2012" startWordPosition="2160" endWordPosition="2163">tion to the word embeddings. For bag-of-words contexts we used the original word2vec implementation, and for syntactic contexts, we used our modified version. The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15. prep with amod nsubj dobj prep amod nsubj dobj pobj 304 All embeddings were trained on English Wikipedia. For DEPS, the corpus was tagged with parts-of-speech using the Stanford tagger (Toutanova et al., 2003) and parsed into labeled Stanford dependencies (de Marneffe and Manning, 2008) using an implementation of the parser described in (Goldberg and Nivre, 2012). All tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntactic contexts. We report results for 300 dimension embeddings, though similar trends were also observed with 600 dimensions. 4.1 Qualitative Evaluation Our first evaluation is qualitative: we manually inspect the 5 most similar words (by cosine similarity) to a given set of target words (Table 1). The first target word, Batman, results in similar sets across the different setups. This is the case </context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for the arc-eager system. In Proc. of COLING 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<journal>Transactions of the association for Computational Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="10079" citStr="Goldberg and Nivre, 2013" startWordPosition="1657" endWordPosition="1660">ntext discovers). Moreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist, which may result in stars and scientists ending up as neighbours in the embedded space. A window size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word. Dependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This is facilitated by recent advances in parsing technology (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy. After parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m1, ... , mk and a head h, we consider the contexts (m1, lbl1), ... , (mk, lblk), (h, lbl−1 h ), 2word2vec’s implementation is slightly more complicated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. In addition, the </context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the association for Computational Linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1341" citStr="Harris (1954)" startWordPosition="192" endWordPosition="193">uage processing. The default approach of representing words as discrete and distinct symbols is insufficient for many tasks, and suffers from poor generalization. For example, the symbolic representation of the words “pizza” and “hamburger” are completely unrelated: even if we know that the word “pizza” is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="19760" citStr="Levy and Goldberg, 2014" startWordPosition="3151" endWordPosition="3154">the model for the contexts that are most activated by (have the highest dot product with) a given target word. By doing so, we can see what the model learned to be a good discriminative context for the word. To demonstrate, we list the 5 most activated contexts for our example words with DEPS embeddings in Table 2. Interestingly, the most discriminative syntactic contexts in these cases are 5Additional experiments (not presented in this paper) reinforce our conclusion. In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al., 2013c; Levy and Goldberg, 2014). batman hogwarts turing superman/conj−1 students/prep at−1 machine/nn−1 spider-man/conj−1 educated/prep at−1 test/nn−1 superman/conj student/prep at−1 theorem/poss−1 spider-man/conj stay/prep at−1 machines/nn−1 robin/conj learned/prep at−1 tests/nn−1 florida object-oriented dancing marlins/nn−1 programming/amod−1 dancing/conj beach/appos−1 language/amod−1 dancing/conj−1 jacksonville/appos−1 framework/amod−1 singing/conj−1 tampa/appos−1 interface/amod−1 singing/conj florida/conj−1 software/amod−1 ballroom/nn Table 2: Words and their top syntactic contexts. not associated with subjects or objec</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Linguistic regularities in sparse and explicit word representations. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4005" citStr="Lin, 1998" startWordPosition="613" endWordPosition="614"> to be its linear context – words that precede and follow the target word, typically in a window of k tokens to each side. However, other types of contexts can be explored too. In this work, we generalize the SKIPGRAM model, and move from linear bag-of-words contexts to arbitrary word contexts. Specifically, 1code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics following work in sparse vector-space models (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees. The different kinds of contexts produce noticeably different embeddings, and induce different word similarities. In particular, the bag-ofwords nature of the contexts in the “original” SKIPGRAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature. This effect is demonstrated using both qualitative and quantitative analysis (Section 4). The neural word-embeddin</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98, pages 768–774, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>JH Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5528--5531</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2518" citStr="Mikolov et al., 2011" startWordPosition="378" endWordPosition="381">is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does i</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, JH Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5528–5531. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="2540" citStr="Mikolov et al., 2013" startWordPosition="382" endWordPosition="385">sociation between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word </context>
<context position="5309" citStr="Mikolov et al., 2013" startWordPosition="819" endWordPosition="822">ensions of the induced representation. In Section 5 we show that the SKIPGRAM model does allow for some introspection by querying it for contexts that are “activated by” a target word. This allows us to peek into the learned representation and explore the contexts that are found by the learning process to be most discriminative of particular words (or groups of words). To the best of our knowledge, this is the first work to suggest such an analysis of discriminativelytrained word-embedding models. 2 The Skip-Gram Model Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al., 2013a) trained using the negative-sampling procedure presented in (Mikolov et al., 2013b). In this section we summarize the model and training objective following the derivation presented by Goldberg and Levy (2014), and highlight the ease of incorporating arbitrary contexts in the model. In the skip-gram model, each word w E W is associated with a vector vw E Rd and similarly each context c E C is represented as a vector vc E Rd, where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality. The entries in the vectors are latent, and treated as parameters to</context>
<context position="16545" citStr="Mikolov et al., 2013" startWordPosition="2619" endWordPosition="2622">ainesville fla texas fla alabama louisiana jacksonville gainesville georgia tampa tallahassee california lauderdale texas carolina object-oriented aspect-oriented aspect-oriented event-driven smalltalk event-driven domain-specific event-driven objective-c rule-based prolog dataflow data-driven domain-specific 4gl human-centered dancing singing singing singing dance dance rapping dances dances breakdancing dancers breakdancing miming tap-dancing clowning busking Table 1: Target words and their 5 most similar words, as induced by different embeddings. We also tried using the subsampling option (Mikolov et al., 2013b) with BOW contexts (not shown). Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality. 4.2 Quantitative Evaluation We supplement the examples in Table 1 with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread. To that end, we use the WordSim353 dataset (Finkelstein et al., 2002; Agirre et al., 2009). This dataset contains pairs of similar words that reflect either relatedness (topical similarity) </context>
<context position="19733" citStr="Mikolov et al., 2013" startWordPosition="3147" endWordPosition="3150">beddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word. By doing so, we can see what the model learned to be a good discriminative context for the word. To demonstrate, we list the 5 most activated contexts for our example words with DEPS embeddings in Table 2. Interestingly, the most discriminative syntactic contexts in these cases are 5Additional experiments (not presented in this paper) reinforce our conclusion. In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al., 2013c; Levy and Goldberg, 2014). batman hogwarts turing superman/conj−1 students/prep at−1 machine/nn−1 spider-man/conj−1 educated/prep at−1 test/nn−1 superman/conj student/prep at−1 theorem/poss−1 spider-man/conj stay/prep at−1 machines/nn−1 robin/conj learned/prep at−1 tests/nn−1 florida object-oriented dancing marlins/nn−1 programming/amod−1 dancing/conj beach/appos−1 language/amod−1 dancing/conj−1 jacksonville/appos−1 framework/amod−1 singing/conj−1 tampa/appos−1 interface/amod−1 singing/conj florida/conj−1 software/amod−1 ballroom/nn Table 2: Words and their top syntactic contexts. not associ</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</booktitle>
<pages>3111--3119</pages>
<location>Lake Tahoe, Nevada, United States,</location>
<contexts>
<context position="2540" citStr="Mikolov et al., 2013" startWordPosition="382" endWordPosition="385">sociation between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word </context>
<context position="5309" citStr="Mikolov et al., 2013" startWordPosition="819" endWordPosition="822">ensions of the induced representation. In Section 5 we show that the SKIPGRAM model does allow for some introspection by querying it for contexts that are “activated by” a target word. This allows us to peek into the learned representation and explore the contexts that are found by the learning process to be most discriminative of particular words (or groups of words). To the best of our knowledge, this is the first work to suggest such an analysis of discriminativelytrained word-embedding models. 2 The Skip-Gram Model Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al., 2013a) trained using the negative-sampling procedure presented in (Mikolov et al., 2013b). In this section we summarize the model and training objective following the derivation presented by Goldberg and Levy (2014), and highlight the ease of incorporating arbitrary contexts in the model. In the skip-gram model, each word w E W is associated with a vector vw E Rd and similarly each context c E C is represented as a vector vc E Rd, where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality. The entries in the vectors are latent, and treated as parameters to</context>
<context position="16545" citStr="Mikolov et al., 2013" startWordPosition="2619" endWordPosition="2622">ainesville fla texas fla alabama louisiana jacksonville gainesville georgia tampa tallahassee california lauderdale texas carolina object-oriented aspect-oriented aspect-oriented event-driven smalltalk event-driven domain-specific event-driven objective-c rule-based prolog dataflow data-driven domain-specific 4gl human-centered dancing singing singing singing dance dance rapping dances dances breakdancing dancers breakdancing miming tap-dancing clowning busking Table 1: Target words and their 5 most similar words, as induced by different embeddings. We also tried using the subsampling option (Mikolov et al., 2013b) with BOW contexts (not shown). Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality. 4.2 Quantitative Evaluation We supplement the examples in Table 1 with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread. To that end, we use the WordSim353 dataset (Finkelstein et al., 2002; Agirre et al., 2009). This dataset contains pairs of similar words that reflect either relatedness (topical similarity) </context>
<context position="19733" citStr="Mikolov et al., 2013" startWordPosition="3147" endWordPosition="3150">beddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word. By doing so, we can see what the model learned to be a good discriminative context for the word. To demonstrate, we list the 5 most activated contexts for our example words with DEPS embeddings in Table 2. Interestingly, the most discriminative syntactic contexts in these cases are 5Additional experiments (not presented in this paper) reinforce our conclusion. In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al., 2013c; Levy and Goldberg, 2014). batman hogwarts turing superman/conj−1 students/prep at−1 machine/nn−1 spider-man/conj−1 educated/prep at−1 test/nn−1 superman/conj student/prep at−1 theorem/poss−1 spider-man/conj stay/prep at−1 machines/nn−1 robin/conj learned/prep at−1 tests/nn−1 florida object-oriented dancing marlins/nn−1 programming/amod−1 dancing/conj beach/appos−1 language/amod−1 dancing/conj−1 jacksonville/appos−1 framework/amod−1 singing/conj−1 tampa/appos−1 interface/amod−1 singing/conj florida/conj−1 software/amod−1 ballroom/nn Table 2: Words and their top syntactic contexts. not associ</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 3111– 3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2540" citStr="Mikolov et al., 2013" startWordPosition="382" endWordPosition="385">sociation between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word </context>
<context position="5309" citStr="Mikolov et al., 2013" startWordPosition="819" endWordPosition="822">ensions of the induced representation. In Section 5 we show that the SKIPGRAM model does allow for some introspection by querying it for contexts that are “activated by” a target word. This allows us to peek into the learned representation and explore the contexts that are found by the learning process to be most discriminative of particular words (or groups of words). To the best of our knowledge, this is the first work to suggest such an analysis of discriminativelytrained word-embedding models. 2 The Skip-Gram Model Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al., 2013a) trained using the negative-sampling procedure presented in (Mikolov et al., 2013b). In this section we summarize the model and training objective following the derivation presented by Goldberg and Levy (2014), and highlight the ease of incorporating arbitrary contexts in the model. In the skip-gram model, each word w E W is associated with a vector vw E Rd and similarly each context c E C is represented as a vector vc E Rd, where W is the words vocabulary, C is the contexts vocabulary, and d is the embedding dimensionality. The entries in the vectors are latent, and treated as parameters to</context>
<context position="16545" citStr="Mikolov et al., 2013" startWordPosition="2619" endWordPosition="2622">ainesville fla texas fla alabama louisiana jacksonville gainesville georgia tampa tallahassee california lauderdale texas carolina object-oriented aspect-oriented aspect-oriented event-driven smalltalk event-driven domain-specific event-driven objective-c rule-based prolog dataflow data-driven domain-specific 4gl human-centered dancing singing singing singing dance dance rapping dances dances breakdancing dancers breakdancing miming tap-dancing clowning busking Table 1: Target words and their 5 most similar words, as induced by different embeddings. We also tried using the subsampling option (Mikolov et al., 2013b) with BOW contexts (not shown). Since word2vec removes the subsampled words from the corpus before creating the window contexts, this option effectively increases the window size, resulting in greater topicality. 4.2 Quantitative Evaluation We supplement the examples in Table 1 with quantitative evaluation to show that the qualitative differences pointed out in the previous section are indeed widespread. To that end, we use the WordSim353 dataset (Finkelstein et al., 2002; Agirre et al., 2009). This dataset contains pairs of similar words that reflect either relatedness (topical similarity) </context>
<context position="19733" citStr="Mikolov et al., 2013" startWordPosition="3147" endWordPosition="3150">beddings, we can query the model for the contexts that are most activated by (have the highest dot product with) a given target word. By doing so, we can see what the model learned to be a good discriminative context for the word. To demonstrate, we list the 5 most activated contexts for our example words with DEPS embeddings in Table 2. Interestingly, the most discriminative syntactic contexts in these cases are 5Additional experiments (not presented in this paper) reinforce our conclusion. In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al., 2013c; Levy and Goldberg, 2014). batman hogwarts turing superman/conj−1 students/prep at−1 machine/nn−1 spider-man/conj−1 educated/prep at−1 test/nn−1 superman/conj student/prep at−1 theorem/poss−1 spider-man/conj stay/prep at−1 machines/nn−1 robin/conj learned/prep at−1 tests/nn−1 florida object-oriented dancing marlins/nn−1 programming/amod−1 dancing/conj beach/appos−1 language/amod−1 dancing/conj−1 jacksonville/appos−1 framework/amod−1 singing/conj−1 tampa/appos−1 interface/amod−1 singing/conj florida/conj−1 software/amod−1 ballroom/nn Table 2: Words and their top syntactic contexts. not associ</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="2496" citStr="Mnih and Hinton, 2008" startWordPosition="374" endWordPosition="377">rs in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec soft</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>424--434</pages>
<contexts>
<context position="2221" citStr="Ritter et al., 2010" startWordPosition="331" endWordPosition="334">contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because the</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In ACL, pages 424–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>435--444</pages>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In ACL, pages 435–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2747" citStr="Socher et al., 2011" startWordPosition="415" endWordPosition="418">reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies. Previous work on n</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Chris Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="13177" citStr="Toutanova et al., 2003" startWordPosition="2136" endWordPosition="2139">th k = 2) and DEPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings. For bag-of-words contexts we used the original word2vec implementation, and for syntactic contexts, we used our modified version. The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15. prep with amod nsubj dobj prep amod nsubj dobj pobj 304 All embeddings were trained on English Wikipedia. For DEPS, the corpus was tagged with parts-of-speech using the Stanford tagger (Toutanova et al., 2003) and parsed into labeled Stanford dependencies (de Marneffe and Manning, 2008) using an implementation of the parser described in (Goldberg and Nivre, 2012). All tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntactic contexts. We report results for 300 dimension embeddings, though similar trends were also observed with 600 dimensions. 4.1 Qualitative Evaluation Our first evaluation is qualitative: we manually inspect the 5 most similar words (by cosine</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Chris Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2702" citStr="Turian et al., 2010" startWordPosition="407" endWordPosition="410">nality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>188</pages>
<contexts>
<context position="2001" citStr="Turney and Pantel, 2010" startWordPosition="296" endWordPosition="299">xts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings”</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P.D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="14412" citStr="Turney (2012)" startWordPosition="2343" endWordPosition="2344">n set of target words (Table 1). The first target word, Batman, results in similar sets across the different setups. This is the case for many target words. However, other target words show clear differences between embeddings. In Hogwarts - the school of magic from the fictional Harry Potter series - it is evident that BOW contexts reflect the domain aspect, whereas DEPS yield a list of famous schools, capturing the semantic type of the target word. This observation holds for Turing3 and many other nouns as well; BOW find words that associate with w, while DEPS find words that behave like w. Turney (2012) described this distinction as domain similarity versus functional similarity. The Florida example presents an ontological difference; bag-of-words contexts generate meronyms (counties or cities within Florida), while dependency-based contexts provide cohyponyms (other US states). We observed the same behavior with other geographical locations, particularly with countries (though not all of them). The next two examples demonstrate that similarities induced from DEPS share a syntactic function (adjectives and gerunds), while similarities based on BOW are more diverse. Finally, we observe that w</context>
<context position="18108" citStr="Turney (2012)" startWordPosition="2875" endWordPosition="2876">ds one subset (“similarity”) over another (“relatedness”). We expect DEPS’s curve to be higher than BOW2’s curve, which in turn is expected to be higher than 4Some word pairs are judged to exhibit both types of similarity, and were ignored in this experiment. 305 Figure 2: Recall-precision curve when attempting to rank the similar words above the related ones. (a) is based on the WordSim353 dataset, and (b) on the Chiarello et al. dataset. BOW5’s. The graph in Figure 2a shows this is indeed the case. We repeated the experiment with a different dataset (Chiarello et al., 1990) that was used by Turney (2012) to distinguish between domain and functional similarities. The results show a similar trend (Figure 2b). When reversing the task such that the goal is to rank the related terms above the similar ones, the results are reversed, as expected (not shown).5 5 Model Introspection Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics. While this is true to a large extent, we observe that SKIPGRAM does allow a non-tr</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>755--762</pages>
<contexts>
<context position="1659" citStr="Uszkoreit and Brants, 2008" startWordPosition="240" endWordPosition="244"> is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). are represented as a very high dimensional but sparse vectors in which each entry is a measure of the association between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012)</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proc. of ACL, pages 755–762.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>