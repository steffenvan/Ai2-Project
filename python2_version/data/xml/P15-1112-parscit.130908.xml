<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002563">
<title confidence="0.9997285">
A Re-ranking Model for Dependency Parser
with Recursive Convolutional Neural Network
</title>
<author confidence="0.999335">
Chenxi Zhu, Xipeng Qiu∗, Xinchi Chen, Xuanjing Huang
</author>
<affiliation confidence="0.999392">
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
</affiliation>
<address confidence="0.962897">
825 Zhangheng Road, Shanghai, China
</address>
<email confidence="0.998801">
{czhu13,xpqiu,xinchichen13,xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956523809524">
In this work, we address the prob-
lem to model all the nodes (words or
phrases) in a dependency tree with the
dense representations. We propose a
recursive convolutional neural network
(RCNN) architecture to capture syntac-
tic and compositional-semantic represen-
tations of phrases and words in a depen-
dency tree. Different with the original re-
cursive neural network, we introduce the
convolution and pooling layers, which can
model a variety of compositions by the
feature maps and choose the most infor-
mative compositions by the pooling lay-
ers. Based on RCNN, we use a discrimina-
tive model to re-rank a k-best list of can-
didate dependency parsing trees. The ex-
periments show that RCNN is very effec-
tive to improve the state-of-the-art depen-
dency parsing on both English and Chi-
nese datasets.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996168">
Feature-based discriminative supervised models
have achieved much progress in dependency pars-
ing (Nivre, 2004; Yamada and Matsumoto, 2003;
McDonald et al., 2005), which typically use mil-
lions of discrete binary features generated from a
limited size training data. However, the ability of
these models is restricted by the design of features.
The number of features could be so large that the
result models are too complicated for practical use
and prone to overfit on training corpus due to data
sparseness.
Recently, many methods are proposed to learn
various distributed representations on both syn-
tax and semantics levels. These distributed repre-
sentations have been extensively applied on many
</bodyText>
<note confidence="0.846653">
∗Corresponding author.
</note>
<figureCaption confidence="0.999932">
Figure 1: Illustration of a RCNN unit.
</figureCaption>
<bodyText confidence="0.999597580645161">
natural language processing (NLP) tasks, such as
syntax (Turian et al., 2010; Mikolov et al., 2010;
Collobert et al., 2011; Chen and Manning, 2014)
and semantics (Huang et al., 2012; Mikolov et al.,
2013). Distributed representations are to represent
words (or phrase) by the dense, low-dimensional
and real-valued vectors, which help address the
curse of dimensionality and have better general-
ization than discrete representations.
For dependency parsing, Chen et al. (2014)
and Bansal et al. (2014) used the dense vectors
(embeddings) to represent words or features and
found these representations are complementary
to the traditional discrete feature representation.
However, these two methods only focus on the
dense representations (embeddings) of words or
features. These embeddings are pre-trained and
keep unchanged in the training phase of parsing
model, which cannot be optimized for the specific
tasks.
Besides, it is also important to represent the
(unseen) phrases with dense vector in dependency
parsing. Since the dependency tree is also in re-
cursive structure, it is intuitive to use the recur-
sive neural network (RNN), which is used for con-
stituent parsing (Socher et al., 2013a). However,
recursive neural network can only process the bi-
nary combination and is not suitable for depen-
dency parsing, since a parent node may have two
or more child nodes in dependency tree.
In this work, we address the problem to rep-
</bodyText>
<figure confidence="0.999204">
a red bike,NN
Pooling
Convolution
a bike,NN red bike,NN
a,Det red,JJ bike,NN
</figure>
<page confidence="0.970636">
1159
</page>
<note confidence="0.976694">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999071">
resent all level nodes (words or phrases) with
dense representations in a dependency tree. We
propose a recursive convolutional neural net-
work (RCNN) architecture to capture syntac-
tic and compositional-semantic representations of
phrases and words. RCNN is a general architec-
ture and can deal with k-ary parsing tree, there-
fore it is very suitable for dependency parsing. For
each node in a given dependency tree, we first use
a RCNN unit to model the interactions between it
and each of its children and choose the most infor-
mative features by a pooling layer. Thus, we can
apply the RCNN unit recursively to get the vector
representation of the whole dependency tree. The
output of each RCNN unit is used as the input of
the RCNN unit of its parent node, until it outputs a
single fixed-length vector at root node. Figure 1 il-
lustrates an example how a RCNN unit represents
the phrases “a red bike” as continuous vectors.
The contributions of this paper can be summa-
rized as follows.
</bodyText>
<listItem confidence="0.8738169">
• RCNN is a general architecture to model the
distributed representations of a phrase or sen-
tence with its dependency tree. Although
RCNN is just used for the re-ranking of the
dependency parser in this paper, it can be
regarded as semantic modelling of text se-
quences and handle the input sequences of
varying length into a fixed-length vector. The
parameters in RCNN can be learned jointly
with some other NLP tasks, such as text clas-
sification.
• Each RCNN unit can model the complicated
interactions of the head word and its children.
Combined with a specific task, RCNN can
capture the most useful semantic and struc-
ture information by the convolution and pool-
ing layers.
• When applied to the re-ranking model for
parsing, RCNN improve the accuracy of base
parser to make accurate parsing decisions.
</listItem>
<bodyText confidence="0.722592">
The experiments on two benchmark datasets
show that RCNN outperforms the state-of-
the-art models.
</bodyText>
<sectionHeader confidence="0.974986" genericHeader="introduction">
2 Recursive Neural Network
</sectionHeader>
<bodyText confidence="0.973802666666667">
In this section, we briefly describe the recur-
sive neural network architecture of (Socher et al.,
2013a).
</bodyText>
<figureCaption confidence="0.999036">
Figure 2: Illustration of a RNN unit.
</figureCaption>
<bodyText confidence="0.995026142857143">
The idea of recursive neural networks (RNN)
for natural language processing (NLP) is to train a
deep learning model that can be applied to phrases
and sentences, which have a grammatical structure
(Pollack, 1990; Socher et al., 2013c). RNN can be
also regarded as a general structure to model sen-
tence. At every node in the tree, the contexts at the
left and right children of the node are combined
by a classical layer. The weights of the layer are
shared across all nodes in the tree. The layer com-
puted at the top node gives a representation for the
whole sentence.
Following the binary tree structure, RNN can
assign a fixed-length vector to each word at the
leaves of the tree, and combine word and phrase
pairs recursively to create intermediate node vec-
tors of the same length, eventually having one fi-
nal vector representing the whole sentence. Multi-
ple recursive combination functions have been ex-
plored, from linear transformation matrices to ten-
sor products (Socher et al., 2013c). Figure 2 illus-
trates the architecture of RNN.
The binary tree can be represented in the form
of branching triplets (p → c1c2). Each such triplet
denotes that a parent node p has two children and
each ck can be either a word or a non-terminal
node in the tree.
Given a labeled binary parse tree,
</bodyText>
<equation confidence="0.918134">
((p2 → ap1), (p1 → bc)), the node represen-
tations are computed by
), p2 = f (W L P1 J ), (1)
</equation>
<bodyText confidence="0.998438444444445">
where (p1, p2, a, b, c) are the vector representa-
tion of (p1, p2, a, b, c) respectively, which are de-
noted by lowercase bold font letters; W is a matrix
of parameters of the RNN.
Based on RNN, Socher et al. (2013a) intro-
duced a compositional vector grammar, which
uses the syntactically untied weights W to learn
the syntactic-semantic, compositional vector rep-
resentations. In order to compute the score of
</bodyText>
<figure confidence="0.9164024">
a red bike,NP
red bike,NP
a,Det red,JJ bike,NN
p1 = f(W L bJ
c
</figure>
<page confidence="0.975212">
1160
</page>
<bodyText confidence="0.9914075">
how plausible of a syntactic constituent a parent
is, RNN uses a single-unit linear layer for all pi:
</bodyText>
<equation confidence="0.995223">
s(pi) = v - pi, (2)
</equation>
<bodyText confidence="0.999960857142857">
where v is a vector of parameters that need to be
trained. This score will be used to find the high-
est scoring tree. For more details on how standard
RNN can be used for parsing, see (Socher et al.,
2011).
Costa et al. (2003) applied recursive neural net-
works to re-rank possible phrase attachments in an
incremental constituency parser. Their work is the
first to show that RNNs can capture enough in-
formation to make the correct parsing decisions.
Menchetti et al. (2005) used RNNs to re-rank dif-
ferent constituency parses. For their results on full
sentence parsing, they re-ranked candidate trees
created by the Collins parser (Collins, 2003).
</bodyText>
<sectionHeader confidence="0.969449" genericHeader="method">
3 Recursive Convolutional Neural
Network
</sectionHeader>
<bodyText confidence="0.999980083333333">
The dependency grammar is a widely used syntac-
tic structure, which directly reflects relationships
among the words in a sentence. In a dependency
tree, all nodes are terminal (words) and each node
may have more than two children. Therefore, the
standard RNN architecture is not suitable for de-
pendency grammar since it is based on the binary
tree. In this section, we propose a more general
architecture, called recursive convolutional neu-
ral network (RCNN), which borrows the idea of
convolutional neural network (CNN) and can deal
with to k-ary tree.
</bodyText>
<subsectionHeader confidence="0.992375">
3.1 RCNN Unit
</subsectionHeader>
<bodyText confidence="0.998768">
For ease of exposition, we first describe the ba-
sic unit of RCNN. A RCNN unit is to model a
head word and its children. Different from the
constituent tree, the dependency tree does not have
non-terminal nodes. Each node consists of a word
and its POS tags. Each node should have a differ-
ent interaction with its head node.
Word Embeddings Given a word dictionary W,
each word w E W is represented as a real-valued
vector (word embedding) w E Rm where m is the
dimensionality of the vector space. The word em-
beddings are then stacked into a embedding ma-
trix M E Rm|W|. For a word w E W, its cor-
responding word embedding Embed(w) E Rm is
retrieved by the lookup table layer. The matrix M
</bodyText>
<subsectionHeader confidence="0.757625">
Phrase Representations of Children
</subsectionHeader>
<figureCaption confidence="0.997623">
Figure 3: Architecture of a RCNN unit.
</figureCaption>
<bodyText confidence="0.989384321428571">
is initialized with pre-training embeddings and up-
dated by back-propagation.
Distance Embeddings Besides word embed-
dings, we also use distributed vector to represent
the relative distance of a head word h and one of
its children c. For example, as shown in Figure 1,
the relative distances of “bike” to “a” and “red” are
-2 and -1, respectively. The relative distances also
are mapped to a vector of dimension md (a hy-
perparameter); this vector is randomly initialized.
Distance embedding is a usual way to encode the
distance information in neural model, which has
been proven effectively in several tasks. Our ex-
perimental results also show that the distance em-
bedding gives more benefits than the traditional
representation. The relative distance can encode
the structure information of a subtree.
Convolution The word and distance embed-
dings are subsequently fed into the convolution
component to model the interactions between two
linked nodes.
Different with standard RNN, there are no non-
terminal nodes in dependency tree. Each node h
in dependency tree has two associated distributed
representations:
1. word embedding wh E Rm, which is denoted
as its own information according to its word
form;
</bodyText>
<figure confidence="0.9954032">
x1
x2
xK
xh =
Max pooling
Convolution
tanh
···
···
d(h,ci)
Distance Embedding
wh
···
Word Embedding
···
</figure>
<page confidence="0.980772">
1161
</page>
<bodyText confidence="0.991694">
2. phrase representation xh E Rm, which is de-
noted as the joint representation of the whole
subtree rooted at h. In particular, when h is
leaf node, xh = wh.
Given a subtree rooted at h in dependency tree,
we define ci, 0 &lt; i &lt; L as the i-th child node of
h, where L represents the number of children.
For each pair (h, ci), we use a convolutional
hidden layer to compute their combination repre-
sentation zi.
</bodyText>
<figure confidence="0.999013285714286">
X(root)
RCNN Unit
w(root)
X(eat)
root
eat
w(eat)
RCNN Unit
w(with)
RCNN Unit
w(I) w(sashimi) X(with)
I sashimi with
chopsticks
w(ChopstiCks)
</figure>
<figureCaption confidence="0.999985">
Figure 4: Example of a RCNN unit
</figureCaption>
<equation confidence="0.612378">
zi = tanh(W(h,ci)pi), 0 &lt; i &lt; K, (3)
</equation>
<bodyText confidence="0.999922714285714">
where W(h,ci) E Rm×n is the linear composition
matrix, which depends on the POS tags of h and
ci; pi E Rn is the concatenated representation of
h and the i-th child, which consists of the head
word embeddings wh, the child phrase represen-
tation xci and the distance embeddings dh,ci of h
and ci,
</bodyText>
<equation confidence="0.918572">
pi = xh ® xci ® d(h,ci), (4)
</equation>
<bodyText confidence="0.999808826086957">
where ® represents the concatenation operation.
The distances dh,ci is the relative distance of h
and ci in a given sentence. Then, the relative dis-
tances also are mapped to m-dimensional vectors.
Different from constituent tree, the combination
should consider the order or position of each child
in dependency tree.
In our model, we do not use the POS tags em-
beddings directly. Since the composition matrix
varies on the different pair of POS tags of h and
ci, it can capture the different syntactic combina-
tions. For example, the combination of adjective
and noun should be different with that of verb and
noun.
After the composition operations, we use tank
as the non-linear activation function to get a hid-
den representation z.
Max Pooling After convolution, we get Z(h) =
[z1, z2, · · · , zK], where K is dynamic and de-
pends on the number of children of h. To trans-
form Z to a fixed length and determine the most
useful semantic and structure information, we per-
form a max pooling operation to Z on rows.
</bodyText>
<equation confidence="0.986820666666667">
xj = max
(h) Z�h), 0 &lt; j &lt; m. (5)
i
</equation>
<bodyText confidence="0.998889714285714">
Thus, we obtain the vector representation xh E
Rm of the whole subtree rooted at node h.
Figure 3 shows the architecture of our proposed
RCNN unit.
Given a whole dependency tree, we can apply
the RCNN unit recursively to get the vector rep-
resentation of the whole sentence. The output of
each RCNN unit is used as the input of the RCNN
unit of its parent node.
Thus, RCNN can be used to model the dis-
tributed representations of a phrase or sentence
with its dependency tree and applied to many NLP
tasks. The parameters in RCNN can be learned
jointly with the specific NLP tasks. Each RCNN
unit can model the complicated interactions of the
head word and its children. Combined with a spe-
cific task, RCNN can select the useful semantic
and structure information by the convolution and
max pooling layers.
Figure 4 shows an example of RCNN to model
the sentence “I eat sashimi with chopsitcks”.
</bodyText>
<sectionHeader confidence="0.997396" genericHeader="method">
4 Parsing
</sectionHeader>
<bodyText confidence="0.999914666666667">
In order to measure the plausibility of a subtree
rooted at h in dependency tree, we use a single-
unit linear layer neural network to compute the
score of its RCNN unit.
For constituent parsing, the representation of a
non-terminal node only depends on its two chil-
dren. The combination is relative simple and its
correctness can be measured with the final repre-
sentation of the non-terminal node (Socher et al.,
2013a).
However for dependency parsing, all combina-
tions of the head h and its children ci(0 &lt; i &lt; K)
are important to measure the correctness of the
subtree. Therefore, our score function s(h) is
computed on all of hidden layers zi(0 &lt; i &lt; K):
</bodyText>
<equation confidence="0.996815">
K
s(h) = v(h,ci) · zi, (6)
i=1
</equation>
<bodyText confidence="0.962948">
where v(h,ci) E Rm×1 is the score vector, which
</bodyText>
<page confidence="0.974965">
1162
</page>
<bodyText confidence="0.995181">
also depends on the POS tags of h and ci.
Given a sentence x and its dependency tree y,
the goodness of a complete tree is measured by
summing the scores of all the RCNN units.
</bodyText>
<equation confidence="0.960597">
�s(x, y, Θ) =
hEy
</equation>
<bodyText confidence="0.997581166666667">
where h ∈ y is the node in tree y; Θ =
{ΘW, Θv, Θw, Θd} including the combination
matrix set ΘW, the score vector set Θv, the word
embeddings Θw and distance embeddings Θd.
Finally, we can predict dependency tree yˆ with
highest score for sentence x.
</bodyText>
<equation confidence="0.998936">
yˆ = arg max s(x, y, Θ), (8)
yEgen(x)
</equation>
<bodyText confidence="0.99967475">
where gen(x) is defined as the set of all possible
trees for sentence x. When applied in re-ranking,
gen(x) is the set of the k-best outputs of a base
parser.
</bodyText>
<sectionHeader confidence="0.995571" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.999809333333334">
For a given training instance (xi, yi), we use the
max-margin criterion to train our model. We first
predict the dependency tree ˆyi with the highest
score for each xi and define a structured margin
loss Δ(yi, ˆyi) between the predicted tree ˆyi and
the given correct tree yi. Δ(yi, ˆyi) is measured
by counting the number of nodes yi with an incor-
rect span (or label) in the proposed tree (Goodman,
1998).
</bodyText>
<equation confidence="0.995875">
Δ(yi, ˆyi) = � κ1{d ∈/ yi} (9)
dE ˆyi
</equation>
<bodyText confidence="0.998954">
where κ is a discount parameter and d represents
the nodes in trees.
Given a set of training dependency parses D,
the final training objective is to minimize the loss
function J(Θ), plus a l2-regulation term:
</bodyText>
<equation confidence="0.996063">
1 J(Θ) = |D |ri(Θ) + 2 kΘk22, (10)
(xi,yi)ED
</equation>
<bodyText confidence="0.805695">
where
</bodyText>
<equation confidence="0.999068333333333">
ri(Θ) = max ( 0, st(xi, ˆyi, Θ)
ˆyi EY (xi )
+ Δ(yi, ˆyi) − st(xi, yi, Θ) ) . (11)
</equation>
<bodyText confidence="0.999915857142857">
By minimizing this object, the score of the cor-
rect tree yi is increased and the score of the highest
scoring incorrect tree ˆyi is decreased.
We use a generalization of gradient descent
called subgradient method (Ratliff et al., 2007)
which computes a gradient-like direction. The
subgradient of equation is:
</bodyText>
<equation confidence="0.9980258">
1 �(∂st(xi, ˆyi, Θ)
|D|
(xi,yi)ED ∂Θ
∂st(xi, yi, Θ) ) + λΘ. (12)
∂Θ
</equation>
<bodyText confidence="0.999044">
To minimize the objective, we use the diagonal
variant of AdaGrad (Duchi et al., 2011). The pa-
rameter update for the i-th parameter Θt,i at time
step t is as follows:
</bodyText>
<equation confidence="0.988435">
Θt,i = Θ p (t—1,i — t 2 gt,i, 13)
�Eτ=1 gτ,i
</equation>
<bodyText confidence="0.999764">
where ρ is the initial learning rate and gτ ∈ R|θi|
is the subgradient at time step τ for parameter θi.
</bodyText>
<sectionHeader confidence="0.992601" genericHeader="method">
6 Re-rankers
</sectionHeader>
<bodyText confidence="0.999980066666667">
Re-ranking k-best lists was introduced by Collins
and Koo (2005) and Charniak and Johnson (2005).
They used discriminative methods to re-rank the
constituent parsing. In the dependency parsing,
Sangati et al. (2009) used a third-order generative
model for re-ranking k-best lists of base parser.
Hayashi et al. (2013) used a discriminative for-
est re-ranking algorithm for dependency parsing.
These re-ranking models achieved a substantial
raise on the parsing performances.
Given T (x), the set of k-best trees of a sentence
x from a base parser, we use the popular mixture
re-ranking strategy (Hayashi et al., 2013; Le and
Mikolov, 2014), which is a combination of the our
model and the base parser.
</bodyText>
<equation confidence="0.981254333333333">
ˆyi = arg max αst(xi, y, Θ) + (1 − α)sb(xi, y)
yET (xi)
(14)
</equation>
<bodyText confidence="0.997910454545455">
where α ∈ [0, 1] is a hyperparameter; st(xi, y, Θ)
and sb(xi, y) are the scores given by RCNN and
the base parser respectively.
To apply RCNN into re-ranking model, we first
get the k-best outputs of all sentences in train
set with a base parser. Thus, we can train the
RCNN in a discriminative way and optimize the
re-ranking strategy for a particular base parser.
Note that the role of RCNN is not fully valued
when applied in re-ranking model since that the
gen(x) in Eq.(8) is just the k-best outputs of abase
</bodyText>
<equation confidence="0.978327">
s(h), (7) ∂J
∂Θ =
</equation>
<page confidence="0.834449">
1163
</page>
<bodyText confidence="0.999095333333333">
parser, not the set of all possible trees for sentence
x. The parameters of RCNN could overfit to k-
best outputs of training set.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.946323">
7.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99998975">
To empirically demonstrate the effectiveness of
our approach, we use two datasets in different lan-
guages (English and Chinese) in our experimen-
tal evaluation and compare our model against the
other state-of-the-art methods using the unlabeled
attachment score (UAS) metric ignoring punctua-
tion.
English For English dataset, we follow the stan-
dard splits of Penn Treebank (PTB), using
sections 2-21 for training, section 22 as de-
velopment set and section 23 as test set. We
tag the development and test sets using an au-
tomatic POS tagger (at 97.2% accuracy), and
tag the training set using four-way jackknif-
ing similar to (Collins and Koo, 2005).
Chinese For Chinese dataset, we follow the same
split of the Penn Chinese Treeban (CTB5)
as described in (Zhang and Clark, 2008) and
use sections 001-815, 1001-1136 as training
set, sections 886-931, 1148- 1151 as devel-
opment set, and sections 816-885, 1137-1147
as test set. Dependencies are converted by us-
ing the Penn2Malt tool with the head-finding
rules of (Zhang and Clark, 2008). And fol-
lowing (Zhang and Clark, 2008) (Zhang and
Nivre, 2011), we use gold segmentation and
POS tags for the input.
We use the linear-time incremental parser
(Huang and Sagae, 2010) as our base parser and
calculate the 64-best parses at the top cell of the
chart. Note that we optimize the training settings
for base parser and the results are slightly im-
proved on (Huang and Sagae, 2010). Then we use
max-margin criterion to train RCNN. Finally, we
use the mixture strategy to re-rank the top 64-best
parses.
For initialization of parameters, we train
word2vec embeddings (Mikolov et al., 2013) on
Wikipedia corpus for English and Chinese respec-
tively. For the combination matrices and score
vectors, we use the random initialization within
(0.01, 0.01). The parameters which achieve the
best unlabeled attachment score on the develop-
ment set will be chosen for the final evaluation.
</bodyText>
<subsectionHeader confidence="0.992081">
7.2 English Dataset
</subsectionHeader>
<bodyText confidence="0.985685702127659">
We first evaluate the performances of the RCNN
and re-ranker (Eq. (14)) on the development set.
Figure 5 shows UASs of different models with
varying k. The base parser achieves 92.45%.
When k = 64, the oracle best of base parser
achieves 97.34%, while the oracle worst achieves
73.30% (-19.15%) . RCNN achieves the maxi-
mum improvement of 93.00%(+0.55%) when k =
6. When k &gt; 6, the performance of RCNN de-
clines with the increase of k but is still higher
than baseline (92.45%). The reason behind this
is that RCNN could require more negative sam-
ples to avoid overfitting when k is large. Since the
negative samples are limited in the k-best outputs
of a base parser, the learnt parameters could easily
overfits to the training set.
The mixture re-ranker achieves the maximum
improvement of 93.50%(+1.05%) when k = 64.
In mixture re-ranker, α is optimised by searching
with the step-size 0.005.
Therefore, we use the mixture re-ranker in the
following experiments since it can take the advan-
tages of both the RCNN and base models.
Figure 6 shows the accuracies on the top ten
POS tags of the modifier words with the largest
improvements. We can see that our re-ranker
can improve the accuracies of CC and IN, and
therefore may indirectly result in rising the the
well-known coordinating conjunction and PP-
attachment problems.
The final experimental results on test set are
shown in Table 1. The hyperparameters of our
model are set as in Table 2. Our re-ranker achieves
the maximum improvement of 93.83%(+1.48%)
on test set. Our system performs slightly better
than many state-of-the-art systems such as Zhang
and Clark (2008) and Huang and Sagae (2010).
It outperforms Hayashi et al. (2013) and Le and
Zuidema (2014), which also use the mixture re-
ranking strategy.
Since the result of ranker is conditioned to k-
best results of base parser, we also do an experi-
ment to avoid this limitation by adding the oracle
to k-best candidates. With including oracle, the
re-ranker can achieve 94.16% on UAS, which is
shown in the last line (“our re-ranker (with ora-
cle)”) of Table 1.
</bodyText>
<page confidence="0.989125">
1164
</page>
<figure confidence="0.9989676">
1 2 3 4 5 6 7 8 9 10 32 64
k
UAS(%)
97
96
95
94
93
92
Oracle Best
RCNN
Re-ranker
1 2 3 4 5 6 7 8 9 10 32 64
k
UAS(%)
95
90
85
80
75
Oracle Best
RCNN
Re-ranker
Oracle Worst
(a) without the oracle worst result (b) with the oracle worst result
</figure>
<figureCaption confidence="0.95670775">
Figure 5: UAS with varying k on the development set. Oracle best: always choosing the best result in the
k-best of base parser; Oracle worst: always choosing the worst result in the k-best of base parser; RCNN:
choosing the most probable candidate according to the score of RCNN; Re-ranker: a combination of the
RCNN and base parser.
</figureCaption>
<figure confidence="0.398135">
WRB JJR WDT VBG VBP JJS RB CC MD IN
</figure>
<figureCaption confidence="0.887703">
Figure 6: Accuracies on the top ten POS tags of
</figureCaption>
<bodyText confidence="0.8148945">
the modifier words with the largest improvements
on the development set.
</bodyText>
<subsectionHeader confidence="0.998603">
7.3 Chinese Dataset
</subsectionHeader>
<bodyText confidence="0.999843833333333">
We also make experiments on the Penn Chinese
Treebank (CTB5). The hyperparameters is the
same as the previous experiment on English except
that α is optimised by searching with the step-size
0.005.
The final experimental results on the test set
are shown in Table 3. Our re-ranker achieves the
performance of 85.71%(+0.25%) on the test set,
which also outperforms the previous state-of-the-
art methods. With adding oracle, the re-ranker can
achieve 87.43% on UAS, which is shown in the
last line (“our re-ranker (with oracle)”) of Table 3.
</bodyText>
<table confidence="0.999174357142857">
UAS
Traditional Methods
Zhang and Clark (2008) 91.4
Huang and Sagae (2010) 92.1
Distributed Representations
Stenetorp (2013) 86.25
Chen et al. (2014) 93.74
Chen and Manning (2014) 92.0
Re-rankers
Hayashi et al. (2013) 93.12
Le and Zuidema (2014) 93.12
Our baseline 92.35
Our re-ranker 93.83(+1.48)
Our re-ranker (with oracle) 94.16
</table>
<tableCaption confidence="0.999645">
Table 1: Accuracy on English test set. Our base-
</tableCaption>
<bodyText confidence="0.99797275">
line is the result of base parser; our re-ranker uses
the mixture strategy on the 64-best outputs of base
parser; our re-ranker(with oracle) is to add the or-
acle to k-best outputs of base parser.
Compared with the re-ranking model of Hayashi et
al. (2013), that use a large number of handcrafted
features, our model can achieve a competitive per-
formance with the minimal feature engineering.
</bodyText>
<subsectionHeader confidence="0.984814">
7.4 Discussions
</subsectionHeader>
<bodyText confidence="0.99948225">
The performance of the re-ranking model is af-
fected by the base parser. The small divergence of
the dependency trees in the output list also results
to overfitting in training phase. Although our re-
</bodyText>
<figure confidence="0.994021857142857">
UAS(%)
95
90
85
80
75
Base Paser Re-ranker
</figure>
<page confidence="0.967379">
1165
</page>
<table confidence="0.99779">
Word embedding size m = 25
Distance embedding size md = 25
Initial learning rate p = 0.1
Margin loss discount κ = 2.0
Regularization A = 10−4
k-best k = 64
</table>
<tableCaption confidence="0.948765">
Table 2: Hyperparameters of our model
</tableCaption>
<table confidence="0.999899">
UAS
Traditional Methods
Zhang and Clark (2008) 84.33
Huang and Sagae (2010) 85.20
Distributed Representations
Chen et al. (2014) 82.94
Chen and Manning (2014) 83.9
Re-rankers
Hayashi et al. (2013) 85.9
Our baseline 85.46
Our re-ranker 85.71(+0.25)
Our re-ranker (with oracle) 87.43
</table>
<tableCaption confidence="0.999917">
Table 3: Accuracy on Chinese test set.
</tableCaption>
<bodyText confidence="0.999923428571429">
ranker outperforms the state-of-the-art methods, it
can also benefit from improving the quality of the
candidate results. It was also reported in other re-
ranking works that a larger k (eg. k &gt; 64) results
the worse performance. We think the reason is that
the oracle best increases when k is larger, but the
oracle worst decrease with larger degree. The er-
ror types increase greatly. The re-ranking model
requires more negative samples to avoid overfit-
ting. When k is larger, the number of negative
samples also needs to multiply increase for train-
ing. However, we just can obtain at most k neg-
ative samples from the k-best outputs of the base
parser.
The experiments also show that the our model
can achieves significant improvements by adding
the oracles into the output lists of the base parser.
This indicates that our model can be boosted by
a better set of the candidate results, which can be
implemented by combining the RCNN in the de-
coding algorithm.
</bodyText>
<sectionHeader confidence="0.999863" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.985663333333333">
There have been several works to use neural net-
works and distributed representation for depen-
dency parsing.
</bodyText>
<figureCaption confidence="0.999257">
Figure 7: Example of a DT-RNN unit
</figureCaption>
<bodyText confidence="0.999914853658537">
Stenetorp (2013) attempted to build recursive
neural networks for transition-based dependency
parsing, however the empirical performance of his
model is still unsatisfactory. Chen and Manning
(2014) improved the transition-based dependency
parsing by representing all words, POS tags and
arc labels as dense vectors, and modeled their in-
teractions with neural network to make predictions
of actions. Their methods aim to transition-based
parsing and can not model the sentence in seman-
tic vector space for other NLP tasks.
Socher et al. (2013b) proposed a composi-
tional vectors computed by dependency tree RNN
(DT-RNN) to map sentences and images into a
common embedding space. However, there are
two major differences as follows. 1) They first
summed up all child nodes into a dense vector v,
and then composed subtree representation from v,
and vector parent node. In contrast, our model
first combine the parent and each child and then
choose the most informative features with a pool-
ing layer. 2) We represent the relative position
of each child and its parent with distributed rep-
resentation (position embeddings), which is very
useful for convolutional layer. Figure 7 shows an
example of DTRNN to illustrates how RCNN rep-
resents phrases as continuous vectors.
Specific to the re-ranking model, Le and
Zuidema (2014) proposed a generative re-ranking
model with Inside-Outside Recursive Neural Net-
work (IORNN), which can process trees both
bottom-up and top-down. However, IORNN
works in generative way and just estimates the
probability of a given tree, so IORNN cannot fully
utilize the incorrect trees in k-best candidate re-
sults. Besides, IORNN treats dependency tree as a
sequence, which can be regarded as a generaliza-
tion of simple recurrent neural network (SRNN)
(Elman, 1990). Unlike IORNN, our proposed
RCNN is a discriminative model and can opti-
mize the re-ranking strategy for a particular base
</bodyText>
<figure confidence="0.998554">
a red bike,NN
Σ
a,Det red,JJ bike,NN
</figure>
<page confidence="0.988647">
1166
</page>
<bodyText confidence="0.999749571428571">
parser. Another difference is that RCNN computes
the score of tree in a recursive way, which is more
natural for the hierarchical structure of natural lan-
guage. Besides, the RCNN can not only be used
for the re-ranking, but also be regarded as general
model to represent sentence with its dependency
tree.
</bodyText>
<sectionHeader confidence="0.99668" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99997636">
In this work, we address the problem to rep-
resent all level nodes (words or phrases) with
dense representations in a dependency tree. We
propose a recursive convolutional neural net-
work (RCNN) architecture to capture the syntac-
tic and compositional-semantic representations of
phrases and words. RCNN is a general architec-
ture and can deal with k-ary parsing tree, there-
fore RCNN is very suitable for many NLP tasks
to minimize the effort in feature engineering with
a external dependency parser. Although RCNN
is just used for the re-ranking of the dependency
parser in this paper, it can be regarded as seman-
tic modelling of text sequences and handle the in-
put sequences of varying length into a fixed-length
vector. The parameters in RCNN can be learned
jointly with some other NLP tasks, such as text
classification.
For the future research, we will develop an inte-
grated parser to combine RCNN with a decoding
algorithm. We believe that the integrated parser
can achieve better performance without the limi-
tation of base parser. Moreover, we also wish to
investigate the ability of our model for other NLP
tasks.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999422888888889">
We would like to thank the anonymous review-
ers for their valuable comments. This work
was partially funded by the National Natural Sci-
ence Foundation of China (61472088, 61473092),
the National High Technology Research and De-
velopment Program of China (2015AA015408),
Shanghai Science and Technology Development
Funds (14ZR1403200), Shanghai Leading Aca-
demic Discipline Project (B114).
</bodyText>
<sectionHeader confidence="0.999313" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999643327272728">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 173–180, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature embedding for dependency parsing. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 816–826, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25–70.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589–637.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
and Giovanni Soda. 2003. Towards incremental
parsing of natural language using recursive neural
networks. Applied Intelligence, 19(1-2):9–25.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Joshua Goodman. 1998. Parsing inside-out. arXiv
preprint cmp-lg/9805007.
Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-
sumoto. 2013. Efficient stacked dependency pars-
ing by forest reranking. TACL, 1:139–150.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086. Association for Computational Linguistics.
</reference>
<page confidence="0.87661">
1167
</page>
<reference confidence="0.99985">
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873–882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of ICML.
Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 729–739, Doha, Qatar,
October. Association for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 91–98.
Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natural
language processing using kernel methods and neu-
ral networks for structured data. Pattern Recogni-
tion Letters, 26(12):1896–1906.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57. Association
for Computational Linguistics.
Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.
Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2007. (online) subgradient methods
for structured prediction. In Eleventh International
Conference on Artificial Intelligence and Statistics
(AIStats).
Federico Sangati, Willem Zuidema, and Rens Bod.
2009. A generative re-ranking model for depen-
dency parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
238–241. Association for Computational Linguis-
tics.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129–136.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Q Le, C Manning, and A Ng. 2013b.
Grounded compositional semantics for finding and
describing images with sentences. In NIPS Deep
Learning Workshop.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013c. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP.
Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In NIPS
Workshop on Deep Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of the International Workshop on Pars-
ing Technologies (IWPT), volume 3.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571. Association for Computa-
tional Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188–193. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.995232">
1168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.769651">
<title confidence="0.9968515">A Re-ranking Model for Dependency with Recursive Convolutional Neural Network</title>
<author confidence="0.9096935">Xipeng Xinchi Chen Zhu</author>
<author confidence="0.9096935">Xuanjing Shanghai Key Laboratory of Intelligent Information Processing</author>
<author confidence="0.9096935">Fudan</author>
<affiliation confidence="0.996551">School of Computer Science, Fudan</affiliation>
<address confidence="0.995453">825 Zhangheng Road, Shanghai,</address>
<abstract confidence="0.997605863636364">In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminamodel to re-rank a list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2424" citStr="Bansal et al. (2014)" startWordPosition="363" endWordPosition="366">ted representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="16952" citStr="Charniak and Johnson (2005)" startWordPosition="2927" endWordPosition="2930">nt descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction. The subgradient of equation is: 1 �(∂st(xi, ˆyi, Θ) |D| (xi,yi)ED ∂Θ ∂st(xi, yi, Θ) ) + λΘ. (12) ∂Θ To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter Θt,i at time step t is as follows: Θt,i = Θ p (t—1,i — t 2 gt,i, 13) �Eτ=1 gτ,i where ρ is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. 6 Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base par</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<contexts>
<context position="2071" citStr="Chen and Manning, 2014" startWordPosition="311" endWordPosition="314"> of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embedding</context>
<context position="23796" citStr="Chen and Manning (2014)" startWordPosition="4109" endWordPosition="4112">s the previous experiment on English except that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted features, our model can achieve a competitive performance with the minimal feature engineering. 7.4 Disc</context>
<context position="26481" citStr="Chen and Manning (2014)" startWordPosition="4559" endWordPosition="4562">del can achieves significant improvements by adding the oracles into the output lists of the base parser. This indicates that our model can be boosted by a better set of the candidate results, which can be implemented by combining the RCNN in the decoding algorithm. 8 Related Work There have been several works to use neural networks and distributed representation for dependency parsing. Figure 7: Example of a DT-RNN unit Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in semantic vector space for other NLP tasks. Socher et al. (2013b) proposed a compositional vectors computed by dependency tree RNN (DT-RNN) to map sentences and images into a common embedding space. However, there are two major differences as follows. 1) They first summed up all child nodes into a dense vector v, and </context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yue Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Feature embedding for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>816--826</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2399" citStr="Chen et al. (2014)" startWordPosition="358" endWordPosition="361"> levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure</context>
<context position="23766" citStr="Chen et al. (2014)" startWordPosition="4104" endWordPosition="4107">rparameters is the same as the previous experiment on English except that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted features, our model can achieve a competitive performance with the minimal</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2014</marker>
<rawString>Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Feature embedding for dependency parsing. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 816–826, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16920" citStr="Collins and Koo (2005)" startWordPosition="2922" endWordPosition="2925"> a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction. The subgradient of equation is: 1 �(∂st(xi, ˆyi, Θ) |D| (xi,yi)ED ∂Θ ∂st(xi, yi, Θ) ) + λΘ. (12) ∂Θ To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter Θt,i at time step t is as follows: Θt,i = Θ p (t—1,i — t 2 gt,i, 13) �Eτ=1 gτ,i where ρ is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. 6 Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination o</context>
<context position="18955" citStr="Collins and Koo, 2005" startWordPosition="3273" endWordPosition="3276">rate the effectiveness of our approach, we use two datasets in different languages (English and Chinese) in our experimental evaluation and compare our model against the other state-of-the-art methods using the unlabeled attachment score (UAS) metric ignoring punctuation. English For English dataset, we follow the standard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as development set and section 23 as test set. We tag the development and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to (Collins and Koo, 2005). Chinese For Chinese dataset, we follow the same split of the Penn Chinese Treeban (CTB5) as described in (Zhang and Clark, 2008) and use sections 001-815, 1001-1136 as training set, sections 886-931, 1148- 1151 as development set, and sections 816-885, 1137-1147 as test set. Dependencies are converted by using the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. We use the linear-time incremental parser (Huang and Sagae, 2010) as our base parser and calcul</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<booktitle>Computational linguistics,</booktitle>
<pages>29--4</pages>
<contexts>
<context position="8386" citStr="Collins, 2003" startWordPosition="1379" endWordPosition="1380">s that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003). 3 Recursive Convolutional Neural Network The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words in a sentence. In a dependency tree, all nodes are terminal (words) and each node may have more than two children. Therefore, the standard RNN architecture is not suitable for dependency grammar since it is based on the binary tree. In this section, we propose a more general architecture, called recursive convolutional neural network (RCNN), which borrows the idea of convolutional neural network (CNN) and can deal with to k-ary tree. 3.1 </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2046" citStr="Collobert et al., 2011" startWordPosition="307" endWordPosition="310">ta. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense r</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Vincenzo Lombardo</author>
<author>Giovanni Soda</author>
</authors>
<title>Towards incremental parsing of natural language using recursive neural networks.</title>
<date>2003</date>
<journal>Applied Intelligence,</journal>
<pages>19--1</pages>
<contexts>
<context position="7965" citStr="Costa et al. (2003)" startWordPosition="1312" endWordPosition="1315">l. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations. In order to compute the score of a red bike,NP red bike,NP a,Det red,JJ bike,NN p1 = f(W L bJ c 1160 how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi: s(pi) = v - pi, (2) where v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003). 3 Recursive Convolutional Neural Network The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words in a sentence. In a d</context>
</contexts>
<marker>Costa, Frasconi, Lombardo, Soda, 2003</marker>
<rawString>Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo, and Giovanni Soda. 2003. Towards incremental parsing of natural language using recursive neural networks. Applied Intelligence, 19(1-2):9–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="16613" citStr="Duchi et al., 2011" startWordPosition="2861" endWordPosition="2864"> l2-regulation term: 1 J(Θ) = |D |ri(Θ) + 2 kΘk22, (10) (xi,yi)ED where ri(Θ) = max ( 0, st(xi, ˆyi, Θ) ˆyi EY (xi ) + Δ(yi, ˆyi) − st(xi, yi, Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree ˆyi is decreased. We use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction. The subgradient of equation is: 1 �(∂st(xi, ˆyi, Θ) |D| (xi,yi)ED ∂Θ ∂st(xi, yi, Θ) ) + λΘ. (12) ∂Θ To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter Θt,i at time step t is as follows: Θt,i = Θ p (t—1,i — t 2 gt,i, 13) �Eτ=1 gτ,i where ρ is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. 6 Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="28072" citStr="Elman, 1990" startWordPosition="4815" endWordPosition="4816">ows an example of DTRNN to illustrates how RCNN represents phrases as continuous vectors. Specific to the re-ranking model, Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Unlike IORNN, our proposed RCNN is a discriminative model and can optimize the re-ranking strategy for a particular base a red bike,NN Σ a,Det red,JJ bike,NN 1166 parser. Another difference is that RCNN computes the score of tree in a recursive way, which is more natural for the hierarchical structure of natural language. Besides, the RCNN can not only be used for the re-ranking, but also be regarded as general model to represent sentence with its dependency tree. 9 Conclusion In this work, we address the problem to represent all level nodes (words or phrases) with dense representations in a</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing inside-out. arXiv preprint cmp-lg/9805007.</title>
<date>1998</date>
<contexts>
<context position="15766" citStr="Goodman, 1998" startWordPosition="2704" endWordPosition="2705"> yˆ = arg max s(x, y, Θ), (8) yEgen(x) where gen(x) is defined as the set of all possible trees for sentence x. When applied in re-ranking, gen(x) is the set of the k-best outputs of a base parser. 5 Training For a given training instance (xi, yi), we use the max-margin criterion to train our model. We first predict the dependency tree ˆyi with the highest score for each xi and define a structured margin loss Δ(yi, ˆyi) between the predicted tree ˆyi and the given correct tree yi. Δ(yi, ˆyi) is measured by counting the number of nodes yi with an incorrect span (or label) in the proposed tree (Goodman, 1998). Δ(yi, ˆyi) = � κ1{d ∈/ yi} (9) dE ˆyi where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l2-regulation term: 1 J(Θ) = |D |ri(Θ) + 2 kΘk22, (10) (xi,yi)ED where ri(Θ) = max ( 0, st(xi, ˆyi, Θ) ˆyi EY (xi ) + Δ(yi, ˆyi) − st(xi, yi, Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree ˆyi is decreased. We use a generalization of gradient descent called subgradient method (Rat</context>
</contexts>
<marker>Goodman, 1998</marker>
<rawString>Joshua Goodman. 1998. Parsing inside-out. arXiv preprint cmp-lg/9805007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhiko Hayashi</author>
<author>Shuhei Kondo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Efficient stacked dependency parsing by forest reranking.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--139</pages>
<contexts>
<context position="17173" citStr="Hayashi et al. (2013)" startWordPosition="2960" endWordPosition="2963">, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter Θt,i at time step t is as follows: Θt,i = Θ p (t—1,i — t 2 gt,i, 13) �Eτ=1 gτ,i where ρ is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. 6 Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. ˆyi = arg max αst(xi, y, Θ) + (1 − α)sb(xi, y) yET (xi) (14) where α ∈ [0, 1] is a hyperparameter; st(xi, y, Θ) and sb(xi, y) are the scores given by RCNN and the base parser respectively. To apply RCNN into re-ranki</context>
<context position="21938" citStr="Hayashi et al. (2013)" startWordPosition="3772" endWordPosition="3775">p ten POS tags of the modifier words with the largest improvements. We can see that our re-ranker can improve the accuracies of CC and IN, and therefore may indirectly result in rising the the well-known coordinating conjunction and PPattachment problems. The final experimental results on test set are shown in Table 1. The hyperparameters of our model are set as in Table 2. Our re-ranker achieves the maximum improvement of 93.83%(+1.48%) on test set. Our system performs slightly better than many state-of-the-art systems such as Zhang and Clark (2008) and Huang and Sagae (2010). It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy. Since the result of ranker is conditioned to kbest results of base parser, we also do an experiment to avoid this limitation by adding the oracle to k-best candidates. With including oracle, the re-ranker can achieve 94.16% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 1. 1164 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 97 96 95 94 93 92 Oracle Best RCNN Re-ranker 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 95 90 85 80 75 Oracle Best RCNN Re-ranker Oracle Worst (a) without the oracle worst result (b) </context>
<context position="23834" citStr="Hayashi et al. (2013)" startWordPosition="4115" endWordPosition="4118">ept that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted features, our model can achieve a competitive performance with the minimal feature engineering. 7.4 Discussions The performance of the re-rank</context>
</contexts>
<marker>Hayashi, Kondo, Matsumoto, 2013</marker>
<rawString>Katsuhiko Hayashi, Shuhei Kondo, and Yuji Matsumoto. 2013. Efficient stacked dependency parsing by forest reranking. TACL, 1:139–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="19525" citStr="Huang and Sagae, 2010" startWordPosition="3367" endWordPosition="3370">r-way jackknifing similar to (Collins and Koo, 2005). Chinese For Chinese dataset, we follow the same split of the Penn Chinese Treeban (CTB5) as described in (Zhang and Clark, 2008) and use sections 001-815, 1001-1136 as training set, sections 886-931, 1148- 1151 as development set, and sections 816-885, 1137-1147 as test set. Dependencies are converted by using the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. We use the linear-time incremental parser (Huang and Sagae, 2010) as our base parser and calculate the 64-best parses at the top cell of the chart. Note that we optimize the training settings for base parser and the results are slightly improved on (Huang and Sagae, 2010). Then we use max-margin criterion to train RCNN. Finally, we use the mixture strategy to re-rank the top 64-best parses. For initialization of parameters, we train word2vec embeddings (Mikolov et al., 2013) on Wikipedia corpus for English and Chinese respectively. For the combination matrices and score vectors, we use the random initialization within (0.01, 0.01). The parameters which achi</context>
<context position="21900" citStr="Huang and Sagae (2010)" startWordPosition="3766" endWordPosition="3769">Figure 6 shows the accuracies on the top ten POS tags of the modifier words with the largest improvements. We can see that our re-ranker can improve the accuracies of CC and IN, and therefore may indirectly result in rising the the well-known coordinating conjunction and PPattachment problems. The final experimental results on test set are shown in Table 1. The hyperparameters of our model are set as in Table 2. Our re-ranker achieves the maximum improvement of 93.83%(+1.48%) on test set. Our system performs slightly better than many state-of-the-art systems such as Zhang and Clark (2008) and Huang and Sagae (2010). It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy. Since the result of ranker is conditioned to kbest results of base parser, we also do an experiment to avoid this limitation by adding the oracle to k-best candidates. With including oracle, the re-ranker can achieve 94.16% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 1. 1164 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 97 96 95 94 93 92 Oracle Best RCNN Re-ranker 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 95 90 85 80 75 Oracle Best RCNN Re-ranker Oracle Worst (a</context>
<context position="23691" citStr="Huang and Sagae (2010)" startWordPosition="4094" endWordPosition="4097"> Dataset We also make experiments on the Penn Chinese Treebank (CTB5). The hyperparameters is the same as the previous experiment on English except that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted</context>
<context position="24919" citStr="Huang and Sagae (2010)" startWordPosition="4303" endWordPosition="4306">res, our model can achieve a competitive performance with the minimal feature engineering. 7.4 Discussions The performance of the re-ranking model is affected by the base parser. The small divergence of the dependency trees in the output list also results to overfitting in training phase. Although our reUAS(%) 95 90 85 80 75 Base Paser Re-ranker 1165 Word embedding size m = 25 Distance embedding size md = 25 Initial learning rate p = 0.1 Margin loss discount κ = 2.0 Regularization A = 10−4 k-best k = 64 Table 2: Hyperparameters of our model UAS Traditional Methods Zhang and Clark (2008) 84.33 Huang and Sagae (2010) 85.20 Distributed Representations Chen et al. (2014) 82.94 Chen and Manning (2014) 83.9 Re-rankers Hayashi et al. (2013) 85.9 Our baseline 85.46 Our re-ranker 85.71(+0.25) Our re-ranker (with oracle) 87.43 Table 3: Accuracy on Chinese test set. ranker outperforms the state-of-the-art methods, it can also benefit from improving the quality of the candidate results. It was also reported in other reranking works that a larger k (eg. k &gt; 64) results the worse performance. We think the reason is that the oracle best increases when k is larger, but the oracle worst decrease with larger degree. The </context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077– 1086. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>873--882</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2105" citStr="Huang et al., 2012" startWordPosition="317" endWordPosition="320">esign of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These emb</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="17494" citStr="Mikolov, 2014" startWordPosition="3014" endWordPosition="3015">was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. ˆyi = arg max αst(xi, y, Θ) + (1 − α)sb(xi, y) yET (xi) (14) where α ∈ [0, 1] is a hyperparameter; st(xi, y, Θ) and sb(xi, y) are the scores given by RCNN and the base parser respectively. To apply RCNN into re-ranking model, we first get the k-best outputs of all sentences in train set with a base parser. Thus, we can train the RCNN in a discriminative way and optimize the re-ranking strategy for a particular base parser. Note that the role of RCNN is not fully valued when applied in re-ranking model since that the gen(x) in Eq.(8</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>729--739</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="21964" citStr="Zuidema (2014)" startWordPosition="3779" endWordPosition="3780">rds with the largest improvements. We can see that our re-ranker can improve the accuracies of CC and IN, and therefore may indirectly result in rising the the well-known coordinating conjunction and PPattachment problems. The final experimental results on test set are shown in Table 1. The hyperparameters of our model are set as in Table 2. Our re-ranker achieves the maximum improvement of 93.83%(+1.48%) on test set. Our system performs slightly better than many state-of-the-art systems such as Zhang and Clark (2008) and Huang and Sagae (2010). It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy. Since the result of ranker is conditioned to kbest results of base parser, we also do an experiment to avoid this limitation by adding the oracle to k-best candidates. With including oracle, the re-ranker can achieve 94.16% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 1. 1164 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 97 96 95 94 93 92 Oracle Best RCNN Re-ranker 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 95 90 85 80 75 Oracle Best RCNN Re-ranker Oracle Worst (a) without the oracle worst result (b) with the oracle worst resu</context>
<context position="23862" citStr="Zuidema (2014)" startWordPosition="4122" endWordPosition="4123">g with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted features, our model can achieve a competitive performance with the minimal feature engineering. 7.4 Discussions The performance of the re-ranking model is affected by the</context>
<context position="27605" citStr="Zuidema (2014)" startWordPosition="4743" endWordPosition="4744">ifferences as follows. 1) They first summed up all child nodes into a dense vector v, and then composed subtree representation from v, and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful for convolutional layer. Figure 7 shows an example of DTRNN to illustrates how RCNN represents phrases as continuous vectors. Specific to the re-ranking model, Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Unlike IORNN, our proposed RCNN is a discriminative model and can optimize the re-ranking strategy for a particular base a red bike</context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 729–739, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1324" citStr="McDonald et al., 2005" startWordPosition="193" endWordPosition="196">ive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Introduction Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sauro Menchetti</author>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Wide coverage natural language processing using kernel methods and neural networks for structured data.</title>
<date>2005</date>
<journal>Pattern Recognition Letters,</journal>
<volume>26</volume>
<issue>12</issue>
<contexts>
<context position="8213" citStr="Menchetti et al. (2005)" startWordPosition="1351" endWordPosition="1354">ike,NN p1 = f(W L bJ c 1160 how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi: s(pi) = v - pi, (2) where v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003). 3 Recursive Convolutional Neural Network The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words in a sentence. In a dependency tree, all nodes are terminal (words) and each node may have more than two children. Therefore, the standard RNN architecture is not suitable for dependency grammar since it is based on the binary tree. In this section, we propose a more g</context>
</contexts>
<marker>Menchetti, Costa, Frasconi, Pontil, 2005</marker>
<rawString>Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and Massimiliano Pontil. 2005. Wide coverage natural language processing using kernel methods and neural networks for structured data. Pattern Recognition Letters, 26(12):1896–1906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2128" citStr="Mikolov et al., 2013" startWordPosition="321" endWordPosition="324">he number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained</context>
<context position="19939" citStr="Mikolov et al., 2013" startWordPosition="3437" endWordPosition="3440"> (Zhang and Clark, 2008). And following (Zhang and Clark, 2008) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. We use the linear-time incremental parser (Huang and Sagae, 2010) as our base parser and calculate the 64-best parses at the top cell of the chart. Note that we optimize the training settings for base parser and the results are slightly improved on (Huang and Sagae, 2010). Then we use max-margin criterion to train RCNN. Finally, we use the mixture strategy to re-rank the top 64-best parses. For initialization of parameters, we train word2vec embeddings (Mikolov et al., 2013) on Wikipedia corpus for English and Chinese respectively. For the combination matrices and score vectors, we use the random initialization within (0.01, 0.01). The parameters which achieve the best unlabeled attachment score on the development set will be chosen for the final evaluation. 7.2 English Dataset We first evaluate the performances of the RCNN and re-ranker (Eq. (14)) on the development set. Figure 5 shows UASs of different models with varying k. The base parser achieves 92.45%. When k = 64, the oracle best of base parser achieves 97.34%, while the oracle worst achieves 73.30% (-19.</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1272" citStr="Nivre, 2004" startWordPosition="187" endWordPosition="188"> tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Introduction Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Correspon</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="5972" citStr="Pollack, 1990" startWordPosition="944" endWordPosition="945">ers. • When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions. The experiments on two benchmark datasets show that RCNN outperforms the state-ofthe-art models. 2 Recursive Neural Network In this section, we briefly describe the recursive neural network architecture of (Socher et al., 2013a). Figure 2: Illustration of a RNN unit. The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c). RNN can be also regarded as a general structure to model sentence. At every node in the tree, the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the whole sentence. Following the binary tree structure, RNN can assign a fixed-length vector to each word at the leaves of the tree, and combine word and phrase pairs recursively to create intermediate node vectors of the same length, eventually having one fina</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan D Ratliff</author>
<author>J Andrew Bagnell</author>
<author>Martin A Zinkevich</author>
</authors>
<title>(online) subgradient methods for structured prediction.</title>
<date>2007</date>
<booktitle>In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</booktitle>
<contexts>
<context position="16384" citStr="Ratliff et al., 2007" startWordPosition="2821" endWordPosition="2824">98). Δ(yi, ˆyi) = � κ1{d ∈/ yi} (9) dE ˆyi where κ is a discount parameter and d represents the nodes in trees. Given a set of training dependency parses D, the final training objective is to minimize the loss function J(Θ), plus a l2-regulation term: 1 J(Θ) = |D |ri(Θ) + 2 kΘk22, (10) (xi,yi)ED where ri(Θ) = max ( 0, st(xi, ˆyi, Θ) ˆyi EY (xi ) + Δ(yi, ˆyi) − st(xi, yi, Θ) ) . (11) By minimizing this object, the score of the correct tree yi is increased and the score of the highest scoring incorrect tree ˆyi is decreased. We use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradient-like direction. The subgradient of equation is: 1 �(∂st(xi, ˆyi, Θ) |D| (xi,yi)ED ∂Θ ∂st(xi, yi, Θ) ) + λΘ. (12) ∂Θ To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter Θt,i at time step t is as follows: Θt,i = Θ p (t—1,i — t 2 gt,i, 13) �Eτ=1 gτ,i where ρ is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. 6 Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative metho</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2007</marker>
<rawString>Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. 2007. (online) subgradient methods for structured prediction. In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federico Sangati</author>
<author>Willem Zuidema</author>
<author>Rens Bod</author>
</authors>
<title>A generative re-ranking model for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>238--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17071" citStr="Sangati et al. (2009)" startWordPosition="2944" endWordPosition="2947">tion is: 1 �(∂st(xi, ˆyi, Θ) |D| (xi,yi)ED ∂Θ ∂st(xi, yi, Θ) ) + λΘ. (12) ∂Θ To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011). The parameter update for the i-th parameter Θt,i at time step t is as follows: Θt,i = Θ p (t—1,i — t 2 gt,i, 13) �Eτ=1 gτ,i where ρ is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi. 6 Re-rankers Re-ranking k-best lists was introduced by Collins and Koo (2005) and Charniak and Johnson (2005). They used discriminative methods to re-rank the constituent parsing. In the dependency parsing, Sangati et al. (2009) used a third-order generative model for re-ranking k-best lists of base parser. Hayashi et al. (2013) used a discriminative forest re-ranking algorithm for dependency parsing. These re-ranking models achieved a substantial raise on the parsing performances. Given T (x), the set of k-best trees of a sentence x from a base parser, we use the popular mixture re-ranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which is a combination of the our model and the base parser. ˆyi = arg max αst(xi, y, Θ) + (1 − α)sb(xi, y) yET (xi) (14) where α ∈ [0, 1] is a hyperparameter; st(xi, y, Θ) an</context>
</contexts>
<marker>Sangati, Zuidema, Bod, 2009</marker>
<rawString>Federico Sangati, Willem Zuidema, and Rens Bod. 2009. A generative re-ranking model for dependency parsing. In Proceedings of the 11th International Conference on Parsing Technologies, pages 238–241. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="7944" citStr="Socher et al., 2011" startWordPosition="1308" endWordPosition="1311">ed on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations. In order to compute the score of a red bike,NP red bike,NP a,Det red,JJ bike,NN p1 = f(W L bJ c 1160 how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi: s(pi) = v - pi, (2) where v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et al. (2003) applied recursive neural networks to re-rank possible phrase attachments in an incremental constituency parser. Their work is the first to show that RNNs can capture enough information to make the correct parsing decisions. Menchetti et al. (2005) used RNNs to re-rank different constituency parses. For their results on full sentence parsing, they re-ranked candidate trees created by the Collins parser (Collins, 2003). 3 Recursive Convolutional Neural Network The dependency grammar is a widely used syntactic structure, which directly reflects relationships among the words </context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="3118" citStr="Socher et al., 2013" startWordPosition="470" endWordPosition="473">nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to repa red bike,NN Pooling Convolution a bike,NN red bike,NN a,Det red,JJ bike,NN 1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resent all level </context>
<context position="5719" citStr="Socher et al., 2013" startWordPosition="901" endWordPosition="904">asks, such as text classification. • Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a specific task, RCNN can capture the most useful semantic and structure information by the convolution and pooling layers. • When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions. The experiments on two benchmark datasets show that RCNN outperforms the state-ofthe-art models. 2 Recursive Neural Network In this section, we briefly describe the recursive neural network architecture of (Socher et al., 2013a). Figure 2: Illustration of a RNN unit. The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c). RNN can be also regarded as a general structure to model sentence. At every node in the tree, the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the whole</context>
<context position="7354" citStr="Socher et al. (2013" startWordPosition="1200" endWordPosition="1203">er et al., 2013c). Figure 2 illustrates the architecture of RNN. The binary tree can be represented in the form of branching triplets (p → c1c2). Each such triplet denotes that a parent node p has two children and each ck can be either a word or a non-terminal node in the tree. Given a labeled binary parse tree, ((p2 → ap1), (p1 → bc)), the node representations are computed by ), p2 = f (W L P1 J ), (1) where (p1, p2, a, b, c) are the vector representation of (p1, p2, a, b, c) respectively, which are denoted by lowercase bold font letters; W is a matrix of parameters of the RNN. Based on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations. In order to compute the score of a red bike,NP red bike,NP a,Det red,JJ bike,NN p1 = f(W L bJ c 1160 how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi: s(pi) = v - pi, (2) where v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et</context>
<context position="14382" citStr="Socher et al., 2013" startWordPosition="2436" endWordPosition="2439">CNN can select the useful semantic and structure information by the convolution and max pooling layers. Figure 4 shows an example of RCNN to model the sentence “I eat sashimi with chopsitcks”. 4 Parsing In order to measure the plausibility of a subtree rooted at h in dependency tree, we use a singleunit linear layer neural network to compute the score of its RCNN unit. For constituent parsing, the representation of a non-terminal node only depends on its two children. The combination is relative simple and its correctness can be measured with the final representation of the non-terminal node (Socher et al., 2013a). However for dependency parsing, all combinations of the head h and its children ci(0 &lt; i &lt; K) are important to measure the correctness of the subtree. Therefore, our score function s(h) is computed on all of hidden layers zi(0 &lt; i &lt; K): K s(h) = v(h,ci) · zi, (6) i=1 where v(h,ci) E Rm×1 is the score vector, which 1162 also depends on the POS tags of h and ci. Given a sentence x and its dependency tree y, the goodness of a complete tree is measured by summing the scores of all the RCNN units. �s(x, y, Θ) = hEy where h ∈ y is the node in tree y; Θ = {ΘW, Θv, Θw, Θd} including the combinatio</context>
<context position="26825" citStr="Socher et al. (2013" startWordPosition="4614" endWordPosition="4617">ed representation for dependency parsing. Figure 7: Example of a DT-RNN unit Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in semantic vector space for other NLP tasks. Socher et al. (2013b) proposed a compositional vectors computed by dependency tree RNN (DT-RNN) to map sentences and images into a common embedding space. However, there are two major differences as follows. 1) They first summed up all child nodes into a dense vector v, and then composed subtree representation from v, and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful f</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Q Le</author>
<author>C Manning</author>
<author>A Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2013</date>
<booktitle>In NIPS Deep Learning Workshop.</booktitle>
<contexts>
<context position="3118" citStr="Socher et al., 2013" startWordPosition="470" endWordPosition="473">nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to repa red bike,NN Pooling Convolution a bike,NN red bike,NN a,Det red,JJ bike,NN 1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resent all level </context>
<context position="5719" citStr="Socher et al., 2013" startWordPosition="901" endWordPosition="904">asks, such as text classification. • Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a specific task, RCNN can capture the most useful semantic and structure information by the convolution and pooling layers. • When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions. The experiments on two benchmark datasets show that RCNN outperforms the state-ofthe-art models. 2 Recursive Neural Network In this section, we briefly describe the recursive neural network architecture of (Socher et al., 2013a). Figure 2: Illustration of a RNN unit. The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c). RNN can be also regarded as a general structure to model sentence. At every node in the tree, the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the whole</context>
<context position="7354" citStr="Socher et al. (2013" startWordPosition="1200" endWordPosition="1203">er et al., 2013c). Figure 2 illustrates the architecture of RNN. The binary tree can be represented in the form of branching triplets (p → c1c2). Each such triplet denotes that a parent node p has two children and each ck can be either a word or a non-terminal node in the tree. Given a labeled binary parse tree, ((p2 → ap1), (p1 → bc)), the node representations are computed by ), p2 = f (W L P1 J ), (1) where (p1, p2, a, b, c) are the vector representation of (p1, p2, a, b, c) respectively, which are denoted by lowercase bold font letters; W is a matrix of parameters of the RNN. Based on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations. In order to compute the score of a red bike,NP red bike,NP a,Det red,JJ bike,NN p1 = f(W L bJ c 1160 how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi: s(pi) = v - pi, (2) where v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et</context>
<context position="14382" citStr="Socher et al., 2013" startWordPosition="2436" endWordPosition="2439">CNN can select the useful semantic and structure information by the convolution and max pooling layers. Figure 4 shows an example of RCNN to model the sentence “I eat sashimi with chopsitcks”. 4 Parsing In order to measure the plausibility of a subtree rooted at h in dependency tree, we use a singleunit linear layer neural network to compute the score of its RCNN unit. For constituent parsing, the representation of a non-terminal node only depends on its two children. The combination is relative simple and its correctness can be measured with the final representation of the non-terminal node (Socher et al., 2013a). However for dependency parsing, all combinations of the head h and its children ci(0 &lt; i &lt; K) are important to measure the correctness of the subtree. Therefore, our score function s(h) is computed on all of hidden layers zi(0 &lt; i &lt; K): K s(h) = v(h,ci) · zi, (6) i=1 where v(h,ci) E Rm×1 is the score vector, which 1162 also depends on the POS tags of h and ci. Given a sentence x and its dependency tree y, the goodness of a complete tree is measured by summing the scores of all the RCNN units. �s(x, y, Θ) = hEy where h ∈ y is the node in tree y; Θ = {ΘW, Θv, Θw, Θd} including the combinatio</context>
<context position="26825" citStr="Socher et al. (2013" startWordPosition="4614" endWordPosition="4617">ed representation for dependency parsing. Figure 7: Example of a DT-RNN unit Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in semantic vector space for other NLP tasks. Socher et al. (2013b) proposed a compositional vectors computed by dependency tree RNN (DT-RNN) to map sentences and images into a common embedding space. However, there are two major differences as follows. 1) They first summed up all child nodes into a dense vector v, and then composed subtree representation from v, and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful f</context>
</contexts>
<marker>Socher, Le, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Q Le, C Manning, and A Ng. 2013b. Grounded compositional semantics for finding and describing images with sentences. In NIPS Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3118" citStr="Socher et al., 2013" startWordPosition="470" endWordPosition="473">nd these representations are complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. These embeddings are pre-trained and keep unchanged in the training phase of parsing model, which cannot be optimized for the specific tasks. Besides, it is also important to represent the (unseen) phrases with dense vector in dependency parsing. Since the dependency tree is also in recursive structure, it is intuitive to use the recursive neural network (RNN), which is used for constituent parsing (Socher et al., 2013a). However, recursive neural network can only process the binary combination and is not suitable for dependency parsing, since a parent node may have two or more child nodes in dependency tree. In this work, we address the problem to repa red bike,NN Pooling Convolution a bike,NN red bike,NN a,Det red,JJ bike,NN 1159 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1159–1168, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resent all level </context>
<context position="5719" citStr="Socher et al., 2013" startWordPosition="901" endWordPosition="904">asks, such as text classification. • Each RCNN unit can model the complicated interactions of the head word and its children. Combined with a specific task, RCNN can capture the most useful semantic and structure information by the convolution and pooling layers. • When applied to the re-ranking model for parsing, RCNN improve the accuracy of base parser to make accurate parsing decisions. The experiments on two benchmark datasets show that RCNN outperforms the state-ofthe-art models. 2 Recursive Neural Network In this section, we briefly describe the recursive neural network architecture of (Socher et al., 2013a). Figure 2: Illustration of a RNN unit. The idea of recursive neural networks (RNN) for natural language processing (NLP) is to train a deep learning model that can be applied to phrases and sentences, which have a grammatical structure (Pollack, 1990; Socher et al., 2013c). RNN can be also regarded as a general structure to model sentence. At every node in the tree, the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the whole</context>
<context position="7354" citStr="Socher et al. (2013" startWordPosition="1200" endWordPosition="1203">er et al., 2013c). Figure 2 illustrates the architecture of RNN. The binary tree can be represented in the form of branching triplets (p → c1c2). Each such triplet denotes that a parent node p has two children and each ck can be either a word or a non-terminal node in the tree. Given a labeled binary parse tree, ((p2 → ap1), (p1 → bc)), the node representations are computed by ), p2 = f (W L P1 J ), (1) where (p1, p2, a, b, c) are the vector representation of (p1, p2, a, b, c) respectively, which are denoted by lowercase bold font letters; W is a matrix of parameters of the RNN. Based on RNN, Socher et al. (2013a) introduced a compositional vector grammar, which uses the syntactically untied weights W to learn the syntactic-semantic, compositional vector representations. In order to compute the score of a red bike,NP red bike,NP a,Det red,JJ bike,NN p1 = f(W L bJ c 1160 how plausible of a syntactic constituent a parent is, RNN uses a single-unit linear layer for all pi: s(pi) = v - pi, (2) where v is a vector of parameters that need to be trained. This score will be used to find the highest scoring tree. For more details on how standard RNN can be used for parsing, see (Socher et al., 2011). Costa et</context>
<context position="14382" citStr="Socher et al., 2013" startWordPosition="2436" endWordPosition="2439">CNN can select the useful semantic and structure information by the convolution and max pooling layers. Figure 4 shows an example of RCNN to model the sentence “I eat sashimi with chopsitcks”. 4 Parsing In order to measure the plausibility of a subtree rooted at h in dependency tree, we use a singleunit linear layer neural network to compute the score of its RCNN unit. For constituent parsing, the representation of a non-terminal node only depends on its two children. The combination is relative simple and its correctness can be measured with the final representation of the non-terminal node (Socher et al., 2013a). However for dependency parsing, all combinations of the head h and its children ci(0 &lt; i &lt; K) are important to measure the correctness of the subtree. Therefore, our score function s(h) is computed on all of hidden layers zi(0 &lt; i &lt; K): K s(h) = v(h,ci) · zi, (6) i=1 where v(h,ci) E Rm×1 is the score vector, which 1162 also depends on the POS tags of h and ci. Given a sentence x and its dependency tree y, the goodness of a complete tree is measured by summing the scores of all the RCNN units. �s(x, y, Θ) = hEy where h ∈ y is the node in tree y; Θ = {ΘW, Θv, Θw, Θd} including the combinatio</context>
<context position="26825" citStr="Socher et al. (2013" startWordPosition="4614" endWordPosition="4617">ed representation for dependency parsing. Figure 7: Example of a DT-RNN unit Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in semantic vector space for other NLP tasks. Socher et al. (2013b) proposed a compositional vectors computed by dependency tree RNN (DT-RNN) to map sentences and images into a common embedding space. However, there are two major differences as follows. 1) They first summed up all child nodes into a dense vector v, and then composed subtree representation from v, and vector parent node. In contrast, our model first combine the parent and each child and then choose the most informative features with a pooling layer. 2) We represent the relative position of each child and its parent with distributed representation (position embeddings), which is very useful f</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013c. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>Transition-based dependency parsing using recursive neural networks.</title>
<date>2013</date>
<booktitle>In NIPS Workshop on Deep Learning.</booktitle>
<contexts>
<context position="23741" citStr="Stenetorp (2013)" startWordPosition="4101" endWordPosition="4102">eebank (CTB5). The hyperparameters is the same as the previous experiment on English except that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a large number of handcrafted features, our model can achieve a competitive per</context>
<context position="26299" citStr="Stenetorp (2013)" startWordPosition="4537" endWordPosition="4538">to multiply increase for training. However, we just can obtain at most k negative samples from the k-best outputs of the base parser. The experiments also show that the our model can achieves significant improvements by adding the oracles into the output lists of the base parser. This indicates that our model can be boosted by a better set of the candidate results, which can be implemented by combining the RCNN in the decoding algorithm. 8 Related Work There have been several works to use neural networks and distributed representation for dependency parsing. Figure 7: Example of a DT-RNN unit Stenetorp (2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their methods aim to transition-based parsing and can not model the sentence in semantic vector space for other NLP tasks. Socher et al. (2013b) proposed a compositional vectors computed by dependency tree RNN (DT-RN</context>
</contexts>
<marker>Stenetorp, 2013</marker>
<rawString>Pontus Stenetorp. 2013. Transition-based dependency parsing using recursive neural networks. In NIPS Workshop on Deep Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2000" citStr="Turian et al., 2010" startWordPosition="299" endWordPosition="302">s generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illustration of a RCNN unit. natural language processing (NLP) tasks, such as syntax (Turian et al., 2010; Mikolov et al., 2010; Collobert et al., 2011; Chen and Manning, 2014) and semantics (Huang et al., 2012; Mikolov et al., 2013). Distributed representations are to represent words (or phrase) by the dense, low-dimensional and real-valued vectors, which help address the curse of dimensionality and have better generalization than discrete representations. For dependency parsing, Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are complementary to the traditional discrete feature representation. Howeve</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies (IWPT),</booktitle>
<volume>3</volume>
<contexts>
<context position="1300" citStr="Yamada and Matsumoto, 2003" startWordPosition="189" endWordPosition="192">ent with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a k-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets. 1 Introduction Feature-based discriminative supervised models have achieved much progress in dependency parsing (Nivre, 2004; Yamada and Matsumoto, 2003; McDonald et al., 2005), which typically use millions of discrete binary features generated from a limited size training data. However, the ability of these models is restricted by the design of features. The number of features could be so large that the result models are too complicated for practical use and prone to overfit on training corpus due to data sparseness. Recently, many methods are proposed to learn various distributed representations on both syntax and semantics levels. These distributed representations have been extensively applied on many ∗Corresponding author. Figure 1: Illus</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the International Workshop on Parsing Technologies (IWPT), volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19085" citStr="Zhang and Clark, 2008" startWordPosition="3295" endWordPosition="3298">ation and compare our model against the other state-of-the-art methods using the unlabeled attachment score (UAS) metric ignoring punctuation. English For English dataset, we follow the standard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as development set and section 23 as test set. We tag the development and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to (Collins and Koo, 2005). Chinese For Chinese dataset, we follow the same split of the Penn Chinese Treeban (CTB5) as described in (Zhang and Clark, 2008) and use sections 001-815, 1001-1136 as training set, sections 886-931, 1148- 1151 as development set, and sections 816-885, 1137-1147 as test set. Dependencies are converted by using the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. We use the linear-time incremental parser (Huang and Sagae, 2010) as our base parser and calculate the 64-best parses at the top cell of the chart. Note that we optimize the training settings for base parser and the results a</context>
<context position="21873" citStr="Zhang and Clark (2008)" startWordPosition="3761" endWordPosition="3764"> the RCNN and base models. Figure 6 shows the accuracies on the top ten POS tags of the modifier words with the largest improvements. We can see that our re-ranker can improve the accuracies of CC and IN, and therefore may indirectly result in rising the the well-known coordinating conjunction and PPattachment problems. The final experimental results on test set are shown in Table 1. The hyperparameters of our model are set as in Table 2. Our re-ranker achieves the maximum improvement of 93.83%(+1.48%) on test set. Our system performs slightly better than many state-of-the-art systems such as Zhang and Clark (2008) and Huang and Sagae (2010). It outperforms Hayashi et al. (2013) and Le and Zuidema (2014), which also use the mixture reranking strategy. Since the result of ranker is conditioned to kbest results of base parser, we also do an experiment to avoid this limitation by adding the oracle to k-best candidates. With including oracle, the re-ranker can achieve 94.16% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 1. 1164 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 97 96 95 94 93 92 Oracle Best RCNN Re-ranker 1 2 3 4 5 6 7 8 9 10 32 64 k UAS(%) 95 90 85 80 75 Oracle Best RCN</context>
<context position="23663" citStr="Zhang and Clark (2008)" startWordPosition="4089" endWordPosition="4092">development set. 7.3 Chinese Dataset We also make experiments on the Penn Chinese Treebank (CTB5). The hyperparameters is the same as the previous experiment on English except that α is optimised by searching with the step-size 0.005. The final experimental results on the test set are shown in Table 3. Our re-ranker achieves the performance of 85.71%(+0.25%) on the test set, which also outperforms the previous state-of-theart methods. With adding oracle, the re-ranker can achieve 87.43% on UAS, which is shown in the last line (“our re-ranker (with oracle)”) of Table 3. UAS Traditional Methods Zhang and Clark (2008) 91.4 Huang and Sagae (2010) 92.1 Distributed Representations Stenetorp (2013) 86.25 Chen et al. (2014) 93.74 Chen and Manning (2014) 92.0 Re-rankers Hayashi et al. (2013) 93.12 Le and Zuidema (2014) 93.12 Our baseline 92.35 Our re-ranker 93.83(+1.48) Our re-ranker (with oracle) 94.16 Table 1: Accuracy on English test set. Our baseline is the result of base parser; our re-ranker uses the mixture strategy on the 64-best outputs of base parser; our re-ranker(with oracle) is to add the oracle to k-best outputs of base parser. Compared with the re-ranking model of Hayashi et al. (2013), that use a</context>
<context position="24890" citStr="Zhang and Clark (2008)" startWordPosition="4298" endWordPosition="4301">e number of handcrafted features, our model can achieve a competitive performance with the minimal feature engineering. 7.4 Discussions The performance of the re-ranking model is affected by the base parser. The small divergence of the dependency trees in the output list also results to overfitting in training phase. Although our reUAS(%) 95 90 85 80 75 Base Paser Re-ranker 1165 Word embedding size m = 25 Distance embedding size md = 25 Initial learning rate p = 0.1 Margin loss discount κ = 2.0 Regularization A = 10−4 k-best k = 64 Table 2: Hyperparameters of our model UAS Traditional Methods Zhang and Clark (2008) 84.33 Huang and Sagae (2010) 85.20 Distributed Representations Chen et al. (2014) 82.94 Chen and Manning (2014) 83.9 Re-rankers Hayashi et al. (2013) 85.9 Our baseline 85.46 Our re-ranker 85.71(+0.25) Our re-ranker (with oracle) 87.43 Table 3: Accuracy on Chinese test set. ranker outperforms the state-of-the-art methods, it can also benefit from improving the quality of the candidate results. It was also reported in other reranking works that a larger k (eg. k &gt; 64) results the worse performance. We think the reason is that the oracle best increases when k is larger, but the oracle worst decr</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 562–571. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19405" citStr="Zhang and Nivre, 2011" startWordPosition="3347" endWordPosition="3350"> tag the development and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to (Collins and Koo, 2005). Chinese For Chinese dataset, we follow the same split of the Penn Chinese Treeban (CTB5) as described in (Zhang and Clark, 2008) and use sections 001-815, 1001-1136 as training set, sections 886-931, 1148- 1151 as development set, and sections 816-885, 1137-1147 as test set. Dependencies are converted by using the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). And following (Zhang and Clark, 2008) (Zhang and Nivre, 2011), we use gold segmentation and POS tags for the input. We use the linear-time incremental parser (Huang and Sagae, 2010) as our base parser and calculate the 64-best parses at the top cell of the chart. Note that we optimize the training settings for base parser and the results are slightly improved on (Huang and Sagae, 2010). Then we use max-margin criterion to train RCNN. Finally, we use the mixture strategy to re-rank the top 64-best parses. For initialization of parameters, we train word2vec embeddings (Mikolov et al., 2013) on Wikipedia corpus for English and Chinese respectively. For the</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 188–193. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>