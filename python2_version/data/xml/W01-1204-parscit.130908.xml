<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000645">
<title confidence="0.968742">
A Statistical Method for Short Answer Extraction
</title>
<author confidence="0.996173">
Gideon S. Mann
</author>
<affiliation confidence="0.9294635">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.881505">
Baltimore, Maryland 21218
</address>
<email confidence="0.997565">
gsm@cs. jhu.edu
</email>
<sectionHeader confidence="0.995604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909636363637">
This paper presents a simple, general
method for using the Mutual Informa-
tion (MI) statistic trained on unanno-
tated trivia questions to estimate ques-
tion class/semantic tag correlation. This
MI method and a variety of question
classifiers and semantic taggers are used
to build short-answer extractors that
show improvement over a hand-built
match module using a similar question
classifier and semantic tagger.
</bodyText>
<sectionHeader confidence="0.997907" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971857142857">
Many of the recent question answering systems
integrate statistical NLP/IR tools with a hand-
crafted component, a question class/semantic tag
(QC/ST) match module (Prager et al., 1999),
(Breck et al., 1999). Hovy et al. (2001) describes
a parsing method for learning QC/ST match. It-
tycheriah et al. (2001) trains on trivia questions
annotated with the semantic tag of the answer
to build a Maximum Entropy model which pre-
dicts semantic tags given a question. When the
Max-Ent model is used, the estimated probabili-
ties are thrown out and only the most likely tag is
returned.
This paper presents a novel method for learning
QC/ST correlation from unannotated data. The
method introduced is based on the Mutual Infor-
mation (MI) statistic (Section 2) and is trained
on a trivia question database (Section 3) using a
question classifier and semantic tagger.
The MI method is general in that it can be ap-
plied to a wide variety of question classifiers and
semantic taggers. In this paper we examine a few
different methods questions classifiers and seman-
tic taggers described in Figure 1.
This MI QC/ST match module, along with a
question classifier and semantic tagger, can func-
tion as a short answer extractor that selects a
short answer from a sentence given a question.
</bodyText>
<listItem confidence="0.979174181818182">
Question Classifiers:
1. U : a simple initial unigram model
2. UH : a slightly more complicated model
that combines initial unigram and wh-phrase
heads
3. Qgrok : a hand-built question typing mecha-
nism (Breck et al., 1999)
Semantic Taggers:
1. NE : Phrag, a HMM Named Entity Tagger
(Breck et al., 1999)
2. WN : WordNet (Miller, 1990)
</listItem>
<figureCaption confidence="0.9661325">
Figure 1: Question Classifiers and Semantic Tag-
gers Investigated
</figureCaption>
<bodyText confidence="0.9999715">
To simplify the problem, we make the assump-
tion that all answers are strictly one word in
lengthl. Even so, this task is non-trivial and rele-
vant especially in the case of trivia questions where
most of the answers are only one or two words
long. The disparity of performance in the 50-byte
and 250-byte TREC-8 Question Answering eval-
uations (Voorhees, 1999) gives further evidence
that extracting a shorter, multi-word answer from
a longer, sentence length answer is a task worthy
of consideration in its own right. We empirically
test the performance of the short answer extrac-
tion on a held-out set of trivia questions and com-
pare with a number of baseline systems including
a hand-crafted system that uses a similar question
classifier and semantic tagger (Section 4).
</bodyText>
<sectionHeader confidence="0.392341" genericHeader="introduction">
2 MI as an Estimator for Question
Class/Semantic Tag Correlation
</sectionHeader>
<bodyText confidence="0.787047285714286">
A simple approach to building a question answer-
ing system would be to (1) collect a huge database
of questions and answers, and (2) when a question
is asked, look for it in the database and return the
&apos;Recent experiments have used a Base NP tagger
to extract full phrases given the best one-word answer
but a thorough evaluation has not been completed.
</bodyText>
<figure confidence="0.9941202">
defaultnp
sentence
thingname
explanation
title
event
temporal
definition
statement
agent
time date
defaultvp
ambighow
comparison
location
number
organization person
city
age measure
quantity
duration
money
planet
country
province
persondesc personname
ambigbig
frequency
area
length mass
</figure>
<bodyText confidence="0.999642166666667">
This derivation shows that if we compute (8) we
approximate MI(Q,W) given the independence as-
sumptions above. In effect, we have learned how
predictive a QC/ST pair is. Table 1 gives an ex-
ample of the type of information learned by the
MI model.
</bodyText>
<subsectionHeader confidence="0.9949485">
2.3 Estimating the MI Model from
Unan.notated Data
</subsectionHeader>
<bodyText confidence="0.99829425">
Estimating the above probabilities can be done
with a trivia database that contains a large num-
ber of questions and answers. The method is the
following:
</bodyText>
<listItem confidence="0.999462545454545">
• For each question identify the question class.
• Apply the semantic tagger to the trivia
database to generate Pr(STIW). Alterna-
tively, tag a very large corpus to generate high
precision priors, and ensure that answers in
the training data are tagged at least once. If
high quality priors are available, as outputs
from an HMM for example, they also might
be able to be used as fractional counts to es-
timate the probabilities3.
• Estimate
</listItem>
<equation confidence="0.993719">
Pr(STIQC) = E Pr(STIW)Pr(WIQC)
w
</equation>
<bodyText confidence="0.999955416666667">
As stated above, this estimation method makes
the assumption, expressed in equation (3), that
for each question there will only be one question
class. For most current Q/A systems this is the
case. Perhaps future systems will have more so-
phisticated question classifiers that assign a prob-
ability to a number of question classes. To ac-
comodate increased sophistication, these formulas
will change slightly, but the general method may
still be applicable.
When this MI method is trained by the above
method, it takes into account the actual perfor-
mance of the semantic tagger on data. Ittycheriah
et al. (2001) builds a statistical model on anno-
tated data which predicts semantic tag from the
question and notes that improvement in this pre-
diction does not necessarily lead to higher perfor-
mance, since there is a complex interaction be-
tween this module and the semantic tagger and
answer selector. One advantage of training on
unannotated text using the MI approach is that
the correlation between the question class and the
performance of the semantic tagger is explicitly
modeled.
</bodyText>
<footnote confidence="0.897464333333333">
31n this paper, even though Phrag is a HMM, its
priors were unavailable, so it was treated as a non-
statistical tagger
</footnote>
<table confidence="0.999745269230769">
Q Class Sem Tag MI(QC,ST)
In Country Country 39.605
Location 3.683
City 3.562
Organization 1.572
Name 1.116
Person 0.175
Other 0.012
Date 0.028
Noun Group 0.005
Who Person 6.166
Location 2.186
Name 1.174
Organization 1.333
Country 0.583
Date 0.402
Time 0.311
Title 0.210
City 0.198
Age 0.180
Volume 0.130
Noun Group 0.021
Other 0.010
Duration 0.011
Quantity 0.039
Length 0.007
</table>
<tableCaption confidence="0.962576">
Table 1: Examples of MI(QC,ST) induced from
the Phishy (PH) trivia database
</tableCaption>
<sectionHeader confidence="0.969602" genericHeader="method">
3 Trivia Questions
</sectionHeader>
<bodyText confidence="0.99994152173913">
With the advent of the Internet, trivia games are
becoming big business. The general public sub-
mits questions, and trivia game companies award
prizes to those who correctly answer those ques-
tions. Some of these trivia databases are quite
large, reaching nearly two hundred thousand trivia
questions (Ford).
In this paper we use two trivia databases as
main resources : &amp;quot;Phishy&amp;quot; or PH (MacDonald)
and &amp;quot;Triviaspot&amp;quot; or TS (Trivia Machine Inc.). PH
has approximately 5k questions, each with the cor-
rect answer. TS is larger, but only a small part
(11k questions) is currently available to us for re-
search. Each TS database entry, along with the
correct answer, includes three wrong answers and
in many cases an &amp;quot;explanation&amp;quot;. The explanations
in TS vary in content. Some are, in fact, justifica-
tions for the answer (as in Figure 3). Others pro-
vide additional information for those interested in
the topic of the question (e.g. &amp;quot;Leonardo Da Vinci
described ideas for contact lenses in 1508.&amp;quot;) or for
those upset at answering wrong (e.g. &amp;quot;Franklin
wore glasses, but didn&apos;t invent them.&amp;quot;). Both
</bodyText>
<figure confidence="0.995533785714286">
Start with
Question /
Answer pairs
Perform IR
Query to Find
Sentences
Containing
Term Answer
Pick Sentences
&amp;quot;Most Related&amp;quot;
to Question
Noisy Training Data
Build Statistical
Model
</figure>
<bodyText confidence="0.996389055555556">
3k questions. Of course the explanations were
quite noisy. Some, though they included the short
answer terms, did not have enough information to
conclude that the short answer in fact answered
the question. Many explanations were ungram-
matical and many were odd mixtures of sentence
fragments and random punctuation. Since the ex-
planations were collected automatically, in some
cases we found multiple explanations for the same
question. The results reported for PH in the
next section report scores computed over all ques-
tion/explanations pairs.
We tested the efficacy of the MI model by ex-
amining the performance of this model at selecting
a one-word answer from the web explanations for
PH and from the explanations provided by TS.
Our experimental method for testing the statis-
tical method was as follows:
</bodyText>
<listItem confidence="0.9854275">
1. Divide the questions into a testing and train-
ing set (Table 3). Tokenize all explanations
using a text chunker (Florian and Ngai 2001)
and ensure that those in the test set contains
an answer.
2. Run the question classifier and semantic tag-
ger over the training set. NE-U refers to
the first MI system tested, where the seman-
tic tagger = Phrag, and question classifier =
Initial Unigram. In training, throw out uni-
grams that don&apos;t occur more then ten times
in the data. Estimate a back-off &amp;quot;all&amp;quot; as
</listItem>
<sectionHeader confidence="0.422019" genericHeader="method">
EQ EwEcorrect P(STIMP(W IQ)
EQ,E Ew P(sTIMP(w1Q)
</sectionHeader>
<bodyText confidence="0.990845">
Note that the numerator is P(STIanswer),
and the denominator is P(ST).
</bodyText>
<listItem confidence="0.95248">
3. Estimate MI(ST, QC) from the training set
as described in Section 2.
4. For each question in the test set run the ques-
tion classifier to determine the question class,
backing off to &amp;quot;all&amp;quot; in the case of unseen un-
igrams. Use the semantic tagger to assign a
distribution Pr(STIW) to each word in the
explanation. Rank all non-stop words in the
sentence according to :
</listItem>
<bodyText confidence="0.785333333333333">
MI(Q, W) = E MI(ST, QC)Pr(STIW)
ST
This process is depicted in Figure 5 below.
</bodyText>
<listItem confidence="0.941965">
5. Evaluate using two methods :
(a) correctness of the top ranked answer
(correct)
(b) the reciprocal answer rank (rar) as used
</listItem>
<bodyText confidence="0.7185955">
in TREC, (1/rank of first correct answer)
for the top 5 answers
</bodyText>
<table confidence="0.987884">
#training #testing avg ans
questions questions length
PH 3,105 1,714 1.36
TS 6,681 4,275 1.79
</table>
<tableCaption confidence="0.99983">
Table 3: Statistics of Trivia Databases PH &amp; TS
</tableCaption>
<bodyText confidence="0.999909875">
We compared the performance of this system
with a number of baselines. The most naive
method was Random which is the expected per-
formance of a system that picks a word at ran-
dom within the sentence (excluding stop words).
We also tried Word Order, which ranked words
by their position in the sentence (i.e. first word
ranked first).
</bodyText>
<sectionHeader confidence="0.997724" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999378857142857">
Vanilla used Qgrok and Phrag as question classi-
fier and semantic tagger respectively and a hand-
crafted match module to detect matches. Qgrok
is only slightly more complicated than the initial
unigram method. NE-U is the first MI model,
which uses Phrag as a semantic tagger and initial
unigrams as a method of question classification.
</bodyText>
<table confidence="0.999066333333333">
PH TS
Correct Rar Correct Rar
Random .108 .170
Word Order .150 .272 .440 .548
Vanilla .163 .282 .350 .480
NE-U .315 .478 .414 .571
</table>
<tableCaption confidence="0.999691">
Table 4: Peformance of Baselines vs. NE-U
</tableCaption>
<bodyText confidence="0.999002272727273">
Table 4 shows the results of this experiment.
The improvement in both absolute correct and
in absolute reciprocal answer rank achieved by
NE over the baseline Vanilla system is surprising.
Both systems use very similar question and se-
mantic taggers, with the largest difference being
that NE-U uses a Pr(STIW) distribution, while
Vanilla chooses argmaxsTPr(STIW) as computed
by Phrag. The question classification mechanism
used in Vanilla is more complex than that used
in this system (though this might be a source of
problems — see Section 4.2).
Another striking result from these experiments
is the impressive performance that is achieved by
using Word Order on the TS database. The fact
that this method can do so well illustrates that the
explanations in TS are unnatural — most of them
start with the correct answer. This information
was not used in the rest of the paper or in any
of the models, since it is a property of this spe-
cific database of questions, as the results on PH
demonstrate.
</bodyText>
<table confidence="0.769489714285714">
Question (&amp;quot;Who invented eyeglasses&amp;quot;) Explanation (&amp;quot;Marco Polo reported seeing many .. &amp;quot;)
Semantic Tagger
Question Classifier
Question Class (&amp;quot;Who&amp;quot;)
Tagged Sentence(&amp;quot;Marco/[Person 1] Polo/[Person .45],[Org .5],[Loc .05] ...)
Question Class /Semantic Tag Correlation
Ranked Words (&amp;quot;Marco/.4 Polo/.3 Chinese/.2 West/.1...)
</table>
<bodyText confidence="0.9986775">
It used not only the initial sentence unigram, but
also the head word of the initial wh-phrase (when
it could find one), again modeling these types only
when they occured more than ten times in the
training corpus. As a result of this process, it&apos;s
possible to create a question class that hasn&apos;t ap-
peared yet in data. In these cases, the model
backed-off to the initial unigram or to &amp;quot;all&amp;quot;. This
back-off was not optimal, and future investigations
into more effective methods may be profitable.
The results from these comparisons are listed in
Table 6. The performance on each of the test sets
is strikingly different. On PH, the Unigram+Head
(UH) method achieves the best performance, while
on TS Qgrok does. One explanation behind these
differences might be found in Table 2, which shows
that PH has a much higher concentration in fewer
initial unigrams (.881) which TS is more varied
(.802). This might explain why Qgrok, which
does more complex question classification achieves
a bigger benefit in TS than UH. These results sug-
gest that while simple question classification works
reasonably well for simply phrased questions, it
degrades with more complicated phrasing.
</bodyText>
<table confidence="0.9992406">
PH TS
NE CB NE CB
Qgrok .292 .299 .427 .433
Unigram .315 .321 .414 .424
Unigram + Head .328 .345 .408 .418
</table>
<tableCaption confidence="0.989997">
Table 6: Effect of Different Question Classifiers
with Respect to Overall Performance
</tableCaption>
<subsectionHeader confidence="0.999372">
4.3 Using Wrong Answers
</subsectionHeader>
<bodyText confidence="0.999774166666667">
One final piece of information in some trivia ques-
tion databases is a set of wrong answers for each
question. In multiple choice trivia questions, typi-
cally the correct answers and incorrect answers all
could be possible responses to the question or else
a contestant would be able to answer the question
without any other knowledge. We assume that one
of the main similarities is that all answers have
a semantic tag that is highly correlated with the
question. In other words, the tag of the wrong
answers should be a possible tag expected by the
question, and thus should help estimation of the
question class/semantic (QC/ST) tag correlation.
We tested this hypothesis by using these wrong
answers to estimate MI(ST,QC), and seeing if in-
deed they improve performance. For these experi-
ments, we took the training questions, correct an-
swers (C) and incorrect answers (I) and recom-
</bodyText>
<equation confidence="0.966012">
puted Pr(STIQC) using:
Pr(STIQC) = E Pr(STIW)Pr(WIQC)
WEICull
</equation>
<bodyText confidence="0.999859">
For these experiments we used only the TS
database, since PH did not contain wrong an-
swers. Table 7 shows an improvement in perfor-
mance with the use of wrong answers in estimating
MI(ST,QC), though not a large one.
</bodyText>
<table confidence="0.9997192">
QC ST plain +wrong
Unigram NE .414 .424
CB .423 .433
Unigram + Head NE .408 .413
CB .418 .423
</table>
<tableCaption confidence="0.984655">
Table 7: Effect of Using Wrong Answers As Ad-
ditional Training (TS)
</tableCaption>
<subsectionHeader confidence="0.845464">
4.4 Probing the Open Domain Nature of
Trivia Questions
</subsectionHeader>
<bodyText confidence="0.998561444444444">
One important question is to what degree mod-
els learned from trivia question databases can be
broadly applied to question answering in general.
To begin an answer to this question, we examine
how well models learned on one trivia database
can be used on another unrelated one. We built
models from PH and TS, exchanged Pr(STIQC)
models, and re-tested. With this replacement, we
expect a degradation solely due to QC/ST correla-
tion differences. We did not replace the other two
models since if one was given a new set of questions
and a corpus of sentences, Pr(STIW) and Pr(ST)
could be calculated for that domain without know-
ing the correlation between questions and answers.
Formally, we defined PrPH to be probailities esti-
mated from the PH corpus, and PrTs to be those
estimated on TS. The model we normally test on
PH is built by:
</bodyText>
<figure confidence="0.726901555555556">
MIPH (Q,W) =
PrPH (STIW)PrPH (STIQC)
E
ST PrPH (ST)
Instead in this model, we computed
PrPH (STIW) PrTs (STIQC)
MI- XP H (Q , W) = E
PrPH (ST)
ST
</figure>
<bodyText confidence="0.9982395">
The performance detailed in Table 8 shows
that the degradation is minimal when models are
shared across two trivia databases. This result
suggests either that the trivia questions, at least
to a first approximation, are very similar or that
what these models have learned is a general phe-
nomena of question answering. Which hypothesis
is correct is left to future research.
</bodyText>
<table confidence="0.99817475">
PH TS
cor rar cor rar
MI .315 .478 .414 .571
MI-X .321 .461 .386 .549
</table>
<tableCaption confidence="0.904904666666667">
Table 8: Performance when Pr(STIQC) is ex-
changed, using Unigram Question Classes, and NE
Semantic Tagging
</tableCaption>
<sectionHeader confidence="0.996085" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999695227272727">
In this paper we examined a component of Q/A
systems which is often over-looked : the compo-
nent which measures fit between question classes
and semantic tags. We described a novel way to
use the mutual information statistic and an unan-
notated corpus to automatically induce correla-
tions between semantic tags and question classes.
We have shown that wrong answers can help
improve performance and that different seman-
tic taggers can be combined to improve perfor-
mance. The MI statistic as presented here can be
used as &amp;quot;glue&amp;quot; to combine a variety of question
class/semantic tag components, and as such it is
of general usefulness to the Q/A community.
The similarity between trivia databases, shown
by cross-training experiments, is another interest-
ing result. Although it does not prove that trivia
questions constitute an open domain, it suggests
that trivia questions are at least a self-consistent
domain.
We have shown that another component of a
Q/A system can be built statistically to yield nice
performance when even simple statistics are used.
This prompts the question : what other compo-
nents can be built statistically? Already some
components are consistently built by statistical
methods (semantic taggers, information retrieval
engines), yet some remain predominately hand-
crafted (e.g. question classifiers — with Ittycheriah
et al. (2001) as an exception), and thus are prime
targets for statistical methods.
This paper demonstrated performance on ex-
tracting one-word answers, but this method can
be extended to extracting multiple words. We
built a system which chooses the base noun phrase
containing the highest ranked word, but have not
completed evaluation.
Aside from its use in question answering, once
the short answer extraction task can be performed
with high precision, systems will be able to ex-
tract facts from heterogenous text. This will be
an enabling technology which will allow integra-
tion with symbolic processing systems to allow for
complex natural language understanding.
</bodyText>
<sectionHeader confidence="0.982998" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999812333333333">
The author would like to thank Ellen Riloff, John
Hale and Jun Wu for their insightful and helpful
comments and suggestions and John Burger, Marc
Light, Eric Breck, Inderjeet Mani, and Lynette
Hirshman for providing Phrag, Qgrok and the var-
ious question corpera. Thanks are also due to
Grant MacDonald, and to Tiffany Kosel, Hazel
Kight, and Trivia Machine Incorporated for the
trivia questions used in these experiments.
</bodyText>
<sectionHeader confidence="0.997925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999327357142857">
E. Breck, J. Burger, L. Ferro, D. House, M. Light,
and I. Manni. 1999. A sys called qanda. Proc. of
TREC-8.
E. Breck, M. Light, G. Mann, E. Riloff, B. Brown,
P. Anand, M. Rooth, and M. Thelen. 2001. Look-
ing under the hood: Tools for diagnosing your ques-
tion answering engine. ACL Workshop on Open-
Domain Question Answering.
N. Chinchor, E. Brown, L. Ferro, and P. Robinson.
1999. 1999 named entity recognition task definition.
Tech Report.
G. MacDonald (ed). Phishy web trivia.
www.phishy.net/trivia.
T. Ford (ed). Fun trivia.com : the trivia portal.
www.funtrivia.corn.
R. Florian and G. Ngai. 2001. Multidimensional
transformation-based learning. Conference on Nat-
ural Language Learning.
W. Francis and H. Kucera. 1982. Frequency Analysis
of English Usage. Houghton Mifflin, Boston, MA.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and
P. Mor. 2000. Falcon : Boosting knowledge for
answer engines. Proc. of TREC-9.
E. Hovy, L. Gerber, U. Hermjakob, C-Y. Lin, and
D. Ravichandran. 2001. Towards semantics-based
answer pinpointing. Human Language Technologies
2001.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratna-
parkhi. 2001. Question answering using maximum
entropy components. Proc. of NAACL.
G. Miller. 1990. An on-line lexical database. Interna-
tional Journal of Lexicography, 3(4).
J. Prager, D. Radev, E. Brown, A. Coden, and
V. Samn. 1999. The use of predictive annotation
for question answering in trec8. Proc. of TREC-8.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. Proceedings
of IJCAI.
TriviaMachine Inc. Triviaspot.com.
E. Voorhees. 1999. The trec-8 question answering
track report. Proc. of TREC-8.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540376">
<title confidence="0.999925">A Statistical Method for Short Answer Extraction</title>
<author confidence="0.999846">S Gideon</author>
<affiliation confidence="0.7767355">Department of Computer Johns Hopkins</affiliation>
<address confidence="0.999166">Baltimore, Maryland</address>
<email confidence="0.998734">gsm@cs.jhu.edu</email>
<abstract confidence="0.99819825">This paper presents a simple, general method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation. This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>J Burger</author>
<author>L Ferro</author>
<author>D House</author>
<author>M Light</author>
<author>I Manni</author>
</authors>
<title>A sys called qanda.</title>
<date>1999</date>
<booktitle>Proc. of TREC-8.</booktitle>
<contexts>
<context position="803" citStr="Breck et al., 1999" startWordPosition="114" endWordPosition="117">esents a simple, general method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation. This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger. 1 Introduction Many of the recent question answering systems integrate statistical NLP/IR tools with a handcrafted component, a question class/semantic tag (QC/ST) match module (Prager et al., 1999), (Breck et al., 1999). Hovy et al. (2001) describes a parsing method for learning QC/ST match. Ittycheriah et al. (2001) trains on trivia questions annotated with the semantic tag of the answer to build a Maximum Entropy model which predicts semantic tags given a question. When the Max-Ent model is used, the estimated probabilities are thrown out and only the most likely tag is returned. This paper presents a novel method for learning QC/ST correlation from unannotated data. The method introduced is based on the Mutual Information (MI) statistic (Section 2) and is trained on a trivia question database (Section 3) </context>
<context position="2089" citStr="Breck et al., 1999" startWordPosition="333" endWordPosition="336">eneral in that it can be applied to a wide variety of question classifiers and semantic taggers. In this paper we examine a few different methods questions classifiers and semantic taggers described in Figure 1. This MI QC/ST match module, along with a question classifier and semantic tagger, can function as a short answer extractor that selects a short answer from a sentence given a question. Question Classifiers: 1. U : a simple initial unigram model 2. UH : a slightly more complicated model that combines initial unigram and wh-phrase heads 3. Qgrok : a hand-built question typing mechanism (Breck et al., 1999) Semantic Taggers: 1. NE : Phrag, a HMM Named Entity Tagger (Breck et al., 1999) 2. WN : WordNet (Miller, 1990) Figure 1: Question Classifiers and Semantic Taggers Investigated To simplify the problem, we make the assumption that all answers are strictly one word in lengthl. Even so, this task is non-trivial and relevant especially in the case of trivia questions where most of the answers are only one or two words long. The disparity of performance in the 50-byte and 250-byte TREC-8 Question Answering evaluations (Voorhees, 1999) gives further evidence that extracting a shorter, multi-word ans</context>
</contexts>
<marker>Breck, Burger, Ferro, House, Light, Manni, 1999</marker>
<rawString>E. Breck, J. Burger, L. Ferro, D. House, M. Light, and I. Manni. 1999. A sys called qanda. Proc. of TREC-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>M Light</author>
<author>G Mann</author>
<author>E Riloff</author>
<author>B Brown</author>
<author>P Anand</author>
<author>M Rooth</author>
<author>M Thelen</author>
</authors>
<title>Looking under the hood: Tools for diagnosing your question answering engine. ACL Workshop on OpenDomain Question Answering.</title>
<date>2001</date>
<marker>Breck, Light, Mann, Riloff, Brown, Anand, Rooth, Thelen, 2001</marker>
<rawString>E. Breck, M. Light, G. Mann, E. Riloff, B. Brown, P. Anand, M. Rooth, and M. Thelen. 2001. Looking under the hood: Tools for diagnosing your question answering engine. ACL Workshop on OpenDomain Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>E Brown</author>
<author>L Ferro</author>
<author>P Robinson</author>
</authors>
<title>named entity recognition task definition.</title>
<date>1999</date>
<tech>Tech Report.</tech>
<marker>Chinchor, Brown, Ferro, Robinson, 1999</marker>
<rawString>N. Chinchor, E. Brown, L. Ferro, and P. Robinson. 1999. 1999 named entity recognition task definition. Tech Report.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G MacDonald</author>
</authors>
<note>Phishy web trivia. www.phishy.net/trivia.</note>
<marker>MacDonald, </marker>
<rawString>G. MacDonald (ed). Phishy web trivia. www.phishy.net/trivia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Ford</author>
</authors>
<title>Fun trivia.com : the trivia portal.</title>
<note>www.funtrivia.corn.</note>
<marker>Ford, </marker>
<rawString>T. Ford (ed). Fun trivia.com : the trivia portal. www.funtrivia.corn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>G Ngai</author>
</authors>
<date>2001</date>
<booktitle>Multidimensional transformation-based learning. Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="8556" citStr="Florian and Ngai 2001" startWordPosition="1394" endWordPosition="1397">e explanations were collected automatically, in some cases we found multiple explanations for the same question. The results reported for PH in the next section report scores computed over all question/explanations pairs. We tested the efficacy of the MI model by examining the performance of this model at selecting a one-word answer from the web explanations for PH and from the explanations provided by TS. Our experimental method for testing the statistical method was as follows: 1. Divide the questions into a testing and training set (Table 3). Tokenize all explanations using a text chunker (Florian and Ngai 2001) and ensure that those in the test set contains an answer. 2. Run the question classifier and semantic tagger over the training set. NE-U refers to the first MI system tested, where the semantic tagger = Phrag, and question classifier = Initial Unigram. In training, throw out unigrams that don&apos;t occur more then ten times in the data. Estimate a back-off &amp;quot;all&amp;quot; as EQ EwEcorrect P(STIMP(W IQ) EQ,E Ew P(sTIMP(w1Q) Note that the numerator is P(STIanswer), and the denominator is P(ST). 3. Estimate MI(ST, QC) from the training set as described in Section 2. 4. For each question in the test set run th</context>
</contexts>
<marker>Florian, Ngai, 2001</marker>
<rawString>R. Florian and G. Ngai. 2001. Multidimensional transformation-based learning. Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Francis</author>
<author>H Kucera</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<location>Houghton Mifflin, Boston, MA.</location>
<marker>Francis, Kucera, 1982</marker>
<rawString>W. Francis and H. Kucera. 1982. Frequency Analysis of English Usage. Houghton Mifflin, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>M Surdeanu</author>
<author>R Bunescu</author>
<author>R Girju</author>
<author>V Rus</author>
<author>P Mor</author>
</authors>
<title>Falcon : Boosting knowledge for answer engines.</title>
<date>2000</date>
<booktitle>Proc. of TREC-9.</booktitle>
<marker>Harabagiu, Moldovan, Pasca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Mor, 2000</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P. Mor. 2000. Falcon : Boosting knowledge for answer engines. Proc. of TREC-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>L Gerber</author>
<author>U Hermjakob</author>
<author>C-Y Lin</author>
<author>D Ravichandran</author>
</authors>
<title>Towards semantics-based answer pinpointing. Human Language Technologies</title>
<date>2001</date>
<contexts>
<context position="823" citStr="Hovy et al. (2001)" startWordPosition="118" endWordPosition="121">ral method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation. This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger. 1 Introduction Many of the recent question answering systems integrate statistical NLP/IR tools with a handcrafted component, a question class/semantic tag (QC/ST) match module (Prager et al., 1999), (Breck et al., 1999). Hovy et al. (2001) describes a parsing method for learning QC/ST match. Ittycheriah et al. (2001) trains on trivia questions annotated with the semantic tag of the answer to build a Maximum Entropy model which predicts semantic tags given a question. When the Max-Ent model is used, the estimated probabilities are thrown out and only the most likely tag is returned. This paper presents a novel method for learning QC/ST correlation from unannotated data. The method introduced is based on the Mutual Information (MI) statistic (Section 2) and is trained on a trivia question database (Section 3) using a question cla</context>
</contexts>
<marker>Hovy, Gerber, Hermjakob, Lin, Ravichandran, 2001</marker>
<rawString>E. Hovy, L. Gerber, U. Hermjakob, C-Y. Lin, and D. Ravichandran. 2001. Towards semantics-based answer pinpointing. Human Language Technologies 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W Zhu</author>
<author>A Ratnaparkhi</author>
</authors>
<title>Question answering using maximum entropy components.</title>
<date>2001</date>
<booktitle>Proc. of NAACL.</booktitle>
<contexts>
<context position="902" citStr="Ittycheriah et al. (2001)" startWordPosition="130" endWordPosition="134">annotated trivia questions to estimate question class/semantic tag correlation. This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger. 1 Introduction Many of the recent question answering systems integrate statistical NLP/IR tools with a handcrafted component, a question class/semantic tag (QC/ST) match module (Prager et al., 1999), (Breck et al., 1999). Hovy et al. (2001) describes a parsing method for learning QC/ST match. Ittycheriah et al. (2001) trains on trivia questions annotated with the semantic tag of the answer to build a Maximum Entropy model which predicts semantic tags given a question. When the Max-Ent model is used, the estimated probabilities are thrown out and only the most likely tag is returned. This paper presents a novel method for learning QC/ST correlation from unannotated data. The method introduced is based on the Mutual Information (MI) statistic (Section 2) and is trained on a trivia question database (Section 3) using a question classifier and semantic tagger. The MI method is general in that it can be applied</context>
<context position="5278" citStr="Ittycheriah et al. (2001)" startWordPosition="859" endWordPosition="862">STIW)Pr(WIQC) w As stated above, this estimation method makes the assumption, expressed in equation (3), that for each question there will only be one question class. For most current Q/A systems this is the case. Perhaps future systems will have more sophisticated question classifiers that assign a probability to a number of question classes. To accomodate increased sophistication, these formulas will change slightly, but the general method may still be applicable. When this MI method is trained by the above method, it takes into account the actual performance of the semantic tagger on data. Ittycheriah et al. (2001) builds a statistical model on annotated data which predicts semantic tag from the question and notes that improvement in this prediction does not necessarily lead to higher performance, since there is a complex interaction between this module and the semantic tagger and answer selector. One advantage of training on unannotated text using the MI approach is that the correlation between the question class and the performance of the semantic tagger is explicitly modeled. 31n this paper, even though Phrag is a HMM, its priors were unavailable, so it was treated as a nonstatistical tagger Q Class </context>
<context position="17673" citStr="Ittycheriah et al. (2001)" startWordPosition="2926" endWordPosition="2929"> is another interesting result. Although it does not prove that trivia questions constitute an open domain, it suggests that trivia questions are at least a self-consistent domain. We have shown that another component of a Q/A system can be built statistically to yield nice performance when even simple statistics are used. This prompts the question : what other components can be built statistically? Already some components are consistently built by statistical methods (semantic taggers, information retrieval engines), yet some remain predominately handcrafted (e.g. question classifiers — with Ittycheriah et al. (2001) as an exception), and thus are prime targets for statistical methods. This paper demonstrated performance on extracting one-word answers, but this method can be extended to extracting multiple words. We built a system which chooses the base noun phrase containing the highest ranked word, but have not completed evaluation. Aside from its use in question answering, once the short answer extraction task can be performed with high precision, systems will be able to extract facts from heterogenous text. This will be an enabling technology which will allow integration with symbolic processing syste</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, 2001</marker>
<rawString>A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi. 2001. Question answering using maximum entropy components. Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2200" citStr="Miller, 1990" startWordPosition="356" endWordPosition="357">mine a few different methods questions classifiers and semantic taggers described in Figure 1. This MI QC/ST match module, along with a question classifier and semantic tagger, can function as a short answer extractor that selects a short answer from a sentence given a question. Question Classifiers: 1. U : a simple initial unigram model 2. UH : a slightly more complicated model that combines initial unigram and wh-phrase heads 3. Qgrok : a hand-built question typing mechanism (Breck et al., 1999) Semantic Taggers: 1. NE : Phrag, a HMM Named Entity Tagger (Breck et al., 1999) 2. WN : WordNet (Miller, 1990) Figure 1: Question Classifiers and Semantic Taggers Investigated To simplify the problem, we make the assumption that all answers are strictly one word in lengthl. Even so, this task is non-trivial and relevant especially in the case of trivia questions where most of the answers are only one or two words long. The disparity of performance in the 50-byte and 250-byte TREC-8 Question Answering evaluations (Voorhees, 1999) gives further evidence that extracting a shorter, multi-word answer from a longer, sentence length answer is a task worthy of consideration in its own right. We empirically te</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. An on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>D Radev</author>
<author>E Brown</author>
<author>A Coden</author>
<author>V Samn</author>
</authors>
<title>The use of predictive annotation for question answering in trec8.</title>
<date>1999</date>
<booktitle>Proc. of TREC-8.</booktitle>
<contexts>
<context position="781" citStr="Prager et al., 1999" startWordPosition="110" endWordPosition="113"> Abstract This paper presents a simple, general method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation. This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger. 1 Introduction Many of the recent question answering systems integrate statistical NLP/IR tools with a handcrafted component, a question class/semantic tag (QC/ST) match module (Prager et al., 1999), (Breck et al., 1999). Hovy et al. (2001) describes a parsing method for learning QC/ST match. Ittycheriah et al. (2001) trains on trivia questions annotated with the semantic tag of the answer to build a Maximum Entropy model which predicts semantic tags given a question. When the Max-Ent model is used, the estimated probabilities are thrown out and only the most likely tag is returned. This paper presents a novel method for learning QC/ST correlation from unannotated data. The method introduced is based on the Mutual Information (MI) statistic (Section 2) and is trained on a trivia question</context>
</contexts>
<marker>Prager, Radev, Brown, Coden, Samn, 1999</marker>
<rawString>J. Prager, D. Radev, E. Brown, A. Coden, and V. Samn. 1999. The use of predictive annotation for question answering in trec8. Proc. of TREC-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>Proceedings of IJCAI.</booktitle>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Triviaspot com E Voorhees</author>
</authors>
<title>The trec-8 question answering track report.</title>
<date>1999</date>
<booktitle>Proc. of TREC-8.</booktitle>
<contexts>
<context position="2624" citStr="Voorhees, 1999" startWordPosition="427" endWordPosition="428">ase heads 3. Qgrok : a hand-built question typing mechanism (Breck et al., 1999) Semantic Taggers: 1. NE : Phrag, a HMM Named Entity Tagger (Breck et al., 1999) 2. WN : WordNet (Miller, 1990) Figure 1: Question Classifiers and Semantic Taggers Investigated To simplify the problem, we make the assumption that all answers are strictly one word in lengthl. Even so, this task is non-trivial and relevant especially in the case of trivia questions where most of the answers are only one or two words long. The disparity of performance in the 50-byte and 250-byte TREC-8 Question Answering evaluations (Voorhees, 1999) gives further evidence that extracting a shorter, multi-word answer from a longer, sentence length answer is a task worthy of consideration in its own right. We empirically test the performance of the short answer extraction on a held-out set of trivia questions and compare with a number of baseline systems including a hand-crafted system that uses a similar question classifier and semantic tagger (Section 4). 2 MI as an Estimator for Question Class/Semantic Tag Correlation A simple approach to building a question answering system would be to (1) collect a huge database of questions and answe</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>TriviaMachine Inc. Triviaspot.com. E. Voorhees. 1999. The trec-8 question answering track report. Proc. of TREC-8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>