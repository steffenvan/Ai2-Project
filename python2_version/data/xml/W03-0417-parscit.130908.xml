<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000138">
<title confidence="0.9946145">
Training a Naive Bayes Classifier via the EM Algorithm with a Class
Distribution Constraint
</title>
<author confidence="0.995201">
Yoshimasa Tsuruokatt and Jun’ichi Tsujiitt
</author>
<affiliation confidence="0.999784">
tDepartment of Computer Science, University of Tokyo
</affiliation>
<address confidence="0.813076">
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
$CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
</address>
<email confidence="0.999732">
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999777695652174">
Combining a naive Bayes classifier with the
EM algorithm is one of the promising ap-
proaches for making use of unlabeled data for
disambiguation tasks when using local con-
text features including word sense disambigua-
tion and spelling correction. However, the use
of unlabeled data via the basic EM algorithm
often causes disastrous performance degrada-
tion instead of improving classification perfor-
mance, resulting in poor classification perfor-
mance on average. In this study, we introduce
a class distribution constraint into the iteration
process of the EM algorithm. This constraint
keeps the class distribution of unlabeled data
consistent with the class distribution estimated
from labeled data, preventing the EM algorithm
from converging into an undesirable state. Ex-
perimental results from using 26 confusion sets
and a large amount of unlabeled data show
that our proposed method for using unlabeled
data considerably improves classification per-
formance when the amount of labeled data is
small.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993665">
Many of the tasks in natural language processing can
be addressed as classification problems. State-of-the-
art machine learning techniques including Support Vec-
tor Machines (Vapnik, 1995), AdaBoost (Schapire and
Singer, 2000) and Maximum Entropy Models (Ratna-
parkhi, 1998; Berger et al., 1996) provide high perfor-
mance classifiers if one has abundant correctly labeled
examples.
However, annotating a large set of examples generally
requires a huge amount of human labor and time. This
annotation cost is one of the major obstacles to applying
machine learning techniques to real-world NLP applica-
tions.
Recently, learning algorithms called minimally super-
vised learning or unsupervised learning that can make use
of unlabeled data have received much attention. Since
collecting unlabeled data is generally much easier than
annotating data, such techniques have potential for solv-
ing the problem of annotation cost. Those approaches in-
clude a naive Bayes classifier combined with the EM al-
gorithm (Dempster et al., 1977; Nigam et al., 2000; Ped-
ersen and Bruce, 1998), Co-training (Blum and Mitchell,
1998; Collins and Singer, 1999; Nigam and Ghani, 2000),
and Transductive Support Vector Machines (Joachims,
1999). These algorithms have been applied to some
tasks including text classification and word sense disam-
biguation and their effectiveness has been demonstrated
to some extent.
Combining a naive Bayes classifier with the EM algo-
rithm is one of the promising minimally supervised ap-
proaches because its computational cost is low (linear to
the size of unlabeled data), and it does not require the
features to be split into two independent sets unlike co-
training.
However, the use of unlabeled data via the basic EM
algorithm does not always improve classification perfor-
mance. In fact, this often causes disastrous performance
degradation resulting in poor classification performance
on average. To alleviate this problem, we introduce a
class distribution constraint into the iteration process of
the EM algorithm. This constraint keeps the class dis-
tribution of unlabeled data consistent with the class dis-
tribution estimated from labeled data, preventing the EM
algorithm from converging into an undesirable state.
In order to assess the effectiveness of the proposed
method, we applied it to the problem of semantic disam-
biguation using local context features. Experiments were
conducted with 26 confusion sets and a large number of
unlabeled examples collected from a corpus of one hun-
dred million words.
This paper is organized as follows. Section 2 briefly
reviews the naive Bayes classifier and the EM algorithm
as means of using unlabeled data. Section 3 presents the
idea of using a class distribution constraint and how to
impose this constraint on the learning process. Section
4 describes the problem of confusion set disambiguation
and the features used in the experiments. Experimental
results are presented in Section 5. Related work is dis-
cussed in Section 6. Section 7 offers some concluding
remarks.
</bodyText>
<sectionHeader confidence="0.99576" genericHeader="method">
2 Naive Bayes Classifier
</sectionHeader>
<bodyText confidence="0.9999">
The naive Bayes classifier is a simple but effective classi-
fier which has been used in numerous applications of in-
formation processing such as image recognition, natural
language processing, information retrieval, etc. (Escud-
ero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000;
Pedersen, 2000).
In this section, we briefly review the naive Bayes clas-
sifier and the EM algorithm that is used for making use
of unlabeled data.
</bodyText>
<subsectionHeader confidence="0.593221">
2.1 Naive Bayes Model
</subsectionHeader>
<bodyText confidence="0.997434">
Let x~ be a vector we want to classify, and ck be a possible
class. What we want to know is the probability that the
vector x~ belongs to the class ck. We first transform the
probability P(ck|x) using Bayes’ rule,
</bodyText>
<equation confidence="0.998493">
P(�x|ck)
P(ck|�x) = P(ck) × P (~x) . (1)
</equation>
<bodyText confidence="0.999848333333333">
Class probability P(ck) can be estimated from training
data. However, direct estimation of P(ck|x) is impossi-
ble in most cases because of the sparseness of training
data.
By assuming the conditional independence of the ele-
ments of a vector, P(x|ck) is decomposed as follows,
</bodyText>
<equation confidence="0.921221">
d
P(x|ck) = H P(xj|ck), (2)
j=1
</equation>
<bodyText confidence="0.663652">
where xj is the jth element of vector Y. Then Equation 1
becomes
</bodyText>
<equation confidence="0.992975">
P(x) .(3)
</equation>
<bodyText confidence="0.9997503125">
With this equation, we can calculate P(ck|x) and classify
x~ into the class with the highest P(ck|x).
Note that the naive Bayes classifier assumes the con-
ditional independence of features. This assumption how-
ever does not hold in most cases. For example, word oc-
currence is a commonly used feature for text classifica-
tion. However, obvious strong dependencies exist among
word occurrences. Despite this apparent violation of the
assumption, the naive Bayes classifier exhibits good per-
formance for various natural language processing tasks.
There are some implementation variants of the naive
Bayes classifier depending on their event models (Mc-
Callum and Nigam, 1998). In this paper, we adopt the
multi-variate Bernoulli event model. Smoothing was
done by replacing zero-probability with a very small con-
stant (1.0 × 10−4).
</bodyText>
<subsectionHeader confidence="0.997884">
2.2 EM Algorithm
</subsectionHeader>
<bodyText confidence="0.9954340625">
The Expectation Maximization (EM) algorithm (Demp-
ster et al., 1977) is a general framework for estimating
the parameters of a probability model when the data has
missing values. This algorithm can be applied to min-
imally supervised learning, in which the missing values
correspond to missing labels of the examples.
The EM algorithm consists of the E-step in which the
expected values of the missing sufficient statistics given
the observed data and the current parameter estimates are
computed, and the M-step in which the expected values
of the sufficient statistics computed in the E-step are used
to compute complete data maximum likelihood estimates
of the parameters (Dempster et al., 1977).
In our implementation of the EM algorithm with the
naive Bayes classifier, the learning process using unla-
beled data proceeds as follows:
</bodyText>
<listItem confidence="0.999512285714286">
1. Train the classifier using only labeled data.
2. Classify unlabeled examples, assigning probabilistic
labels to them.
3. Update the parameters of the model. Each proba-
bilistically labeled example is counted as its proba-
bility instead of one.
4. Go back to (2) until convergence.
</listItem>
<sectionHeader confidence="0.968541" genericHeader="method">
3 Class Distribution Constraint
</sectionHeader>
<subsectionHeader confidence="0.997273">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999951928571429">
As described in the previous section, the naive Bayes
classifier can be easily extended to exploit unlabeled data
by using the EM algorithm. However, the use of unla-
beled data for actual tasks exhibits mixed results. The
performance is improved for some cases, but not in all
cases. In our preliminary experiments, using unlabeled
data by means of the EM algorithm often caused signifi-
cant deterioration of classification performance.
To investigate the cause of this, we observed the
change of class distribution of unlabeled data occuring in
the process of the EM algorithm. What we found is that
sometimes the class distribution of unlabeled data greatly
diverges from that of the labeled data. For example, when
the proportion of class A examples in labeled data was
</bodyText>
<equation confidence="0.9961775">
11d j=1 P(xj|ck)
P(ck|�x) = P(ck) ×
</equation>
<bodyText confidence="0.999901818181818">
about 0.9, the EM algorithm would sometimes converge
into states where the proportion of class A is about 0.7.
This divergence of class distribution clearly indicated the
EM algorithm converged into an undesirable state.
One of the possible remedies for this phenomenon is
that of forcing class distribution of unlabeled data not to
diverge from the class distribution estimated from labeled
data. In this work, we introduce a class distribution con-
straint (CDC) into the training process of the EM algo-
rithm. This constraint keeps the class distribution of un-
labeled data consistent with that of labeled data.
</bodyText>
<subsectionHeader confidence="0.999888">
3.2 Calibrating Probabilistic Labels
</subsectionHeader>
<bodyText confidence="0.999838869565218">
We implement class distribution constraints by calibrat-
ing probabilistic labels assigned to unlabeled data in the
process of the EM algorithm. In this work, we consider
only binary classification: classes A and B.
Let pi be the probabilistic label of the ith example
representing the probability that this example belongs to
class A.
Let 0 be the proportion of class A examples in the la-
beled data L. If the proportion of the class A examples
(the proportion of the examples whose pi is greater than
0.5) in unlabeled data U is different from 0, we consider
that the values of the probabilistic labels should be cali-
brated.
The basic idea of the calibration is to shift all the prob-
ability values of unlabeled data to the extent that the class
distribution of unlabeled data becomes identical to that of
labeled data. In order for the shifting of the probability
values not to cause the values to go outside of the range
from 0 to 1, we transform the probability values by an
inverse sigmoid function in advance. After the shifting,
the values are returned to probability values by a sigmoid
function.
The whole calibration process is given below:
</bodyText>
<listItem confidence="0.989511272727273">
1. Transform the probabilistic labels p1, ...pn by the in-
verse function of the sigmoid function,
2. Sort q1, ...qn in descending order. Then, pick up the
value qborder that is located at the position of pro-
portion 0 in these n values.
3. Since qborder is located at the border between the
examples of label A and those of label B, the value
should be close to zero (= probability is 0.5). Thus
we calibrate all qi by subtracting qborder.
4. Transform q1, ...qn by a sigmoid function back into
probability values.
</listItem>
<bodyText confidence="0.98621">
This calibration process is conducted between the E-
step and the M-step in the EM algorithm.
</bodyText>
<sectionHeader confidence="0.985207" genericHeader="method">
4 Confusion Set Disambiguation
</sectionHeader>
<bodyText confidence="0.9999875">
We applied the naive Bayes classifier with the EM algo-
rithm to confusion set disambiguation. Confusion set dis-
ambiguation is defined as the problem of choosing the
correct word from a set of words that are commonly
confused. For example, quite may easily be mistyped
as quiet. An automatic proofreading system would
need to judge which is the correct use given the con-
text surrounding the target. Example confusion sets in-
clude: {principle, principal}, {then, than}, and {weather,
whether}.
Until now, many methods have been proposed for this
problem including winnow-based algorithms (Golding
and Roth, 1999), differential grammars (Powers, 1998),
transformation based learning (Mangu and Brill, 1997),
decision lists (Yarowsky, 1994).
Confusion set disambiguation has very similar char-
acteristics to a word sense disambiguation problem in
which the system has to identify the meaning of a pol-
ysemous word given the surrounding context. The merit
of using confusion set disambiguation as a test-bed for a
learning algorithm is that since one does not need to an-
notate the examples to make labeled data, one can con-
duct experiments using an arbitrary amount of labeled
data.
</bodyText>
<subsectionHeader confidence="0.925556">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.998894176470588">
As the input of the classifier, the context of the target must
be represented in the form of a vector. We use a binary
feature vector which contains only the values of 0 or 1 for
each element.
In this work, we use the local context surrounding the
target as the feature of an example. The features of a
target are the two preceding words and the two following
words. For example, if the disambiguation target is quiet
and the system is given the following sentence
“...between busy and quiet periods and it...”
the contexts of this example are represented as follows:
busy−2, and−1, periods+1, and+2
In the input vector, only the elements corresponding to
these features are set to 1, while all the other elements are
set to 0.
into real value ranging from −oo to oo. Let the
transformed values be q1, ...qn.
</bodyText>
<equation confidence="0.96814025">
1
f(x) =
. (4)
1 + e−x
</equation>
<table confidence="0.997420607142857">
NB NB+EM NB+EM
+CDC
87.4 96.3 96.0
77.2 89.0 81.1
86.4 91.6 93.6
80.1 64.4 79.5
69.6 61.6 68.8
95.1 86.6 95.1
95.1 95.1 95.1
77.5 70.4 76.0
89.0 77.4 85.4
85.3 92.3 94.2
65.3 64.2 63.7
91.1 77.6 92.9
77.9 70.2 82.0
78.4 81.5 82.1
72.8 88.7 79.4
85.3 75.9 83.5
83.7 86.1 81.0
67.7 68.7 67.9
96.2 93.3 92.8
74.7 84.0 85.3
88.4 91.4 90.2
96.4 96.4 89.1
96.9 96.9 96.9
90.6 92.3 93.7
87.8 81.8 90.3
83.8 82.9 85.4
</table>
<tableCaption confidence="0.900018">
Table 2: Results of Confusion Sets Disambiguation with
32 Labeled Data
Table 1: Confusion Sets used in the Experiments
</tableCaption>
<figure confidence="0.913734555555555">
Confusion Set
I, me
accept,except
affect, effect
among, between
amount, number
begin, being
cite, sight
country, county
fewer, less
its, it’s
lead, led
maybe, may be
passed, past
peace, piece
principal, principle
quiet, quite
raise, rise
sight, site
site, cite
than, then
their, there
there, they’re
they’re, their
weather, whether
your, you’re
AVERAGE
Baseline #Unlabeled
86.4 474726
53.2 14876
79.1 20653
80.1 101621
76.1 50310
93.0 82448
95.1 3498
80.8 17810
91.6 35413
83.7 177488
53.5 25195
92.4 36519
66.8 24450
57.0 11219
61.7 8670
88.8 29618
60.8 13392
61.1 9618
96.0 5594
63.8 216286
63.8 372471
96.4 146462
96.9 237443
87.5 29730
88.6 108185
78.2 90147
Confusion Set
I, me
accept, except
affect, effect
among, between
amount, number
begin, being
cite, sight
country, county
fewer, less
its, it’s
lead, led
maybe, may be
passed, past
peace, piece
principal, principle
quiet, quite
raise, rise
sight, site
site, cite
than, then
their, there
there, they’re
they’re, their
weather, whether
your, you’re
AVERAGE
</figure>
<sectionHeader confidence="0.987803" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999729">
To conduct large scale experiments, we used the British
National Corpus 1 that is currently one of the largest cor-
pora available. The corpus contains roughly one hundred
million words collected from various sources.
The confusion sets used in our experiments are the
same as in Golding’s experiment (1999). Since our al-
gorithm requires the classification to be binary, we de-
composed three-class confusion sets into pairwise binary
classifications. Table 1 shows the resulting confusion sets
used in the following experiments. The baseline perfor-
mances, achieved by simply selecting the majority class,
are shown in the second column. The number of unla-
beled data are shown in the rightmost column.
The 1,000 test sets were randomly selected from the
corpus for each confusion set. They do not overlap the
labeled data or the unlabeled data used in the learning
process.
</bodyText>
<footnote confidence="0.9713665">
1Data cited herein has been extracted from the British Na-
tional Corpus Online service, managed by Oxford University
Computing Services on behalf of the BNC Consortium. All
rights in the texts cited are reserved.
</footnote>
<bodyText confidence="0.997870095238095">
The results are shown in Table 2 through Table 5.
These four tables correspond to the cases in which the
number of labeled examples is 32, 64, 128 and 256 as
indicated by the table captions. The first column shows
the confusion sets. The second column shows the clas-
sification performance of the naive Bayes classifier with
which only labeled data was used for training. The third
column shows the performance of the naive Bayes classi-
fier with which unlabeled data was used via the basic EM
algorithm. The rightmost column shows the performance
of the EM algorithm that was extended with our proposed
calibration process.
Notice that the effect of unlabeled data were very dif-
ferent for each confusion set. As shown in Table 2, the
precision was significantly improved for some confusion
sets including {I, me}, {accept, except} and {affect, ef-
fect} . However, disastrous performance deterioration
can be observed, especially that of the basic EM algo-
rithm, in some confusion sets including {among, be-
tween}, {country, county}, and {site, cite}.
On average, precision was degraded by the use of un-
</bodyText>
<tableCaption confidence="0.990416">
Table 3: Results of Confusion Sets Disambiguation with
64 Labeled Data
Table 4: Results of Confusion Sets Disambiguation with
</tableCaption>
<table confidence="0.993259807017544">
128 Labeled Data
NB NB+EM NB+EM
+CDC
89.4 96.8 95.7
82.9 89.3 87.5
89.4 92.4 93.6
79.9 76.3 80.5
71.5 68.7 69.1
95.8 92.1 95.7
95.1 95.8 96.4
78.7 73.4 74.5
87.6 74.3 87.3
85.8 94.0 92.5
76.2 66.8 72.8
92.6 84.0 96.2
79.7 72.5 88.4
81.1 81.2 82.4
75.2 90.2 89.8
86.5 84.0 89.2
85.7 85.6 86.9
71.9 69.0 69.0
96.3 95.8 95.5
79.7 83.8 83.2
90.5 91.9 92.1
96.2 85.2 91.4
96.9 96.9 95.8
90.6 91.4 93.3
88.0 83.3 94.2
85.7 84.6 87.7
NB NB+EM NB+EM
+CDC
90.7 96.9 96.4
85.7 90.7 89.4
91.9 93.1 93.3
80.0 76.3 80.1
78.2 68.9 69.3
94.4 88.1 95.0
96.9 96.9 98.1
81.3 75.1 75.7
89.9 74.9 89.4
88.6 93.2 95.2
80.5 82.5 82.2
94.5 80.9 94.4
81.8 74.1 85.5
84.1 81.3 82.5
79.8 89.8 89.5
86.5 82.7 90.1
85.2 86.4 87.7
75.6 70.3 70.5
96.1 95.8 97.0
81.7 84.2 84.5
91.8 91.5 91.2
95.9 83.4 91.3
96.9 96.9 96.7
92.0 92.6 95.1
88.9 84.1 94.5
87.6 85.2 88.6
</table>
<figure confidence="0.964326769230769">
Confusion Set
I, me
accept, except
affect, effect
among, between
amount, number
begin, being
cite, sight
country, county
fewer, less
its, it’s
lead, led
maybe, may be
passed, past
peace, piece
principal, principle
quiet, quite
raise, rise
sight, site
site, cite
than, then
their, there
there, they’re
they’re, their
weather, whether
your, you’re
AVERAGE
Confusion Set
I, me
accept, except
affect, effect
among, between
amount, number
begin, being
cite, sight
country, county
fewer, less
its, it’s
lead, led
</figure>
<bodyText confidence="0.948644977777778">
maybe, may be
passed, past
peace, piece
principal, principle
quiet, quite
raise, rise
sight, site
site, cite
than, then
their, there
there, they’re
they’re, their
weather, whether
your, you’re
AVERAGE
labeled data via the basic EM algorithm (from 83.3% to
82.9%). On the other hand, the EM algorithm with the
class distribution constraint improved average classifica-
tion performance (from 83.3% to 85.4%). This improved
precision nearly reached the performance achieved by
twice the size of labeled data without unlabeled data (see
the average precision of NB in Table 3). This perfor-
mance gain indicates that the use of unlabeled data ef-
fectively doubles the labeled training data.
In Table 3, the tendency of performance improvement
(or degradation) in the use of unlabeled data is almost the
same as in Table 2. The basic EM algorithm degraded the
performance on average, while our method improved av-
erage performance (from 85.7% to 87.7%). This perfor-
mance gain effectively doubled the size of labeled data.
The results with 128 labeled examples are shown in Ta-
ble 4. Although the use of unlabeled examples by means
of our proposed method still improved average perfor-
mance (from 87.6% to 88.6%), the gain is smaller than
that for a smaller amount of labeled data.
With 256 labeled examples (Table 5), the average per-
formance gain was negligible (from 89.2% to 89.3%).
Figure 1 summarizes the average precisions for differ-
ent number of labeled examples. Average peformance
was improved by the use of unlabeled data with our pro-
posed method when the amount of labeled data was small
(from 32 to 256) as shown in Table 2 through Table
5. However, when the number of labeled examples was
large (more than 512), the use of unlabeled data degraded
average performance.
</bodyText>
<subsectionHeader confidence="0.997759">
5.1 Effect of the amount of unlabeled data
</subsectionHeader>
<bodyText confidence="0.9992431">
When the use of unlabeled data improves classification
performance, the question of how much unlabeled data
are needed becomes very important. Although unlabeled
data are generally much more obtainable than labeled
data, acquiring more than several-thousand unlabeled ex-
amples is not always an easy task. As for confusion set
disambiguation, Table 1 indicates that it is sometimes im-
possible to collect tens of thousands examples even in a
very large corpus.
In order to investigate the effect of the amount of un-
</bodyText>
<figure confidence="0.982225666666667">
100
95
90
85
80
75
NB
NB+EM
NB+EM+CDC
</figure>
<tableCaption confidence="0.93912">
Table 5: Results of Confusion Sets Disambiguation with
</tableCaption>
<table confidence="0.95318671875">
256 Labeled Data
NB NB+EM NB+EM
+CDC
93.4 96.6 96.4
89.7 90.3 91.2
93.4 93.5 93.9
79.6 75.1 80.4
81.4 68.9 69.2
94.6 89.9 96.6
97.6 97.9 98.4
84.2 76.5 77.5
90.8 83.0 89.2
90.2 93.3 94.5
82.9 79.8 82.6
96.0 87.1 94.7
83.5 74.6 86.3
84.6 81.4 85.7
83.4 90.5 90.5
88.6 86.8 91.2
88.0 87.1 88.4
79.2 71.7 73.2
97.3 97.6 97.4
82.3 85.5 85.9
93.6 92.1 92.0
96.5 83.0 91.1
96.8 90.8 97.3
93.8 91.9 94.7
89.7 83.8 94.6
89.2 85.9 89.3
Confusion Set
I, me
accept, except
</table>
<bodyText confidence="0.877345644444444">
affect, effect
among, between
amount, number
begin, being
cite, sight
country, county
fewer, less
its, it’s
lead, led
maybe, may be
passed, past
peace, piece
principal, principle
quiet, quite
raise, rise
sight, site
site, cite
than, then
their, there
there, they’re
they’re, their
weather, whether
your, you’re
AVERAGE
labeled data, we conducted experiments by varying the
amount of unlabeled data for some confusion sets that ex-
hibited significant performance gain by using unlabeled
data.
Figure 2 shows the relationship between the classifica-
tion performance and the amount of unlabeled data for
three confusion sets: {I, me}, {principal, principle}, and
{passed, past}. The number of labeled examples in all
cases was 64.
Note that performance continued to improve even
when the number of unlabeled data reached more than
ten thousands. This suggests that we can further improve
the performance for some confusion sets by using a very
large corpus containing more than one hundred million
words.
Figure 2 also indicates that the use of unlabeled data
was not effective when the amount of unlabeled data was
smaller than one thousand. It is often the case with mi-
nor words that the number of occurrences does not reach
one thousand even in a one-hundred-million word corpus.
Thus, constructing a very very large corpus (containing
</bodyText>
<figure confidence="0.9891355">
100 1000
Number of Labeled Examples
</figure>
<figureCaption confidence="0.962478">
Figure 1: Relationship between Average Precision and
the Amount of Labeled Data
</figureCaption>
<figure confidence="0.9956945">
1000 10000 100000
Number of Unlabeled Examples
</figure>
<figureCaption confidence="0.967484">
Figure 2: Relationship between Precision and the
Amount of Unlabeled Data
</figureCaption>
<bodyText confidence="0.9466875">
more than billions of words) appears to be beneficial for
infrequent words.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999938857142857">
Nigam et al.(2000) reported that the accuracy of text clas-
sification can be improved by a large pool of unlabeled
documents using a naive Bayes classifier and the EM al-
gorithm. They presented two extensions to the basic EM
algorithm. One is a weighting factor to modulate the con-
tribution of the unlabeled data. The other is the use of
multiple mixture components per class. With these exten-
sions, they reported that the use of unlabeled data reduces
classification error by up to 30%.
Pedersen et al.(1998) employed the EM algorithm and
Gibbs Sampling for word sense disambiguation by using
a naive Bayes classifier. Although Gibbs Sampling re-
sults in a small improvement over the EM algorithm, the
results for verbs and adjectives did not reach baseline per-
</bodyText>
<figure confidence="0.991251571428571">
Precision (%)
100
95
90
85
80
75
70
65
60
I, me
principal, principle
passed, past
Precision (%)
</figure>
<bodyText confidence="0.999475352941176">
formance on average. The amount of unlabeled data used
in their experiments was relatively small (from several
hundreds to a few thousands).
Yarowsky (1995) presented an approach that signif-
icantly reduces the amount of labeled data needed for
word sense disambiguation. Yarowsky achieved accura-
cies of more than 90% for two-sense polysemous words.
This success was likely due to the use of “one sense per
discourse” characteristic of polysemous words.
Yarowsky’s approach can be viewed in the context of
co-training (Blum and Mitchell, 1998) in which the fea-
tures can be split into two independent sets. For word
sense disambiguation, the sets correspond to the local
contexts of the target word and the “one sense per dis-
course” characteristic. Confusion sets however do not
have the latter characteristic.
The effect of a huge amount of unlabeled data for
confusion set disambiguation is discussed in (Banko and
Brill, 2001). Bank and Brill conducted experiments of
committee-based unsupervised learning for two confu-
sion sets. Their results showed that they gained a slight
improvement by using a certain amount of unlabeled
data. However, test set accuracy began to decline as ad-
ditional data were harvested.
As for the performance of confusion set disambigua-
tion, Golding (1999) achieved over 96% by a winnow-
based approach. Although our results are not directly
comparable with their results since the data sets are
different, our results does not reach the state-of-the-
art performance. Because the performance of a naive
Bayes classifier is significantly affected by the smoothing
method used for paramter estimation, there is a chance to
improve our performance by using a more sophisticated
smoothing technique.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999959055555556">
The naive Bayes classifier can be combined with the well-
established EM algorithm to exploit the unlabeled data
. However, the use of unlabeled data sometimes causes
disastrous degradation of classification performance.
In this paper, we introduce a class distribution con-
straint into the iteration process of the EM algorithm.
This constraint keeps the class distribution of unlabeled
data consistent with the true class distribution estimated
from labeled data, preventing the EM algorithm from
converging into an undesirable state.
Experimental results using 26 confusion sets and a
large amount of unlabeled data showed that combining
the EM algorithm with our proposed constraint consis-
tently reduced the average classification error rates when
the amount of labeled data is small. The results also
showed that use of unlabeled data is especially advan-
tageous when the amount of labeled data is small (up to
about one hundred).
</bodyText>
<subsectionHeader confidence="0.738304">
7.1 Future Work
</subsectionHeader>
<bodyText confidence="0.9999856">
In this paper, we empirically demonstrated that a class
distribution constraint reduced the chance of undesirable
convergence of the EM algorithm. However, the theoret-
ical justification of this constraint should be clarified in
future work.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999472726190476">
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of the Association for Computational Lin-
guistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39–71.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT: Proceedings of the Workshop on Computa-
tional Learning Theory, Morgan Kaufmann Publish-
ers.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Royal Statstical Society B 39, pages 1–38.
G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes
and exemplar-based approaches to word sense disam-
biguation revisited. In Proceedings of the 14th Euro-
pean Conference on Artificial Intelligence.
Andrew R. Golding and Dan Roth. 1999. A winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107–130.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. 16th International Conf. on Machine Learning,
pages 200–209. Morgan Kaufmann, San Francisco,
CA.
David D. Lewis. 1998. Naive Bayes at forty: The in-
dependence assumption in information retrieval. In
Claire N´edellec and C´eline Rouveirol, editors, Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 4–15, Chem-
nitz, DE. Springer Verlag, Heidelberg, DE.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In Proc. 14th Interna-
tional Conference on Machine Learning, pages 187–
194. Morgan Kaufmann.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifica-
tion. In AAAI-98 Workshop on Learning for Text Cat-
egorization.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the ef-
fectiveness and applicability of co-training. In CIKM,
pages 86–93.
Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39(2/3):103–134.
Ted Pedersen and Rebecca Bruce. 1998. Knowledge
lean word-sense disambiguation. In AAAI/IAAI, pages
800–805.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 63–69,
Seattle, WA, May.
David M. W. Powers. 1998. Learning and application
of differential grammars. In T. Mark Ellison, editor,
CoNLL97: Computational Natural Language Learn-
ing, pages 88–96. Association for Computational Lin-
guistics, Somerset, New Jersey.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, the University of Pennsylvania.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135–168.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. New York.
David Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in
spanish and french. In Meeting of the Association for
Computational Linguistics, pages 88–95.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. Proc. of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721303">
<title confidence="0.997898">Training a Naive Bayes Classifier via the EM Algorithm with a Distribution Constraint</title>
<author confidence="0.985876">Jun’ichi</author>
<affiliation confidence="0.974996">of Computer Science, University of</affiliation>
<address confidence="0.79722">Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033</address>
<affiliation confidence="0.983508">JST (Japan Science and Technology</affiliation>
<address confidence="0.953191">Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012</address>
<abstract confidence="0.9986665">Combining a naive Bayes classifier with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction. However, the use of unlabeled data via the basic EM algorithm often causes disastrous performance degradation instead of improving classification performance, resulting in poor classification performance on average. In this study, we introduce distribution constraint the iteration process of the EM algorithm. This constraint keeps the class distribution of unlabeled data consistent with the class distribution estimated from labeled data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24339" citStr="Banko and Brill, 2001" startWordPosition="3992" endWordPosition="3995">re than 90% for two-sense polysemous words. This success was likely due to the use of “one sense per discourse” characteristic of polysemous words. Yarowsky’s approach can be viewed in the context of co-training (Blum and Mitchell, 1998) in which the features can be split into two independent sets. For word sense disambiguation, the sets correspond to the local contexts of the target word and the “one sense per discourse” characteristic. Confusion sets however do not have the latter characteristic. The effect of a huge amount of unlabeled data for confusion set disambiguation is discussed in (Banko and Brill, 2001). Bank and Brill conducted experiments of committee-based unsupervised learning for two confusion sets. Their results showed that they gained a slight improvement by using a certain amount of unlabeled data. However, test set accuracy began to decline as additional data were harvested. As for the performance of confusion set disambiguation, Golding (1999) achieved over 96% by a winnowbased approach. Although our results are not directly comparable with their results since the data sets are different, our results does not reach the state-of-theart performance. Because the performance of a naive</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1697" citStr="Berger et al., 1996" startWordPosition="239" endWordPosition="242">ed data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small. 1 Introduction Many of the tasks in natural language processing can be addressed as classification problems. State-of-theart machine learning techniques including Support Vector Machines (Vapnik, 1995), AdaBoost (Schapire and Singer, 2000) and Maximum Entropy Models (Ratnaparkhi, 1998; Berger et al., 1996) provide high performance classifiers if one has abundant correctly labeled examples. However, annotating a large set of examples generally requires a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the probl</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT: Proceedings of the Workshop on Computational Learning Theory,</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="2508" citStr="Blum and Mitchell, 1998" startWordPosition="364" endWordPosition="367">This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotraining. However, the use of unlabeled dat</context>
<context position="23954" citStr="Blum and Mitchell, 1998" startWordPosition="3928" endWordPosition="3931">100 95 90 85 80 75 70 65 60 I, me principal, principle passed, past Precision (%) formance on average. The amount of unlabeled data used in their experiments was relatively small (from several hundreds to a few thousands). Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed for word sense disambiguation. Yarowsky achieved accuracies of more than 90% for two-sense polysemous words. This success was likely due to the use of “one sense per discourse” characteristic of polysemous words. Yarowsky’s approach can be viewed in the context of co-training (Blum and Mitchell, 1998) in which the features can be split into two independent sets. For word sense disambiguation, the sets correspond to the local contexts of the target word and the “one sense per discourse” characteristic. Confusion sets however do not have the latter characteristic. The effect of a huge amount of unlabeled data for confusion set disambiguation is discussed in (Banko and Brill, 2001). Bank and Brill conducted experiments of committee-based unsupervised learning for two confusion sets. Their results showed that they gained a slight improvement by using a certain amount of unlabeled data. However</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="2534" citStr="Collins and Singer, 1999" startWordPosition="368" endWordPosition="371">ne of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotraining. However, the use of unlabeled data via the basic EM algorit</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Royal Statstical Society B</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="2423" citStr="Dempster et al., 1977" startWordPosition="350" endWordPosition="353"> a large set of examples generally requires a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be</context>
<context position="6450" citStr="Dempster et al., 1977" startWordPosition="997" endWordPosition="1001">ed feature for text classification. However, obvious strong dependencies exist among word occurrences. Despite this apparent violation of the assumption, the naive Bayes classifier exhibits good performance for various natural language processing tasks. There are some implementation variants of the naive Bayes classifier depending on their event models (McCallum and Nigam, 1998). In this paper, we adopt the multi-variate Bernoulli event model. Smoothing was done by replacing zero-probability with a very small constant (1.0 × 10−4). 2.2 EM Algorithm The Expectation Maximization (EM) algorithm (Dempster et al., 1977) is a general framework for estimating the parameters of a probability model when the data has missing values. This algorithm can be applied to minimally supervised learning, in which the missing values correspond to missing labels of the examples. The EM algorithm consists of the E-step in which the expected values of the missing sufficient statistics given the observed data and the current parameter estimates are computed, and the M-step in which the expected values of the sufficient statistics computed in the E-step are used to compute complete data maximum likelihood estimates of the param</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Royal Statstical Society B 39, pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L arquez</author>
<author>G Rigau</author>
</authors>
<title>Naive bayes and exemplar-based approaches to word sense disambiguation revisited.</title>
<date>2000</date>
<booktitle>In Proceedings of the 14th European Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="4697" citStr="Escudero et al., 2000" startWordPosition="704" endWordPosition="708">presents the idea of using a class distribution constraint and how to impose this constraint on the learning process. Section 4 describes the problem of confusion set disambiguation and the features used in the experiments. Experimental results are presented in Section 5. Related work is discussed in Section 6. Section 7 offers some concluding remarks. 2 Naive Bayes Classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing, information retrieval, etc. (Escudero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000; Pedersen, 2000). In this section, we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data. 2.1 Naive Bayes Model Let x~ be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x~ belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(�x|ck) P(ck|�x) = P(ck) × P (~x) . (1) Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of </context>
</contexts>
<marker>Escudero, arquez, Rigau, 2000</marker>
<rawString>G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes and exemplar-based approaches to word sense disambiguation revisited. In Proceedings of the 14th European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>A winnowbased approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="11396" citStr="Golding and Roth, 1999" startWordPosition="1814" endWordPosition="1817">isambiguation We applied the naive Bayes classifier with the EM algorithm to confusion set disambiguation. Confusion set disambiguation is defined as the problem of choosing the correct word from a set of words that are commonly confused. For example, quite may easily be mistyped as quiet. An automatic proofreading system would need to judge which is the correct use given the context surrounding the target. Example confusion sets include: {principle, principal}, {then, than}, and {weather, whether}. Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context. The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data. 4.1 Features As the input of the c</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>Andrew R. Golding and Dan Roth. 1999. A winnowbased approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proc. 16th International Conf. on Machine Learning,</booktitle>
<pages>200--209</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="2617" citStr="Joachims, 1999" startWordPosition="381" endWordPosition="382">ions. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotraining. However, the use of unlabeled data via the basic EM algorithm does not always improve classification performance. In fact, this often causes d</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In Proc. 16th International Conf. on Machine Learning, pages 200–209. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive Bayes at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398,</booktitle>
<pages>4--15</pages>
<publisher>Springer Verlag,</publisher>
<location>Chemnitz, DE.</location>
<contexts>
<context position="4710" citStr="Lewis, 1998" startWordPosition="709" endWordPosition="710">ing a class distribution constraint and how to impose this constraint on the learning process. Section 4 describes the problem of confusion set disambiguation and the features used in the experiments. Experimental results are presented in Section 5. Related work is discussed in Section 6. Section 7 offers some concluding remarks. 2 Naive Bayes Classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing, information retrieval, etc. (Escudero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000; Pedersen, 2000). In this section, we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data. 2.1 Naive Bayes Model Let x~ be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x~ belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(�x|ck) P(ck|�x) = P(ck) × P (~x) . (1) Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparsenes</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive Bayes at forty: The independence assumption in information retrieval. In Claire N´edellec and C´eline Rouveirol, editors, Proceedings of ECML-98, 10th European Conference on Machine Learning, number 1398, pages 4–15, Chemnitz, DE. Springer Verlag, Heidelberg, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction. In</title>
<date>1997</date>
<booktitle>Proc. 14th International Conference on Machine Learning,</booktitle>
<pages>187--194</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11489" citStr="Mangu and Brill, 1997" startWordPosition="1825" endWordPosition="1828">ambiguation. Confusion set disambiguation is defined as the problem of choosing the correct word from a set of words that are commonly confused. For example, quite may easily be mistyped as quiet. An automatic proofreading system would need to judge which is the correct use given the context surrounding the target. Example confusion sets include: {principle, principal}, {then, than}, and {weather, whether}. Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context. The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data. 4.1 Features As the input of the classifier, the context of the target must be represented in the form of a vector. We use a bi</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>Lidia Mangu and Eric Brill. 1997. Automatic rule acquisition for spelling correction. In Proc. 14th International Conference on Machine Learning, pages 187– 194. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="6209" citStr="McCallum and Nigam, 1998" startWordPosition="959" endWordPosition="963">) and classify x~ into the class with the highest P(ck|x). Note that the naive Bayes classifier assumes the conditional independence of features. This assumption however does not hold in most cases. For example, word occurrence is a commonly used feature for text classification. However, obvious strong dependencies exist among word occurrences. Despite this apparent violation of the assumption, the naive Bayes classifier exhibits good performance for various natural language processing tasks. There are some implementation variants of the naive Bayes classifier depending on their event models (McCallum and Nigam, 1998). In this paper, we adopt the multi-variate Bernoulli event model. Smoothing was done by replacing zero-probability with a very small constant (1.0 × 10−4). 2.2 EM Algorithm The Expectation Maximization (EM) algorithm (Dempster et al., 1977) is a general framework for estimating the parameters of a probability model when the data has missing values. This algorithm can be applied to minimally supervised learning, in which the missing values correspond to missing labels of the examples. The EM algorithm consists of the E-step in which the expected values of the missing sufficient statistics give</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for naive bayes text classification. In AAAI-98 Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>In CIKM,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="2558" citStr="Nigam and Ghani, 2000" startWordPosition="372" endWordPosition="375">to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotraining. However, the use of unlabeled data via the basic EM algorithm does not always impro</context>
<context position="4733" citStr="Nigam and Ghani, 2000" startWordPosition="711" endWordPosition="714">istribution constraint and how to impose this constraint on the learning process. Section 4 describes the problem of confusion set disambiguation and the features used in the experiments. Experimental results are presented in Section 5. Related work is discussed in Section 6. Section 7 offers some concluding remarks. 2 Naive Bayes Classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing, information retrieval, etc. (Escudero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000; Pedersen, 2000). In this section, we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data. 2.1 Naive Bayes Model Let x~ be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x~ belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(�x|ck) P(ck|�x) = P(ck) × P (~x) . (1) Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By </context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. In CIKM, pages 86–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites Mccallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="2443" citStr="Nigam et al., 2000" startWordPosition="354" endWordPosition="357">s generally requires a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two inde</context>
</contexts>
<marker>Nigam, Mccallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites Mccallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Rebecca Bruce</author>
</authors>
<title>Knowledge lean word-sense disambiguation.</title>
<date>1998</date>
<booktitle>In AAAI/IAAI,</booktitle>
<pages>800--805</pages>
<contexts>
<context position="2470" citStr="Pedersen and Bruce, 1998" startWordPosition="358" endWordPosition="362"> a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotrain</context>
</contexts>
<marker>Pedersen, Bruce, 1998</marker>
<rawString>Ted Pedersen and Rebecca Bruce. 1998. Knowledge lean word-sense disambiguation. In AAAI/IAAI, pages 800–805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="4750" citStr="Pedersen, 2000" startWordPosition="715" endWordPosition="716">and how to impose this constraint on the learning process. Section 4 describes the problem of confusion set disambiguation and the features used in the experiments. Experimental results are presented in Section 5. Related work is discussed in Section 6. Section 7 offers some concluding remarks. 2 Naive Bayes Classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing, information retrieval, etc. (Escudero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000; Pedersen, 2000). In this section, we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data. 2.1 Naive Bayes Model Let x~ be a vector we want to classify, and ck be a possible class. What we want to know is the probability that the vector x~ belongs to the class ck. We first transform the probability P(ck|x) using Bayes’ rule, P(�x|ck) P(ck|�x) = P(ck) × P (~x) . (1) Class probability P(ck) can be estimated from training data. However, direct estimation of P(ck|x) is impossible in most cases because of the sparseness of training data. By assuming the cond</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. 2000. A simple approach to building ensembles of naive bayesian classifiers for word sense disambiguation. In Proceedings of the First Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 63–69, Seattle, WA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M W Powers</author>
</authors>
<title>Learning and application of differential grammars.</title>
<date>1998</date>
<booktitle>CoNLL97: Computational Natural Language Learning,</booktitle>
<pages>88--96</pages>
<editor>In T. Mark Ellison, editor,</editor>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="11434" citStr="Powers, 1998" startWordPosition="1820" endWordPosition="1821">ier with the EM algorithm to confusion set disambiguation. Confusion set disambiguation is defined as the problem of choosing the correct word from a set of words that are commonly confused. For example, quite may easily be mistyped as quiet. An automatic proofreading system would need to judge which is the correct use given the context surrounding the target. Example confusion sets include: {principle, principal}, {then, than}, and {weather, whether}. Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context. The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data. 4.1 Features As the input of the classifier, the context of the target m</context>
</contexts>
<marker>Powers, 1998</marker>
<rawString>David M. W. Powers. 1998. Learning and application of differential grammars. In T. Mark Ellison, editor, CoNLL97: Computational Natural Language Learning, pages 88–96. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>the University of Pennsylvania.</institution>
<contexts>
<context position="1675" citStr="Ratnaparkhi, 1998" startWordPosition="236" endWordPosition="238">stimated from labeled data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small. 1 Introduction Many of the tasks in natural language processing can be addressed as classification problems. State-of-theart machine learning techniques including Support Vector Machines (Vapnik, 1995), AdaBoost (Schapire and Singer, 2000) and Maximum Entropy Models (Ratnaparkhi, 1998; Berger et al., 1996) provide high performance classifiers if one has abundant correctly labeled examples. However, annotating a large set of examples generally requires a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, the University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="1629" citStr="Schapire and Singer, 2000" startWordPosition="228" endWordPosition="231">unlabeled data consistent with the class distribution estimated from labeled data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small. 1 Introduction Many of the tasks in natural language processing can be addressed as classification problems. State-of-theart machine learning techniques including Support Vector Machines (Vapnik, 1995), AdaBoost (Schapire and Singer, 2000) and Maximum Entropy Models (Ratnaparkhi, 1998; Berger et al., 1996) provide high performance classifiers if one has abundant correctly labeled examples. However, annotating a large set of examples generally requires a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than a</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<location>New York.</location>
<contexts>
<context position="1591" citStr="Vapnik, 1995" startWordPosition="225" endWordPosition="226">he class distribution of unlabeled data consistent with the class distribution estimated from labeled data, preventing the EM algorithm from converging into an undesirable state. Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small. 1 Introduction Many of the tasks in natural language processing can be addressed as classification problems. State-of-theart machine learning techniques including Support Vector Machines (Vapnik, 1995), AdaBoost (Schapire and Singer, 2000) and Maximum Entropy Models (Ratnaparkhi, 1998; Berger et al., 1996) provide high performance classifiers if one has abundant correctly labeled examples. However, annotating a large set of examples generally requires a huge amount of human labor and time. This annotation cost is one of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabele</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french.</title>
<date>1994</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="11522" citStr="Yarowsky, 1994" startWordPosition="1831" endWordPosition="1832">n is defined as the problem of choosing the correct word from a set of words that are commonly confused. For example, quite may easily be mistyped as quiet. An automatic proofreading system would need to judge which is the correct use given the context surrounding the target. Example confusion sets include: {principle, principal}, {then, than}, and {weather, whether}. Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context. The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data. 4.1 Features As the input of the classifier, the context of the target must be represented in the form of a vector. We use a binary feature vector which contain</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>David Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french. In Meeting of the Association for Computational Linguistics, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="23568" citStr="Yarowsky (1995)" startWordPosition="3870" endWordPosition="3871">, they reported that the use of unlabeled data reduces classification error by up to 30%. Pedersen et al.(1998) employed the EM algorithm and Gibbs Sampling for word sense disambiguation by using a naive Bayes classifier. Although Gibbs Sampling results in a small improvement over the EM algorithm, the results for verbs and adjectives did not reach baseline perPrecision (%) 100 95 90 85 80 75 70 65 60 I, me principal, principle passed, past Precision (%) formance on average. The amount of unlabeled data used in their experiments was relatively small (from several hundreds to a few thousands). Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed for word sense disambiguation. Yarowsky achieved accuracies of more than 90% for two-sense polysemous words. This success was likely due to the use of “one sense per discourse” characteristic of polysemous words. Yarowsky’s approach can be viewed in the context of co-training (Blum and Mitchell, 1998) in which the features can be split into two independent sets. For word sense disambiguation, the sets correspond to the local contexts of the target word and the “one sense per discourse” characteristic. Confusion</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>