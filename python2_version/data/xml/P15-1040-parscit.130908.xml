<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.908357">
Aligning Opinions: Cross-Lingual Opinion Mining with Dependencies
</title>
<author confidence="0.504542">
Mariana S. C. Almeida*† Cl´audia Pinto* Helena Figueira*
</author>
<affiliation confidence="0.455530666666667">
Pedro Mendes* Andr´e F. T. Martins*†
*Priberam Labs, Alameda D. Afonso Henriques, 41, 2°, 1000-123 Lisboa, Portugal
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal
</affiliation>
<email confidence="0.992369">
{mla,atm}@priberam.pt
</email>
<sectionHeader confidence="0.997284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998332">
We propose a cross-lingual framework for
fine-grained opinion mining using bitext
projection. The only requirements are a
running system in a source language and
word-aligned parallel data. Our method
projects opinion frames from the source to
the target language, and then trains a sys-
tem on the target language using the auto-
matic annotations. Key to our approach is
a novel dependency-based model for opin-
ion mining, which we show, as a byprod-
uct, to be on par with the current state
of the art for English, while avoiding the
need for integer programming or rerank-
ing. In cross-lingual mode (English to Por-
tuguese), our approach compares favor-
ably to a supervised system (with scarce
labeled data), and to a delexicalized model
trained using universal tags and bilingual
word embeddings.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974384615385">
The goal of opinion mining is to extract opinions
and sentiments from text (Pang and Lee, 2008;
Wilson, 2008; Liu, 2012). With the advent of so-
cial media and the increasing amount of data avail-
able on the Web, this has become a very active
area of research, with applications in summariza-
tion of customer reviews (Hu and Liu, 2004; Wu et
al., 2011), tracking of newswire and blogs (Ku et
al., 2006), question answering (Yu and Hatzivas-
siloglou, 2003), and text-to-speech synthesis (Alm
et al., 2005).
While early work has focused on determining
sentiment at document and sentence level (Pang
et al., 2002; Turney, 2002; Balog et al., 2006),
research has gradually progressed towards fine-
grained opinion mining, where rather than deter-
mining global sentiment, the goal is to parse text
into opinion frames, identifying opinion expres-
sions, agents, targets, and polarities (Ding et al.,
2008), or addressing compositionality (Socher et
al., 2013b). Since the release of the MPQA cor-
pus1 (Wiebe et al., 2005; Wilson, 2008), a stan-
dard corpus for fine-grained opinion mining of
news documents, a long string of work has been
produced (reviewed in §2). Despite the large vol-
ume of prior work, opinion mining has by and
large been limited to monolingual approaches in
English.2 This is explained by the heavy effort
of annotation necessary for current learning-based
approaches to succeed, which delays the deploy-
ment of opinion miners for new languages.
We bridge the existing gap by proposing a
cross-lingual approach to fine-grained opinion
mining via bitext projection. This technique has
been quite effective in several NLP tasks, such
as part-of-speech (POS) tagging (T¨ackstr¨om et
al., 2013), named entity recognition (Wang and
Manning, 2014), syntactic parsing (Yarowsky and
Ngai, 2001; Hwa et al., 2005), semantic role label-
ing (Pad´o and Lapata, 2009), and coreference res-
olution (Martins, 2015). Given a corpus of parallel
sentences (bitext), the idea is to run a pre-trained
system on the source side and then to use word
alignments to transfer the produced annotations to
the target side, creating an automatic training cor-
pus for the impoverished language.
To alleviate the complexity of the task, we
start by introducing a lightweight representation—
called dependency-based opinion mining—and
convert the MPQA corpus to this formalism (§3).
We propose a simple arc-factored model that per-
mits easy decoding (§4) and we show that, despite
</bodyText>
<footnote confidence="0.992062333333333">
1http://mpqa.cs.pitt.edu/corpora/mpqa_
corpus.
2Besides English, monolingual systems have also been
developed for Chinese and Japanese (Seki et al., 2007), Ger-
man (Clematide et al., 2012) and Bengali (Das and Bandy-
opadhyay, 2010).
</footnote>
<page confidence="0.872489">
408
</page>
<note confidence="0.972672666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 408–418,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999822625">
its simplicity, this model is on par with state-of-
the-art opinion mining systems for English (§5).
Then, through bitext projection, we transfer these
dependency-based opinion frames to Portuguese
(our target language), and train a system on the
resulting corpus (§6).
As part of this work, a validation corpus in Por-
tuguese with subjectivity annotations was created,
along with a translation of the MPQA Subjectiv-
ity lexicon of Wilson et al. (2005).3 Experimental
evaluation (§7) shows that our cross-lingual ap-
proach surpasses a supervised system trained on
a small corpus in the target language, as well as a
delexicalized baseline trained using universal POS
tags, bilingual word embeddings and a projected
lexicon.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999516096774193">
A considerable amount of work on fine-grained
opinion mining is based on the MPQA corpus.
Kim and Hovy (2006) proposed a method for find-
ing opinion holders and topics, with the aid of a se-
mantic role labeler. Choi et al. (2005) and Breck et
al. (2007) used CRFs for finding opinion holders
and recognizing opinion expressions, respectively.
The two things are predicted jointly by Choi et al.
(2006), with integer programming, and Johansson
and Moschitti (2010), via reranking. The same
method was applied later for joint prediction of
opinion expressions and their polarities (Johans-
son and Moschitti, 2011). The advantage of a
joint model was also shown by Choi and Cardie
(2010) and Yang and Cardie (2014). Yang and
Cardie (2012) classified expressions with a semi-
Markov decoder, outperforming a B-I-O tagger; in
later work, the same authors proposed an ILP de-
coder to jointly retrieve opinion expressions, hold-
ers, and targets (Yang and Cardie, 2013). A more
recent work (˙Irsoy and Cardie, 2014) proposes a
recurrent neural network to identify opinion spans.
All the approaches above rely on a span-based
representation of the opinion elements. This
makes joint decoding procedures more compli-
cated, since they must forbid overlap of opinion
elements or add further constraints, leading to in-
teger programming or reranking strategies. Be-
sides, there is little consensus about what should
be the correct span boundaries, the inter-annotator
agreement being quite low (Wiebe et al., 2005). In
</bodyText>
<footnote confidence="0.9843265">
3The Portuguese corpus and the lexicon are available at
http://labs.priberam.com/Resources.
</footnote>
<bodyText confidence="0.999918612903226">
constrast, we use dependencies to model opinion
elements and relations, leading to a compact repre-
sentation that does not depend on spans and which
is tractable to decode. A dependency scheme was
also used by Wu et al. (2011) for fine-grained
opinion mining. Our work differs in which we
mine opinions in news articles instead of product
reviews, a considerably different task. In addition,
the approach of Wu et al. (2011) relies on “span
nodes” (instead of head words), requiring solving
an ILP followed by an approximate heuristic.
Query-based multilingual opinion mining was
addressed in several NTCIR shared tasks (Seki
et al., 2007; Seki et al., 2010).4 However, to
our best knowledge, a cross-lingual approach has
never been attempted. Some steps were taken by
Mihalcea et al. (2007) and Banea et al. (2008),
who translated an English lexicon and the MPQA
corpus to Romanian and Spanish, but for the much
simpler task of sentence-level subjectivity anal-
ysis. Cross-lingual sentiment classification was
addressed by Wan (2009), Prettenhofer and Stein
(2010) and Wei and Pal (2010) at document level,
and by Lu et al. (2011) at sentence level. Recently,
Gui et al. (2013) applied projection learning for
opinion mining in Chinese. However, this work
only addresses agent detection and requires trans-
lating the MPQA corpus. While all these works
are relevant, none addresses fine-grained opinion
mining in its full generality, where the goal is to
predict full opinion frames.
</bodyText>
<sectionHeader confidence="0.992456" genericHeader="method">
3 Dependency-Based Opinion Mining
</sectionHeader>
<bodyText confidence="0.9930975">
This work addresses various elements of subjec-
tivity annotated in the MPQA corpus, namely:
</bodyText>
<listItem confidence="0.951125222222222">
• direct-subjective expressions (henceforth, opin-
ions) that are direct mentions of a private state,
e.g. opinions, beliefs, emotions, sentiments,
speculations, goals, etc.;
• the opinion agent, i.e., the holder of the opinion;
• the opinion target, i.e., what is being argued
about;
• the opinion polarity, i.e., the sentiment (posi-
tive, negative or neutral) towards the target.
</listItem>
<bodyText confidence="0.9986515">
As an example, consider the sentence in Fig-
ure 1, which has two opinions, expressed by the
</bodyText>
<footnote confidence="0.982212666666667">
4NTCIR-8 had a cross-lingual track but in a very differ-
ent sense: there, queries and documents are in different lan-
guages; in contrast, we transfer a model accross languages.
</footnote>
<page confidence="0.998886">
409
</page>
<bodyText confidence="0.999438166666667">
spans “is believed” (O1) and “are against” (O2).
The first opinion has an implicit agent and a neu-
tral polarity toward the target “the rich elites”
(T1). This target is also the agent (A2) of the sec-
ond opinion, which has a negative polarity toward
“Hugo Ch´avez” (T2).
</bodyText>
<subsectionHeader confidence="0.998811">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999983666666667">
As noted in prior work (Choi et al., 2005; Kim and
Hovy, 2006; Johansson and Moschitti, 2010), one
source of difficulty when learning opinion min-
ers on MPQA is with the boundaries of the en-
tity spans. The fact that no criterion for choosing
these boundaries is explicitly defined in the anno-
tation guidelines (Wiebe et al., 2005) leads to a
low inter-annotator agreement. To circumvent this
problem and make the learning task easier, we de-
part from the classical span-based approaches to-
ward dependency-based opinion mining. This
decision is inspired by the success of dependency
models for syntax and semantics (Buchholz and
Marsi, 2006; Surdeanu et al., 2008). These depen-
dency relations can be further converted to opinion
spans (as described in §3.3), or directly used as
features in downstream applications. As we will
see, a compact representation based on dependen-
cies can achieve state-of-the-art results and has the
advantage of being easily transferred to other lan-
guages through a parallel corpus.
</bodyText>
<subsectionHeader confidence="0.999542">
3.2 Dependency Graph
</subsectionHeader>
<bodyText confidence="0.999740128205128">
Figure 1 depicts a sentence-level dependency rep-
resentation for fine-grained opinion mining. The
overall structure is a graph whose nodes are head
words (plus two special nodes, root and null),
connected by labeled arcs, as outlined below.
Determining head nodes. The three opinion el-
ements that we want to detect (opinions, agents
and targets) are each represented by a head node,
which corresponds to a single word (underlined in
Figure 1). When converting the MPQA corpus
to dependencies, we determine this “representa-
tive” word automatically, by using the following
simple heuristic: we first parse the sentence us-
ing the Stanford dependency parser (Socher et al.,
2013a); then, we pick the last word in the span
whose syntactic parent is outside the span (if the
span is a syntactic phrase, there is only one word
whose parent is outside the span, which is the lex-
ical head). The same heuristic has been used for
identifying the heads of mention spans in corefer-
ence resolution (Durrett and Klein, 2013).
Defining labeled arcs. The opinion relations are
represented as labeled arcs that link these head
nodes. Two artificial nodes are added: a root
node, which links to all nodes that represent opin-
ion words, with the label OPINION; and a null
node, which is used for representing implicit re-
lations. To represent opinion-agent relations, we
draw an arc labeled AGENT toward the agent word.
For opinion-target relations, the arc is toward the
target word and has one of the labels TARGET:0,
TARGET:+, or TARGET:-; this encodes the polarity
in addition to the type of relation. We also include
implicit arcs for opinion elements whose agent or
target is not mentioned inside the sentence—these
are modeled as arcs pointing to the null node.
Dependency opinion graph. We have the fol-
lowing requirements for a well-formed depen-
dency opinion graph:
</bodyText>
<listItem confidence="0.974511125">
1. No self-arcs or arcs linking root to null.
2. An arc is labeled as OPINION if and only if it
comes from the root node.
3. Arcs labeled as AGENT or TARGET must come
from an opinion node (i.e., a node with an in-
coming OPINION arc).
4. Every opinion node has exactly one AGENT and
one TARGET outgoing arcs (possibly implicit).5
</listItem>
<bodyText confidence="0.999462875">
Similarly to prior work (Choi and Cardie, 2010;
Johansson and Moschitti, 2011; Johansson and
Moschitti, 2013), we map the MPQA’s polarity-
into three levels: positive, negative and neutral,
where the latter includes spans without polarity
annotation or annotated as “both”. As in Johans-
son and Moschitti (2013), we also ignore the “un-
certain” aspect of the annotated polarities.
</bodyText>
<subsectionHeader confidence="0.986433">
3.3 Dependency-to-Span Conversion
</subsectionHeader>
<bodyText confidence="0.9996556">
To evaluate the opinion miner against manual an-
notations and compare with other systems, we
need a procedure to convert back from predicted
dependencies to spans. In this work, we used
a very simple procedure that we next describe,
</bodyText>
<footnote confidence="0.655113">
5Even though this assumption is not always met in prac-
tice, it is typical in MPQA (only 10% of the opinions have
multiple agents, typically coreferent; and only 13% have
multiple targets). When multiple agents or targets exist, we
keep the ones that are closest to the opinion expression.
</footnote>
<page confidence="0.992363">
410
</page>
<figureCaption confidence="0.999858">
Figure 1: Example of an opinion mining graph in our dependency formalism. Heads are underlined.
</figureCaption>
<bodyText confidence="0.999372823529412">
which assumes the sentence was previously parsed
using a syntactic dependency parser.
To generate agent and target spans, we compute
the largest span, containing the head word, whose
words are all descendants in the dependency parse
tree and that are, simultaneously, not punctuations.
To generate opinion spans, we start with the head
word and expand the span by adding all neigh-
bouring verbal words. In the case of English, we
also allow adverbs, adjectives, modal verbs and
the word to, when expanding to the left.
The application of this simple approach to the
gold dependency graphs in the training partition
of the MPQA leads to oracle F1 scores of 86.0%,
95.8% and 93.0% in the reconstruction of opinion,
agent and target spans, respectively, according to
the proportional scores described in §5.2.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="method">
4 Arc-Factored Model
</sectionHeader>
<bodyText confidence="0.999967166666667">
One of the advantages of the dependency represen-
tation is that we can easily decode opinion-agent-
target relations without the need of complicated
constrained sequence models or integer program-
ming, as done in prior work (Choi et al., 2006;
Yang and Cardie, 2012; Yang and Cardie, 2013).
</bodyText>
<subsectionHeader confidence="0.996989">
4.1 Decoding
</subsectionHeader>
<bodyText confidence="0.999997">
We model dependency-based opinion mining as a
structured classification problem. Let x be a sen-
tence and y E Y(x) a set of well-formed depen-
dency graphs, according to the constraints stated
in §3. We define a score function that decomposes
as a sum of labeled arc scores,
</bodyText>
<equation confidence="0.762923">
f(x, y) = � fa(x, ya) (1)
a∈y
</equation>
<bodyText confidence="0.999243666666667">
where ya is a labeled arc and the sum is over the
arcs of the graph y. We use a linear model with
weight vector w and local features Oa(x, ya):
</bodyText>
<equation confidence="0.76348925">
fa(x, ya) = w - Oa(x, ya). (2)
For making predictions, we need to compute
y = arg max)f (x, y). (3)
Y(x
</equation>
<bodyText confidence="0.999622142857143">
Under the assumptions stated in §3, this problem
decouples into independent maximization prob-
lems (one for each possible opinion word in the
sentence). The detailed procedure is as follows,
where arcs a can take the form o —* h (opinion to
agent) and o —* t (opinion to target). For every
candidate opinion word o:
</bodyText>
<listItem confidence="0.9772558">
1. Obtain the most compatible agent word, h :=
arg maxh fo→h(x, AGENT);
2. Obtain the best target word and its polarity,
(�t, -) := arg maxt,p fo→t(x, TARGET:p);
3. Compute the total score of this candidate
opinion as so := froot→o(x, OPINION) +
fo→h(x, AGENT) + fo→�x, TARGET:�p). Then,
if so &gt; 0, add the arcs root —* o, o —* h, and
o —* t to the dependency graph, respectively
with labels OPINION, AGENT, and TARGET:p.
</listItem>
<bodyText confidence="0.999812666666667">
For a sentence with L words, this decoding pro-
cedure takes O(L2) time. In practice, we speed
up this process by pruning from the candidate list
arcs whose connected POS were not observed in
the training set and whose length were larger than
the ones observed in the training set.
</bodyText>
<subsectionHeader confidence="0.915394">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.999928428571429">
We now describe our features Oa, which are
computed after processing the sentence to predict
POS tags, syntactic dependency trees, lemmas and
voice (active or passive) information. For English,
we used the Stanford dependency parser (Socher
et al., 2013a) for the syntactic annotations, the
Porter stemmer to compute word stems, and a set
of rules for computing the voice of each word. Our
Portuguese corpus include all these preprocessing
elements (§6.3), with the exception of the voice in-
formation (features depending on voice were only
used for English).
We also used the Subjectivity Lexicon6 of Wil-
son et al. (2005) that we translated to Portuguese
</bodyText>
<footnote confidence="0.9797145">
6http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
</footnote>
<page confidence="0.996371">
411
</page>
<bodyText confidence="0.995592307692308">
(§6.3), and a set of negation words (e.g. not, never,
nor) and quantity words (e.g. very, much, less)
collected for both languages.
Our arc-factored features are described below;
they are inspired by prior work on dependency
parsing (Martins et al., 2013) and fine-grained
opinion mining (Breck et al., 2007; Johansson and
Moschitti, 2013).
Opinion features. We define a set of features
that only look at the opinion word; special sym-
bols are used if the opinion is connected to a root
or null node. The features below are also con-
joined with the arc label.
</bodyText>
<listItem confidence="0.998267964285714">
• OPINION WORD. The word itself, the lemma,
the POS, and the voice. Conjunction of the word
with the POS, and of the lemma with the POS.
• BIGRAMS. Bigrams of words and POS corre-
sponding to the opinion word conjoined with its
previous (and next) word.
• LEXICON (BASIC). Conjunction of the strength
and polarity of the opinion word in the Subjec-
tivity Lexicon6 (e.g., “weaksubj+neg”).
• LEXICON (COUNT). Number of subjective
words (total, positive and negative) in a sen-
tence, with and without being conjoined with
the polarity of the opinion word in the lexicon.
• LEXICON (CONTEXT). For each word that is in
the lexicon and within the 4-word context of the
opinion, the form and the polarity of that word
in the lexicon, with and without being conjoined
with the form and the polarity in the lexicon of
the opinion word. Besides the 4-word context,
we also used the next/previous word in the sen-
tence which is in the lexicon.
• NEGATION AND QUANTITY WORDS. Within
the 4-word context, features indicating if a word
is a negation or quantity word, conjoined with
the word itself and the opinion word.
• SYNTACTIC PATH. The number of words up to
the top of the syntactic dependency tree, and the
sequence of POS tags in that path.
</listItem>
<bodyText confidence="0.995935571428571">
Opinion-Argument features. In case of arcs
that neither connect to null nor root, the fea-
tures above are also conjoined with the binned dis-
tance between the two words.For these arcs, we
did not use the LEXICON (COUNT)/(CONTEXT)
features, but we added features regarding the pair
of opinion-argument words (below).
</bodyText>
<listItem confidence="0.9486078">
• OPINION-ARGUMENT WORD PAIR. Several
conjunctions of word form, POS, voice and syn-
tactic dependency relations corresponding to the
pair opinion-argument.
• OPINION-ARGUMENT SYNTACTIC PATH. The
</listItem>
<bodyText confidence="0.791688">
syntactic path from the opinion word to the ar-
gument, conjoined with the POS and the de-
pendency relations in the path (in Figure 1, for
the agent “elites” headed by “are” with relation
nsuj, we have: “VBP↓NNS” and “nsuj↓”).
For arcs that neither connect to null or root,
we conjoin voice features with the label, distance,
and the direction of the arc. For these arcs, we
also include back-off features where the polarity
information is removed from the (target) labels.
</bodyText>
<sectionHeader confidence="0.998235" genericHeader="method">
5 English Monolingual Experiments
</sectionHeader>
<bodyText confidence="0.999829333333333">
In a first set of experiments, we evaluated the
performance of our dependency-based model for
opinion mining (§3) in the MPQA English corpus.
</bodyText>
<subsectionHeader confidence="0.985964">
5.1 Learning
</subsectionHeader>
<bodyText confidence="0.999982888888889">
We trained arc-factored models by running 25
epochs of max-loss MIRA (Crammer et al., 2006).
Our cost function takes into account mismatches
between predicted and gold dependencies, with
a cost CP on labeled arcs incorrectly predicted
(false positives) and a cost CR = 1 − CP on
missed gold labeled arcs (false negatives). The
cost CP, the regularization constant, and the num-
ber of epochs were tuned in the development set.
</bodyText>
<subsectionHeader confidence="0.988863">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999881">
Opinion spans (Op.) are evaluated with F1 scores,
according to two matching criteria commonly
used in the literature: overlap matching (OM),
where a predicted span is counted as correct if
it overlaps a gold one, and proportional match-
ing (PM), proposed by Johansson and Moschitti
(2010). For the latter, we use the following for-
mula for the recall, where we consider the sets of
gold (G) and predicted (P) opinion spans:7
</bodyText>
<equation confidence="0.981793">
|g np|/|p|. (4)
|P |&apos;
</equation>
<bodyText confidence="0.921002571428571">
7This metric is slightly different from the PM metric of Jo-
hansson and Moschitti (2010), in which recall was computed
as R(G, P) = �p∈P �g∈G |g∩ |p|/|p |The reason why we
replace the “sum” by a “max” is that each predicted span P in
(4) could contribute to the recall with a value greater than 1.
Since most of the predicted spans only overlap a single gold
span, this fix has a very small effect in the final scores.
</bodyText>
<figure confidence="0.757704">
�
R(G, P) =
pEP
max
9EG
</figure>
<page confidence="0.991401">
412
</page>
<bodyText confidence="0.999974">
the precision is P(G, P) = R(P, G). We also re-
port metrics based on a head matching (HM) cri-
terion, where a predicted span is considered cor-
rect if its syntactic head matches the head of the
gold span. We consider that a pair opinion-agent
(Op-Ag.) or opinion-target (Op-Tg.) is correctly
extracted according to the OM or the HM criteria,
if both the elements satisfy these criteria and the
relation holds in the gold data. We also compute
the metric described in Johansson and Moschitti
(2010) which measures how well agents of opin-
ions are predicted based on a proportional match-
ing (PM) criterion. This metric is applied to eval-
uate the extraction of both agents and targets. Fi-
nally, to evaluate the opinions’ polarities (Op-Pol.
metric) we consider as correct opinions where the
span and polarity both match the gold ones.
</bodyText>
<subsectionHeader confidence="0.883263">
5.3 Results: Dependency-Based Model
</subsectionHeader>
<bodyText confidence="0.999976714285714">
We assess the quality of our monolingual
dependency-based model by comparing it to the
recent state-of-the-art approach of Johansson and
Moschitti (2013), whose code is available online.8
That paper reports the performance of a basic
span-based pipeline system (which extracts opin-
ions with a CRF, followed by two separate classi-
fiers to detect polarities and agents), and of a more
sophisticated system that applies a reranking pro-
cedure to account for more complex features that
consider interactions accross opinion elements.
We ran experiments using the same data and
MPQA partitions as Johansson and Moschitti
(2013). However, since our system is designed for
predicting opinion, agents and targets together, we
removed the documents that were not annotated
with targets. The final train/development/test sets
have a total of 6,774/1,404/2,559 sentences and
3,834/881/1,426 opinions, respectively.
Table 1 reports the results; since the systems
of Johansson and Moschitti (2013) do not pre-
dict targets, Table 1 omits target scores.9 We ob-
serve that our dependency-based system achieves
results competitive with the best results of Johans-
son and Moschitti (2013) and clearly above the
ones reached by their basic system that does not
use re-ranking features. Though the two systems
are not fully comparable,10 the results in Table 1
</bodyText>
<footnote confidence="0.974589">
8http://demo.spraakdata.gu.se/richard/
unitn_opinion/details.html
9We will report target scores later in §7.
10Our system makes use of target annotations to predict
the opinion frames, while Johansson and Moschitti (2013)
</footnote>
<bodyText confidence="0.999980222222222">
show that our dependency-based approach (§3.2)
followed by a simple dependency-to-span conver-
sion (§3.3) is, despite its simplicity, on par with
a top-performing opinion mining system. We con-
jecture that this is due to the ability to extract opin-
ions, agents, and targets jointly using exact decod-
ing. Note that our proposed dependency scheme
would also be able to include additional global fea-
tures relating pairs of opinions (by adding scores
to pairs of opinion arcs) or two opinions having
the same agent (by adding scores to pairs of agent
arcs sharing its argument), similar to the reranking
features used by Johansson and Moschitti (2013).
Similar second-order scores have been used in
syntactic and semantic dependency parsing (Mar-
tins et al., 2013; Martins and Almeida, 2014), but
with an increase in the complexity of the model
and of the decoder.
</bodyText>
<sectionHeader confidence="0.993913" genericHeader="method">
6 Cross-Lingual Opinion Mining
</sectionHeader>
<bodyText confidence="0.999904166666667">
We now turn to the problem of learning a opin-
ion mining system for a resource-poor language
(Portuguese), in a cross-lingual manner. We use
a bitext projection approach (§6.1), whose only
requirements are a model for a resource-rich lan-
guage (English) and parallel data (§6.2).
</bodyText>
<subsectionHeader confidence="0.997827">
6.1 Bitext Projection
</subsectionHeader>
<bodyText confidence="0.999553625">
Our methodology is outlined as Algorithm 1. For
simplicity, we call the source and target languages
English (e) and “foreign” (f), respectively. The
procedure is inspired by the idea of bitext pro-
jection (Yarowsky and Ngai, 2001). We start by
training an English system on the labeled data Ge
(line 1), which in our case is the MPQA v.2.0 cor-
pus. This system is then used to label the English
side of the parallel data, automatically identifying
opinion frames (line 2). The next step is to run
a word aligner on the parallel data (line 3). The
automatic alignments are then used to project the
opinion frames to the target language (along _with
i5 )
(line 4), which finally serves to train a system for
the target language (line 5).
</bodyText>
<subsectionHeader confidence="0.999616">
6.2 Parallel Data
</subsectionHeader>
<bodyText confidence="0.988928142857143">
We use an English-Portuguese parallel corpus
based on the scientific news Brazilian magazine
Revista Pesquisa FAPESP, collected by Aziz and
has access not only to direct subjective spans but also to sub-
jective expressions annotations with their agents and polarity
information.
some filtering), yielding an automatic corpus
</bodyText>
<page confidence="0.994364">
413
</page>
<table confidence="0.997622833333333">
JM13, BASIC JM13, RERANKING HM OUR SYSTEM
HM PM OM HM PM OM PM OM
Op. 56.3 56.2 60.6 58.6 59.2 63.7 61.6* 59.8 65.1
Op-Ag. 40.3 47.1 44.9 42.4 51.4 48.1 45.7* 51.4 50.3*
Op-Tg. - - - - - - 31.3* 48.3* 48.3*
Op-Pol. 46.1 45.9 49.3 48.5 48.9* 52.5 47.9 47.0 50.7
</table>
<tableCaption confidence="0.983472">
Table 1: Method comparison: F1 scores obtained in the MPQA corpus, for our dependency based method
</tableCaption>
<figureCaption confidence="0.821161833333333">
and the approaches in Johansson and Moschitti (2013), with and without reranking. The symbol *
indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and
according to a bootstrap resampling test (Koehn, 2004).
Figure 2: Excerpt of a bitext document from FAPESP, with automatic opinion dependencies. The anno-
tations are directly projected to Portuguese via automatic word alignments.
Algorithm 1 Cross-Lingual Opinion Mining
</figureCaption>
<bodyText confidence="0.918466">
Input: Labeled data Le, parallel data De and Df.
Output: Target opinion mining system Sf.
</bodyText>
<listItem confidence="0.9969988">
1: Se ← LEARNOPINIONMINER(Le)
2: bDe ← RUNOPINIONMINER(Se, De)
3: Dbe↔f ← RUNWORDALIGNER(De, Dbf )
4: Df ← PROJECTANDFILTER(De↔f , De)
5: Sf ← LEARNOPINIONMINER(bDf)
</listItem>
<bodyText confidence="0.99984262962963">
Specia (2011). Though this corpus is in Brazil-
ian Portuguese (while our validation corpus is in
European Portuguese), we preferred FAPESP over
other commonly used parallel corpora (such as the
Europarl and UN datasets), since it is closer to
our newswire target domain, with a smaller promi-
nence of direct speech. We computed word align-
ments using the Berkeley aligner (Liang et al.,
2006), intersected them and filtered out all the
alignments whose confidence is below 0.95.
After annotating the English side of FAPESP
with the pre-trained system ( bDe in Algorithm 1,
with a total of 166,719 sentences and 81,492 opin-
ions), the high confidence alignments (De↔f) are
used to project the annotations to the Portuguese
side of the corpus. The automatic annotations pro-
duced by our dependency-based system are easily
transferred at a word level (for words with high
confidence alignments), as illustrated in Figure 2.
To improve the quality of the resulting corpus,
we excluded sentences whose alignments cover
less than 70% of the words in the target side of
the corpus, or sentences whose opinion elements
were not fully projected through high confidence
alignments. At this point, we obtain anbautomat-
ically annotated corpus in Portuguese (Df ), with
106,064 sentences and 32,817 opinions.
</bodyText>
<subsectionHeader confidence="0.99886">
6.3 Portuguese Opinion Mining Corpus
</subsectionHeader>
<bodyText confidence="0.999985538461538">
For validation purposes, we also created a Por-
tuguese corpus with manually annotated fine-
grained opinions. The corpus consists of a sub-
set of the documents of the Priberam Compressive
Summarization Corpus11 (Almeida et al., 2014),
which contains 80 news topics with 10 documents
each, collected from several Portuguese newspa-
pers, TV and radio websites in the biennia 2010–
2011 and 2012–2013. In the scope of the current
work, we selected and annotated one document of
each of the 80 topics. The first biennium was se-
lected as the test set and the second biennium was
split into development and training sets (see Ta-
</bodyText>
<footnote confidence="0.9790935">
11http://labs.priberam.com/Resources/
PCSC
</footnote>
<page confidence="0.988975">
414
</page>
<table confidence="0.8633786">
ble 2 for statistics).
#doc. #sent. #opin.
Train 20 441 240
Dev 20 225 197
Test 40 560 391
</table>
<tableCaption confidence="0.9934625">
Table 2: Number of documents, sentences and
opinions in the Portuguese Corpus.
</tableCaption>
<table confidence="0.9999204">
HM PM OM
Op. 77.0 76.7 79.2
Op-Ag. 69.1 72.3 73.5
Op-Tg. 61.9 65.4 71.4
Op-Pol. 49.4 49.1 50.7
</table>
<tableCaption confidence="0.9167555">
Table 3: Inter-annotator agreement in the test par-
tition (shown are F1 scores).
</tableCaption>
<bodyText confidence="0.999878962962963">
The corpus was annotated in a similar vein as
the MPQA (Wiebe et al., 2005), with the addition
of the head node for each element of the opin-
ion frame. It includes spans for direct-subjective
expressions with intensity and polarity informa-
tion; agent spans; and target spans. The annotation
was carried out by three linguists, after reading the
MPQA annotation guidelines (Wiebe et al., 2005;
Wilson, 2008) and having a small practice period
using the provided examples and some MPQA an-
notated sentences. Each document was annotated
by two of the three linguists and then revised by
the third linguist, who (in case of any doubts) dis-
cussed with the initial annotators to reach for the
final consensus. Scores for inter-annotator agree-
ment are shown in Table 3.
The corpus was annotated with automatic
POS tags and dependency parse trees using
TurboParser (Martins et al., 2013).12 We used
an in-house lemmatizer to obtain lemmas for
each inflected word in the corpus. A Por-
tuguese lexicon of subjectivity was created by
translating the words in the Subjectivity Lex-
icon of Wilson et al. (2005). The annotated
corpus and the translated subjectivity lexicon
are available at http://labs.priberam.com/
Resources/Fine-Grained-Opinion-Corpus,
</bodyText>
<footnote confidence="0.685498">
and http://labs.priberam.com/Resources/
Subjectivity-Lexicon-PT, respectively.
12http://www.ark.cs.cmu.edu/TurboParser
</footnote>
<table confidence="0.999107">
OUR SYSTEM DELEXICALIZED
HM PM OM HM PM OM
Op. 65.7 63.5 69.8 50.1 45.8 52.7
Op-Ag. 47.6 48.8 51.1 33.8 34.8 35.7
Op-Tg. 34.9 44.8 50.3 19.9 28.0 32.1
Op-Pol. 51.5 50.2 54.4 36.7 34.7 38.8
</table>
<tableCaption confidence="0.979466">
Table 4: F1 scores obtained in English (MPQA),
for our full system and the DELEXICALIZED one.
</tableCaption>
<sectionHeader confidence="0.991349" genericHeader="method">
7 Cross-Lingual Experiments
</sectionHeader>
<bodyText confidence="0.996384">
In a final set of experiments, we compare three
systems of fine-grained opinion mining for Por-
tuguese. All were trained as described in §5.1.
</bodyText>
<subsectionHeader confidence="0.99213">
7.1 System Description
</subsectionHeader>
<bodyText confidence="0.973589714285714">
Baseline #1: Supervised System. A SUPER-
VISED system was trained on the small Portuguese
training set described in §6.3. Though being a
small training corpus, this is, to the best of our
knowledge, the only existing corpus with fine-
grained opinions in Portuguese. We used the same
arc-factored model and features described in §4.
</bodyText>
<subsectionHeader confidence="0.589991">
Baseline #2: Delexicalized System with Bilin-
</subsectionHeader>
<bodyText confidence="0.996895916666667">
gual Embeddings. This baseline consists of a
direct model transfer: a DELEXICALIZED system
is trained in the source language, without lan-
guage specific features, so that it can be directly
applied to the target language. Despite its simplic-
ity, this strategy managed to provide a fairly strong
baseline in several NLP tasks (Zeman and Resnik,
2008; McDonald et al., 2011; Søgaard, 2011).
To achieve a unified feature representation, we
mapped all language-specific POS tags to univer-
sal tags (Petrov et al., 2012), and removed all
features depending on the dependency relations,
but maintained those depending on the syntactic
path (but not on the dependency relations them-
selves). In addition, we replaced the lexical fea-
tures by 128-dimensional cross-lingual word em-
beddings.13 To obtain these bilingual neural em-
beddings, we ran the method of Hermann and
Blunsom (2014) on the parallel data (§6.1). We
scaled the embeddings by a factor of 2.0 (selected
on the dev-set), following the procedure described
in Turian et al. (2010).
We trained the English delexicalized system on
the MPQA corpus, using the same test documents
</bodyText>
<footnote confidence="0.621721">
13A delexicalized system trained without the word embed-
dings had a worse performance.
</footnote>
<page confidence="0.994086">
415
</page>
<table confidence="0.998609333333333">
BASELINE #1 (SUP.) BASELINE #2 (DELEX. ) BITEXT PROJECTION
HM PM OM HM PM OM HM PM OM
Op. 49.4 48.7 50.8 33.1 32.1 34.3 58.0* 55.7* 58.0*
Op-Ag. 23.5 27.2 31.5 14.3 18.8 20.0 30.8* 31.2* 36.2*
Op-Tg. 23.0 24.9 30.6 11.0 15.7 19.0 29.4* 29.4* 35.6*
Op-Pol. 24.1 23.8 24.7 16.6 16.4 17.6 35.7* 34.1* 35.7*
</table>
<tableCaption confidence="0.8423645">
Table 5: Comparison of cross-lingual approaches. F1 scores obtained in our Portuguese validation corpus
using: a SUPERVISED system trained on the small available data, a DELEXICALIZED system trained with
</tableCaption>
<bodyText confidence="0.984359041666667">
universal POS tags and multilingual embeddings and our BITEXT PROJECTION OF DEPENDENCIES.
The symbol * indicates that the best system beats the other systems with statistical significance, with
p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004).
as Riloff and Wiebe (2003) and whose list is avail-
able with the corpus, but selecting only documents
annotated with targets. We randomly split the
remaining documents into train and development
sets, respectively with a total of 6,471 and 782 sen-
tences.14 Table 4 shows the performance of the
delexicalized baseline in English, compared with
a lexicalized system. We will see how this model
behaves in a cross-lingual setting in §7.2.
Our System: Bitext Projection of Opinion De-
pendencies. Finally, we implemented our cross-
lingual BITEXT approach (§6). We trained the
(lexicalized) English model on the MPQA corpus
(the performance of this model is shown in Ta-
ble 4). Then, we ran this model on the English
side of the parallel corpus, generating automatic
annotations, and projected these annotations to the
Portuguese side, as described in §6.2. Finally, a
Portuguese model was trained on these projected
annotations using the arc-factored model and fea-
tures described in §4.
</bodyText>
<subsectionHeader confidence="0.989934">
7.2 Comparison
</subsectionHeader>
<bodyText confidence="0.989687655172414">
Table 5 shows the F1 scores obtained by the three
systems on the Portuguese test partition. We ob-
serve that the BITEXT approach outperformed the
SUPERVISED and the DELEXICALIZED ones in all
metrics with a considerable margin, which shows
the effectiveness of our proposed method. The
SUPERVISED system suffers from the fact that the
training set is too small to allow good general-
ization; the bitext projection method, in contrast,
can create arbitrarily large training corpora with-
out any annotation effort. The performance of
14Note that this split is different from the one we used in
§5. There we used the same split as Johansson and Moschitti
(2013), for a fair comparison with their system; here, we fol-
low the standard MPQA test partition.
the DELEXICALIZED system is rather disappoint-
ing. This result is justified by a decrease of per-
formance in English due to the delexicalization
(cf. Table 4), followed by an extra loss of quality
due to language differences.
Though our BITEXT approach scores the best,
the scores are behind the range of values ob-
tained for English (Table 4), and far from the inter-
annotator agreement numbers (Table 3), suggest-
ing room for improvement. The polarity scores in
Table 5 appear to be relatively low. This fact is
probably be justified with the annotator agreement
scores (Table 3) which are considerably lower for
these metrics.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999789">
We presented a cross-lingual framework for fine-
grained opinion mining. We used a bitext pro-
jection technique to transfer dependency-based
opinion frames from English to Portuguese. Ex-
perimentally, our dependency model achieved
state-of-the-art results for English, and the Por-
tuguese system trained with bitext projection out-
performed two baselines: a supervised system
trained on a small dataset, and a delexicalized
model with bilingual word embeddings.
</bodyText>
<sectionHeader confidence="0.99578" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999855">
We would like to thank the anonymous review-
ers for their insightful comments, and Richard
Johansson for sharing his code and for answer-
ing several questions. This work was par-
tially supported by the EU/FEDER programme,
QREN/POR Lisboa (Portugal), under the Intel-
ligo project (contract 2012/24803) and by a FCT
grants UID/EEA/50008/2013 and PTDC/EEI-
SII/2312/2012.
</bodyText>
<page confidence="0.998811">
416
</page>
<sectionHeader confidence="0.996369" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844831775701">
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.
2005. Emotions from text: machine learning for text-
based emotion prediction. In EMNLP.
Miguel B. Almeida, Mariana S. C. Almeida, Andr´e F. T. Mar-
tins, Helena Figueira, Pedro Mendes, and Cl´audia Pinto.
2014. Priberam compressive summarization corpus: A
new multi-document summarization corpus for european
portuguese. In LREC.
Wilker Aziz and Lucia Specia. 2011. Fully automatic com-
pilation of a Portuguese-English parallel corpus for statis-
tical machine translation. In STIL.
Krisztian Balog, Gilad Mishne, and Maarten de Rijke. 2006.
Why are they excited?: Identifying and explaining spikes
in blog mood levels. In EACL.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual subjectivity analysis using
machine translation. In EMNLP.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying
expressions of opinion in context. In IJCAI.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In CoNLL.
Yejin Choi and Claire Cardie. 2010. Hierarchical sequential
learning for extracting opinions and their attributes. In
ACL.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Pat-
wardhan. 2005. Identifying sources of opinions with con-
ditional random fields and extraction patterns. In EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint ex-
traction of entities and relations for opinion recognition.
In EMNLP.
Simon Clematide, Stefan Gindl, Manfred Klenner, Ste-
fanos Petrakis, Robert Remus, Josef Ruppenhofer, Ulli
Waltinger, and Michael Wiegand. 2012. MLSA A Multi-
layered Reference Corpus for German Sentiment Analy-
sis. In LREC.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning Re-
search.
Dipankar Das and Sivaji Bandyopadhyay. 2010. Labeling
emotion in bengali blog corpus a fine grained tagging at
sentence level. In (ALR8), COLING.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic
lexicon-based approach to opinion mining. In WSDM.
Greg Durrett and Dan Klein. 2013. Easy victories and uphill
battles in coreference resolution. In EMNLP.
Lin Gui, Ruifeng Xu, Jun Xu, and Chenxiang Liu. 2013.
A cross-lingual approach for opinion holder extraction.
Journal of Computational Information Systems, 9(6).
Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual
Models for Compositional Distributional Semantics. In
ACL.
Minqing Hu and Bing Liu. 2004. Mining opinion features in
customer reviews. In AAAI.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas,
and Okan Kolak. 2005. Bootstrapping parsers via syn-
tactic projection across parallel texts. Natural Language
Engineering, 11(3).
Ozan ˙Irsoy and Claire Cardie. 2014. Opinion mining with
deep recurrent neural networks. In EMNLP.
Richard Johansson and Alessandro Moschitti. 2010. Rerank-
ing models in fine-grained opinion analysis. In COLING.
Richard Johansson and Alessandro Moschitti. 2011. Extract-
ing opinion expressions and their polarities: exploration of
pipelines and joint models. In ACL.
Richard Johansson and Alessandro Moschitti. 2013. Rela-
tional features in fine-grained opinion analysis. Computa-
tional Linguistics, 39(3).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news me-
dia text. In SST.
P. Koehn. 2004. Statistical signicance tests for machine
translation evaluation. In ACL.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006.
Opinion extraction, summarization and tracking in news
and blog corpora. In AAAI.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment
by agreement. In NAACL.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Technolo-
gies, 5(1).
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou.
2011. Joint bilingual sentiment classification with unla-
beled parallel corpora. In ACL.
Andr´e F. T. Martins and M. S. C. Almeida. 2014. Priberam:
A turbo semantic parser with second order features. In
SemEval.
Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In ACL.
Andr´e F. T. Martins. 2015. Transferring coreference re-
solvers with posterior regularization. In ACL.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-
source transfer of delexicalized dependency parsers. In
EMNLP.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In ACL.
Sebastian Pad´o and Mirella Lapata. 2009. Cross-lingual an-
notation projection for semantic roles. Journal of Artifi-
cial Intelligence Research, 36(1).
Bo Pang and Lillian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval, 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up?: Sentiment classification using machine
learning techniques. In EMNLP.
</reference>
<page confidence="0.922717">
417
</page>
<reference confidence="0.989981109589041">
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A
universal part-of-speech tagset. In LREC.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2011. Structural opinion mining for graph-based senti-
ment representation. In EMNLP.
Peter Prettenhofer and Benno Stein. 2010. Cross-language
text classification using structural correspondence learn-
ing. In ACL.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction
patterns for subjective expressions. In EMNLP.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi Chen,
Noriko Kando, and Chin-Yew Lin. 2007. Overview of
opinion analysis pilot task at NTCIR-6. In NTCIR-6.
Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko
Kando. 2010. Overview of opinion analysis pilot task at
NTCIR-8: A Step Toward Cross Lingual Opinion Analy-
sis. In NTCIR-8.
Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013a. Parsing with compositional vector
grammars. In ACL.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts. 2013b. Recursive deep models for semantic com-
positionality over a sentiment treebank. In EMNLP.
Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In ACL.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Lu´ıs
M`arquez, and Joakim Nivre. 2008. The CoNLL-2008
Shared Task on Joint Parsing of Syntactic and Semantic
Dependencies. In CoNLL.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDon-
ald, and Joakim Nivre. 2013. Token and type constraints
for cross-lingual part-of-speech tagging. Trans. of the As-
sociation for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word
representations: a simple and general method for semi-
supervised learning. In ACL.
Peter D. Turney. 2002. Thumbs up or thumbs down?: Se-
mantic orientation applied to unsupervised classification
of reviews. In ACL.
Xiaojun Wan. 2009. Co-training for cross-lingual sentiment
classification. In ACL.
Mengqiu Wang and Chris Manning. 2014. Cross-lingual
projected expectation regularization for weakly super-
vised learning. Trans. of the Association for Computa-
tional Linguistics, 2.
Bin Wei and Christopher Pal. 2010. Cross lingual adaptation:
an experiment on sentiment classifications. In ACL.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2-3).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In EMNLP.
Theresa Wilson. 2008. Fine-Grained Subjectivity Analysis.
Ph.D. thesis, University of Pittsburgh.
Bishan Yang and Claire Cardie. 2012. Extracting opinion
expressions with semi-markov conditional random fields.
In EMNLP.
Bishan Yang and Claire Cardie. 2013. Joint inference for
fine-grained opinion extraction. In ACL.
Bishan Yang and Claire Cardie. 2014. Joint modeling of
opinion expression extraction and attribute classification.
Trans. of the Association for Computational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing multilin-
gual pos taggers and np bracketers via robust projection
across aligned corpora. In NAACL.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from opin-
ions and identifying the polarity of opinion sentences. In
EMNLP.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJCNLP.
</reference>
<page confidence="0.993026">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514860">
<title confidence="0.998389">Aligning Opinions: Cross-Lingual Opinion Mining with Dependencies</title>
<author confidence="0.7984475">S C F T</author>
<affiliation confidence="0.773254">Labs, Alameda D. Afonso Henriques, 41, 2°, 1000-123 Lisboa, de Instituto Superior T´ecnico, 1049-001 Lisboa,</affiliation>
<abstract confidence="0.998761285714286">We propose a cross-lingual framework for fine-grained opinion mining using bitext projection. The only requirements are a running system in a source language and word-aligned parallel data. Our method projects opinion frames from the source to the target language, and then trains a system on the target language using the automatic annotations. Key to our approach is a novel dependency-based model for opinion mining, which we show, as a byproduct, to be on par with the current state of the art for English, while avoiding the need for integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecilia Ovesdotter Alm</author>
<author>Dan Roth</author>
<author>Richard Sproat</author>
</authors>
<title>Emotions from text: machine learning for textbased emotion prediction.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1665" citStr="Alm et al., 2005" startWordPosition="258" endWordPosition="261">abeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long st</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005. Emotions from text: machine learning for textbased emotion prediction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel B Almeida</author>
<author>Mariana S C Almeida</author>
<author>Andr´e F T Martins</author>
<author>Helena Figueira</author>
<author>Pedro Mendes</author>
<author>Cl´audia Pinto</author>
</authors>
<title>Priberam compressive summarization corpus: A new multi-document summarization corpus for european portuguese.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="28258" citStr="Almeida et al., 2014" startWordPosition="4604" endWordPosition="4607">ty of the resulting corpus, we excluded sentences whose alignments cover less than 70% of the words in the target side of the corpus, or sentences whose opinion elements were not fully projected through high confidence alignments. At this point, we obtain anbautomatically annotated corpus in Portuguese (Df ), with 106,064 sentences and 32,817 opinions. 6.3 Portuguese Opinion Mining Corpus For validation purposes, we also created a Portuguese corpus with manually annotated finegrained opinions. The corpus consists of a subset of the documents of the Priberam Compressive Summarization Corpus11 (Almeida et al., 2014), which contains 80 news topics with 10 documents each, collected from several Portuguese newspapers, TV and radio websites in the biennia 2010– 2011 and 2012–2013. In the scope of the current work, we selected and annotated one document of each of the 80 topics. The first biennium was selected as the test set and the second biennium was split into development and training sets (see Ta11http://labs.priberam.com/Resources/ PCSC 414 ble 2 for statistics). #doc. #sent. #opin. Train 20 441 240 Dev 20 225 197 Test 40 560 391 Table 2: Number of documents, sentences and opinions in the Portuguese Cor</context>
</contexts>
<marker>Almeida, Almeida, Martins, Figueira, Mendes, Pinto, 2014</marker>
<rawString>Miguel B. Almeida, Mariana S. C. Almeida, Andr´e F. T. Martins, Helena Figueira, Pedro Mendes, and Cl´audia Pinto. 2014. Priberam compressive summarization corpus: A new multi-document summarization corpus for european portuguese. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>Fully automatic compilation of a Portuguese-English parallel corpus for statistical machine translation.</title>
<date>2011</date>
<booktitle>In STIL.</booktitle>
<marker>Aziz, Specia, 2011</marker>
<rawString>Wilker Aziz and Lucia Specia. 2011. Fully automatic compilation of a Portuguese-English parallel corpus for statistical machine translation. In STIL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krisztian Balog</author>
<author>Gilad Mishne</author>
<author>Maarten de Rijke</author>
</authors>
<title>Why are they excited?: Identifying and explaining spikes in blog mood levels.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<marker>Balog, Mishne, de Rijke, 2006</marker>
<rawString>Krisztian Balog, Gilad Mishne, and Maarten de Rijke. 2006. Why are they excited?: Identifying and explaining spikes in blog mood levels. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7247" citStr="Banea et al. (2008)" startWordPosition="1126" endWordPosition="1129"> also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its </context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="5096" citStr="Breck et al. (2007)" startWordPosition="790" endWordPosition="793"> along with a translation of the MPQA Subjectivity lexicon of Wilson et al. (2005).3 Experimental evaluation (§7) shows that our cross-lingual approach surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors propos</context>
<context position="16936" citStr="Breck et al., 2007" startWordPosition="2736" endWordPosition="2739">ese corpus include all these preprocessing elements (§6.3), with the exception of the voice information (features depending on voice were only used for English). We also used the Subjectivity Lexicon6 of Wilson et al. (2005) that we translated to Portuguese 6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 411 (§6.3), and a set of negation words (e.g. not, never, nor) and quantity words (e.g. very, much, less) collected for both languages. Our arc-factored features are described below; they are inspired by prior work on dependency parsing (Martins et al., 2013) and fine-grained opinion mining (Breck et al., 2007; Johansson and Moschitti, 2013). Opinion features. We define a set of features that only look at the opinion word; special symbols are used if the opinion is connected to a root or null node. The features below are also conjoined with the arc label. • OPINION WORD. The word itself, the lemma, the POS, and the voice. Conjunction of the word with the POS, and of the lemma with the POS. • BIGRAMS. Bigrams of words and POS corresponding to the opinion word conjoined with its previous (and next) word. • LEXICON (BASIC). Conjunction of the strength and polarity of the opinion word in the Subjectivi</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="9610" citStr="Buchholz and Marsi, 2006" startWordPosition="1507" endWordPosition="1510">rior work (Choi et al., 2005; Kim and Hovy, 2006; Johansson and Moschitti, 2010), one source of difficulty when learning opinion miners on MPQA is with the boundaries of the entity spans. The fact that no criterion for choosing these boundaries is explicitly defined in the annotation guidelines (Wiebe et al., 2005) leads to a low inter-annotator agreement. To circumvent this problem and make the learning task easier, we depart from the classical span-based approaches toward dependency-based opinion mining. This decision is inspired by the success of dependency models for syntax and semantics (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These dependency relations can be further converted to opinion spans (as described in §3.3), or directly used as features in downstream applications. As we will see, a compact representation based on dependencies can achieve state-of-the-art results and has the advantage of being easily transferred to other languages through a parallel corpus. 3.2 Dependency Graph Figure 1 depicts a sentence-level dependency representation for fine-grained opinion mining. The overall structure is a graph whose nodes are head words (plus two special nodes, root and null), connected by </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5526" citStr="Choi and Cardie (2010)" startWordPosition="856" endWordPosition="859">g is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraint</context>
<context position="12228" citStr="Choi and Cardie, 2010" startWordPosition="1944" endWordPosition="1947">cit arcs for opinion elements whose agent or target is not mentioned inside the sentence—these are modeled as arcs pointing to the null node. Dependency opinion graph. We have the following requirements for a well-formed dependency opinion graph: 1. No self-arcs or arcs linking root to null. 2. An arc is labeled as OPINION if and only if it comes from the root node. 3. Arcs labeled as AGENT or TARGET must come from an opinion node (i.e., a node with an incoming OPINION arc). 4. Every opinion node has exactly one AGENT and one TARGET outgoing arcs (possibly implicit).5 Similarly to prior work (Choi and Cardie, 2010; Johansson and Moschitti, 2011; Johansson and Moschitti, 2013), we map the MPQA’s polarityinto three levels: positive, negative and neutral, where the latter includes spans without polarity annotation or annotated as “both”. As in Johansson and Moschitti (2013), we also ignore the “uncertain” aspect of the annotated polarities. 3.3 Dependency-to-Span Conversion To evaluate the opinion miner against manual annotations and compare with other systems, we need a procedure to convert back from predicted dependencies to spans. In this work, we used a very simple procedure that we next describe, 5Ev</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5072" citStr="Choi et al. (2005)" startWordPosition="785" endWordPosition="788">nnotations was created, along with a translation of the MPQA Subjectivity lexicon of Wilson et al. (2005).3 Experimental evaluation (§7) shows that our cross-lingual approach surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work,</context>
<context position="9014" citStr="Choi et al., 2005" startWordPosition="1411" endWordPosition="1414"> towards the target. As an example, consider the sentence in Figure 1, which has two opinions, expressed by the 4NTCIR-8 had a cross-lingual track but in a very different sense: there, queries and documents are in different languages; in contrast, we transfer a model accross languages. 409 spans “is believed” (O1) and “are against” (O2). The first opinion has an implicit agent and a neutral polarity toward the target “the rich elites” (T1). This target is also the agent (A2) of the second opinion, which has a negative polarity toward “Hugo Ch´avez” (T2). 3.1 Motivation As noted in prior work (Choi et al., 2005; Kim and Hovy, 2006; Johansson and Moschitti, 2010), one source of difficulty when learning opinion miners on MPQA is with the boundaries of the entity spans. The fact that no criterion for choosing these boundaries is explicitly defined in the annotation guidelines (Wiebe et al., 2005) leads to a low inter-annotator agreement. To circumvent this problem and make the learning task easier, we depart from the classical span-based approaches toward dependency-based opinion mining. This decision is inspired by the success of dependency models for syntax and semantics (Buchholz and Marsi, 2006; Su</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5244" citStr="Choi et al. (2006)" startWordPosition="812" endWordPosition="815">ch surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014</context>
<context position="14279" citStr="Choi et al., 2006" startWordPosition="2274" endWordPosition="2277"> adjectives, modal verbs and the word to, when expanding to the left. The application of this simple approach to the gold dependency graphs in the training partition of the MPQA leads to oracle F1 scores of 86.0%, 95.8% and 93.0% in the reconstruction of opinion, agent and target spans, respectively, according to the proportional scores described in §5.2. 4 Arc-Factored Model One of the advantages of the dependency representation is that we can easily decode opinion-agenttarget relations without the need of complicated constrained sequence models or integer programming, as done in prior work (Choi et al., 2006; Yang and Cardie, 2012; Yang and Cardie, 2013). 4.1 Decoding We model dependency-based opinion mining as a structured classification problem. Let x be a sentence and y E Y(x) a set of well-formed dependency graphs, according to the constraints stated in §3. We define a score function that decomposes as a sum of labeled arc scores, f(x, y) = � fa(x, ya) (1) a∈y where ya is a labeled arc and the sum is over the arcs of the graph y. We use a linear model with weight vector w and local features Oa(x, ya): fa(x, ya) = w - Oa(x, ya). (2) For making predictions, we need to compute y = arg max)f (x, </context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Clematide</author>
<author>Stefan Gindl</author>
<author>Manfred Klenner</author>
<author>Stefanos Petrakis</author>
<author>Robert Remus</author>
<author>Josef Ruppenhofer</author>
<author>Ulli Waltinger</author>
<author>Michael Wiegand</author>
</authors>
<title>MLSA A Multilayered Reference Corpus for German Sentiment Analysis.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="3802" citStr="Clematide et al., 2012" startWordPosition="587" endWordPosition="590">hen to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored model that permits easy decoding (§4) and we show that, despite 1http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus (§6). As part of this work, a</context>
</contexts>
<marker>Clematide, Gindl, Klenner, Petrakis, Remus, Ruppenhofer, Waltinger, Wiegand, 2012</marker>
<rawString>Simon Clematide, Stefan Gindl, Manfred Klenner, Stefanos Petrakis, Robert Remus, Josef Ruppenhofer, Ulli Waltinger, and Michael Wiegand. 2012. MLSA A Multilayered Reference Corpus for German Sentiment Analysis. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online PassiveAggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="19676" citStr="Crammer et al., 2006" startWordPosition="3201" endWordPosition="3204">, for the agent “elites” headed by “are” with relation nsuj, we have: “VBP↓NNS” and “nsuj↓”). For arcs that neither connect to null or root, we conjoin voice features with the label, distance, and the direction of the arc. For these arcs, we also include back-off features where the polarity information is removed from the (target) labels. 5 English Monolingual Experiments In a first set of experiments, we evaluated the performance of our dependency-based model for opinion mining (§3) in the MPQA English corpus. 5.1 Learning We trained arc-factored models by running 25 epochs of max-loss MIRA (Crammer et al., 2006). Our cost function takes into account mismatches between predicted and gold dependencies, with a cost CP on labeled arcs incorrectly predicted (false positives) and a cost CR = 1 − CP on missed gold labeled arcs (false negatives). The cost CP, the regularization constant, and the number of epochs were tuned in the development set. 5.2 Evaluation Metrics Opinion spans (Op.) are evaluated with F1 scores, according to two matching criteria commonly used in the literature: overlap matching (OM), where a predicted span is counted as correct if it overlaps a gold one, and proportional matching (PM)</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online PassiveAggressive Algorithms. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipankar Das</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Labeling emotion in bengali blog corpus a fine grained tagging at sentence level.</title>
<date>2010</date>
<booktitle>In (ALR8), COLING.</booktitle>
<contexts>
<context position="3844" citStr="Das and Bandyopadhyay, 2010" startWordPosition="593" endWordPosition="597">r the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored model that permits easy decoding (§4) and we show that, despite 1http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus (§6). As part of this work, a validation corpus in Portuguese with subj</context>
</contexts>
<marker>Das, Bandyopadhyay, 2010</marker>
<rawString>Dipankar Das and Sivaji Bandyopadhyay. 2010. Labeling emotion in bengali blog corpus a fine grained tagging at sentence level. In (ALR8), COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In WSDM.</booktitle>
<contexts>
<context position="2056" citStr="Ding et al., 2008" startWordPosition="318" endWordPosition="321">ons in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long string of work has been produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic lexicon-based approach to opinion mining. In WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="11011" citStr="Durrett and Klein, 2013" startWordPosition="1735" endWordPosition="1738">ich corresponds to a single word (underlined in Figure 1). When converting the MPQA corpus to dependencies, we determine this “representative” word automatically, by using the following simple heuristic: we first parse the sentence using the Stanford dependency parser (Socher et al., 2013a); then, we pick the last word in the span whose syntactic parent is outside the span (if the span is a syntactic phrase, there is only one word whose parent is outside the span, which is the lexical head). The same heuristic has been used for identifying the heads of mention spans in coreference resolution (Durrett and Klein, 2013). Defining labeled arcs. The opinion relations are represented as labeled arcs that link these head nodes. Two artificial nodes are added: a root node, which links to all nodes that represent opinion words, with the label OPINION; and a null node, which is used for representing implicit relations. To represent opinion-agent relations, we draw an arc labeled AGENT toward the agent word. For opinion-target relations, the arc is toward the target word and has one of the labels TARGET:0, TARGET:+, or TARGET:-; this encodes the polarity in addition to the type of relation. We also include implicit </context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Gui</author>
<author>Ruifeng Xu</author>
<author>Jun Xu</author>
<author>Chenxiang Liu</author>
</authors>
<title>A cross-lingual approach for opinion holder extraction.</title>
<date>2013</date>
<journal>Journal of Computational Information Systems,</journal>
<volume>9</volume>
<issue>6</issue>
<contexts>
<context position="7609" citStr="Gui et al. (2013)" startWordPosition="1184" endWordPosition="1187"> opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. 3 Dependency-Based Opinion Mining This work addresses various elements of subjectivity annotated in the MPQA corpus, namely: • direct-subjective expressions (henceforth, opinions) that are direct mentions of a private state, e.g. opinions, beliefs, emotions, sentiments, speculations, goals, etc</context>
</contexts>
<marker>Gui, Xu, Xu, Liu, 2013</marker>
<rawString>Lin Gui, Ruifeng Xu, Jun Xu, and Chenxiang Liu. 2013. A cross-lingual approach for opinion holder extraction. Journal of Computational Information Systems, 9(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual Models for Compositional Distributional Semantics.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32116" citStr="Hermann and Blunsom (2014)" startWordPosition="5219" endWordPosition="5222">his strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the procedure described in Turian et al. (2010). We trained the English delexicalized system on the MPQA corpus, using the same test documents 13A delexicalized system trained without the word embeddings had a worse performance. 415 BASELINE #1 (SUP.) BASELINE #2 (DELEX. ) BITEXT PROJECTION HM PM OM HM PM OM HM PM OM Op. 49.4 48.7 50.8 33.1 32.1 34.3 58.0* 55.7* 58.0* Op-Ag. 23.5 27.2 31.5 14.3 18.8 20.0 30.8* 31.2* 36.2* Op-Tg. 23.0 24.9 30.6 11.0 15.7 19.0 29.4* 29.4* 35.6* Op-Pol. </context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual Models for Compositional Distributional Semantics. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1496" citStr="Hu and Liu, 2004" startWordPosition="231" endWordPosition="234">ding the need for integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="2973" citStr="Hwa et al., 2005" startWordPosition="462" endWordPosition="465">inion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored model that permits eas</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan ˙Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Opinion mining with deep recurrent neural networks.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<marker>˙Irsoy, Cardie, 2014</marker>
<rawString>Ozan ˙Irsoy and Claire Cardie. 2014. Opinion mining with deep recurrent neural networks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reranking models in fine-grained opinion analysis.</title>
<date>2010</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="5306" citStr="Johansson and Moschitti (2010)" startWordPosition="820" endWordPosition="823">l corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion span</context>
<context position="9066" citStr="Johansson and Moschitti, 2010" startWordPosition="1419" endWordPosition="1422">sider the sentence in Figure 1, which has two opinions, expressed by the 4NTCIR-8 had a cross-lingual track but in a very different sense: there, queries and documents are in different languages; in contrast, we transfer a model accross languages. 409 spans “is believed” (O1) and “are against” (O2). The first opinion has an implicit agent and a neutral polarity toward the target “the rich elites” (T1). This target is also the agent (A2) of the second opinion, which has a negative polarity toward “Hugo Ch´avez” (T2). 3.1 Motivation As noted in prior work (Choi et al., 2005; Kim and Hovy, 2006; Johansson and Moschitti, 2010), one source of difficulty when learning opinion miners on MPQA is with the boundaries of the entity spans. The fact that no criterion for choosing these boundaries is explicitly defined in the annotation guidelines (Wiebe et al., 2005) leads to a low inter-annotator agreement. To circumvent this problem and make the learning task easier, we depart from the classical span-based approaches toward dependency-based opinion mining. This decision is inspired by the success of dependency models for syntax and semantics (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These dependency relations can</context>
<context position="20320" citStr="Johansson and Moschitti (2010)" startWordPosition="3306" endWordPosition="3309">ction takes into account mismatches between predicted and gold dependencies, with a cost CP on labeled arcs incorrectly predicted (false positives) and a cost CR = 1 − CP on missed gold labeled arcs (false negatives). The cost CP, the regularization constant, and the number of epochs were tuned in the development set. 5.2 Evaluation Metrics Opinion spans (Op.) are evaluated with F1 scores, according to two matching criteria commonly used in the literature: overlap matching (OM), where a predicted span is counted as correct if it overlaps a gold one, and proportional matching (PM), proposed by Johansson and Moschitti (2010). For the latter, we use the following formula for the recall, where we consider the sets of gold (G) and predicted (P) opinion spans:7 |g np|/|p|. (4) |P |&apos; 7This metric is slightly different from the PM metric of Johansson and Moschitti (2010), in which recall was computed as R(G, P) = �p∈P �g∈G |g∩ |p|/|p |The reason why we replace the “sum” by a “max” is that each predicted span P in (4) could contribute to the recall with a value greater than 1. Since most of the predicted spans only overlap a single gold span, this fix has a very small effect in the final scores. � R(G, P) = pEP max 9EG </context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010. Reranking models in fine-grained opinion analysis. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Extracting opinion expressions and their polarities: exploration of pipelines and joint models.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5453" citStr="Johansson and Moschitti, 2011" startWordPosition="841" endWordPosition="845">xicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, sin</context>
<context position="12259" citStr="Johansson and Moschitti, 2011" startWordPosition="1948" endWordPosition="1951">ements whose agent or target is not mentioned inside the sentence—these are modeled as arcs pointing to the null node. Dependency opinion graph. We have the following requirements for a well-formed dependency opinion graph: 1. No self-arcs or arcs linking root to null. 2. An arc is labeled as OPINION if and only if it comes from the root node. 3. Arcs labeled as AGENT or TARGET must come from an opinion node (i.e., a node with an incoming OPINION arc). 4. Every opinion node has exactly one AGENT and one TARGET outgoing arcs (possibly implicit).5 Similarly to prior work (Choi and Cardie, 2010; Johansson and Moschitti, 2011; Johansson and Moschitti, 2013), we map the MPQA’s polarityinto three levels: positive, negative and neutral, where the latter includes spans without polarity annotation or annotated as “both”. As in Johansson and Moschitti (2013), we also ignore the “uncertain” aspect of the annotated polarities. 3.3 Dependency-to-Span Conversion To evaluate the opinion miner against manual annotations and compare with other systems, we need a procedure to convert back from predicted dependencies to spans. In this work, we used a very simple procedure that we next describe, 5Even though this assumption is no</context>
</contexts>
<marker>Johansson, Moschitti, 2011</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2011. Extracting opinion expressions and their polarities: exploration of pipelines and joint models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational features in fine-grained opinion analysis.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="12291" citStr="Johansson and Moschitti, 2013" startWordPosition="1952" endWordPosition="1955"> not mentioned inside the sentence—these are modeled as arcs pointing to the null node. Dependency opinion graph. We have the following requirements for a well-formed dependency opinion graph: 1. No self-arcs or arcs linking root to null. 2. An arc is labeled as OPINION if and only if it comes from the root node. 3. Arcs labeled as AGENT or TARGET must come from an opinion node (i.e., a node with an incoming OPINION arc). 4. Every opinion node has exactly one AGENT and one TARGET outgoing arcs (possibly implicit).5 Similarly to prior work (Choi and Cardie, 2010; Johansson and Moschitti, 2011; Johansson and Moschitti, 2013), we map the MPQA’s polarityinto three levels: positive, negative and neutral, where the latter includes spans without polarity annotation or annotated as “both”. As in Johansson and Moschitti (2013), we also ignore the “uncertain” aspect of the annotated polarities. 3.3 Dependency-to-Span Conversion To evaluate the opinion miner against manual annotations and compare with other systems, we need a procedure to convert back from predicted dependencies to spans. In this work, we used a very simple procedure that we next describe, 5Even though this assumption is not always met in practice, it is </context>
<context position="16968" citStr="Johansson and Moschitti, 2013" startWordPosition="2740" endWordPosition="2743">ll these preprocessing elements (§6.3), with the exception of the voice information (features depending on voice were only used for English). We also used the Subjectivity Lexicon6 of Wilson et al. (2005) that we translated to Portuguese 6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 411 (§6.3), and a set of negation words (e.g. not, never, nor) and quantity words (e.g. very, much, less) collected for both languages. Our arc-factored features are described below; they are inspired by prior work on dependency parsing (Martins et al., 2013) and fine-grained opinion mining (Breck et al., 2007; Johansson and Moschitti, 2013). Opinion features. We define a set of features that only look at the opinion word; special symbols are used if the opinion is connected to a root or null node. The features below are also conjoined with the arc label. • OPINION WORD. The word itself, the lemma, the POS, and the voice. Conjunction of the word with the POS, and of the lemma with the POS. • BIGRAMS. Bigrams of words and POS corresponding to the opinion word conjoined with its previous (and next) word. • LEXICON (BASIC). Conjunction of the strength and polarity of the opinion word in the Subjectivity Lexicon6 (e.g., “weaksubj+neg</context>
<context position="21941" citStr="Johansson and Moschitti (2013)" startWordPosition="3593" endWordPosition="3596">the relation holds in the gold data. We also compute the metric described in Johansson and Moschitti (2010) which measures how well agents of opinions are predicted based on a proportional matching (PM) criterion. This metric is applied to evaluate the extraction of both agents and targets. Finally, to evaluate the opinions’ polarities (Op-Pol. metric) we consider as correct opinions where the span and polarity both match the gold ones. 5.3 Results: Dependency-Based Model We assess the quality of our monolingual dependency-based model by comparing it to the recent state-of-the-art approach of Johansson and Moschitti (2013), whose code is available online.8 That paper reports the performance of a basic span-based pipeline system (which extracts opinions with a CRF, followed by two separate classifiers to detect polarities and agents), and of a more sophisticated system that applies a reranking procedure to account for more complex features that consider interactions accross opinion elements. We ran experiments using the same data and MPQA partitions as Johansson and Moschitti (2013). However, since our system is designed for predicting opinion, agents and targets together, we removed the documents that were not </context>
<context position="23346" citStr="Johansson and Moschitti (2013)" startWordPosition="3802" endWordPosition="3805">ts; since the systems of Johansson and Moschitti (2013) do not predict targets, Table 1 omits target scores.9 We observe that our dependency-based system achieves results competitive with the best results of Johansson and Moschitti (2013) and clearly above the ones reached by their basic system that does not use re-ranking features. Though the two systems are not fully comparable,10 the results in Table 1 8http://demo.spraakdata.gu.se/richard/ unitn_opinion/details.html 9We will report target scores later in §7. 10Our system makes use of target annotations to predict the opinion frames, while Johansson and Moschitti (2013) show that our dependency-based approach (§3.2) followed by a simple dependency-to-span conversion (§3.3) is, despite its simplicity, on par with a top-performing opinion mining system. We conjecture that this is due to the ability to extract opinions, agents, and targets jointly using exact decoding. Note that our proposed dependency scheme would also be able to include additional global features relating pairs of opinions (by adding scores to pairs of opinion arcs) or two opinions having the same agent (by adding scores to pairs of agent arcs sharing its argument), similar to the reranking f</context>
<context position="26028" citStr="Johansson and Moschitti (2013)" startWordPosition="4254" endWordPosition="4257">FAPESP, collected by Aziz and has access not only to direct subjective spans but also to subjective expressions annotations with their agents and polarity information. some filtering), yielding an automatic corpus 413 JM13, BASIC JM13, RERANKING HM OUR SYSTEM HM PM OM HM PM OM PM OM Op. 56.3 56.2 60.6 58.6 59.2 63.7 61.6* 59.8 65.1 Op-Ag. 40.3 47.1 44.9 42.4 51.4 48.1 45.7* 51.4 50.3* Op-Tg. - - - - - - 31.3* 48.3* 48.3* Op-Pol. 46.1 45.9 49.3 48.5 48.9* 52.5 47.9 47.0 50.7 Table 1: Method comparison: F1 scores obtained in the MPQA corpus, for our dependency based method and the approaches in Johansson and Moschitti (2013), with and without reranking. The symbol * indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004). Figure 2: Excerpt of a bitext document from FAPESP, with automatic opinion dependencies. The annotations are directly projected to Portuguese via automatic word alignments. Algorithm 1 Cross-Lingual Opinion Mining Input: Labeled data Le, parallel data De and Df. Output: Target opinion mining system Sf. 1: Se ← LEARNOPINIONMINER(Le) 2: bDe ← RUNOPINIONMINER(Se, De) 3: Dbe↔f ← RUNWORDALIGNER(De, </context>
<context position="34878" citStr="Johansson and Moschitti (2013)" startWordPosition="5669" endWordPosition="5672">res obtained by the three systems on the Portuguese test partition. We observe that the BITEXT approach outperformed the SUPERVISED and the DELEXICALIZED ones in all metrics with a considerable margin, which shows the effectiveness of our proposed method. The SUPERVISED system suffers from the fact that the training set is too small to allow good generalization; the bitext projection method, in contrast, can create arbitrarily large training corpora without any annotation effort. The performance of 14Note that this split is different from the one we used in §5. There we used the same split as Johansson and Moschitti (2013), for a fair comparison with their system; here, we follow the standard MPQA test partition. the DELEXICALIZED system is rather disappointing. This result is justified by a decrease of performance in English due to the delexicalization (cf. Table 4), followed by an extra loss of quality due to language differences. Though our BITEXT approach scores the best, the scores are behind the range of values obtained for English (Table 4), and far from the interannotator agreement numbers (Table 3), suggesting room for improvement. The polarity scores in Table 5 appear to be relatively low. This fact i</context>
</contexts>
<marker>Johansson, Moschitti, 2013</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Computational Linguistics, 39(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In SST.</booktitle>
<contexts>
<context position="4954" citStr="Kim and Hovy (2006)" startWordPosition="762" endWordPosition="765">rain a system on the resulting corpus (§6). As part of this work, a validation corpus in Portuguese with subjectivity annotations was created, along with a translation of the MPQA Subjectivity lexicon of Wilson et al. (2005).3 Experimental evaluation (§7) shows that our cross-lingual approach surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014).</context>
<context position="9034" citStr="Kim and Hovy, 2006" startWordPosition="1415" endWordPosition="1418">. As an example, consider the sentence in Figure 1, which has two opinions, expressed by the 4NTCIR-8 had a cross-lingual track but in a very different sense: there, queries and documents are in different languages; in contrast, we transfer a model accross languages. 409 spans “is believed” (O1) and “are against” (O2). The first opinion has an implicit agent and a neutral polarity toward the target “the rich elites” (T1). This target is also the agent (A2) of the second opinion, which has a negative polarity toward “Hugo Ch´avez” (T2). 3.1 Motivation As noted in prior work (Choi et al., 2005; Kim and Hovy, 2006; Johansson and Moschitti, 2010), one source of difficulty when learning opinion miners on MPQA is with the boundaries of the entity spans. The fact that no criterion for choosing these boundaries is explicitly defined in the annotation guidelines (Wiebe et al., 2005) leads to a low inter-annotator agreement. To circumvent this problem and make the learning task easier, we depart from the classical span-based approaches toward dependency-based opinion mining. This decision is inspired by the success of dependency models for syntax and semantics (Buchholz and Marsi, 2006; Surdeanu et al., 2008)</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In SST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical signicance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26229" citStr="Koehn, 2004" startWordPosition="4288" endWordPosition="4289">M13, BASIC JM13, RERANKING HM OUR SYSTEM HM PM OM HM PM OM PM OM Op. 56.3 56.2 60.6 58.6 59.2 63.7 61.6* 59.8 65.1 Op-Ag. 40.3 47.1 44.9 42.4 51.4 48.1 45.7* 51.4 50.3* Op-Tg. - - - - - - 31.3* 48.3* 48.3* Op-Pol. 46.1 45.9 49.3 48.5 48.9* 52.5 47.9 47.0 50.7 Table 1: Method comparison: F1 scores obtained in the MPQA corpus, for our dependency based method and the approaches in Johansson and Moschitti (2013), with and without reranking. The symbol * indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004). Figure 2: Excerpt of a bitext document from FAPESP, with automatic opinion dependencies. The annotations are directly projected to Portuguese via automatic word alignments. Algorithm 1 Cross-Lingual Opinion Mining Input: Labeled data Le, parallel data De and Df. Output: Target opinion mining system Sf. 1: Se ← LEARNOPINIONMINER(Le) 2: bDe ← RUNOPINIONMINER(Se, De) 3: Dbe↔f ← RUNWORDALIGNER(De, Dbf ) 4: Df ← PROJECTANDFILTER(De↔f , De) 5: Sf ← LEARNOPINIONMINER(bDf) Specia (2011). Though this corpus is in Brazilian Portuguese (while our validation corpus is in European Portuguese), we preferr</context>
<context position="33229" citStr="Koehn, 2004" startWordPosition="5404" endWordPosition="5405">.5 14.3 18.8 20.0 30.8* 31.2* 36.2* Op-Tg. 23.0 24.9 30.6 11.0 15.7 19.0 29.4* 29.4* 35.6* Op-Pol. 24.1 23.8 24.7 16.6 16.4 17.6 35.7* 34.1* 35.7* Table 5: Comparison of cross-lingual approaches. F1 scores obtained in our Portuguese validation corpus using: a SUPERVISED system trained on the small available data, a DELEXICALIZED system trained with universal POS tags and multilingual embeddings and our BITEXT PROJECTION OF DEPENDENCIES. The symbol * indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004). as Riloff and Wiebe (2003) and whose list is available with the corpus, but selecting only documents annotated with targets. We randomly split the remaining documents into train and development sets, respectively with a total of 6,471 and 782 sentences.14 Table 4 shows the performance of the delexicalized baseline in English, compared with a lexicalized system. We will see how this model behaves in a cross-lingual setting in §7.2. Our System: Bitext Projection of Opinion Dependencies. Finally, we implemented our crosslingual BITEXT approach (§6). We trained the (lexicalized) English model on</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical signicance tests for machine translation evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Yu-Ting Liang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Opinion extraction, summarization and tracking in news and blog corpora.</title>
<date>2006</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1564" citStr="Ku et al., 2006" startWordPosition="244" endWordPosition="247"> mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al.</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion extraction, summarization and tracking in news and blog corpora. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="27090" citStr="Liang et al., 2006" startWordPosition="4420" endWordPosition="4423">rallel data De and Df. Output: Target opinion mining system Sf. 1: Se ← LEARNOPINIONMINER(Le) 2: bDe ← RUNOPINIONMINER(Se, De) 3: Dbe↔f ← RUNWORDALIGNER(De, Dbf ) 4: Df ← PROJECTANDFILTER(De↔f , De) 5: Sf ← LEARNOPINIONMINER(bDf) Specia (2011). Though this corpus is in Brazilian Portuguese (while our validation corpus is in European Portuguese), we preferred FAPESP over other commonly used parallel corpora (such as the Europarl and UN datasets), since it is closer to our newswire target domain, with a smaller prominence of direct speech. We computed word alignments using the Berkeley aligner (Liang et al., 2006), intersected them and filtered out all the alignments whose confidence is below 0.95. After annotating the English side of FAPESP with the pre-trained system ( bDe in Algorithm 1, with a total of 166,719 sentences and 81,492 opinions), the high confidence alignments (De↔f) are used to project the annotations to the Portuguese side of the corpus. The automatic annotations produced by our dependency-based system are easily transferred at a word level (for words with high confidence alignments), as illustrated in Figure 2. To improve the quality of the resulting corpus, we excluded sentences who</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1286" citStr="Liu, 2012" startWordPosition="194" endWordPosition="195">g the automatic annotations. Key to our approach is a novel dependency-based model for opinion mining, which we show, as a byproduct, to be on par with the current state of the art for English, while avoiding the need for integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rat</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Chenhao Tan</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Joint bilingual sentiment classification with unlabeled parallel corpora.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7562" citStr="Lu et al. (2011)" startWordPosition="1176" endWordPosition="1179">pproximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. 3 Dependency-Based Opinion Mining This work addresses various elements of subjectivity annotated in the MPQA corpus, namely: • direct-subjective expressions (henceforth, opinions) that are direct mentions of a private state, e.g. opinions, beliefs,</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>M S C Almeida</author>
</authors>
<title>Priberam: A turbo semantic parser with second order features.</title>
<date>2014</date>
<booktitle>In SemEval.</booktitle>
<contexts>
<context position="24131" citStr="Martins and Almeida, 2014" startWordPosition="3928" endWordPosition="3931">ing opinion mining system. We conjecture that this is due to the ability to extract opinions, agents, and targets jointly using exact decoding. Note that our proposed dependency scheme would also be able to include additional global features relating pairs of opinions (by adding scores to pairs of opinion arcs) or two opinions having the same agent (by adding scores to pairs of agent arcs sharing its argument), similar to the reranking features used by Johansson and Moschitti (2013). Similar second-order scores have been used in syntactic and semantic dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), but with an increase in the complexity of the model and of the decoder. 6 Cross-Lingual Opinion Mining We now turn to the problem of learning a opinion mining system for a resource-poor language (Portuguese), in a cross-lingual manner. We use a bitext projection approach (§6.1), whose only requirements are a model for a resource-rich language (English) and parallel data (§6.2). 6.1 Bitext Projection Our methodology is outlined as Algorithm 1. For simplicity, we call the source and target languages English (e) and “foreign” (f), respectively. The procedure is inspired by the idea of bitext pr</context>
</contexts>
<marker>Martins, Almeida, 2014</marker>
<rawString>Andr´e F. T. Martins and M. S. C. Almeida. 2014. Priberam: A turbo semantic parser with second order features. In SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Miguel B Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order nonprojective turbo parsers.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="16884" citStr="Martins et al., 2013" startWordPosition="2728" endWordPosition="2731">rules for computing the voice of each word. Our Portuguese corpus include all these preprocessing elements (§6.3), with the exception of the voice information (features depending on voice were only used for English). We also used the Subjectivity Lexicon6 of Wilson et al. (2005) that we translated to Portuguese 6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 411 (§6.3), and a set of negation words (e.g. not, never, nor) and quantity words (e.g. very, much, less) collected for both languages. Our arc-factored features are described below; they are inspired by prior work on dependency parsing (Martins et al., 2013) and fine-grained opinion mining (Breck et al., 2007; Johansson and Moschitti, 2013). Opinion features. We define a set of features that only look at the opinion word; special symbols are used if the opinion is connected to a root or null node. The features below are also conjoined with the arc label. • OPINION WORD. The word itself, the lemma, the POS, and the voice. Conjunction of the word with the POS, and of the lemma with the POS. • BIGRAMS. Bigrams of words and POS corresponding to the opinion word conjoined with its previous (and next) word. • LEXICON (BASIC). Conjunction of the strengt</context>
<context position="24103" citStr="Martins et al., 2013" startWordPosition="3923" endWordPosition="3927">par with a top-performing opinion mining system. We conjecture that this is due to the ability to extract opinions, agents, and targets jointly using exact decoding. Note that our proposed dependency scheme would also be able to include additional global features relating pairs of opinions (by adding scores to pairs of opinion arcs) or two opinions having the same agent (by adding scores to pairs of agent arcs sharing its argument), similar to the reranking features used by Johansson and Moschitti (2013). Similar second-order scores have been used in syntactic and semantic dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), but with an increase in the complexity of the model and of the decoder. 6 Cross-Lingual Opinion Mining We now turn to the problem of learning a opinion mining system for a resource-poor language (Portuguese), in a cross-lingual manner. We use a bitext projection approach (§6.1), whose only requirements are a model for a resource-rich language (English) and parallel data (§6.2). 6.1 Bitext Projection Our methodology is outlined as Algorithm 1. For simplicity, we call the source and target languages English (e) and “foreign” (f), respectively. The procedure is inspi</context>
<context position="29915" citStr="Martins et al., 2013" startWordPosition="4884" endWordPosition="4887"> and target spans. The annotation was carried out by three linguists, after reading the MPQA annotation guidelines (Wiebe et al., 2005; Wilson, 2008) and having a small practice period using the provided examples and some MPQA annotated sentences. Each document was annotated by two of the three linguists and then revised by the third linguist, who (in case of any doubts) discussed with the initial annotators to reach for the final consensus. Scores for inter-annotator agreement are shown in Table 3. The corpus was annotated with automatic POS tags and dependency parse trees using TurboParser (Martins et al., 2013).12 We used an in-house lemmatizer to obtain lemmas for each inflected word in the corpus. A Portuguese lexicon of subjectivity was created by translating the words in the Subjectivity Lexicon of Wilson et al. (2005). The annotated corpus and the translated subjectivity lexicon are available at http://labs.priberam.com/ Resources/Fine-Grained-Opinion-Corpus, and http://labs.priberam.com/Resources/ Subjectivity-Lexicon-PT, respectively. 12http://www.ark.cs.cmu.edu/TurboParser OUR SYSTEM DELEXICALIZED HM PM OM HM PM OM Op. 65.7 63.5 69.8 50.1 45.8 52.7 Op-Ag. 47.6 48.8 51.1 33.8 34.8 35.7 Op-Tg.</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e F. T. Martins, Miguel B. Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order nonprojective turbo parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
</authors>
<title>Transferring coreference resolvers with posterior regularization.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3066" citStr="Martins, 2015" startWordPosition="478" endWordPosition="479">ned by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored model that permits easy decoding (§4) and we show that, despite 1http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2Bes</context>
</contexts>
<marker>Martins, 2015</marker>
<rawString>Andr´e F. T. Martins. 2015. Transferring coreference resolvers with posterior regularization. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multisource transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="31614" citStr="McDonald et al., 2011" startWordPosition="5142" endWordPosition="5145">ough being a small training corpus, this is, to the best of our knowledge, the only existing corpus with finegrained opinions in Portuguese. We used the same arc-factored model and features described in §4. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a DELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multisource transfer of delexicalized dependency parsers. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via crosslingual projections.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7223" citStr="Mihalcea et al. (2007)" startWordPosition="1121" endWordPosition="1124">de. A dependency scheme was also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-graine</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via crosslingual projections. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Cross-lingual annotation projection for semantic roles.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>36</volume>
<issue>1</issue>
<marker>Pad´o, Lapata, 2009</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2009. Cross-lingual annotation projection for semantic roles. Journal of Artificial Intelligence Research, 36(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1260" citStr="Pang and Lee, 2008" startWordPosition="188" endWordPosition="191">system on the target language using the automatic annotations. Key to our approach is a novel dependency-based model for opinion mining, which we show, as a byproduct, to be on par with the current state of the art for English, while avoiding the need for integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1770" citStr="Pang et al., 2002" startWordPosition="275" endWordPosition="278">Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long string of work has been produced (reviewed in §2). Despite the large volume of prior work, opinion mining h</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="31757" citStr="Petrov et al., 2012" startWordPosition="5164" endWordPosition="5167">used the same arc-factored model and features described in §4. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a DELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the procedure described in Turian et al. (2010). We trained the English delexicalized system on the MPQA corpus, using the same te</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Structural opinion mining for graph-based sentiment representation.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1514" citStr="Wu et al., 2011" startWordPosition="235" endWordPosition="238">integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). S</context>
<context position="6658" citStr="Wu et al. (2011)" startWordPosition="1032" endWordPosition="1035">icated, since they must forbid overlap of opinion elements or add further constraints, leading to integer programming or reranking strategies. Besides, there is little consensus about what should be the correct span boundaries, the inter-annotator agreement being quite low (Wiebe et al., 2005). In 3The Portuguese corpus and the lexicon are available at http://labs.priberam.com/Resources. constrast, we use dependencies to model opinion elements and relations, leading to a compact representation that does not depend on spans and which is tractable to decode. A dependency scheme was also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who trans</context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2011</marker>
<rawString>Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2011. Structural opinion mining for graph-based sentiment representation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Cross-language text classification using structural correspondence learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7496" citStr="Prettenhofer and Stein (2010)" startWordPosition="1162" endWordPosition="1165">“span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. 3 Dependency-Based Opinion Mining This work addresses various elements of subjectivity annotated in the MPQA corpus, namely: • direct-subjective expressions (henceforth, opinions) th</context>
</contexts>
<marker>Prettenhofer, Stein, 2010</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2010. Cross-language text classification using structural correspondence learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="33257" citStr="Riloff and Wiebe (2003)" startWordPosition="5407" endWordPosition="5410"> 30.8* 31.2* 36.2* Op-Tg. 23.0 24.9 30.6 11.0 15.7 19.0 29.4* 29.4* 35.6* Op-Pol. 24.1 23.8 24.7 16.6 16.4 17.6 35.7* 34.1* 35.7* Table 5: Comparison of cross-lingual approaches. F1 scores obtained in our Portuguese validation corpus using: a SUPERVISED system trained on the small available data, a DELEXICALIZED system trained with universal POS tags and multilingual embeddings and our BITEXT PROJECTION OF DEPENDENCIES. The symbol * indicates that the best system beats the other systems with statistical significance, with p &lt; 0.05 and according to a bootstrap resampling test (Koehn, 2004). as Riloff and Wiebe (2003) and whose list is available with the corpus, but selecting only documents annotated with targets. We randomly split the remaining documents into train and development sets, respectively with a total of 6,471 and 782 sentences.14 Table 4 shows the performance of the delexicalized baseline in English, compared with a lexicalized system. We will see how this model behaves in a cross-lingual setting in §7.2. Our System: Bitext Projection of Opinion Dependencies. Finally, we implemented our crosslingual BITEXT approach (§6). We trained the (lexicalized) English model on the MPQA corpus (the perfor</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen</author>
<author>Noriko Kando</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Overview of opinion analysis pilot task at NTCIR-6.</title>
<date>2007</date>
<booktitle>In NTCIR-6.</booktitle>
<contexts>
<context position="3769" citStr="Seki et al., 2007" startWordPosition="581" endWordPosition="584">tem on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored model that permits easy decoding (§4) and we show that, despite 1http://mpqa.cs.pitt.edu/corpora/mpqa_ corpus. 2Besides English, monolingual systems have also been developed for Chinese and Japanese (Seki et al., 2007), German (Clematide et al., 2012) and Bengali (Das and Bandyopadhyay, 2010). 408 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 408–418, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting cor</context>
<context position="7070" citStr="Seki et al., 2007" startWordPosition="1096" endWordPosition="1099">pendencies to model opinion elements and relations, leading to a compact representation that does not depend on spans and which is tractable to decode. A dependency scheme was also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. H</context>
</contexts>
<marker>Seki, Evans, Ku, Chen, Kando, Lin, 2007</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin. 2007. Overview of opinion analysis pilot task at NTCIR-6. In NTCIR-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of opinion analysis pilot task at NTCIR-8: A Step Toward Cross Lingual Opinion Analysis.</title>
<date>2010</date>
<booktitle>In NTCIR-8.</booktitle>
<marker>Seki, Ku, Le Sun, Kando, 2010</marker>
<rawString>Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando. 2010. Overview of opinion analysis pilot task at NTCIR-8: A Step Toward Cross Lingual Opinion Analysis. In NTCIR-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2109" citStr="Socher et al., 2013" startWordPosition="325" endWordPosition="328">, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long string of work has been produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion minin</context>
<context position="10676" citStr="Socher et al., 2013" startWordPosition="1674" endWordPosition="1677">ation for fine-grained opinion mining. The overall structure is a graph whose nodes are head words (plus two special nodes, root and null), connected by labeled arcs, as outlined below. Determining head nodes. The three opinion elements that we want to detect (opinions, agents and targets) are each represented by a head node, which corresponds to a single word (underlined in Figure 1). When converting the MPQA corpus to dependencies, we determine this “representative” word automatically, by using the following simple heuristic: we first parse the sentence using the Stanford dependency parser (Socher et al., 2013a); then, we pick the last word in the span whose syntactic parent is outside the span (if the span is a syntactic phrase, there is only one word whose parent is outside the span, which is the lexical head). The same heuristic has been used for identifying the heads of mention spans in coreference resolution (Durrett and Klein, 2013). Defining labeled arcs. The opinion relations are represented as labeled arcs that link these head nodes. Two artificial nodes are added: a root node, which links to all nodes that represent opinion words, with the label OPINION; and a null node, which is used for</context>
<context position="16174" citStr="Socher et al., 2013" startWordPosition="2618" endWordPosition="2621">dependency graph, respectively with labels OPINION, AGENT, and TARGET:p. For a sentence with L words, this decoding procedure takes O(L2) time. In practice, we speed up this process by pruning from the candidate list arcs whose connected POS were not observed in the training set and whose length were larger than the ones observed in the training set. 4.2 Features We now describe our features Oa, which are computed after processing the sentence to predict POS tags, syntactic dependency trees, lemmas and voice (active or passive) information. For English, we used the Stanford dependency parser (Socher et al., 2013a) for the syntactic annotations, the Porter stemmer to compute word stems, and a set of rules for computing the voice of each word. Our Portuguese corpus include all these preprocessing elements (§6.3), with the exception of the voice information (features depending on voice were only used for English). We also used the Subjectivity Lexicon6 of Wilson et al. (2005) that we translated to Portuguese 6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 411 (§6.3), and a set of negation words (e.g. not, never, nor) and quantity words (e.g. very, much, less) collected for both languages. Our arc-facto</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013a. Parsing with compositional vector grammars. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2109" citStr="Socher et al., 2013" startWordPosition="325" endWordPosition="328">, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long string of work has been produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion minin</context>
<context position="10676" citStr="Socher et al., 2013" startWordPosition="1674" endWordPosition="1677">ation for fine-grained opinion mining. The overall structure is a graph whose nodes are head words (plus two special nodes, root and null), connected by labeled arcs, as outlined below. Determining head nodes. The three opinion elements that we want to detect (opinions, agents and targets) are each represented by a head node, which corresponds to a single word (underlined in Figure 1). When converting the MPQA corpus to dependencies, we determine this “representative” word automatically, by using the following simple heuristic: we first parse the sentence using the Stanford dependency parser (Socher et al., 2013a); then, we pick the last word in the span whose syntactic parent is outside the span (if the span is a syntactic phrase, there is only one word whose parent is outside the span, which is the lexical head). The same heuristic has been used for identifying the heads of mention spans in coreference resolution (Durrett and Klein, 2013). Defining labeled arcs. The opinion relations are represented as labeled arcs that link these head nodes. Two artificial nodes are added: a root node, which links to all nodes that represent opinion words, with the label OPINION; and a null node, which is used for</context>
<context position="16174" citStr="Socher et al., 2013" startWordPosition="2618" endWordPosition="2621">dependency graph, respectively with labels OPINION, AGENT, and TARGET:p. For a sentence with L words, this decoding procedure takes O(L2) time. In practice, we speed up this process by pruning from the candidate list arcs whose connected POS were not observed in the training set and whose length were larger than the ones observed in the training set. 4.2 Features We now describe our features Oa, which are computed after processing the sentence to predict POS tags, syntactic dependency trees, lemmas and voice (active or passive) information. For English, we used the Stanford dependency parser (Socher et al., 2013a) for the syntactic annotations, the Porter stemmer to compute word stems, and a set of rules for computing the voice of each word. Our Portuguese corpus include all these preprocessing elements (§6.3), with the exception of the voice information (features depending on voice were only used for English). We also used the Subjectivity Lexicon6 of Wilson et al. (2005) that we translated to Portuguese 6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 411 (§6.3), and a set of negation words (e.g. not, never, nor) and quantity words (e.g. very, much, less) collected for both languages. Our arc-facto</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Data point selection for crosslanguage adaptation of dependency parsers.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31630" citStr="Søgaard, 2011" startWordPosition="5146" endWordPosition="5147">ning corpus, this is, to the best of our knowledge, the only existing corpus with finegrained opinions in Portuguese. We used the same arc-factored model and features described in §4. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a DELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the</context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Data point selection for crosslanguage adaptation of dependency parsers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In CoNLL.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Trans. of the Association for Computational Linguistics.</journal>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Trans. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="32274" citStr="Turian et al. (2010)" startWordPosition="5246" endWordPosition="5249">eature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (selected on the dev-set), following the procedure described in Turian et al. (2010). We trained the English delexicalized system on the MPQA corpus, using the same test documents 13A delexicalized system trained without the word embeddings had a worse performance. 415 BASELINE #1 (SUP.) BASELINE #2 (DELEX. ) BITEXT PROJECTION HM PM OM HM PM OM HM PM OM Op. 49.4 48.7 50.8 33.1 32.1 34.3 58.0* 55.7* 58.0* Op-Ag. 23.5 27.2 31.5 14.3 18.8 20.0 30.8* 31.2* 36.2* Op-Tg. 23.0 24.9 30.6 11.0 15.7 19.0 29.4* 29.4* 35.6* Op-Pol. 24.1 23.8 24.7 16.6 16.4 17.6 35.7* 34.1* 35.7* Table 5: Comparison of cross-lingual approaches. F1 scores obtained in our Portuguese validation corpus using:</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1784" citStr="Turney, 2002" startWordPosition="279" endWordPosition="280">al of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long string of work has been produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and larg</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7465" citStr="Wan (2009)" startWordPosition="1160" endWordPosition="1161">) relies on “span nodes” (instead of head words), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. 3 Dependency-Based Opinion Mining This work addresses various elements of subjectivity annotated in the MPQA corpus, namely: • direct-subjective expres</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Chris Manning</author>
</authors>
<title>Cross-lingual projected expectation regularization for weakly supervised learning.</title>
<date>2014</date>
<journal>Trans. of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<contexts>
<context position="2910" citStr="Wang and Manning, 2014" startWordPosition="452" endWordPosition="455">produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalis</context>
</contexts>
<marker>Wang, Manning, 2014</marker>
<rawString>Mengqiu Wang and Chris Manning. 2014. Cross-lingual projected expectation regularization for weakly supervised learning. Trans. of the Association for Computational Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Wei</author>
<author>Christopher Pal</author>
</authors>
<title>Cross lingual adaptation: an experiment on sentiment classifications.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7519" citStr="Wei and Pal (2010)" startWordPosition="1167" endWordPosition="1170">s), requiring solving an ILP followed by an approximate heuristic. Query-based multilingual opinion mining was addressed in several NTCIR shared tasks (Seki et al., 2007; Seki et al., 2010).4 However, to our best knowledge, a cross-lingual approach has never been attempted. Some steps were taken by Mihalcea et al. (2007) and Banea et al. (2008), who translated an English lexicon and the MPQA corpus to Romanian and Spanish, but for the much simpler task of sentence-level subjectivity analysis. Cross-lingual sentiment classification was addressed by Wan (2009), Prettenhofer and Stein (2010) and Wei and Pal (2010) at document level, and by Lu et al. (2011) at sentence level. Recently, Gui et al. (2013) applied projection learning for opinion mining in Chinese. However, this work only addresses agent detection and requires translating the MPQA corpus. While all these works are relevant, none addresses fine-grained opinion mining in its full generality, where the goal is to predict full opinion frames. 3 Dependency-Based Opinion Mining This work addresses various elements of subjectivity annotated in the MPQA corpus, namely: • direct-subjective expressions (henceforth, opinions) that are direct mentions </context>
</contexts>
<marker>Wei, Pal, 2010</marker>
<rawString>Bin Wei and Christopher Pal. 2010. Cross lingual adaptation: an experiment on sentiment classifications. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="2170" citStr="Wiebe et al., 2005" startWordPosition="337" endWordPosition="340">t al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-grained opinion mining of news documents, a long string of work has been produced (reviewed in §2). Despite the large volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effect</context>
<context position="6336" citStr="Wiebe et al., 2005" startWordPosition="982" endWordPosition="985">jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer programming or reranking strategies. Besides, there is little consensus about what should be the correct span boundaries, the inter-annotator agreement being quite low (Wiebe et al., 2005). In 3The Portuguese corpus and the lexicon are available at http://labs.priberam.com/Resources. constrast, we use dependencies to model opinion elements and relations, leading to a compact representation that does not depend on spans and which is tractable to decode. A dependency scheme was also used by Wu et al. (2011) for fine-grained opinion mining. Our work differs in which we mine opinions in news articles instead of product reviews, a considerably different task. In addition, the approach of Wu et al. (2011) relies on “span nodes” (instead of head words), requiring solving an ILP follow</context>
<context position="9302" citStr="Wiebe et al., 2005" startWordPosition="1460" endWordPosition="1463"> 409 spans “is believed” (O1) and “are against” (O2). The first opinion has an implicit agent and a neutral polarity toward the target “the rich elites” (T1). This target is also the agent (A2) of the second opinion, which has a negative polarity toward “Hugo Ch´avez” (T2). 3.1 Motivation As noted in prior work (Choi et al., 2005; Kim and Hovy, 2006; Johansson and Moschitti, 2010), one source of difficulty when learning opinion miners on MPQA is with the boundaries of the entity spans. The fact that no criterion for choosing these boundaries is explicitly defined in the annotation guidelines (Wiebe et al., 2005) leads to a low inter-annotator agreement. To circumvent this problem and make the learning task easier, we depart from the classical span-based approaches toward dependency-based opinion mining. This decision is inspired by the success of dependency models for syntax and semantics (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These dependency relations can be further converted to opinion spans (as described in §3.3), or directly used as features in downstream applications. As we will see, a compact representation based on dependencies can achieve state-of-the-art results and has the adva</context>
<context position="29113" citStr="Wiebe et al., 2005" startWordPosition="4753" endWordPosition="4756">t of each of the 80 topics. The first biennium was selected as the test set and the second biennium was split into development and training sets (see Ta11http://labs.priberam.com/Resources/ PCSC 414 ble 2 for statistics). #doc. #sent. #opin. Train 20 441 240 Dev 20 225 197 Test 40 560 391 Table 2: Number of documents, sentences and opinions in the Portuguese Corpus. HM PM OM Op. 77.0 76.7 79.2 Op-Ag. 69.1 72.3 73.5 Op-Tg. 61.9 65.4 71.4 Op-Pol. 49.4 49.1 50.7 Table 3: Inter-annotator agreement in the test partition (shown are F1 scores). The corpus was annotated in a similar vein as the MPQA (Wiebe et al., 2005), with the addition of the head node for each element of the opinion frame. It includes spans for direct-subjective expressions with intensity and polarity information; agent spans; and target spans. The annotation was carried out by three linguists, after reading the MPQA annotation guidelines (Wiebe et al., 2005; Wilson, 2008) and having a small practice period using the provided examples and some MPQA annotated sentences. Each document was annotated by two of the three linguists and then revised by the third linguist, who (in case of any doubts) discussed with the initial annotators to reac</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4559" citStr="Wilson et al. (2005)" startWordPosition="700" endWordPosition="703">nd the 7th International Joint Conference on Natural Language Processing, pages 408–418, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics its simplicity, this model is on par with state-ofthe-art opinion mining systems for English (§5). Then, through bitext projection, we transfer these dependency-based opinion frames to Portuguese (our target language), and train a system on the resulting corpus (§6). As part of this work, a validation corpus in Portuguese with subjectivity annotations was created, along with a translation of the MPQA Subjectivity lexicon of Wilson et al. (2005).3 Experimental evaluation (§7) shows that our cross-lingual approach surpasses a supervised system trained on a small corpus in the target language, as well as a delexicalized baseline trained using universal POS tags, bilingual word embeddings and a projected lexicon. 2 Related Work A considerable amount of work on fine-grained opinion mining is based on the MPQA corpus. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion </context>
<context position="16542" citStr="Wilson et al. (2005)" startWordPosition="2678" endWordPosition="2682">e now describe our features Oa, which are computed after processing the sentence to predict POS tags, syntactic dependency trees, lemmas and voice (active or passive) information. For English, we used the Stanford dependency parser (Socher et al., 2013a) for the syntactic annotations, the Porter stemmer to compute word stems, and a set of rules for computing the voice of each word. Our Portuguese corpus include all these preprocessing elements (§6.3), with the exception of the voice information (features depending on voice were only used for English). We also used the Subjectivity Lexicon6 of Wilson et al. (2005) that we translated to Portuguese 6http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 411 (§6.3), and a set of negation words (e.g. not, never, nor) and quantity words (e.g. very, much, less) collected for both languages. Our arc-factored features are described below; they are inspired by prior work on dependency parsing (Martins et al., 2013) and fine-grained opinion mining (Breck et al., 2007; Johansson and Moschitti, 2013). Opinion features. We define a set of features that only look at the opinion word; special symbols are used if the opinion is connected to a root or null node. The features </context>
<context position="30131" citStr="Wilson et al. (2005)" startWordPosition="4921" endWordPosition="4924">some MPQA annotated sentences. Each document was annotated by two of the three linguists and then revised by the third linguist, who (in case of any doubts) discussed with the initial annotators to reach for the final consensus. Scores for inter-annotator agreement are shown in Table 3. The corpus was annotated with automatic POS tags and dependency parse trees using TurboParser (Martins et al., 2013).12 We used an in-house lemmatizer to obtain lemmas for each inflected word in the corpus. A Portuguese lexicon of subjectivity was created by translating the words in the Subjectivity Lexicon of Wilson et al. (2005). The annotated corpus and the translated subjectivity lexicon are available at http://labs.priberam.com/ Resources/Fine-Grained-Opinion-Corpus, and http://labs.priberam.com/Resources/ Subjectivity-Lexicon-PT, respectively. 12http://www.ark.cs.cmu.edu/TurboParser OUR SYSTEM DELEXICALIZED HM PM OM HM PM OM Op. 65.7 63.5 69.8 50.1 45.8 52.7 Op-Ag. 47.6 48.8 51.1 33.8 34.8 35.7 Op-Tg. 34.9 44.8 50.3 19.9 28.0 32.1 Op-Pol. 51.5 50.2 54.4 36.7 34.7 38.8 Table 4: F1 scores obtained in English (MPQA), for our full system and the DELEXICALIZED one. 7 Cross-Lingual Experiments In a final set of experim</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Fine-Grained Subjectivity Analysis.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="1274" citStr="Wilson, 2008" startWordPosition="192" endWordPosition="193"> language using the automatic annotations. Key to our approach is a novel dependency-based model for opinion mining, which we show, as a byproduct, to be on par with the current state of the art for English, while avoiding the need for integer programming or reranking. In cross-lingual mode (English to Portuguese), our approach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion minin</context>
<context position="29443" citStr="Wilson, 2008" startWordPosition="4807" endWordPosition="4808">inions in the Portuguese Corpus. HM PM OM Op. 77.0 76.7 79.2 Op-Ag. 69.1 72.3 73.5 Op-Tg. 61.9 65.4 71.4 Op-Pol. 49.4 49.1 50.7 Table 3: Inter-annotator agreement in the test partition (shown are F1 scores). The corpus was annotated in a similar vein as the MPQA (Wiebe et al., 2005), with the addition of the head node for each element of the opinion frame. It includes spans for direct-subjective expressions with intensity and polarity information; agent spans; and target spans. The annotation was carried out by three linguists, after reading the MPQA annotation guidelines (Wiebe et al., 2005; Wilson, 2008) and having a small practice period using the provided examples and some MPQA annotated sentences. Each document was annotated by two of the three linguists and then revised by the third linguist, who (in case of any doubts) discussed with the initial annotators to reach for the final consensus. Scores for inter-annotator agreement are shown in Table 3. The corpus was annotated with automatic POS tags and dependency parse trees using TurboParser (Martins et al., 2013).12 We used an in-house lemmatizer to obtain lemmas for each inflected word in the corpus. A Portuguese lexicon of subjectivity </context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Wilson. 2008. Fine-Grained Subjectivity Analysis. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Extracting opinion expressions with semi-markov conditional random fields.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5577" citStr="Yang and Cardie (2012)" startWordPosition="865" endWordPosition="868">proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer programming or reranking stra</context>
<context position="14302" citStr="Yang and Cardie, 2012" startWordPosition="2278" endWordPosition="2281">verbs and the word to, when expanding to the left. The application of this simple approach to the gold dependency graphs in the training partition of the MPQA leads to oracle F1 scores of 86.0%, 95.8% and 93.0% in the reconstruction of opinion, agent and target spans, respectively, according to the proportional scores described in §5.2. 4 Arc-Factored Model One of the advantages of the dependency representation is that we can easily decode opinion-agenttarget relations without the need of complicated constrained sequence models or integer programming, as done in prior work (Choi et al., 2006; Yang and Cardie, 2012; Yang and Cardie, 2013). 4.1 Decoding We model dependency-based opinion mining as a structured classification problem. Let x be a sentence and y E Y(x) a set of well-formed dependency graphs, according to the constraints stated in §3. We define a score function that decomposes as a sum of labeled arc scores, f(x, y) = � fa(x, ya) (1) a∈y where ya is a labeled arc and the sum is over the arcs of the graph y. We use a linear model with weight vector w and local features Oa(x, ya): fa(x, ya) = w - Oa(x, ya). (2) For making predictions, we need to compute y = arg max)f (x, y). (3) Y(x Under the a</context>
</contexts>
<marker>Yang, Cardie, 2012</marker>
<rawString>Bishan Yang and Claire Cardie. 2012. Extracting opinion expressions with semi-markov conditional random fields. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5799" citStr="Yang and Cardie, 2013" startWordPosition="900" endWordPosition="903">ctively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer programming or reranking strategies. Besides, there is little consensus about what should be the correct span boundaries, the inter-annotator agreement being quite low (Wiebe et al., 2005). In 3The Portuguese corpus and the lexicon are available at ht</context>
<context position="14326" citStr="Yang and Cardie, 2013" startWordPosition="2282" endWordPosition="2285">when expanding to the left. The application of this simple approach to the gold dependency graphs in the training partition of the MPQA leads to oracle F1 scores of 86.0%, 95.8% and 93.0% in the reconstruction of opinion, agent and target spans, respectively, according to the proportional scores described in §5.2. 4 Arc-Factored Model One of the advantages of the dependency representation is that we can easily decode opinion-agenttarget relations without the need of complicated constrained sequence models or integer programming, as done in prior work (Choi et al., 2006; Yang and Cardie, 2012; Yang and Cardie, 2013). 4.1 Decoding We model dependency-based opinion mining as a structured classification problem. Let x be a sentence and y E Y(x) a set of well-formed dependency graphs, according to the constraints stated in §3. We define a score function that decomposes as a sum of labeled arc scores, f(x, y) = � fa(x, ya) (1) a∈y where ya is a labeled arc and the sum is over the arcs of the graph y. We use a linear model with weight vector w and local features Oa(x, ya): fa(x, ya) = w - Oa(x, ya). (2) For making predictions, we need to compute y = arg max)f (x, y). (3) Y(x Under the assumptions stated in §3,</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint modeling of opinion expression extraction and attribute classification.</title>
<date>2014</date>
<journal>Trans. of the Association for Computational Linguistics.</journal>
<contexts>
<context position="5553" citStr="Yang and Cardie (2014)" startWordPosition="861" endWordPosition="864">us. Kim and Hovy (2006) proposed a method for finding opinion holders and topics, with the aid of a semantic role labeler. Choi et al. (2005) and Breck et al. (2007) used CRFs for finding opinion holders and recognizing opinion expressions, respectively. The two things are predicted jointly by Choi et al. (2006), with integer programming, and Johansson and Moschitti (2010), via reranking. The same method was applied later for joint prediction of opinion expressions and their polarities (Johansson and Moschitti, 2011). The advantage of a joint model was also shown by Choi and Cardie (2010) and Yang and Cardie (2014). Yang and Cardie (2012) classified expressions with a semiMarkov decoder, outperforming a B-I-O tagger; in later work, the same authors proposed an ILP decoder to jointly retrieve opinion expressions, holders, and targets (Yang and Cardie, 2013). A more recent work (˙Irsoy and Cardie, 2014) proposes a recurrent neural network to identify opinion spans. All the approaches above rely on a span-based representation of the opinion elements. This makes joint decoding procedures more complicated, since they must forbid overlap of opinion elements or add further constraints, leading to integer progr</context>
</contexts>
<marker>Yang, Cardie, 2014</marker>
<rawString>Bishan Yang and Claire Cardie. 2014. Joint modeling of opinion expression extraction and attribute classification. Trans. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In NAACL.</title>
<date>2001</date>
<contexts>
<context position="2954" citStr="Yarowsky and Ngai, 2001" startWordPosition="458" endWordPosition="461"> volume of prior work, opinion mining has by and large been limited to monolingual approaches in English.2 This is explained by the heavy effort of annotation necessary for current learning-based approaches to succeed, which delays the deployment of opinion miners for new languages. We bridge the existing gap by proposing a cross-lingual approach to fine-grained opinion mining via bitext projection. This technique has been quite effective in several NLP tasks, such as part-of-speech (POS) tagging (T¨ackstr¨om et al., 2013), named entity recognition (Wang and Manning, 2014), syntactic parsing (Yarowsky and Ngai, 2001; Hwa et al., 2005), semantic role labeling (Pad´o and Lapata, 2009), and coreference resolution (Martins, 2015). Given a corpus of parallel sentences (bitext), the idea is to run a pre-trained system on the source side and then to use word alignments to transfer the produced annotations to the target side, creating an automatic training corpus for the impoverished language. To alleviate the complexity of the task, we start by introducing a lightweight representation— called dependency-based opinion mining—and convert the MPQA corpus to this formalism (§3). We propose a simple arc-factored mod</context>
<context position="24765" citStr="Yarowsky and Ngai, 2001" startWordPosition="4031" endWordPosition="4034"> an increase in the complexity of the model and of the decoder. 6 Cross-Lingual Opinion Mining We now turn to the problem of learning a opinion mining system for a resource-poor language (Portuguese), in a cross-lingual manner. We use a bitext projection approach (§6.1), whose only requirements are a model for a resource-rich language (English) and parallel data (§6.2). 6.1 Bitext Projection Our methodology is outlined as Algorithm 1. For simplicity, we call the source and target languages English (e) and “foreign” (f), respectively. The procedure is inspired by the idea of bitext projection (Yarowsky and Ngai, 2001). We start by training an English system on the labeled data Ge (line 1), which in our case is the MPQA v.2.0 corpus. This system is then used to label the English side of the parallel data, automatically identifying opinion frames (line 2). The next step is to run a word aligner on the parallel data (line 3). The automatic alignments are then used to project the opinion frames to the target language (along _with i5 ) (line 4), which finally serves to train a system for the target language (line 5). 6.2 Parallel Data We use an English-Portuguese parallel corpus based on the scientific news Bra</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1616" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="250" endWordPosition="254">roach compares favorably to a supervised system (with scarce labeled data), and to a delexicalized model trained using universal tags and bilingual word embeddings. 1 Introduction The goal of opinion mining is to extract opinions and sentiments from text (Pang and Lee, 2008; Wilson, 2008; Liu, 2012). With the advent of social media and the increasing amount of data available on the Web, this has become a very active area of research, with applications in summarization of customer reviews (Hu and Liu, 2004; Wu et al., 2011), tracking of newswire and blogs (Ku et al., 2006), question answering (Yu and Hatzivassiloglou, 2003), and text-to-speech synthesis (Alm et al., 2005). While early work has focused on determining sentiment at document and sentence level (Pang et al., 2002; Turney, 2002; Balog et al., 2006), research has gradually progressed towards finegrained opinion mining, where rather than determining global sentiment, the goal is to parse text into opinion frames, identifying opinion expressions, agents, targets, and polarities (Ding et al., 2008), or addressing compositionality (Socher et al., 2013b). Since the release of the MPQA corpus1 (Wiebe et al., 2005; Wilson, 2008), a standard corpus for fine-gr</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="31591" citStr="Zeman and Resnik, 2008" startWordPosition="5138" endWordPosition="5141">et described in §6.3. Though being a small training corpus, this is, to the best of our knowledge, the only existing corpus with finegrained opinions in Portuguese. We used the same arc-factored model and features described in §4. Baseline #2: Delexicalized System with Bilingual Embeddings. This baseline consists of a direct model transfer: a DELEXICALIZED system is trained in the source language, without language specific features, so that it can be directly applied to the target language. Despite its simplicity, this strategy managed to provide a fairly strong baseline in several NLP tasks (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). To achieve a unified feature representation, we mapped all language-specific POS tags to universal tags (Petrov et al., 2012), and removed all features depending on the dependency relations, but maintained those depending on the syntactic path (but not on the dependency relations themselves). In addition, we replaced the lexical features by 128-dimensional cross-lingual word embeddings.13 To obtain these bilingual neural embeddings, we ran the method of Hermann and Blunsom (2014) on the parallel data (§6.1). We scaled the embeddings by a factor of 2.0 (</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>