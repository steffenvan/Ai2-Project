<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000045">
<title confidence="0.998897">
Automated Detection of Language Issues Affecting Accuracy, Ambiguity
and Verifiability in Software Requirements Written in Natural Language
</title>
<author confidence="0.995035">
Allan Berrocal Rojas, Gabriela Barrantes Sliesarieva
</author>
<affiliation confidence="0.945471">
Escuela de Ciencias de la Computaci´on e Inform´atica
Universidad de Costa Rica, San Jos´e, Costa Rica
</affiliation>
<email confidence="0.996067">
{allan.berrocal,gabriela.barrantes}@ecci.ucr.ac.cr
</email>
<sectionHeader confidence="0.993834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999618961538461">
Most embedded systems for the avionics in-
dustry are considered safety critical systems;
as a result, strict software development stan-
dards exist to ensure critical software is built
with the highest quality possible. One of such
standards, DO-178B, establishes a number
of properties that software requirements must
satisfy including: accuracy, non-ambiguity
and verifiability. From a language perspec-
tive, it is possible to automate the analysis of
software requirements to determine whether
or not they satisfy some quality properties.
This work suggests a bounded definition for
three properties (accuracy, non-ambiguity and
verifiability) considering the main character-
istics that software requirements must exhibit
to satisfy those objectives. A software proto-
type that combines natural language process-
ing (NLP) techniques and specialized dictio-
naries was built to examine software require-
ments written in English with the goal of iden-
tifying whether or not they satisfy the de-
sired properties. Preliminary results are pre-
sented showing how the tool effectively iden-
tifies critical issues that are normally ignored
by human reviewers.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965375">
Software requirements play a critical role in the
software life cycle. It has been observed that
poorly written software requirements often lead to
weak and unpredictable software applications (Wil-
son et al., 1997). Besides, the cost of fixing er-
rors increases exponentially throughout the differ-
ent phases of software development (Galin, 2004;
Leffingwell and Widrig, 2003). In other words, it is
less expensive to fix an error in the software require-
ments phase than it is to fix the same error during
the integration or verification phase.
Embedded systems for the avionics industry are
developed following particularly rigorous restric-
tions due to strict safety and availability constraints
that need to be satisfied during air or ground op-
erations. DO-178B (RTCA, 1992) is a recognized
standard for development of safety critical embed-
ded systems. It is widely used by software certi-
fication authorities such as FAA (Federal Aviation
Association), and it establishes some guidelines and
quality objectives for each phase of a software devel-
opment effort. In particular, the standard estates that
software requirements must be accurate, verifiable,
and non-ambiguous.
</bodyText>
<subsectionHeader confidence="0.998234">
1.1 Software Quality
</subsectionHeader>
<bodyText confidence="0.9999683">
Milicic suggests that software quality can be under-
stood as conformity with a given specification (Mili-
cic et al., 2005). This definition is in total agree-
ment with DO-178B, which requires that software is
designed, built, and tested following approved stan-
dards for each phase of the development cycle.
Extrapolating the previous definition, one can ar-
gue that quality of software requirements can be un-
derstood as the degree to which software require-
ments also comply with a given specification. In
other words, in order to produce high quality soft-
ware requirements, one needs to ensure that they
satisfy the criteria established in a software require-
ments standard.
In the case of software requirements written in
natural language (NL), some of the criteria can
be addressed from a linguistic perspective, observ-
ing certain types of language constructs and lan-
guage usage in general that may represent violations
against desired quality criteria in a standard.
</bodyText>
<page confidence="0.910222">
100
</page>
<note confidence="0.979188">
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 100–108, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.8855">
1.2 Overview of Research Goals
</subsectionHeader>
<bodyText confidence="0.999975961538462">
The overall objective of this research was to identify
some of the linguistic elements that one can observe
to determine whether or not software requirements
written in natural language1 comply with three spe-
cific properties established by DO-178B: accuracy,
verifiability and non-ambiguity. Those linguistic el-
ements can be seen as rules in an expert system, so
that requirements are said to be compliant with their
quality objectives when they satisfy all rules. They
are said to be non-compliant with their quality ob-
jectives when rules are not satisfied.
In this research, linguistic elements were identi-
fied and independently validated by professionals in
the field of software verification. Later on, a soft-
ware prototype capable of examining a list of re-
quirements was built to automatically detect when
requirements do not satisfy a given rule.
The main contribution of this research is that it
provides a quantitative evaluation of the target re-
quirements. More specifically, based on the num-
ber of satisfied and non satisfied rules, the proto-
type scores each requirement in a 1 to 10 scale. The
tool also provides additional information (qualitative
analysis) to the user indicating the root of the prob-
lem when a given rule is not satisfied, as well as pos-
sible ways to fix the issue.
</bodyText>
<subsectionHeader confidence="0.985409">
1.3 Justification
</subsectionHeader>
<bodyText confidence="0.999992571428571">
The author’s experience in the field of requirements
verification suggests that the task of reviewing a set
of requirements for compliance with properties such
as accuracy, verifiability and non-ambiguity is a non
trivial task. This is particularly true when the re-
viewer lacks the proper training and tools. Some of
the known difficulties for this process are:
</bodyText>
<listItem confidence="0.999097714285714">
• It requires linguistic (e.g. grammar, semantics)
and technical knowledge from a reviewer.
• There is no warranty that two or more review-
ers will produce the same findings for the same
input (mostly due to the informal nature of NL).
• The process is error prone since reviewers be-
come fatigued after some time.
</listItem>
<footnote confidence="0.818218">
1This research assumes requirements are written in English.
</footnote>
<listItem confidence="0.9921335">
• The process is time consuming, which directly
affects budget and schedule performance.
</listItem>
<bodyText confidence="0.998099375">
Having a tool that partially automates the pro-
cess of reviewing software requirements may rep-
resent significant improvements in the overall soft-
ware life cycle process. Even when current devel-
opments in computational linguistics do not provide
a complete solution for the problem at hand, a par-
tial approach is still valuable producing numerous
advantages such as:
</bodyText>
<listItem confidence="0.999402714285714">
• Linguistic and technical knowledge is input
into the system in a cumulative manner, reduc-
ing dependency on highly qualified personnel.
• Results are reproducible for any given set of in-
puts, reducing inconsistencies while adding re-
liability to the results.
• Review time is significantly reduced.
</listItem>
<sectionHeader confidence="0.963685" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999898153846154">
Significant work has been done in the area of soft-
ware requirements analysis. Lami (Lami et al.,
2004) classifies these efforts in three groups. A first
group consists of preventive techniques that need
to be applied during the process of writing require-
ments. Those techniques normally trigger checklists
that are enforced by a person with no support from
tools, see for instance (Firesmith, 2003). Another
group consists of restrictive techniques that limit the
degree of language freedom when writing require-
ments. One example in this group is Fuchs (Fuchs
et al., 1998) who introduces ACE (Attempto Con-
trolled English), a restricted subset of English with
a restricted grammar and domain specific vocabu-
lary. Requirements can be written in natural lan-
guage with enough expressive power. They are later
translated into first order predicate logic to be pro-
cessed formally by a computer program.
The last group of efforts consists of analytic tech-
niques that perform automated analysis of require-
ments once they have been produced. The follow-
ing are two relevant projects in this group. Wilson
(Wilson et al., 1997) developed a tool named ARM
that performs automated analysis of a requirements
document. The tool focuses on lexical analysis to
detect specific keywords such as vague adverbs and
</bodyText>
<page confidence="0.998151">
101
</page>
<bodyText confidence="0.999872888888889">
vague adjectives that are not desired. Different from
our work, ARM also checks that the document it-
self complies with a specific format. Then, Lami
(Lami et al., 2004) described a systematic method
for automated analysis of requirements detecting de-
ficiencies such as ambiguity, inconsistencies, and in-
completeness. A tool named QuARS implements
the suggested methodology and appears to be a good
contribution in this area 2.
</bodyText>
<sectionHeader confidence="0.996316" genericHeader="method">
3 Theoretical Framework
</sectionHeader>
<bodyText confidence="0.999912">
This section provides a basic explanation of some
concepts that are commonly used in the field of soft-
ware verification. Emphasis will be made on con-
cepts related to the software engineering field in
an attempt to set the grounds for the investigation.
Other linguistic related concepts will be mentioned
along the paper assuming the reader has basic under-
standing of them.
</bodyText>
<subsectionHeader confidence="0.999275">
3.1 Software Life Cycle
</subsectionHeader>
<bodyText confidence="0.999977375">
A Software Life Cycle Model or Software Develop-
ment Model consists of a group of concepts and well
coordinated methodologies that guide the software
development process from beginning to end (Galin,
2004). The classic software life cycle model (a.k.a.
the waterfall model) consists of linear sequence of
activities or phases that take place during a software
development effort.
In the Requirements elicitation phase, a detailed
description of what the software shall do is pro-
duced. Although there are various methods, a natu-
ral language description in the form of a list of state-
ments is widely used to produce requirements.
A software requirement is a condition or charac-
teristic that a system must possess to satisfy a con-
tract, a standard, a formal specification or other ap-
plicable regulation (IEEE, 1990).
In simple words, a software requirement explains
how the system should behave or react given a spe-
cific set of inputs and initial conditions. While not
true for all software applications, in the avionics in-
dustry, all software functionalities are required to be
fully deterministic. This means that the system must
behave exactly the same all the time for a given set
</bodyText>
<footnote confidence="0.960312">
2The author has not been able to use QuARS yet.
</footnote>
<bodyText confidence="0.998206571428571">
of inputs and initial conditions. This is why correct-
ness of requirements is so critical.
The following section briefly comments on three
of the properties that requirements must satisfy to
meet quality objectives. Although there are many
such properties, we focus on three whose detection
is partially automated in this research.
</bodyText>
<subsectionHeader confidence="0.9648465">
3.2 Quality Properties for Software
Requirements
</subsectionHeader>
<bodyText confidence="0.999907">
To meet quality objectives, software requirement
must be accurate, non-ambiguous and verifiable.
This section provides a brief explanation of these
terms in the context of software verification. Ad-
ditionally, it describes the main language elements
used in this research to automatically detect when
software requirements do not satisfy a given prop-
erty.
</bodyText>
<subsectionHeader confidence="0.796055">
3.2.1 Ambiguity
</subsectionHeader>
<bodyText confidence="0.948600142857143">
A word or phrase is said to be ambiguous when
it has more than one possible meaning causing con-
fusion or uncertainty. Similarly, software require-
ments are said to be ambiguous when they admit
more than one possible interpretation. An ambigu-
ous requirement is notably incompatible with the
goal of producing deterministic software.
Berry (Berry, 2003) distinguishes six major forms
of ambiguity in software requirements: lexical, syn-
tactical, semantic, pragmatic, vagueness and lan-
guage error. In this research, we focused on lexi-
cal, syntactic, vagueness, and language errors since
this group covers common deficiencies that show in
requirements.
One form of syntactical ambiguity occurs when
requirements fail to group logical conditions (e.g.
AND, OR) with appropriate punctuation marks or
explicit parenthesis. In the following example, for
instance, it is not clear what the conditions are for
the system to enter into normal mode: “The system
shall enter Normal mode when SDI field on label 227
equals 2 or SSM in label 268 equals 3 and WOW is true
or AIR is false.”
Vague adverbs usually modifying nouns (such as:
acceptable, high, low, fast, in/sufficient, normal,
similar and many others) also create ambiguous re-
quirements like the following: “The system shall allow
the operator to adjust volume to an acceptable level.”
</bodyText>
<page confidence="0.994631">
102
</page>
<bodyText confidence="0.9997554">
Finally, non deterministic constructs such as
and/or, any, not limited to also create ambiguity in
requirements, such as the following case: “The sys-
tem shall display altitude and/or temperature at the bot-
tom line of the screen.”
</bodyText>
<subsectionHeader confidence="0.952067">
3.2.2 Accuracy
</subsectionHeader>
<bodyText confidence="0.999485864864865">
In a requirement, accuracy refers to how concise
and precise a requirement is specified. Accuracy
should be present not only in the content but also
in the structure of a requirement.
In terms of structure, a requirement must clearly
distinguish between at least two parts: condition and
action. A requirement with a clear action and no
condition opens a possibility to think that the spec-
ified action is permanent (which is rarely the case).
On the other hand, by definition, there can not exist
a requirement with no action.
For instance, the following requirement is inaccu-
rate: “The system shall clear the DMA shared space,”
since no one knows when the action must occur.
In terms of content, a requirement must include
clear and detailed information about the condition
and the action that is being described. Accurate re-
quirements also include explicit units for physical
values as well as tolerances and thresholds for nu-
merical computations.
For instance, the following requirement is inaccu-
rate “The system shall send ARINC label 251 every 50
ms,” but adding a tolerance value solves the issue as
in “The system shall send ARINC label 251 every 50 ms
+/- 5ms.”
Non deterministic adverbs usually modifying
verbs (such as: continually, periodically, regularly
and others) also create inaccurate requirements like
the following: “The system shall periodically perform
CBITE.”
Finally, there are a number of general verbs that
should be avoided in requirements since they cre-
ate inaccurate descriptions. Some of these verbs are:
process, monitor, support, check among others. For
instance, it is not clear to see the software action that
this requirement implies: “The system shall monitor re-
sponses from the slave processor.”
</bodyText>
<subsectionHeader confidence="0.592612">
3.2.3 Verifiability
</subsectionHeader>
<bodyText confidence="0.998688409090909">
A requirement is said to be verifiable if it is pos-
sible to create and execute a test to demonstrate that
the software behaves exactly as specified in the re-
quirement.
Sometimes a test can not be executed primarily
because of hardware or test equipment limitations.
In other cases, conflicts or inconsistencies between
requirements are revealed which prevent a test from
being performed. However, another group of re-
quirements become non verifiable due to language
usage errors.
For instance, by definition requirements are meant
to describe actions that the system shall perform. In
that sense, a requirement must not describe anything
that the system shall not perform. To illustrate, a
requirement such as the following is non verifiable:
“The system shall not enter INTERACTIVE mode when
WOW is false.” The reason is that a tester can not
expect any specific system action during a test for
this requirement.
Furthermore, requirements using the adverbs al-
ways and never are also non-verifiable since a test
for them would require infinite time. Similarly, the
term only must be used correctly when modifying
the main action (verb) of the requirement. For ex-
ample, the requirement “The system shall only display
invalid data in red color” implies that the only ac-
tion this system performs is “display invalid data in
red color.” The intended meaning is probably “The
system shall display only invalid data in red color.”
Finally, some requirements contain verbs that im-
ply actions that a software application can not per-
form; instead, these are usually human-specific tasks
that are incorrectly assigned to software. Some of
these verbs are: determine, ignore, consider, anal-
yse and others. One example of wrong usage is “The
system shall consider fault history during CBITE.”
As mentioned in section 1.2, one objective was to
provide a quantitative evaluation of a set of require-
ments against three properties: accuracy, ambigu-
ity and verifiability. With that goal in mind, section
4.1 introduces some of the formulations that will be
used to perform the evaluation of the requirements
against the selected properties.
</bodyText>
<sectionHeader confidence="0.998025" genericHeader="method">
4 Research Foundation
</sectionHeader>
<bodyText confidence="0.999786">
To accomplish the general objectives described in
section 1.2, section 4.1 introduces a semi-formal
nomenclature used to express the various situations
</bodyText>
<page confidence="0.997303">
103
</page>
<bodyText confidence="0.999949">
when a requirement either satisfies or violates any
of the desired properties. This nomenclature is valu-
able for it allows to represent various situations
in a symbolic and summarized way. Section 4.2
describes the process followed to select the crite-
ria against which the software requirements will be
evaluated for quality.
</bodyText>
<subsectionHeader confidence="0.998401">
4.1 Proposing General Nomenclature
</subsectionHeader>
<bodyText confidence="0.921113631578947">
We will use the term element to refer to individual
linguistics elements or rules as mentioned in section
1.2. Similarly, the term attribute refers to quality
properties: accuracy, ambiguity and verifiability.
To represent the attribute ambiguity, we define the
set A = {λ1, λ2,..., λk} with k E N&gt;1 where
each λi is an element that reveals a non compliance
for the attribute of ambiguity by a given requirement.
For instance, let’s assume λ1 = “A requirement must
not use vague or general adverbs to describe an ac-
tion,” then, if we apply λ1 to the requirement R1 =
“The system shall allow the operator to adjust vol-
ume to an acceptable level,” we conclude that R1 is
ambiguous since the adverb “acceptable” is vague.
In that case we say that R1 does not satisfy λ1.
For accuracy we define F = {γ1,γ2, ...,γY}
with r E N&gt;1, and for verifiability we define
T = {υ1, υ2,..., υs} with s E N&gt;1 in an analo-
gous way. Summarizing, we define:
</bodyText>
<equation confidence="0.898177">
X1 = A , X2=1&apos; , X3 = T
</equation>
<bodyText confidence="0.998709333333333">
where Xi = {21, 22, ..., 2n} and each 2i is an
element that tells us if a requirement does not satisfy
a specific attribute.
We propose the following notation to represent
situations where requirements fail to satisfy an el-
ement.
</bodyText>
<listItem confidence="0.997186444444444">
• When a requirement Re meets the restriction
imposed by an element 2k, we say that Re
satisfies 2k, and we write Re O 2k.
• When a requirement Re does not meet the re-
striction imposed by an element 2k, we say that
Re does not satisfy 2k, and we write Re 0 2k.
• When the restriction imposed by an element 2k
is not applicable for a requirement Re, we say
that 2k is not applicable for Re and we write
</listItem>
<equation confidence="0.477256">
Re ® 2k
</equation>
<bodyText confidence="0.998329470588236">
Notice how the expressions Re O 2k , Re 0 2k
and Re ® 2k can be seen as logical predicates for a
binary relation. For instance, we could read the first
expression as O(Re, 2k) or SATISFIES(Re, 2k) .
However, computing the degree in which a re-
quirement satisfies an attribute is not a binary rela-
tion. For instance, a requirement can meet some re-
strictions and not others; besides, some restrictions
are more critical than others.
For a more objective evaluation, a scale from 0
to 10 is proposed. Each element 21, 22, ..., 2n in
Xi is assigned a value or score so that the scores
for all elements in an attribute add up to 10 and
score(2i) = p , p E R+ (where R+ = [0, 10]).
Values are assigned depending on the criticality and
type of error revealed by each element. Summariz-
ing:
</bodyText>
<equation confidence="0.996844">
|Xi|
j=1
E score(2j) = 10 (1)
</equation>
<bodyText confidence="0.978844333333333">
where 2j E Xi and Xi E {A, 1&apos;, T}
Now, in order to evaluate a requirement Re in re-
gards to an attribute, we define θ: (R) —* R+:
</bodyText>
<equation confidence="0.9745476">
�
θ(Re, Xi) = [10 −
j,ReOE7
[ � score(2j)] where 2j E Xi
j,ReGE7
</equation>
<bodyText confidence="0.999297375">
Notice how we write θ using either predicate
does not satisfy 0 or satisfies O. In both cases, when
an element is not applicable ® to a requirement, we
assume that the requirement satisfies such element.
To understand the meaning of θ, suppose that
x = θ(Re, f&apos;) is the score a requirement Re gets
when it is evaluated against a given attribute, let’s
say accuracy.
</bodyText>
<listItem confidence="0.98622525">
• When x = 10 we say that Re satisfies IF and we
write Re O 1. Re is accurate, since it meets all
the restrictions imposed by each element in 1.
• When x = 0 we say that Re does not satisfy F
</listItem>
<bodyText confidence="0.717471333333333">
and we write Re 00 1. Re is not accurate, since
it does not meet any of the restrictions imposed
by elements in 1.
</bodyText>
<equation confidence="0.787917">
score(2j)] =
(2)
</equation>
<page confidence="0.988138">
104
</page>
<listItem confidence="0.952797833333333">
• When 0 &lt; x &lt; 10 we say that Re
does not satisfy F with a degree x and we write
Re Ox F. Re meets some of the restrictions im-
posed by elements in F but not all. In this case,
the closer x is to 10, the better the requirement
will be3.
</listItem>
<bodyText confidence="0.9426915">
Finally, to get a requirement’s overall score
against all three attributes, we define a function
</bodyText>
<equation confidence="0.971445">
φ : (R) —* R+ as follows:
φ(Rk) = Ei 1 θ3Rk&apos; Xi) where Xi E {A, F, T}
(3)
</equation>
<bodyText confidence="0.9997976">
φ is the arithmetic mean of the scores a require-
ment gets against each attribute hXi in (2). The
overall score is a measure of a requirement’s qual-
ity, and it could be used potentially to estimate costs
in a software project.
</bodyText>
<listItem confidence="0.788160266666667">
To understand φ suppose x = φ(Rk) is the score
a requirement Rk gets when it is evaluated against
all three attributes (ambiguity, accuracy and verifia-
bility) using all elements in {A, F, T}.
• When x = 10, we say that Rk is accurate, ver-
ifiable and non-ambiguous since
Rk O Xi VXa E {A, F, T}.
• When x = 0 we say that Rk is inaccurate, non-
verifiable and ambiguous since
Rk 0 Xi VXa E {A, F, T}.
• When 0 &lt; x &lt; 10, we say that Rk is ei-
ther inaccurate, non-verifiable or ambiguous
since it does not satisfy at least one element in
{A, F, T}. In this case, θ provides more infor-
mation about the weakness detected in Rk.
</listItem>
<bodyText confidence="0.999350833333334">
The value of the suggested notation comes from
the fact that we can now produce quantitative evalu-
ations of requirements, as opposed to common qual-
itative evaluations. The following sections briefly
describe a bottom up process we followed to select
evaluation criteria for the prototype that was built.
</bodyText>
<footnote confidence="0.931763333333333">
3We will use either notation m or mx to indicate that a re-
quirement does not satisfy an attribute and the degree x is not
relevant.
</footnote>
<subsectionHeader confidence="0.993356">
4.2 Selecting Criteria for Evaluation
</subsectionHeader>
<bodyText confidence="0.998310666666667">
The process of selecting the elements for each at-
tribute was conducted in a series of steps that are
summarized below. The objective of our approach
was to provide a selection of elements that satis-
fied three main goals. The first goal was to have
representative and useful selection within the field
of software verification. The second goal was that
the selection could be independently validated by a
group of professionals in the field. And finally, the
third goal was that the selection of elements refers
to weaknesses that can be automatically detected by
a software.
The following is a summary of the process that
was followed to select the criteria to evaluate re-
quirements.
</bodyText>
<listItem confidence="0.97259425">
1. A list of elements was first suggested by the
author based on relevant literature and his own
experience in software verification for embed-
ded systems. The list contained 19 elements
(10 for accuracy, 5 for ambiguity and 4 for ver-
ifiability).
2. Five elements were filtered out as they were
not candidates to be automated. Feasible can-
</listItem>
<bodyText confidence="0.928502375">
didates were those that could be automated us-
ing techniques such as parsing, tagging, regular
expressions, and specialized dictionaries like
WordNet (Miller, 1993) and VerbNet (Kipper,
2005). The list ended with 6 elements in accu-
racy, 4 in ambiguity and 4 in verifiability.
3. The author suggested an initial value or score
for each element in the list.
</bodyText>
<listItem confidence="0.662055">
4. Both the element selection and the value distri-
bution were independently validated by a group
of three professionals with demonstrable expe-
rience in software verification4.
5. A numerical model was prepared based on the
proposed approach described in section 4.1.
This is already a contribution since the evalu-
ation of the requirements could be done manu-
ally in case no tool had been created.
</listItem>
<footnote confidence="0.951358">
4Although these individuals are not language experts, since
they have valuable experience in requirements verification, their
feedback was considered a valid complement in this research.
</footnote>
<page confidence="0.99638">
105
</page>
<bodyText confidence="0.7493">
6. A software prototype was written for a tool that
is capable of examining a list of requirements
applying equations 2 and 3 in section 4.1.
Section 5 briefly describes the capabilities of the
prototype tool that was developed.
</bodyText>
<sectionHeader confidence="0.985259" genericHeader="method">
5 Automated Evaluation
</sectionHeader>
<bodyText confidence="0.999943285714286">
This section provides a brief description of the soft-
ware prototype. A more in depth description would
be ideal; however, due to space limitations we will
focus on two items only. First, an overview of the
tool’s architecture and technologies involved (sec-
tion 5.1). Second, a description of the outputs this
tool produces (section 5.2).
</bodyText>
<subsectionHeader confidence="0.998514">
5.1 Building the Prototype
</subsectionHeader>
<bodyText confidence="0.999729703703704">
Our prototype tool receives the name of SRR-
Director from Software Requirements Reviewer Di-
rector. This prototype was built using open source
software and tools that are freely available for re-
search. Our goal was to integrate several of these
available resources into a single piece of software
that helped us solving the problem we are studying.
Perl5 was used as the main language for the soft-
ware and Awk6 was used as an independent tool to
check some of the results while developing the tool.
Input requirements normally exist in various formats
such as MS Word7, MS Excel8, structured XML, or
plain text files. We provide a tool that can be config-
ured to read those inputs converting them into XML
documents that follow a normalized structure which
basically separates requirements identifiers from the
actual text of the requirement.
The three main techniques used during automated
inspection of the requirements were the following:
Lexical Analysis: this is a common and simple
technique that is based on regular expressions. Perl’s
engine for regular expressions was particularly use-
ful in this task. This type of analysis allows identifi-
cation of key words or phrase structures that reveal
specific types of weaknesses in requirements.
This technique helped identifying issues of all
three types. For ambiguity it allowed to locate
</bodyText>
<footnote confidence="0.999913">
5http://www.perl.org
6http://www.gnu.org/software/gawk/
7http://office.microsoft.com/word
8http://office.microsoft.com/excel
</footnote>
<bodyText confidence="0.999797541666667">
vague adverbs and non deterministic language con-
structs; for accuracy, we detected tolerance issues,
non deterministic adverbs and general verbs. Fi-
nally, to check for verifiability this technique was
used to capture negative requirements, infinite re-
quirements, and wrong usage of the term only.
Syntactic Analysis: consists of parsing the re-
quirements to transform language statements into
their grammatical constituents which enables other
specific analysis such as ambiguity analysis. This
process was performed using a parser made avail-
able by Eugine Charniak and Brown University
(Charniak et al., 2006). In this case, the CLAIR
group at University of Michigan made available a
Perl wrapper for the Charniak parser(CLAIR, 2009).
Studying the syntax tree produced by the parser, it
was possible to identify accuracy issues such as re-
quirements without explicit condition statements (or
condition blocks). Also, studying the output of the
parser along with lexical analysis of the requirement
reveals cases of ambiguity when logical conditions
are not stated clearly.
Dictionaries: two great resources were also in-
corporated in this research to support our analy-
sis: WordNet (Miller, 1993) and VerbNet (Kipper,
2005). Both of this tools can also be accessed from
Perl via wrappers and provide useful information
about words and verbs that were used to ensure some
conditions were valid while we perform the analysis
of the requirements.
Dictionaries allow identification of human spe-
cific verbs and ambiguous verbs. In this case, the
parser makes it possible to capture the main verb for
a requirement, and further queries into dictionaries
complete the task. VerbNet provides a mechanism
to classify requirements according to their degree of
ambiguity. This mechanism may be too stringent
for flagging ambiguous verbs sometimes. There are
verbs tagged as ambiguous in VerbNet, but they have
a fairly well known and shared meaning in the do-
main of software engineering such as: set, shut-
down, turnoff, send, receive among others.
SRR-Director runs from a command line and it
is currently controlled using a number of arguments
and switches. Even when this is still a prototype
tool, our experiments show that the tool is very effi-
cient capturing weaknesses in the requirements with
a marginal error rate (&lt; 5%) for the rules included
</bodyText>
<page confidence="0.994895">
106
</page>
<bodyText confidence="0.9999625">
in the current version of the tool. More importantly,
the tool is able to examine hundreds of requirements
in a matter of minutes when the same work takes
hours or even days for a human reviewer.
</bodyText>
<subsectionHeader confidence="0.99935">
5.2 Using the Reports
</subsectionHeader>
<bodyText confidence="0.999978695652174">
In the current prototype version, the tool produces
seven types of reports that provide information for
three types of users:
Quality engineers: two reports show general in-
formation about the quality of the requirements that
were analysed. Quality engineers are interested in
the overall percentage of requirements compliance
with the quality objectives, and they don’t need de-
tails on the types of failures.
Requirements engineers: four reports are avail-
able for the largest audience of users who are actu-
ally interested in learning the details about the types
of failures identified in the requirements. Not only
are the engineers notified of the weaknesses but also
they are provided with suggestions on how to fix the
issues. The evaluation they receive is not only qual-
itative but also quantitative since they can see the
score for individual requirements against each of the
three properties being studied.
Software engineers: this is a miscellaneous re-
port that provides performance information which
may later help software engineers while tuning cer-
tain processes in the tool.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999992282051282">
Experiments were performed using sample require-
ments from three distinct and real word applications
in embedded systems. Test data was selected from
a pool of reserved requirements that were not used
during development of the tool.
Four groups of 20 requirements each were se-
lected and given to three experienced professionals
in the field of software verification. The subjects
were asked to identify weaknesses in the require-
ments using their own criteria. They were asked to
classify ill requirements as inaccurate, ambiguous
or non-verifiable when applicable. The same groups
were input to the prototype for evaluation, and re-
sults were compared.
As it was mentioned before, the tool recognizes
all deficiencies described by a rule or element with
a low error rate (&lt; 5%). We believe this is mostly
due to the fact that –in this initial phase of the tool–
rules are not complex, and can indeed be automated
without using complex techniques.
One interesting result was that a high degree of
discrepancies and disagreement between the subject
reviewers was observed . On average, the three re-
viewers agreed only in 14% of their evaluations, and
only in 62% of the cases there was agreement be-
tween at least two reviewers. These unexpected dis-
crepancies certainly make it difficult to compare the
tool’s results with the reviewer’s results to identify
areas of agreement or disagreement.
A more in depth analysis of the results suggests
that human beings may perform erratically when it
comes to reviewing requirements that contain the
types of errors we are looking for. Some of the
weaknesses we want to uncover are rather subtle
and, as we argued before (section 1.3), require a
good level of language and technical knowledge as
well as a detail oriented attitude. People are also
affected by external factors such as fatigue that neg-
atively affects the quality of their work.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999611764705882">
The results of this research show that it is actually
possible to automate the review process of software
requirements identifying valuable sources of defi-
ciencies that otherwise make requirements inaccu-
rate, ambiguous or non-verifiable.
Besides, there are resources freely available for
research that can be integrated into more specific
tools to solve a variety of problems. Specialized dic-
tionaries, stand alone tools, such as parsers, and a
general purpose scripting language (Perl) were com-
bined in order to create the tool prototype.
Finally, a simple but rather useful nomenclature
to represent different scenarios that occur during re-
quirements verification was proposed. This con-
tribution allows us to provide a quantitative anal-
ysis of the requirements as opposed to traditional
qualitative-only analyses.
</bodyText>
<sectionHeader confidence="0.992807" genericHeader="acknowledgments">
8 Collaboration Opportunities
</sectionHeader>
<bodyText confidence="0.998892666666667">
This section answers two specific questions to de-
scribe possible collaboration opportunities between
investigators doing research on similar topics.
</bodyText>
<page confidence="0.995878">
107
</page>
<bodyText confidence="0.977569906976744">
8.1 How can this work benefit other research
projects?
This research was focused on three properties ap-
plicable to software requirements for aerospace sys-
tems. However, it would be ideal to apply similar
techniques to examine other types of properties that
are crucial in similarly critical application domains
such as finance, transportation, medicine and com-
munications.
In this work, inputs are text documents with nat-
ural language text in the form of software require-
ments. Those inputs are preprocessed and converted
into simpler representations that basically consist of
sentences. Those sentences at the end are the main
input for the tool that performs the automated qual-
ity analysis.
Researchers wishing to learn more about this
work are strongly encouraged to contact the author
to share ideas on this topic and benefit from one an-
other. We believe it is possible to reuse part of the
approach to build similar tools to analyse require-
ments in languages other than English.
8.2 What are some resources and expertise the
author lacks?
One of the main difficulties the author faced is the
absence of collaboration between researchers inter-
ested in similar topics. This work has been produced
mostly in isolation as part of academic research in a
masters program.
Being able to share ideas with working groups
either academic or industry sponsored would be a
great channel to improve research scope and pro-
duce more significant results. For the nature of this
research, a mixed team of linguists and software en-
gineers would presumably improve the quality of the
work.
On the one hand, linguists would provide valu-
able knowledge that would help identifying addi-
tional language structures that represent symptoms
of weaknesses in requirements. On the other hand,
software engineers would be closer to requirements
engineers and could contribute with implementation
details so that new rules are added to the system.
</bodyText>
<sectionHeader confidence="0.996123" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999468632653061">
Wilson, W. and Rosenberg, L. and Hyatt, L. 1997. Auto-
mated Analysis of Requirement Specifications. Nine-
teenth International Conference on Software Engineer-
ing (ICSE-97), Boston, MA.
Galin, D. 2004. Software Quality Assurance From theory
to implementation. Pearson Education Ltd.
Leffingwell, D. and Widrig, D. 2003. Managing Soft-
ware Requirements, 2nd Ed. Addison-Wesley.
RTCA/EUROCAE. 1992. DO-178 Software Consider-
ations for Airborne Systems and Equipment Certifica-
tion. RTCA, Inc., Washington, DC.
Drazen, M. and Berander, P. and Damm, L. and Eriksson,
J. and Gorschek, T. and Henningsson, K. and Jonsson,
P. and Kagstrom, S. and Martensson, F. and Ronkko,
K. and Tomaszewski, P.. 2005. Software quality at-
tributes and trade-offs, Software Quality Models and
Philosophies. Blekinge Institute of Technology.
IEEE Standards Board. 1990. IEEE Standard Glossary
of Software Engineering Terminology, Std 610.12-
1990.
Berry, D. and Kamsties, E. and Krieger, M.. 2003. From
Contract Drafting to Software Specification: Linguis-
tic Sources of Ambiguity.
Miller, G. and Beckwith, R. and Fellbaum, C. and Gross,
D. and Miller, K.. 1993. Introduction to WordNet: An
Online Lexical Database. Cognitive Science Labora-
tory, Princeton University.
Kipper, K.. 2005. VerbNet: A broad-coverage, compre-
hensive verb lexicon. Computer and Information Sci-
ence, University of Pennsylvania.
Lami, G. and Gnesi, S. and Fabbrini, F. and Fusani, M.
and Trentanni, G.. 1997. An Automatic Tool for the
Analysis of Natural Language Requirements. C.N.R.
Information Science and Technology Institute, Pisa
Italy.
McClosky, D. and Charniak, E. and Johnson, M.. 2006.
Reranking and Self-Training for Parser Adaptation.
21st International Conference on Computational Lin-
guistics.
CLAIR official website. 2009. URL http:
//belobog.si.umich.edu/clair/clair/
downloads.html. Visited on March 12, 2009.
Fuchs, N. and Schwertel, U. and Schwitter, D.. 1998.
Attempto Controlled English Not Just Another Logic
Specification Language. Eighth International Work-
shop on Logic-based Program Synthesis and Transfor-
mation LOPSTR’98, Manchester, UK.
Firesmith, D.. 2003. Specifying Good Requirements.
Journal of Object Technology, ETH Zurich.
</reference>
<page confidence="0.998383">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520475">
<title confidence="0.9984015">Automated Detection of Language Issues Affecting Accuracy, and Verifiability in Software Requirements Written in Natural Language</title>
<author confidence="0.988881">Allan Berrocal Rojas</author>
<author confidence="0.988881">Gabriela Barrantes</author>
<affiliation confidence="0.738064">Escuela de Ciencias de la Computaci´on e Universidad de Costa Rica, San Jos´e, Costa</affiliation>
<abstract confidence="0.999112888888889">Most embedded systems for the avionics industry are considered safety critical systems; as a result, strict software development standards exist to ensure critical software is built with the highest quality possible. One of such standards, DO-178B, establishes a number of properties that software requirements must including: From a language perspective, it is possible to automate the analysis of software requirements to determine whether or not they satisfy some quality properties. This work suggests a bounded definition for properties considering the main characteristics that software requirements must exhibit to satisfy those objectives. A software prototype that combines natural language processing (NLP) techniques and specialized dictionaries was built to examine software requirements written in English with the goal of identifying whether or not they satisfy the desired properties. Preliminary results are presented showing how the tool effectively identifies critical issues that are normally ignored by human reviewers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Wilson</author>
<author>L Rosenberg</author>
<author>L Hyatt</author>
</authors>
<title>Automated Analysis of Requirement Specifications.</title>
<date>1997</date>
<booktitle>Nineteenth International Conference on Software Engineering (ICSE-97),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1726" citStr="Wilson et al., 1997" startWordPosition="238" endWordPosition="242"> software prototype that combines natural language processing (NLP) techniques and specialized dictionaries was built to examine software requirements written in English with the goal of identifying whether or not they satisfy the desired properties. Preliminary results are presented showing how the tool effectively identifies critical issues that are normally ignored by human reviewers. 1 Introduction Software requirements play a critical role in the software life cycle. It has been observed that poorly written software requirements often lead to weak and unpredictable software applications (Wilson et al., 1997). Besides, the cost of fixing errors increases exponentially throughout the different phases of software development (Galin, 2004; Leffingwell and Widrig, 2003). In other words, it is less expensive to fix an error in the software requirements phase than it is to fix the same error during the integration or verification phase. Embedded systems for the avionics industry are developed following particularly rigorous restrictions due to strict safety and availability constraints that need to be satisfied during air or ground operations. DO-178B (RTCA, 1992) is a recognized standard for developmen</context>
<context position="7860" citStr="Wilson et al., 1997" startWordPosition="1211" endWordPosition="1214">riting requirements. One example in this group is Fuchs (Fuchs et al., 1998) who introduces ACE (Attempto Controlled English), a restricted subset of English with a restricted grammar and domain specific vocabulary. Requirements can be written in natural language with enough expressive power. They are later translated into first order predicate logic to be processed formally by a computer program. The last group of efforts consists of analytic techniques that perform automated analysis of requirements once they have been produced. The following are two relevant projects in this group. Wilson (Wilson et al., 1997) developed a tool named ARM that performs automated analysis of a requirements document. The tool focuses on lexical analysis to detect specific keywords such as vague adverbs and 101 vague adjectives that are not desired. Different from our work, ARM also checks that the document itself complies with a specific format. Then, Lami (Lami et al., 2004) described a systematic method for automated analysis of requirements detecting deficiencies such as ambiguity, inconsistencies, and incompleteness. A tool named QuARS implements the suggested methodology and appears to be a good contribution in th</context>
</contexts>
<marker>Wilson, Rosenberg, Hyatt, 1997</marker>
<rawString>Wilson, W. and Rosenberg, L. and Hyatt, L. 1997. Automated Analysis of Requirement Specifications. Nineteenth International Conference on Software Engineering (ICSE-97), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Galin</author>
</authors>
<title>Software Quality Assurance From theory to implementation.</title>
<date>2004</date>
<publisher>Pearson Education Ltd.</publisher>
<contexts>
<context position="1855" citStr="Galin, 2004" startWordPosition="260" endWordPosition="261">requirements written in English with the goal of identifying whether or not they satisfy the desired properties. Preliminary results are presented showing how the tool effectively identifies critical issues that are normally ignored by human reviewers. 1 Introduction Software requirements play a critical role in the software life cycle. It has been observed that poorly written software requirements often lead to weak and unpredictable software applications (Wilson et al., 1997). Besides, the cost of fixing errors increases exponentially throughout the different phases of software development (Galin, 2004; Leffingwell and Widrig, 2003). In other words, it is less expensive to fix an error in the software requirements phase than it is to fix the same error during the integration or verification phase. Embedded systems for the avionics industry are developed following particularly rigorous restrictions due to strict safety and availability constraints that need to be satisfied during air or ground operations. DO-178B (RTCA, 1992) is a recognized standard for development of safety critical embedded systems. It is widely used by software certification authorities such as FAA (Federal Aviation Asso</context>
<context position="9097" citStr="Galin, 2004" startWordPosition="1410" endWordPosition="1411">Framework This section provides a basic explanation of some concepts that are commonly used in the field of software verification. Emphasis will be made on concepts related to the software engineering field in an attempt to set the grounds for the investigation. Other linguistic related concepts will be mentioned along the paper assuming the reader has basic understanding of them. 3.1 Software Life Cycle A Software Life Cycle Model or Software Development Model consists of a group of concepts and well coordinated methodologies that guide the software development process from beginning to end (Galin, 2004). The classic software life cycle model (a.k.a. the waterfall model) consists of linear sequence of activities or phases that take place during a software development effort. In the Requirements elicitation phase, a detailed description of what the software shall do is produced. Although there are various methods, a natural language description in the form of a list of statements is widely used to produce requirements. A software requirement is a condition or characteristic that a system must possess to satisfy a contract, a standard, a formal specification or other applicable regulation (IEEE</context>
</contexts>
<marker>Galin, 2004</marker>
<rawString>Galin, D. 2004. Software Quality Assurance From theory to implementation. Pearson Education Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Leffingwell</author>
<author>D Widrig</author>
</authors>
<date>2003</date>
<booktitle>Managing Software Requirements, 2nd Ed.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="1886" citStr="Leffingwell and Widrig, 2003" startWordPosition="262" endWordPosition="265">written in English with the goal of identifying whether or not they satisfy the desired properties. Preliminary results are presented showing how the tool effectively identifies critical issues that are normally ignored by human reviewers. 1 Introduction Software requirements play a critical role in the software life cycle. It has been observed that poorly written software requirements often lead to weak and unpredictable software applications (Wilson et al., 1997). Besides, the cost of fixing errors increases exponentially throughout the different phases of software development (Galin, 2004; Leffingwell and Widrig, 2003). In other words, it is less expensive to fix an error in the software requirements phase than it is to fix the same error during the integration or verification phase. Embedded systems for the avionics industry are developed following particularly rigorous restrictions due to strict safety and availability constraints that need to be satisfied during air or ground operations. DO-178B (RTCA, 1992) is a recognized standard for development of safety critical embedded systems. It is widely used by software certification authorities such as FAA (Federal Aviation Association), and it establishes so</context>
</contexts>
<marker>Leffingwell, Widrig, 2003</marker>
<rawString>Leffingwell, D. and Widrig, D. 2003. Managing Software Requirements, 2nd Ed. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RTCAEUROCAE</author>
</authors>
<title>DO-178 Software Considerations for Airborne Systems and Equipment Certification.</title>
<date>1992</date>
<publisher>RTCA, Inc.,</publisher>
<location>Washington, DC.</location>
<marker>RTCAEUROCAE, 1992</marker>
<rawString>RTCA/EUROCAE. 1992. DO-178 Software Considerations for Airborne Systems and Equipment Certification. RTCA, Inc., Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Drazen</author>
<author>P Berander</author>
<author>L Damm</author>
<author>J Eriksson</author>
<author>T Gorschek</author>
<author>K Henningsson</author>
<author>P Jonsson</author>
<author>S Kagstrom</author>
<author>F Martensson</author>
<author>K Ronkko</author>
<author>P Tomaszewski</author>
</authors>
<title>Software quality attributes and trade-offs, Software Quality Models and Philosophies.</title>
<date>2005</date>
<institution>Blekinge Institute of Technology.</institution>
<marker>Drazen, Berander, Damm, Eriksson, Gorschek, Henningsson, Jonsson, Kagstrom, Martensson, Ronkko, Tomaszewski, 2005</marker>
<rawString>Drazen, M. and Berander, P. and Damm, L. and Eriksson, J. and Gorschek, T. and Henningsson, K. and Jonsson, P. and Kagstrom, S. and Martensson, F. and Ronkko, K. and Tomaszewski, P.. 2005. Software quality attributes and trade-offs, Software Quality Models and Philosophies. Blekinge Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IEEE Standards Board</author>
</authors>
<date>1990</date>
<journal>IEEE Standard Glossary of Software Engineering Terminology, Std</journal>
<pages>610--12</pages>
<marker>Board, 1990</marker>
<rawString>IEEE Standards Board. 1990. IEEE Standard Glossary of Software Engineering Terminology, Std 610.12-1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Berry</author>
<author>E Kamsties</author>
<author>M Krieger</author>
</authors>
<title>From Contract Drafting to Software Specification: Linguistic Sources of Ambiguity.</title>
<date>2003</date>
<marker>Berry, Kamsties, Krieger, 2003</marker>
<rawString>Berry, D. and Kamsties, E. and Krieger, M.. 2003. From Contract Drafting to Software Specification: Linguistic Sources of Ambiguity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An Online Lexical Database.</title>
<date>1993</date>
<institution>Cognitive Science Laboratory, Princeton University.</institution>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>Miller, G. and Beckwith, R. and Fellbaum, C. and Gross, D. and Miller, K.. 1993. Introduction to WordNet: An Online Lexical Database. Cognitive Science Laboratory, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<institution>Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="23104" citStr="Kipper, 2005" startWordPosition="3827" endWordPosition="3828">ry of the process that was followed to select the criteria to evaluate requirements. 1. A list of elements was first suggested by the author based on relevant literature and his own experience in software verification for embedded systems. The list contained 19 elements (10 for accuracy, 5 for ambiguity and 4 for verifiability). 2. Five elements were filtered out as they were not candidates to be automated. Feasible candidates were those that could be automated using techniques such as parsing, tagging, regular expressions, and specialized dictionaries like WordNet (Miller, 1993) and VerbNet (Kipper, 2005). The list ended with 6 elements in accuracy, 4 in ambiguity and 4 in verifiability. 3. The author suggested an initial value or score for each element in the list. 4. Both the element selection and the value distribution were independently validated by a group of three professionals with demonstrable experience in software verification4. 5. A numerical model was prepared based on the proposed approach described in section 4.1. This is already a contribution since the evaluation of the requirements could be done manually in case no tool had been created. 4Although these individuals are not lan</context>
<context position="27159" citStr="Kipper, 2005" startWordPosition="4454" endWordPosition="4455">6). In this case, the CLAIR group at University of Michigan made available a Perl wrapper for the Charniak parser(CLAIR, 2009). Studying the syntax tree produced by the parser, it was possible to identify accuracy issues such as requirements without explicit condition statements (or condition blocks). Also, studying the output of the parser along with lexical analysis of the requirement reveals cases of ambiguity when logical conditions are not stated clearly. Dictionaries: two great resources were also incorporated in this research to support our analysis: WordNet (Miller, 1993) and VerbNet (Kipper, 2005). Both of this tools can also be accessed from Perl via wrappers and provide useful information about words and verbs that were used to ensure some conditions were valid while we perform the analysis of the requirements. Dictionaries allow identification of human specific verbs and ambiguous verbs. In this case, the parser makes it possible to capture the main verb for a requirement, and further queries into dictionaries complete the task. VerbNet provides a mechanism to classify requirements according to their degree of ambiguity. This mechanism may be too stringent for flagging ambiguous ver</context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>Kipper, K.. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lami</author>
<author>S Gnesi</author>
<author>F Fabbrini</author>
<author>M Fusani</author>
<author>G Trentanni</author>
</authors>
<title>An Automatic Tool for the Analysis of Natural Language Requirements.</title>
<date>1997</date>
<booktitle>C.N.R. Information Science and Technology Institute,</booktitle>
<location>Pisa</location>
<marker>Lami, Gnesi, Fabbrini, Fusani, Trentanni, 1997</marker>
<rawString>Lami, G. and Gnesi, S. and Fabbrini, F. and Fusani, M. and Trentanni, G.. 1997. An Automatic Tool for the Analysis of Natural Language Requirements. C.N.R. Information Science and Technology Institute, Pisa Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and Self-Training for Parser Adaptation.</title>
<date>2006</date>
<booktitle>21st International Conference on Computational Linguistics.</booktitle>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>McClosky, D. and Charniak, E. and Johnson, M.. 2006. Reranking and Self-Training for Parser Adaptation. 21st International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CLAIR official website</author>
</authors>
<date>2009</date>
<note>URL http: //belobog.si.umich.edu/clair/clair/ downloads.html. Visited on</note>
<marker>website, 2009</marker>
<rawString>CLAIR official website. 2009. URL http: //belobog.si.umich.edu/clair/clair/ downloads.html. Visited on March 12, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuchs</author>
<author>U Schwertel</author>
<author>D Schwitter</author>
</authors>
<title>Attempto Controlled English Not Just Another Logic Specification Language.</title>
<date>1998</date>
<booktitle>Eighth International Workshop on Logic-based Program Synthesis and Transformation LOPSTR’98,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="7316" citStr="Fuchs et al., 1998" startWordPosition="1123" endWordPosition="1126">time is significantly reduced. 2 Related Work Significant work has been done in the area of software requirements analysis. Lami (Lami et al., 2004) classifies these efforts in three groups. A first group consists of preventive techniques that need to be applied during the process of writing requirements. Those techniques normally trigger checklists that are enforced by a person with no support from tools, see for instance (Firesmith, 2003). Another group consists of restrictive techniques that limit the degree of language freedom when writing requirements. One example in this group is Fuchs (Fuchs et al., 1998) who introduces ACE (Attempto Controlled English), a restricted subset of English with a restricted grammar and domain specific vocabulary. Requirements can be written in natural language with enough expressive power. They are later translated into first order predicate logic to be processed formally by a computer program. The last group of efforts consists of analytic techniques that perform automated analysis of requirements once they have been produced. The following are two relevant projects in this group. Wilson (Wilson et al., 1997) developed a tool named ARM that performs automated anal</context>
</contexts>
<marker>Fuchs, Schwertel, Schwitter, 1998</marker>
<rawString>Fuchs, N. and Schwertel, U. and Schwitter, D.. 1998. Attempto Controlled English Not Just Another Logic Specification Language. Eighth International Workshop on Logic-based Program Synthesis and Transformation LOPSTR’98, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Firesmith</author>
</authors>
<title>Specifying Good Requirements.</title>
<date>2003</date>
<journal>Journal of Object Technology, ETH Zurich.</journal>
<contexts>
<context position="7141" citStr="Firesmith, 2003" startWordPosition="1097" endWordPosition="1098">ependency on highly qualified personnel. • Results are reproducible for any given set of inputs, reducing inconsistencies while adding reliability to the results. • Review time is significantly reduced. 2 Related Work Significant work has been done in the area of software requirements analysis. Lami (Lami et al., 2004) classifies these efforts in three groups. A first group consists of preventive techniques that need to be applied during the process of writing requirements. Those techniques normally trigger checklists that are enforced by a person with no support from tools, see for instance (Firesmith, 2003). Another group consists of restrictive techniques that limit the degree of language freedom when writing requirements. One example in this group is Fuchs (Fuchs et al., 1998) who introduces ACE (Attempto Controlled English), a restricted subset of English with a restricted grammar and domain specific vocabulary. Requirements can be written in natural language with enough expressive power. They are later translated into first order predicate logic to be processed formally by a computer program. The last group of efforts consists of analytic techniques that perform automated analysis of require</context>
</contexts>
<marker>Firesmith, 2003</marker>
<rawString>Firesmith, D.. 2003. Specifying Good Requirements. Journal of Object Technology, ETH Zurich.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>