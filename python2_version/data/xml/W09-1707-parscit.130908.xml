<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004851">
<title confidence="0.997683">
Using DEDICOM for
Completely Unsupervised Part-of-Speech Tagging
</title>
<author confidence="0.797422">
Peter A. Chew, Brett W. Bader
</author>
<affiliation confidence="0.643758">
Sandia National Laboratories
</affiliation>
<address confidence="0.59432">
P. O. Box 5800, MS 1012
Albuquerque, NM 87185-1012, USA
</address>
<email confidence="0.996963">
{pchew,bwbader}@sandia.gov
</email>
<author confidence="0.990753">
Alla Rozovskaya
</author>
<affiliation confidence="0.846277666666667">
Department of Computer Science
University of Illinois
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.999357">
rozovska@illinois.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990359375">
A standard and widespread approach to
part-of-speech tagging is based on Hidden
Markov Models (HMMs). An alternative
approach, pioneered by Schütze (1993),
induces parts of speech from scratch using
singular value decomposition (SVD). We
introduce DEDICOM as an alternative to
SVD for part-of-speech induction.
DEDICOM retains the advantages of
SVD in that it is completely unsupervised:
no prior knowledge is required to induce
either the tagset or the associations of
types with tags. However, unlike SVD, it
is also fully compatible with the HMM
framework, in that it can be used to esti-
mate emission- and transition-probability
matrices which can then be used as the
input for an HMM. We apply the
DEDICOM method to the CONLL corpus
(CONLL 2000) and compare the output of
DEDICOM to the part-of-speech tags
given in the corpus, and find that the cor-
relation (almost 0.5) is quite high. Using
DEDICOM, we also estimate part-of-
speech ambiguity for each type, and find
that these estimates correlate highly with
part-of-speech ambiguity as measured in
the original corpus (around 0.88). Finally,
we show how the output of DEDICOM
can be evaluated and compared against
the more familiar output of supervised
HMM-based tagging.
</bodyText>
<page confidence="0.990108">
54
</page>
<sectionHeader confidence="0.999322" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975972222222">
Traditionally, part-of-speech tagging has been ap-
proached either in a rule-based fashion, or stochas-
tically. Harris (1962) was among the first to
develop algorithms of the former type. The rule-
based approach relies on two elements: a dictio-
nary to assign possible parts of speech to each
word, and a list of hand-written rules – which must
be painstakingly developed for each new language
or domain – to disambiguate tokens in context.
Stochastic taggers, on the other hand, avoid the
need for hand-written rules by tabulating probabili-
ties of types and part-of-speech tags (which must
be gathered from a tagged training corpus), and
applying a special case of Bayesian inference
(usually, Hidden Markov Models [HMMs]) to dis-
ambiguate tokens in context. The latter approach
was pioneered by Stolz et al. (1965) and Bahl and
Mercer (1976), and became widely known through
the work of e.g. Church (1988) and DeRose
(1988).
A third and more recent approach, known as
‘distributional tagging’ and exemplified by
Schütze (1993, 1995) and Biemann (2006), aims to
eliminate the need for both hand-written rules and
a tagged training corpus, since the latter may not
be available for every language or domain. Distri-
butional tagging is fully-unsupervised, unlike the
two traditional approaches described above.
Schütze suggests analyzing the distributional pat-
terns of words by forming a term adjacency matrix,
then subjecting that matrix to Singular Value De-
composition (SVD) to reveal latent dimensions. He
shows that in the reduced-dimensional space im-
plied by SVD, tokens do indeed cluster intuitively
by part-of-speech; and that if context is taken into
account, something akin to part-of-speech tagging
</bodyText>
<note confidence="0.9895645">
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 54–62,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999857416666667">
can be achieved. Whereas the performance of sto-
chastic taggers is generally sub-optimal when the
domain of the training data differs from that of the
test data, distributional tagging sidesteps this prob-
lem, since each corpus can be considered in its
own right. Schütze (1995) notes two general draw-
backs of distributional tagging methods: the per-
formance is relatively modest compared to that of
supervised methods; and languages with rich mor-
phology may pose a challenge.1
In this paper, we present an alternative unsuper-
vised approach to distributional tagging. Instead of
SVD, we use a dimensionality reduction technique
known as DEDICOM, which has various advan-
tages over the SVD-based approach. Principal
among these is that, even though no pre-tagged
corpus is required, DEDICOM can easily be used
as input to a HMM-based approach (and the two
share linear-algebraic similarities, as we will make
clear in section 4). Although our empirical results,
like those of Schütze (1995), are perhaps still rela-
tively modest, the fact that a clearer connection
exists between DEDICOM and HMMs than be-
tween SVD and HMMs gives us good reason to
believe that with further refinements, DEDICOM
may be able to give us ‘the best of both worlds’ in
many respects: the benefits of avoiding the need
for a pre-tagged corpus, with empirical results ap-
proaching those of HMM-based tagging.
In the following sections, we introduce
DEDICOM, describe its applicability to the part-
of-speech tagging problem, and outline its connec-
tions to the standard HMM-based approach to tag-
ging. We evaluate the use of DEDICOM on the
CONLL 2000 shared task data, discuss the results
and suggest avenues for improvement.
</bodyText>
<sectionHeader confidence="0.996906" genericHeader="introduction">
2 DEDICOM
</sectionHeader>
<bodyText confidence="0.92789225">
DEDICOM, which stands for ‘DEcomposition into
DIrectional COMponents’, is a linear-algebraic
decomposition method attributable to Harshman
(1978) which has been used to analyze matrices of
</bodyText>
<footnote confidence="0.932089333333333">
1 We note the latter is also true for languages in which word
order is relatively free – usually the same languages as those
with rich morphology. While English word order is signifi-
cantly constrained by part-of-speech categorizations, this is
not as true of, say, Russian. Thus, an adjacency matrix formed
from a Russian corpus is likely to be less informative about
part-of-speech classifications as one formed from an English
corpus. Quite possibly, this is as much of a limitation for
DEDICOM as it is for SVD.
</footnote>
<bodyText confidence="0.999197621621621">
asymmetrical directional relationships between
objects or persons. Early on, the technique was
applied by Harshman et al. (1982) to the analysis
of two types of marketing data: ‘free associations’
– how often one phrase (describing hair shampoo)
evokes another in the minds of survey respondents,
and ‘car switching data’ – how often people switch
from one to another of 16 car types. Both datasets
are asymmetric and directional: in the first dataset,
for example, the phrase ‘body’ (referring to sham-
poo) evoked the phrase ‘fullness’ twice as often in
the minds of respondents as ‘fullness’ evoked
‘body’. Likewise, the data from Harshman et al.
(1982) show that in the given period, 3,820 people
switched from ‘midsize import’ cars to ‘midsize
domestic’ cars, but only 2,140 switches were made
in the reverse direction. Another characteristic of
these ‘asymmetric directional’ datasets is that they
can be represented in square matrices. For exam-
ple, the raw car switching data can be represented
in a 16 x 16 matrix, since there are 16 car types.
The objective of DEDICOM, which can be
compared to that of SVD, is to factorize the raw
data matrices into a lower-dimensional space iden-
tifying underlying, idealized directional patterns in
the data. For example, while there are 16 car types
in the raw car switching data, Harshman shows
that under a 4-dimensional DEDICOM analysis,
these can be ‘boiled down’ to the basic types ‘plain
large-midsize’, ‘specialty’, ‘fancy large’, and
‘small’ – and that patterns of switching among
these more basic types can then be identified.
If X represents the original n x n matrix of
asymmetric relationships, and a general entry xij in
X represents the strength of the directed relation-
ship of object i to object j, then the single-domain
DEDICOM model2 can be written as follows:
</bodyText>
<equation confidence="0.980149">
X=ARAT+E (1)
</equation>
<bodyText confidence="0.9999558">
where A denotes an n x q matrix of weights of the
n observed objects in q dimensions (where q &lt; n),
and R is a dense q x q asymmetric matrix express-
ing the directional relationships between the q di-
mensions or basic types. AT is simply the transpose
</bodyText>
<footnote confidence="0.997429333333333">
2 There is a dual-domain DEDICOM model, which is also
described in Harshman (1978). The dual-domain DEDICOM
model is not relevant to our discussion, and thus it will not be
mentioned further. References in this paper to ‘DEDICOM’
are to be understood as references in shorthand to ‘single-
domain DEDICOM’.
</footnote>
<page confidence="0.999291">
55
</page>
<bodyText confidence="0.9941715">
of A, and E is a matrix of error terms. Our objec-
tive is to minimize E, so we can also write:
</bodyText>
<equation confidence="0.997698">
X ~ ARAT (2)
</equation>
<bodyText confidence="0.999967285714286">
As noted by Harshman (1978: 209), the fact that
A appears on both the left and right of R means
that the data is described ‘in terms of asymmetric
relations among a single set of things’ – in other
words, when objects are on the receiving end of the
directional relationships, they are still of the same
type as those on the initiating end.
One difference between DEDICOM and SVD is
that there is no unique solution: either A or R can
be scaled or rotated without changing the goodness
of fit, so long as the inverse operation is applied to
the other. For example, if we let Â = AD, where D
is any diagonal scaling matrix (or, more generally,
any nonsingular matrix), then we can write
</bodyText>
<equation confidence="0.985975">
X ~ ARAT = ÂD-1RD-1ÂT (3)
since ÂT = (AD) T = DAT
</equation>
<bodyText confidence="0.998373875">
(In our application, we constrain A and R to be
nonnegative as noted below.)
To our knowledge, there have been no applica-
tions of DEDICOM to date in computational lin-
guistics. This is in contrast to SVD, which has
been extensively used for text analysis (for appli-
cations other than unsupervised part-of-speech
tagging, see Baeza-Yates and Ribeiro-Neto 1999).
</bodyText>
<sectionHeader confidence="0.83104" genericHeader="method">
3 Applicability of DEDICOM to part-of-
</sectionHeader>
<subsectionHeader confidence="0.560618">
speech tagging
</subsectionHeader>
<bodyText confidence="0.999973033333333">
Schütze’s (1993) key insight is that – at least in
English – adjacencies between types are a good
guide to their grammatical functions. That insight
can be leveraged by applying either SVD or
DEDICOM to a type-by-type adjacency matrix.
With DEDICOM, however, we add the constraint
(already stated) that the types are a ‘single set of
things’: whether a type ‘precedes’ or ‘follows’ –
i.e., whether it is in a row or a column of the ma-
trix – does not affect its grammatical function. This
constraint is as it should be, and, to our knowledge,
sets DEDICOM apart from all previous unsuper-
vised approaches including those of Schütze (1993,
1995) and Biemann (2006).
Given any corpus containing n types and k to-
kens, we can let X be an n × n token-adjacency
matrix. Let each entry xij in X denote the number
of times in the corpus that type i immediately pre-
cedes type j. X is thus a matrix of bigram frequen-
cies. It follows that the sum of the elements of X
equals k – 1 (because the first token in the corpus
is preceded by nothing, and the last token is fol-
lowed by nothing). Any given row sum of X (the
type frequency corresponding to the particular
row) will equal the corresponding column sum,
except if the type happens to occur in the first or
last position in the corpus. X will be asymmetric,
since the frequency of bigram ij is clearly not the
same as that of bigram ji for all i and j.
It can be seen, therefore, that our X represents
asymmetric directional data, very similar to the
data analyzed in Harshman (1978) and Harshman
et al. (1982). If we fit the DEDICOM model to our
X matrix, then we obtain an A matrix which
represents types by latent classes, and an R matrix
which represents directional relationships between
latent classes. We can think of the latent classes as
induced parts of speech.
With SVD, we believe that the orthogonality of
the reduced-dimensional features militates against
any attempt to correlate these features with parts of
speech. From a linguistic point of view, there is no
reason to believe that parts of speech are orthogon-
al to one another in any sense. For example, nouns
and adjectives (traditionally classified together as
‘nominals’) seem to share more in common with
one another than nouns and verbs. With
DEDICOM, this is not an issue, because the col-
umns of A are not required to be mutually ortho-
gonal to one another, unlike the left and right
singular vectors from SVD.
Thus, the A matrix from DEDICOM shows how
strongly associated each type is with the different
induced parts of speech; we would expect types
which are ambiguous (such as ‘claims’, which can
be either a noun or a verb) to have high loadings
on more than one column in A. Again, if the
classes correlate with parts of speech, the R matrix
will show the latent patterns of adjacency between
different parts of speech.
</bodyText>
<sectionHeader confidence="0.880528" genericHeader="method">
4 Connections between DEDICOM and
HMM-based tagging
</sectionHeader>
<bodyText confidence="0.999788333333333">
For any HMM, two components are necessary: a
set of emission probabilities and a set of transition
probabilities. Applying this framework to part-of-
</bodyText>
<page confidence="0.980615">
56
</page>
<bodyText confidence="0.999971875">
speech tagging, the tags are conceived of as the
hidden layer of the HMM and the tokens (each of
which is associated with a type) as the visible
layer. The emission probabilities are then the prob-
abilities of types given the tags, and the transition
probabilities are the probabilities of the tags given
the preceding tags. If these probabilities are
known, then there are algorithms (such as the Vi-
terbi algorithm) to determine the most likely se-
quence of tags given the visible sequence of types.
In the case of supervised learning, we obtain the
emission and transition probabilities by observing
actual frequencies in a tagged corpus. Suppose our
corpus, as previously discussed, consists of n types
and k tokens. Since we are dealing with supervised
learning, the number of the tags in the tagset is also
known: we denote this number q. Now, the ob-
served frequencies can be represented, respective-
ly, as n x q and q x q matrices: we denote these A*
and R*. Each entry aij in A* denotes the number of
times type i is associated with tag j, and each entry
rij in R* denotes the number of times tag j imme-
diately follows tag i. Moreover, we know some
other properties of A* and R*:
</bodyText>
<listItem confidence="0.992693888888889">
• the respective sums of the elements of A* and
R* are equal to k – 1;
q
• each row sum of A* (E aix ) corresponds to
x=1
the frequency in the corpus of type i;
• each column sum of A*, as well as the corres-
ponding row and column sums of R*, are the
frequencies of the given tags in the corpus (for
</listItem>
<equation confidence="0.96692475">
q q q
all j, axj =E r =�
xj
x=1 x=1 x=1
</equation>
<bodyText confidence="0.9997056875">
If A* and R* contain frequencies, however, we
must perform a matrix operation to obtain transi-
tion and emission probabilities for use in an
HMM-based tagger. In effect, A* must be made
column-stochastic, and R* must be made row-
stochastic. Since the column sums of A* equal the
respective row sums of R*, this can be achieved by
post-multiplying both A* and R* by DA, where DA
is a diagonal scaling matrix containing the inverses
of the column sums of A (or equivalently, the row
sums of R). Then the matrix of emission probabili-
ties is given by A*DA, and the matrix of transition
probabilities by R*DA.
We can now make the connection to DEDICOM
explicit. Let A = A*DA and R = R*, then we can
rewrite (2) as follows:
</bodyText>
<equation confidence="0.9984">
X,, ARAT = (A*DA) R* (A*DA)T (4)
X - A*DA R*DA A*T (5)
</equation>
<bodyText confidence="0.99995">
In other words, for any corpus we may compute
a probabilistic representation of the type adjacency
matrix X (which will contain expected frequencies
comparable to the actual frequencies) by multiply-
ing the emission probability matrix A*DA, the
transition probability matrix R*DA, and the type-
by-tag frequency matrix A*. (Presumably, the
closer the approximation, the better the tagging in
the training set actually factorizes the true direc-
tional relationships.)
Conversely, for fully unsupervised tagging, we
can fit the DEDICOM model to the type adjacency
matrix X. The resulting A matrix contains esti-
mates of what the tags should be (if a tagged train-
ing corpus is unavailable), as well as the emission
probability of each type given each tag, and the
resulting R matrix is the corresponding transition
probability matrix given those tags. In this case, a
column-stochastic A can be used directly as the
emission probability matrix, and we simply make
R* row-stochastic to obtain the matrix of transition
probabilities. The only difference then between the
output of the fully-unsupervised DEDICOM/HMM
tagger and that of a supervised HMM tagger is that
in the first case, the ‘tags’ are numeric indices
representing the corresponding column of A, and
in the second case, they are the members of the
tagset used in the training data.
The fact that emission and transition probabili-
ties (or at least something very like them) are a
natural by-product of DEDICOM sets DEDICOM
apart from Schütze’s SVD-based approach, and is
for us a significant reason which recommends the
use of DEDICOM.
</bodyText>
<sectionHeader confidence="0.999" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.998353571428571">
For all evaluation described here, we used the
CONLL 2000 shared task data (CONLL 2000).
This English-language newswire corpus consists of
19,440 types and 259,104 tokens (including punc-
tuation marks as separate types/tokens). Each to-
ken is associated with a part-of-speech tag and a
chunk tag, although we did not use the chunk tags
</bodyText>
<equation confidence="0.958729">
r ).
jx
</equation>
<page confidence="0.982573">
57
</page>
<bodyText confidence="0.998928755555555">
in the work described here. The tags are from a 44-
item tagset. The CONLL 2000 tags against which
we measure our own results are in fact assigned by
the Brill tagger (Brill 1992), and while these may
not correlate perfectly with those that would have
been assigned by a human linguist, we believe that
the correlation is likely to be good enough to allow
for an informative evaluation of our method.
Before discussing the evaluation of unsuper-
vised DEDICOM, let us briefly reconsider the si-
milarities of DEDICOM to the supervised HMM
model in the light of actual data in the CONLL
corpus. We stated in (5) that X - A*DAR*DAA*T.
For the CONLL 2000 tagged data, A* is a 19,440
× 44 matrix and R* is a 44 × 44 matrix. Using
A*DA and R*DA as emission- and transition-
probability matrices within a standard HMM
(where the entire CONLL 2000 corpus is treated as
both training and test data), we obtained a tagging
accuracy of 95.6%. By multiplying
A*DAR*DAA*T, we expect to obtain a matrix ap-
proximating X, the table of bigram frequencies.
This is indeed what we found: it will be apparent
from Table 1 that the top 10 expected bigram fre-
quencies based on this matrix multiplication are
generally quite close to actual frequencies. Moreo-
ver, the sum of the elements in A*DAR*DAA*T is
equal to the sum of the elements in X, and if we let
E be the matrix of error terms (X -
A*DAR*DAA*T), then we find that ||E ||(the Frobe-
nius norm of E) is 38.764% of ||X ||- in other
words, A*DAR*DAA*T accounts for just over 60%
of the data in X.
Type 1 Type 2 Actual Expected
frequency frequency
of the 1,421.000 1,202.606
in the 1,213.000 875.822
for the 553.000 457.067
to the 445.000 415.524
on the 439.000 271.528
the company 383.000 105.794
a share 371.000 32.447
that the 315.000 258.679
and the 302.000 296.737
to be 285.000 499.315
</bodyText>
<tableCaption confidence="0.995809">
Table 1. Actual versus expected frequencies for 10 most
common bigrams in CONLL 2000 corpus
</tableCaption>
<bodyText confidence="0.95698882">
Having confirmed that there exists an A
(=A*DA) and R (=R*) which both satisfies the
DEDICOM model and can be used directly within
a HMM-based tagger to achieve satisfactory re-
sults, we now consider whether A and R can be
estimated if no tagged training set is available.
We start, therefore, from X, the square 19,440 ×
19,440 (sparse) matrix of raw bigram frequencies
from the CONLL 2000 data. Using Matlab and the
Tensor Toolbox (Bader and Kolda 2006, 2007), we
computed the best rank-44 non-negative
DEDICOM3 decomposition of this matrix using
the 2-way version of the ASALSAN algorithm
presented in Bader et al. (2007), which is based on
iteratively improving random initial guesses for A
and R. As with SVD, the rank of the decomposi-
tion can be selected by the user; we chose 44 since
that was known to be the number of items in the
CONLL 2000 tagset, but a lower number could be
selected for a coarser-grained part-of-speech anal-
ysis. Ultimately, perhaps the best way to determine
the optimal rank would be to evaluate different
options within a larger end-to-end system, for ex-
ample an information retrieval system; this, how-
ever, was beyond our scope in this study.
As already mentioned, there are indeterminacies
of rotation and scale in DEDICOM. As Harshman
et al. (1982: 211) point out, ‘when the columns of
A are standardized... the R matrix can then be in-
terpreted as expressing relationships among the
dimensions in the same units as the original data.
That is, the R matrix can be interpreted as a ma-
trix of the same kind as the original data matrix X,
but describing the relations among the latent as-
pects of the phrases, rather than the phrases them-
selves’. Thus, if DEDICOM is constrained so that
A is column-stochastic (which is required in any
case of the matrix of emission probabilities), then
the sum of the elements in R should approximate
the sum of the elements in X. R is therefore com-
parable to R* (with some provisos which shall be
enumerated below), and to obtain the row-
stochastic transition-probability matrix, we simply
multiply R by a diagonal matrix DR whose ele-
ments are the inverses of R’s row sums.
3 Non-negative DEDICOM imposes the constraint not present
in Harshman (1978, 1982) that all entries in A and R must be
non-negative. This constraint is appropriate in the present
case, since the entries in A* and R* (and of course the proba-
bilities in A*D and R*D) are by definition non-negative.
</bodyText>
<page confidence="0.999149">
58
</page>
<tableCaption confidence="0.998868">
Table 2. Partial confusion matrix of gold-standard tags against DEDICOM-induced tags for CONLL 2000 dataset
</tableCaption>
<bodyText confidence="0.9999875">
With A as an emission-probability matrix and
RDR as a transition-probability matrix, we now
have all that is needed for an HMM-based tagger
to estimate the most likely sequence of ‘tags’ given
the corpus. However, since the ‘tags’ here are nu-
merical indices, as mentioned, to evaluate the out-
put we must look at the correlation between these
‘tags’ and the gold-standard tags given in the
CONLL 2000 data. One way this can be done is by
presenting a 44 × 44 confusion matrix (of gold-
standard tags against induced tags), and then mea-
suring the correlation coefficient (Pearson’s R)
between that matrix and the ‘idealized’ confusion
matrix in which each induced tag corresponds to
one and only one ‘gold standard’ tag. Using A and
RDR as the input to a HMM-based tagger, we
tagged the CONLL 2000 dataset with induced tags
and obtained the confusion matrix shown in Table
2 (owing to space constraints, only the first 20 col-
umns are shown). The correlation between this
matrix and the equivalent diagonalized ‘ideal’ ma-
trix is in fact 0.4942, which is significantly higher
than could have occurred by chance.
It should be noted that a lack of correlation be-
tween the induced tags and the gold standard tags
can be attributed to at least two independent fac-
tors. The first, of course, is any inability of the
DEDICOM model to fit the particular problem and
data. Clearly, this is undesirable. The other factor
to be borne in mind, which works to DEDICOM’s
favor, is that the DEDICOM model could yield an
A and R which factorize the data more optimally
than the A*D and R* implied by the gold-standard
tags. There are three methods we can use to try and
tease apart these competing explanations of the
results, two quantitative and the other subjective.
Quantitatively, we can compare the respective er-
ror matrices E. We have already mentioned that
</bodyText>
<equation confidence="0.998282">
 ||X— A*DAR*DAA* T  ||- 0.38764 (6)
 ||X ARAT ||
� � 0.24078 (7)
X ||
||
 ||X ||
</equation>
<bodyText confidence="0.9972995">
Similarly, using the A and R from DEDICOM we
can compute
</bodyText>
<page confidence="0.997372">
59
</page>
<bodyText confidence="0.999971246753247">
The fact that the error is lower in the second case
implies that DEDICOM allows us to find a part-of-
speech ‘factorization’ of the data which fits better
even than the gold standard, although again there
are some caveats to this; we will return to these in
the discussion.
Another way to evaluate the output of
DEDICOM is by comparing the number of part-of-
speech tags for a type in the gold standard to the
number of classes in the A matrix with which the
type is strongly associated. We test this by measur-
ing the Pearson correlation between the two va-
riables. First, we compute the average number of
part-of-speech tags per type using the gold stan-
dard. We refer to this value as ambiguity coeffi-
cient; for the CONLL dataset, this is 1.05. Because
A is dense, if we count all non-zero columns for a
type in the A matrix as possible classes, we obtain
a much higher ambiguity coefficient. We therefore
set a threshold and consider only those columns
whose values exceed a certain threshold. The thre-
shold is selected so that the ambiguity coefficient
of the A matrix is the same as that of the gold stan-
dard. For a given type, every column with a value
exceeding the threshold is counted as a possible
class for that type. We then compute the Pearson
correlation coefficient between the number of
classes for a type in the A matrix and the number
of part-of speech tags for that type in the CONLL
dataset as provided by the Brill tagger. We ob-
tained a correlation coefficient of 0.88, which
shows that there is indeed a high correlation be-
tween the induced tags and the gold standard tags
obtained with DEDICOM.
Finally, we can evaluate the output subjectively
by looking at the content of the A matrix. For each
‘tag’ (column) in A, the ‘types’ (rows) can be
listed in decreasing order of their weighting in A.
This gives us an idea of which types are most cha-
racteristic of which tags, and whether the grouping
into tags makes any intuitive sense. These results
(for selected tags only, owing to limitations of
space) are given in Table 3.
Many groupings in Table 3 do make sense: for
example, the fourth tag is clearly associated with
verbs, while the two types with significant weight-
ings for tag 2 are both determiners. By referring
back to Table 2, we can see that many tokens in the
CONLL 2000 dataset tagged as verbs are indeed
tagged by the DEDICOM tagger as ‘tag 4’, while
many determiners are tagged as ‘tag 3’. To under-
stand where a lack of correlation may arise, how-
ever, it is informative to look at apparent
anomalies in the A matrix. For example, it can be
seen from Table 3 that ‘new’, an adjective, is
grouped in the third tag with ‘a’ and ‘the’ (and
ranking above ‘an’). Although not in agreement
with the CONLL 2000 ‘gold standard’ tagging, the
idea that determiners are a type of adjective is in
fact in accordance with traditional English gram-
mar. Here, the grouping of ‘new’, ‘a’ and ‘the’ can
be explained by the distributional similarities (all
precede nouns). It should also be emphasized that
the A matrix is essentially a ‘soft clustering’ of
types (meaning that types can belong to more than
one cluster). Thus, for example, ‘u.s.’ (the abbrevi-
ation for United States) appears under both tag 2
(which appears to have high loadings for nouns)
and tag 8 (with high loadings for adjectives).
We have alluded above in passing to possible
methods for improving the results of the
DEDICOM analysis. One would be to pre-process
the data differently. Here, a variety of options are
available which maintain a generally unsupervised
approach (one example is to avoid treating punctu-
ation as tokens). However, variations in pre-
processing are beyond the scope of this paper.
</bodyText>
<table confidence="0.985846454545454">
Tag Top 10 types (by weight) with weightings
1 million share said . year billion inc. corp. years quarter
0.0246 0.0146 0.0129 0.0098 0.0088 0.0069 0.0064 0.0061 0.0058 0.0054
2 company u.s. new first market share year stock . government
0.0264 0.0136 0.0113 0.0095 0.0086 0.0086 0.0079 0.0077 0.0065 0.006
3 the a new an other its any addition their 1988
0.2889 0.1194 0.0121 0.0094 0.0092 0.0085 0.0067 0.0062 0.0062 0.0057
...
8 the its his about those their all u.s. . this
0.0935 0.0462 0.0208 0.0160 0.0096 0.0095 0.0088 0.0077 0.0074 0.0071
...
</table>
<tableCaption confidence="0.999396">
Table 3. Type weightings in A matrix, by tag
</tableCaption>
<page confidence="0.997375">
60
</page>
<bodyText confidence="0.999903428571429">
Another method would be to constrain
DEDICOM so that the output more closely models
the characteristics of A* and R*, the emission- and
transition-probability matrices obtained from a
tagged training set. In particular, there is one im-
portant constraint on R* which is not replicated in
R: the constraint mentioned above that for all j,
</bodyText>
<equation confidence="0.6988615">
. We note that this constraint can be
x=1 x=1
</equation>
<bodyText confidence="0.999878285714286">
satisfied by Sinkhorn balancing (Sinkhorn 1964)4,
although it remains to be seen how the constraint
on R can best be incorporated into the DEDICOM
architecture. Assuming that A is column-
stochastic, another desirable constraint is that the
rows of A(DR)-1 should sum to the same as the
rows of X (the respective type frequencies). With
the implementation of these (and any other) con-
straints, one would expect the fit of DEDICOM to
the data to worsen (cf. (6) and (7) above), but in-
curring this cost could be worthwhile if the payoff
were somehow linguistically interesting (for ex-
ample, if it turned out we could achieve a much
higher correlation to gold-standard tagging).
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997665">
In this paper, we have introduced DEDICOM, an
analytical technique which to our knowledge has
not previously been used in computational linguis-
tics, and applied it to the problem of completely
unsupervised part-of-speech tagging. Theoretical-
ly, the model has features which recommend it
over other previous approaches to unsupervised
tagging, specifically SVD. Principal among the
advantages is the compatibility of DEDICOM with
the standard HMM-based approach to part-of-
speech tagging, but another significant advantage
is the fact that types are treated as ‘a single set of
objects’ regardless of whether they occupy the first
or second position in a bigram.
By applying DEDICOM to a tagged dataset, we
have shown that there is a significant correlation
between the tags induced by unsupervised,
DEDICOM-based tagging, and the pre-existing
gold-standard tags. This points both to an inherent
validity in the gold-standard tags (as a reasonable
</bodyText>
<footnote confidence="0.642071333333333">
4 It is also worth noting that Sinkhorn was motivated by the
same problem which concerns us, that of estimating a transi-
tion-probability matrix for a Markov model.
</footnote>
<bodyText confidence="0.9999794">
factorization of the data) and to the fact that
DEDICOM appears promising as a method of in-
ducing tags in cases where no gold standard is
available.
We have also shown that the factors of
DEDICOM are interesting in their own right: our
tests show that the A matrix (similar to an emis-
sion-probability matrix) models type part-of-
speech ambiguity well. Using insights from
DEDICOM, we have also shown how linear alge-
braic techniques may be used to estimate the fit of
a given part-of-speech factorization (whether in-
duced or manually created) to a given dataset, by
comparing actual versus expected bigram frequen-
cies.
In summary, it appears that DEDICOM is a
promising way forward for bridging the gap be-
tween unsupervised and supervised approaches to
part-of-speech tagging, and we are optimistic that
with further refinements to DEDICOM (such as
the addition of appropriate constraints), more in-
sight will be gained on how DEDICOM may most
profitably be used to improve part-of-speech tag-
ging when few pre-existing resources (such as
tagged corpora) are available.
</bodyText>
<sectionHeader confidence="0.996039" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997471285714286">
We are grateful to Danny Dunlavy for contributing
his thoughts to this work.
Sandia is a multiprogram laboratory operated by
Sandia Corporation, a Lockheed Martin Company,
for the United States Department of Energy’s Na-
tional Nuclear Security Administration under con-
tract DE-AC04-94AL85000.
</bodyText>
<figure confidence="0.978367333333333">
q q
Irxj =Zr
jx
</figure>
<page confidence="0.992074">
61
</page>
<sectionHeader confidence="0.995168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940049180328">
Brett W. Bader, Richard A. Harshman, and Tamara G.
Kolda. 2007. Temporal analysis of semantic graphs
using ASALSAN. In Proceedings of the 7th IEEE In-
ternational Conference on Data Mining, 33-42.
Brett W. Bader and Tamara G. Kolda. 2006. Efficient
MATLAB Computations with Sparse and Factored
Tensors. Technical Report SAND2006-7592, Sandia
National Laboratories, Albuquerque, NM and Liver-
more, CA.
Brett W. Bader and Tamara G. Kolda. 2007. The
MATLAB Tensor Toolbox, version 2.2.
http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. New York: ACM
Press.
L. R. Bahl and R. L. Mercer. 1976. Part of speech as-
signment by a statistical decision algorithm. In Pro-
ceedings of the IEEE International Symposium on
Information Theory, 88-89.
C. Biemann. 2006. Unsupervised part-of-speech tagging
employing efficient graph clustering. In Proceedings
of the COLING/ACL 2006 Student Research Work-
shop, 7-12.
E. Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the Third Conference on Ap-
plied Natural Language Processing, 152-155.
K. W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP
1988, 136-143.
CONLL 2000. Shared task data. Retrieved Dec. 1, 2008
from http://www.cnts.ua.ac.be/conll2000/chunking/.
S. J. DeRose. 1988. Grammatical category disambigua-
tion by statistical optimization. Computational Lin-
guistics 14, 31-39.
Harris, Z. S. 1962. String Analysis of Sentence Struc-
ture. Mouton: The Hague.
Richard Harshman. 1978. Models for Analysis of
Asymmetrical Relationships Among N Objects or
Stimuli. Paper presented at the First Joint Meeting of
the Psychometric Society and The Society for Ma-
thematical Psychology. Hamilton, Canada.
Richard Harshman, Paul Green, Yoram Wind, and Mar-
garet Lundy. 1982. A Model for the Analysis of
Asymmetric Data in Marketing Research. Marketing
Science 1(2), 205-242.
Hinrich Schütze. 1993. Part-of-Speech Induction from
Scratch. In Proceedings of the 31st Annual Meeting of
the Association for Computational Linguistics, 251-
258.
Hinrich Schütze. 1995. Distributional Part-of-Speech
Tagging. In Proceedings of the 7th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, 141-148.
Richard Sinkhorn. 1964. A Relationship Between Arbi-
trary Positive Matrices and Doubly Stochastic Ma-
trices. The Annals of Mathematical Statistics 35(2),
876-879.
W. S. Stolz, P. H. Tannenbaum, and F. V. Carstensen.
1965. A stochastic approach to the grammatical cod-
ing of English. Communications of the ACM 8(6),
399-405.
</reference>
<page confidence="0.999185">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.740876">
<title confidence="0.9994055">Using DEDICOM for Completely Unsupervised Part-of-Speech Tagging</title>
<author confidence="0.99721">Peter A Chew</author>
<author confidence="0.99721">W Brett</author>
<affiliation confidence="0.971487">Sandia National</affiliation>
<address confidence="0.9738965">P. O. Box 5800, MS Albuquerque, NM 87185-1012, USA</address>
<email confidence="0.991294">pchew@sandia.gov</email>
<email confidence="0.991294">bwbader@sandia.gov</email>
<author confidence="0.904385">Alla</author>
<affiliation confidence="0.9997695">Department of Computer University of</affiliation>
<address confidence="0.986415">Urbana, IL 61801, USA</address>
<email confidence="0.999804">rozovska@illinois.edu</email>
<abstract confidence="0.999803151515152">A standard and widespread approach to part-of-speech tagging is based on Hidden Markov Models (HMMs). An alternative approach, pioneered by Schütze (1993), induces parts of speech from scratch using singular value decomposition (SVD). We introduce DEDICOM as an alternative to SVD for part-of-speech induction. DEDICOM retains the advantages of SVD in that it is completely unsupervised: no prior knowledge is required to induce either the tagset or the associations of types with tags. However, unlike SVD, it is also fully compatible with the HMM framework, in that it can be used to estimate emissionand transition-probability matrices which can then be used as the input for an HMM. We apply the DEDICOM method to the CONLL corpus (CONLL 2000) and compare the output of DEDICOM to the part-of-speech tags given in the corpus, and find that the correlation (almost 0.5) is quite high. Using DEDICOM, we also estimate part-ofspeech ambiguity for each type, and find that these estimates correlate highly with part-of-speech ambiguity as measured in the original corpus (around 0.88). Finally, we show how the output of DEDICOM can be evaluated and compared against the more familiar output of supervised HMM-based tagging.</abstract>
<intro confidence="0.900337">54</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Richard A Harshman</author>
<author>Tamara G Kolda</author>
</authors>
<title>Temporal analysis of semantic graphs using ASALSAN.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th IEEE International Conference on Data Mining,</booktitle>
<pages>33--42</pages>
<contexts>
<context position="19352" citStr="Bader et al. (2007)" startWordPosition="3293" endWordPosition="3296">ng confirmed that there exists an A (=A*DA) and R (=R*) which both satisfies the DEDICOM model and can be used directly within a HMM-based tagger to achieve satisfactory results, we now consider whether A and R can be estimated if no tagged training set is available. We start, therefore, from X, the square 19,440 × 19,440 (sparse) matrix of raw bigram frequencies from the CONLL 2000 data. Using Matlab and the Tensor Toolbox (Bader and Kolda 2006, 2007), we computed the best rank-44 non-negative DEDICOM3 decomposition of this matrix using the 2-way version of the ASALSAN algorithm presented in Bader et al. (2007), which is based on iteratively improving random initial guesses for A and R. As with SVD, the rank of the decomposition can be selected by the user; we chose 44 since that was known to be the number of items in the CONLL 2000 tagset, but a lower number could be selected for a coarser-grained part-of-speech analysis. Ultimately, perhaps the best way to determine the optimal rank would be to evaluate different options within a larger end-to-end system, for example an information retrieval system; this, however, was beyond our scope in this study. As already mentioned, there are indeterminacies </context>
</contexts>
<marker>Bader, Harshman, Kolda, 2007</marker>
<rawString>Brett W. Bader, Richard A. Harshman, and Tamara G. Kolda. 2007. Temporal analysis of semantic graphs using ASALSAN. In Proceedings of the 7th IEEE International Conference on Data Mining, 33-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Efficient MATLAB Computations with Sparse and Factored Tensors.</title>
<date>2006</date>
<tech>Technical Report SAND2006-7592,</tech>
<institution>Sandia National Laboratories,</institution>
<location>Albuquerque, NM and Livermore, CA.</location>
<contexts>
<context position="19182" citStr="Bader and Kolda 2006" startWordPosition="3267" endWordPosition="3270">447 that the 315.000 258.679 and the 302.000 296.737 to be 285.000 499.315 Table 1. Actual versus expected frequencies for 10 most common bigrams in CONLL 2000 corpus Having confirmed that there exists an A (=A*DA) and R (=R*) which both satisfies the DEDICOM model and can be used directly within a HMM-based tagger to achieve satisfactory results, we now consider whether A and R can be estimated if no tagged training set is available. We start, therefore, from X, the square 19,440 × 19,440 (sparse) matrix of raw bigram frequencies from the CONLL 2000 data. Using Matlab and the Tensor Toolbox (Bader and Kolda 2006, 2007), we computed the best rank-44 non-negative DEDICOM3 decomposition of this matrix using the 2-way version of the ASALSAN algorithm presented in Bader et al. (2007), which is based on iteratively improving random initial guesses for A and R. As with SVD, the rank of the decomposition can be selected by the user; we chose 44 since that was known to be the number of items in the CONLL 2000 tagset, but a lower number could be selected for a coarser-grained part-of-speech analysis. Ultimately, perhaps the best way to determine the optimal rank would be to evaluate different options within a </context>
</contexts>
<marker>Bader, Kolda, 2006</marker>
<rawString>Brett W. Bader and Tamara G. Kolda. 2006. Efficient MATLAB Computations with Sparse and Factored Tensors. Technical Report SAND2006-7592, Sandia National Laboratories, Albuquerque, NM and Livermore, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>The MATLAB Tensor Toolbox, version 2.2.</title>
<date>2007</date>
<note>http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/.</note>
<marker>Bader, Kolda, 2007</marker>
<rawString>Brett W. Bader and Tamara G. Kolda. 2007. The MATLAB Tensor Toolbox, version 2.2. http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>ACM Press.</publisher>
<location>New York:</location>
<contexts>
<context position="9477" citStr="Baeza-Yates and Ribeiro-Neto 1999" startWordPosition="1544" endWordPosition="1547">out changing the goodness of fit, so long as the inverse operation is applied to the other. For example, if we let Â = AD, where D is any diagonal scaling matrix (or, more generally, any nonsingular matrix), then we can write X ~ ARAT = ÂD-1RD-1ÂT (3) since ÂT = (AD) T = DAT (In our application, we constrain A and R to be nonnegative as noted below.) To our knowledge, there have been no applications of DEDICOM to date in computational linguistics. This is in contrast to SVD, which has been extensively used for text analysis (for applications other than unsupervised part-of-speech tagging, see Baeza-Yates and Ribeiro-Neto 1999). 3 Applicability of DEDICOM to part-ofspeech tagging Schütze’s (1993) key insight is that – at least in English – adjacencies between types are a good guide to their grammatical functions. That insight can be leveraged by applying either SVD or DEDICOM to a type-by-type adjacency matrix. With DEDICOM, however, we add the constraint (already stated) that the types are a ‘single set of things’: whether a type ‘precedes’ or ‘follows’ – i.e., whether it is in a row or a column of the matrix – does not affect its grammatical function. This constraint is as it should be, and, to our knowledge, sets</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. New York: ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>R L Mercer</author>
</authors>
<title>Part of speech assignment by a statistical decision algorithm.</title>
<date>1976</date>
<booktitle>In Proceedings of the IEEE International Symposium on Information Theory,</booktitle>
<pages>88--89</pages>
<contexts>
<context position="2412" citStr="Bahl and Mercer (1976)" startWordPosition="373" endWordPosition="376">on two elements: a dictionary to assign possible parts of speech to each word, and a list of hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Sin</context>
</contexts>
<marker>Bahl, Mercer, 1976</marker>
<rawString>L. R. Bahl and R. L. Mercer. 1976. Part of speech assignment by a statistical decision algorithm. In Proceedings of the IEEE International Symposium on Information Theory, 88-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Student Research Workshop,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="2622" citStr="Biemann (2006)" startWordPosition="408" endWordPosition="409">t. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He shows that in the reduced-dimensional space implied by SVD, tokens do indeed cluster intuitively by part-of-speech; and that if context is taken i</context>
<context position="10192" citStr="Biemann (2006)" startWordPosition="1668" endWordPosition="1669"> in English – adjacencies between types are a good guide to their grammatical functions. That insight can be leveraged by applying either SVD or DEDICOM to a type-by-type adjacency matrix. With DEDICOM, however, we add the constraint (already stated) that the types are a ‘single set of things’: whether a type ‘precedes’ or ‘follows’ – i.e., whether it is in a row or a column of the matrix – does not affect its grammatical function. This constraint is as it should be, and, to our knowledge, sets DEDICOM apart from all previous unsupervised approaches including those of Schütze (1993, 1995) and Biemann (2006). Given any corpus containing n types and k tokens, we can let X be an n × n token-adjacency matrix. Let each entry xij in X denote the number of times in the corpus that type i immediately precedes type j. X is thus a matrix of bigram frequencies. It follows that the sum of the elements of X equals k – 1 (because the first token in the corpus is preceded by nothing, and the last token is followed by nothing). Any given row sum of X (the type frequency corresponding to the particular row) will equal the corresponding column sum, except if the type happens to occur in the first or last position</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In Proceedings of the COLING/ACL 2006 Student Research Workshop, 7-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="16996" citStr="Brill 1992" startWordPosition="2880" endWordPosition="2881">, and is for us a significant reason which recommends the use of DEDICOM. 5 Evaluation For all evaluation described here, we used the CONLL 2000 shared task data (CONLL 2000). This English-language newswire corpus consists of 19,440 types and 259,104 tokens (including punctuation marks as separate types/tokens). Each token is associated with a part-of-speech tag and a chunk tag, although we did not use the chunk tags r ). jx 57 in the work described here. The tags are from a 44- item tagset. The CONLL 2000 tags against which we measure our own results are in fact assigned by the Brill tagger (Brill 1992), and while these may not correlate perfectly with those that would have been assigned by a human linguist, we believe that the correlation is likely to be good enough to allow for an informative evaluation of our method. Before discussing the evaluation of unsupervised DEDICOM, let us briefly reconsider the similarities of DEDICOM to the supervised HMM model in the light of actual data in the CONLL corpus. We stated in (5) that X - A*DAR*DAA*T. For the CONLL 2000 tagged data, A* is a 19,440 × 44 matrix and R* is a 44 × 44 matrix. Using A*DA and R*DA as emission- and transitionprobability matr</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In ANLP 1988,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="2476" citStr="Church (1988)" startWordPosition="386" endWordPosition="387">word, and a list of hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He </context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In ANLP 1988, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CONLL</author>
</authors>
<title>Shared task data. Retrieved</title>
<date>2000</date>
<volume>1</volume>
<note>from http://www.cnts.ua.ac.be/conll2000/chunking/.</note>
<contexts>
<context position="1079" citStr="CONLL 2000" startWordPosition="160" endWordPosition="161">y Schütze (1993), induces parts of speech from scratch using singular value decomposition (SVD). We introduce DEDICOM as an alternative to SVD for part-of-speech induction. DEDICOM retains the advantages of SVD in that it is completely unsupervised: no prior knowledge is required to induce either the tagset or the associations of types with tags. However, unlike SVD, it is also fully compatible with the HMM framework, in that it can be used to estimate emission- and transition-probability matrices which can then be used as the input for an HMM. We apply the DEDICOM method to the CONLL corpus (CONLL 2000) and compare the output of DEDICOM to the part-of-speech tags given in the corpus, and find that the correlation (almost 0.5) is quite high. Using DEDICOM, we also estimate part-ofspeech ambiguity for each type, and find that these estimates correlate highly with part-of-speech ambiguity as measured in the original corpus (around 0.88). Finally, we show how the output of DEDICOM can be evaluated and compared against the more familiar output of supervised HMM-based tagging. 54 1 Introduction Traditionally, part-of-speech tagging has been approached either in a rule-based fashion, or stochastica</context>
<context position="5092" citStr="CONLL 2000" startWordPosition="794" endWordPosition="795"> relatively modest, the fact that a clearer connection exists between DEDICOM and HMMs than between SVD and HMMs gives us good reason to believe that with further refinements, DEDICOM may be able to give us ‘the best of both worlds’ in many respects: the benefits of avoiding the need for a pre-tagged corpus, with empirical results approaching those of HMM-based tagging. In the following sections, we introduce DEDICOM, describe its applicability to the partof-speech tagging problem, and outline its connections to the standard HMM-based approach to tagging. We evaluate the use of DEDICOM on the CONLL 2000 shared task data, discuss the results and suggest avenues for improvement. 2 DEDICOM DEDICOM, which stands for ‘DEcomposition into DIrectional COMponents’, is a linear-algebraic decomposition method attributable to Harshman (1978) which has been used to analyze matrices of 1 We note the latter is also true for languages in which word order is relatively free – usually the same languages as those with rich morphology. While English word order is significantly constrained by part-of-speech categorizations, this is not as true of, say, Russian. Thus, an adjacency matrix formed from a Russian cor</context>
<context position="16529" citStr="CONLL 2000" startWordPosition="2798" endWordPosition="2799">etween the output of the fully-unsupervised DEDICOM/HMM tagger and that of a supervised HMM tagger is that in the first case, the ‘tags’ are numeric indices representing the corresponding column of A, and in the second case, they are the members of the tagset used in the training data. The fact that emission and transition probabilities (or at least something very like them) are a natural by-product of DEDICOM sets DEDICOM apart from Schütze’s SVD-based approach, and is for us a significant reason which recommends the use of DEDICOM. 5 Evaluation For all evaluation described here, we used the CONLL 2000 shared task data (CONLL 2000). This English-language newswire corpus consists of 19,440 types and 259,104 tokens (including punctuation marks as separate types/tokens). Each token is associated with a part-of-speech tag and a chunk tag, although we did not use the chunk tags r ). jx 57 in the work described here. The tags are from a 44- item tagset. The CONLL 2000 tags against which we measure our own results are in fact assigned by the Brill tagger (Brill 1992), and while these may not correlate perfectly with those that would have been assigned by a human linguist, we believe that the corre</context>
<context position="18721" citStr="CONLL 2000" startWordPosition="3189" endWordPosition="3190"> of the elements in X, and if we let E be the matrix of error terms (X - A*DAR*DAA*T), then we find that ||E ||(the Frobenius norm of E) is 38.764% of ||X ||- in other words, A*DAR*DAA*T accounts for just over 60% of the data in X. Type 1 Type 2 Actual Expected frequency frequency of the 1,421.000 1,202.606 in the 1,213.000 875.822 for the 553.000 457.067 to the 445.000 415.524 on the 439.000 271.528 the company 383.000 105.794 a share 371.000 32.447 that the 315.000 258.679 and the 302.000 296.737 to be 285.000 499.315 Table 1. Actual versus expected frequencies for 10 most common bigrams in CONLL 2000 corpus Having confirmed that there exists an A (=A*DA) and R (=R*) which both satisfies the DEDICOM model and can be used directly within a HMM-based tagger to achieve satisfactory results, we now consider whether A and R can be estimated if no tagged training set is available. We start, therefore, from X, the square 19,440 × 19,440 (sparse) matrix of raw bigram frequencies from the CONLL 2000 data. Using Matlab and the Tensor Toolbox (Bader and Kolda 2006, 2007), we computed the best rank-44 non-negative DEDICOM3 decomposition of this matrix using the 2-way version of the ASALSAN algorithm p</context>
<context position="21266" citStr="CONLL 2000" startWordPosition="3628" endWordPosition="3629">(with some provisos which shall be enumerated below), and to obtain the rowstochastic transition-probability matrix, we simply multiply R by a diagonal matrix DR whose elements are the inverses of R’s row sums. 3 Non-negative DEDICOM imposes the constraint not present in Harshman (1978, 1982) that all entries in A and R must be non-negative. This constraint is appropriate in the present case, since the entries in A* and R* (and of course the probabilities in A*D and R*D) are by definition non-negative. 58 Table 2. Partial confusion matrix of gold-standard tags against DEDICOM-induced tags for CONLL 2000 dataset With A as an emission-probability matrix and RDR as a transition-probability matrix, we now have all that is needed for an HMM-based tagger to estimate the most likely sequence of ‘tags’ given the corpus. However, since the ‘tags’ here are numerical indices, as mentioned, to evaluate the output we must look at the correlation between these ‘tags’ and the gold-standard tags given in the CONLL 2000 data. One way this can be done is by presenting a 44 × 44 confusion matrix (of goldstandard tags against induced tags), and then measuring the correlation coefficient (Pearson’s R) between th</context>
<context position="25556" citStr="CONLL 2000" startWordPosition="4395" endWordPosition="4396">the A matrix. For each ‘tag’ (column) in A, the ‘types’ (rows) can be listed in decreasing order of their weighting in A. This gives us an idea of which types are most characteristic of which tags, and whether the grouping into tags makes any intuitive sense. These results (for selected tags only, owing to limitations of space) are given in Table 3. Many groupings in Table 3 do make sense: for example, the fourth tag is clearly associated with verbs, while the two types with significant weightings for tag 2 are both determiners. By referring back to Table 2, we can see that many tokens in the CONLL 2000 dataset tagged as verbs are indeed tagged by the DEDICOM tagger as ‘tag 4’, while many determiners are tagged as ‘tag 3’. To understand where a lack of correlation may arise, however, it is informative to look at apparent anomalies in the A matrix. For example, it can be seen from Table 3 that ‘new’, an adjective, is grouped in the third tag with ‘a’ and ‘the’ (and ranking above ‘an’). Although not in agreement with the CONLL 2000 ‘gold standard’ tagging, the idea that determiners are a type of adjective is in fact in accordance with traditional English grammar. Here, the grouping of ‘new’, ‘</context>
</contexts>
<marker>CONLL, 2000</marker>
<rawString>CONLL 2000. Shared task data. Retrieved Dec. 1, 2008 from http://www.cnts.ua.ac.be/conll2000/chunking/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<pages>31--39</pages>
<contexts>
<context position="2494" citStr="DeRose (1988)" startWordPosition="389" endWordPosition="390">f hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He shows that in the </context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>S. J. DeRose. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics 14, 31-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>String Analysis of Sentence Structure. Mouton: The Hague.</title>
<date>1962</date>
<contexts>
<context position="1697" citStr="Harris (1962)" startWordPosition="256" endWordPosition="257">compare the output of DEDICOM to the part-of-speech tags given in the corpus, and find that the correlation (almost 0.5) is quite high. Using DEDICOM, we also estimate part-ofspeech ambiguity for each type, and find that these estimates correlate highly with part-of-speech ambiguity as measured in the original corpus (around 0.88). Finally, we show how the output of DEDICOM can be evaluated and compared against the more familiar output of supervised HMM-based tagging. 54 1 Introduction Traditionally, part-of-speech tagging has been approached either in a rule-based fashion, or stochastically. Harris (1962) was among the first to develop algorithms of the former type. The rulebased approach relies on two elements: a dictionary to assign possible parts of speech to each word, and a list of hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to </context>
</contexts>
<marker>Harris, 1962</marker>
<rawString>Harris, Z. S. 1962. String Analysis of Sentence Structure. Mouton: The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Harshman</author>
</authors>
<title>Models for Analysis of Asymmetrical Relationships Among N Objects or Stimuli. Paper presented at the First Joint Meeting of the Psychometric Society and The Society for Mathematical Psychology.</title>
<date>1978</date>
<location>Hamilton, Canada.</location>
<contexts>
<context position="5323" citStr="Harshman (1978)" startWordPosition="824" endWordPosition="825">orlds’ in many respects: the benefits of avoiding the need for a pre-tagged corpus, with empirical results approaching those of HMM-based tagging. In the following sections, we introduce DEDICOM, describe its applicability to the partof-speech tagging problem, and outline its connections to the standard HMM-based approach to tagging. We evaluate the use of DEDICOM on the CONLL 2000 shared task data, discuss the results and suggest avenues for improvement. 2 DEDICOM DEDICOM, which stands for ‘DEcomposition into DIrectional COMponents’, is a linear-algebraic decomposition method attributable to Harshman (1978) which has been used to analyze matrices of 1 We note the latter is also true for languages in which word order is relatively free – usually the same languages as those with rich morphology. While English word order is significantly constrained by part-of-speech categorizations, this is not as true of, say, Russian. Thus, an adjacency matrix formed from a Russian corpus is likely to be less informative about part-of-speech classifications as one formed from an English corpus. Quite possibly, this is as much of a limitation for DEDICOM as it is for SVD. asymmetrical directional relationships be</context>
<context position="8050" citStr="Harshman (1978)" startWordPosition="1284" endWordPosition="1285">es can then be identified. If X represents the original n x n matrix of asymmetric relationships, and a general entry xij in X represents the strength of the directed relationship of object i to object j, then the single-domain DEDICOM model2 can be written as follows: X=ARAT+E (1) where A denotes an n x q matrix of weights of the n observed objects in q dimensions (where q &lt; n), and R is a dense q x q asymmetric matrix expressing the directional relationships between the q dimensions or basic types. AT is simply the transpose 2 There is a dual-domain DEDICOM model, which is also described in Harshman (1978). The dual-domain DEDICOM model is not relevant to our discussion, and thus it will not be mentioned further. References in this paper to ‘DEDICOM’ are to be understood as references in shorthand to ‘singledomain DEDICOM’. 55 of A, and E is a matrix of error terms. Our objective is to minimize E, so we can also write: X ~ ARAT (2) As noted by Harshman (1978: 209), the fact that A appears on both the left and right of R means that the data is described ‘in terms of asymmetric relations among a single set of things’ – in other words, when objects are on the receiving end of the directional relat</context>
<context position="11055" citStr="Harshman (1978)" startWordPosition="1836" endWordPosition="1837"> It follows that the sum of the elements of X equals k – 1 (because the first token in the corpus is preceded by nothing, and the last token is followed by nothing). Any given row sum of X (the type frequency corresponding to the particular row) will equal the corresponding column sum, except if the type happens to occur in the first or last position in the corpus. X will be asymmetric, since the frequency of bigram ij is clearly not the same as that of bigram ji for all i and j. It can be seen, therefore, that our X represents asymmetric directional data, very similar to the data analyzed in Harshman (1978) and Harshman et al. (1982). If we fit the DEDICOM model to our X matrix, then we obtain an A matrix which represents types by latent classes, and an R matrix which represents directional relationships between latent classes. We can think of the latent classes as induced parts of speech. With SVD, we believe that the orthogonality of the reduced-dimensional features militates against any attempt to correlate these features with parts of speech. From a linguistic point of view, there is no reason to believe that parts of speech are orthogonal to one another in any sense. For example, nouns and </context>
<context position="20942" citStr="Harshman (1978" startWordPosition="3573" endWordPosition="3574">mong the latent aspects of the phrases, rather than the phrases themselves’. Thus, if DEDICOM is constrained so that A is column-stochastic (which is required in any case of the matrix of emission probabilities), then the sum of the elements in R should approximate the sum of the elements in X. R is therefore comparable to R* (with some provisos which shall be enumerated below), and to obtain the rowstochastic transition-probability matrix, we simply multiply R by a diagonal matrix DR whose elements are the inverses of R’s row sums. 3 Non-negative DEDICOM imposes the constraint not present in Harshman (1978, 1982) that all entries in A and R must be non-negative. This constraint is appropriate in the present case, since the entries in A* and R* (and of course the probabilities in A*D and R*D) are by definition non-negative. 58 Table 2. Partial confusion matrix of gold-standard tags against DEDICOM-induced tags for CONLL 2000 dataset With A as an emission-probability matrix and RDR as a transition-probability matrix, we now have all that is needed for an HMM-based tagger to estimate the most likely sequence of ‘tags’ given the corpus. However, since the ‘tags’ here are numerical indices, as menti</context>
</contexts>
<marker>Harshman, 1978</marker>
<rawString>Richard Harshman. 1978. Models for Analysis of Asymmetrical Relationships Among N Objects or Stimuli. Paper presented at the First Joint Meeting of the Psychometric Society and The Society for Mathematical Psychology. Hamilton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Harshman</author>
<author>Paul Green</author>
<author>Yoram Wind</author>
<author>Margaret Lundy</author>
</authors>
<title>A Model for the Analysis of Asymmetric Data in Marketing Research.</title>
<date>1982</date>
<journal>Marketing Science</journal>
<volume>1</volume>
<issue>2</issue>
<pages>205--242</pages>
<contexts>
<context position="6010" citStr="Harshman et al. (1982)" startWordPosition="935" endWordPosition="938">is also true for languages in which word order is relatively free – usually the same languages as those with rich morphology. While English word order is significantly constrained by part-of-speech categorizations, this is not as true of, say, Russian. Thus, an adjacency matrix formed from a Russian corpus is likely to be less informative about part-of-speech classifications as one formed from an English corpus. Quite possibly, this is as much of a limitation for DEDICOM as it is for SVD. asymmetrical directional relationships between objects or persons. Early on, the technique was applied by Harshman et al. (1982) to the analysis of two types of marketing data: ‘free associations’ – how often one phrase (describing hair shampoo) evokes another in the minds of survey respondents, and ‘car switching data’ – how often people switch from one to another of 16 car types. Both datasets are asymmetric and directional: in the first dataset, for example, the phrase ‘body’ (referring to shampoo) evoked the phrase ‘fullness’ twice as often in the minds of respondents as ‘fullness’ evoked ‘body’. Likewise, the data from Harshman et al. (1982) show that in the given period, 3,820 people switched from ‘midsize import</context>
<context position="11082" citStr="Harshman et al. (1982)" startWordPosition="1839" endWordPosition="1842"> sum of the elements of X equals k – 1 (because the first token in the corpus is preceded by nothing, and the last token is followed by nothing). Any given row sum of X (the type frequency corresponding to the particular row) will equal the corresponding column sum, except if the type happens to occur in the first or last position in the corpus. X will be asymmetric, since the frequency of bigram ij is clearly not the same as that of bigram ji for all i and j. It can be seen, therefore, that our X represents asymmetric directional data, very similar to the data analyzed in Harshman (1978) and Harshman et al. (1982). If we fit the DEDICOM model to our X matrix, then we obtain an A matrix which represents types by latent classes, and an R matrix which represents directional relationships between latent classes. We can think of the latent classes as induced parts of speech. With SVD, we believe that the orthogonality of the reduced-dimensional features militates against any attempt to correlate these features with parts of speech. From a linguistic point of view, there is no reason to believe that parts of speech are orthogonal to one another in any sense. For example, nouns and adjectives (traditionally c</context>
<context position="20010" citStr="Harshman et al. (1982" startWordPosition="3407" endWordPosition="3410">ving random initial guesses for A and R. As with SVD, the rank of the decomposition can be selected by the user; we chose 44 since that was known to be the number of items in the CONLL 2000 tagset, but a lower number could be selected for a coarser-grained part-of-speech analysis. Ultimately, perhaps the best way to determine the optimal rank would be to evaluate different options within a larger end-to-end system, for example an information retrieval system; this, however, was beyond our scope in this study. As already mentioned, there are indeterminacies of rotation and scale in DEDICOM. As Harshman et al. (1982: 211) point out, ‘when the columns of A are standardized... the R matrix can then be interpreted as expressing relationships among the dimensions in the same units as the original data. That is, the R matrix can be interpreted as a matrix of the same kind as the original data matrix X, but describing the relations among the latent aspects of the phrases, rather than the phrases themselves’. Thus, if DEDICOM is constrained so that A is column-stochastic (which is required in any case of the matrix of emission probabilities), then the sum of the elements in R should approximate the sum of the e</context>
</contexts>
<marker>Harshman, Green, Wind, Lundy, 1982</marker>
<rawString>Richard Harshman, Paul Green, Yoram Wind, and Margaret Lundy. 1982. A Model for the Analysis of Asymmetric Data in Marketing Research. Marketing Science 1(2), 205-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Part-of-Speech Induction from Scratch.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>251--258</pages>
<contexts>
<context position="2596" citStr="Schütze (1993" startWordPosition="404" endWordPosition="405">mbiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then subjecting that matrix to Singular Value Decomposition (SVD) to reveal latent dimensions. He shows that in the reduced-dimensional space implied by SVD, tokens do indeed cluster intuitively by part-of-speech; and </context>
<context position="10166" citStr="Schütze (1993" startWordPosition="1664" endWordPosition="1665">nsight is that – at least in English – adjacencies between types are a good guide to their grammatical functions. That insight can be leveraged by applying either SVD or DEDICOM to a type-by-type adjacency matrix. With DEDICOM, however, we add the constraint (already stated) that the types are a ‘single set of things’: whether a type ‘precedes’ or ‘follows’ – i.e., whether it is in a row or a column of the matrix – does not affect its grammatical function. This constraint is as it should be, and, to our knowledge, sets DEDICOM apart from all previous unsupervised approaches including those of Schütze (1993, 1995) and Biemann (2006). Given any corpus containing n types and k tokens, we can let X be an n × n token-adjacency matrix. Let each entry xij in X denote the number of times in the corpus that type i immediately precedes type j. X is thus a matrix of bigram frequencies. It follows that the sum of the elements of X equals k – 1 (because the first token in the corpus is preceded by nothing, and the last token is followed by nothing). Any given row sum of X (the type frequency corresponding to the particular row) will equal the corresponding column sum, except if the type happens to occur in </context>
</contexts>
<marker>Schütze, 1993</marker>
<rawString>Hinrich Schütze. 1993. Part-of-Speech Induction from Scratch. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, 251-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Distributional Part-of-Speech Tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="3754" citStr="Schütze (1995)" startWordPosition="577" endWordPosition="578">ns do indeed cluster intuitively by part-of-speech; and that if context is taken into account, something akin to part-of-speech tagging Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 54–62, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics can be achieved. Whereas the performance of stochastic taggers is generally sub-optimal when the domain of the training data differs from that of the test data, distributional tagging sidesteps this problem, since each corpus can be considered in its own right. Schütze (1995) notes two general drawbacks of distributional tagging methods: the performance is relatively modest compared to that of supervised methods; and languages with rich morphology may pose a challenge.1 In this paper, we present an alternative unsupervised approach to distributional tagging. Instead of SVD, we use a dimensionality reduction technique known as DEDICOM, which has various advantages over the SVD-based approach. Principal among these is that, even though no pre-tagged corpus is required, DEDICOM can easily be used as input to a HMM-based approach (and the two share linear-algebraic si</context>
</contexts>
<marker>Schütze, 1995</marker>
<rawString>Hinrich Schütze. 1995. Distributional Part-of-Speech Tagging. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics, 141-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sinkhorn</author>
</authors>
<title>A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices.</title>
<date>1964</date>
<journal>The Annals of Mathematical Statistics</journal>
<volume>35</volume>
<issue>2</issue>
<pages>876--879</pages>
<contexts>
<context position="27979" citStr="Sinkhorn 1964" startWordPosition="4808" endWordPosition="4809">62 0.0057 ... 8 the its his about those their all u.s. . this 0.0935 0.0462 0.0208 0.0160 0.0096 0.0095 0.0088 0.0077 0.0074 0.0071 ... Table 3. Type weightings in A matrix, by tag 60 Another method would be to constrain DEDICOM so that the output more closely models the characteristics of A* and R*, the emission- and transition-probability matrices obtained from a tagged training set. In particular, there is one important constraint on R* which is not replicated in R: the constraint mentioned above that for all j, . We note that this constraint can be x=1 x=1 satisfied by Sinkhorn balancing (Sinkhorn 1964)4, although it remains to be seen how the constraint on R can best be incorporated into the DEDICOM architecture. Assuming that A is columnstochastic, another desirable constraint is that the rows of A(DR)-1 should sum to the same as the rows of X (the respective type frequencies). With the implementation of these (and any other) constraints, one would expect the fit of DEDICOM to the data to worsen (cf. (6) and (7) above), but incurring this cost could be worthwhile if the payoff were somehow linguistically interesting (for example, if it turned out we could achieve a much higher correlation </context>
</contexts>
<marker>Sinkhorn, 1964</marker>
<rawString>Richard Sinkhorn. 1964. A Relationship Between Arbitrary Positive Matrices and Doubly Stochastic Matrices. The Annals of Mathematical Statistics 35(2), 876-879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Stolz</author>
<author>P H Tannenbaum</author>
<author>F V Carstensen</author>
</authors>
<title>A stochastic approach to the grammatical coding of English.</title>
<date>1965</date>
<journal>Communications of the ACM</journal>
<volume>8</volume>
<issue>6</issue>
<pages>399--405</pages>
<contexts>
<context position="2385" citStr="Stolz et al. (1965)" startWordPosition="368" endWordPosition="371">lebased approach relies on two elements: a dictionary to assign possible parts of speech to each word, and a list of hand-written rules – which must be painstakingly developed for each new language or domain – to disambiguate tokens in context. Stochastic taggers, on the other hand, avoid the need for hand-written rules by tabulating probabilities of types and part-of-speech tags (which must be gathered from a tagged training corpus), and applying a special case of Bayesian inference (usually, Hidden Markov Models [HMMs]) to disambiguate tokens in context. The latter approach was pioneered by Stolz et al. (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988). A third and more recent approach, known as ‘distributional tagging’ and exemplified by Schütze (1993, 1995) and Biemann (2006), aims to eliminate the need for both hand-written rules and a tagged training corpus, since the latter may not be available for every language or domain. Distributional tagging is fully-unsupervised, unlike the two traditional approaches described above. Schütze suggests analyzing the distributional patterns of words by forming a term adjacency matrix, then su</context>
</contexts>
<marker>Stolz, Tannenbaum, Carstensen, 1965</marker>
<rawString>W. S. Stolz, P. H. Tannenbaum, and F. V. Carstensen. 1965. A stochastic approach to the grammatical coding of English. Communications of the ACM 8(6), 399-405.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>