<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006789">
<title confidence="0.997818">
Broadcast News Story Segmentation Using Manifold Learning on Latent
Topic Distributions
</title>
<author confidence="0.999114">
Xiaoming Lu1,2, Lei Xie1∗, Cheung-Chi Leung2, Bin Ma2, Haizhou Li2
</author>
<affiliation confidence="0.971623">
1School of Computer Science, Northwestern Polytechnical University, China
2Institute for Infocomm Research, A⋆STAR, Singapore
</affiliation>
<email confidence="0.989725">
luxiaomingnpu@gmail.com, lxie@nwpu.edu.cn, {ccleung,mabin,hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984409090909">
We present an efficient approach for
broadcast news story segmentation using a
manifold learning algorithm on latent top-
ic distributions. The latent topic distribu-
tion estimated by Latent Dirichlet Alloca-
tion (LDA) is used to represent each text
block. We employ Laplacian Eigenmap-
s (LE) to project the latent topic distribu-
tions into low-dimensional semantic rep-
resentations while preserving the intrinsic
local geometric structure. We evaluate t-
wo approaches employing LDA and prob-
abilistic latent semantic analysis (PLSA)
distributions respectively. The effects of
different amounts of training data and dif-
ferent numbers of latent topics on the two
approaches are studied. Experimental re-
sults show that our proposed LDA-based
approach can outperform the correspond-
ing PLSA-based approach. The proposed
approach provides the best performance
with the highest F1-measure of 0.7860.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993939543859649">
Story segmentation refers to partitioning a mul-
timedia stream into homogenous segments each
embodying a main topic or coherent story (Allan,
2002). With the explosive growth of multimedia
data, it becomes difficult to retrieve the most rel-
evant components. For indexing broadcast news
programs, it is desirable to divide each of them
into a number of independent stories. Manual seg-
mentation is accurate but labor-intensive and cost-
ly. Therefore, automatic story segmentation ap-
proaches are highly demanded.
Lexical-cohesion based approaches have been
widely studied for automatic broadcast news story
segmentation (Beeferman et al., 1997; Choi, 1999;
Hearst, 1997; Rosenberg and Hirschberg, 2006;
*corresponding author
Lo et al., 2009; Malioutov and Barzilay, 2006;
Yamron et al., 1999; Tur et al., 2001). In this
kind of approaches, the audio portion of the da-
ta stream is passed to an automatic speech recog-
nition (ASR) system. Lexical cues are extracted
from the ASR transcripts. Lexical cohesion is the
phenomenon that different stories tend to employ
different sets of terms. Term repetition is one of
the most common appearances.
These rigid lexical-cohesion based approach-
es simply take term repetition into consideration,
while term association in lexical cohesion is ig-
nored. Moreover, polysemy and synonymy are not
considered. To deal with these problems, some
topic model techniques which provide conceptu-
al level matching have been introduced to text and
story segmentation task (Hearst, 1997). Proba-
bilistic latent semantic analysis (PLSA) (Hofman-
n, 1999) is a typical instance and used widely.
PLSA is the probabilistic variant of latent seman-
tic analysis (LSA) (Choi et al., 2001), and offers a
more solid statistical foundation. PLSA provides
more significant improvement than LSA for story
segmentation (Lu et al., 2011; Blei and Moreno,
2001).
Despite the success of PLSA, there are con-
cerns that the number of parameters in PLSA
grows linearly with the size of the corpus. This
makes PLSA not desirable if there is a consid-
erable amount of data available, and causes seri-
ous over-fitting problems (Blei, 2012). To deal
with this issue, Latent Dirichlet Allocation (L-
DA) (Blei et al., 2003) has been proposed. LDA
has been proved to be effective in many segmenta-
tion tasks (Arora and Ravindran, 2008; Hall et al.,
2008; Sun et al., 2008; Riedl and Biemann, 2012;
Chien and Chueh, 2012).
Recent studies have shown that intrinsic di-
mensionality of natural text corpus is significant-
ly lower than its ambient Euclidean space (Belkin
and Niyogi, 2002; Xie et al., 2012). Therefore,
</bodyText>
<page confidence="0.96425">
190
</page>
<bodyText confidence="0.933413952380952">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
Laplacian Eigenmaps (LE) was proposed to com-
pute corresponding natural low-dimensional struc-
ture. LE is a geometrically motivated dimen-
sionality reduction method. It projects data into
a low-dimensional representation while preserv-
ing the intrinsic local geometric structure infor-
mation (Belkin and Niyogi, 2002). The locali-
ty preserving property attempts to make the low-
dimensional data representation more robust to the
noise from ASR errors (Xie et al., 2012).
To further improve the segmentation perfor-
mance, using latent topic distributions and LE in-
stead of term frequencies to represent text blocks
is studied in this paper. We study the effects of
the size of training data and the number of latent
topics on the LDA-based and the PLSA-based ap-
proaches. Another related work (Lu et al., 2013)
is to use local geometric information to regularize
the log-likelihood computation in PLSA.
</bodyText>
<sectionHeader confidence="0.986951" genericHeader="method">
2 Our Proposed Approach
</sectionHeader>
<bodyText confidence="0.9998765">
In this paper, we propose to apply LE on the L-
DA topic distributions, each of which is estimat-
ed from a text block. The low-dimensional vec-
tors obtained by LE projection are used to detect
story boundaries through dynamic programming.
Moreover, as in (Xie et al., 2012), we incorporate
the temporal distances between block pairs as a
penalty factor in the weight matrix.
</bodyText>
<subsectionHeader confidence="0.994767">
2.1 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.998869125">
Latent Dirichlet allocation (LDA) (Blei et al.,
2003) is a generative probabilistic model of a cor-
pus. It considers that documents are represented
as random mixtures over latent topics, where each
topic is characterized by a distribution over terms.
In LDA, given a corpus D = {d1, d2, ... , dM}
and a set of terms W = (w1, w2, ... , wV ), the
generative process can be summarized as follows:
</bodyText>
<listItem confidence="0.957976875">
1) For each document d, pick a multinomial dis-
tribution θ from a Dirichlet distribution parameter
α, denoted as θ — Dir(α).
2) For each term w in document d, select a topic
z from the multinomial distribution θ, denoted as
z — Multinomial(θ).
3) Select a term w from P(w|z,β), which is a
multinomial probability conditioned on the topic.
</listItem>
<bodyText confidence="0.9982914">
An LDA model is characterized by two sets of
prior parameters α and β. α = (α1, α2,... , αK)
represents the Dirichlet prior distributions for each
K latent topics. β is a KxV matrix, which defines
the latent topic distributions over terms.
</bodyText>
<subsectionHeader confidence="0.9526365">
2.2 Construction of weight matrix in
Laplacian Eigenmaps
</subsectionHeader>
<bodyText confidence="0.999983875">
Laplacian Eigenmaps (LE) is introduced to project
high-dimensional data into a low-dimensional rep-
resentation while preserving its locality property.
Given the ASR transcripts of N text blocks, we ap-
ply LDA algorithm to compute the corresponding
latent topic distributions X = [x1, x2, ... , xiv] in
RK, where K is the number of latent topics, name-
ly the dimensionality of LDA distributions.
We use G to denote an N-node (N is number of
LDA distributions) graph which represents the re-
lationship between all the text block pairs. If dis-
tribution vectors xi and xj come from the same
story, we put an edge between nodes i and j. We
define a weight matrix S of the graph G to denote
the cohesive strength between the text block pairs.
Each element of this weight matrix is defined as:
</bodyText>
<equation confidence="0.97246">
sij = cos(xi,xj)µ|i−j|, (1)
</equation>
<bodyText confidence="0.999997666666667">
where µ|i−j |serves the penalty factor for the dis-
tance between i and j. µ is a constant lower than
1.0 that we tune from a set of development data.
It makes the cohesive strength of two text blocks
dramatically decrease when their distance is much
larger than the normal length of a story.
</bodyText>
<subsectionHeader confidence="0.972157">
2.3 Data projection in Laplacian Eigenmaps
</subsectionHeader>
<bodyText confidence="0.9998075">
Given the weight matrix S, we define C as the di-
agonal matrix with its element:
</bodyText>
<equation confidence="0.999405">
K
cij = ∑sij. (2)
i=1
</equation>
<bodyText confidence="0.998081">
Finally, we obtain the Laplacian matrix L, which
is defined as:
</bodyText>
<equation confidence="0.999277">
L = C — S. (3)
</equation>
<bodyText confidence="0.999356">
We use Y = [y1, y2, . . . , yiv] (yi is a column
vector) to indicate the low-dimensional represen-
tation of the latent topic distributions X. The pro-
jection from the latent topic distribution space to
the target space can be defined as:
</bodyText>
<equation confidence="0.681254">
f : xi ==�&apos; yi. (4)
</equation>
<bodyText confidence="0.9987805">
A reasonable criterion for computing an optimal
mapping is to minimize the objective as follows:
</bodyText>
<equation confidence="0.5413">
∥ Yi − Yj ∥2 sij. (5)
</equation>
<bodyText confidence="0.999906">
Under this constraint condition, we can preserve
the local geometrical property in LDA distribu-
tions. The objective function can be transformed
</bodyText>
<equation confidence="0.855557125">
K
∑
j=1
∑K
i=1
191
as:
(yi − yj)sij = tr(YT LY). (6)
</equation>
<bodyText confidence="0.999897714285714">
Meanwhile, zero matrix and matrices with it-
s rank less than K are meaningless solutions for
our task. We impose YTLY = I to prevent this
situation, where I is an identity matrix. By the
Reyleigh-Ritz theorem (Lutkepohl, 1997), the so-
lution can obtained by the Q smallest eigenvalues
of the generalized eigenmaps problem:
</bodyText>
<equation confidence="0.892486">
XLXTy = AXCXTy. (7)
</equation>
<bodyText confidence="0.99954025">
With this formula, we calculate the mapping ma-
trix Y, and its row vectors y′1, y′2, ... , y′Q are in the
order of their eigenvalues A1 ≤ A2 ≤ ... ≤ AQ.
y′i is a Q-dimensional (Q&lt;K) eigenvectors.
</bodyText>
<subsectionHeader confidence="0.989039">
2.4 Story boundary detection
</subsectionHeader>
<bodyText confidence="0.9997678">
In story boundary detection, dynamic program-
ming (DP) approach is adopted to obtain the glob-
al optimal solution. Given the low-dimensional se-
mantic representation of the test data, an objective
function can be defined as follows:
</bodyText>
<equation confidence="0.94359">
∥ yi − yj ∥2), (8)
</equation>
<bodyText confidence="0.999788666666667">
where yi and yj are the latent topic distributions of
text blocks i and j respectively, and ∥ yi − yj ∥2
is the Euclidean distance between them. 5egt in-
dicates these text blocks assigned to a certain hy-
pothesized story. Ns is the number of hypothe-
sized stories.
The story boundaries which minimize the ob-
jective function ℑ in Eq.(8) form the optimal re-
sult. Compared with classical local optimal ap-
proach, DP can more effectively capture the s-
mooth story shifts, and achieve better segmenta-
tion performance.
</bodyText>
<sectionHeader confidence="0.99853" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.999094181818182">
Our experiments were evaluated on the ASR tran-
scripts provided in TDT2 English Broadcast news
corpus1, which involved 1033 news programs. We
separated this corpus into three non-overlapping
sets: a training set of 500 programs for parameter
estimation in topic modeling and LE, a develop-
ment set of 133 programs for empirical tuning and
a test set of 400 programs for performance evalu-
ation.
In the training stage, ASR transcripts with man-
ually labeled boundary tags were provided. Text
</bodyText>
<footnote confidence="0.945578">
1http://projects.ldc.upenn.edu/TDT2/
</footnote>
<bodyText confidence="0.9993974375">
streams were broken into block units according to
the given boundary tags, with each text block be-
ing a complete story. In the segmentation stage,
we divided test data into text blocks using the time
labels of pauses in the transcripts. If the pause du-
ration between two blocks last for more than 1.0
sec, it was considered as a boundary candidate. To
avoid the segmentation being suffered from ASR
errors and the out-of-vocabulary issue, phoneme
bigram was used as the basic term unit (Xie et al.,
2012). Since the ASR transcripts were at word lev-
el, we performed word-to-phoneme conversion to
obtain the phoneme bigram basic units. The fol-
lowing approaches, in which DP was used in story
boundary detection, were evaluated in the experi-
ments:
</bodyText>
<listItem confidence="0.9982393">
• PLSA-DP: PLSA topic distributions were
used to compute sentence cohesive strength.
• LDA-DP: LDA topic distributions were used
to compute sentence cohesive strength.
• PLSA-LE-DP: PLSA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesive strength.
• LDA-LE-DP: LDA topic distributions fol-
lowed by LE projection were used to com-
pute sentence cohesion strength.
</listItem>
<bodyText confidence="0.998501842105263">
For LDA, we used the implementation from
David M. Blei’s webpage2. For PLSA, we used
the Lemur Toolkit3.
F1-measure was used as the evaluation crite-
rion.We followed the evaluation rule: a detected
boundary candidate is considered correct if it lies
within a 15 sec tolerant window on each side of a
reference boundary. A number of parameters were
set through empirical tuning on the developent set.
The penalty factor was set to 0.8. When evaluating
the effects of different size of the training set, the
number of latent topics in topic modeling process
was set to 64. After the number of latent topics
was fixed, the dimensionality after LE projection
was set to 32. When evaluating the effects of d-
ifferent number of latent topics in topic modeling
computation, we fixed the size of the training set
to 500 news programs and changed the number of
latent topics from 16 to 256.
</bodyText>
<sectionHeader confidence="0.997346" genericHeader="method">
4 Experimental results and analysis
</sectionHeader>
<subsectionHeader confidence="0.999501">
4.1 Effect of the size of training dataset
</subsectionHeader>
<bodyText confidence="0.999069">
We used the training set from 100 programs to 500
programs (adding 100 programs in each step) to e-
</bodyText>
<footnote confidence="0.9984925">
2http://www.cs.princeton.edu/ blei/lda-c/
3http://www.lemurproject.org/
</footnote>
<equation confidence="0.841224">
EK
i=1
K
E
j=1
E
(
i,jESegt
N.
E
t=1
ℑ =
</equation>
<page confidence="0.9668">
192
</page>
<figureCaption confidence="0.861617714285714">
valuate the effects of different size of training data
in both PLSA-based and LDA-based approaches.
Figure 1 shows the results on the development set
and the test set.
test data. Moreover, compared with PLSA, the pa-
rameters in LDA do not grow linearly with the size
of the corpus.
</figureCaption>
<figure confidence="0.9452355">
100 200 300 400 500
Number of programs in training data
</figure>
<figureCaption confidence="0.987475">
Figure 1: Segmentation performance with differ-
ent amounts of training data
</figureCaption>
<bodyText confidence="0.999754433333333">
LDA-LE-DP approach achieved the best result
(0.7927 and 0.7860) on both the development and
the test sets, when there were 500 programs in the
training set. This demonstrates that LDA model
and LE projection used in combination is excellent
for the story segmentation task. The LE projection
applied on the latent topic representations made
relatively 9.88% and 10.93% improvement over
the LDA-based approach and the PLSA-based ap-
proach, respectively on the test set. We can reveal
that employing LE on PLSA and LDA topic dis-
tributions achieves much better performance than
the corresponding approaches without using LE.
We have compared the performances between
PLSA and LDA. We found that when the train-
ing data size was small, PLSA performed better
than LDA. Both PLSA-based and LDA-based ap-
proaches got better with the increase in the size of
the training data set. All the four approaches had
similar performances on the development set and
the test set.
With the increase in the size of the training da-
ta, the LDA-based approaches were improved dra-
matically. They even outperformed the PLSA-
based approaches when the training data contained
more than 300 programs. This may be attributed
to the fact that LDA needs more training data to
estimate the parameters. When the training data is
not enough, its parameters estimated in the train-
ing stage is not stable for the development and the
</bodyText>
<subsectionHeader confidence="0.994743">
4.2 Effect of the number of latent topics
</subsectionHeader>
<bodyText confidence="0.998832">
We evaluated the F1-measure of the four ap-
proaches with different number of latent topics
prior to LE projection. Figure 2 shows the cor-
responding results.
</bodyText>
<figure confidence="0.978819">
16 32 48 64 80 96 128 256
Number of latent topics
</figure>
<figureCaption confidence="0.9874405">
Figure 2: Segmentation performance with differ-
ent numbers of latent topics
</figureCaption>
<bodyText confidence="0.999798882352941">
The best performances (0.7816-0.7847) were
achieved at the number of latent topics between
64 and 96. When the number of latent topics was
increased from 16 to 64, F1-measure increased.
When the number of latent topics was larger than
96, F1-measure decreased gradually. We found
that the best results were achieved when the num-
ber of topics was close to the real number of top-
ics. There are 80 manually labeled main topics in
the test set.
We observe that LE projection makes the topic
model more stable with different numbers of latent
topics. The best and the worst performances dif-
fered by relatively 9.12% in LDA-DP and 7.97%
in PLSA-DP. However, the relative difference of
2.79% and 2.46% were observed in LDA-LE-DP
and PLSA-LE-DP respectively.
</bodyText>
<sectionHeader confidence="0.996113" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999777888888889">
Our proposed approach achieves the best F1-
measure of 0.7860. In the task of story segmen-
tation, we believe that LDA can avoid data overfit-
ting problem when there is a sufficient amount of
training data. This is also applicable to LDA-LE-
LP. Moreover, we find that when we apply LE pro-
jection to latent topic distributions, the segmen-
tation performances become less sensitive to the
predefined number of latent topics.
</bodyText>
<figure confidence="0.999222833333333">
0.8
LDA-LE-DP
0.75
PLSA-LE-DP
LDA-DP
PLSA-DP
0.6
0.55
100 200
Development Set
300 400 500
F1-measure
0.7
0.65
F1-measure
0.75
0.65
0.55
0.8
0.7
0.6
LDA-LE-DP
PLSA-LE-DP
LDA-DP
PLSA-DP
Test Set
LDA-LE-DP
PLSA-DP
0.66
0.64
0.62
0.6
PLSA-LE-DP
0.8
F1-measure
0.72
0.7
0.68
LDA-DP
0.78
0.76
0.74
</figure>
<page confidence="0.996303">
193
</page>
<sectionHeader confidence="0.998206" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9876512">
This work is supported by the National Natu-
ral Science Foundation of China (61175018), the
Natural Science Basic Research Plan of Shaanx-
i Province (2011JM8009) and the Fok Ying Tung
Education Foundation (131059).
</bodyText>
<sectionHeader confidence="0.998186" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999751677083333">
J. Allan. 2002. Topic Detection and Tracking: Event-
Based Information Organization. Kluwer Academic
Publisher, Norwell, MA.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A Model of Lexical Attraction and repulsion.
In Proceedings of the 8th Conference on European
Chapter of the Association for Computational Lin-
guistics (EACL), pp.373-380.
Freddy Y. Y. Choi. 2000. Advances in Domain In-
dependent Linear Text Segmentation. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference
(NAACL), pp.26-33.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pp.20-57.
Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
Haizhou Li. 2011. Probabilistic Latent Seman-
tic Analysis for Broadcast New Story Segmentation.
In Proceedings of the 11th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pp.1109-1112.
David M. Blei. 2012. Probabilistic topic models.
Communication of the ACM, vol. 55, pp.77-84.
David M. Blei, Andrew Y. Ng, Michael I. Jordan.
2003. Latent Dirichlet Allocation. the Journal of
Machine Learning Research, vol. 3, pp.993-1022.
Marti A. Hearst. 1997. TextTiling: Segmenting Text
into Multiparagraph subtopic passages. Computa-
tional Liguistic, vol. 23, pp.33-64.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke,
Elizabeth Shriberg. 2001. Integrating Prosodic
and Lexicial Cues for Automatic Topic Segmenta-
tion. Computational Liguistic, vol. 27, pp.31-57.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
Segmentation of Broadcast News in English, Man-
darin and Aribic. In Proceedings of the 7th North
American Chapter of the Association for Compu-
tational Linguistics Conference (NAACL), pp.125-
128.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with An Aspect Hidden Markov Model. In
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrival (SIGIR), pp.343-348.
Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Au-
tomatic Story Segmentation Using a Bayesian De-
cision Framwork for Statistical Models of Lexical
Chain Feature. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp.357-364.
Igor Malioutov and Regina Barzilay. 2006. Minimum
Cut Model for Spoken Lecture Segmenation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp.25-32.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna
Moore. 2001. Latent Semantic Analysis for Tex-
t Segmentation. In Proceedings of the 2001 Con-
ference on Empirical Methods on Natural Language
Processing (EMNLP), pp.109-117.
Rachit Arora and Balaraman Ravindran. 2008. Latent
Dirichlet Allocation Based Multi-document Summa-
rization. In Proceedings of the 2nd Workshop on
Analytics for Noisy Unstructured Text Data (AND),
pp.91-97.
David Hall, Daniel Jurafsky, Christopher D. Manning.
2008. Latent Studying the History Ideas Using Topic
Models. In Proceedings of the 2008 Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP), pp.363-371.
Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008.
Text Segmentation with LDA-based Fisher Kernel.
In Proceedings of the 46th Annual Meeting of the As-
socation for Computational Linguistics on Human
Language Technologies (HLT-ACL), pp.269-272.
Mikhail Belkin and Partha Niyogi. 2002. Laplacian
Eigenmaps for Dimensionality Reduction and Da-
ta Representation. Neural Computation, vol. 15,
pp.1383-1396.
Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang.
2012. Laplacian Eigenmaps for Automatic Story
Segmentation of Broadcast News. IEEE Transaction
on Audio, Speech and Language Processing, vol. 20,
pp.264-277.
Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang
Zhai. 2008. Modeling Hidden Topics on Document
Manifold. In Proceedings of the 17th ACM Confer-
ence on Information and Knowledge Managemen-
t (CIKM), pp.911-120.
Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma,
and Haizhou Li. 2013. Broadcast News Story Seg-
mentation Using Latent Topics on Data Manifold. In
Proceedings of the 38th International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
</reference>
<page confidence="0.986753">
194
</page>
<reference confidence="0.9997997">
J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van
Mulbregt. 1999. A Hidden Markov Model Approach
to Text Segmenation and Event Tracking. In Pro-
ceedings of the 1999 International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
pp.333-336.
Martin Riedl and Chris Biemann. 2012. Text Segmen-
tation with Topic Models. the Journal for Language
Technology and Computational Linguistics, pp.47-
69.
P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dy-
namic Programming algorithm for Linear Text Story
Segmentation. the Joural of Intelligent Information
Systems, vol. 23, pp.179-197.
H. Lutkepohl. 1997. Handbook of Matrices. Wiley,
Chichester, UK.
Jen-Tzung Chien and Chuang-Hua Chueh. 2012.
Topic-Based Hieraachical Segmentation. IEEE
Transaction on Audio, Speech and Language Pro-
cessing, vol. 20, pp.55-66.
</reference>
<page confidence="0.998943">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761348">
<title confidence="0.9989105">Broadcast News Story Segmentation Using Manifold Learning on Topic Distributions</title>
<author confidence="0.998261">Lei Cheung-Chi Bin Haizhou</author>
<affiliation confidence="0.999806">of Computer Science, Northwestern Polytechnical University,</affiliation>
<address confidence="0.784894">for Infocomm Research, Singapore</address>
<email confidence="0.994103">lxie@nwpu.edu.cn,</email>
<abstract confidence="0.999005347826087">We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental results show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Topic Detection and Tracking: EventBased Information Organization.</title>
<date>2002</date>
<publisher>Kluwer Academic Publisher,</publisher>
<location>Norwell, MA.</location>
<contexts>
<context position="1417" citStr="Allan, 2002" startWordPosition="191" endWordPosition="192">We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental results show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860. 1 Introduction Story segmentation refers to partitioning a multimedia stream into homogenous segments each embodying a main topic or coherent story (Allan, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded. Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006; *corresponding author Lo et al., 2009; Malioutov</context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>J. Allan. 2002. Topic Detection and Tracking: EventBased Information Organization. Kluwer Academic Publisher, Norwell, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>A Model of Lexical Attraction and repulsion.</title>
<date>1997</date>
<booktitle>In Proceedings of the 8th Conference on European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>373--380</pages>
<contexts>
<context position="1909" citStr="Beeferman et al., 1997" startWordPosition="261" endWordPosition="264">on refers to partitioning a multimedia stream into homogenous segments each embodying a main topic or coherent story (Allan, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded. Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006; *corresponding author Lo et al., 2009; Malioutov and Barzilay, 2006; Yamron et al., 1999; Tur et al., 2001). In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. Lexical cues are extracted from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while t</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1997. A Model of Lexical Attraction and repulsion. In Proceedings of the 8th Conference on European Chapter of the Association for Computational Linguistics (EACL), pp.373-380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in Domain Independent Linear Text Segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL),</booktitle>
<pages>26--33</pages>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in Domain Independent Linear Text Segmentation. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL), pp.26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>20--57</pages>
<contexts>
<context position="2840" citStr="Hofmann, 1999" startWordPosition="406" endWordPosition="408">ed from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considered. To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task (Hearst, 1997). Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirich</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pp.20-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mimi Lu</author>
<author>Cheung-Chi Leung</author>
<author>Lei Xie</author>
<author>Bin Ma</author>
<author>Haizhou Li</author>
</authors>
<title>Probabilistic Latent Semantic Analysis for Broadcast New Story Segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH),</booktitle>
<pages>1109--1112</pages>
<contexts>
<context position="3108" citStr="Lu et al., 2011" startWordPosition="448" endWordPosition="451">eration, while term association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considered. To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task (Hearst, 1997). Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that </context>
</contexts>
<marker>Lu, Leung, Xie, Ma, Li, 2011</marker>
<rawString>Mimi Lu, Cheung-Chi Leung, Lei Xie, Bin Ma, Haizhou Li. 2011. Probabilistic Latent Semantic Analysis for Broadcast New Story Segmentation. In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), pp.1109-1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2012</date>
<journal>Communication of the ACM,</journal>
<volume>55</volume>
<pages>77--84</pages>
<contexts>
<context position="3400" citStr="Blei, 2012" startWordPosition="501" endWordPosition="502">tic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August </context>
</contexts>
<marker>Blei, 2012</marker>
<rawString>David M. Blei. 2012. Probabilistic topic models. Communication of the ACM, vol. 55, pp.77-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>the Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="3480" citStr="Blei et al., 2003" startWordPosition="513" endWordPosition="516"> and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (</context>
<context position="5432" citStr="Blei et al., 2003" startWordPosition="822" endWordPosition="825">aches. Another related work (Lu et al., 2013) is to use local geometric information to regularize the log-likelihood computation in PLSA. 2 Our Proposed Approach In this paper, we propose to apply LE on the LDA topic distributions, each of which is estimated from a text block. The low-dimensional vectors obtained by LE projection are used to detect story boundaries through dynamic programming. Moreover, as in (Xie et al., 2012), we incorporate the temporal distances between block pairs as a penalty factor in the weight matrix. 2.1 Latent Dirichlet Allocation Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus. It considers that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over terms. In LDA, given a corpus D = {d1, d2, ... , dM} and a set of terms W = (w1, w2, ... , wV ), the generative process can be summarized as follows: 1) For each document d, pick a multinomial distribution θ from a Dirichlet distribution parameter α, denoted as θ — Dir(α). 2) For each term w in document d, select a topic z from the multinomial distribution θ, denoted as z — Multinomial(θ). 3) Select a term </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003. Latent Dirichlet Allocation. the Journal of Machine Learning Research, vol. 3, pp.993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting Text into Multiparagraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Liguistic,</journal>
<volume>23</volume>
<pages>33--64</pages>
<contexts>
<context position="1935" citStr="Hearst, 1997" startWordPosition="267" endWordPosition="268">ia stream into homogenous segments each embodying a main topic or coherent story (Allan, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded. Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006; *corresponding author Lo et al., 2009; Malioutov and Barzilay, 2006; Yamron et al., 1999; Tur et al., 2001). In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. Lexical cues are extracted from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting Text into Multiparagraph subtopic passages. Computational Liguistic, vol. 23, pp.33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
</authors>
<title>Dilek Hakkani-Tur, Andreas Stolcke, Elizabeth Shriberg.</title>
<date>2001</date>
<journal>Computational Liguistic,</journal>
<volume>27</volume>
<pages>31--57</pages>
<marker>Tur, 2001</marker>
<rawString>Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, Elizabeth Shriberg. 2001. Integrating Prosodic and Lexicial Cues for Automatic Topic Segmentation. Computational Liguistic, vol. 27, pp.31-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Story Segmentation of Broadcast News in English, Mandarin and Aribic.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th North American Chapter of the Association for Computational Linguistics Conference (NAACL),</booktitle>
<pages>125--128</pages>
<contexts>
<context position="1967" citStr="Rosenberg and Hirschberg, 2006" startWordPosition="269" endWordPosition="272"> homogenous segments each embodying a main topic or coherent story (Allan, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded. Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006; *corresponding author Lo et al., 2009; Malioutov and Barzilay, 2006; Yamron et al., 1999; Tur et al., 2001). In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. Lexical cues are extracted from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. Moreover, </context>
</contexts>
<marker>Rosenberg, Hirschberg, 2006</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2006. Story Segmentation of Broadcast News in English, Mandarin and Aribic. In Proceedings of the 7th North American Chapter of the Association for Computational Linguistics Conference (NAACL), pp.125-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Pedro J Moreno</author>
</authors>
<title>Topic Segmentation with An Aspect Hidden Markov Model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrival (SIGIR),</booktitle>
<pages>343--348</pages>
<contexts>
<context position="3132" citStr="Blei and Moreno, 2001" startWordPosition="452" endWordPosition="455">rm association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considered. To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task (Hearst, 1997). Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>David M. Blei and Pedro J. Moreno. 2001. Topic Segmentation with An Aspect Hidden Markov Model. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrival (SIGIR), pp.343-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wai-Kit Lo</author>
<author>Wenying Xiong</author>
<author>Helen Meng</author>
</authors>
<title>Automatic Story Segmentation Using a Bayesian Decision Framwork for Statistical Models of Lexical Chain Feature.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>357--364</pages>
<contexts>
<context position="2006" citStr="Lo et al., 2009" startWordPosition="275" endWordPosition="278">herent story (Allan, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded. Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006; *corresponding author Lo et al., 2009; Malioutov and Barzilay, 2006; Yamron et al., 1999; Tur et al., 2001). In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. Lexical cues are extracted from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considere</context>
</contexts>
<marker>Lo, Xiong, Meng, 2009</marker>
<rawString>Wai-Kit Lo, Wenying Xiong, Helen Meng. 2009. Automatic Story Segmentation Using a Bayesian Decision Framwork for Statistical Models of Lexical Chain Feature. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL), pp.357-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum Cut Model for Spoken Lecture Segmenation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2036" citStr="Malioutov and Barzilay, 2006" startWordPosition="279" endWordPosition="282">an, 2002). With the explosive growth of multimedia data, it becomes difficult to retrieve the most relevant components. For indexing broadcast news programs, it is desirable to divide each of them into a number of independent stories. Manual segmentation is accurate but labor-intensive and costly. Therefore, automatic story segmentation approaches are highly demanded. Lexical-cohesion based approaches have been widely studied for automatic broadcast news story segmentation (Beeferman et al., 1997; Choi, 1999; Hearst, 1997; Rosenberg and Hirschberg, 2006; *corresponding author Lo et al., 2009; Malioutov and Barzilay, 2006; Yamron et al., 1999; Tur et al., 2001). In this kind of approaches, the audio portion of the data stream is passed to an automatic speech recognition (ASR) system. Lexical cues are extracted from the ASR transcripts. Lexical cohesion is the phenomenon that different stories tend to employ different sets of terms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considered. To deal with these problems</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum Cut Model for Spoken Lecture Segmenation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), pp.25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Juhanna Moore</author>
</authors>
<title>Latent Semantic Analysis for Text Segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods on Natural Language Processing (EMNLP),</booktitle>
<pages>109--117</pages>
<contexts>
<context position="2967" citStr="Choi et al., 2001" startWordPosition="427" endWordPosition="430">ms. Term repetition is one of the most common appearances. These rigid lexical-cohesion based approaches simply take term repetition into consideration, while term association in lexical cohesion is ignored. Moreover, polysemy and synonymy are not considered. To deal with these problems, some topic model techniques which provide conceptual level matching have been introduced to text and story segmentation task (Hearst, 1997). Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) is a typical instance and used widely. PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al., 2001), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Aro</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Freddy Y. Y. Choi, Peter Wiemer-Hastings, Juhanna Moore. 2001. Latent Semantic Analysis for Text Segmentation. In Proceedings of the 2001 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp.109-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachit Arora</author>
<author>Balaraman Ravindran</author>
</authors>
<title>Latent Dirichlet Allocation Based Multi-document Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Workshop on Analytics for Noisy Unstructured Text Data (AND),</booktitle>
<pages>91--97</pages>
<contexts>
<context position="3589" citStr="Arora and Ravindran, 2008" startWordPosition="532" endWordPosition="535">01), and offers a more solid statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated </context>
</contexts>
<marker>Arora, Ravindran, 2008</marker>
<rawString>Rachit Arora and Balaraman Ravindran. 2008. Latent Dirichlet Allocation Based Multi-document Summarization. In Proceedings of the 2nd Workshop on Analytics for Noisy Unstructured Text Data (AND), pp.91-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Latent Studying the History Ideas Using Topic Models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods on Natural Language Processing (EMNLP),</booktitle>
<pages>363--371</pages>
<contexts>
<context position="3608" citStr="Hall et al., 2008" startWordPosition="536" endWordPosition="539">d statistical foundation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality redu</context>
</contexts>
<marker>Hall, Jurafsky, Manning, 2008</marker>
<rawString>David Hall, Daniel Jurafsky, Christopher D. Manning. 2008. Latent Studying the History Ideas Using Topic Models. In Proceedings of the 2008 Conference on Empirical Methods on Natural Language Processing (EMNLP), pp.363-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Sun</author>
<author>Runxin Li</author>
<author>Dingsheng Luo</author>
<author>Xihong Wu</author>
</authors>
<title>Text Segmentation with LDA-based Fisher Kernel.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Assocation for Computational Linguistics on Human Language Technologies (HLT-ACL),</booktitle>
<pages>269--272</pages>
<contexts>
<context position="3626" citStr="Sun et al., 2008" startWordPosition="540" endWordPosition="543">ation. PLSA provides more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality reduction method. It p</context>
</contexts>
<marker>Sun, Li, Luo, Wu, 2008</marker>
<rawString>Qi Sun, Runxin Li, Dingsheng Luo, Xihong Wu. 2008. Text Segmentation with LDA-based Fisher Kernel. In Proceedings of the 46th Annual Meeting of the Assocation for Computational Linguistics on Human Language Technologies (HLT-ACL), pp.269-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
</authors>
<title>Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>15</volume>
<pages>1383--1396</pages>
<contexts>
<context position="3836" citStr="Belkin and Niyogi, 2002" startWordPosition="573" endWordPosition="576">n PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality reduction method. It projects data into a low-dimensional representation while preserving the intrinsic local geometric structure information (Belkin and Niyogi, 2002). The locality preserving property attempts to make the lowdimens</context>
</contexts>
<marker>Belkin, Niyogi, 2002</marker>
<rawString>Mikhail Belkin and Partha Niyogi. 2002. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation, vol. 15, pp.1383-1396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Xie</author>
<author>Lilei Zheng</author>
<author>Zihan Liu</author>
<author>Yanning Zhang</author>
</authors>
<title>Laplacian Eigenmaps for Automatic Story Segmentation of Broadcast News.</title>
<date>2012</date>
<journal>IEEE Transaction on Audio, Speech and Language Processing,</journal>
<volume>20</volume>
<pages>264--277</pages>
<contexts>
<context position="3855" citStr="Xie et al., 2012" startWordPosition="577" endWordPosition="580">h the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality reduction method. It projects data into a low-dimensional representation while preserving the intrinsic local geometric structure information (Belkin and Niyogi, 2002). The locality preserving property attempts to make the lowdimensional data represen</context>
<context position="5245" citStr="Xie et al., 2012" startWordPosition="794" endWordPosition="797">frequencies to represent text blocks is studied in this paper. We study the effects of the size of training data and the number of latent topics on the LDA-based and the PLSA-based approaches. Another related work (Lu et al., 2013) is to use local geometric information to regularize the log-likelihood computation in PLSA. 2 Our Proposed Approach In this paper, we propose to apply LE on the LDA topic distributions, each of which is estimated from a text block. The low-dimensional vectors obtained by LE projection are used to detect story boundaries through dynamic programming. Moreover, as in (Xie et al., 2012), we incorporate the temporal distances between block pairs as a penalty factor in the weight matrix. 2.1 Latent Dirichlet Allocation Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a generative probabilistic model of a corpus. It considers that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over terms. In LDA, given a corpus D = {d1, d2, ... , dM} and a set of terms W = (w1, w2, ... , wV ), the generative process can be summarized as follows: 1) For each document d, pick a multinomial distribution θ from a Dirichle</context>
<context position="10676" citStr="Xie et al., 2012" startWordPosition="1749" endWordPosition="1752">ge, ASR transcripts with manually labeled boundary tags were provided. Text 1http://projects.ldc.upenn.edu/TDT2/ streams were broken into block units according to the given boundary tags, with each text block being a complete story. In the segmentation stage, we divided test data into text blocks using the time labels of pauses in the transcripts. If the pause duration between two blocks last for more than 1.0 sec, it was considered as a boundary candidate. To avoid the segmentation being suffered from ASR errors and the out-of-vocabulary issue, phoneme bigram was used as the basic term unit (Xie et al., 2012). Since the ASR transcripts were at word level, we performed word-to-phoneme conversion to obtain the phoneme bigram basic units. The following approaches, in which DP was used in story boundary detection, were evaluated in the experiments: • PLSA-DP: PLSA topic distributions were used to compute sentence cohesive strength. • LDA-DP: LDA topic distributions were used to compute sentence cohesive strength. • PLSA-LE-DP: PLSA topic distributions followed by LE projection were used to compute sentence cohesive strength. • LDA-LE-DP: LDA topic distributions followed by LE projection were used to c</context>
</contexts>
<marker>Xie, Zheng, Liu, Zhang, 2012</marker>
<rawString>Lei Xie, Lilei Zheng, Zihan Liu and Yanning Zhang. 2012. Laplacian Eigenmaps for Automatic Story Segmentation of Broadcast News. IEEE Transaction on Audio, Speech and Language Processing, vol. 20, pp.264-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deng Cai</author>
<author>Qiaozhu Mei</author>
<author>Jiawei Han</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Modeling Hidden Topics on Document Manifold.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>911--120</pages>
<marker>Cai, Mei, Han, Zhai, 2008</marker>
<rawString>Deng Cai, Qiaozhu Mei, Jiawei Han, and Chengxiang Zhai. 2008. Modeling Hidden Topics on Document Manifold. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM), pp.911-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoming Lu</author>
<author>Cheung-Chi Leung</author>
<author>Lei Xie</author>
<author>Bin Ma</author>
<author>Haizhou Li</author>
</authors>
<title>Broadcast News Story Segmentation Using Latent Topics on Data Manifold.</title>
<date>2013</date>
<booktitle>In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="4859" citStr="Lu et al., 2013" startWordPosition="729" endWordPosition="732"> a low-dimensional representation while preserving the intrinsic local geometric structure information (Belkin and Niyogi, 2002). The locality preserving property attempts to make the lowdimensional data representation more robust to the noise from ASR errors (Xie et al., 2012). To further improve the segmentation performance, using latent topic distributions and LE instead of term frequencies to represent text blocks is studied in this paper. We study the effects of the size of training data and the number of latent topics on the LDA-based and the PLSA-based approaches. Another related work (Lu et al., 2013) is to use local geometric information to regularize the log-likelihood computation in PLSA. 2 Our Proposed Approach In this paper, we propose to apply LE on the LDA topic distributions, each of which is estimated from a text block. The low-dimensional vectors obtained by LE projection are used to detect story boundaries through dynamic programming. Moreover, as in (Xie et al., 2012), we incorporate the temporal distances between block pairs as a penalty factor in the weight matrix. 2.1 Latent Dirichlet Allocation Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a generative probabilis</context>
</contexts>
<marker>Lu, Leung, Xie, Ma, Li, 2013</marker>
<rawString>Xiaoming Lu, Cheung-Chi Leung, Lei Xie, Bin Ma, and Haizhou Li. 2013. Broadcast News Story Segmentation Using Latent Topics on Data Manifold. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Yamron</author>
<author>I Carp</author>
<author>L Gillick</author>
<author>S Lowe</author>
<author>P van Mulbregt</author>
</authors>
<title>A Hidden Markov Model Approach to Text Segmenation and Event Tracking.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>333--336</pages>
<marker>Yamron, Carp, Gillick, Lowe, van Mulbregt, 1999</marker>
<rawString>J. P. Yamron, I. Carp, L. Gillick, S. Lowe, and P. van Mulbregt. 1999. A Hidden Markov Model Approach to Text Segmenation and Event Tracking. In Proceedings of the 1999 International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pp.333-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Riedl</author>
<author>Chris Biemann</author>
</authors>
<title>Text Segmentation with Topic Models.</title>
<date>2012</date>
<booktitle>the Journal for Language Technology and Computational Linguistics,</booktitle>
<pages>47--69</pages>
<contexts>
<context position="3651" citStr="Riedl and Biemann, 2012" startWordPosition="544" endWordPosition="547">es more significant improvement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality reduction method. It projects data into a low-d</context>
</contexts>
<marker>Riedl, Biemann, 2012</marker>
<rawString>Martin Riedl and Chris Biemann. 2012. Text Segmentation with Topic Models. the Journal for Language Technology and Computational Linguistics, pp.47-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kehagias</author>
</authors>
<title>A Dynamic Programming algorithm for Linear Text Story Segmentation.</title>
<date>2002</date>
<journal>the Joural of Intelligent Information Systems,</journal>
<volume>23</volume>
<pages>179--197</pages>
<marker>Kehagias, 2002</marker>
<rawString>P. Fragkou , V. Petridis , Ath. Kehagias. 2002. A Dynamic Programming algorithm for Linear Text Story Segmentation. the Joural of Intelligent Information Systems, vol. 23, pp.179-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lutkepohl</author>
</authors>
<title>Handbook of Matrices.</title>
<date>1997</date>
<publisher>Wiley, Chichester, UK.</publisher>
<contexts>
<context position="8530" citStr="Lutkepohl, 1997" startWordPosition="1387" endWordPosition="1388">tribution space to the target space can be defined as: f : xi ==�&apos; yi. (4) A reasonable criterion for computing an optimal mapping is to minimize the objective as follows: ∥ Yi − Yj ∥2 sij. (5) Under this constraint condition, we can preserve the local geometrical property in LDA distributions. The objective function can be transformed K ∑ j=1 ∑K i=1 191 as: (yi − yj)sij = tr(YT LY). (6) Meanwhile, zero matrix and matrices with its rank less than K are meaningless solutions for our task. We impose YTLY = I to prevent this situation, where I is an identity matrix. By the Reyleigh-Ritz theorem (Lutkepohl, 1997), the solution can obtained by the Q smallest eigenvalues of the generalized eigenmaps problem: XLXTy = AXCXTy. (7) With this formula, we calculate the mapping matrix Y, and its row vectors y′1, y′2, ... , y′Q are in the order of their eigenvalues A1 ≤ A2 ≤ ... ≤ AQ. y′i is a Q-dimensional (Q&lt;K) eigenvectors. 2.4 Story boundary detection In story boundary detection, dynamic programming (DP) approach is adopted to obtain the global optimal solution. Given the low-dimensional semantic representation of the test data, an objective function can be defined as follows: ∥ yi − yj ∥2), (8) where yi an</context>
</contexts>
<marker>Lutkepohl, 1997</marker>
<rawString>H. Lutkepohl. 1997. Handbook of Matrices. Wiley, Chichester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Tzung Chien</author>
<author>Chuang-Hua Chueh</author>
</authors>
<title>Topic-Based Hieraachical Segmentation.</title>
<date>2012</date>
<journal>IEEE Transaction on Audio, Speech and Language Processing,</journal>
<volume>20</volume>
<pages>55--66</pages>
<contexts>
<context position="3675" citStr="Chien and Chueh, 2012" startWordPosition="548" endWordPosition="551">vement than LSA for story segmentation (Lu et al., 2011; Blei and Moreno, 2001). Despite the success of PLSA, there are concerns that the number of parameters in PLSA grows linearly with the size of the corpus. This makes PLSA not desirable if there is a considerable amount of data available, and causes serious over-fitting problems (Blei, 2012). To deal with this issue, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been proposed. LDA has been proved to be effective in many segmentation tasks (Arora and Ravindran, 2008; Hall et al., 2008; Sun et al., 2008; Riedl and Biemann, 2012; Chien and Chueh, 2012). Recent studies have shown that intrinsic dimensionality of natural text corpus is significantly lower than its ambient Euclidean space (Belkin and Niyogi, 2002; Xie et al., 2012). Therefore, 190 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 190–195, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Laplacian Eigenmaps (LE) was proposed to compute corresponding natural low-dimensional structure. LE is a geometrically motivated dimensionality reduction method. It projects data into a low-dimensional representatio</context>
</contexts>
<marker>Chien, Chueh, 2012</marker>
<rawString>Jen-Tzung Chien and Chuang-Hua Chueh. 2012. Topic-Based Hieraachical Segmentation. IEEE Transaction on Audio, Speech and Language Processing, vol. 20, pp.55-66.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>