<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.980823">
String Transformation Learning
</title>
<author confidence="0.951753">
Giorgio Satta
</author>
<affiliation confidence="0.5570885">
Dipartimento di Elettronica e Informatica
Universita di Padova
</affiliation>
<address confidence="0.8844285">
via Gradenigo, 6/A
1-35131 Padova, Italy
</address>
<email confidence="0.793867">
sattadei.unipd.it
</email>
<author confidence="0.993046">
John C. Henderson
</author>
<affiliation confidence="0.925791">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.905453">
Baltimore, MD 21218-2694
</address>
<email confidence="0.994514">
jhndrsn@cs. jhu.edu
</email>
<sectionHeader confidence="0.979596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943133333333">
String transformation systems have been
introduced in (Brill, 1995) and have sev-
eral applications in natural language pro-
cessing. In this work we consider the com-
putational problem of automatically learn-
ing from a given corpus the set of transfor-
mations presenting the best evidence. We
introduce an original data structure and
efficient algorithms that learn some fam-
ilies of transformations that are relevant
for part-of-speech tagging and phonologi-
cal rule systems. We also show that the
same learning problem becomes NP-hard
in cases of an unbounded use of don&apos;t care
symbols in a transformation.
</bodyText>
<sectionHeader confidence="0.996297" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979743589743">
Ordered sequences of rewriting rules are used in
several applications in natural language process-
ing, including phonological and morphological sys-
tems (Kaplan and Kay, 1994), morphological disam-
biguation, part-of-speech tagging and shallow syn-
tactic parsing (Brill, 1995), (Karlsson et al., 1995).
In (Brill, 1995) a learning paradigm, called error-
driven learning, has been introduced for automatic
induction of a specific kind of rewriting rules called
transformations, and it has been shown that the
achieved accuracy of the resulting transformation
systems is competitive with that of existing systems.
In this work we further elaborate on the error-
driven learning paradigm. Our main contribution
is summarized in what follows. We consider some
families of transformations and design efficient al-
gorithms for the associated learning problem that
improve existing methods. Our results are achieved
by exploiting a data structure originally introduced
in this work. This allows us to simultaneously repre-
sent and test the search space of all possible transfor-
mations. The transformations we investigate make
use of classes of symbols, in order to generalize regu-
larities in rule applications. We also show that when
an unbounded number of these symbol classes are al-
lowed within a transformation, then the associated
learning problem becomes NP-hard.
The notation we use in the remainder of the paper
is briefly introduced here. E denotes a fixed, finite
alphabet and c the null string. E* and E+ are the
set of all strings and all non-null strings over E, re-
spectively. Let w E E*. We denote by lw I the length
of w. Let w = uxv; u is a prefix and v is a suf-
fix of w; when x is non-null, it is called a factor of
w. The suffix of w of length i is denoted suffi(w),
for 0 &lt; i &lt; I wl. Assume that x is non-null, and
w = uixsuffi(w) for yo &gt; 0 different values of i but
not for so + 1, or x is not a factor of w and so = 0.
Then we say that so is the statistic of factor x in w.
</bodyText>
<sectionHeader confidence="0.580539" genericHeader="method">
2 The learning paradigm
</sectionHeader>
<bodyText confidence="0.998843230769231">
The learning paradigm we adopt is called error-
driven learning and has been originally proposed
in (Brill, 1995) for part of speech tagging applica-
tions. We briefly introduce here the basic assump-
tions of the approach.
A string transformation is a rewriting rule de-
noted as u v, where u and v are strings such that
Iu = Iv . This means that if u appears as a factor of
some string w, then u should be replaced by v in w.
The application of the transformation might be con-
ditioned by the requirement that some additionally
specified pattern matches some part of the string w
to be rewritten.
We now describe how transformations can be au-
tomatically learned. A pair of strings (w, w&apos;) is an
aligned pair if I w I = WI. When w = uxsuffi(w),
WI = u&apos;x&apos;suffi(0) and Ix = x&apos;, we say that fac-
tors x and x&apos; occur at aligned positions within
(w, w&apos;). A multi-set of aligned pairs is called an
aligned corpus. Let (w, w&apos;) be an aligned pair and
let r be some transformation of the form u v.
The positive evidence of r (w.r.t. (w, w&apos;)) is the
number of different positions at which factors it and
v are aligned within (w, w&apos;). The negative evi-
dence of 7 (w.r.t. w, w&apos;) is the number of different
positions at which factors it and u are aligned within
</bodyText>
<page confidence="0.998404">
444
</page>
<figureCaption confidence="0.767886">
Figure 1: Trie and suffix tree for string w = accbacac$. Pair [i, j] denotes the factor of w starting at position
i and ending at position j (hence [1,2] denotes ac).
</figureCaption>
<bodyText confidence="0.9988618">
(w, w&apos;). Intuitively speaking, positive (negative) ev-
idence is a count of how many times we will do well
(badly, respectively) when using 7 on w in trying to
get w&apos;. The score associated with T is the differ-
ence between the positive evidence and the negative
evidence of r. This extends to an aligned corpus
in the obvious way. We are interested in the set of
transformations that are associated with the high-
est score in a given aligned corpus, and will develop
algorithms to find such a set in the next sections.
</bodyText>
<sectionHeader confidence="0.986968" genericHeader="method">
3 Data Structures
</sectionHeader>
<bodyText confidence="0.999908">
This section introduces two data structures that are
basic to the development of the algorithms presented
in this paper.
</bodyText>
<subsectionHeader confidence="0.998763">
3.1 Suffix trees
</subsectionHeader>
<bodyText confidence="0.999828538461539">
We briefly present here a data structure that is
well known in the text processing literature; the
reader is referred to (Crochemore and Rytter, 1994)
and (Apostolic°, 1985) for definitions and further
references.
Let w be some non-null string. Throughout the
paper we assume that the rightmost symbol of w is
an end-marker not found at any other position in
the string. The suffix tree associated with w is a
&amp;quot;compressed&amp;quot; trie of all strings suffi(w), 1 &lt; i &lt; I w I.
Edges are labeled by factors of w which are encoded
by means of two natural numbers denoting endpoints
in the string. An example is reported in Figure 1.
An implicit node is a node not explicitly repre-
sented in the suffix tree, that splits the label of some
edge at a given position. (Each implicit node cor-
responds to some node in the original trie having
only one child.) We denote by parent(p) the parent
node of (implicit) node p and by label(p, q) the la-
bel of the edge spanning (implicit) nodes p and q.
Throughout the paper, we take the dominance rela-
tion between nodes to be reflexive, unless we write
proper dominance. We also say that implicit node
q immediately dominates node p if q splits the arc
between parent(p) and p. Of main interest here are
the following properties of suffix trees:
</bodyText>
<listItem confidence="0.95927775">
• if node p has children pl, ,p, then d &gt; 2 and
strings label(p, pi) differ one from the other at
the leftmost symbol;
• all and only the factors of w are represented by
paths from the root to some (implicit) node;
• the statistic of factor u of w is the number of
leaves dominated by the (implicit) node ending
the path representing u.
</listItem>
<bodyText confidence="0.999636958333333">
In the remainder of the paper, we sometimes identify
an (implicit) node of a suffix tree with the factor
represented by the path from the root to that node.
The suffix tree and the statistics of all factors of
w can be constructed/computed in time ()(Iwl), as
reported in (Weiner, 1973) and (McCreight, 1976).
McCreight algorithm uses two basic functions to
scan paths in the suffix tree under construction.
These functions are briefly introduced here and will
be exploited in the next subsection. Below, p is a
node in a tree and u is a non-null string.
function Slow_scan(p, u): Starting at p, scan u sym-
bol by symbol. Return the (implicit) node corre-
sponding to the last matching symbol.
The next function runs faster than Slow_scan, and
can be used whenever we already know that u is an
(implicit) node in the tree (u completely matches
some path in the tree).
function Fast_scan(p, u): Starting at p, scan u by
iteratively (i) finding the edge between the current
node and one of its children, that has the same first
symbol as the suffix of u yet to be scanned, and
(ii) skipping a prefix of u equal to the length of the
selected edge label. Return the (implicit) node u.
</bodyText>
<page confidence="0.998372">
445
</page>
<figure confidence="0.982285375">
[1,2
2
[1,1
[3,9]
[8,9] [7,9]
[8.9] [6.9]
[9,9]
[9,9]
</figure>
<figureCaption confidence="0.9717305">
Figure 2: Suffix tree alignment for strings w accbacad, w&apos; = acabacba$ and the identity homomorphism
h(a) = a, h(b) = b, h(c) = c. Each a-link is denoted by indexing the incident nodes with the same integer
number; if the incident node is an implicit node, then we add between parentheses the relative position w.r.t.
the arc label.
</figureCaption>
<bodyText confidence="0.9995688">
From each node au in the suffix tree, au some factor,
McCreight&apos;s algorithm creates a pointer, called an s-
link, to node u which necessarily exists in the suffix
tree. We write q = s-link(p) if there is an s-link from
p to q.
</bodyText>
<subsectionHeader confidence="0.99944">
3.2 Suffix tree alignment
</subsectionHeader>
<bodyText confidence="0.967529287671233">
In the next section each transformation will be as-
sociated with several strings. Given an input text,
we will compute transformation scores by comput-
ing statistics of these strings. This can easily be
done using suffix trees, and by pairing statistics cor-
responding to the same transformation. The latter
task can be done using the data structure originally
introduced here.
A total function h : E E&apos;, E and E&apos; two alpha-
bets, is called a (restricted) homomorphism. We
extend h to a string function in the usual way by
posing h(E) = E and h(au) = h(a)h(u), a E E and
u E E*. Given w, w&apos; E E+, we need to pair each
factor it of w with factor h(u) possibly occurring in
w&apos;, To solve this problem, we construct the suffix
trees T, T&apos; for w, w&apos;, respectively. Then we estab-
lish an a-link (a pointer) from each node u of T,
u some factor, to the (implicit) node h(u) of T&apos;, if
h(u) exists. Furthermore, if factor ua with a E E is
an (implicit) node of T such that h(u) but not h(ua)
are (implicit) nodes of T&apos;, we create node u in T (if
u was an implicit node) and establish an a-link from
u to (implicit) node h(u) of T&apos;. Note that the to-
tal number of a-links is 0( lw I). The resulting data
structure is called here suffix tree alignment. An
example is reported in Figure 2.
We now specify a method to compute suffix tree
alignments. In what follows p, p&apos; are tree nodes and
U is a non-null string. Crucially, we assume we can
access the s-links of T and T&apos;. Paths it and v in T
and T&apos;, respectively, are aligned if v = h(u). The
next two functions are used to move a-links up and
down two aligned paths.
function Move_link_down(p,p&apos; , u): Starting at p
and p&apos;, simultaneously scan it and h(u), respectively,
using function Slow_scan. Stop as soon as a symbol
is not matched. At each encountered node of T and
at the (implicit) node of T corresponding to the last
successful match, create an a-link to the paired (im-
plicit) node of T&apos;. Return the pair of nodes in the
lastly created a-link along with the length of the suc-
cessfully matched prefix of it.
In the next function, we use function Fastscan in-
troduced in Section 3.1, but we run it upward the
tree (with the obvious modifications).
function Move_link_up(p,p&apos;): Starting at p and p1,
simultaneously scan the paths to the roots of T and
T&apos;, respectively, using function Fastscan. Stop as
soon as a node of T is encountered that already has
an a-link. At each encountered node of T create an
a-link to the paired (implicit) node of T&apos;
We also need a function that &amp;quot;shifts&amp;quot; a-links to a new
pair of aligned paths. This is done using s-links. The
next auxiliary function takes care of those (implicit)
nodes for which the s-link is missing. (This is the
case for implicit nodes of T&apos; and for some nodes of
T that have been newly created.) We rest on the
property that the parent node of any such (implicit)
node always has an s-link, when it differs from the
root.
function Up_link_down(p): If s-link(p) is defined
then return s-link(p). Else, let pi = parent(p). If
P1 is not the root node, let p2 = s-link(pi) and
return (implicit) node Fastscan(p2, label(pi,p)).
If pi is the root node, return (implicit) node
Fast_scan(p11 SUA1abel(pi,p)i_i(label(PI)1))).
function Shift_link(p,p1): p1 = Up_link_down(p),
= Up_link_down(p&apos;). Return (pi ,
We can now present the algorithm for the con-
struction of suffix tree alignments.
Algorithm 1 Let T and T&apos; be the suffix trees for
strings w and w&apos;, respectively:
(1)11 , , d) +— Move_link_down(root of T,
</bodyText>
<page confidence="0.986882">
446
</page>
<figure confidence="0.9948868">
i sbi bi a-link
9 — ac 1,2
8 c c —
7 e cba 4
6 ba bac 5
5 ac aca 6
4 ca ca 7
3 a ac —
2 c c 8
1 e $ _
</figure>
<figureCaption confidence="0.8731208">
Figure 3: The table reports the values of sbi, bi
and the established a-links at each iteration of Algo-
rithm 1, when constructing the suffix tree alignment
in Figure 2. To denote a-links we use the same inte-
ger numbers as in Figure 2.
</figureCaption>
<bodyText confidence="0.853066">
root of T&apos; , suffm(w))
for i from 1w1 — 1 downto 1 do begin
</bodyText>
<equation confidence="0.734148571428571">
(sbi, sb/i) Shift_link(bi.4.1,641)
Move_link_up(sbi, sb)
(bi,wi,dd) Move_link_down(sbi,
subjtvi—d(w))
d d + dd
end
end
</equation>
<bodyText confidence="0.96731075">
In Figure 3 a sample run of Algorithm 1 is schemat-
ically represented.
In the next section we use the following properties
of Algorithm 1:
</bodyText>
<listItem confidence="0.990327666666667">
• after T and T&apos; have been processed, for every
node p of T representing factor u of w, (implicit)
node a-link(p) of T&apos; is defined if and only if
a-link(p) represents factor h(u) of w&apos;;
• the algorithm can be executed in time
0(iwi +
</listItem>
<bodyText confidence="0.999910666666667">
The first property above can be proved as follows.
For 1 &lt; i &lt; Iwi, bi in Algorithm 1 is (the node
representing) the longest prefix of suffi(w) such that
h(b2) is an (implicit) node of T&apos; (is a factor of w&apos;).
This can be proved by induction on 1w1 — i, using the
definition of Move_link_down and of s-link. We then
observe that, if u is a node of T, then factor u is a
prefix of some suffi(w) and either u dominates bi or
bi properly dominates u in T. If u dominates bi, then
</bodyText>
<listItem confidence="0.964865">
• h(u) must be an (implicit) node of T&apos;. In this case an
a-link is established from u to h(u) by Move_link_up
or Move_link_down, depending on whether u dom-
inates or is dominated by sbi in T. If bi properly
dominates u, h(u) does not occur in w&apos;. In this
case, node u is never reached by the algorithm and
no a-link is established for this node.
</listItem>
<bodyText confidence="0.999955875">
The proof of the linear time result is rather long,
we only give an outline here. The interesting case
is the function Shift_link, which is executed itul — 1
times by the algorithm. When executed once on
nodes p and p&apos;, Shift_link uses time 0(1) if s-link(p)
and s-link(p&apos;) are both defined. In all other cases,
it uses an amount of time proportional to the num-
ber of (implicit) nodes visited by function Fast_scan,
which is called through function Up_link_down. We
use an amortization technique and charge a constant
amount of time to the symbols in w and w&apos;, for each
node visited in this way. Consider the execution of
Shift_link(bi+1,b41) for some i, 1&lt; i &lt; lwl— 1. As-
sume that, correspondingly, Fast_scan visits nodes
ud of T in this order, with d &gt; 1 and each
u • some factor of w. Then we have that each u • is
a (proper) prefix of uj+i, and ud = sbi. For each
uj, 1 &lt;j &lt;d — 1, we charge a constant amount of
time to the symbol in w &amp;quot;corresponding&amp;quot; to the last
symbol of uj. The visit to ud, on the other hand, is
charged to the ith symbol of w. (Note that charging
the visit to ud to the symbol in w &amp;quot;corresponding&amp;quot;
to the last symbol of ud does not work, since in the
case of sbi = bi the same symbol would be charged
again at the next iteration of the for-cycle.) It is not
difficult to see that, in this way, each symbol of w is
charged at most once. A similar argument works for
visits to nodes of T&apos; by Fast_scan, which are charged
to symbols of w&apos;. This shows that the time used by
all executions of Shift_link is Oawl+ iwiD•
Suffix trees and suffix tree alignments can be gen-
eralized to finite multi-sets of strings, each string
ending with the same end-marker not found at any
other position. In this case each leaf holds a record,
called count, of the number of times the correspond-
ing suffix appears in the entire multi-set, which will
be propagated appropriately when computing factor
statistic. Most important here, all of the above re-
sults still hold for these generalizations. In the next
section, we will deal with the multi-set case.
</bodyText>
<sectionHeader confidence="0.98949" genericHeader="method">
4 Transformation learning
</sectionHeader>
<bodyText confidence="0.999980285714286">
This section deals with the computational problem
of learning string transformations from an aligned
corpus. We show that some families of transforma-
tions can be efficiently learned exploiting the data
structures of Section 3. We also consider more gen-
eral kinds of transformations and show that for this
class the learning problem is NP-hard.
</bodyText>
<subsectionHeader confidence="0.997083">
4.1 Data representation
</subsectionHeader>
<bodyText confidence="0.988388428571429">
We introduce a representation of aligned corpora
that reduces the problem of computing the pos-
itive/negative evidence of transformations to the
problem of computing factor statistics.
Let (w, w&apos;) be an aligned pair, w = al • • • an and
w&apos; = • • a&apos;, with ai E E for 1 &lt; i &lt; n, and n &gt; 1.
We define
</bodyText>
<equation confidence="0.812729">
w x w&apos; = (ai, ai) • • • (an, a&apos;n). (1)
447
Note that w x w&apos; is a string over the new al-
phabet E x E. Let N &gt; 1 and let L
{(wi, wi), • . . , (w N , w&apos;N)} be an aligned corpus. We
represent L as a string multi-set over alphabet E x E:
Lx = {w x (w,wi) E L} , (2)
</equation>
<bodyText confidence="0.9998775">
where w x w&apos; appears in Lx as many times as (w, w&apos;)
appears in L.
</bodyText>
<subsectionHeader confidence="0.999229">
4.2 Learning algorithms
</subsectionHeader>
<bodyText confidence="0.999963">
Let L be an aligned corpus with N aligned pairs over
a fixed alphabet E, and let n be the length of the
longest string in a pair in L. We start by considering
plain transformations of the form
</bodyText>
<equation confidence="0.950213">
U -&gt; V, (3)
</equation>
<bodyText confidence="0.997559142857143">
where u, v E E+, rui = 14 We want to find all in-
stances of strings u, v E E* such that, in L, u v
has score greater or equal than the score of any other
transformation. Existing methods for this problem
are data-driven. They consider all pairs of factors
(with lengths bounded by n) occurring at aligned
positions within some pair in L, and update the
positive and the negative evidence of the associated
transformations. They thus consider 0(Nn2) fac-
tor pairs, where each pair takes time 0(n) to be
read/stored. We conclude that these methods use
an amount of time 0(Nn3). We can improve on
this by using suffix tree alignments.
Let Lx be defined as in (2) and let h1 : (E XE)
</bodyText>
<equation confidence="0.939671">
(E x E) be the homomorphism specified as:
hi((a,b)) -=- (a, a).
</equation>
<bodyText confidence="0.999831666666667">
Recall that each suffix of a multi-set of strings is
represented by a leaf in the associated suffix-tree,
because of the use of the end-marker, and that each
leaf stores the count of the occurrences of the corre-
sponding suffix in the source multi-set. We schemat-
ically specify our first learning algorithm below.
</bodyText>
<sectionHeader confidence="0.45937" genericHeader="method">
Algorithm 2
</sectionHeader>
<bodyText confidence="0.968316444444444">
Step 1: construct two copies Tx and 7; of the suf-
fix tree associated with Lx and align them using h1;
Step 2: visit trees Tx and T;&lt; in post-order, and an-
notate each node p with the number e(p) computed
as the sum of the counts at leaves that p dominates;
Step 3: annotate each node p of Tx with the score
e(p) — e(p&apos;), where p&apos; = a-link(p) if a-link(p) is an
actual node, p&apos; is the node immediately dominated
by a-link(p) if a-link(p) is an implicit node, and
e(p&apos;) = 0 if a-link(p) is undefined; make a list of
the nodes with the highest annotated score.
Let p be a node of Tx associated with factor u x v.
Integer e(p) computed at Step 2 is the number of
times a suffix having u x v as a prefix appears in
strings in Lx. Thus e(p) is the number of differ-
ent positions at which factors u and v are aligned
within Lx and hence the positive evidence of trans-
formation u v w.r.t. L, as defined in Section 2.
Similarly, e(p1) is the statistic of factor u x u and
hence the negative evidence of u v (as well as the
negative evidence of all transformations having u as
left-hand side). It follows that Algorithm 2 records,
at Step 3, the transformations having the highest
score in L among all transformations represented by
nodes of Tx . It is not difficult to see that the re-
maining transformations, denoted by implicit nodes
of Tx , do not have score greater than the one above.
The latter transformations with highest score, if any,
can be easily recovered by visiting the implicit nodes
that immediately dominate the nodes of Tx recorded
at Step 3.
A complexity analysis of Algorithm 2 is straight-
forward. Step 1 can be executed in time 0(Nn), as
discussed in Section 3. Since the size of Tx and T;(
is 0(Nn); all other steps can be easily executed in
linear time. Hence Algorithm 2 runs in time 0(Nn).
We now turn to a more general kind of transfor-
mations. In several natural language processing ap-
plications it is useful to generalize over some trans-
formations of the form in (3), by using classes of
symbols in E. Let t &gt; 1 and let CI., Ct be a par-
tition of E (each Ci 0). Consider = . • • ,
as an alphabet. We say that string al • • ad E E+
matches string G1 • • Cid E rs+ if ak E Ci,, for
1 &lt; k &lt; d. We define transformations&apos;
</bodyText>
<equation confidence="0.931848">
try -&gt; v (4)
u,v E E+, lui = iv!, E and assume the follow-
</equation>
<bodyText confidence="0.999865916666667">
ing interpretation. An occurrence of string u must
be rewritten to v in a text whenever u is followed
by a substring matching 7. String 7 is called the
right context of the transformation. The positive
evidence for such transformation is the number of
positions at which factors ux and vx&apos; are aligned
within the corpus, for all possible x, x1 E E+ with
x matching -y. (We do not require x = x&apos;, since
later transformations can change the right context.)
The negative evidence for the transformation is the
number of positions at which factors ux and ux&apos; are
aligned within the corpus, x, x&apos; as above.
We are not aware of any learning method for
transformations of the form in (4). A naive method
for this task would consider all factor pairs appear-
ing at aligned positions in some pair in L. The left
component of each factor must then be split into
a string in E+ and a string in r+, to represent a
transformation in the desired form. Overall, there
are 0(Nn3) possible transformations, and we need
time 0(n) to read/store each transformation. Then
the method uses an amount of time 0(Nn4). Again,
we can improve on this. We need a representa-
tion for right context strings. Define homomorphism
</bodyText>
<equation confidence="0.999772">
h2 : (E x E) r as
h2((a,b)) = C, a E C.
</equation>
<bodyText confidence="0.868524333333333">
&apos;In generative phonology (4) is usually written as u —›
v / _ 7. Our notation can more easily be generalized, as
it is needed in some transformation systems.
</bodyText>
<page confidence="0.99676">
448
</page>
<bodyText confidence="0.993100571428571">
(h2 is well defined since F is a partition of E.) Let
also
have an a-link to a leaf of Tr dominated by p;
otherwise, e&apos; = 0;
Step 4: find all pairs (p, q), p a node of Tr and
(q, e, e&apos;) E r(p), such that e — e&apos; is greater than or
equal to any other el — el, (qi, el, el) in some r(pi)•
</bodyText>
<equation confidence="0.889956">
Lr = {h2(w x w&apos;) I w x E L.},
</equation>
<bodyText confidence="0.943688">
where h2(w x w&apos;) appears in Lr as many times as
w x w&apos; appears in L..
</bodyText>
<note confidence="0.402175">
UXV
</note>
<figureCaption confidence="0.899759">
Figure 4: At Step 3 of Algorithm 3, triple (q, e, e&apos;)
</figureCaption>
<bodyText confidence="0.955483461538462">
is inserted in r(p) if the relations depicted above are
realized, where dashed arrows denote a-links, black
circles denote nodes, and white circles denote nodes
that might be implicit. Integer e &gt; 0 is a count of
the paths from node q downward, having the form
y x y&apos; with a prefix of y matching 7. Similarly, e&apos; is a
count of the paths from node q&apos; downward satisfying
the same matching condition with 7. The matching
condition is enforced by the fact that the above paths
have their ending leaf nodes a-linked to a leaf node
of Ti-&apos; dominated by node p.
Below we link a suffix-tree to more than one suffix-
tree. In the notation of a-links we then use a sub-
script indicating the suffix tree of the target node,
in order to distinguish among different linkings. We
now schematically specify the learning algorithm;
additional computational details will be provided
later in the discussion of the complexity.
Algorithm 3
Step 1: construct two copies Tx and r&lt; of the suffix
tree associated with L. and construct the suffix tree
Tr associated with Lr;
Step 2: align Tx with r, using h1 and align the
resulting suffix trees Tx and T&lt; with Tr using h2;
Step 3: for each node p of Ti-&apos;, store a set r(p)
including all triples (q, e, e&apos;) such that (see Figure 4):
</bodyText>
<listItem confidence="0.998586">
• q is a node of Tx such that a-linkTr(q) properly
dominates p
• e &gt; 0 is the sum of the counts at leaves of Tx
dominated by q that have an a-link to a leaf of
Tr dominated by p
• if q&apos; = a-link T (q) is defined, e&apos; is the sum of
</listItem>
<bodyText confidence="0.990852735849056">
the counts at leaves of r&lt; dominated by q&apos; that
We next show that if pair (p, q) is found at Step 4,
then q represents a factor u x v, p represents a factor
h2(u x v)7, and transformation u7 v — has the
highest score among all transformations represented
by nodes of Tx and Tr. Similarly to the case of Al-
gorithm 2, this is the highest score achieved in L,
and other transformations with the same score can
be obtained from some of the implicit nodes imme-
diately dominating p and q.
Let p add q be defined as in Step 3 above. Assume
that q represents a factor u x v of some string in L.
and p represents a factor 67 E F* of some string in
Lr, where 161 = 1u1. Since a-linkTr(q) dominates
p, we must have h2(u x v) = b. Consider a suffix
(u x v)(x x x&apos;)(y x y&apos;) appearing in so &gt; 0 strings
in L x , such that h2(x x x&apos;) = 7. (This means that
x matches 7, and there are at least so positions at
which u v has been applied with a right-context of
7.) We have that string h2 ((u x v)(x x x&apos;)(y x y&apos;)) =
67h2(y x y&apos;) must be a suffix of some strings in Lr.
It follows that (u x v)(x x x&apos;)(y x y&apos;) is a leaf of
Tx with a count of so, 67h2(y x y&apos;) is a leaf of Ti&apos;,
and there is an a-link between these two nodes. Leaf
(u x v)(x x x&apos;)(y x y&apos;) is dominated by q, and leaf
67h2(y x y&apos;) is dominated by p. Then, at Step 3,
integer so is added to e. Since no condition has been
imposed above on string x&apos; and on suffix (y x y&apos;), we
conclude that the final value of e must be the positive
evidence of transformation u-y —&gt; v —. A similar
argument shows that the negative evidence of this
transformation is stored in e&apos;. It then follows that, at
Step 4, Algorithm 3 finds the transformations with
the highest score among those represented by nodes
of Tx and Ti&apos;.
Algorithm 3 can be executed in time 0(N n2). We
only outline a proof of this property here, by fo-
cusing on Step 3. To execute this step we visit Ti-&apos;
in post order. At leaf node p, we consider the set
F(p) of all leaves q of T. such that p = a-linkTx(q),
and the set F1(p) of all leaves q&apos; of r, such that
p = a-linkTx (q&apos;). For each (implicit) node of r&lt;
that dominates some node in F&apos; (p) and that is
the target of some a-link (from some source node
of Tx), we record the sum of the counts of the
dominated nodes in F&apos; (p). This can be done in
time 0 (IF&apos; (p)I n). For each node q of Tx dominat-
ing some node in F(p), we store in r(p) the triple
(q, e, e&apos;), since a-link Tr(q) necessarily dominates p.
We let e &gt; 0 be the sum of the counts of the dom-
inated nodes in F(p), and let e&apos; be the value re-
trieved from the a-link to r&lt;, if any. This takes
time 0 (IF (p)i n). When p ranges over the leaves of
</bodyText>
<page confidence="0.998437">
449
</page>
<bodyText confidence="0.999883272727273">
Tr, we have E, IF(p)1= Ep }r(p)1 = 0(Nn). We
then conclude that sets 7(p) for all leaves p of Tr
can be computed in time 0(Nn2). At internal node
p with children pi, 1 &lt; i &lt; d, d &gt; 1, we assume
that sets r(pi)&apos;s have already been computed. As-
sume that for some i we have (q, ei , E r(pi) and
a-linkTr(q) does not immediately dominate pi. If
(q, e, e&apos;) E 7(p), we add ei, e to e, e&apos;, respectively;
otherwise, we insert (q,ei,e) in r(p). We can then
compute sets r(p) for all internal nodes p of Tr using
an amount of time E 1r(p)1 = 0(Nn2).
</bodyText>
<subsectionHeader confidence="0.993151">
4.3 General transformations
</subsectionHeader>
<bodyText confidence="0.9999975">
We have mentioned that the introduction of classes
of alphabet symbols allows abstraction over plain
transformations that is of interest to natural lan-
guage applications. We generalize here transforma-
tions in (4) by letting 7 be a string over E U F. More
precisely, we assume 7 has the form:
</bodyText>
<equation confidence="0.960634">
7 = uoalui -ud-ladud, (5)
</equation>
<bodyText confidence="0.9995735">
where uo, ud E E*, ui E E+ and aj E r+ for 1 &lt;
i &lt; d -1 and 1 &lt;j &lt;d, and d&gt; 1. The notion of
matching previously defined is now extended in such
a way that, for a, b E E, a matches b if a = b. Then
the interpretation of the resulting transformation is
the usual one. The parameter d in (5) is called the
number of alternations of the transformation. We
have established the following results:
</bodyText>
<listItem confidence="0.970915">
• transformations with a bounded number of al-
ternations can be learned in polynomial time;
• learning transformations with an unbounded
number of alternations is NP-hard.
</listItem>
<bodyText confidence="0.999912545454546">
Again, we only give an outline of the proof below.
The first result is easy to show, by observing that
in an aligned corpus there are polynomially many
occurrences of transformations with a bounded num-
ber of alternations. The second result holds even if
we restrict ourselves to 1E1 = 2 and in = 1, that is
if we use a don&apos;t care symbol. Here we introduce
a decision problem associated with the optimiza-
tion problem of learning the transformations with
the highest score, and outline an NP-completeness
proof.
</bodyText>
<sectionHeader confidence="0.994451" genericHeader="method">
TRANSFORMATION SCORING (TS)
</sectionHeader>
<bodyText confidence="0.9996435625">
Instance: (L, K), with L an aligned corpus, K a
positive integer.
Question: Is there a transformation that has score
greater than or equal to K w.r.t. L?
Membership in NP is easy to establish for TS. To
show NP-hardness, we consider the CLIQUE de-
cision problem for undirected, simple, connected
graphs and transform such a problem to the TS
problem. (The NP-completeness for the used restric-
tion of the CLIQUE problem (Garey and Johnson,
1979) is easy to establish.) Let (G, K&apos;) be an in-
stance of the CLIQUE problem as above, G = (V, E)
and K&apos;&gt; 0. Without loss of generality, we assume
that V = {1,2, , q}. Let E = {a, b}; we construct
an instance of the TS problem (L, K) over E as fol-
lows. For each {i, j} E V with i &lt; j let
</bodyText>
<equation confidence="0.879969">
wi,j = (6)
</equation>
<bodyText confidence="0.990596">
We add to the aligned corpus L:
</bodyText>
<listItem confidence="0.9919424">
1. one instance of pair pi,j = btvi,j) for each
i &lt;j, {i,j} E E;
2. q2 instances of pair pi = (awi,j , awi,j) for each
j E V with i &lt;j and {i, j} E;
3. q2 instances of pair pa = (aa, baa).
</listItem>
<bodyText confidence="0.993521964285714">
Also, we set K = q2 + (i). The above instance of
TS can easily be constructed in polynomial deter-
ministic time with respect to the length of (G, K&apos;) .
It is easy to show that when (G, K&apos;) is a positive
instance of the source problem, then the correspond-
ing instance of TS is satisfied by at least one trans-
formation. Assume now that there exists a trans-
formation r having score greater equal than K &gt; 0,
w.r.t. L. Since the replacement of a with b is the
only rewriting that appears in pairs of L, 7 must
have the form a7 b If 7 includes some occur-
rence of b, then 7 cannot match pa and the positive
evidence of 7 will not exceed 1E1 &lt; &lt; K, con-
trary to our assumption. We then conclude that 7
has the form (? denotes the don&apos;t care symbol):
a21-l?a32-21-i?...?aq&apos;-ja,
where V&amp;quot; =,id} C V, d &gt; 0 and q&apos; &lt; q. If
there exists i, j E V&amp;quot; such that {i, j} E, then 7
would match some pair pi E L and it would have
negative evidence smaller or equal than q2. Since
the positive evidence of 7 cannot exceed q2
7 would have a score not exceeding 1E1 &lt; (?)) &lt; K,
contrary to our assumption. Then 7 matches no pair
E L and, for each i, j E V&amp;quot;, we have j} E E.
Since K - q2 = (K2&apos;), at least (K2&apos;) pairs pi,j E L are
matched by r. We therefore conclude that d &gt; K&apos;
and that V&amp;quot; is a clique in G of size greater equal
than K&apos;. This concludes our outline of the proof.
</bodyText>
<sectionHeader confidence="0.990682" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999900083333333">
With some minor technical changes to function
Up_link_down, we can align a suffix tree with itself
(w.r.t. a given homomorphism). In this way we
improve space performance of Algorithms 2 and 3,
avoiding the construction of two copies of the same
suffix tree. Algorithm 3 can trivially be adapted to
learn transformations in (4) where a left context is
specified in place of a right context. The algorithm
can also be used to learn traditional phonological
rules of the form a b I _-y, where a, b are sin-
gle phonemes and 7 is a sequence over {C, V}, the
classes of consonants and vowels. In this case the
</bodyText>
<page confidence="0.988611">
450
</page>
<bodyText confidence="0.999948086956522">
algorithm runs in time 0(Nn) (for fixed alphabet).
We leave it as an open problem whether rules of the
form in (4) can be learned in linear time.
We have been concerned with learning the best
transformations that should be applied at a given
step. An ordered sequence of transformations can
be learned by iteratively learning a single transfor-
mation and by processing the aligned corpus with
the transformation just learned (Brill, 1995). Dy-
namic techniques for processing the aligned corpus
were first proposed in (Ramshaw and Marcus, 1996)
to re-edit the corpus only where needed. Those au-
thors report that this is not space efficient if trans-
formation learning is done by independently test-
ing all possible transformations in the search space
(as in (Brill, 1995)). The suffix tree alignment data
structure allows simultaneous scoring for all trans-
formations. We can now take advantage of this and
design dynamical algorithms that re-edit a suffix tree
alignment only where needed, on the line of a similar
method for suffix trees in (McCreight, 1976).
An alternative data structure to suffix trees for
the representations of string factors, called DAWG,
has been presented in (Blumer et al., 1985). We
point out here that, because a DAWG is an acyclic
graph rather than a tree, straightforward ways of
defining alignment between two DAWGs results in a
quadratic number of a-links, making DAWGs much
less attractive than suffix trees for factor alignment.
We believe that suffix tree alignments are a very flex-
ible data structure, and that other transformations
could be efficiently learned using these structures.
We do not regard the result in Section 4.3 as a neg-
ative one, since general transformations specified as
in (5) seem too powerful for the proposed applica-
tions in natural language processing, and learning
might result in corpus overtraining.
Other than transformation based systems the
methods presented in this paper can be used for
learning rules of constraint grammars (Karlsson et
al., 1995), phonological rule systems as in (Kaplan
and Kay, 1994), and in general those grammatical
systems using constraints represented by means of
rewriting rules. This is the case whenever we can
encode the alphabet of the corpus in such a way
that alignment is possible.
</bodyText>
<sectionHeader confidence="0.997983" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99993675">
Part of the present research was done while the first
author was visiting the Center for Language and
Speech Processing, Johns Hopkins University, Bal-
timore, MD. The second author is a member of the
Center for Language and Speech Processing. This
work was funded in part by NSF grant IRI-9502312.
The authors are indebted to Eric Brill for technical
discussions on topics related to this paper.
</bodyText>
<sectionHeader confidence="0.996975" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986434743589744">
Apostolico, A. 1985. The myriad virtues of suf-
fix trees. In A. Apostolico and Z. Galil, editors,
Combinatorial Algorithms on Words, volume 12.
Springer-Verlag, Berlin, Germany, pages 85-96.
NATO Advanced Science Institutes, Seires F.
Blumer, A., J. Blumer, D. Haussler, A. Ehrenfeucht,
M. Chen, and J. Seiferas. 1985. The smallest au-
tomaton recognizing the subwords of a text. The-
oretical Computer Science, 40:31-55.
Brill, E. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics.
Crochemore, M. and W. Rytter. 1994. Text Algo-
rithms. Oxford University Press, Oxford, UK.
Garey, M. R. and D. S. Johnson. 1979. Computers
and Intractability. Freeman and Co., New York,
NY.
Kaplan, R. M. and M. Kay. 1994. Regular models
of phonological rule sistems. Computational Lin-
guistics, 20(3):331-378.
Karlsson, F., A. Voutilainen, J. Heikkila, and
A. Anttila. 1995. Constraint Grammar. A
Language Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter.
McCreight, E. M. 1976. A space-economical suffix
tree construction algorithm. Journal of the Asso-
ciation for Computing Machinery, 23(2):262-272.
Ramshaw, L. and M. P. Marcus. 1996. Explor-
ing the nature of transformation-based learning.
In J. Klavans and P. Resnik, editors, The Bal-
ancing Act—Combining Symbolic and Statistical
Approaches to Language. The MIT Press, Cam-
bridge, MA, pages 135-156.
Weiner, P. 1973. Linear pattern-matching algo-
rithms. In Proceedings of the 14th IEEE Annual
Symposium on Switching and Automata Theory,
pages 1-11, New York, NY. Institute of Electrical
and Electronics Engineers.
</reference>
<page confidence="0.998754">
451
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964286">
<title confidence="0.999962">String Transformation Learning</title>
<author confidence="0.999567">Giorgio Satta</author>
<affiliation confidence="0.9998">Dipartimento di Elettronica e Informatica Universita di Padova</affiliation>
<address confidence="0.9919705">via Gradenigo, 6/A 1-35131 Padova, Italy</address>
<email confidence="0.99623">sattadei.unipd.it</email>
<author confidence="0.997718">John C Henderson</author>
<affiliation confidence="0.9994925">Department of Computer Science Johns Hopkins University</affiliation>
<address confidence="0.999837">Baltimore, MD 21218-2694</address>
<abstract confidence="0.9991914375">String transformation systems have been introduced in (Brill, 1995) and have several applications in natural language processing. In this work we consider the computational problem of automatically learning from a given corpus the set of transformations presenting the best evidence. We introduce an original data structure and efficient algorithms that learn some families of transformations that are relevant for part-of-speech tagging and phonological rule systems. We also show that the same learning problem becomes NP-hard in cases of an unbounded use of don&apos;t care symbols in a transformation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Apostolico</author>
</authors>
<title>The myriad virtues of suffix trees.</title>
<date>1985</date>
<booktitle>Combinatorial Algorithms on Words,</booktitle>
<volume>12</volume>
<pages>85--96</pages>
<editor>In A. Apostolico and Z. Galil, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany,</location>
<marker>Apostolico, 1985</marker>
<rawString>Apostolico, A. 1985. The myriad virtues of suffix trees. In A. Apostolico and Z. Galil, editors, Combinatorial Algorithms on Words, volume 12. Springer-Verlag, Berlin, Germany, pages 85-96. NATO Advanced Science Institutes, Seires F.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blumer</author>
<author>J Blumer</author>
<author>D Haussler</author>
<author>A Ehrenfeucht</author>
<author>M Chen</author>
<author>J Seiferas</author>
</authors>
<title>The smallest automaton recognizing the subwords of a text.</title>
<date>1985</date>
<journal>Theoretical Computer Science,</journal>
<pages>40--31</pages>
<contexts>
<context position="32407" citStr="Blumer et al., 1985" startWordPosition="6155" endWordPosition="6158">. Those authors report that this is not space efficient if transformation learning is done by independently testing all possible transformations in the search space (as in (Brill, 1995)). The suffix tree alignment data structure allows simultaneous scoring for all transformations. We can now take advantage of this and design dynamical algorithms that re-edit a suffix tree alignment only where needed, on the line of a similar method for suffix trees in (McCreight, 1976). An alternative data structure to suffix trees for the representations of string factors, called DAWG, has been presented in (Blumer et al., 1985). We point out here that, because a DAWG is an acyclic graph rather than a tree, straightforward ways of defining alignment between two DAWGs results in a quadratic number of a-links, making DAWGs much less attractive than suffix trees for factor alignment. We believe that suffix tree alignments are a very flexible data structure, and that other transformations could be efficiently learned using these structures. We do not regard the result in Section 4.3 as a negative one, since general transformations specified as in (5) seem too powerful for the proposed applications in natural language pro</context>
</contexts>
<marker>Blumer, Blumer, Haussler, Ehrenfeucht, Chen, Seiferas, 1985</marker>
<rawString>Blumer, A., J. Blumer, D. Haussler, A. Ehrenfeucht, M. Chen, and J. Seiferas. 1985. The smallest automaton recognizing the subwords of a text. Theoretical Computer Science, 40:31-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="1182" citStr="Brill, 1995" startWordPosition="168" endWordPosition="169">vidence. We introduce an original data structure and efficient algorithms that learn some families of transformations that are relevant for part-of-speech tagging and phonological rule systems. We also show that the same learning problem becomes NP-hard in cases of an unbounded use of don&apos;t care symbols in a transformation. 1 Introduction Ordered sequences of rewriting rules are used in several applications in natural language processing, including phonological and morphological systems (Kaplan and Kay, 1994), morphological disambiguation, part-of-speech tagging and shallow syntactic parsing (Brill, 1995), (Karlsson et al., 1995). In (Brill, 1995) a learning paradigm, called errordriven learning, has been introduced for automatic induction of a specific kind of rewriting rules called transformations, and it has been shown that the achieved accuracy of the resulting transformation systems is competitive with that of existing systems. In this work we further elaborate on the errordriven learning paradigm. Our main contribution is summarized in what follows. We consider some families of transformations and design efficient algorithms for the associated learning problem that improve existing metho</context>
<context position="3024" citStr="Brill, 1995" startWordPosition="502" endWordPosition="503"> the set of all strings and all non-null strings over E, respectively. Let w E E*. We denote by lw I the length of w. Let w = uxv; u is a prefix and v is a suffix of w; when x is non-null, it is called a factor of w. The suffix of w of length i is denoted suffi(w), for 0 &lt; i &lt; I wl. Assume that x is non-null, and w = uixsuffi(w) for yo &gt; 0 different values of i but not for so + 1, or x is not a factor of w and so = 0. Then we say that so is the statistic of factor x in w. 2 The learning paradigm The learning paradigm we adopt is called errordriven learning and has been originally proposed in (Brill, 1995) for part of speech tagging applications. We briefly introduce here the basic assumptions of the approach. A string transformation is a rewriting rule denoted as u v, where u and v are strings such that Iu = Iv . This means that if u appears as a factor of some string w, then u should be replaced by v in w. The application of the transformation might be conditioned by the requirement that some additionally specified pattern matches some part of the string w to be rewritten. We now describe how transformations can be automatically learned. A pair of strings (w, w&apos;) is an aligned pair if I w I =</context>
<context position="31643" citStr="Brill, 1995" startWordPosition="6034" endWordPosition="6035">aditional phonological rules of the form a b I _-y, where a, b are single phonemes and 7 is a sequence over {C, V}, the classes of consonants and vowels. In this case the 450 algorithm runs in time 0(Nn) (for fixed alphabet). We leave it as an open problem whether rules of the form in (4) can be learned in linear time. We have been concerned with learning the best transformations that should be applied at a given step. An ordered sequence of transformations can be learned by iteratively learning a single transformation and by processing the aligned corpus with the transformation just learned (Brill, 1995). Dynamic techniques for processing the aligned corpus were first proposed in (Ramshaw and Marcus, 1996) to re-edit the corpus only where needed. Those authors report that this is not space efficient if transformation learning is done by independently testing all possible transformations in the search space (as in (Brill, 1995)). The suffix tree alignment data structure allows simultaneous scoring for all transformations. We can now take advantage of this and design dynamical algorithms that re-edit a suffix tree alignment only where needed, on the line of a similar method for suffix trees in </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Crochemore</author>
<author>W Rytter</author>
</authors>
<title>Text Algorithms.</title>
<date>1994</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="5143" citStr="Crochemore and Rytter, 1994" startWordPosition="897" endWordPosition="900"> is the difference between the positive evidence and the negative evidence of r. This extends to an aligned corpus in the obvious way. We are interested in the set of transformations that are associated with the highest score in a given aligned corpus, and will develop algorithms to find such a set in the next sections. 3 Data Structures This section introduces two data structures that are basic to the development of the algorithms presented in this paper. 3.1 Suffix trees We briefly present here a data structure that is well known in the text processing literature; the reader is referred to (Crochemore and Rytter, 1994) and (Apostolic°, 1985) for definitions and further references. Let w be some non-null string. Throughout the paper we assume that the rightmost symbol of w is an end-marker not found at any other position in the string. The suffix tree associated with w is a &amp;quot;compressed&amp;quot; trie of all strings suffi(w), 1 &lt; i &lt; I w I. Edges are labeled by factors of w which are encoded by means of two natural numbers denoting endpoints in the string. An example is reported in Figure 1. An implicit node is a node not explicitly represented in the suffix tree, that splits the label of some edge at a given position</context>
</contexts>
<marker>Crochemore, Rytter, 1994</marker>
<rawString>Crochemore, M. and W. Rytter. 1994. Text Algorithms. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Garey</author>
<author>D S Johnson</author>
</authors>
<date>1979</date>
<journal>Computers</journal>
<location>New York, NY.</location>
<contexts>
<context position="28709" citStr="Garey and Johnson, 1979" startWordPosition="5438" endWordPosition="5441"> decision problem associated with the optimization problem of learning the transformations with the highest score, and outline an NP-completeness proof. TRANSFORMATION SCORING (TS) Instance: (L, K), with L an aligned corpus, K a positive integer. Question: Is there a transformation that has score greater than or equal to K w.r.t. L? Membership in NP is easy to establish for TS. To show NP-hardness, we consider the CLIQUE decision problem for undirected, simple, connected graphs and transform such a problem to the TS problem. (The NP-completeness for the used restriction of the CLIQUE problem (Garey and Johnson, 1979) is easy to establish.) Let (G, K&apos;) be an instance of the CLIQUE problem as above, G = (V, E) and K&apos;&gt; 0. Without loss of generality, we assume that V = {1,2, , q}. Let E = {a, b}; we construct an instance of the TS problem (L, K) over E as follows. For each {i, j} E V with i &lt; j let wi,j = (6) We add to the aligned corpus L: 1. one instance of pair pi,j = btvi,j) for each i &lt;j, {i,j} E E; 2. q2 instances of pair pi = (awi,j , awi,j) for each j E V with i &lt;j and {i, j} E; 3. q2 instances of pair pa = (aa, baa). Also, we set K = q2 + (i). The above instance of TS can easily be constructed in pol</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Garey, M. R. and D. S. Johnson. 1979. Computers and Intractability. Freeman and Co., New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule sistems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="1084" citStr="Kaplan and Kay, 1994" startWordPosition="154" endWordPosition="157">onal problem of automatically learning from a given corpus the set of transformations presenting the best evidence. We introduce an original data structure and efficient algorithms that learn some families of transformations that are relevant for part-of-speech tagging and phonological rule systems. We also show that the same learning problem becomes NP-hard in cases of an unbounded use of don&apos;t care symbols in a transformation. 1 Introduction Ordered sequences of rewriting rules are used in several applications in natural language processing, including phonological and morphological systems (Kaplan and Kay, 1994), morphological disambiguation, part-of-speech tagging and shallow syntactic parsing (Brill, 1995), (Karlsson et al., 1995). In (Brill, 1995) a learning paradigm, called errordriven learning, has been introduced for automatic induction of a specific kind of rewriting rules called transformations, and it has been shown that the achieved accuracy of the resulting transformation systems is competitive with that of existing systems. In this work we further elaborate on the errordriven learning paradigm. Our main contribution is summarized in what follows. We consider some families of transformatio</context>
<context position="33275" citStr="Kaplan and Kay, 1994" startWordPosition="6294" endWordPosition="6297">alignment. We believe that suffix tree alignments are a very flexible data structure, and that other transformations could be efficiently learned using these structures. We do not regard the result in Section 4.3 as a negative one, since general transformations specified as in (5) seem too powerful for the proposed applications in natural language processing, and learning might result in corpus overtraining. Other than transformation based systems the methods presented in this paper can be used for learning rules of constraint grammars (Karlsson et al., 1995), phonological rule systems as in (Kaplan and Kay, 1994), and in general those grammatical systems using constraints represented by means of rewriting rules. This is the case whenever we can encode the alphabet of the corpus in such a way that alignment is possible. Acknowledgements Part of the present research was done while the first author was visiting the Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD. The second author is a member of the Center for Language and Speech Processing. This work was funded in part by NSF grant IRI-9502312. The authors are indebted to Eric Brill for technical discussions on topics </context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Kaplan, R. M. and M. Kay. 1994. Regular models of phonological rule sistems. Computational Linguistics, 20(3):331-378.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Karlsson</author>
<author>A Voutilainen</author>
<author>J Heikkila</author>
</authors>
<location>and</location>
<marker>Karlsson, Voutilainen, Heikkila, </marker>
<rawString>Karlsson, F., A. Voutilainen, J. Heikkila, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Anttila</author>
</authors>
<title>Constraint Grammar. A Language Independent System for Parsing Unrestricted Text. Mouton de Gruyter.</title>
<date>1995</date>
<marker>Anttila, 1995</marker>
<rawString>A. Anttila. 1995. Constraint Grammar. A Language Independent System for Parsing Unrestricted Text. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M McCreight</author>
</authors>
<title>A space-economical suffix tree construction algorithm.</title>
<date>1976</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>23--2</pages>
<contexts>
<context position="6909" citStr="McCreight, 1976" startWordPosition="1224" endWordPosition="1225">,p, then d &gt; 2 and strings label(p, pi) differ one from the other at the leftmost symbol; • all and only the factors of w are represented by paths from the root to some (implicit) node; • the statistic of factor u of w is the number of leaves dominated by the (implicit) node ending the path representing u. In the remainder of the paper, we sometimes identify an (implicit) node of a suffix tree with the factor represented by the path from the root to that node. The suffix tree and the statistics of all factors of w can be constructed/computed in time ()(Iwl), as reported in (Weiner, 1973) and (McCreight, 1976). McCreight algorithm uses two basic functions to scan paths in the suffix tree under construction. These functions are briefly introduced here and will be exploited in the next subsection. Below, p is a node in a tree and u is a non-null string. function Slow_scan(p, u): Starting at p, scan u symbol by symbol. Return the (implicit) node corresponding to the last matching symbol. The next function runs faster than Slow_scan, and can be used whenever we already know that u is an (implicit) node in the tree (u completely matches some path in the tree). function Fast_scan(p, u): Starting at p, sc</context>
<context position="32260" citStr="McCreight, 1976" startWordPosition="6134" endWordPosition="6135"> Dynamic techniques for processing the aligned corpus were first proposed in (Ramshaw and Marcus, 1996) to re-edit the corpus only where needed. Those authors report that this is not space efficient if transformation learning is done by independently testing all possible transformations in the search space (as in (Brill, 1995)). The suffix tree alignment data structure allows simultaneous scoring for all transformations. We can now take advantage of this and design dynamical algorithms that re-edit a suffix tree alignment only where needed, on the line of a similar method for suffix trees in (McCreight, 1976). An alternative data structure to suffix trees for the representations of string factors, called DAWG, has been presented in (Blumer et al., 1985). We point out here that, because a DAWG is an acyclic graph rather than a tree, straightforward ways of defining alignment between two DAWGs results in a quadratic number of a-links, making DAWGs much less attractive than suffix trees for factor alignment. We believe that suffix tree alignments are a very flexible data structure, and that other transformations could be efficiently learned using these structures. We do not regard the result in Secti</context>
</contexts>
<marker>McCreight, 1976</marker>
<rawString>McCreight, E. M. 1976. A space-economical suffix tree construction algorithm. Journal of the Association for Computing Machinery, 23(2):262-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Exploring the nature of transformation-based learning.</title>
<date>1996</date>
<booktitle>The Balancing Act—Combining Symbolic and Statistical Approaches to Language. The</booktitle>
<pages>135--156</pages>
<editor>In J. Klavans and P. Resnik, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="31747" citStr="Ramshaw and Marcus, 1996" startWordPosition="6048" endWordPosition="6051"> a sequence over {C, V}, the classes of consonants and vowels. In this case the 450 algorithm runs in time 0(Nn) (for fixed alphabet). We leave it as an open problem whether rules of the form in (4) can be learned in linear time. We have been concerned with learning the best transformations that should be applied at a given step. An ordered sequence of transformations can be learned by iteratively learning a single transformation and by processing the aligned corpus with the transformation just learned (Brill, 1995). Dynamic techniques for processing the aligned corpus were first proposed in (Ramshaw and Marcus, 1996) to re-edit the corpus only where needed. Those authors report that this is not space efficient if transformation learning is done by independently testing all possible transformations in the search space (as in (Brill, 1995)). The suffix tree alignment data structure allows simultaneous scoring for all transformations. We can now take advantage of this and design dynamical algorithms that re-edit a suffix tree alignment only where needed, on the line of a similar method for suffix trees in (McCreight, 1976). An alternative data structure to suffix trees for the representations of string facto</context>
</contexts>
<marker>Ramshaw, Marcus, 1996</marker>
<rawString>Ramshaw, L. and M. P. Marcus. 1996. Exploring the nature of transformation-based learning. In J. Klavans and P. Resnik, editors, The Balancing Act—Combining Symbolic and Statistical Approaches to Language. The MIT Press, Cambridge, MA, pages 135-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Weiner</author>
</authors>
<title>Linear pattern-matching algorithms.</title>
<date>1973</date>
<booktitle>In Proceedings of the 14th IEEE Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>1--11</pages>
<location>New York, NY.</location>
<contexts>
<context position="6887" citStr="Weiner, 1973" startWordPosition="1221" endWordPosition="1222">p has children pl, ,p, then d &gt; 2 and strings label(p, pi) differ one from the other at the leftmost symbol; • all and only the factors of w are represented by paths from the root to some (implicit) node; • the statistic of factor u of w is the number of leaves dominated by the (implicit) node ending the path representing u. In the remainder of the paper, we sometimes identify an (implicit) node of a suffix tree with the factor represented by the path from the root to that node. The suffix tree and the statistics of all factors of w can be constructed/computed in time ()(Iwl), as reported in (Weiner, 1973) and (McCreight, 1976). McCreight algorithm uses two basic functions to scan paths in the suffix tree under construction. These functions are briefly introduced here and will be exploited in the next subsection. Below, p is a node in a tree and u is a non-null string. function Slow_scan(p, u): Starting at p, scan u symbol by symbol. Return the (implicit) node corresponding to the last matching symbol. The next function runs faster than Slow_scan, and can be used whenever we already know that u is an (implicit) node in the tree (u completely matches some path in the tree). function Fast_scan(p,</context>
</contexts>
<marker>Weiner, 1973</marker>
<rawString>Weiner, P. 1973. Linear pattern-matching algorithms. In Proceedings of the 14th IEEE Annual Symposium on Switching and Automata Theory, pages 1-11, New York, NY. Institute of Electrical and Electronics Engineers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>