<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.901571">
Infusion of Labeled Data into Distant Supervision for Relation Extraction
</title>
<author confidence="0.965957">
Maria Pershina + Bonan Minˆ ∗ Wei Xu # Ralph Grishman +
</author>
<affiliation confidence="0.944553">
+New York University, New York, NY
</affiliation>
<email confidence="0.772006">
{pershina, grishman}@cs.nyu.edu
</email>
<affiliation confidence="0.754672">
ˆRaytheon BBN Technologies, Cambridge, MA
bmin@bbn.com
#University of Pennsylvania, Philadelphia, PA
</affiliation>
<email confidence="0.996657">
xwe@cis.upenn.edu
</email>
<sectionHeader confidence="0.994766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910857142857">
Distant supervision usually utilizes only
unlabeled data and existing knowledge
bases to learn relation extraction models.
However, in some cases a small amount
of human labeled data is available. In this
paper, we demonstrate how a state-of-the-
art multi-instance multi-label model can
be modified to make use of these reli-
able sentence-level labels in addition to
the relation-level distant supervision from
a database. Experiments show that our ap-
proach achieves a statistically significant
increase of 13.5% in F-score and 37% in
area under the precision recall curve.
</bodyText>
<sectionHeader confidence="0.998414" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.958722380952381">
Relation extraction is the task of tagging semantic
relations between pairs of entities from free text.
Recently, distant supervision has emerged as an
important technique for relation extraction and has
attracted increasing attention because of its effec-
tive use of readily available databases (Mintz et
al., 2009; Bunescu and Mooney, 2007; Snyder and
Barzilay, 2007; Wu and Weld, 2007). It automat-
ically labels its own training data by heuristically
aligning a knowledge base of facts with an unla-
beled corpus. The intuition is that any sentence
which mentions a pair of entities (e1 and e2) that
participate in a relation, r, is likely to express the
fact r(e1,e2) and thus forms a positive training ex-
ample of r.
One of most crucial problems in distant super-
vision is the inherent errors in the automatically
generated training data (Roth et al., 2013). Ta-
ble 1 illustrates this problem with a toy exam-
ple. Sophisticated multi-instance learning algo-
rithms (Riedel et al., 2010; Hoffmann et al., 2011;
∗ Most of the work was done when this author was at
New York University
Surdeanu et al., 2012) have been proposed to ad-
dress the issue by loosening the distant supervision
assumption. These approaches consider all men-
tions of the same pair (e1,e2) and assume that at-
least-one mention actually expresses the relation.
On top of that, researchers further improved per-
formance by explicitly adding preprocessing steps
(Takamatsu et al., 2012; Xu et al., 2013) or addi-
tional layers inside the model (Ritter et al., 2013;
Min et al., 2013) to reduce the effect of training
noise.
Table 1: Classic errors in the training data gener-
ated by a toy knowledge base of only one entry
personTitle(Abu Zubaydah, leader).
However, the potential of these previously pro-
posed approaches is limited by the inevitable
gap between the relation-level knowledge and the
instance-level extraction task. In this paper, we
present the first effective approach, Guided DS
(distant supervision), to incorporate labeled data
into distant supervision for extracting relations
from sentences. In contrast to simply taking the
union of the hand-labeled data and the corpus la-
beled by distant supervision as in the previous
work by Zhang et al. (2012), we generalize the
labeled data through feature selection and model
this additional information directly in the latent
variable approaches. Aside from previous semi-
supervised work that employs labeled and unla-
beled data (Yarowsky, 2013; Blum and Mitchell,
1998; Collins and Singer, 2011; Nigam, 2001, and
others), this is a learning scheme that combines
unlabeled text and two training sources whose
quantity and quality are radically different (Liang
et al., 2009).
To demonstrate the effectiveness of our pro-
... to get information out of captured
al-Qaida leader Abu Zubaydah.
...Abu Zubaydah and former Taliban
leader Jalaluddin Haqqani ...
</bodyText>
<figure confidence="0.9657452">
True Positive
False Positive
False Negative
...Abu Zubaydah is one of Osama bin
Laden’s senior operational planners...
</figure>
<page confidence="0.933349">
732
</page>
<note confidence="0.4306655">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–738,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.990798333333333">
Guideline g = {gi|i = 1, 2, 3}: Relation r(g)
types of entities, dependency path, span word (optional)
person person, nsubj →← dobj, married personSpouse
person organization, nsubj →← prep of, became personMemberOf
organization organization, nsubj →← prep of, company organizationSubsidiaries
person person, poss →← appos, sister personSiblings
person person, poss →← appos, father personParents
person title, ← nn personTitle
organization person, prep of → appos → organizationTopMembersEmployees
person cause, nsubj →← prep of personCauseOfDeath
person number, ← appos personAge
person date, nsubjpass →← prep on ← num personDateOfBirth
</table>
<tableCaption confidence="0.999465">
Table 2: Some examples from the final set G of extracted guidelines.
</tableCaption>
<bodyText confidence="0.999803">
posed approach, we extend MIML (Surdeanu et
al., 2012), a state-of-the-art distant supervision
model and show a significant improvement of
13.5% in F-score on the relation extraction bench-
mark TAC-KBP (Ji and Grishman, 2011) dataset.
While prior work employed tens of thousands of
human labeled examples (Zhang et al., 2012) and
only got a 6.5% increase in F-score over a logistic
regression baseline, our approach uses much less
labeled data (about 1/8) but achieves much higher
improvement on performance over stronger base-
lines.
</bodyText>
<sectionHeader confidence="0.942562" genericHeader="method">
2 The Challenge
</sectionHeader>
<bodyText confidence="0.999854">
Simply taking the union of the hand-labeled data
and the corpus labeled by distant supervision is not
effective since hand-labeled data will be swamped
by a larger amount of distantly labeled data. An
effective approach must recognize that the hand-
labeled data is more reliable than the automatically
labeled data and so must take precedence in cases
of conflict. Conflicts cannot be limited to those
cases where all the features in two examples are
the same; this would almost never occur, because
of the dozens of features used by a typical relation
extractor (Zhou et al., 2005). Instead we propose
to perform feature selection to generalize human
labeled data into training guidelines, and integrate
them into latent variable model.
</bodyText>
<subsectionHeader confidence="0.94269">
2.1 Guidelines
</subsectionHeader>
<bodyText confidence="0.999216076923077">
The sparse nature of feature space dilutes the dis-
criminative capability of useful features. Given
the small amount of hand-labeled data, it is im-
portant to identify a small set of features that are
general enough while being capable of predicting
quite accurately the type of relation that may hold
between two entities.
We experimentally tested alternative feature
sets by building supervised Maximum Entropy
(MaxEnt) models using the hand-labeled data (Ta-
ble 3), and selected an effective combination of
three features from the full feature set used by Sur-
deanu et al., (2011):
</bodyText>
<listItem confidence="0.989833714285714">
• the semantic types of the two arguments (e.g.
person, organization, location, date, title, ...)
• the sequence of dependency relations along the
path connecting the heads of the two arguments
in the dependency tree.
• a word in the sentence between the two argu-
ments
</listItem>
<bodyText confidence="0.977202357142857">
These three features are strong indicators of the
type of relation between two entities. In some
cases the semantic types of the arguments alone
narrows the possibilities to one or two relation
types. For example, entity types such as person
and title often implies the relation personTitle.
Some lexical items are clear indicators of partic-
ular relations, such as “brother” and “sister” for a
sibling relationship
We extract guidelines from hand-labeled data.
Each guideline g={gi|i=1,2,3} consists of a pair
of semantic types, a dependency path, and option-
ally a span word and is associated with a partic-
ular relation r(g). We keep only those guidelines
</bodyText>
<table confidence="0.99522725">
Model Precision Recall F-score
MaxEntall 18.6 6.3 9.4
MaxEnttwo 24.13 10.75 14.87
MaxEntthree 40.27 12.40 18.97
</table>
<tableCaption confidence="0.6455846">
Table 3: Performance of a MaxEnt, trained on
hand-labeled data using all features (Surdeanu et
al., 2011) vs using a subset of two (types of en-
tities, dependency path), or three (adding a span
word) features, and evaluated on the test set.
</tableCaption>
<page confidence="0.994644">
733
</page>
<bodyText confidence="0.9999842">
which make the correct prediction for all and at
least k=3 examples in the training corpus (thresh-
old 3 was obtained by running experiments on the
development dataset). Table 2 shows some exam-
ples in the final set G of extracted guidelines.
</bodyText>
<sectionHeader confidence="0.992015" genericHeader="method">
3 Guided DS
</sectionHeader>
<bodyText confidence="0.9998705">
Our goal is to jointly model human-labeled ground
truth and structured data from a knowledge base
in distant supervision. To do this, we extend the
MIML model (Surdeanu et al., 2012) by adding a
new layer as shown in Figure 1.
The input to the model consists of (1) distantly
supervised data, represented as a list of n bags1
with a vector yi of binary gold-standard labels, ei-
ther Positive(P) or Negative(N) for each rela-
tion r∈R; (2) generalized human-labeled ground
truth, represented as a set G of feature conjunc-
tions g={gi|i=1,2,3} associated with a unique re-
lation r(g). Given a bag of sentences, xi, which
mention an ith entity pair (e1, e2), our goal is to
correctly predict which relation is mentioned in
each sentence, or NR if none of the relations under
consideration are mentioned. The vector zi con-
tains the latent mention-level classifications for the
ith entity pair. We introduce a set of latent vari-
ables hi which model human ground truth for each
mention in the ith bag and take precedence over
the current model assignment zi.
</bodyText>
<figureCaption confidence="0.99708">
Figure 1: Plate diagram of Guided DS
</figureCaption>
<bodyText confidence="0.9369342">
Let i, j be the index in the bag and the men-
tion level, respectively. We model mention-
level extraction p(zij|xij; wz), human relabel-
ing hij(xij, zij) and multi-label aggregation
p(yri |hi; wy). We define:
</bodyText>
<listItem confidence="0.9788245">
• yri ∈{P, N} : r holds for the ith bag or not.
• xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the same set
of features as in Surdeanu et al. (2012).
</listItem>
<equation confidence="0.969121333333333">
1A bag is a set of mentions sharing same entity pair.
.
y
</equation>
<bodyText confidence="0.997608">
Our approach is aimed at improving the mention-
level classifier, while keeping the multi-instance
multi-label framework to allow for joint modeling.
</bodyText>
<sectionHeader confidence="0.991835" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.999970333333333">
We use a hard expectation maximization algorithm
to train the model. Our objective function is to
maximize log-likelihood of the data:
</bodyText>
<equation confidence="0.8206155">
LL(wy, wz) = Xn log p(yi|xi, wy, wz, G)
i=1
</equation>
<bodyText confidence="0.9984328">
where the last equality is due to conditional
independence. Because of the non-convexity
of LL(wy, wz) we approximate and maximize
the joint log-probability p(yi,hi|xi,wy,wz,G) for
each entity pair in the database:
</bodyText>
<equation confidence="0.969059666666667">
log p(yi,hi|xi,wy,wz,G)
log p(hij|xij,wz,G)+ X log p(yri |hi,wry).
rEPiUNi
</equation>
<bodyText confidence="0.999738875">
Thus, relation r(g) is assigned to hij iff there
exists a unique guideline g ∈ G, such that the
feature vector xij contains all constituents of g,
i.e. entity types, a dependency path and maybe a
span word, if g has one. We use mention relation
zij inferred by the model only in case no such a
guideline exists or there is more than one match-
ing guideline. We also define:
</bodyText>
<listItem confidence="0.9991">
• wz is the weight vector for the multi-class rela-
tion mention-level classifier2
•
</listItem>
<bodyText confidence="0.923288666666667">
2All classifiers are implemented using L2-regularized lo-
gistic regression with Stanford CoreNLP package.
relation
level
mention
level
</bodyText>
<equation confidence="0.970334">
Yi
IRI
hi
Zi
Xi
IXiI
n
G
= Xn X p(yi, hi|xi, wy, wz, G)
i=1 log
hi
X
log
hi
Y
p(hij|xij,wz,G)
p(yri |hi,wry)
|hi|
Y
j=1
rEPiUNi
=
Xn
i=1
</equation>
<listItem confidence="0.968652">
• zij∈R ∪ NR: a latent variable that denotes the
relation of the jth mention in the ith bag
• hij ∈R ∪ NR: a latent variable that denotes the
refined relation of the mention xij
</listItem>
<bodyText confidence="0.943249666666667">
We define relabeled relations hij as following:
�r(g), if ∃!g ∈G s.t.g ={gk}⊆{xij}
hij(xij, zij)=
zij,
otherwise
wry is the weight vector for the rth binary
top-
level aggregation classifier (from mention labels
to bag-level prediction). We use wy to represent
</bodyText>
<equation confidence="0.53066525">
w1 y, w2 y, . . . , w|R|
=
X|hi|
j=1
</equation>
<page confidence="0.932258">
734
</page>
<table confidence="0.99710725">
Iteration 1 2 3 4 5 6 7 8
(a) Corrected relations: 2052 718 648 596 505 545 557 535
(b) Retrieved relations: 10219 860 676 670 621 599 594 592
Total relabelings 12271 1578 1324 1264 1226 1144 1153 1127
</table>
<tableCaption confidence="0.8953955">
Table 4: Number of relabelings for each training iteration of Guided DS: (a) relabelings due to cor-
rected relations, e.g. personChildren —* personSiblings (b) relabelings due to retrieved relations, e.g.
</tableCaption>
<figure confidence="0.295412">
notRelated(NR)—*personTitle
Algorithm 1 : Guided DS training
</figure>
<listItem confidence="0.965962">
1: Phase 1: build set G of guidelines
2: Phase 2: EM training
3: for iteration = 1, ... , T do
4: for i = 1,...,n do
5: for j = 1,...,|xi |do
</listItem>
<equation confidence="0.73318475">
6: z∗ij= argmaxzijp(zij|xi,yi,wz,wy)
(g), if 1! g E G: {gk}C{xij}
7: h*=ti
ij*,otherwise
</equation>
<listItem confidence="0.9589472">
8: update hi with h∗ij
9: end for
10: end for
11: w∗z=argmaxwEn1 Ej|x• ` |1log p(hij  |xij w)
12: for r E R do
wr* =argmaxw E log p(yri |hi,w)
13: y 1&lt;i&lt;n s.t. rEPiUNi
14: end for
15: end for
16: return wz, wy
</listItem>
<bodyText confidence="0.991642666666667">
The pseudocode is presented as algorithm 1.
The following approximation is used for infer-
ence at step 6:
</bodyText>
<equation confidence="0.973364">
p(zij|xi,yi,wz,wy) a p(yi, zij|xi, wy, wz)
� p(zij|xij, wz)p(yi|h�i, wy)
rl= p(zij|xij, wz) p(yri |h„ wry),
rEPiUNi
</equation>
<bodyText confidence="0.99964425">
where h� contains previously inferred and
maybe further relabeled mention labels for group
i (steps 5-10), with the exception of component j
whose label is replaced by zij. In the M-step (lines
12-15) we optimize model parameters wz, wy,
given the current assignment of mention-level la-
bels hi.
Experiments show that Guided DS efficiently
learns new model, resulting in a drastically de-
creasing number of needed relabelings for further
iterations (Table 4). At the inference step we first
classify all mentions:
</bodyText>
<equation confidence="0.728759">
z∗ij = argmaxzERUNR p(z|xij, wz)
</equation>
<bodyText confidence="0.983407">
Then final relation labels for ith entity tuple are
obtained via the top-level classifiers:
</bodyText>
<equation confidence="0.978835">
yr*
i = argmaxyE{P,N} p(y|Z∗i , wry)
</equation>
<sectionHeader confidence="0.998574" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.930576">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999946318181818">
We use the KBP (Ji and Grishman, 2011) dataset3
which is preprocessed by Surdeanu et al. (2011)
using the Stanford parser4 (Klein and Manning,
2003). This dataset is generated by mapping
Wikipedia infoboxes into a large unlabeled corpus
that consists of 1.5M documents from KBP source
corpus and a complete snapshot of Wikipedia.
The KBP 2010 and 2011 data includes 200
query named entities with the relations they are
involved in. We used 40 queries as development
set and the rest 160 queries (3334 entity pairs that
express a relation) as the test set. The official KBP
evaluation is performed by pooling the system re-
sponses and manually reviewing each response,
producing a hand-checked assessment data. We
used KBP 2012 assessment data to generate guide-
lines since queries from different years do not
overlap. It contains about 2500 labeled sentences
of 41 relations, which is less than 0.09% of the
size of the distantly labeled dataset of 2M sen-
tences. The final set G consists of 99 guidelines
(section 2.1).
</bodyText>
<subsectionHeader confidence="0.997326">
5.2 Models
</subsectionHeader>
<bodyText confidence="0.999672181818182">
We implement Guided DS on top of the MIML
(Surdeanu et al., 2012) code base5. Training
MIML on a simple fusion of distantly-labeled
and human-labeled datasets does not improve the
maximum F-score since this hand-labeled data is
swamped by a much larger amount of distant-
supervised data of much lower quality. Upsam-
pling the labeled data did not improve the perfor-
mance either. We experimented with different up-
sampling ratios and report best results using ratio
1:1 in Figure 2.
</bodyText>
<footnote confidence="0.99984275">
3Available from Linguistic Data Consortium (LDC) at
http://projects.ldc.upenn.edu/kbp/data.
4http://nlp.stanford.edu/software/lex-parser.shtml
5Available at http://nlp.stanford.edu/software/mimlre.shtml.
</footnote>
<page confidence="0.997253">
735
</page>
<figure confidence="0.987785">
Recall
Recall
0.8
Guided DS
Semi−MIML
DS+upsampling
MaxEnt
0.5
0.4
0.3
0.2
Precision
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.1
a)
0.7
0.6
0.8
Guided DS
MIML
Mintz++
MultiR
0.5
0.4
0.3
0.2
Precision
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
b) 0.1
0.7
0.6
o el 1 o el 1
ax nt 2 12 1 1 MultiR 3 1 2
+ sam ling 32 2 2 31 2 2 12 Mintz++ 2 1 2 2 1 1
emi 3 2 2 21 2 12 31 MIML 2 2 2 3 11
Guided DS
</figure>
<figureCaption confidence="0.992046">
Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling,
</figureCaption>
<figure confidence="0.626236181818182">
Student Version �� ����� Stdt Vei f MATLAB
Semi-MIML (Min et al., 2013) b) state-of-art models: Mintz++ (Mintz et al., 2009), MultiR (Hoffmann
0.
05 0
et al., 2011), MIML (Surdeanu et al., 2012)
02 0.25 0.3 0.35 0.4 0.45
Our baselines: 1) MaxEnt is a supervised maxi-
mum
data;
peri
a
</figure>
<bodyText confidence="0.952642416666667">
)
Semi-MIML is a recent semi-supervised exten-
sion. We also compare Guided DS with three
state-of-the-art models: 1) M u lti R and 2) MIML
are two distant supervision models that support
multi-instance learning and overlapping relations;
3) Mintz++ is asingle-instance learning algorithm
for distantasupervision. The difference between
Guided DS
with p-valu
t-test assuming a normal distribution.
n
</bodyText>
<subsectionHeader confidence="0.902386">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99989725">
i n
We scored our model against all 41 relations and
thus replicated the actual KBP evaluation. Figure
2 shows that our model cons
</bodyText>
<equation confidence="0.671243222222222">
2 2 1 Acknowledgments
algorithms at
ase
all six
almost all recall levels and im-
+ sam ling32 2 2 31 2
form logistic regression baseline. Performance
t
a
</equation>
<bodyText confidence="0.98693825">
of Guided DS also com ares favorabl with best
scored hand-coded systems for a similar task such
as Sun et al., (2011) system for KBP 2011, which
reports an F-score of 25.7%.
</bodyText>
<subsectionHeader confidence="0.699992">
Work
</subsectionHeader>
<bodyText confidence="0.772403421052631">
We show that relation extractors trained with dis-
tant supervision can benefitnsignificantly from a
small number of human labeled examples. We
propose a strategy to generate and select guide-
lines so that they are more generalized forms of
for relation extraction. Our approach significantly
improves performance in practice and thus opens
up many opportunities for further research in RE
where only a very limited amount of labeled train-
ing data is available.
distantly-labeled and human-labeled data; 3
enltropy baseline trained on a human-labeled p y
2)nDS+upsampling is an upsampling ex-
ment, wherenM I M3L was trained on a mix1of
and all other systems is significant labeled instances. We show1how to incorporate
e less than 0.05 according to a paired
-
d D
these guidelines into an existing state-of
</bodyText>
<figure confidence="0.987552857142857">
art model
istently outperforms of
1 Multi
ate
aximum F-score by more than 13.5%
mi 3 22 21 2 Supported by the Intelligence Advanced Researc
12 31 MIML 2 2 2 3
</figure>
<bodyText confidence="0.94834775">
the
relative to
as increases the area under precision-recall curve
by more than 37% (from 11.74 to 16.1). Also,
Guided DS improves the overall recall by more
than 9% absolute (from 30.9% to 39.93%) at a
comparable level of precision (24.35% for
vs 23.64% for Guided DS), while increases the
running time of MIML by only 3%. Thus, our
approach outperforms
model for
relation extraction using much less labeled data
that was used by Zhan
proves
e
MIML (from 28.35% to 32.19%) as well
</bodyText>
<equation confidence="0.4218002">
Guided DS
MIML
state-of-the-art
g et al., (2012) to outper-
h
</equation>
<sectionHeader confidence="0.995065" genericHeader="conclusions">
6 Conclusions and Future
</sectionHeader>
<bodyText confidence="0.971943846153846">
st
Projects Activity (IARPA) via Air Force Research
Laboratory (AFRL) contract number FA8650-10-
C-7058. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright anno-
1
tation thereon. The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of IARPA, AFRL, or the U.S. Govern-
ment.
</bodyText>
<page confidence="0.996707">
736
</page>
<sectionHeader confidence="0.91085" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986622289719626">
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92–100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Michael Collins and Yorav Singer. 1999. Unsuper-
vised models for named entity classification. Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-VLC). ,
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77–86.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68–74.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 49th Annual Meetings of the
Association for Computational Linguistics (ACL),
pages 286–295.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541–550.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1148–1158.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC-2011 knowledge base popula-
tion track. In Text Analysis Conference Workshop.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Annual Meetings of the Association for Com-
putational Linguistics (ACL).
Percy Liang, Michael I.Jordan and Dan Klein. 2009.
Learning From Measurements in Exponential Fami-
lies. In Proceedings of the 26th Annual International
Conference on Machine Learning (ICML), pages =
641–648
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003–1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64–71.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
277–282.
Kamal Paul Nigam. 2001. Using Unlabeled Data to
Improve Text Classification. Ph.D. thesis, School of
Computer Science, Carnegie Mellon University.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148–163.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics.
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow 2013. A Survey of Noise Reduc-
tion Methods for Distant Supervision. In Proceed-
ings of Conference on Information and Knowledge
Management (CIKM-AKBC).
Benjamin Snyder and Regina Barzilay 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of IJCAI.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In TextAnalysis Conference (TAC-KBP).
Mihai Surdeanu, J. Turmo, and A. Ageno. 2006. A
hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the 11th Con-
ference of the European Chapter of the Associate
for Computational Linguistics Workshop on Adap-
tive Text Extraction and Mining (EACL).
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D.Manning. 2011. Stanford’s
</reference>
<page confidence="0.97072">
737
</page>
<reference confidence="0.999307789473684">
Distantly-Supervised Slot-Filling System. In Pro-
ceedings of the Text Analysis Conference (TAC-
KBP).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455–465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721–729.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 41–50.
Wei Xu, Raphael Hoffmann, Zhao Le, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ce Zhang, Feng Niu, Christopher R´e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 825–834. Associ-
ation for Computational Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL).
</reference>
<page confidence="0.997043">
738
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549416">
<title confidence="0.999343">Infusion of Labeled Data into Distant Supervision for Relation Extraction</title>
<author confidence="0.98345">Pershina Xu Grishman</author>
<affiliation confidence="0.791068">York University, New York, BBN Technologies, Cambridge,</affiliation>
<address confidence="0.956134">of Pennsylvania, Philadelphia,</address>
<email confidence="0.999927">xwe@cis.upenn.edu</email>
<abstract confidence="0.998472733333333">Distant supervision usually utilizes only unlabeled data and existing knowledge bases to learn relation extraction models. However, in some cases a small amount of human labeled data is available. In this paper, we demonstrate how a state-of-theart multi-instance multi-label model can be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom M Mitchell</author>
</authors>
<title>Combining labeled and unlabeled sata with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT),</booktitle>
<pages>92--100</pages>
<contexts>
<context position="3397" citStr="Blum and Mitchell, 1998" startWordPosition="529" endWordPosition="532">level extraction task. In this paper, we present the first effective approach, Guided DS (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences. In contrast to simply taking the union of the hand-labeled data and the corpus labeled by distant supervision as in the previous work by Zhang et al. (2012), we generalize the labeled data through feature selection and model this additional information directly in the latent variable approaches. Aside from previous semisupervised work that employs labeled and unlabeled data (Yarowsky, 2013; Blum and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our pro... to get information out of captured al-Qaida leader Abu Zubaydah. ...Abu Zubaydah and former Taliban leader Jalaluddin Haqqani ... True Positive False Positive False Negative ...Abu Zubaydah is one of Osama bin Laden’s senior operational planners... 732 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Shor</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom M. Mitchell. 1998. Combining labeled and unlabeled sata with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT), pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1252" citStr="Bunescu and Mooney, 2007" startWordPosition="179" endWordPosition="182">ke use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve. 1 Introduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance </context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yorav Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-VLC).</booktitle>
<publisher></publisher>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yorav Singer. 1999. Unsupervised models for named entity classification. Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-VLC). ,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB),</booktitle>
<pages>77--86</pages>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB), pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 49th Annual Meetings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>286--295</pages>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 49th Annual Meetings of the Association for Computational Linguistics (ACL), pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke S Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>541--550</pages>
<contexts>
<context position="1915" citStr="Hoffmann et al., 2011" startWordPosition="293" endWordPosition="296">2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. Table 1: Classic errors</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Knowledge base population: Successful approaches and challenges.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1148--1158</pages>
<contexts>
<context position="5047" citStr="Ji and Grishman, 2011" startWordPosition="768" endWordPosition="771">ss →← appos, sister personSiblings person person, poss →← appos, father personParents person title, ← nn personTitle organization person, prep of → appos → organizationTopMembersEmployees person cause, nsubj →← prep of personCauseOfDeath person number, ← appos personAge person date, nsubjpass →← prep on ← num personDateOfBirth Table 2: Some examples from the final set G of extracted guidelines. posed approach, we extend MIML (Surdeanu et al., 2012), a state-of-the-art distant supervision model and show a significant improvement of 13.5% in F-score on the relation extraction benchmark TAC-KBP (Ji and Grishman, 2011) dataset. While prior work employed tens of thousands of human labeled examples (Zhang et al., 2012) and only got a 6.5% increase in F-score over a logistic regression baseline, our approach uses much less labeled data (about 1/8) but achieves much higher improvement on performance over stronger baselines. 2 The Challenge Simply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data. An effective approach must recognize that the handlabeled data is more reliabl</context>
<context position="13472" citStr="Ji and Grishman, 2011" startWordPosition="2211" endWordPosition="2214">he exception of component j whose label is replaced by zij. In the M-step (lines 12-15) we optimize model parameters wz, wy, given the current assignment of mention-level labels hi. Experiments show that Guided DS efficiently learns new model, resulting in a drastically decreasing number of needed relabelings for further iterations (Table 4). At the inference step we first classify all mentions: z∗ij = argmaxzERUNR p(z|xij, wz) Then final relation labels for ith entity tuple are obtained via the top-level classifiers: yr* i = argmaxyE{P,N} p(y|Z∗i , wry) 5 Experiments 5.1 Data We use the KBP (Ji and Grishman, 2011) dataset3 which is preprocessed by Surdeanu et al. (2011) using the Stanford parser4 (Klein and Manning, 2003). This dataset is generated by mapping Wikipedia infoboxes into a large unlabeled corpus that consists of 1.5M documents from KBP source corpus and a complete snapshot of Wikipedia. The KBP 2010 and 2011 data includes 200 query named entities with the relations they are involved in. We used 40 queries as development set and the rest 160 queries (3334 entity pairs that express a relation) as the test set. The official KBP evaluation is performed by pooling the system responses and manua</context>
</contexts>
<marker>Ji, Grishman, 2011</marker>
<rawString>Heng Ji and Ralph Grishman. 2011. Knowledge base population: Successful approaches and challenges. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1148–1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of the TAC-2011 knowledge base population track.</title>
<date>2011</date>
<booktitle>In Text Analysis Conference Workshop.</booktitle>
<marker>Ji, Grishman, Dang, 2011</marker>
<rawString>Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011. Overview of the TAC-2011 knowledge base population track. In Text Analysis Conference Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meetings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="13582" citStr="Klein and Manning, 2003" startWordPosition="2228" endWordPosition="2231">rameters wz, wy, given the current assignment of mention-level labels hi. Experiments show that Guided DS efficiently learns new model, resulting in a drastically decreasing number of needed relabelings for further iterations (Table 4). At the inference step we first classify all mentions: z∗ij = argmaxzERUNR p(z|xij, wz) Then final relation labels for ith entity tuple are obtained via the top-level classifiers: yr* i = argmaxyE{P,N} p(y|Z∗i , wry) 5 Experiments 5.1 Data We use the KBP (Ji and Grishman, 2011) dataset3 which is preprocessed by Surdeanu et al. (2011) using the Stanford parser4 (Klein and Manning, 2003). This dataset is generated by mapping Wikipedia infoboxes into a large unlabeled corpus that consists of 1.5M documents from KBP source corpus and a complete snapshot of Wikipedia. The KBP 2010 and 2011 data includes 200 query named entities with the relations they are involved in. We used 40 queries as development set and the rest 160 queries (3334 entity pairs that express a relation) as the test set. The official KBP evaluation is performed by pooling the system responses and manually reviewing each response, producing a hand-checked assessment data. We used KBP 2012 assessment data to gen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41th Annual Meetings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning From Measurements in Exponential Families.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning (ICML),</booktitle>
<pages>641--648</pages>
<contexts>
<context position="3602" citStr="Liang et al., 2009" startWordPosition="561" endWordPosition="564">contrast to simply taking the union of the hand-labeled data and the corpus labeled by distant supervision as in the previous work by Zhang et al. (2012), we generalize the labeled data through feature selection and model this additional information directly in the latent variable approaches. Aside from previous semisupervised work that employs labeled and unlabeled data (Yarowsky, 2013; Blum and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our pro... to get information out of captured al-Qaida leader Abu Zubaydah. ...Abu Zubaydah and former Taliban leader Jalaluddin Haqqani ... True Positive False Positive False Negative ...Abu Zubaydah is one of Osama bin Laden’s senior operational planners... 732 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–738, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Guideline g = {gi|i = 1, 2, 3}: Relation r(g) types of entities, dependency path, span </context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Percy Liang, Michael I.Jordan and Dan Klein. 2009. Learning From Measurements in Exponential Families. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pages = 641–648</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="2451" citStr="Min et al., 2013" startWordPosition="384" endWordPosition="387">ed multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. Table 1: Classic errors in the training data generated by a toy knowledge base of only one entry personTitle(Abu Zubaydah, leader). However, the potential of these previously proposed approaches is limited by the inevitable gap between the relation-level knowledge and the instance-level extraction task. In this paper, we present the first effective approach, Guided DS (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences. In contrast to simply taking the union of the hand-labeled data and the</context>
<context position="15712" citStr="Min et al., 2013" startWordPosition="2596" endWordPosition="2599"> at http://nlp.stanford.edu/software/mimlre.shtml. 735 Recall Recall 0.8 Guided DS Semi−MIML DS+upsampling MaxEnt 0.5 0.4 0.3 0.2 Precision 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.1 a) 0.7 0.6 0.8 Guided DS MIML Mintz++ MultiR 0.5 0.4 0.3 0.2 Precision 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 b) 0.1 0.7 0.6 o el 1 o el 1 ax nt 2 12 1 1 MultiR 3 1 2 + sam ling 32 2 2 31 2 2 12 Mintz++ 2 1 2 2 1 1 emi 3 2 2 21 2 12 31 MIML 2 2 2 3 11 Guided DS Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling, Student Version �� ����� Stdt Vei f MATLAB Semi-MIML (Min et al., 2013) b) state-of-art models: Mintz++ (Mintz et al., 2009), MultiR (Hoffmann 0. 05 0 et al., 2011), MIML (Surdeanu et al., 2012) 02 0.25 0.3 0.35 0.4 0.45 Our baselines: 1) MaxEnt is a supervised maximum data; peri a ) Semi-MIML is a recent semi-supervised extension. We also compare Guided DS with three state-of-the-art models: 1) M u lti R and 2) MIML are two distant supervision models that support multi-instance learning and overlapping relations; 3) Mintz++ is asingle-instance learning algorithm for distantasupervision. The difference between Guided DS with p-valu t-test assuming a normal distri</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedigns of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL),</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1226" citStr="Mintz et al., 2009" startWordPosition="175" endWordPosition="178">an be modified to make use of these reliable sentence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve. 1 Introduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sop</context>
<context position="15765" citStr="Mintz et al., 2009" startWordPosition="2604" endWordPosition="2607">735 Recall Recall 0.8 Guided DS Semi−MIML DS+upsampling MaxEnt 0.5 0.4 0.3 0.2 Precision 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.1 a) 0.7 0.6 0.8 Guided DS MIML Mintz++ MultiR 0.5 0.4 0.3 0.2 Precision 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 b) 0.1 0.7 0.6 o el 1 o el 1 ax nt 2 12 1 1 MultiR 3 1 2 + sam ling 32 2 2 31 2 2 12 Mintz++ 2 1 2 2 1 1 emi 3 2 2 21 2 12 31 MIML 2 2 2 3 11 Guided DS Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling, Student Version �� ����� Stdt Vei f MATLAB Semi-MIML (Min et al., 2013) b) state-of-art models: Mintz++ (Mintz et al., 2009), MultiR (Hoffmann 0. 05 0 et al., 2011), MIML (Surdeanu et al., 2012) 02 0.25 0.3 0.35 0.4 0.45 Our baselines: 1) MaxEnt is a supervised maximum data; peri a ) Semi-MIML is a recent semi-supervised extension. We also compare Guided DS with three state-of-the-art models: 1) M u lti R and 2) MIML are two distant supervision models that support multi-instance learning and overlapping relations; 3) Mintz++ is asingle-instance learning algorithm for distantasupervision. The difference between Guided DS with p-valu t-test assuming a normal distribution. n 5.3 Results i n We scored our model against</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedigns of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL), pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Nallapati</author>
</authors>
<title>Discriminative models for information retrieval.</title>
<date>2004</date>
<booktitle>In Proceedigns of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>64--71</pages>
<marker>Nallapati, 2004</marker>
<rawString>Ramesh Nallapati. 2004. Discriminative models for information retrieval. In Proceedigns of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>277--282</pages>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc-Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 277–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Paul Nigam</author>
</authors>
<title>Using Unlabeled Data to Improve Text Classification.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="3436" citStr="Nigam, 2001" startWordPosition="537" endWordPosition="538">e first effective approach, Guided DS (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences. In contrast to simply taking the union of the hand-labeled data and the corpus labeled by distant supervision as in the previous work by Zhang et al. (2012), we generalize the labeled data through feature selection and model this additional information directly in the latent variable approaches. Aside from previous semisupervised work that employs labeled and unlabeled data (Yarowsky, 2013; Blum and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our pro... to get information out of captured al-Qaida leader Abu Zubaydah. ...Abu Zubaydah and former Taliban leader Jalaluddin Haqqani ... True Positive False Positive False Negative ...Abu Zubaydah is one of Osama bin Laden’s senior operational planners... 732 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–738, Baltimore, Ma</context>
</contexts>
<marker>Nigam, 2001</marker>
<rawString>Kamal Paul Nigam. 2001. Using Unlabeled Data to Improve Text Classification. Ph.D. thesis, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedigns of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),</booktitle>
<pages>148--163</pages>
<contexts>
<context position="1892" citStr="Riedel et al., 2010" startWordPosition="289" endWordPosition="292">, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedigns of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD), pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Luke Zettlemoyer</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Modeling missing data in distant supervision for information extraction. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="2432" citStr="Ritter et al., 2013" startWordPosition="380" endWordPosition="383"> example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. Table 1: Classic errors in the training data generated by a toy knowledge base of only one entry personTitle(Abu Zubaydah, leader). However, the potential of these previously proposed approaches is limited by the inevitable gap between the relation-level knowledge and the instance-level extraction task. In this paper, we present the first effective approach, Guided DS (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sentences. In contrast to simply taking the union of the hand-l</context>
</contexts>
<marker>Ritter, Zettlemoyer, Mausam, Etzioni, 2013</marker>
<rawString>Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Etzioni. 2013. Modeling missing data in distant supervision for information extraction. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Tassilo Barth</author>
<author>Michael Wiegand</author>
<author>Dietrich Klakow</author>
</authors>
<title>A Survey of Noise Reduction Methods for Distant Supervision.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Information and Knowledge Management (CIKM-AKBC).</booktitle>
<contexts>
<context position="1768" citStr="Roth et al., 2013" startWordPosition="269" endWordPosition="272">ause of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., </context>
</contexts>
<marker>Roth, Barth, Wiegand, Klakow, 2013</marker>
<rawString>Benjamin Roth, Tassilo Barth, Michael Wiegand, and Dietrich Klakow 2013. A Survey of Noise Reduction Methods for Distant Supervision. In Proceedings of Conference on Information and Knowledge Management (CIKM-AKBC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Database-text alignment via structured multilabel classification.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="1279" citStr="Snyder and Barzilay, 2007" startWordPosition="183" endWordPosition="186">entence-level labels in addition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve. 1 Introduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay 2007. Database-text alignment via structured multilabel classification. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Wei Xu</author>
<author>Bonan Min</author>
</authors>
<title>system for kbp slot filling.</title>
<date>2011</date>
<booktitle>In TextAnalysis Conference (TAC-KBP).</booktitle>
<location>New york university</location>
<contexts>
<context position="16734" citStr="Sun et al., (2011)" startWordPosition="2770" endWordPosition="2773">instance learning and overlapping relations; 3) Mintz++ is asingle-instance learning algorithm for distantasupervision. The difference between Guided DS with p-valu t-test assuming a normal distribution. n 5.3 Results i n We scored our model against all 41 relations and thus replicated the actual KBP evaluation. Figure 2 shows that our model cons 2 2 1 Acknowledgments algorithms at ase all six almost all recall levels and im+ sam ling32 2 2 31 2 form logistic regression baseline. Performance t a of Guided DS also com ares favorabl with best scored hand-coded systems for a similar task such as Sun et al., (2011) system for KBP 2011, which reports an F-score of 25.7%. Work We show that relation extractors trained with distant supervision can benefitnsignificantly from a small number of human labeled examples. We propose a strategy to generate and select guidelines so that they are more generalized forms of for relation extraction. Our approach significantly improves performance in practice and thus opens up many opportunities for further research in RE where only a very limited amount of labeled training data is available. distantly-labeled and human-labeled data; 3 enltropy baseline trained on a huma</context>
</contexts>
<marker>Sun, Grishman, Xu, Min, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New york university 2011 system for kbp slot filling. In TextAnalysis Conference (TAC-KBP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>J Turmo</author>
<author>A Ageno</author>
</authors>
<title>A hybrid approach for the acquisition of information extraction patterns.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Associate for Computational Linguistics Workshop on Adaptive Text Extraction and Mining (EACL).</booktitle>
<marker>Surdeanu, Turmo, Ageno, 2006</marker>
<rawString>Mihai Surdeanu, J. Turmo, and A. Ageno. 2006. A hybrid approach for the acquisition of information extraction patterns. In Proceedings of the 11th Conference of the European Chapter of the Associate for Computational Linguistics Workshop on Adaptive Text Extraction and Mining (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sonal Gupta</author>
<author>John Bauer</author>
<author>David McClosky</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford’s Distantly-Supervised Slot-Filling System.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analysis Conference (TACKBP).</booktitle>
<contexts>
<context position="6702" citStr="Surdeanu et al., (2011)" startWordPosition="1033" endWordPosition="1037"> into latent variable model. 2.1 Guidelines The sparse nature of feature space dilutes the discriminative capability of useful features. Given the small amount of hand-labeled data, it is important to identify a small set of features that are general enough while being capable of predicting quite accurately the type of relation that may hold between two entities. We experimentally tested alternative feature sets by building supervised Maximum Entropy (MaxEnt) models using the hand-labeled data (Table 3), and selected an effective combination of three features from the full feature set used by Surdeanu et al., (2011): • the semantic types of the two arguments (e.g. person, organization, location, date, title, ...) • the sequence of dependency relations along the path connecting the heads of the two arguments in the dependency tree. • a word in the sentence between the two arguments These three features are strong indicators of the type of relation between two entities. In some cases the semantic types of the arguments alone narrows the possibilities to one or two relation types. For example, entity types such as person and title often implies the relation personTitle. Some lexical items are clear indicato</context>
<context position="13529" citStr="Surdeanu et al. (2011)" startWordPosition="2220" endWordPosition="2223">j. In the M-step (lines 12-15) we optimize model parameters wz, wy, given the current assignment of mention-level labels hi. Experiments show that Guided DS efficiently learns new model, resulting in a drastically decreasing number of needed relabelings for further iterations (Table 4). At the inference step we first classify all mentions: z∗ij = argmaxzERUNR p(z|xij, wz) Then final relation labels for ith entity tuple are obtained via the top-level classifiers: yr* i = argmaxyE{P,N} p(y|Z∗i , wry) 5 Experiments 5.1 Data We use the KBP (Ji and Grishman, 2011) dataset3 which is preprocessed by Surdeanu et al. (2011) using the Stanford parser4 (Klein and Manning, 2003). This dataset is generated by mapping Wikipedia infoboxes into a large unlabeled corpus that consists of 1.5M documents from KBP source corpus and a complete snapshot of Wikipedia. The KBP 2010 and 2011 data includes 200 query named entities with the relations they are involved in. We used 40 queries as development set and the rest 160 queries (3334 entity pairs that express a relation) as the test set. The official KBP evaluation is performed by pooling the system responses and manually reviewing each response, producing a hand-checked ass</context>
</contexts>
<marker>Surdeanu, Gupta, Bauer, McClosky, Chang, Spitkovsky, Manning, 2011</marker>
<rawString>Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X. Chang, Valentin I. Spitkovsky, and Christopher D.Manning. 2011. Stanford’s Distantly-Supervised Slot-Filling System. In Proceedings of the Text Analysis Conference (TACKBP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>455--465</pages>
<contexts>
<context position="2011" citStr="Surdeanu et al., 2012" startWordPosition="312" endWordPosition="315"> of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. Table 1: Classic errors in the training data generated by a toy knowledge base of only one entry personTitle(Abu Zubayd</context>
<context position="4877" citStr="Surdeanu et al., 2012" startWordPosition="743" endWordPosition="746"> personSpouse person organization, nsubj →← prep of, became personMemberOf organization organization, nsubj →← prep of, company organizationSubsidiaries person person, poss →← appos, sister personSiblings person person, poss →← appos, father personParents person title, ← nn personTitle organization person, prep of → appos → organizationTopMembersEmployees person cause, nsubj →← prep of personCauseOfDeath person number, ← appos personAge person date, nsubjpass →← prep on ← num personDateOfBirth Table 2: Some examples from the final set G of extracted guidelines. posed approach, we extend MIML (Surdeanu et al., 2012), a state-of-the-art distant supervision model and show a significant improvement of 13.5% in F-score on the relation extraction benchmark TAC-KBP (Ji and Grishman, 2011) dataset. While prior work employed tens of thousands of human labeled examples (Zhang et al., 2012) and only got a 6.5% increase in F-score over a logistic regression baseline, our approach uses much less labeled data (about 1/8) but achieves much higher improvement on performance over stronger baselines. 2 The Challenge Simply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effe</context>
<context position="8420" citStr="Surdeanu et al., 2012" startWordPosition="1320" endWordPosition="1323"> data using all features (Surdeanu et al., 2011) vs using a subset of two (types of entities, dependency path), or three (adding a span word) features, and evaluated on the test set. 733 which make the correct prediction for all and at least k=3 examples in the training corpus (threshold 3 was obtained by running experiments on the development dataset). Table 2 shows some examples in the final set G of extracted guidelines. 3 Guided DS Our goal is to jointly model human-labeled ground truth and structured data from a knowledge base in distant supervision. To do this, we extend the MIML model (Surdeanu et al., 2012) by adding a new layer as shown in Figure 1. The input to the model consists of (1) distantly supervised data, represented as a list of n bags1 with a vector yi of binary gold-standard labels, either Positive(P) or Negative(N) for each relation r∈R; (2) generalized human-labeled ground truth, represented as a set G of feature conjunctions g={gi|i=1,2,3} associated with a unique relation r(g). Given a bag of sentences, xi, which mention an ith entity pair (e1, e2), our goal is to correctly predict which relation is mentioned in each sentence, or NR if none of the relations under consideration a</context>
<context position="9716" citStr="Surdeanu et al. (2012)" startWordPosition="1554" endWordPosition="1557">ons for the ith entity pair. We introduce a set of latent variables hi which model human ground truth for each mention in the ith bag and take precedence over the current model assignment zi. Figure 1: Plate diagram of Guided DS Let i, j be the index in the bag and the mention level, respectively. We model mentionlevel extraction p(zij|xij; wz), human relabeling hij(xij, zij) and multi-label aggregation p(yri |hi; wy). We define: • yri ∈{P, N} : r holds for the ith bag or not. • xij is the feature representation of the jth relation mention in the ith bag. We use the same set of features as in Surdeanu et al. (2012). 1A bag is a set of mentions sharing same entity pair. . y Our approach is aimed at improving the mentionlevel classifier, while keeping the multi-instance multi-label framework to allow for joint modeling. 4 Training We use a hard expectation maximization algorithm to train the model. Our objective function is to maximize log-likelihood of the data: LL(wy, wz) = Xn log p(yi|xi, wy, wz, G) i=1 where the last equality is due to conditional independence. Because of the non-convexity of LL(wy, wz) we approximate and maximize the joint log-probability p(yi,hi|xi,wy,wz,G) for each entity pair in t</context>
<context position="14528" citStr="Surdeanu et al., 2012" startWordPosition="2388" endWordPosition="2391">d the rest 160 queries (3334 entity pairs that express a relation) as the test set. The official KBP evaluation is performed by pooling the system responses and manually reviewing each response, producing a hand-checked assessment data. We used KBP 2012 assessment data to generate guidelines since queries from different years do not overlap. It contains about 2500 labeled sentences of 41 relations, which is less than 0.09% of the size of the distantly labeled dataset of 2M sentences. The final set G consists of 99 guidelines (section 2.1). 5.2 Models We implement Guided DS on top of the MIML (Surdeanu et al., 2012) code base5. Training MIML on a simple fusion of distantly-labeled and human-labeled datasets does not improve the maximum F-score since this hand-labeled data is swamped by a much larger amount of distantsupervised data of much lower quality. Upsampling the labeled data did not improve the performance either. We experimented with different upsampling ratios and report best results using ratio 1:1 in Figure 2. 3Available from Linguistic Data Consortium (LDC) at http://projects.ldc.upenn.edu/kbp/data. 4http://nlp.stanford.edu/software/lex-parser.shtml 5Available at http://nlp.stanford.edu/softw</context>
<context position="15835" citStr="Surdeanu et al., 2012" startWordPosition="2617" endWordPosition="2620">0.4 0.3 0.2 Precision 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.1 a) 0.7 0.6 0.8 Guided DS MIML Mintz++ MultiR 0.5 0.4 0.3 0.2 Precision 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 b) 0.1 0.7 0.6 o el 1 o el 1 ax nt 2 12 1 1 MultiR 3 1 2 + sam ling 32 2 2 31 2 2 12 Mintz++ 2 1 2 2 1 1 emi 3 2 2 21 2 12 31 MIML 2 2 2 3 11 Guided DS Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling, Student Version �� ����� Stdt Vei f MATLAB Semi-MIML (Min et al., 2013) b) state-of-art models: Mintz++ (Mintz et al., 2009), MultiR (Hoffmann 0. 05 0 et al., 2011), MIML (Surdeanu et al., 2012) 02 0.25 0.3 0.35 0.4 0.45 Our baselines: 1) MaxEnt is a supervised maximum data; peri a ) Semi-MIML is a recent semi-supervised extension. We also compare Guided DS with three state-of-the-art models: 1) M u lti R and 2) MIML are two distant supervision models that support multi-instance learning and overlapping relations; 3) Mintz++ is asingle-instance learning algorithm for distantasupervision. The difference between Guided DS with p-valu t-test assuming a normal distribution. n 5.3 Results i n We scored our model against all 41 relations and thus replicated the actual KBP evaluation. Figur</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>721--729</pages>
<contexts>
<context position="2355" citStr="Takamatsu et al., 2012" startWordPosition="365" endWordPosition="368">d training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. Table 1: Classic errors in the training data generated by a toy knowledge base of only one entry personTitle(Abu Zubaydah, leader). However, the potential of these previously proposed approaches is limited by the inevitable gap between the relation-level knowledge and the instance-level extraction task. In this paper, we present the first effective approach, Guided DS (distant supervision), to incorporate labeled data into distant supervision for extracting r</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 721–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Autonomously semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>41--50</pages>
<contexts>
<context position="1299" citStr="Wu and Weld, 2007" startWordPosition="187" endWordPosition="190">ition to the relation-level distant supervision from a database. Experiments show that our approach achieves a statistically significant increase of 13.5% in F-score and 37% in area under the precision recall curve. 1 Introduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2) that participate in a relation, r, is likely to express the fact r(e1,e2) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffm</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the International Conference on Information and Knowledge Management (CIKM), pages 41–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Raphael Hoffmann</author>
<author>Zhao Le</author>
<author>Ralph Grishman</author>
</authors>
<title>Filling knowledge base gaps for distant supervision of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2373" citStr="Xu et al., 2013" startWordPosition="369" endWordPosition="372"> al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University Surdeanu et al., 2012) have been proposed to address the issue by loosening the distant supervision assumption. These approaches consider all mentions of the same pair (e1,e2) and assume that atleast-one mention actually expresses the relation. On top of that, researchers further improved performance by explicitly adding preprocessing steps (Takamatsu et al., 2012; Xu et al., 2013) or additional layers inside the model (Ritter et al., 2013; Min et al., 2013) to reduce the effect of training noise. Table 1: Classic errors in the training data generated by a toy knowledge base of only one entry personTitle(Abu Zubaydah, leader). However, the potential of these previously proposed approaches is limited by the inevitable gap between the relation-level knowledge and the instance-level extraction task. In this paper, we present the first effective approach, Guided DS (distant supervision), to incorporate labeled data into distant supervision for extracting relations from sent</context>
</contexts>
<marker>Xu, Hoffmann, Le, Grishman, 2013</marker>
<rawString>Wei Xu, Raphael Hoffmann, Zhao Le, and Ralph Grishman. 2013. Filling knowledge base gaps for distant supervision of relation extraction. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ce Zhang</author>
<author>Feng Niu</author>
<author>Christopher R´e</author>
<author>Jude Shavlik</author>
</authors>
<title>Big data versus the crowd: Looking for relationships in all the right places.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>825--834</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Zhang, Niu, R´e, Shavlik, 2012</marker>
<rawString>Ce Zhang, Feng Niu, Christopher R´e, and Jude Shavlik. 2012. Big data versus the crowd: Looking for relationships in all the right places. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 825–834. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5952" citStr="Zhou et al., 2005" startWordPosition="917" endWordPosition="920"> stronger baselines. 2 The Challenge Simply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data. An effective approach must recognize that the handlabeled data is more reliable than the automatically labeled data and so must take precedence in cases of conflict. Conflicts cannot be limited to those cases where all the features in two examples are the same; this would almost never occur, because of the dozens of features used by a typical relation extractor (Zhou et al., 2005). Instead we propose to perform feature selection to generalize human labeled data into training guidelines, and integrate them into latent variable model. 2.1 Guidelines The sparse nature of feature space dilutes the discriminative capability of useful features. Given the small amount of hand-labeled data, it is important to identify a small set of features that are general enough while being capable of predicting quite accurately the type of relation that may hold between two entities. We experimentally tested alternative feature sets by building supervised Maximum Entropy (MaxEnt) models us</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>