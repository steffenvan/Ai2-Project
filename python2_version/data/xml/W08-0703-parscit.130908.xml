<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000035">
<title confidence="0.9978495">
A Bayesian model of natural language phonology:
generating alternations from underlying forms
</title>
<author confidence="0.862897">
David Ellis
</author>
<email confidence="0.947662">
de@cs.brown.edu
</email>
<note confidence="0.2691025">
Brown University
Providence, RI 02912
</note>
<sectionHeader confidence="0.98489" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997873">
A stochastic approach to learning phonology.
The model presented captures 7-15% more
phonologically plausible underlying forms
than a simple majority solution, because it
prefers “pure” alternations. It could be use-
ful in cases where an approximate solution is
needed, or as a seed for more complex mod-
els. A similar process could be involved in
some stages of child language acquisition; in
particular, early learning of phonotactics.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891761904762">
Sound changes in natural language, such as stem
variation in inflected forms, can be described as
phonological processes. These are governed by a
constraint hierarchy as in Optimality Theory (OT),
or by a set of ordered rules. Both rely on a sin-
gle lexical representation of each morpheme (i.e., its
underlying form), and context-sensitive transforma-
tions to surface forms. Phonological changes often
affect segments near morpheme boundaries, but can
also apply over an entire prosodic word, as in vowel
harmony.
It does not seem straightforward to incorporate
context into a Bayesian model of phonology, al-
though a clever solution may yet be found. A
standard way of incorporating conditioning envi-
ronments is to treat them as factors in a Gibbs
model (Liang and Klein, 2007), but such models
require an explicit calculation of the partition func-
tion. Unless the rule contexts possess some kind of
locality, we don’t know how to compute this par-
tition function efficiently. Some context could be
</bodyText>
<page confidence="0.988056">
12
</page>
<bodyText confidence="0.997208333333333">
captured by generating underlying phonemes from
an n-gram model, or by annotating surface forms
with neighborhood features. However, the effects of
autosegmental phonology and other long-range de-
pendencies (like vowel harmony) cannot be easily
Bayesianized.
</bodyText>
<sectionHeader confidence="0.713677" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999973">
In the last decade, finite-state approaches to phonol-
ogy (Gildea and Jurafsky, 1996; Beesley and Kart-
tunen, 2000) have effectively brought theoretical lin-
guistic work on rewrite rules into the computational
realm. A finite-state approximation of optimality
theory (Karttunen, 1998) was later refined into a
compact treatment of gradient constraints (Gerde-
mann and van Noord, 2000).
Recent work on Bayesian models of morpholog-
ical segmentation (Johnson et al., 2007) could be
combined with phonological rule induction (Gold-
water and Johnson, 2004) in a variety of ways,
some of which will be explored in our discussion
of future work. Also, the Hierarchical Bayes Com-
piler (Daume III, 2007) could be used to generate a
model similar to the one presented here, but less con-
strained&apos; which makes correspondingly more ran-
dom, less accurate predictions.
</bodyText>
<subsectionHeader confidence="0.653532">
1.2 Dataset
</subsectionHeader>
<bodyText confidence="0.9994685">
As we describe the model and its implementation in
this and subsequent sections, we will refer to a sam-
</bodyText>
<note confidence="0.78960925">
&apos;Recent updates to HBC, inspired by discussions with the
author, have addressed some of these limitations.
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12–19,
Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9993019">
ple dataset (in Figure 1), consisting of a paradigm2
of verb stems and person/number suffixes. The
head of each row or column is an /underlying/ form,
which in 3rd person singular is a phonologically null
segment (represented as /ø/). In [surface] forms, the
realization of each morpheme is affected by phono-
logical processes. For example, in the combination
of /tiet¨a/ + /vat/, the result is [tiet¨a+v¨at], where the
3rd person plural /a/ becomes [¨a] due to vowel har-
mony.
</bodyText>
<subsectionHeader confidence="0.999185">
1.3 Bayesian Approach
</subsectionHeader>
<bodyText confidence="0.999974571428571">
As a baseline model, we select the most frequently
occurring allophone as the underlying form. Our
goal is to outperform this baseline using a Bayesian
model. In other words, what patterns in phonologi-
cal processes can be inferred with such a statistical
model? This simple framework begins learning with
the assumption that the underlying forms are faithful
to the surface (i.e., without considering markedness
or phonotactics).
We model the generation of surface forms from
underlying ones on the segmental (character) level.
Input is an inflectional paradigm, with tokens of the
form stem+suffix. Morphology is limited to a
single suffix (no agglutination), and is already iden-
tified. Each character of an underlying stem or suf-
fix (ui) generates surface characters (sig) in an entire
row or column of the input.
To capture the phonology of a variety of lan-
guages with a single model, we need constraints
from linguistically plausible priors (universal gram-
mar). We prefer that underlying characters be pre-
served in surface forms, especially when there is no
alternation. It is also reasonable that there be fewer
underlying forms (phonemes) than surface forms
(phones, phonetic inventory), to account for allo-
phones. We expect to be able to capture a signifi-
cant subset of phonological processes using a simple
model (only faithfulness constraints).
</bodyText>
<subsectionHeader confidence="0.998155">
1.4 Pure Generators
</subsectionHeader>
<bodyText confidence="0.999438">
Our model has an advantage over the baseline in its
preference for “purity” in underlying forms. Each
underlying segment should generate as few distinct
</bodyText>
<footnote confidence="0.443145">
2The paradigm format lends itself to analysis of word types,
but if supplemented with surface counts, can also handle tokens.
</footnote>
<bodyText confidence="0.9984299">
surface segments as possible: if it generates non-
alternating (identical) segments, it will be less likely
to generate an alternation in addition. This means
that when two segments alternate, the underlying
form should be the one that appears less frequently
in other contexts, irrespective of the majority within
the alternation.
In the first stem of our Finnish verb conjugation
(Figure 1), we see a [t,d] alternation (a case of con-
sonant gradation), as well as unalternating [t]. If we
isolate three of the surface forms where /tiet¨a/ is in-
flected (1st person singular, and 3rd person singular
and plural), and consider only the dental segments in
the stem of each, we have two underlying segments.
Here, we use question marks to indicate unknown
underlying segments.
/??/ [dt] [tt] [tt]
In this subset of the data, the reasonable candidate
underlying forms are /t/ and /d/. These two compete
to explain the observed data (surface forms). The na-
ture of the prior probability distribution determines
whether the majority is hypothesized for each under-
lying form, so /t/ produces both alternating and unal-
ternating surface segments, or /d/ is hypothesized as
the source of the alternation (and /t/ remains “pure”).
In a Bayesian setting, we impose a sparse prior over
underlying forms conditioned on the surface forms
they generate.
If u2 is hypothesized to be /t/, the posterior prob-
ability of u1 being /t/ goes down:
</bodyText>
<equation confidence="0.98796">
P(u1 = /t/Ju2 = /t/) &lt; P(u1 = /t/)
</equation>
<bodyText confidence="0.981722">
The probability of u1 being the competitor, /d/, cor-
respondingly increases:
</bodyText>
<equation confidence="0.993761">
P(u1 = /d/|u2 = /t/) &gt; P(u1 = /d/)
</equation>
<bodyText confidence="0.9999852">
Even though the majority in this case would be /t/,
the favored candidate for the alternating form was
/d/. This happened because of how we defined the
model’s prior, in combination with the evidence that
/t/ (assigned to u2) generated the sequence of [t]. So
selection bias prefers /d/ as the source of an ambigu-
ous segment, leaving /t/ to always generate itself.
A similar effect can occur if there are both unal-
ternating [t]’s and [d]’s on the surface, in addition to
the [t,d] alternation. The candidate (/t/ or /d/) that is
</bodyText>
<page confidence="0.993586">
13
</page>
<table confidence="0.650874444444444">
����� /n/ (1s) /t/ (2s) /ø/ (3s) /mme/ (1p) /tte/ (2p) /vat/ (3p)
StemSuffix
/tiet¨a/ [tied¨a+n] [tied¨a+t] [tiet¨a+¨a] [tied¨a+mme] [tied¨a+tte] [tiet¨a+v¨at]
/aiko/ [aiøo+n] [aiøo+t] [aiko+o] [aiøo+mme] [aiøo+tte] [aiko+vat]
/luke/ [luøe+n] [luøe+t] [luke+e] [luøe+mme] [luøe+tte] [luke+vat]
/puhu/ [puhu+n] [puhu+t] [puhu+u] [puhu+mme] [puhu+tte] [puhu+vat]
/saa/ [saa+n] [saa+t] [saa+ø] [saa+mme] [saa+tte] [saa+vat]
/tule/ [tule+n] [tule+t] [tule+e] [tule+mme] [tule+tte] [tule+vat]
/pelk¨a¨a/ [pelk¨a¨a+n] [pelk¨a¨a+t] [pelk¨a¨a+ø] [pelk¨a¨a+mme] [pelk¨a¨a+tte] [pelk¨a¨a+v¨at]
</table>
<figureCaption confidence="0.99442">
Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.
</figureCaption>
<bodyText confidence="0.999867882352941">
generating fewer unalternating segments is preferred
to explain the alternation. For example, if there were
1000 cases of [t], 500 [d] and 500 [t,d], we would ex-
pect the following hypotheses: /t/ → [t], /d/ → [d]
and /d/ → [t, d]. This is because one of the two
candidates must be responsible for both unalternat-
ing and alternating segments, but we prefer to have
as much “‘purity” as possible, to minimize ambigu-
ity.
With this solution, we still have 1000 pure /t/ →
[t], and only the 500 /d/ → [d] are now indistinct
from /d/ → [t, d]. If we had selected /t/ as the
source of the alternation, there would be only 500
remaining “pure” (/d/) segments, and 1500 ambigu-
ous /t/. Our Bayesian model should prefer the less
ambiguous (“purer”) solution, given an appropriate
prior.
</bodyText>
<sectionHeader confidence="0.984385" genericHeader="method">
2 Model
</sectionHeader>
<bodyText confidence="0.997254636363636">
We will use boldface to indicate vectors, and sub-
scripts to identify an element from a vector or ma-
trix. The variable N(u) is a vector of observed
counts with the current underlying form hypothe-
ses. The notation we use for a vector u with one
element i removed is u−i, so we can exclude the
counts associated with a particular underlying form
by indicating that in the parenthesized variable (i.e.,
N(u−4) is all the counts except those associated with
the fourth underlying form). Ni(u) is the number of
times character i is used as an underlying form, and
Ni.7(u) is the number of times character i generated
surface character j.
The priors over surface s and underlying u seg-
ments in Figure 2 are captured by Dirichlet priors
α and ,3, which generate the multinomial distribu-
tions 0 and 0, respectively (see Figure 3). The
prior over underlying form encourages sparse solu-
tions, so βu &lt; 1 for all u. The prior over surface
form given underlying encourages identity mapping,
/x/ → [x], so αxx &gt; 1, and discourages different
segments, /x/ → [y], so αxy &lt; 1 for all x 7� y.
</bodyText>
<figureCaption confidence="0.861716">
Figure 2: Bayesian network: a and ,Q are vectors of hy-
</figureCaption>
<bodyText confidence="0.996528727272727">
perparameters, and ei (for i E {1, ... , nc}) and 0 are
distributions. u is a vector of underlying forms, generated
from 0, and si (for i E nj is a set of observed surface
forms generated from the hidden variable ui according to
ei
Phones and phonemes are drawn from a set of
characters (e.g., IPA, unicode) C used to represent
them. φi is the probability of a character (Ci for
i ∈ n,) being an underlying form, irrespective of
current alignments or its position in the paradigm.
θi.7 is the conditional probability of a surface char-
</bodyText>
<figure confidence="0.991278">
β
α
φ
θ
n,
u
s
mn.
nu
</figure>
<page confidence="0.940034">
14
</page>
<equation confidence="0.9903496">
θc  |α — DIR(α), c = 1, ... , nc
φ  |β — DIR(β)
ui  |φi — MULTI(φi), i = 1, ... , nu
sij  |ui, θui — MULTI(θui), i = 1, ... , nu,
j= 1,...,mi
</equation>
<figureCaption confidence="0.990574">
Figure 3: Model parameters: n, is # different segments,
n., is # underlying segments
</figureCaption>
<bodyText confidence="0.998859543478261">
acter (skn = Cj for j E nc, n E mk) given the
underlying character it is generated from (uk = Ci
for i E nc, k E nu), which is determined by its po-
sition in the paradigm.
In our Finnish example (Figure 1), if k = 1, we
are looking at the first underlying character, which
is /t/ (from /tiet¨a/), so assuming our character set is
the Finnish alphabet, of which ‘t’ is the 20th char-
acter, u1 = C20 = t. It generates the first character
of each inflected form (1st, 2nd, 3rd person, singu-
lar and plural) of that stem, so m1 = 6, and since
there is no alternation s1n = t (for n E 11,... , 6}).
Given the phonologically plausible (gold) underly-
ing forms, the probability of /t/ is φ20 = 7/41.
On the other hand, k = 33 identifies the 3rd per-
son singular /ø/, which inflects each of the seven
stems, so m33 = 7. Since we need our alpha-
bet to identify a null character, we’ll give it in-
dex zero (i.e., u33 = C0 = 0). For each of the
(underlying, surface) alignments in this alternation
(caused by vowel gemination), we can identify the
probability in θ. For 3rd person singular [tiet¨a+¨a],
where s33,1 = C28 = ¨a, the conditional probability
00,28 = 1/7.
The prior hyperparameters can be understood as
follows. As 0i gets smaller, an underlying form uk
is less likely to be Ci. As αij gets smaller, an un-
derlying uk = Ci is less likely to generate a surface
segment skn = Cj bn E mk. In our experiments,
we will vary αi=j (prior over identity map from un-
derlying to surface) and αi�j.
Our implementation of this model uses Gibbs
sampling (c.f., (Bishop, 2006), pp 542-8), an algo-
rithm that produces samples from the posterior dis-
tribution. Each sample is an assignment of the hid-
den variables, u (i.e., a set of hypothesized underly-
ing forms). Our sampler initializes u from a uniform
distribution over segments in the training data, and
resamples underlying forms in a fixed order, as in-
put in the paradigm. Rather than reestimate θ and
φ at each iteration before sampling from u, we can
marginalize these intermediate probability distribu-
tions in order to ease implementation and speed con-
vergence.
Our search procedure tries to sample from the
posterior probability, according to Bayes’ rule.
</bodyText>
<equation confidence="0.854884">
posterior a likelihood * prior
P(u, s|β, α) a P(u|β)P(s, u|α)
</equation>
<bodyText confidence="0.991870714285714">
Each of these probabilities is drawn from a Dirichlet
distribution, which is defined in terms of the multi-
variate Beta function, C. The prior β added to un-
derlying counts N(u) forms the posterior Dirichlet
corresponding to P(u|β). In P(s|u, α), each αi
vector is supplemented by the observed counts of
(underlying, surface) pairs N(si).
</bodyText>
<equation confidence="0.9992846">
C(β + N(u))
P(u, s|β, α) =
C(β)
C(αc + Ei:ui=c N(si))
C(α)
</equation>
<bodyText confidence="0.99898">
The collapsed update procedure consists of re-
sampling each underlying form, u, incorporating the
prior hyperparameters α,β and counts N over the
rest of the dataset. The relevant counts for a can-
didate k being the underlying form ui are Nk(u_i)
and Nksi,(u_i) for j E mi. P(ui = k|u_i, α,β) is
proportional to the probability of generating ui = k,
given the other u_i and all sij (for j E mi), given
s_i and u_i.
</bodyText>
<equation confidence="0.9997078">
Nc(u_i) + βc
P(ui = c|u_i, α,β) a
n − 1 + β•
C(α + Ei,=j4i:ui,=c N(s&apos;) + N(si))
C(α + Ei,=j4i:ui,=c N(sZ))
</equation>
<bodyText confidence="0.999962777777778">
Suppose we were updating this sampler running
on the Finnish verb inflections. If we had all seg-
ments as in Figure 1, but wanted to resample u31 (1st
person singular /n/), we would consider the counts
N excluding that form (i.e., under u_31). The prior
for /n/, 014, is fixed, and there are no other occur-
rences, so N14(u_31) = 0. Another potential un-
derlying form, like /t/, would have higher uncondi-
tioned posterior probability, because of the counts
</bodyText>
<figure confidence="0.4901646">
n.
H
c=1
15
−log Likelihood
</figure>
<bodyText confidence="0.999630428571429">
(7, in this case) added to its prior from ,C3. Then, we
have to multiply by the probability of each generated
surface segment (all are [n], so 7 * P([n]|c, α) for a
given hypothesis u31 = c).
We select a given character c E C for u31 from a
distribution at random. Depending on the prior, /n/
will be the most likely choice, but other values are
still possible with smaller probability. The counts
used for the next resampling, N(u_31), are affected
by this choice, because the new identity of u31 has
contributed to the posterior distribution. After un-
bounded iterations, Gibbs sampling is guaranteed to
converge and produce samples from the true poste-
rior (Geman and Geman, 1984).
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9998965">
This model provides a language agnostic solution to
a subset of phonological problems. We will first
examine performance on the sample Finnish data
(from Figure 1), and then look more closely at the is-
sue of convergence. Finally, we present results from
larger corpora 3.
</bodyText>
<subsectionHeader confidence="0.988607">
3.1 Finnish
</subsectionHeader>
<bodyText confidence="0.9998385">
Output from a trial run on Finnish verbs (from Fig-
ure 1) follows, with hyperparameters αij{100 ��
</bodyText>
<equation confidence="0.732091">
i = j, 0.05 i =� j} and Qi = {0.1}.
</equation>
<bodyText confidence="0.998475666666667">
In the paradigm (a sample after 1000 iterations),
each [sur+face] form is followed by its hypothesized
/under/ + /lying/ morphemes.
</bodyText>
<equation confidence="0.936406777777778">
[tied¨a+n] : /tied¨a/ + /n/
[tied¨a+t] : /tied¨a/ + /t/
[tiet¨a+¨a] : /tied¨a/ + /¨a/
[tied¨a+mme] : /tied¨a/ + /mme/
[tied¨a+tte] : /tied¨a/ + /tte/
[tiet¨a+v¨at] : /tied¨a/ + /v¨at/
[aiøo+n] : /aiøo/ + /n/
...
[pelk¨a¨a+v¨at] : /pelk¨a¨a/ + /vat/
</equation>
<bodyText confidence="0.999373833333333">
With strong enough priors (faithfulness con-
straints), our sampler often selects the most com-
mon surface form aligned with an underlying seg-
ment. Although [vat] is more common than [v¨at],
we choose the latter as the purer underlying form.
So /a/ is always [a], but /¨a/ can be either [¨a] or [a].
</bodyText>
<note confidence="0.714643">
32.8 million word types from Morphochallenge2007 (Ku-
rimo et al., 2007)
</note>
<subsectionHeader confidence="0.998242">
3.2 Convergence
</subsectionHeader>
<bodyText confidence="0.99916875">
Testing convergence, we run again on the sample
data (Figure 1), using αij = 0.1 when i =� j and
10 when i = j and ,3 = 0.1, starting from different
initializations, we get the same solution.
</bodyText>
<figure confidence="0.996927545454546">
3.05 x 108
3.04
3.03
3.02
3.01
3
2.99
2.98
2.97
0 10 20 30 40 50 60 70 80 90 100
Iteration
</figure>
<figureCaption confidence="0.7434346">
Figure 4: Posterior likelihood at each of the first 100 iter-
ations, from 4 runs (with different random seeds) on 10%
of the Morphochallenge dataset (ai0j = 0.001, ai=j =
100, 0 = 0.1), indicating convergence within the first 15
iterations.
</figureCaption>
<bodyText confidence="0.999967576923077">
To confirm that the sampler has converged, we
output and plot trace statistics at each iteration, in-
cluding marginal probability, log likelihood, and
changes in underlying forms (i.e., variables resam-
pled). If the sampler has converged, there should no
longer be a trend (consistent slope) in any of these
statistics (as in Figure 4).
Examining the posterior probability of each se-
lected underlying form reveals interesting patterns
that also help explain the variation. In the above run,
the ambiguous segments (with surface alternations)
were drawn from the distributions (with improbable
segments elided) in Figure 5.
We expect this model to maximize the probabil-
ity of either the “majority” solution or a solution
demonstrating selection bias. We compare likeli-
hood of the posterior sample with that of a “phono-
logically plausible” solution (in which underlying
forms are determined by referring to formal lin-
guistic accounts of phonological derivation) and a
“majority solution” (see Figure 6 for a log-log plot,
where lower is more likely).
The posterior sample has optimal likelihood with
each parameter setting, as expected. The majority
parse is selected with αi=,,�j = 0.5 With lower val-
ues of αi=,,�j, the “phonologically plausible” parse is
</bodyText>
<page confidence="0.938066">
16
</page>
<equation confidence="0.988005363636364">
u4=/d/ s4=[d,d,t,d,d,t]
P(ui = c) r:
d 0.99968
t 0.00014
u8=/k/ s8=[O,O,k,O,O,k]
(same behavior as u12)
P(ui = c) r:
O 0.642
k 0.124
uss=/e/ sss=[¨a,o,e,u,O,e,O]
P(ui = c) r:
</equation>
<figure confidence="0.92418225">
¨a,o,u 0.0029
O 0.215
a 0.0003
e 0.297
</figure>
<figureCaption confidence="0.894538">
Figure 5: Resampling probabilities for alternations, after
1000 iterations.
</figureCaption>
<figure confidence="0.938865">
10−2 10−1 10s
alpha
</figure>
<figureCaption confidence="0.999883">
Figure 6: Parse likelihood
</figureCaption>
<bodyText confidence="0.9999352">
more likely than the majority. However, the sam-
pler does not converge to this solution, because in
this [t,d] alternation, the “phonologically plausible”
solution identifies /t/, but neither selection bias nor
majority rules would lead to that with the given data.
</bodyText>
<subsectionHeader confidence="0.999742">
3.3 Morphologically segmented corpora
</subsectionHeader>
<bodyText confidence="0.9998865">
In our search for appropriate data for additional,
larger-scale experiments, we found several vi-
able alternatives. The correct morphological seg-
mentations for Finnish data used in Morphochal-
lenge2007 (Kurimo et al., 2007) provide a rich and
varied set of words, and are readily analyzable by
our sampler. Rather than associating each surface
form with a position in the paradigm, we use the an-
</bodyText>
<figure confidence="0.440318333333333">
Majority Bayesian
50.84 69.53
65.23 72.11
</figure>
<figureCaption confidence="0.999448">
Figure 7: Accuracy of underlying segment hypotheses.
</figureCaption>
<bodyText confidence="0.99705265">
notated morphemes.
For example, the word ajavalle is listed in the cor-
pus as follows:
ajavalle aja:ajaa|V va:PCP1 lle:ALL The
word is segmented into a verb stem, ‘aja’ (drive),
a present participle marker ‘va’, and the allative suf-
fix (“for”). Each surface realization of a given mor-
pheme is identified by the same tag (e.g., PCP1).
However, in this corpus, insertion and deletion are
not explicitly marked (as they were in the paradigm,
by ø). Rather than introduce another component
to determine which segments in the form were
dropped, we ignore these cases.
The sampling algorithm proceeds as described in
section 2. To run on tokens (as opposed to types), we
incorporate another input file that contains counts
from the original text (ajavalle appeared 8 times).
The counts of each morpheme’s surface forms then
reflect the number of times that form appeared in any
word in the corpus.
</bodyText>
<subsectionHeader confidence="0.836472">
3.3.1 Type or Token
</subsectionHeader>
<bodyText confidence="0.999579">
In Finnish verb conjugation, 3rd person (esp. sin-
gular) forms have high frequency and tend to be un-
marked (i.e., closer to underlying). In types, un-
marked is a minority (one third), but incorporat-
ing token frequency shifts that balance, benefiting
the “majority learner.” Among noun inflections, un-
marked has higher frequency in speech, but marked
tokens may still dominate in text. We might expect
that it is easier to learn from tokens than types, in
part because more data is often helpful.
Testing on half of the Morphochallenge 2007
Finnish data (1M word types, 5M morph types,
17.5M word tokens, 48M morph tokens), we ran
both our Bayesian model and a majority solver on
the morphological analyses, and compared against
phonologically plausible (gold) underlying forms.
Results are reported in Figure 7.
The Bayesian estimate consistently outperformed
the majority solution, and cases where the two differ
could often be ascribed to the preference for “pure”
</bodyText>
<figure confidence="0.988008833333333">
−log likelihood
posterior sample
majority solution
phonologically plausible
types
tokens
</figure>
<page confidence="0.958803">
17
</page>
<bodyText confidence="0.736559">
analyses.
</bodyText>
<sectionHeader confidence="0.997201" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999940888888889">
We have described a model where surface forms
are generated from underlying representations seg-
ment by segment. Taking this approach allowed us
to investigate the properties of a Bayesian statistical
learner, and how these can be useful in the context
of sound systems, a basic component of language.
Experiments with our implementation of a collapsed
sampler have produced results largely confirming
our hypotheses.
Without context, we can often learn about 60 to 80
percent of the mapping from underlying phonemes
to surface phones. Especially with lower values of
αZ�j, closer to 0, our model does prefer pure alter-
nations. Gibbs sampling tends to select the major-
ity underlying form, particularly with αZ�j relatively
high, closer to 1. So, a sparser prior leads us further
from the baseline, and often closer to a phonologi-
cally plausible solution.
</bodyText>
<subsectionHeader confidence="0.945461">
4.1 Directions
</subsectionHeader>
<bodyText confidence="0.999952921052632">
In future research, we hope to integrate morpholog-
ical analysis into this sort of a treatment of phonol-
ogy. This is a natural approach for children learn-
ing their first language. They intuitively discover
phonotactics, and how it affects the prosodic shape
of each word, as they learn meaningful units and
compose them together. It is clear that many lay-
ers of linguistic information interact in the early
stages of child language acquisition (Demuth and
Ellis, 2005 in press), so they should also interact
in our models. As discussed above, the present
model should be applicable to analysis of language-
learners’ speech errors, and this connection should
be explored in greater depth.
It might be interesting to predispose the sampler
to select underlying forms from open syllables. That
is, set α to increase the probability of matching
one of the surface segments if its context (feature
annotations) includes a vocalic segment or a word
boundary immediately following. The probability
of phonological processes like assimilation could be
similarly modeled, with the prior higher for choos-
ing a segment that appears on the surface in a con-
trastive context (where it shares few features with
neighboring segments).
If we define a MaxEnt distribution over Optimal-
ity Theoretic constraints, we might use that to in-
form our selection of underlying forms. In (Gold-
water and Johnson, 2003), the learning algorithm
was given a set of candidate surface forms asso-
ciated with an underlying form, and tried to opti-
mize the constraint weights. In addition to the con-
straint weights, we must also optimize the underly-
ing form, since our goal is to take as input only ob-
servable data. Sampling from this type of complex
distribution is quite difficult, but some approaches
(e.g., (Murray et al., 2006)) may help reduce the in-
tractability.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999641555555556">
Kenneth R. Beesley and Lauri Karttunen. 2000. Finite-
state non-concatenative morphotactics. In Lauri Kart-
tunen, Jason Eisner, and Alain Th´eriault, editors, SIG-
PHON2000, August 6 2000. Proceedings of the Fifth
Workshop of the ACL Special Interest Group in Com-
putational Phonology., pages 1–12.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer, August.
Hal Daume III. 2007. Hbc: Hierarchical bayes compiler.
Katherine Demuth and David Ellis, 2005 (in press). Re-
visiting the acquisition of Sesotho noun class prefixes.
Lawrence Erlbaum.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Trans. Pattern Anal. Machine
Intell., 6(6):721–741, Nov.
Dale Gerdemann and Gertjan van Noord. 2000. Approx-
imation and exactness in finite state optimality theory.
Daniel Gildea and Daniel Jurafsky. 1996. Learning bias
and phonological-rule induction. Computational Lin-
guistics, 22(4):497–530.
Sharon Goldwater and Mark Johnson. 2003. Learning ot
constraint rankings using a maximum entropy model.
Sharon Goldwater and Mark Johnson. 2004. Priors in
Bayesian learning of phonological rules. In Proceed-
ings of the Seventh Meeting of the ACL Special Inter-
est Group in Computational Phonology, pages 35–42,
Barcelona, Spain, July. Association for Computational
Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641–648. MIT Press, Cambridge, MA.
</reference>
<page confidence="0.98487">
18
</page>
<reference confidence="0.9970254">
Lauri Karttunen. 1998. The proper treatment of optimal-
ity in computational phonology. In Lauri Karttunen,
editor, FSMNLP’98: International Workshop on Fi-
nite State Methods in Natural Language Processing,
pages 1–12. Association for Computational Linguis-
tics, Somerset, New Jersey.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of morpho challenge in clef 2007.
In Working Notes for the CLEF 2007 Workshop, Bu-
dapest, Hungary.
Percy Liang and Dan Klein. 2007. Tutorial 1: Bayesian
nonparametric structured models, June.
Iain Murray, Zoubin Ghahramani, and David MacKay.
2006. MCMC for doubly-intractable distributions. In
UAI. AUAI Press.
</reference>
<page confidence="0.999325">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912498">
<title confidence="0.999092">A Bayesian model of natural language generating alternations from underlying forms</title>
<author confidence="0.99118">David</author>
<affiliation confidence="0.980062">Brown</affiliation>
<address confidence="0.9979">Providence, RI 02912</address>
<abstract confidence="0.99415">A stochastic approach to learning phonology. The model presented captures 7-15% more phonologically plausible underlying forms than a simple majority solution, because it prefers “pure” alternations. It could be useful in cases where an approximate solution is needed, or as a seed for more complex models. A similar process could be involved in some stages of child language acquisition; in particular, early learning of phonotactics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>Finitestate non-concatenative morphotactics.</title>
<date>2000</date>
<booktitle>Proceedings of the Fifth Workshop of the ACL Special Interest Group in Computational Phonology.,</booktitle>
<volume>6</volume>
<pages>2000</pages>
<editor>In Lauri Karttunen, Jason Eisner, and Alain Th´eriault, editors,</editor>
<contexts>
<context position="2008" citStr="Beesley and Karttunen, 2000" startWordPosition="303" endWordPosition="307">and Klein, 2007), but such models require an explicit calculation of the partition function. Unless the rule contexts possess some kind of locality, we don’t know how to compute this partition function efficiently. Some context could be 12 captured by generating underlying phonemes from an n-gram model, or by annotating surface forms with neighborhood features. However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized. 1.1 Related Work In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm. A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000). Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work. Also, the Hierarchical Bayes Compiler (Daume III, 2007) could be used to genera</context>
</contexts>
<marker>Beesley, Karttunen, 2000</marker>
<rawString>Kenneth R. Beesley and Lauri Karttunen. 2000. Finitestate non-concatenative morphotactics. In Lauri Karttunen, Jason Eisner, and Alain Th´eriault, editors, SIGPHON2000, August 6 2000. Proceedings of the Fifth Workshop of the ACL Special Interest Group in Computational Phonology., pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning (Information Science and Statistics).</booktitle>
<publisher>Springer,</publisher>
<contexts>
<context position="12352" citStr="Bishop, 2006" startWordPosition="2068" endWordPosition="2069">urface) alignments in this alternation (caused by vowel gemination), we can identify the probability in θ. For 3rd person singular [tiet¨a+¨a], where s33,1 = C28 = ¨a, the conditional probability 00,28 = 1/7. The prior hyperparameters can be understood as follows. As 0i gets smaller, an underlying form uk is less likely to be Ci. As αij gets smaller, an underlying uk = Ci is less likely to generate a surface segment skn = Cj bn E mk. In our experiments, we will vary αi=j (prior over identity map from underlying to surface) and αi�j. Our implementation of this model uses Gibbs sampling (c.f., (Bishop, 2006), pp 542-8), an algorithm that produces samples from the posterior distribution. Each sample is an assignment of the hidden variables, u (i.e., a set of hypothesized underlying forms). Our sampler initializes u from a uniform distribution over segments in the training data, and resamples underlying forms in a fixed order, as input in the paradigm. Rather than reestimate θ and φ at each iteration before sampling from u, we can marginalize these intermediate probability distributions in order to ease implementation and speed convergence. Our search procedure tries to sample from the posterior pr</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Hbc: Hierarchical bayes compiler. Katherine Demuth and</title>
<date>2007</date>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Hbc: Hierarchical bayes compiler. Katherine Demuth and David Ellis, 2005 (in press). Revisiting the acquisition of Sesotho noun class prefixes. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Trans. Pattern Anal. Machine Intell.,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="15129" citStr="Geman and Geman, 1984" startWordPosition="2553" endWordPosition="2556">e to multiply by the probability of each generated surface segment (all are [n], so 7 * P([n]|c, α) for a given hypothesis u31 = c). We select a given character c E C for u31 from a distribution at random. Depending on the prior, /n/ will be the most likely choice, but other values are still possible with smaller probability. The counts used for the next resampling, N(u_31), are affected by this choice, because the new identity of u31 has contributed to the posterior distribution. After unbounded iterations, Gibbs sampling is guaranteed to converge and produce samples from the true posterior (Geman and Geman, 1984). 3 Evaluation This model provides a language agnostic solution to a subset of phonological problems. We will first examine performance on the sample Finnish data (from Figure 1), and then look more closely at the issue of convergence. Finally, we present results from larger corpora 3. 3.1 Finnish Output from a trial run on Finnish verbs (from Figure 1) follows, with hyperparameters αij{100 �� i = j, 0.05 i =� j} and Qi = {0.1}. In the paradigm (a sample after 1000 iterations), each [sur+face] form is followed by its hypothesized /under/ + /lying/ morphemes. [tied¨a+n] : /tied¨a/ + /n/ [tied¨a</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Trans. Pattern Anal. Machine Intell., 6(6):721–741, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Gerdemann</author>
<author>Gertjan van Noord</author>
</authors>
<title>Approximation and exactness in finite state optimality theory.</title>
<date>2000</date>
<marker>Gerdemann, van Noord, 2000</marker>
<rawString>Dale Gerdemann and Gertjan van Noord. 2000. Approximation and exactness in finite state optimality theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning bias and phonological-rule induction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="1978" citStr="Gildea and Jurafsky, 1996" startWordPosition="299" endWordPosition="302">rs in a Gibbs model (Liang and Klein, 2007), but such models require an explicit calculation of the partition function. Unless the rule contexts possess some kind of locality, we don’t know how to compute this partition function efficiently. Some context could be 12 captured by generating underlying phonemes from an n-gram model, or by annotating surface forms with neighborhood features. However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized. 1.1 Related Work In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm. A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000). Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work. Also, the Hierarchical Bayes Compiler (Daume III,</context>
</contexts>
<marker>Gildea, Jurafsky, 1996</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 1996. Learning bias and phonological-rule induction. Computational Linguistics, 22(4):497–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Learning ot constraint rankings using a maximum entropy model.</title>
<date>2003</date>
<marker>Goldwater, Johnson, 2003</marker>
<rawString>Sharon Goldwater and Mark Johnson. 2003. Learning ot constraint rankings using a maximum entropy model.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Priors in Bayesian learning of phonological rules.</title>
<date>2004</date>
<booktitle>In Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology,</booktitle>
<pages>35--42</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2441" citStr="Goldwater and Johnson, 2004" startWordPosition="366" endWordPosition="370">dependencies (like vowel harmony) cannot be easily Bayesianized. 1.1 Related Work In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm. A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000). Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work. Also, the Hierarchical Bayes Compiler (Daume III, 2007) could be used to generate a model similar to the one presented here, but less constrained&apos; which makes correspondingly more random, less accurate predictions. 1.2 Dataset As we describe the model and its implementation in this and subsequent sections, we will refer to a sam&apos;Recent updates to HBC, inspired by discussions with the author, have addressed some of these limitations. Proceedings of the Tenth Meeting of the ACL Special Interest Group on Compu</context>
</contexts>
<marker>Goldwater, Johnson, 2004</marker>
<rawString>Sharon Goldwater and Mark Johnson. 2004. Priors in Bayesian learning of phonological rules. In Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology, pages 35–42, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2360" citStr="Johnson et al., 2007" startWordPosition="355" endWordPosition="358">res. However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized. 1.1 Related Work In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm. A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000). Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work. Also, the Hierarchical Bayes Compiler (Daume III, 2007) could be used to generate a model similar to the one presented here, but less constrained&apos; which makes correspondingly more random, less accurate predictions. 1.2 Dataset As we describe the model and its implementation in this and subsequent sections, we will refer to a sam&apos;Recent updates to HBC, inspired by discussions with the author, have addressed some of these limitat</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The proper treatment of optimality in computational phonology.</title>
<date>1998</date>
<booktitle>FSMNLP’98: International Workshop on Finite State Methods in Natural Language Processing,</booktitle>
<pages>1--12</pages>
<editor>In Lauri Karttunen, editor,</editor>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="2176" citStr="Karttunen, 1998" startWordPosition="328" endWordPosition="329">is partition function efficiently. Some context could be 12 captured by generating underlying phonemes from an n-gram model, or by annotating surface forms with neighborhood features. However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized. 1.1 Related Work In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm. A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000). Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work. Also, the Hierarchical Bayes Compiler (Daume III, 2007) could be used to generate a model similar to the one presented here, but less constrained&apos; which makes correspondingly more random, less accurate predictions. 1.2 Dataset As we describe the m</context>
</contexts>
<marker>Karttunen, 1998</marker>
<rawString>Lauri Karttunen. 1998. The proper treatment of optimality in computational phonology. In Lauri Karttunen, editor, FSMNLP’98: International Workshop on Finite State Methods in Natural Language Processing, pages 1–12. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Ville Turunen</author>
</authors>
<title>Overview of morpho challenge in clef</title>
<date>2007</date>
<booktitle>In Working Notes for the CLEF 2007 Workshop,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="16310" citStr="Kurimo et al., 2007" startWordPosition="2753" endWordPosition="2757">es. [tied¨a+n] : /tied¨a/ + /n/ [tied¨a+t] : /tied¨a/ + /t/ [tiet¨a+¨a] : /tied¨a/ + /¨a/ [tied¨a+mme] : /tied¨a/ + /mme/ [tied¨a+tte] : /tied¨a/ + /tte/ [tiet¨a+v¨at] : /tied¨a/ + /v¨at/ [aiøo+n] : /aiøo/ + /n/ ... [pelk¨a¨a+v¨at] : /pelk¨a¨a/ + /vat/ With strong enough priors (faithfulness constraints), our sampler often selects the most common surface form aligned with an underlying segment. Although [vat] is more common than [v¨at], we choose the latter as the purer underlying form. So /a/ is always [a], but /¨a/ can be either [¨a] or [a]. 32.8 million word types from Morphochallenge2007 (Kurimo et al., 2007) 3.2 Convergence Testing convergence, we run again on the sample data (Figure 1), using αij = 0.1 when i =� j and 10 when i = j and ,3 = 0.1, starting from different initializations, we get the same solution. 3.05 x 108 3.04 3.03 3.02 3.01 3 2.99 2.98 2.97 0 10 20 30 40 50 60 70 80 90 100 Iteration Figure 4: Posterior likelihood at each of the first 100 iterations, from 4 runs (with different random seeds) on 10% of the Morphochallenge dataset (ai0j = 0.001, ai=j = 100, 0 = 0.1), indicating convergence within the first 15 iterations. To confirm that the sampler has converged, we output and plo</context>
<context position="18967" citStr="Kurimo et al., 2007" startWordPosition="3186" endWordPosition="3189">ilities for alternations, after 1000 iterations. 10−2 10−1 10s alpha Figure 6: Parse likelihood more likely than the majority. However, the sampler does not converge to this solution, because in this [t,d] alternation, the “phonologically plausible” solution identifies /t/, but neither selection bias nor majority rules would lead to that with the given data. 3.3 Morphologically segmented corpora In our search for appropriate data for additional, larger-scale experiments, we found several viable alternatives. The correct morphological segmentations for Finnish data used in Morphochallenge2007 (Kurimo et al., 2007) provide a rich and varied set of words, and are readily analyzable by our sampler. Rather than associating each surface form with a position in the paradigm, we use the anMajority Bayesian 50.84 69.53 65.23 72.11 Figure 7: Accuracy of underlying segment hypotheses. notated morphemes. For example, the word ajavalle is listed in the corpus as follows: ajavalle aja:ajaa|V va:PCP1 lle:ALL The word is segmented into a verb stem, ‘aja’ (drive), a present participle marker ‘va’, and the allative suffix (“for”). Each surface realization of a given morpheme is identified by the same tag (e.g., PCP1). </context>
</contexts>
<marker>Kurimo, Creutz, Turunen, 2007</marker>
<rawString>Mikko Kurimo, Mathias Creutz, and Ville Turunen. 2007. Overview of morpho challenge in clef 2007. In Working Notes for the CLEF 2007 Workshop, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>Tutorial 1: Bayesian nonparametric structured models,</title>
<date>2007</date>
<contexts>
<context position="1396" citStr="Liang and Klein, 2007" startWordPosition="211" endWordPosition="214">nt hierarchy as in Optimality Theory (OT), or by a set of ordered rules. Both rely on a single lexical representation of each morpheme (i.e., its underlying form), and context-sensitive transformations to surface forms. Phonological changes often affect segments near morpheme boundaries, but can also apply over an entire prosodic word, as in vowel harmony. It does not seem straightforward to incorporate context into a Bayesian model of phonology, although a clever solution may yet be found. A standard way of incorporating conditioning environments is to treat them as factors in a Gibbs model (Liang and Klein, 2007), but such models require an explicit calculation of the partition function. Unless the rule contexts possess some kind of locality, we don’t know how to compute this partition function efficiently. Some context could be 12 captured by generating underlying phonemes from an n-gram model, or by annotating surface forms with neighborhood features. However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized. 1.1 Related Work In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Kart</context>
</contexts>
<marker>Liang, Klein, 2007</marker>
<rawString>Percy Liang and Dan Klein. 2007. Tutorial 1: Bayesian nonparametric structured models, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iain Murray</author>
<author>Zoubin Ghahramani</author>
<author>David MacKay</author>
</authors>
<title>MCMC for doubly-intractable distributions. In UAI.</title>
<date>2006</date>
<publisher>AUAI Press.</publisher>
<marker>Murray, Ghahramani, MacKay, 2006</marker>
<rawString>Iain Murray, Zoubin Ghahramani, and David MacKay. 2006. MCMC for doubly-intractable distributions. In UAI. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>