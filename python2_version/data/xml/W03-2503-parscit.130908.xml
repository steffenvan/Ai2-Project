<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000545">
<title confidence="0.739283">
Language-Models for Questions
</title>
<author confidence="0.909463">
Ed Schofield
</author>
<affiliation confidence="0.877212">
Telecommunications Research Center ftw.
</affiliation>
<address confidence="0.663327">
Vienna, Austria
</address>
<email confidence="0.980537">
schofield@ftw.at
</email>
<sectionHeader confidence="0.994305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999707818181818">
Natural-language question-answering is
a promising interface for retrieving in-
formation in mobile contexts because
it by-passes the problem of presenting
documents and interim search results on
a small screen. This paper considers lan-
guage-models suitable for rapid predic-
tive text-input and spoken input of nat-
ural-language questions. It describes a
varied corpus of fact-seeking questions
posed by users online and analyzes its
structure. We find it to be highly con-
strained lexically despite its wide spec-
trum of topics, with a per-word per-
plexity less than 47 with around 2.6%
of words in the test set out-of-vocabu-
lary. One implication is that predictive
interfaces can greatly speed up the in-
put of natural-language questions with a
keypad or stylus. Another is that auto-
matic speech-recognition of such ques-
tions can be quite accurate.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999850413793103">
Mobile devices have relatively small screens, and
are cumbersome to use for retrieving informa-
tion with traditional interfaces that return lists
of matching documents in response to keywords.
Schofield and Kubin (2002) argue that the case for
question-answering as an alternative interface for
finding information on a small device is strong be-
cause answers to questions are more likely to fit
comfortably on a small screen than arbitrary docu-
ments. Question-answering systems shift a user&apos;s
burden from filtering documents for relevance to
describing his or her need for information more
precisely at the outset with a question rather than a
string of keywords. Question-answering thus de-
mands less of the output-facilities of mobile de-
vices but more of their input-facilities. This paper
investigates how language-models customized to
questions can speed up their input.
Section 2 describes the origin and nature of the
corpus of natural-language questions analyzed in
Section 3, which compares various rt-gram lan-
guage-models and shows the per-word perplexity
of these questions to be lower than that of the utter-
ances modeled in several common speech-recog-
nition tasks. The paper then discusses the impli-
cations of this for input using keypads and sty-
luses (Section 4) and speech (Section 5). Section 6
discusses opportunities for better models of ques-
tions.
</bodyText>
<sectionHeader confidence="0.93299" genericHeader="introduction">
2 The corpus of questions
</sectionHeader>
<bodyText confidence="0.999905882352941">
We collected around 450,000 questions from var-
ious online sources—logs of the Ask Jeeves
and Excite search engines, FAQFinder (Ham-
mond et al., 1995), AnswerBus (Zheng, 2002),
and test questions from the TREC question-an-
swering track (Voorhees, 2001) from 1998 to
2001—and wrote scripts to correct common ty-
pos and spelling mistakes and to filter the cor-
pus in the various ways in Table 1. After this
massaging the corpus had 279,456 unique ques-
tions. Table 2 shows a random sample of ques-
tions from the processed corpus. It still includes
spelling mistakes (&apos;alcahol&apos;), irregular punctua-
tion (&apos;advanced-screening&apos;, &apos;science related&apos;), and
nonsense (`How can I figure?&apos;).
Notice that some of the questions in the cor-
pus provide no more information than a string
</bodyText>
<page confidence="0.999813">
17
</page>
<tableCaption confidence="0.998704">
Table 1: Elements removed from the corpus.
</tableCaption>
<bodyText confidence="0.9308481875">
strings of fewer than 3 words
duplicate questions
requests for web sites
requests for pornography
requests for downloads
various punctuation characters
of keywords (e.g. &apos;Where can I find information
on species in salt marshes?&apos;). This is especially
common of questions beginning &apos;Where can I find
&apos;, which constitute about 10% of the corpus.
Sometimes people want general information about
a new topic or pointers to general references. The
capabilities of current search engines also likely
encourage users to ask for links to categories of in-
formation rather than directly for the information
itself.
</bodyText>
<sectionHeader confidence="0.78161" genericHeader="method">
3 n-grams of questions
</sectionHeader>
<bodyText confidence="0.999734376811594">
This section describes and compares the fit of var-
ious n-gram models to the training corpus. We
trained models for n = 2, 3, 4 with various sizes
of lexicon and &apos;cut-off&apos; thresholds for ignoring in-
frequent sequences. For each model we randomly
constructed five partitions of the corpus, each 90%
for training and 10% for testing. Table 3 reports
the geometric mean perplexity on the test sets per
word. We trained models with both the Good-
Turing and Witten-Bell discounting schemes. Ta-
ble 3 reports perplexities for the latter, denoted
type C in (Witten and Bell, 1991), which were 1-
2% lower than for Good-Turing discounting.
&apos;Perplexity&apos; (Jurafsky and Martin, 2000) is
commonly used in speech recognition research
as a measure of the goodness-of-fit of language-
models; a language-model with a lower perplex-
ity will usually—but not always (Clarkson and
Robinson, 1999)—induce fewer mis-recognitions
for the same task. The perplexity statistic also in-
dicates the relative difficulty of prediction across
different domains. Table 5 summarizes the per-
plexities of language-models for various bench-
mark tasks in speech-recognition research. Here
we adopt the standard practice of excluding from
calculation any words encountered in the test set
that are not in the lexicon. Note that models
with small lexica have artificially low perplexity
scores: models with larger lexica are penalized
for their unreliable predictions about infrequent
words, whereas models with smaller lexica have
no power whatever to predict infrequent words,
and incur no penalty. Thus the perplexity com-
puted this way is incomparable across models with
lexica of different sizes, and is imperfect as a mea-
sure of a language-model&apos;s predictive power.
The vocabulary of open-domain questions is po-
tentially as large as a language itself. We can ex-
pect that, as per Zipf&apos;s second law (Zipf, 1965),
no corpus of practical size will include all words;
interfaces for textual or spoken input must be de-
signed to accommodate omissions in the lexicon
gracefully. Figure 1 shows the effect of the size of
the training set on the rate of occurrence of new
words in unseen questions. Extrapolating, we can
predict that unseen questions in this domain are
unlikely to have an out-of-vocabulary rate under
about 1.5% for models trained with any practi-
cally-sized corpus.
Speech recognizers have little opportunity for
phonetically transcribing a word not in their lex-
icon, especially for languages like English with
many homophones. The usual consequence is a
mis-recognition of the offending word and often of
its neighbors. Predictive typing aids can be more
forgiving. Users of a stylus or keypad can input
unusual words normally, ignoring any bogus sug-
gestions. This suggests an alternative measure to
perplexity for the effectiveness of language-mod-
els for predictive typing, like the expected num-
ber of keystrokes per character. Such a measure
(MacKenzie, 2002) would depend on implemen-
tation-specific characteristics of the interface. We
describe one such implementation in progress in
the next section; meanwhile we conjecture that,
other factors being equal, lower-perplexity models
generally imply better prediction. Hingeing upon
this, the relatively low perplexities in Tables 3 and
4 bode well for the impatient questioner.
</bodyText>
<sectionHeader confidence="0.969805" genericHeader="method">
4 Implications for entering questions
</sectionHeader>
<subsectionHeader confidence="0.494986">
with a keypad or stylus
</subsectionHeader>
<bodyText confidence="0.981829">
Most Palm devices display about 11 lines; most
Pocket PCs about 15. Locating the intended
</bodyText>
<page confidence="0.998717">
18
</page>
<tableCaption confidence="0.944394">
Table 2: A random sample of questions from the processed corpus.
</tableCaption>
<figure confidence="0.971511043478261">
WHERE CAN I LEARN HOW TO BREW ALCAHOL ?
WHERE CAN I FIND A LIST SECTION 8 PROVIDERS?
NAME A FEMALE FIGURE SKATER.
WHERE CAN I FIND IMAGES OF THE COMIC DARK CHYLD ?
WHAT ARE SOME HOTELS IN COLORADO?
WHAT IS THE MEDICAL DISORDER ARRYTHEMIA ?
WHERE CAN I FIND INFORMATION ON SPECIES IN SALT MARSHES?
WHEN WAS THE WEB PAGE FOR THE UNIVERSITY OF MICHIGAN OFFICE OF NEW STUDENT PRO-
GRAMS LAST UPDATED?
HOW CAN I SEE ADVANCED-SCREENING OF MOVIES?
WHERE CAN I FIND A ALTERNATIVE FOR MESCALINE?
ARE BANKERS REQUIRED TO GET CONTINUING EDUCATION?
WHERE CAN I BUY SCIENCE RELATED TOYS?
HOW CAN I GET A PC TO MAC ETHERNET CONNECTION?
WHERE CAN I FIND INFORMATION ON THE HUMAN RESPIRATORY SYSTEM?
WHERE CAN I FIND AN AFRICAN RECIPE?
I AM LOOKING FOR INFORMATION ON THE MASSACHUSETTS EDUCATION REFORM.
WHERE CAN I FIND A PICTURE OF KURT CUBAN WITH A GUN IN HIS MOUTH?
HOW DO I REMOVE THINGS FROM MY FAVORITES LIST?
WHERE CAN I GET THE PASS REPORT FOR SNOWQUALMIE PASS?
WHAT IS A 110 PUNCHDOWN BLOCK?
WHAT IS THE DEFINITION OF ASTRONOMY?
HOW CAN I FIGURE?
</figure>
<figureCaption confidence="0.999193">
Figure 1: The effect of the size of the training-set on the proportion of out-of-vocabulary (00V) words.
</figureCaption>
<figure confidence="0.979855857142857">
•
•
0.16
0.14
0.12
0.1
0
LI0.08
.0
0.06
0.04
0.02
00
0.5
</figure>
<footnote confidence="0.359718">
1 1.5 2 2.5 3
Questions in training corpus x 10 5
</footnote>
<page confidence="0.988693">
19
</page>
<tableCaption confidence="0.85864275">
Table 3: The average cross-perplexities of n-gram models on a 10% test-set, with the models&apos; sizes on
disk and proportions of out-of-vocabulary (00V) words. The size of the language-models is a function of
cut-off thresholds (omitted) for the minimal number of occurrences of each n-gram necessary to estimate
its frequency.
</tableCaption>
<table confidence="0.999939764705883">
Lexicon size rt Model size (MB) 00V rate Perplexity
8k 2 1.0 8.1% 57.9
8k 2 1.3 8.1% 47.8
8k 2 1.9 8.1% 43.7
16k 2 1.7 5.3% 58.9
16k 3 2.0 5.3% 65.6
16k 2 2.5 5.3% 52.1
16k 3 3.4 5.3% 48.1
16k 3 7.5 5.3% 39.4
32k 2 2.5 3.6% 67.8
32k 2 3.4 3.6% 59.3
32k 3 4.3 3.6% 56.1
32k 3 8.3 3.6% 45.0
65k 2 5.1 2.6% 64.2
65k 3 10 2.6% 48.8
65k 4 18 2.6% 46.6
65k 5 28 2.6% 46.6
</table>
<tableCaption confidence="0.9847555">
Table 5: Benchmark tasks in speech-recognition research and their approximate 3-gram language-model
perplexities, with the present domain for comparison.
</tableCaption>
<figure confidence="0.660078928571428">
Task Perplexity
TI Digits 10
Air Travel Information System 15
Natural-language questions 45
Naval Resource Management 60
Wall Street Journal 170
Switchboard 200
20
0.18 •
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0o 7
•
•
•
Proportion of corpus •
•
•
f I1■ • • • • • S.
18 21 24
6 9 12 15
Question length in words
</figure>
<figureCaption confidence="0.747224">
Figure 2: Breakdown of questions in the corpus by length.
Table 4: 3-gram models clustered by initial word.
PPI is the test-set perplexity for models trained on
all questions. PP2 is for models trained on those
questions with the initial word.
</figureCaption>
<table confidence="0.9993376">
% of corpus Initial word PPI PP2 00V
53% Where 25.3 22.7 2.2%
26% What 73.3 53.7 3.5%
17% How 60.5 40.5 1.9%
5.6% Who 111 50.1 5.3%
1.5% Why 252 2.6%
1.5% When 131 2.3%
1.5% Is 207 3.4%
1.1% Can 119 2.3%
10.3% Other 362 4.1%
</table>
<bodyText confidence="0.99738772">
word in a list on such a screen would, with
language-models of perplexity 45-50, often re-
quire scrolling. So any list of likely continua-
tions should update more frequently than once per
word.
We envisage two scenarios. In the first, predic-
tive text software would reside on the mobile de-
vice. The capacity of the device would constrain
the comprehensiveness of the language-models;
current PDAs and handheld computers might limit
them to 0.5-5 MB. For comparison, the version of
T9 that Tegic Communications released in 1998
for Palm devices was 170 kB. The sizes shown in
Table 3 assume four-byte floating point storage of
log probabilities. Using two-byte single precision
floats and a simple text compression scheme for
words would roughly halve these space require-
ments with a negligible increase in perplexity.
The second scenario is that the device use an
always-on data connection to send each chosen
character to a remote server. The server would
recompute its predictions with few resource con-
straints and return an updated list. Our tests in
Austria&apos;s GPRS networks indicate that round-trip
times for TCP packets are commonly around 1000
</bodyText>
<page confidence="0.998063">
21
</page>
<bodyText confidence="0.9982632">
milliseconds. This may, depending on the inter-
face, be too long to wait. We expect UMTS net-
works to have less latency.
We have not analyzed the average number
of keystrokes or stylus gestures that an inter-
face coupled with our language-models would de-
mand. See (MacKenzie, 2002) for a compari-
son of the keystroke requirements of existing text-
entry methods. Such a measure is more relevant
than perplexity to the speed of text-entry but in-
terface-dependent. We are developing text-predic-
tion software for the Palm and Pocket PC plat-
forms that uses the language-models we have fit
to our question corpus. After each new charac-
ter it presents a fresh list of choices, from which
the user can select a whole or partial continuation.
Figure 3 depicts a web-based prototype. Once
we finalize the interface we intend to measure the
throughput and mean time required to input each
question.
</bodyText>
<figureCaption confidence="0.8341735">
Figure 3: A web-based prototype of text-predic-
tion for questions based on 3-gram language-mod-
els. Available at http : //speech. ftw. . at/
-ejs/pocketanswer.
</figureCaption>
<subsectionHeader confidence="0.840035">
Pocket Answer
</subsectionHeader>
<bodyText confidence="0.789212">
what is
</bodyText>
<listItem confidence="0.998672888888889">
• what is a good recipe for the country of virgin
islands ...
• what is an Internet tutorial foi beginneis on the
intei net ...
• what is another name for the country of virgin
islands british
• what is chi istmas celebrated in australia
• what is going to be a millionaire board game
called intelligent ...
• what is happening in the world series in cincinnati
oh and cleveland
• what is my computer is y2k compliant ...
• what is playing tonight or coming soon to san
francisco ca and ...
• what is the distanc• between san francisco ca
and san francisco
• what is there a site that has the most popular
christnnas gifts ...
</listItem>
<bodyText confidence="0.959188416666667">
IAnswer 1
For now consider the following naive estimate.
Assume we employ the 2-gram language-model
in Table 3 of 3.4 MB with a perplexity of 59.3
and 00V rate of 3.6%. If the next word to be
typed is in the lexicon, the first letter typed reduces
the average branching factor from 59.3 by a fac-
tor of about 26% to 2-3, and the screen displays
the intended word, which is chosen with another
tap. The remaining 3.6% of words must be en-
tered in full. So, with a good interface, a rate of
2-3 keystrokes per word might be possible.
</bodyText>
<sectionHeader confidence="0.780068" genericHeader="method">
5 Implications for entering questions
with speech
</sectionHeader>
<bodyText confidence="0.988240194444444">
The first prerequisite for speech input to a mo-
bile device is a programmable microphone. Most
Pocket PC devices include these; most Palm de-
vices do not. Networked devices can recognize
speech alone or di stributedly. Alone, a device
must store a language-model, phonetic dictionary,
acoustic model, and decoder; the decoding al-
gorithm it runs to find the likeliest hypothesis
must be efficient enough given the device&apos;s mem-
ory constraints and speed. In distributed speech-
recognition the device extracts features from the
waveform of the utterance, transmits these to
a powerful server using an established protocol
like the Aurora DSR protocol of the European
Telecommunications Standards Institute (Pearce,
2000), and after processing receives a list or lattice
of likely hypotheses. There is no reason in princi-
ple why a distributed architecture cannot employ
speaker-dependent models trained for individual
users, although this may in practice be expensive.
In either scenario the interface the device
presents should allow quick correction of mis-rec-
ognized words by keypad or stylus. Schofield and
Kubin (2002) describe mobile interfaces for pos-
ing questions in more depth.
We conjecture that, with a wide-band signal and
mild background noise, a speech recognizer cus-
tomized for questions may mis-transcribe around
5% of words with speaker-dependent acoustic
models, or 10-15% otherwise. The sources (Cole
et al., 1996; Zheng and Picone, 2001; Chelba,
2000) report similar word-error rates for tasks of
this perplexity or greater.
We hope in the future to build such a recognizer
An over-estimate. More words beginning e than q allow
less disambiguation.
</bodyText>
<page confidence="0.989113">
22
</page>
<bodyText confidence="0.993808">
and test various interfaces for efficiently correct-
ing mis-transcriptions.
</bodyText>
<sectionHeader confidence="0.985665" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999861263157895">
the Wall Street Journal. In the light of these re-
sults it has discussed the requirements and poten-
tial for predictive text-input and speech-recogni-
tion of questions with mobile devices.
Language-modeling for small devices differs in
one essential respect from traditional language-
modeling: that the space available to store models
may be constrained. Various models with fewer
parameters than n-grams have been proposed,
among which class-based n-grams and maximum-
entropy models appear promising for this domain,
being intuitively sensible and suited to small cor-
pora. We plan to investigate the applicability of
these models to natural-language questions in due
course.
Accurate language-models are necessary but
not sufficient for predictive text-input. A suit-
able interface must overcome at least three hur-
dles: that screens on mobile devices are small; that
choosing text from a list requires time, visual at-
tention, and concentration; and that a manual fa-
cility for entering uncommon words is necessary.
To create an interface supporting easy, rapid en-
try of text requires careful thought and thorough
testing.
For input with speech the decoder must be ap-
propriate to the language-models. The time re-
quired for Viterbi decoding is proportional to the
square of the number of states in the compound
hidden Markov model, while the number of states
is, for 3-gram language-models, itself proportional
to the square of the vocabulary size. A tree
search, or stack-decoding, framework offers more
promise for language-models of arbitrary com-
plexity. We have investigated approximate tree-
search algorithms suitable for speech-recognition
with complex language-models; a paper is forth-
coming
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999903428571429">
This paper has investigated language-models suit-
able for textual or spoken input of natural-lan-
guage questions. It has examined a corpus of
about 280k unique questions asked by users of the
Internet and shown their short-range lexical struc-
ture to be more constrained than several corpora
like DARPA&apos;s Navel Resource Management and
</bodyText>
<sectionHeader confidence="0.995518" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99817">
I would like to thank Professors Gernot Kubin
of the Graz University of Technology and Stefan
Rtiger of Imperial College London for their ad-
vice. A Marie Curie fellowship from the European
Commission supported this research.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995672">
Ciprian Chelba. 2000. Exploiting Syntactic Struc-
ture for Natural Language Modeling. Ph.D. thesis,
CLSP, The Johns Hopkins University.
Philip Clarkson and Tony Robinson. 1999. Towards
improved language model evaluation measures. In
Proceedings of EUROSPEECH 99, 6th European
Conference on Speech Communication and Technol-
ogy, volume 5, pages 1927-1930, September. Bu-
dapest, Hungary.
R. Cole, J. Mariani, H. Uszkoreit, A. Zaenen, and
V. Zue. 1996. Survey of the state of the art in human
language technology. Technical report, Center for
Spoken Language Understanding CSLU, Carnegie
Mellon University, Pittsburgh, PA, USA.
K. Hammond, R. Burke, C. Martin, and S. Lytinen.
1995. FAQ finder: A case-based approach to knowl-
edge navigation. In Proceedings of the Eleventh
Conference on Artificial Intelligence for Applica-
tions, pages 80-86, Los Alamitos, February. IEEE
Computer Society Press.
D. Jurafsky and J. H. Martin. 2000. Speech and Lan-
guage Processing: An Introduction to Natural Lan-
guage Processing, Computational Linguistics, and
Speech Recognition. Prentice-Hall.
I. Scott MacKenzie. 2002. KSPC (keystrokes per char-
acter) as a characteristic of text entry techniques. In
Fabio PaternO, editor, Proceedings of the Fourth In-
ternational Symposium on Human Computer Inter-
action with Mobile Devices, number 2411 in Lec-
ture Notes in Computer Science, pages 195-210.
Springer-Verlag, September.
D. Pearce. 2000. An overview of ETSI standards ac-
tivities for distributed speech recognition front-ends.
In Proceedings ofAVIOS 2000: The Speech Applica-
tions Conference, May.
</reference>
<page confidence="0.970714">
23
</page>
<reference confidence="0.9980774">
Ed Schofield and Gernot Kubin. 2002. On interfaces
for mobile information retrieval. In Fabio Patern6,
editor, Proceedings of the Fourth International Sym-
posium on Human Computer Interaction with Mo-
bile Devices, number 2411 in Lecture Notes in
Computer Science, pages 383-387. Springer-Verlag,
September.
Ellen M. Voorhees. 2001. Overview of the TREC
2001 question answering track. In Proceedings of
the Tenth Text REtrieval Conference (TREC-10). De-
partment of Commerce, National Institute of Stan-
dards and Technology.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4), July.
F. Zheng and J. Picone. 2001. Robust low perplexity
voice interfaces. Technical report, MITRE Corpora-
tion, August.
Zhiping Zheng. 2002. AnswerBus question answering
system. In Human Language Technology Confer-
ence (HLT 2002), San Diego, CA., March.
George Kingsley Zipf. 1965. The Psycho-biology of
Language: An Introduction to Dynamic Philology.
MIT Press.
</reference>
<page confidence="0.999146">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885256">
<title confidence="0.999577">Language-Models for Questions</title>
<author confidence="0.966131">Ed</author>
<affiliation confidence="0.999091">Telecommunications Research Center</affiliation>
<address confidence="0.933932">Vienna,</address>
<email confidence="0.992948">schofield@ftw.at</email>
<abstract confidence="0.999462913043479">Natural-language question-answering is a promising interface for retrieving information in mobile contexts because it by-passes the problem of presenting documents and interim search results on a small screen. This paper considers language-models suitable for rapid predictive text-input and spoken input of natural-language questions. It describes a varied corpus of fact-seeking questions posed by users online and analyzes its structure. We find it to be highly constrained lexically despite its wide spectrum of topics, with a per-word perplexity less than 47 with around 2.6% of words in the test set out-of-vocabulary. One implication is that predictive interfaces can greatly speed up the input of natural-language questions with a keypad or stylus. Another is that automatic speech-recognition of such questions can be quite accurate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>CLSP, The Johns Hopkins University.</institution>
<contexts>
<context position="15087" citStr="Chelba, 2000" startWordPosition="2508" endWordPosition="2509">nnot employ speaker-dependent models trained for individual users, although this may in practice be expensive. In either scenario the interface the device presents should allow quick correction of mis-recognized words by keypad or stylus. Schofield and Kubin (2002) describe mobile interfaces for posing questions in more depth. We conjecture that, with a wide-band signal and mild background noise, a speech recognizer customized for questions may mis-transcribe around 5% of words with speaker-dependent acoustic models, or 10-15% otherwise. The sources (Cole et al., 1996; Zheng and Picone, 2001; Chelba, 2000) report similar word-error rates for tasks of this perplexity or greater. We hope in the future to build such a recognizer An over-estimate. More words beginning e than q allow less disambiguation. 22 and test various interfaces for efficiently correcting mis-transcriptions. 6 Future Work the Wall Street Journal. In the light of these results it has discussed the requirements and potential for predictive text-input and speech-recognition of questions with mobile devices. Language-modeling for small devices differs in one essential respect from traditional languagemodeling: that the space avail</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, CLSP, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Tony Robinson</author>
</authors>
<title>Towards improved language model evaluation measures.</title>
<date>1999</date>
<booktitle>In Proceedings of EUROSPEECH 99, 6th European Conference on Speech Communication and Technology,</booktitle>
<volume>5</volume>
<pages>1927--1930</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="4676" citStr="Clarkson and Robinson, 1999" startWordPosition="732" endWordPosition="735">y constructed five partitions of the corpus, each 90% for training and 10% for testing. Table 3 reports the geometric mean perplexity on the test sets per word. We trained models with both the GoodTuring and Witten-Bell discounting schemes. Table 3 reports perplexities for the latter, denoted type C in (Witten and Bell, 1991), which were 1- 2% lower than for Good-Turing discounting. &apos;Perplexity&apos; (Jurafsky and Martin, 2000) is commonly used in speech recognition research as a measure of the goodness-of-fit of languagemodels; a language-model with a lower perplexity will usually—but not always (Clarkson and Robinson, 1999)—induce fewer mis-recognitions for the same task. The perplexity statistic also indicates the relative difficulty of prediction across different domains. Table 5 summarizes the perplexities of language-models for various benchmark tasks in speech-recognition research. Here we adopt the standard practice of excluding from calculation any words encountered in the test set that are not in the lexicon. Note that models with small lexica have artificially low perplexity scores: models with larger lexica are penalized for their unreliable predictions about infrequent words, whereas models with small</context>
</contexts>
<marker>Clarkson, Robinson, 1999</marker>
<rawString>Philip Clarkson and Tony Robinson. 1999. Towards improved language model evaluation measures. In Proceedings of EUROSPEECH 99, 6th European Conference on Speech Communication and Technology, volume 5, pages 1927-1930, September. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cole</author>
<author>J Mariani</author>
<author>H Uszkoreit</author>
<author>A Zaenen</author>
<author>V Zue</author>
</authors>
<title>Survey of the state of the art in human language technology.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Center for Spoken Language Understanding CSLU, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="15048" citStr="Cole et al., 1996" startWordPosition="2500" endWordPosition="2503">principle why a distributed architecture cannot employ speaker-dependent models trained for individual users, although this may in practice be expensive. In either scenario the interface the device presents should allow quick correction of mis-recognized words by keypad or stylus. Schofield and Kubin (2002) describe mobile interfaces for posing questions in more depth. We conjecture that, with a wide-band signal and mild background noise, a speech recognizer customized for questions may mis-transcribe around 5% of words with speaker-dependent acoustic models, or 10-15% otherwise. The sources (Cole et al., 1996; Zheng and Picone, 2001; Chelba, 2000) report similar word-error rates for tasks of this perplexity or greater. We hope in the future to build such a recognizer An over-estimate. More words beginning e than q allow less disambiguation. 22 and test various interfaces for efficiently correcting mis-transcriptions. 6 Future Work the Wall Street Journal. In the light of these results it has discussed the requirements and potential for predictive text-input and speech-recognition of questions with mobile devices. Language-modeling for small devices differs in one essential respect from traditional</context>
</contexts>
<marker>Cole, Mariani, Uszkoreit, Zaenen, Zue, 1996</marker>
<rawString>R. Cole, J. Mariani, H. Uszkoreit, A. Zaenen, and V. Zue. 1996. Survey of the state of the art in human language technology. Technical report, Center for Spoken Language Understanding CSLU, Carnegie Mellon University, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hammond</author>
<author>R Burke</author>
<author>C Martin</author>
<author>S Lytinen</author>
</authors>
<title>FAQ finder: A case-based approach to knowledge navigation.</title>
<date>1995</date>
<booktitle>In Proceedings of the Eleventh Conference on Artificial Intelligence for Applications,</booktitle>
<pages>80--86</pages>
<publisher>IEEE Computer Society Press.</publisher>
<location>Los Alamitos,</location>
<contexts>
<context position="2520" citStr="Hammond et al., 1995" startWordPosition="384" endWordPosition="388">f the corpus of natural-language questions analyzed in Section 3, which compares various rt-gram language-models and shows the per-word perplexity of these questions to be lower than that of the utterances modeled in several common speech-recognition tasks. The paper then discusses the implications of this for input using keypads and styluses (Section 4) and speech (Section 5). Section 6 discusses opportunities for better models of questions. 2 The corpus of questions We collected around 450,000 questions from various online sources—logs of the Ask Jeeves and Excite search engines, FAQFinder (Hammond et al., 1995), AnswerBus (Zheng, 2002), and test questions from the TREC question-answering track (Voorhees, 2001) from 1998 to 2001—and wrote scripts to correct common typos and spelling mistakes and to filter the corpus in the various ways in Table 1. After this massaging the corpus had 279,456 unique questions. Table 2 shows a random sample of questions from the processed corpus. It still includes spelling mistakes (&apos;alcahol&apos;), irregular punctuation (&apos;advanced-screening&apos;, &apos;science related&apos;), and nonsense (`How can I figure?&apos;). Notice that some of the questions in the corpus provide no more information t</context>
</contexts>
<marker>Hammond, Burke, Martin, Lytinen, 1995</marker>
<rawString>K. Hammond, R. Burke, C. Martin, and S. Lytinen. 1995. FAQ finder: A case-based approach to knowledge navigation. In Proceedings of the Eleventh Conference on Artificial Intelligence for Applications, pages 80-86, Los Alamitos, February. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.</title>
<date>2000</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="4474" citStr="Jurafsky and Martin, 2000" startWordPosition="701" endWordPosition="704">t of various n-gram models to the training corpus. We trained models for n = 2, 3, 4 with various sizes of lexicon and &apos;cut-off&apos; thresholds for ignoring infrequent sequences. For each model we randomly constructed five partitions of the corpus, each 90% for training and 10% for testing. Table 3 reports the geometric mean perplexity on the test sets per word. We trained models with both the GoodTuring and Witten-Bell discounting schemes. Table 3 reports perplexities for the latter, denoted type C in (Witten and Bell, 1991), which were 1- 2% lower than for Good-Turing discounting. &apos;Perplexity&apos; (Jurafsky and Martin, 2000) is commonly used in speech recognition research as a measure of the goodness-of-fit of languagemodels; a language-model with a lower perplexity will usually—but not always (Clarkson and Robinson, 1999)—induce fewer mis-recognitions for the same task. The perplexity statistic also indicates the relative difficulty of prediction across different domains. Table 5 summarizes the perplexities of language-models for various benchmark tasks in speech-recognition research. Here we adopt the standard practice of excluding from calculation any words encountered in the test set that are not in the lexic</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J. H. Martin. 2000. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Scott MacKenzie</author>
</authors>
<title>KSPC (keystrokes per character) as a characteristic of text entry techniques.</title>
<date>2002</date>
<booktitle>Proceedings of the Fourth International Symposium on Human Computer Interaction with Mobile Devices, number 2411 in Lecture Notes in Computer Science,</booktitle>
<pages>195--210</pages>
<editor>In Fabio PaternO, editor,</editor>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="6734" citStr="MacKenzie, 2002" startWordPosition="1056" endWordPosition="1057">cally-sized corpus. Speech recognizers have little opportunity for phonetically transcribing a word not in their lexicon, especially for languages like English with many homophones. The usual consequence is a mis-recognition of the offending word and often of its neighbors. Predictive typing aids can be more forgiving. Users of a stylus or keypad can input unusual words normally, ignoring any bogus suggestions. This suggests an alternative measure to perplexity for the effectiveness of language-models for predictive typing, like the expected number of keystrokes per character. Such a measure (MacKenzie, 2002) would depend on implementation-specific characteristics of the interface. We describe one such implementation in progress in the next section; meanwhile we conjecture that, other factors being equal, lower-perplexity models generally imply better prediction. Hingeing upon this, the relatively low perplexities in Tables 3 and 4 bode well for the impatient questioner. 4 Implications for entering questions with a keypad or stylus Most Palm devices display about 11 lines; most Pocket PCs about 15. Locating the intended 18 Table 2: A random sample of questions from the processed corpus. WHERE CAN </context>
<context position="11632" citStr="MacKenzie, 2002" startWordPosition="1926" endWordPosition="1927">y. The second scenario is that the device use an always-on data connection to send each chosen character to a remote server. The server would recompute its predictions with few resource constraints and return an updated list. Our tests in Austria&apos;s GPRS networks indicate that round-trip times for TCP packets are commonly around 1000 21 milliseconds. This may, depending on the interface, be too long to wait. We expect UMTS networks to have less latency. We have not analyzed the average number of keystrokes or stylus gestures that an interface coupled with our language-models would demand. See (MacKenzie, 2002) for a comparison of the keystroke requirements of existing textentry methods. Such a measure is more relevant than perplexity to the speed of text-entry but interface-dependent. We are developing text-prediction software for the Palm and Pocket PC platforms that uses the language-models we have fit to our question corpus. After each new character it presents a fresh list of choices, from which the user can select a whole or partial continuation. Figure 3 depicts a web-based prototype. Once we finalize the interface we intend to measure the throughput and mean time required to input each quest</context>
</contexts>
<marker>MacKenzie, 2002</marker>
<rawString>I. Scott MacKenzie. 2002. KSPC (keystrokes per character) as a characteristic of text entry techniques. In Fabio PaternO, editor, Proceedings of the Fourth International Symposium on Human Computer Interaction with Mobile Devices, number 2411 in Lecture Notes in Computer Science, pages 195-210. Springer-Verlag, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pearce</author>
</authors>
<title>An overview of ETSI standards activities for distributed speech recognition front-ends.</title>
<date>2000</date>
<booktitle>In Proceedings ofAVIOS 2000: The Speech Applications Conference,</booktitle>
<contexts>
<context position="14337" citStr="Pearce, 2000" startWordPosition="2394" endWordPosition="2395"> PC devices include these; most Palm devices do not. Networked devices can recognize speech alone or di stributedly. Alone, a device must store a language-model, phonetic dictionary, acoustic model, and decoder; the decoding algorithm it runs to find the likeliest hypothesis must be efficient enough given the device&apos;s memory constraints and speed. In distributed speechrecognition the device extracts features from the waveform of the utterance, transmits these to a powerful server using an established protocol like the Aurora DSR protocol of the European Telecommunications Standards Institute (Pearce, 2000), and after processing receives a list or lattice of likely hypotheses. There is no reason in principle why a distributed architecture cannot employ speaker-dependent models trained for individual users, although this may in practice be expensive. In either scenario the interface the device presents should allow quick correction of mis-recognized words by keypad or stylus. Schofield and Kubin (2002) describe mobile interfaces for posing questions in more depth. We conjecture that, with a wide-band signal and mild background noise, a speech recognizer customized for questions may mis-transcribe</context>
</contexts>
<marker>Pearce, 2000</marker>
<rawString>D. Pearce. 2000. An overview of ETSI standards activities for distributed speech recognition front-ends. In Proceedings ofAVIOS 2000: The Speech Applications Conference, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ed Schofield</author>
<author>Gernot Kubin</author>
</authors>
<title>On interfaces for mobile information retrieval.</title>
<date>2002</date>
<booktitle>In Fabio Patern6, editor, Proceedings of the Fourth International Symposium on Human Computer Interaction with Mobile Devices, number 2411 in Lecture Notes in Computer Science,</booktitle>
<pages>383--387</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="1201" citStr="Schofield and Kubin (2002)" startWordPosition="176" endWordPosition="179">t to be highly constrained lexically despite its wide spectrum of topics, with a per-word perplexity less than 47 with around 2.6% of words in the test set out-of-vocabulary. One implication is that predictive interfaces can greatly speed up the input of natural-language questions with a keypad or stylus. Another is that automatic speech-recognition of such questions can be quite accurate. 1 Introduction Mobile devices have relatively small screens, and are cumbersome to use for retrieving information with traditional interfaces that return lists of matching documents in response to keywords. Schofield and Kubin (2002) argue that the case for question-answering as an alternative interface for finding information on a small device is strong because answers to questions are more likely to fit comfortably on a small screen than arbitrary documents. Question-answering systems shift a user&apos;s burden from filtering documents for relevance to describing his or her need for information more precisely at the outset with a question rather than a string of keywords. Question-answering thus demands less of the output-facilities of mobile devices but more of their input-facilities. This paper investigates how language-mo</context>
<context position="14739" citStr="Schofield and Kubin (2002)" startWordPosition="2453" endWordPosition="2456">vice extracts features from the waveform of the utterance, transmits these to a powerful server using an established protocol like the Aurora DSR protocol of the European Telecommunications Standards Institute (Pearce, 2000), and after processing receives a list or lattice of likely hypotheses. There is no reason in principle why a distributed architecture cannot employ speaker-dependent models trained for individual users, although this may in practice be expensive. In either scenario the interface the device presents should allow quick correction of mis-recognized words by keypad or stylus. Schofield and Kubin (2002) describe mobile interfaces for posing questions in more depth. We conjecture that, with a wide-band signal and mild background noise, a speech recognizer customized for questions may mis-transcribe around 5% of words with speaker-dependent acoustic models, or 10-15% otherwise. The sources (Cole et al., 1996; Zheng and Picone, 2001; Chelba, 2000) report similar word-error rates for tasks of this perplexity or greater. We hope in the future to build such a recognizer An over-estimate. More words beginning e than q allow less disambiguation. 22 and test various interfaces for efficiently correct</context>
</contexts>
<marker>Schofield, Kubin, 2002</marker>
<rawString>Ed Schofield and Gernot Kubin. 2002. On interfaces for mobile information retrieval. In Fabio Patern6, editor, Proceedings of the Fourth International Symposium on Human Computer Interaction with Mobile Devices, number 2411 in Lecture Notes in Computer Science, pages 383-387. Springer-Verlag, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference (TREC-10). Department of Commerce, National Institute of Standards and Technology.</booktitle>
<contexts>
<context position="2621" citStr="Voorhees, 2001" startWordPosition="401" endWordPosition="402">odels and shows the per-word perplexity of these questions to be lower than that of the utterances modeled in several common speech-recognition tasks. The paper then discusses the implications of this for input using keypads and styluses (Section 4) and speech (Section 5). Section 6 discusses opportunities for better models of questions. 2 The corpus of questions We collected around 450,000 questions from various online sources—logs of the Ask Jeeves and Excite search engines, FAQFinder (Hammond et al., 1995), AnswerBus (Zheng, 2002), and test questions from the TREC question-answering track (Voorhees, 2001) from 1998 to 2001—and wrote scripts to correct common typos and spelling mistakes and to filter the corpus in the various ways in Table 1. After this massaging the corpus had 279,456 unique questions. Table 2 shows a random sample of questions from the processed corpus. It still includes spelling mistakes (&apos;alcahol&apos;), irregular punctuation (&apos;advanced-screening&apos;, &apos;science related&apos;), and nonsense (`How can I figure?&apos;). Notice that some of the questions in the corpus provide no more information than a string 17 Table 1: Elements removed from the corpus. strings of fewer than 3 words duplicate qu</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001. Overview of the TREC 2001 question answering track. In Proceedings of the Tenth Text REtrieval Conference (TREC-10). Department of Commerce, National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="4375" citStr="Witten and Bell, 1991" startWordPosition="687" endWordPosition="690">y for the information itself. 3 n-grams of questions This section describes and compares the fit of various n-gram models to the training corpus. We trained models for n = 2, 3, 4 with various sizes of lexicon and &apos;cut-off&apos; thresholds for ignoring infrequent sequences. For each model we randomly constructed five partitions of the corpus, each 90% for training and 10% for testing. Table 3 reports the geometric mean perplexity on the test sets per word. We trained models with both the GoodTuring and Witten-Bell discounting schemes. Table 3 reports perplexities for the latter, denoted type C in (Witten and Bell, 1991), which were 1- 2% lower than for Good-Turing discounting. &apos;Perplexity&apos; (Jurafsky and Martin, 2000) is commonly used in speech recognition research as a measure of the goodness-of-fit of languagemodels; a language-model with a lower perplexity will usually—but not always (Clarkson and Robinson, 1999)—induce fewer mis-recognitions for the same task. The perplexity statistic also indicates the relative difficulty of prediction across different domains. Table 5 summarizes the perplexities of language-models for various benchmark tasks in speech-recognition research. Here we adopt the standard pra</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zerofrequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Zheng</author>
<author>J Picone</author>
</authors>
<title>Robust low perplexity voice interfaces.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>MITRE Corporation,</institution>
<contexts>
<context position="15072" citStr="Zheng and Picone, 2001" startWordPosition="2504" endWordPosition="2507">tributed architecture cannot employ speaker-dependent models trained for individual users, although this may in practice be expensive. In either scenario the interface the device presents should allow quick correction of mis-recognized words by keypad or stylus. Schofield and Kubin (2002) describe mobile interfaces for posing questions in more depth. We conjecture that, with a wide-band signal and mild background noise, a speech recognizer customized for questions may mis-transcribe around 5% of words with speaker-dependent acoustic models, or 10-15% otherwise. The sources (Cole et al., 1996; Zheng and Picone, 2001; Chelba, 2000) report similar word-error rates for tasks of this perplexity or greater. We hope in the future to build such a recognizer An over-estimate. More words beginning e than q allow less disambiguation. 22 and test various interfaces for efficiently correcting mis-transcriptions. 6 Future Work the Wall Street Journal. In the light of these results it has discussed the requirements and potential for predictive text-input and speech-recognition of questions with mobile devices. Language-modeling for small devices differs in one essential respect from traditional languagemodeling: that </context>
</contexts>
<marker>Zheng, Picone, 2001</marker>
<rawString>F. Zheng and J. Picone. 2001. Robust low perplexity voice interfaces. Technical report, MITRE Corporation, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiping Zheng</author>
</authors>
<title>AnswerBus question answering system.</title>
<date>2002</date>
<booktitle>In Human Language Technology Conference (HLT 2002),</booktitle>
<location>San Diego, CA.,</location>
<contexts>
<context position="2545" citStr="Zheng, 2002" startWordPosition="390" endWordPosition="391">uestions analyzed in Section 3, which compares various rt-gram language-models and shows the per-word perplexity of these questions to be lower than that of the utterances modeled in several common speech-recognition tasks. The paper then discusses the implications of this for input using keypads and styluses (Section 4) and speech (Section 5). Section 6 discusses opportunities for better models of questions. 2 The corpus of questions We collected around 450,000 questions from various online sources—logs of the Ask Jeeves and Excite search engines, FAQFinder (Hammond et al., 1995), AnswerBus (Zheng, 2002), and test questions from the TREC question-answering track (Voorhees, 2001) from 1998 to 2001—and wrote scripts to correct common typos and spelling mistakes and to filter the corpus in the various ways in Table 1. After this massaging the corpus had 279,456 unique questions. Table 2 shows a random sample of questions from the processed corpus. It still includes spelling mistakes (&apos;alcahol&apos;), irregular punctuation (&apos;advanced-screening&apos;, &apos;science related&apos;), and nonsense (`How can I figure?&apos;). Notice that some of the questions in the corpus provide no more information than a string 17 Table 1: </context>
</contexts>
<marker>Zheng, 2002</marker>
<rawString>Zhiping Zheng. 2002. AnswerBus question answering system. In Human Language Technology Conference (HLT 2002), San Diego, CA., March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kingsley Zipf</author>
</authors>
<title>The Psycho-biology of Language: An Introduction to Dynamic Philology.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5673" citStr="Zipf, 1965" startWordPosition="888" endWordPosition="889">in the lexicon. Note that models with small lexica have artificially low perplexity scores: models with larger lexica are penalized for their unreliable predictions about infrequent words, whereas models with smaller lexica have no power whatever to predict infrequent words, and incur no penalty. Thus the perplexity computed this way is incomparable across models with lexica of different sizes, and is imperfect as a measure of a language-model&apos;s predictive power. The vocabulary of open-domain questions is potentially as large as a language itself. We can expect that, as per Zipf&apos;s second law (Zipf, 1965), no corpus of practical size will include all words; interfaces for textual or spoken input must be designed to accommodate omissions in the lexicon gracefully. Figure 1 shows the effect of the size of the training set on the rate of occurrence of new words in unseen questions. Extrapolating, we can predict that unseen questions in this domain are unlikely to have an out-of-vocabulary rate under about 1.5% for models trained with any practically-sized corpus. Speech recognizers have little opportunity for phonetically transcribing a word not in their lexicon, especially for languages like Eng</context>
</contexts>
<marker>Zipf, 1965</marker>
<rawString>George Kingsley Zipf. 1965. The Psycho-biology of Language: An Introduction to Dynamic Philology. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>