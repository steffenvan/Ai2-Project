<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002863">
<title confidence="0.999068">
Unsupervised techniques for discovering ontology
elements from Wikipedia article links
</title>
<author confidence="0.998932">
Zareen Syed Tim Finin
</author>
<affiliation confidence="0.99973">
University of Maryland, Baltimore County University of Maryland, Baltimore County
</affiliation>
<address confidence="0.8640195">
1000 Hilltop Circle 1000 Hilltop Circle
Baltimore, MD 21250, USA Baltimore, MD 21250, USA
</address>
<email confidence="0.998168">
zarsyed1@umbc.edu finin@umbc.edu
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997868125">
We present an unsupervised and unrestricted
approach to discovering an infobox like on-
tology by exploiting the inter-article links
within Wikipedia. It discovers new slots and
fillers that may not be available in the
Wikipedia infoboxes. Our results demonstrate
that there are certain types of properties that
are evident in the link structure of resources
like Wikipedia that can be predicted with high
accuracy using little or no linguistic analysis.
The discovered properties can be further used
to discover a class hierarchy. Our experiments
have focused on analyzing people in Wikipe-
dia, but the techniques can be directly applied
to other types of entities in text resources that
are rich with hyperlinks.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938764705882">
One of the biggest challenges faced by the Seman-
tic Web vision is the availability of structured data
that can be published as RDF. One approach is to
develop techniques to translate information in
spreadsheets, databases, XML documents and
other traditional data formats into RDF (Syed et al.
2010). Another is to refine the technology needed
to extract structured information from unstructured
free text (McNamee and Dang, 2009).
For both approaches, there is a second problem
that must be addressed: do we start with an ontol-
ogy or small catalog of ontologies that will be used
to encode the data or is extracting the right ontol-
ogy part of the problem. We describe exploratory
work on a system that can discover ontological
elements as well as data from a free text with em-
bedded hyperlinks.
</bodyText>
<page confidence="0.97227">
78
</page>
<bodyText confidence="0.999988947368421">
Wikipedia is a remarkable and rich online en-
cyclopedia with a wealth of general knowledge
about varied concepts, entities, events and facts in
the world. Its size and coverage make it a valuable
resource for extracting information about different
entities and concepts. Wikipedia contains both free
text and structured information related to concepts
in the form of infoboxes, category hierarchy and
inter-article links. Infoboxes are the most struc-
tured form and are composed of a set of subject-
attribute-value triples that summarize or highlight
the key features of the concept or subject of the
article. Resources like DBpedia (Auer et al., 2007)
and Freebase (Bollacker et al., 2007) have har-
vested this structured data and have made it avail-
able as triples for semantic querying.
While infoboxes are a readily available source
of structured data, the free text of the article con-
tains much more information about the entity.
Barker et al. (2007) unified the state of the art ap-
proaches in natural language processing and
knowledge representation in their prototype system
for understanding free text. Text resources which
are rich in hyperlinks especially to knowledge
based resources (such as encyclopedias or diction-
aries) have additional information encoded in the
form of links, which can be used to complement
the existing systems for text understanding and
knowledge discovery. Furthermore, systems such
as Wikify (Mihalcea and Csomai, 2007) can be
employed to link words in free text to knowledge
resources like Wikipedia and thus enrich the free
text with hyperlinks.
We describe an approach for unsupervised on-
tology discovery from links in the free text of the
Wikipedia articles, without specifying a relation or
set of relations in advance. We first identify candi-
date slots and fillers for an entity, then classify en-
</bodyText>
<note confidence="0.892977">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 78–86,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.996511325581396">
tities and finally derive a class hierarchy. We have
evaluated our approach for the Person class, but it
can be easily generalized to other entity types such
as organizations, places, and products.
The techniques we describe are not suggested
as alternatives to natural language understanding or
information extraction, but as a source for addi-
tional evidence that can be used to extract onto-
logical elements and relations from the kind of text
found in Wikipedia and other heavily-linked text
collections. This approach might be particularly
useful in “slot fillings” tasks like the one in the
Knowledge Base Population track (McNamee and
Dang, 2010) at the 2009 Text Analysis Confer-
ence. We see several contributions that this work
has to offer:
• Unsupervised and unrestricted ontology discov-
ery. We describe an automatic approach that
does not require a predefined list of relations or
training data. The analysis uses inter-article
links in the text and does not depend on existing
infoboxes, enabling it to suggest slots and fillers
that do not exist in any extant infoboxes.
• Meaningful slot labels. We use WordNet (Mil-
ler et al., 1990) nodes to represent and label
slots enabling us to exploit WordNet’s hy-
pernym and hyponym relations as a property hi-
erarchy.
• Entity classification and class labeling. We in-
troduce a new feature set for entity classifica-
tion, i.e. the discovered ranked slots, which per-
forms better than other feature sets extracted
from Wikipedia. We also present an approach
for assigning meaningful class label vectors us-
ing WordNet nodes.
• Deriving a class hierarchy. We have developed
an approach for deriving a class hierarchy based
on the ranked slot similarity between classes
and the label vectors.
In the remainder of the paper we describe the de-
tails of the approach, mention closely related work,
present and discuss preliminary results and provide
some conclusions and possible next steps.
</bodyText>
<sectionHeader confidence="0.572657" genericHeader="introduction">
2 Approach
</sectionHeader>
<figureCaption confidence="0.988798428571428">
Figure 1 shows our ontology discovery framework
and its major steps. We describe each step in the
rest of this section.
2.1 Discovering Candidate Slots and Fillers
79
Figure 1: The ontology discovery framework com-
prises a number of steps, including candidate slot and
</figureCaption>
<bodyText confidence="0.940739444444445">
filler discovery followed by slot ranking, slot selec-
tion, entity classification, slot re-ranking, class label-
ing, and class hierarchy discovery.
Most Wikipedia articles represent a concept, i.e., a
generic class of objects (e.g., Musician), an indi-
vidual object (e.g., Michael_Jackson), or a generic
relation or property (e.g., age). Inter-article links
within Wikipedia represent relations between con-
cepts. In our approach we consider the linked con-
cepts as candidate fillers for slots related to the
primary article/concept. There are several cases
where the filler is subsumed by the slot label for
example, the infobox present in the article on “Mi-
chael_Jackson” (Figure 2) mentions pop, rock and
soul as fillers for the slot Genre and all three of
these are a type of Genre. The Labels slot contains
fillers such as Motown, Epic and Legacy which are
all Record Label Companies. Based on this obser-
vation, we discover and exploit “isa” relations be-
tween fillers (linked concepts) and WordNet nodes
to serve as candidate slot labels.
In order to find an “isa” relation between a con-
cept and a WordNet synset we use manually cre-
ated mappings by DBpedia, which links about
467,000 articles to synsets. However, Wikipedia
has more than two million articles1, therefore, to
map any remaining concepts we use the automati-
cally generated mappings available between
WordNet synsets and Wikipedia categories
(Ponzetto and Navigli, 2009). A single Wikipedia
article might have multiple categories associated
with it and therefore multiple WordNet synsets.
Wikipedia’s category system serves more as a way
to tag articles and facilitate navigation rather than
1 This estimate is for the English version and does not
include redirects and administrative pages such as dis-
ambiguation pages.
the same type have common slots. For example,
there is a set of slots common for musical artists
whereas, a different set is common for basketball
players. The Wikipedia infobox templates based
on classes also provide a set of properties or slots
to use for particular types of entities or concepts.
In case of people, it is common to note that
there is a set of slots that are generalized, i.e., they
are common across all types of persons. Examples
are name, born, and spouse. There are also sets of
specialized slots, which are generally associated
with a given profession. For example, the slots for
basketball players have information for basketball
related activities and musical artists have slots with
music related activities. The slots for “Mi-
chael_Jordan” include Professional Team(s), NBA
Draft, Position(s) and slots for “Michael_Jackson”
include Genres, Instruments and Labels.
Another observation is that people engaged in a
particular profession tend to be linked to others
within the same profession. Hence the maxim “A
man is known by the company he keeps.” For ex-
ample, basketball players are linked to other bas-
ketball players and politicians are linked to other
politicians. We rank the slots based on the number
of linked persons having the same slots. We gener-
</bodyText>
<figureCaption confidence="0.671363615384615">
ated a list of person articles in Wikipedia by get-
ting all Wikipedia articles under the Person type in
Freebase2. We randomly select up to 25 linked per-
sons (which also link back) and extract their candi-
date slots and vote for a slot based on the number
of times it appears as a slot in a linked person nor-
malized by the number of linked persons to assign
a slot score.
Figure 2. The Wikipedia infobox
for the Michael_Jackson article has
a number of slots from appropriate
infobox templates.
to categorize them. The article on Michael Jordan,
</figureCaption>
<bodyText confidence="0.99749502">
for example, has 36 categories associated with it.
In order to select an individual WordNet synset as
a label for the concept’s type, we use two heuris-
tics:
• Category label extraction. Since the first sen-
tence in Wikipedia articles usually defines the
concept, we extract a category label from the
first sentence using patterns based on POS tags
similar to Kazama and Torisawa (2007).
• Assign matching WordNet synset. We con-
sider all the WordNet synsets associated with
the categories of the article using the category
to WordNet mapping (Ponzetto and Navigli,
2009) and assign the WordNet synset if any of
the words in the synset matches with the ex-
tracted category label. We repeat the process
with hypernyms and hyponyms of the synset
up to three levels.
2.2 Slot Ranking
All slots discovered using outgoing links might not
be meaningful, therefore we have developed tech-
niques for ranking and selecting slots. Our ap-
proach is based on the observation that entities of
80
2.3 Entity Classification and Slot Re-Ranking
The ranked candidate slots are used to classify en-
tities and then further ranked based on number of
times they appear among the entities in the cluster.
We use complete link clustering using a simple slot
similarity function:
This similarity metric for slots is computed as the
cosine similarity between tf.idf weighted slot vec-
tors, where the slot score represents the term fre-
2 We found that the Freebase classification for Person
was more extensive that DBpedia’s in the datasets avail-
able to us in early 2009.
quency component and the inverse document fre-
quency is based on the number of times the slot
appears in different individuals.
We also collapsed location expressing slots
(country, county, state, district, island etc.) into the
slot labeled location by generating a list of location
words from WordNet as these slots were causing
the persons related to same type of geographical
location to cluster together.
After clustering, we re-score the slots based on
number of times they appear among the individuals
in the cluster normalized by the cluster size. The
output of clustering is a vector of scored slots as-
sociated with each cluster.
</bodyText>
<subsectionHeader confidence="0.996666">
2.4 Slot Selection
</subsectionHeader>
<bodyText confidence="0.99996724137931">
The slot selection process identifies and filters out
slots judged to be irrelevant. Our intuition is that
specialized slots or attributes for a particular entity
type should be somehow related to each other. For
example, we would expect attributes like league,
season and team for basketball players and genre,
label, song and album for musical artists. If an at-
tribute like album appears for basketball players it
should be discarded as it is not related to other at-
tributes.
We adopted a clustering approach for finding
attributes that are related to each other. For each
pair of attributes in the slot vector, we compute a
similarity score based on how many times the two
attribute labels appear together in Wikipedia per-
son articles within a distance of 100 words as
compared to the number of times they appear in
total and weigh it using weights of the individual
attributes in the slot vector. This metric is captured
in the following equation, where Df is the docu-
ment frequency and wt is the attribute weight.
Our initial experiments using single and com-
plete link clustering revealed that single link was
more appropriate for slot selection. We got clusters
at a partition distance of 0.9 and selected the larg-
est cluster from the set of clusters. In addition, we
also added any attributes exceeding a 0.4 score into
the set of selected attributes. Selected ranked slots
for Michael Jackson are given in Table 1.
</bodyText>
<subsectionHeader confidence="0.971697">
2.5 Class Labeling
</subsectionHeader>
<table confidence="0.999378823529412">
Slot Score Fillers Example
Musician 1.00 ray charles, sam cooke ...
Album 0.99 bad (album), ...
Location 0.97 gary, indiana, chicago, ...
Music genre 0.90 pop music, soul music, ...
Label 0.79 a&amp;m records, epic records, ...
Phonograph_ 0.67 give_in_to_me,
record this place hotel ...
Act 0.59 singing
Movie 0.46 moonwalker ...
Company 0.43 war child (charity), ...
Actor 0.41 stan winston, eddie murphy,
Singer 0.40 britney spears, ...
Magazine 0.29 entertainment weekly,...
Writing style 0.27 hip hop music
Group 0.21 &apos;n sync, RIAA
Song 0.20 d.s. (song) ...
</table>
<tableCaption confidence="0.9948255">
Table 1: Fifteen slots were discovered for musician
Michael Jackson along with scores and example fillers.
</tableCaption>
<bodyText confidence="0.999958875">
Assigning class labels to clusters gives additional
information about the type of entities in a cluster.
We generate a cluster label vector for each cluster
which represents the type of entities in the cluster.
We compute a list of person types by taking all
hyponyms under the corresponding person sense in
WordNet. That list mostly contained the profes-
sions list for persons such as basketball player,
president, bishop etc. To assign a WordNet type to
a person in Wikipedia we matched the entries in
the list to the words in the first sentence of the per-
son article and assigned it the set of types that
matched. For example, for Michael Jordan the
matching types found were basketball_player,
businessman and player.
We assigned the most frequent sense to the
matching word as followed by Suchanek et al.
(2008) and Wu and Weld (2008), which works for
majority of the cases. We then also add all the hy-
pernyms of the matching types under the Person
node. The vector for Michael Jordan has entries
basketball_player, athlete, businessperson, person,
contestant, businessman and player. After getting
matching types and their hypernyms for all the
members of the cluster, we score each type based
on the number of times it occurs in its members
normalized by the cluster size. For example for one
of the clusters with 146 basketball players we got
the following label vector: {player:0.97, contest-
ant:0.97, athlete:0.96, basketball_player:0.96}. To
select an individual label for a class we can pick
the label with the highest score (the most general-
</bodyText>
<page confidence="0.993444">
81
</page>
<bodyText confidence="0.9976855">
ized label) or the most specialized label having a
score above a given threshold.
</bodyText>
<subsectionHeader confidence="0.967807">
2.6 Discovering Class Hierarchy
</subsectionHeader>
<bodyText confidence="0.999985523809524">
We employ two different feature sets to discover
the class hierarchy, i.e., the selected slot vectors
and the class label vectors and combine both func-
tions using their weighted sum. The similarity
functions are described below.
The common slot similarity function is the co-
sine similarity between the common slot tf.idf vec-
tors, where the slot score represents the tf and the
idf is based on the number of times a particular slot
appears in different clusters at that iteration. We
re-compute the idf term in each iteration. We de-
fine the common slot tf.idf vector for a cluster as
one where we assign a non-zero weight to only the
slots that have non-zero weight for all cluster
members. The label similarity function is the co-
sine similarity between the label vectors for clus-
ters. The hybrid similarity function is a weighted
sum of the common slot and label similarity func-
tions. Using these similarity functions we apply
complete link hierarchical clustering algorithm to
discover the class hierarchy.
</bodyText>
<sectionHeader confidence="0.998167" genericHeader="method">
3 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.9999798">
For our experiments and evaluation we used the
Wikipedia dump from March 2008 and the DBpe-
dia infobox ontology created from Wikipedia
infoboxes using hand-generated mappings (Auer et
al., 2007). The Person class is a direct subclass of
the owl:Thing class and has 21 immediate sub-
classes and 36 subclasses at the second level. We
used the persons in different classes in DBpedia
ontology at level two to generate data sets for ex-
periments.
There are several articles in Wikipedia that are
very small and have very few out-links and in-
links. Our approach is based on the out-links and
availability of information about different related
things on the article, therefore, in order to avoid
data sparseness, we randomly select articles with
greater than 100 in-links and out-links, at least
5KB page length and having at least five links to
entities of the same type that link back (in our case
persons).
We first compare our slot vector features with
other features extracted from Wikipedia for entity
classification task and then evaluate their accuracy.
We then discover the class hierarchy and compare
the different similarity functions.
</bodyText>
<subsectionHeader confidence="0.971811">
3.1 Entity Classification
</subsectionHeader>
<bodyText confidence="0.9998045">
We did some initial experiments to compare our
ranked slot features with other feature sets ex-
tracted from Wikipedia. We created a dataset com-
posed of 25 different classes of Persons present at
level 2 in the DBpedia ontology by randomly se-
lecting 200 person articles from each class. For
several classes we got less than 200 articles which
fulfilled our selection criteria defined earlier. We
generated twelve types of feature sets and evalu-
ated them using ground truth from DBpedia ontol-
ogy.
We compare tf.idf vectors constructed using
twelve different feature sets: (1) Ranked slot fea-
tures, where tf is the slot score; (2) Words in first
sentence of an article; (3) Associated categories;
(4) Assigned WordNet nodes (see section 2.2); (5)
Associated categories tokenized into words; (6)
Combined Feature Sets 1 to 5 (All); (7-11) Feature
sets 7 to 11 are combinations excluding one feature
set at a time; (12) Unranked slots where tf is 1 for
all slots. We applied complete link clustering and
evaluated the precision, recall and F-measure at
different numbers of clusters ranging from one to
100. Table 2 gives the precision, recall and num-
ber of clusters where we got the maximum F-
measure using different feature sets.
</bodyText>
<page confidence="0.996608">
82
</page>
<table confidence="0.998955">
No. Feature Set k P R F
1 Ranked Slots 40 0.74 0.72 0.73
2 First Sentence 89 0.07 0.53 0.12
3 Categories 1 0.05 1.00 0.10
4 WordNet Nodes 87 0.40 0.22 0.29
5 (3 tokenized) 93 0.85 0.47 0.60
6 All (1 to 5) 68 0.87 0.62 0.72
7 (All – 5) 82 0.79 0.46 0.58
8 (All – 4) 58 0.78 0.63 0.70
9 (All – 3) 53 0.76 0.65 0.70
10 (All – 2) 58 0.88 0.63 0.74
11 (All – 1) 57 0.77 0.60 0.68
12 (1 unranked) 34 0.57 0.65 0.61
</table>
<tableCaption confidence="0.90044675">
Table 2: Comparison of the precision, recall and F-
measure for different feature sets for entity classifi-
cation. The k column shows the number of clusters
that maximized the F score.
</tableCaption>
<bodyText confidence="0.9999845">
Feature set 10 (all features except feature 2) gave
the best F-measure i.e. 0.74, whereas, feature set 1
(ranked slots only) gave the second best F-measure
i.e. 0.73 which is very close to the best result. Fea-
ture set 12 (unranked slots) gave a lower F-
measure i.e. 0.61 which shows that ranking or
weighing slots based on linked entities of the same
type performs better for classification.
</bodyText>
<subsectionHeader confidence="0.998933">
3.2 Slot and Filler Evaluation
</subsectionHeader>
<bodyText confidence="0.999985809523809">
To evaluate our approach to finding slot fillers, we
focused on DBpedia classes two levels below Per-
son (e.g., Governor and FigureSkater). We ran-
domly selected 200 articles from each of these
classes using the criteria defined earlier to avoid
data sparseness. Classes for which fewer than 20
articles were found were discarded. The resulting
dataset comprised 28 classes and 3810 articles3.
We used our ranked slots tf.idf feature set and
ran a complete link clustering algorithm producing
clusters at partition distance of 0.8. The slots were
re-scored based on the number of times they ap-
peared in the cluster members normalized by the
cluster size. We applied slot selection over the re-
scored slots for each cluster. In order to evaluate
our slots and fillers we mapped each cluster to a
DBpedia class based on the maximum number of
members of a particular DBpedia class in our clus-
ter. This process predicted 124 unique properties
for the classes. Of these, we were able to manually
align 46 to properties in either DBpedia or Free-
</bodyText>
<footnote confidence="0.3678635">
3 For some of the classes, fewer than the full comple-
ment of 200 articles were found.
</footnote>
<table confidence="0.9994980625">
No. Property Accuracy
1 automobile race 1.00
2 championship 1.00
3 expressive style 1.00
4 fictional character 1.00
5 label 1.00
6 racetrack 1.00
7 team sport 1.00
8 writing style 1.00
9 academic degree 0.95
10 album 0.95
11 book 0.95
12 contest 0.95
13 election 0.95
14 league 0.95
15 phonograph record 0.95
16 race 0.95
17 tournament 0.94
18 award 0.90
19 movie 0.90
20 novel 0.90
21 school 0.90
22 season 0.90
23 serial 0.90
24 song 0.90
25 car 0.85
26 church 0.85
27 game 0.85
28 musical_instrument 0.85
29 show 0.85
30 sport 0.85
31 stadium 0.85
32 broadcast 0.80
33 telecast 0.80
34 hockey league 0.75
35 music genre 0.70
36 trophy 0.70
37 university 0.65
38 character 0.60
39 disease 0.60
40 magazine 0.55
41 team 0.50
42 baseball club 0.45
43 club 0.45
44 party 0.45
45 captain 0.30
46 coach 0.25
Avg. Accuracy: 0.81
</table>
<tableCaption confidence="0.999761">
Table 3: Manual evaluation of discovered properties
</tableCaption>
<bodyText confidence="0.9999623">
base for the corresponding class. We initially tried
to evaluate the discovered slots by comparing them
with those found in the ontologies underlying
DBpedia and Freebase, but were able to find an
overlap in the subject and object pairs for very few
properties.
We randomly selected 20 subject object pairs
for each of the 46 properties from the correspond-
ing classes and manually judged whether or not the
relation was correct by consulting the correspond-
</bodyText>
<page confidence="0.996509">
83
</page>
<table confidence="0.998497714285714">
Similarity Function k F k F
(L=2) (L=2) (L=1) (L=1)
simslot 56 0.61 13 0.55
simcomslot 74 0.61 15 0.65
simlabel 50 0.63 10 0.76
simhyb wc=wl=0.5 59 0.63 10 0.76
simhyb wc=0.2, wl=0.8 61 0.63 8 0.79
</table>
<tableCaption confidence="0.9822055">
Table 4: Evaluation results for class hierarchy predic-
tion using different similarity functions.
</tableCaption>
<bodyText confidence="0.9804095">
ing Wikipedia articles (Table 3). The average ac-
curacy for the 46 relations was 81%.
</bodyText>
<subsectionHeader confidence="0.998973">
3.3 Discovering Class Hierarchy
</subsectionHeader>
<bodyText confidence="0.99999292">
In order to discover the class hierarchy, we took all
of the clusters obtained earlier at partition distance
of 0.8 and their corresponding slot vectors after
slot selection. We experimented with different
similarity functions and evaluated their accuracy
by comparing the results with the DBpedia ontol-
ogy. A complete link clustering algorithm was ap-
plied using different settings of the similarity func-
tions and the resulting hierarchy compared to
DBpedia’s Person class hierarchy. Table 4 shows
the highest F measure obtained for Person’s imme-
diate sub-classes (L1), “sub-sub-classes” (L2) and
the number of clusters (k) for which we got the
highest F-measure using a particular similarity
function.
The highest F-measure both at level 2 (0.63) and
level 1 (0.79) was obtained by simhyb with wc=0.2,
wl=0.8 and also at lowest number of clusters at L1
(k=8). The simhyb (wc=wl=0.5) and simlabel functions
gave almost the same F-measure at both levels.
The simcom_slot function gave better performance at
L1 (F=0.65) than the base line simslot (F=0.55)
which was originally used for entity clustering.
However, both these functions gave the same F-
measure at L2 (F=0.61).
</bodyText>
<sectionHeader confidence="0.998706" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999979647058824">
In case of property evaluation, properties for which
the accuracy was 60% or below include coach,
captain, baseball_club, club, party, team and
magazine. For the magazine property (correspond-
ing to Writer and ComicsCreator class) we ob-
served that many times a magazine name was men-
tioned in an article because it published some news
about a person rather than that person contributing
any article in that magazine. For all the remaining
properties we observed that these were related to
some sort of competition. For example, a person
played against a team, club, coach or captain. The
political party relation is a similar case, where arti-
cles frequently mention a politician’s party affilia-
tion as well as significant opposition parties. For
such properties, we need to exploit additional con-
textual information to judge whether the person
competed “for” or “against” a particular team,
club, coach or party. Even if the accuracy for fill-
ers for such slots is low, it can still be useful to
discover the kind of slots associated with an entity.
We also observed that there were some cases
where the property was related to a family member
of the primary person such as for disease, school
and university. Certain other properties such as
spouse, predecessor, successor, etc. require more
contextual information and are not directly evident
in the link structure. However, our experiments
show that there are certain properties that can be
predicted with high accuracy using the article links
only and can be used to enrich the existing infobox
ontology or for other purposes.
While our work has mostly experimented with
person entities, the approach can be applied to oth-
er types as well. For example, we were able to dis-
cover software as a candidate slot for companies
like Microsoft, Google and Yahoo!, which ap-
peared among the top three ranked slots using our
slot ranking scheme and corresponds to the prod-
ucts slot in the infoboxes of these companies.
For class hierarchy discovery, we have ex-
ploited the specialized slots after slot selection.
One way to incorporate generalized slots in the
hierarchy is to consider all slots for class members
(without slot selection) and recursively propagate
the common slots present at any level to the level
above it. For example, if we find the slot team to
be common for different types of Athletes such as
basketball players, soccer players etc. we can
propagate it to the Athlete class, which is one level
higher in the hierarchy.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999945857142857">
Unsupervised relation discovery was initially in-
troduced by Hasegawa et al. (2004). They devel-
oped an approach to discover relations by cluster-
ing pairs of entities based on intervening words
represented as context vectors. Shinyama and Se-
kine (2006) generated basic patterns using parts of
text syntactically connected to the entity and then
</bodyText>
<page confidence="0.993844">
84
</page>
<bodyText confidence="0.999972314285714">
generated a basic cluster composed of a set of
events having the same relation.
Several approaches have used linguistic analysis
to generate features for supervised or un-
supervised relation extraction (Nguyen et al., 2007;
Etzioni et al., 2008; Yan et al., 2009). Our ap-
proach mainly exploits the heavily linked structure
of Wikipedia and demonstrates that there are sev-
eral relations that can be discovered with high ac-
curacy without the need of features generated from
a linguistic analysis of the Wikipedia article text.
Suchanek et al. (2008) used Wikipedia catego-
ries and infoboxes to extract 92 relations by apply-
ing specialized heuristics for each relation and in-
corporated the relations in their YAGO ontology,
whereas our techniques do not use specialized heu-
ristics based on the type of relation. Kylin (Weld
et al., 2008) generated infoboxes for articles by
learning from existing infoboxes, whereas we can
discover new fillers for several existing slots and
also discover new slots for infoboxes. KOG (Wu
and Weld, 2008) automatically refined the Wiki-
pedia infobox ontology and integrated Wikipedia’s
infobox-class schemata with WordNet. Since we
already use the WordNet nodes for representing
slots, it eliminates the need for several of KOG’s
infobox refinement steps.
While YAGO, Kylin and KOG all rely on rela-
tions present in the infoboxes, our approach can
complement these by discovering new relations
evident in inter-article links in Wikipedia. For ex-
ample, we could add slots like songs and albums to
the infobox schema for Musical Artists, movies for
the Actors infobox schema, and party for the Poli-
ticians schema.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999989595744681">
People have been learning by reading for thou-
sands of years. The past decade, however, has
seen a significant change in the way people read.
The developed world now does much of its reading
online and this change will soon be nearly univer-
sal. Most online content is read as hypertext via a
Web browser or custom reading device. Unlike
text, hypertext is semi-structured information, es-
pecially when links are drawn from global name-
space, making it easy for many documents to link
unambiguously to a common referent.
The structured component of hypertext aug-
ments the information in its plain text and provides
an additional source of information from which
both people and machines can learn. The work
described in this paper is aimed at learning useful
information, both about the implicit ontology and
facts, from the links embedded in collection of hy-
pertext documents.
Our approach is fully unsupervised and does
not require having a pre-defined catalogue of rela-
tions. We have discovered several new slots and
fillers that are not present in existing Wikipedia
infoboxes and also a scheme to rank the slots based
on linked entities of the same type. We compared
our results with ground truth from the DBpedia
infobox ontology and Freebase for the set of prop-
erties that were common and manually evaluated
the accuracy of the common properties. Our results
show that there are several properties that can be
discovered with high accuracy from the link struc-
ture in Wikipedia and can also be used to discover
a class hierarchy.
We plan to explore the discovery of slots from
non-Wikipedia articles by linking them to Wikipe-
dia concepts using existing systems like Wikify
(Mihalcea and Csomai, 2007). Wikipedia articles
are encyclopedic in nature with the whole article
revolving around a single topic or concept. Con-
sequently, linked articles are a good source of
properties and relations. This might not be the case
in other genres, such as news articles, that discuss
a number of different entities and events. One way
to extend this work to other genres is by first de-
tecting the entities in the article and then only
processing links in sentences that mention an entity
to discover its properties.
</bodyText>
<sectionHeader confidence="0.996552" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9807586">
The research described in this paper was supported
in part by a Fulbright fellowship, a gift from Mi-
crosoft Research, NSF award IIS-0326460 and the
Johns Hopkins University Human Language Tech-
nology Center of Excellence.
</bodyText>
<page confidence="0.999678">
85
</page>
<sectionHeader confidence="0.995876" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998527139240506">
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann and Zachary Ives. 2007. DBpedia: A nu-
cleus for a web of open data. In Proceedings of the
6th International Semantic Web Conference: 11–15.
Ken Barker et al. 2007. Learning by reading: A proto-
type system, performance baseline and lessons
learned, Proceedings of the 22nd National Confer-
ence on Artificial Intelligence, AAAI Press.
K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A
Shared Database of Structured General Human
Knowledge. Proceedings of the National Conference
on Artificial Intelligence (Volume 2): 1962-1963.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Communications of the ACM 51, 12
(December): 68-74.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named enti-
ties from large corpora. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics: 415-422.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named en-
tity recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning: 698–707.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the TAC 2009 knowledge base population track.
In Proceedings of the 2009 Text Analysis Confer-
ence. National Institute of Standards and Technol-
ogy, November.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge. In
Proceedings of the 16th ACM Conference on
Information and Knowledge Management: 233–242.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
WordNet: An on-line lexical database. International
Journal of Lexicography, 3:235–244.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizu-
ka. 2007. Subtree mining for relation extraction from
Wikipedia. In Proceedings of Human Language
Technologies: The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics:125–128.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
Wikipedia and WordNet. Web Semantics, 6(3):203–
217.
Zareen Syed, Tim Finin, Varish Mulwad and Anupam
Joshi. 2010. Exploiting a Web of Semantic Data for
Interpreting Tables, Proceedings of the Second Web
Science Conference.
Simone P. Ponzetto and Roberto Navigli. 2009. Large-
scale taxonomy mapping for restructuring and inte-
grating Wikipedia. In Proceedings of the Twenty-
First International Joint Conference on Artificial In-
telligence: 2083–2088.
Yusuke Shinyama and Satoshi Sekine. 2006. Pre-emp-
tive information extraction using unrestricted relation
discovery. In Proceedings of Human Language Tech-
nologies: The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics:.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008.
Using Wikipedia to bootstrap open information ex-
trac-tion. SIGMOD Record, 37(4): 62–68.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the Wikipedia infobox ontology. In Proceedings
of the 17th International World Wide Web Confer-
ence, pages 635–644.
Wikipedia. 2008. Wikipedia, the free encyclopedia.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-
lation extraction by mining Wikipedia texts using in-
formation from the web. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics: Volume 2: 1021–1029.
</reference>
<page confidence="0.998511">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948788">
<title confidence="0.975688">Unsupervised techniques for discovering elements from Wikipedia article links</title>
<author confidence="0.999234">Zareen Syed Tim Finin</author>
<affiliation confidence="0.999647">University of Maryland, Baltimore County University of Maryland, Baltimore County</affiliation>
<address confidence="0.999055">1000 Hilltop Circle 1000 Hilltop Circle Baltimore, MD 21250, USA Baltimore, MD 21250, USA</address>
<email confidence="0.998212">zarsyed1@umbc.edufinin@umbc.edu</email>
<abstract confidence="0.999764882352941">We present an unsupervised and unrestricted approach to discovering an infobox like ontology by exploiting the inter-article links within Wikipedia. It discovers new slots and fillers that may not be available in the Wikipedia infoboxes. Our results demonstrate that there are certain types of properties that are evident in the link structure of resources like Wikipedia that can be predicted with high accuracy using little or no linguistic analysis. The discovered properties can be further used to discover a class hierarchy. Our experiments have focused on analyzing people in Wikipedia, but the techniques can be directly applied to other types of entities in text resources that are rich with hyperlinks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sören Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Zachary Ives</author>
</authors>
<title>DBpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference:</booktitle>
<pages>11--15</pages>
<contexts>
<context position="2498" citStr="Auer et al., 2007" startWordPosition="389" endWordPosition="392">ich online encyclopedia with a wealth of general knowledge about varied concepts, entities, events and facts in the world. Its size and coverage make it a valuable resource for extracting information about different entities and concepts. Wikipedia contains both free text and structured information related to concepts in the form of infoboxes, category hierarchy and inter-article links. Infoboxes are the most structured form and are composed of a set of subjectattribute-value triples that summarize or highlight the key features of the concept or subject of the article. Resources like DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2007) have harvested this structured data and have made it available as triples for semantic querying. While infoboxes are a readily available source of structured data, the free text of the article contains much more information about the entity. Barker et al. (2007) unified the state of the art approaches in natural language processing and knowledge representation in their prototype system for understanding free text. Text resources which are rich in hyperlinks especially to knowledge based resources (such as encyclopedias or dictionaries) have additional inf</context>
<context position="16922" citStr="Auer et al., 2007" startWordPosition="2747" endWordPosition="2750">o weight to only the slots that have non-zero weight for all cluster members. The label similarity function is the cosine similarity between the label vectors for clusters. The hybrid similarity function is a weighted sum of the common slot and label similarity functions. Using these similarity functions we apply complete link hierarchical clustering algorithm to discover the class hierarchy. 3 Experiments and Evaluation For our experiments and evaluation we used the Wikipedia dump from March 2008 and the DBpedia infobox ontology created from Wikipedia infoboxes using hand-generated mappings (Auer et al., 2007). The Person class is a direct subclass of the owl:Thing class and has 21 immediate subclasses and 36 subclasses at the second level. We used the persons in different classes in DBpedia ontology at level two to generate data sets for experiments. There are several articles in Wikipedia that are very small and have very few out-links and inlinks. Our approach is based on the out-links and availability of information about different related things on the article, therefore, in order to avoid data sparseness, we randomly select articles with greater than 100 in-links and out-links, at least 5KB p</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Ives, 2007</marker>
<rawString>Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann and Zachary Ives. 2007. DBpedia: A nucleus for a web of open data. In Proceedings of the 6th International Semantic Web Conference: 11–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
</authors>
<title>Learning by reading: A prototype system, performance baseline and lessons learned,</title>
<date>2007</date>
<booktitle>Proceedings of the 22nd National Conference on Artificial Intelligence,</booktitle>
<publisher>AAAI Press.</publisher>
<marker>Barker, 2007</marker>
<rawString>Ken Barker et al. 2007. Learning by reading: A prototype system, performance baseline and lessons learned, Proceedings of the 22nd National Conference on Artificial Intelligence, AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>R Cook</author>
<author>P Tufts</author>
</authors>
<title>Freebase: A Shared Database of Structured General Human Knowledge.</title>
<date>2007</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence</booktitle>
<volume>2</volume>
<pages>1962--1963</pages>
<contexts>
<context position="2536" citStr="Bollacker et al., 2007" startWordPosition="395" endWordPosition="398">alth of general knowledge about varied concepts, entities, events and facts in the world. Its size and coverage make it a valuable resource for extracting information about different entities and concepts. Wikipedia contains both free text and structured information related to concepts in the form of infoboxes, category hierarchy and inter-article links. Infoboxes are the most structured form and are composed of a set of subjectattribute-value triples that summarize or highlight the key features of the concept or subject of the article. Resources like DBpedia (Auer et al., 2007) and Freebase (Bollacker et al., 2007) have harvested this structured data and have made it available as triples for semantic querying. While infoboxes are a readily available source of structured data, the free text of the article contains much more information about the entity. Barker et al. (2007) unified the state of the art approaches in natural language processing and knowledge representation in their prototype system for understanding free text. Text resources which are rich in hyperlinks especially to knowledge based resources (such as encyclopedias or dictionaries) have additional information encoded in the form of links,</context>
</contexts>
<marker>Bollacker, Cook, Tufts, 2007</marker>
<rawString>K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A Shared Database of Structured General Human Knowledge. Proceedings of the National Conference on Artificial Intelligence (Volume 2): 1962-1963.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2008</date>
<journal>Communications of the ACM</journal>
<volume>51</volume>
<pages>12</pages>
<contexts>
<context position="27276" citStr="Etzioni et al., 2008" startWordPosition="4506" endWordPosition="4509"> the hierarchy. 5 Related Work Unsupervised relation discovery was initially introduced by Hasegawa et al. (2004). They developed an approach to discover relations by clustering pairs of entities based on intervening words represented as context vectors. Shinyama and Sekine (2006) generated basic patterns using parts of text syntactically connected to the entity and then 84 generated a basic cluster composed of a set of events having the same relation. Several approaches have used linguistic analysis to generate features for supervised or unsupervised relation extraction (Nguyen et al., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia and demonstrates that there are several relations that can be discovered with high accuracy without the need of features generated from a linguistic analysis of the Wikipedia article text. Suchanek et al. (2008) used Wikipedia categories and infoboxes to extract 92 relations by applying specialized heuristics for each relation and incorporated the relations in their YAGO ontology, whereas our techniques do not use specialized heuristics based on the type of relation. Kylin (Weld et al., 2008) generated </context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open information extraction from the web. Communications of the ACM 51, 12 (December): 68-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>415--422</pages>
<contexts>
<context position="26769" citStr="Hasegawa et al. (2004)" startWordPosition="4426" endWordPosition="4429">. For class hierarchy discovery, we have exploited the specialized slots after slot selection. One way to incorporate generalized slots in the hierarchy is to consider all slots for class members (without slot selection) and recursively propagate the common slots present at any level to the level above it. For example, if we find the slot team to be common for different types of Athletes such as basketball players, soccer players etc. we can propagate it to the Athlete class, which is one level higher in the hierarchy. 5 Related Work Unsupervised relation discovery was initially introduced by Hasegawa et al. (2004). They developed an approach to discover relations by clustering pairs of entities based on intervening words represented as context vectors. Shinyama and Sekine (2006) generated basic patterns using parts of text syntactically connected to the entity and then 84 generated a basic cluster composed of a set of events having the same relation. Several approaches have used linguistic analysis to generate features for supervised or unsupervised relation extraction (Nguyen et al., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia a</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics: 415-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning:</booktitle>
<pages>698--707</pages>
<contexts>
<context position="10105" citStr="Kazama and Torisawa (2007)" startWordPosition="1620" endWordPosition="1623">n normalized by the number of linked persons to assign a slot score. Figure 2. The Wikipedia infobox for the Michael_Jackson article has a number of slots from appropriate infobox templates. to categorize them. The article on Michael Jordan, for example, has 36 categories associated with it. In order to select an individual WordNet synset as a label for the concept’s type, we use two heuristics: • Category label extraction. Since the first sentence in Wikipedia articles usually defines the concept, we extract a category label from the first sentence using patterns based on POS tags similar to Kazama and Torisawa (2007). • Assign matching WordNet synset. We consider all the WordNet synsets associated with the categories of the article using the category to WordNet mapping (Ponzetto and Navigli, 2009) and assign the WordNet synset if any of the words in the synset matches with the extracted category label. We repeat the process with hypernyms and hyponyms of the synset up to three levels. 2.2 Slot Ranking All slots discovered using outgoing links might not be meaningful, therefore we have developed techniques for ranking and selecting slots. Our approach is based on the observation that entities of 80 2.3 Ent</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: 698–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of the TAC</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Text Analysis Conference. National Institute of Standards and Technology,</booktitle>
<contexts>
<context position="1480" citStr="McNamee and Dang, 2009" startWordPosition="219" endWordPosition="222">experiments have focused on analyzing people in Wikipedia, but the techniques can be directly applied to other types of entities in text resources that are rich with hyperlinks. 1 Introduction One of the biggest challenges faced by the Semantic Web vision is the availability of structured data that can be published as RDF. One approach is to develop techniques to translate information in spreadsheets, databases, XML documents and other traditional data formats into RDF (Syed et al. 2010). Another is to refine the technology needed to extract structured information from unstructured free text (McNamee and Dang, 2009). For both approaches, there is a second problem that must be addressed: do we start with an ontology or small catalog of ontologies that will be used to encode the data or is extracting the right ontology part of the problem. We describe exploratory work on a system that can discover ontological elements as well as data from a free text with embedded hyperlinks. 78 Wikipedia is a remarkable and rich online encyclopedia with a wealth of general knowledge about varied concepts, entities, events and facts in the world. Its size and coverage make it a valuable resource for extracting information </context>
</contexts>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Trang Dang. 2009. Overview of the TAC 2009 knowledge base population track. In Proceedings of the 2009 Text Analysis Conference. National Institute of Standards and Technology, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM Conference on Information and Knowledge Management:</booktitle>
<pages>233--242</pages>
<contexts>
<context position="3301" citStr="Mihalcea and Csomai, 2007" startWordPosition="514" endWordPosition="517">ource of structured data, the free text of the article contains much more information about the entity. Barker et al. (2007) unified the state of the art approaches in natural language processing and knowledge representation in their prototype system for understanding free text. Text resources which are rich in hyperlinks especially to knowledge based resources (such as encyclopedias or dictionaries) have additional information encoded in the form of links, which can be used to complement the existing systems for text understanding and knowledge discovery. Furthermore, systems such as Wikify (Mihalcea and Csomai, 2007) can be employed to link words in free text to knowledge resources like Wikipedia and thus enrich the free text with hyperlinks. We describe an approach for unsupervised ontology discovery from links in the free text of the Wikipedia articles, without specifying a relation or set of relations in advance. We first identify candidate slots and fillers for an entity, then classify enProceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 78–86, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics t</context>
<context position="30403" citStr="Mihalcea and Csomai, 2007" startWordPosition="5018" endWordPosition="5021"> and also a scheme to rank the slots based on linked entities of the same type. We compared our results with ground truth from the DBpedia infobox ontology and Freebase for the set of properties that were common and manually evaluated the accuracy of the common properties. Our results show that there are several properties that can be discovered with high accuracy from the link structure in Wikipedia and can also be used to discover a class hierarchy. We plan to explore the discovery of slots from non-Wikipedia articles by linking them to Wikipedia concepts using existing systems like Wikify (Mihalcea and Csomai, 2007). Wikipedia articles are encyclopedic in nature with the whole article revolving around a single topic or concept. Consequently, linked articles are a good source of properties and relations. This might not be the case in other genres, such as news articles, that discuss a number of different entities and events. One way to extend this work to other genres is by first detecting the entities in the article and then only processing links in sentences that mention an entity to discover its properties. Acknowledgements The research described in this paper was supported in part by a Fulbright fello</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In Proceedings of the 16th ACM Conference on Information and Knowledge Management: 233–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--235</pages>
<contexts>
<context position="5044" citStr="Miller et al., 1990" startWordPosition="793" endWordPosition="797">ht be particularly useful in “slot fillings” tasks like the one in the Knowledge Base Population track (McNamee and Dang, 2010) at the 2009 Text Analysis Conference. We see several contributions that this work has to offer: • Unsupervised and unrestricted ontology discovery. We describe an automatic approach that does not require a predefined list of relations or training data. The analysis uses inter-article links in the text and does not depend on existing infoboxes, enabling it to suggest slots and fillers that do not exist in any extant infoboxes. • Meaningful slot labels. We use WordNet (Miller et al., 1990) nodes to represent and label slots enabling us to exploit WordNet’s hypernym and hyponym relations as a property hierarchy. • Entity classification and class labeling. We introduce a new feature set for entity classification, i.e. the discovered ranked slots, which performs better than other feature sets extracted from Wikipedia. We also present an approach for assigning meaningful class label vectors using WordNet nodes. • Deriving a class hierarchy. We have developed an approach for deriving a class hierarchy based on the ranked slot similarity between classes and the label vectors. In the </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3:235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dat P T Nguyen</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Subtree mining for relation extraction from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:125–128.</booktitle>
<contexts>
<context position="27254" citStr="Nguyen et al., 2007" startWordPosition="4502" endWordPosition="4505">s one level higher in the hierarchy. 5 Related Work Unsupervised relation discovery was initially introduced by Hasegawa et al. (2004). They developed an approach to discover relations by clustering pairs of entities based on intervening words represented as context vectors. Shinyama and Sekine (2006) generated basic patterns using parts of text syntactically connected to the entity and then 84 generated a basic cluster composed of a set of events having the same relation. Several approaches have used linguistic analysis to generate features for supervised or unsupervised relation extraction (Nguyen et al., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia and demonstrates that there are several relations that can be discovered with high accuracy without the need of features generated from a linguistic analysis of the Wikipedia article text. Suchanek et al. (2008) used Wikipedia categories and infoboxes to extract 92 relations by applying specialized heuristics for each relation and incorporated the relations in their YAGO ontology, whereas our techniques do not use specialized heuristics based on the type of relation. Kylin (Weld et</context>
</contexts>
<marker>Nguyen, Matsuo, Ishizuka, 2007</marker>
<rawString>Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Subtree mining for relation extraction from Wikipedia. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from Wikipedia and WordNet. Web Semantics,</title>
<date>2008</date>
<volume>6</volume>
<issue>3</issue>
<pages>217</pages>
<contexts>
<context position="14838" citStr="Suchanek et al. (2008)" startWordPosition="2407" endWordPosition="2410">s in the cluster. We compute a list of person types by taking all hyponyms under the corresponding person sense in WordNet. That list mostly contained the professions list for persons such as basketball player, president, bishop etc. To assign a WordNet type to a person in Wikipedia we matched the entries in the list to the words in the first sentence of the person article and assigned it the set of types that matched. For example, for Michael Jordan the matching types found were basketball_player, businessman and player. We assigned the most frequent sense to the matching word as followed by Suchanek et al. (2008) and Wu and Weld (2008), which works for majority of the cases. We then also add all the hypernyms of the matching types under the Person node. The vector for Michael Jordan has entries basketball_player, athlete, businessperson, person, contestant, businessman and player. After getting matching types and their hypernyms for all the members of the cluster, we score each type based on the number of times it occurs in its members normalized by the cluster size. For example for one of the clusters with 146 basketball players we got the following label vector: {player:0.97, contestant:0.97, athlet</context>
<context position="27579" citStr="Suchanek et al. (2008)" startWordPosition="4556" endWordPosition="4559">terns using parts of text syntactically connected to the entity and then 84 generated a basic cluster composed of a set of events having the same relation. Several approaches have used linguistic analysis to generate features for supervised or unsupervised relation extraction (Nguyen et al., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia and demonstrates that there are several relations that can be discovered with high accuracy without the need of features generated from a linguistic analysis of the Wikipedia article text. Suchanek et al. (2008) used Wikipedia categories and infoboxes to extract 92 relations by applying specialized heuristics for each relation and incorporated the relations in their YAGO ontology, whereas our techniques do not use specialized heuristics based on the type of relation. Kylin (Weld et al., 2008) generated infoboxes for articles by learning from existing infoboxes, whereas we can discover new fillers for several existing slots and also discover new slots for infoboxes. KOG (Wu and Weld, 2008) automatically refined the Wikipedia infobox ontology and integrated Wikipedia’s infobox-class schemata with WordN</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from Wikipedia and WordNet. Web Semantics, 6(3):203– 217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zareen Syed</author>
<author>Tim Finin</author>
<author>Varish Mulwad</author>
<author>Anupam Joshi</author>
</authors>
<title>Exploiting a Web of Semantic Data for Interpreting Tables,</title>
<date>2010</date>
<booktitle>Proceedings of the Second Web Science Conference.</booktitle>
<contexts>
<context position="1349" citStr="Syed et al. 2010" startWordPosition="200" endWordPosition="203">acy using little or no linguistic analysis. The discovered properties can be further used to discover a class hierarchy. Our experiments have focused on analyzing people in Wikipedia, but the techniques can be directly applied to other types of entities in text resources that are rich with hyperlinks. 1 Introduction One of the biggest challenges faced by the Semantic Web vision is the availability of structured data that can be published as RDF. One approach is to develop techniques to translate information in spreadsheets, databases, XML documents and other traditional data formats into RDF (Syed et al. 2010). Another is to refine the technology needed to extract structured information from unstructured free text (McNamee and Dang, 2009). For both approaches, there is a second problem that must be addressed: do we start with an ontology or small catalog of ontologies that will be used to encode the data or is extracting the right ontology part of the problem. We describe exploratory work on a system that can discover ontological elements as well as data from a free text with embedded hyperlinks. 78 Wikipedia is a remarkable and rich online encyclopedia with a wealth of general knowledge about vari</context>
</contexts>
<marker>Syed, Finin, Mulwad, Joshi, 2010</marker>
<rawString>Zareen Syed, Tim Finin, Varish Mulwad and Anupam Joshi. 2010. Exploiting a Web of Semantic Data for Interpreting Tables, Proceedings of the Second Web Science Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone P Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Largescale taxonomy mapping for restructuring and integrating Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the TwentyFirst International Joint Conference on Artificial Intelligence:</booktitle>
<pages>2083--2088</pages>
<contexts>
<context position="7544" citStr="Ponzetto and Navigli, 2009" startWordPosition="1194" endWordPosition="1197"> fillers such as Motown, Epic and Legacy which are all Record Label Companies. Based on this observation, we discover and exploit “isa” relations between fillers (linked concepts) and WordNet nodes to serve as candidate slot labels. In order to find an “isa” relation between a concept and a WordNet synset we use manually created mappings by DBpedia, which links about 467,000 articles to synsets. However, Wikipedia has more than two million articles1, therefore, to map any remaining concepts we use the automatically generated mappings available between WordNet synsets and Wikipedia categories (Ponzetto and Navigli, 2009). A single Wikipedia article might have multiple categories associated with it and therefore multiple WordNet synsets. Wikipedia’s category system serves more as a way to tag articles and facilitate navigation rather than 1 This estimate is for the English version and does not include redirects and administrative pages such as disambiguation pages. the same type have common slots. For example, there is a set of slots common for musical artists whereas, a different set is common for basketball players. The Wikipedia infobox templates based on classes also provide a set of properties or slots to</context>
<context position="10289" citStr="Ponzetto and Navigli, 2009" startWordPosition="1649" endWordPosition="1652">plates. to categorize them. The article on Michael Jordan, for example, has 36 categories associated with it. In order to select an individual WordNet synset as a label for the concept’s type, we use two heuristics: • Category label extraction. Since the first sentence in Wikipedia articles usually defines the concept, we extract a category label from the first sentence using patterns based on POS tags similar to Kazama and Torisawa (2007). • Assign matching WordNet synset. We consider all the WordNet synsets associated with the categories of the article using the category to WordNet mapping (Ponzetto and Navigli, 2009) and assign the WordNet synset if any of the words in the synset matches with the extracted category label. We repeat the process with hypernyms and hyponyms of the synset up to three levels. 2.2 Slot Ranking All slots discovered using outgoing links might not be meaningful, therefore we have developed techniques for ranking and selecting slots. Our approach is based on the observation that entities of 80 2.3 Entity Classification and Slot Re-Ranking The ranked candidate slots are used to classify entities and then further ranked based on number of times they appear among the entities in the c</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>Simone P. Ponzetto and Roberto Navigli. 2009. Largescale taxonomy mapping for restructuring and integrating Wikipedia. In Proceedings of the TwentyFirst International Joint Conference on Artificial Intelligence: 2083–2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Pre-emptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:.</booktitle>
<contexts>
<context position="26937" citStr="Shinyama and Sekine (2006)" startWordPosition="4452" endWordPosition="4456">der all slots for class members (without slot selection) and recursively propagate the common slots present at any level to the level above it. For example, if we find the slot team to be common for different types of Athletes such as basketball players, soccer players etc. we can propagate it to the Athlete class, which is one level higher in the hierarchy. 5 Related Work Unsupervised relation discovery was initially introduced by Hasegawa et al. (2004). They developed an approach to discover relations by clustering pairs of entities based on intervening words represented as context vectors. Shinyama and Sekine (2006) generated basic patterns using parts of text syntactically connected to the entity and then 84 generated a basic cluster composed of a set of events having the same relation. Several approaches have used linguistic analysis to generate features for supervised or unsupervised relation extraction (Nguyen et al., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia and demonstrates that there are several relations that can be discovered with high accuracy without the need of features generated from a linguistic analysis of the Wiki</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Pre-emptive information extraction using unrestricted relation discovery. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Weld</author>
<author>Raphael Hoffmann</author>
<author>Fei Wu</author>
</authors>
<title>Using Wikipedia to bootstrap open information extrac-tion.</title>
<date>2008</date>
<journal>SIGMOD Record,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>62--68</pages>
<contexts>
<context position="27865" citStr="Weld et al., 2008" startWordPosition="4603" endWordPosition="4606">., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia and demonstrates that there are several relations that can be discovered with high accuracy without the need of features generated from a linguistic analysis of the Wikipedia article text. Suchanek et al. (2008) used Wikipedia categories and infoboxes to extract 92 relations by applying specialized heuristics for each relation and incorporated the relations in their YAGO ontology, whereas our techniques do not use specialized heuristics based on the type of relation. Kylin (Weld et al., 2008) generated infoboxes for articles by learning from existing infoboxes, whereas we can discover new fillers for several existing slots and also discover new slots for infoboxes. KOG (Wu and Weld, 2008) automatically refined the Wikipedia infobox ontology and integrated Wikipedia’s infobox-class schemata with WordNet. Since we already use the WordNet nodes for representing slots, it eliminates the need for several of KOG’s infobox refinement steps. While YAGO, Kylin and KOG all rely on relations present in the infoboxes, our approach can complement these by discovering new relations evident in i</context>
</contexts>
<marker>Weld, Hoffmann, Wu, 2008</marker>
<rawString>Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008. Using Wikipedia to bootstrap open information extrac-tion. SIGMOD Record, 37(4): 62–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically refining the Wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International World Wide Web Conference,</booktitle>
<pages>635--644</pages>
<contexts>
<context position="14861" citStr="Wu and Weld (2008)" startWordPosition="2412" endWordPosition="2415">e a list of person types by taking all hyponyms under the corresponding person sense in WordNet. That list mostly contained the professions list for persons such as basketball player, president, bishop etc. To assign a WordNet type to a person in Wikipedia we matched the entries in the list to the words in the first sentence of the person article and assigned it the set of types that matched. For example, for Michael Jordan the matching types found were basketball_player, businessman and player. We assigned the most frequent sense to the matching word as followed by Suchanek et al. (2008) and Wu and Weld (2008), which works for majority of the cases. We then also add all the hypernyms of the matching types under the Person node. The vector for Michael Jordan has entries basketball_player, athlete, businessperson, person, contestant, businessman and player. After getting matching types and their hypernyms for all the members of the cluster, we score each type based on the number of times it occurs in its members normalized by the cluster size. For example for one of the clusters with 146 basketball players we got the following label vector: {player:0.97, contestant:0.97, athlete:0.96, basketball_play</context>
<context position="28065" citStr="Wu and Weld, 2008" startWordPosition="4634" endWordPosition="4637">igh accuracy without the need of features generated from a linguistic analysis of the Wikipedia article text. Suchanek et al. (2008) used Wikipedia categories and infoboxes to extract 92 relations by applying specialized heuristics for each relation and incorporated the relations in their YAGO ontology, whereas our techniques do not use specialized heuristics based on the type of relation. Kylin (Weld et al., 2008) generated infoboxes for articles by learning from existing infoboxes, whereas we can discover new fillers for several existing slots and also discover new slots for infoboxes. KOG (Wu and Weld, 2008) automatically refined the Wikipedia infobox ontology and integrated Wikipedia’s infobox-class schemata with WordNet. Since we already use the WordNet nodes for representing slots, it eliminates the need for several of KOG’s infobox refinement steps. While YAGO, Kylin and KOG all rely on relations present in the infoboxes, our approach can complement these by discovering new relations evident in inter-article links in Wikipedia. For example, we could add slots like songs and albums to the infobox schema for Musical Artists, movies for the Actors infobox schema, and party for the Politicians sc</context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>Fei Wu and Daniel S. Weld. 2008. Automatically refining the Wikipedia infobox ontology. In Proceedings of the 17th International World Wide Web Conference, pages 635–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<title>Wikipedia, the free encyclopedia.</title>
<date>2008</date>
<marker>Wikipedia, 2008</marker>
<rawString>Wikipedia. 2008. Wikipedia, the free encyclopedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan Yan</author>
<author>Naoaki Okazaki</author>
<author>Yutaka Matsuo</author>
<author>Zhenglu Yang</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Unsupervised relation extraction by mining Wikipedia texts using information from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics: Volume</booktitle>
<volume>2</volume>
<pages>1021--1029</pages>
<contexts>
<context position="27295" citStr="Yan et al., 2009" startWordPosition="4510" endWordPosition="4513">ted Work Unsupervised relation discovery was initially introduced by Hasegawa et al. (2004). They developed an approach to discover relations by clustering pairs of entities based on intervening words represented as context vectors. Shinyama and Sekine (2006) generated basic patterns using parts of text syntactically connected to the entity and then 84 generated a basic cluster composed of a set of events having the same relation. Several approaches have used linguistic analysis to generate features for supervised or unsupervised relation extraction (Nguyen et al., 2007; Etzioni et al., 2008; Yan et al., 2009). Our approach mainly exploits the heavily linked structure of Wikipedia and demonstrates that there are several relations that can be discovered with high accuracy without the need of features generated from a linguistic analysis of the Wikipedia article text. Suchanek et al. (2008) used Wikipedia categories and infoboxes to extract 92 relations by applying specialized heuristics for each relation and incorporated the relations in their YAGO ontology, whereas our techniques do not use specialized heuristics based on the type of relation. Kylin (Weld et al., 2008) generated infoboxes for artic</context>
</contexts>
<marker>Yan, Okazaki, Matsuo, Yang, Ishizuka, 2009</marker>
<rawString>Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang, and Mitsuru Ishizuka. 2009. Unsupervised relation extraction by mining Wikipedia texts using information from the web. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics: Volume 2: 1021–1029.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>