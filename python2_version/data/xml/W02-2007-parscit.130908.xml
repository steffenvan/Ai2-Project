<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012227">
<title confidence="0.997557">
Language Independent NER using a Unified Model of Internal and
Contextual Evidence
</title>
<author confidence="0.993905">
Silviu Cucerzan and David Yarowsky
</author>
<affiliation confidence="0.929137">
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.730666">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999322">
{silviu,yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975625">
This paper investigates the use of a language inde-
pendent model for named entity recognition based
on iterative learning in a co-training fashion, using
word-internal and contextual information as inde-
pendent evidence sources. Its bootstrapping pro-
cess begins with only seed entities and seed con-
texts extracted from the provided annotated corpus.
F-measure exceeds 77 in Spanish and 72 in Dutch.
</bodyText>
<sectionHeader confidence="0.995154" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999948476190476">
Our aim has been to build a maximally language-
independent system for named-entity recognition
using minimal supervision or knowledge of the
source language. The core model utilized, ex-
tended and evaluated here is based on Cucerzan and
Yarowsky (1999). It assumes that only an entity ex-
emplar list is provided as a bootstrapping seed set.
For the particular task of CoNLL-2002, the seed
entities are extracted from the provided annotated
corpus. As a consequence, the seed examples may
be ambiguous and the system must therefore han-
dle seeds with probability distribution over entity
classes rather than unambiguous seeds. Another
consequence is that this approach of extracting only
the entity seeds from the annotated text does not use
the full potential of the training data, ignoring con-
textual information. For example, Bosnia appears
labeled 9 times as LOC and 5 times as ORG and
the only information that would be used is that the
word Bosnia denotes a location 64% of the time,
and an organization 36% of the time, but not in
which contexts is labeled one way or the other. In
order to correct this problem, an improved system
also uses context seeds if available (for this particu-
lar task, they are extracted from the annotated cor-
pus). Because the representations of entity candi-
dates and contexts are identical, this modification
imposes only minor changes in algorithm and code.
Because the core model has been presented in de-
tail in Cucerzan and Yarowsky (1999), this paper
focuses primarily on the modifications of the algo-
rithm and its adaptation to the current task. The ma-
jor modifications besides the seed handling include
a different method of smoothing the distributions
along the paths in the tries, a new ’soft’ discourse
segmentation method, and use of a different label-
ing methodology, as required by the current task i.e.
no overlapping entities are allowed (for example,
the correct labeling of colegio San Juan Bosco de
Mérida is considered to be ORG(colegio San Juan
Bosco) de LOC(Mérida) rather than ORG(colegio
PER(San Juan Bosco) de LOC(Mérida))).
</bodyText>
<sectionHeader confidence="0.98953" genericHeader="introduction">
2. Entity-Internal Information
</sectionHeader>
<bodyText confidence="0.999592090909091">
Two types of entity-internal evidence are used in a
unified framework. The first consists of the pre-
fixes and suffixes of candidate entities. For exam-
ple, in Spanish, names ending in -ez (e.g. Alvarez
and Gutiérrez) are often surnames; names ending in
-ia are often locations (e.g. Austria, Australia, and
Italia). Likewise, common beginnings and endings
of multiword entities (e.g. Asociación de la Prensa
de Madrid and Asociación para el Desarrollo Rural
Jerez-Sierra Suroeste, which are both organizations)
are good indicators for entity type.
</bodyText>
<sectionHeader confidence="0.929265" genericHeader="method">
3. Contextual Information
</sectionHeader>
<bodyText confidence="0.9997941">
An entity’s left and right context provides an essen-
tially independent evidence source for model boot-
strapping. This information is also important for en-
tities that do not have a previously seen word struc-
ture, are of foreign origin, or polysemous. Rather
than using word bigrams or trigrams, the system
handles the context in the same way it handles the
entities, allowing for variable-length contexts. The
advantages of this unified approach are presented in
the next paragraph.
</bodyText>
<sectionHeader confidence="0.9357905" genericHeader="method">
4. A Unified Structure for both Internal and
Contextual Information
</sectionHeader>
<bodyText confidence="0.957106">
Character-based tries provide an effective, efficient
and flexible data structure for storing both con-
textual and morphological patterns and statistics.
... organizada por la Concejalía de Cultura , tienen un ...
</bodyText>
<figureCaption confidence="0.749642">
Figure 1: An example of entity candidate and context and
the way the information is introduced in the four tries (arrows
indicate the direction letters are considered)
</figureCaption>
<bodyText confidence="0.999983451612903">
They are very compact representations and support
a natural hierarchical smoothing procedure for dis-
tributional class statistics. In our implementation,
each terminal or branching node contains a prob-
ability distribution which encodes the conditional
probability of entity classes given the sistring cor-
responding to the path from the root to that node.
Each such distribution also has two standard classes,
named “questionable” (unassigned probability mass
in terms of entity classes, to be motivated below)
and “non-entity” (common words).
Two tries (denoted PT and ST) are used for in-
ternal representation of the entity candidates in pre-
fix, respectively suffix form, respectively. Other two
tries are used for left (LCT) and right (RCT) con-
text. Right contexts are introduced in RCT by con-
sidering their component letters from left to right,
left contexts are introduced in LCT using the re-
versed order of letters, from right to left (Figure 1).
In this way, the system handles variable length con-
texts and it attempts to match in each instance the
longest known context (as longer contexts are more
reliable than short contexts, and also the longer con-
text statistics incorporate the shorter context statis-
tics through smoothing along the paths in the tries).
The tries are linked together into two bipartite
structures, PT with LCT, and ST with RCT, by at-
taching to each node a list of links to the entity can-
didates or contexts with, respectively in which the
sistring corresponding to that node has been seen in
the text (Figure 2).
</bodyText>
<sectionHeader confidence="0.960925" genericHeader="method">
5. Unassigned Probability Mass
</sectionHeader>
<bodyText confidence="0.999974461538461">
When faced with a highly skewed observed class
distribution for which there is little confidence due
to small sample size, a typical response is to back-
off or smooth to the more general class distribution.
Unfortunately, this representation makes problem-
atic the distinction between a back-off conditional
distribution and one based on a large sample (and
hence estimated with confidence). We address this
problem by explicitly representing the uncertainty
as a class, called &amp;quot;questionable&amp;quot;. Probability mass
continues to be distributed among the primary en-
tity classes proportional to the observed distribu-
tion in the data, but with a total sum that reflects
</bodyText>
<figureCaption confidence="0.7777745">
Figure 2: An example of links between the Suffix Trie and the
Right Context Trie for the entity candidate Austria and some of
its right contexts as observed in the corpus (&lt; , Holanda &gt;,
&lt; , hizo &gt;, &lt; a Chirac &gt;)
</figureCaption>
<bodyText confidence="0.9996176">
the confidence in the distribution and is equal to
.
Incremental learning essentially becomes the pro-
cess of gradually shifting probability mass from
questionable to one of the primary classes.
</bodyText>
<sectionHeader confidence="0.96069" genericHeader="method">
6. Smoothing
</sectionHeader>
<bodyText confidence="0.998664">
The probability of an entity candidate or context as
being or indicating a certain type of entity is com-
puted along the path from the root to the node in
the trie structure described above. In this way, ef-
fective smoothing can be realized for rare entities
or contexts. A smoothing formula taking advantage
of the distributional representation of uncertainty is
presented below.
For a sistring (i.e. the path in the trie is
) the general smoothing model
for the conditional class probabilities is given by the
recursive formula:
</bodyText>
<equation confidence="0.890268">
(1)
</equation>
<bodyText confidence="0.984574">
where is a normalization factor and
</bodyText>
<listItem confidence="0.768574">
are model parameters.
7. One Sense per Discourse
</listItem>
<bodyText confidence="0.996207166666667">
Clearly, in many cases, the context for only one
instance of an entity and the word-internal infor-
mation is not enough to make a classification de-
cision. But, as noted by Katz (1996), a newly in-
troduced entity will be repeated, “if not for break-
ing the monotonous effect of pronoun use, then for
</bodyText>
<figure confidence="0.999081535714286">
LEFT CONTEXT
PREFIX RIGHT CONTEXT
SUFFIX
...
ST
A...a b ...z
...
...
a
,
u
t
o
a
n
r
#
A
a
#
i
...
c
d
...
.
#
...
.
a
.
t
s
...
... ...
...
#
#
#
H
l
o
i
z
h
p
a
r
C
h
i
...
i
r
RCT
Entity candidate w
</figure>
<figureCaption confidence="0.9918172">
Figure 3: Using contextual clues from all instances of an en-
tity candidate in the corpus. Each instance is depicted as a disc
with the diameter representing the confidence of the classifica-
tion of that instance using word-internal and local contextual
information.
</figureCaption>
<bodyText confidence="0.999855636363637">
emphasis and clarity”. We use this property in con-
junction with the one sense per discourse tendency
noted by Gale et al. (1992). The later paradigm is
not directly usable when analyzing a large corpus
in which there are no document boundaries, like the
one provided for Spanish. Therefore, a segmenta-
tion process needs to be employed, so that all the
instances of a name in a segment have a high proba-
bility of belonging to the same class. Our approach
is to consider a ’soft’ segmentation, which is word-
dependent and does not compute topic/document
boundaries but regions for which the contextual in-
formation for all instances of a word can be used
jointly when making a decision. This is viewed
as an alternative to the classical topic segmenta-
tion approach and can be used in conjunction with a
language-independent segmentation system (Figure
3) like the one presented by Richmond et al. (1997).
After estimating the class probability distribu-
tions for all instances of entity candidates in the cor-
pus, a re-estimation step is employed. The probabil-
ity of an entity class given an entity candidate
</bodyText>
<equation confidence="0.5147225">
at position is re-computed using the formula:
(2)
</equation>
<bodyText confidence="0.979057461538462">
where are the positions of all in-
stances of in the corpus, is the positional
similarity, encoding the physical distance and topic
(if topic or document boundary information exists),
conf is the classification confidence of each instance
(inverse proportional to the the ,
is a normalization factor.
8. Entity Identification / Multiple-Word Entities
There are two major alternatives for handling
multiple-word entities. A first approach is to tok-
enize the text and classify each individual word as
being or not part of an entity, process followed by an
entity assemblance algorithm. A second alternative
</bodyText>
<figureCaption confidence="0.979771">
Figure 4: The structure of an entity candidate represented as
an automaton with two final states
</figureCaption>
<bodyText confidence="0.999959676470588">
is to consider a chunking algorithm that identifies
entity candidates and classify each of the chunks as
Person, Location, Organization, Miscellaneous, or
Non-entity. We use this second alternative, but in a
’soft’ form; i.e. each word can be included in multi-
ple competing chunks (entity candidates). This ap-
proach is suitable for all languages including Chi-
nese, where no word separators are used (the en-
tity candidates are determined by specifying starting
and ending character positions). Another advantage
of this method is that single and multiple-word en-
tities can be handled in the same way.
The boundaries of entity candidates are deter-
mined by a few simple rules incorporated into three
discriminators: is_B_candidate tests if a word can
represent the beginning of an entity, is_I_candidate
tests if a word can be the end of an entity, and
is_E_candidate tests if a word can be an internal
part of an entity. These discriminators use simple
heuristics based on capitalization, position in sen-
tence, length of the word, usage of the word in
the set of seed entities, and co-occurrence with un-
capitalized instances of the same word. A string is
considered an entity candidate if it has the structure
shown in Figure 4.
An extension of the system also makes use of
Part-of-Speech (POS) tags. We used the pro-
vided POS annotation in Dutch (Daelemans et al.,
1996) and a minimally supervised tagger (Yarowsky
and Cucerzan, 2002) for Spanish to restrict the
space of words accepted by the discriminators (e.g.
is_B_candidate rejects prepositions, conjunctions,
pronouns, adverbs, and those determiners that are
the first word in the sentence).
</bodyText>
<sectionHeader confidence="0.670722" genericHeader="method">
9. Algorithm Structure
</sectionHeader>
<bodyText confidence="0.999879">
The core algorithm can be divided into eight stages,
which are summarized in Figure 5. The bootstrap-
ping stage (5) uses the initial or current entity as-
signments to estimate the class conditional distribu-
tions for both entities and contexts along their trie
paths, and then re-estimates the distributions of the
contexts/entity-candidates to which they are linked,
recursively, until all accessible nodes are reached,
as presented in Cucerzan and Yarowsky (1999).
</bodyText>
<figure confidence="0.999134368421053">
Soft boundary
Topic boundary Topic boundary Soft boundary
Topic boundary Topic boundary Topic boundary
Positional similarity
Other occurences
of w
Word position in the corpus
is_B_candidate
is_I_candidate
is_E_candidate
1 Extract the entity (and context) seed sets from the annotated data
2 Read the text to be annotated and extract all entity-candidates
3 Extract the sets LC and RC of all contexts of entity candidates
4 Build the tries using all individual words and entity candidates,
and all instances of the elements LC and RC from the text
5 Apply the bootstrapping procedure using the seed data
6 Classify each entity-candidate in isolation
7 Re-classify each entity-candidate by using formula (2)
8 Resolve conflicts between competing entity candidates
</figure>
<figureCaption confidence="0.999823">
Figure 5: Algorithm structure
</figureCaption>
<sectionHeader confidence="0.995693" genericHeader="evaluation">
10. Results
</sectionHeader>
<bodyText confidence="0.999729384615385">
We compare the results of two variants of the de-
scribed model on the development and test sets pro-
vided (Table 1). The first one uses only exemplar
entity and context seeds extracted from the training
corpus. The second also employs POS information
to rule out unlikely entity candidates.
The system was built and tested initially utiliz-
ing only the provided Spanish data. The parame-
ters were estimated using an 80/20 split of the train-
ing data (esp.train and ned.train). The dev-test data
(testa) were not used during the parameter estima-
tion phase. The programs were run once on the final
test data (files testb). We allocated only one person-
day to adapt the system for Dutch and tune the pa-
rameters to this language in order to show functional
language independence. We opted not to make a
detailed study of parameter variation on test data to
avoid any potential for tuning to this resource and
preserve its value for future system development.
The following table further details the types of
errors made by the algorithm (full system on Span-
ish dev-set). represents the number of over-
generated and under-generated entities in the pre-
cision and recall rows (respectively). repre-
sents the number of entities with correctly identified
boundaries, but wrong classifications.
</bodyText>
<table confidence="0.996330666666667">
PER LOC ORG MISC
Precision 43 + 153 73+118 87+341 76+170
Recall 43+123 34+310 112+279 22+70
</table>
<bodyText confidence="0.99974275">
Because our system takes seed lists rather than
annotated text as input, additional entity lists can
be used by the system. By employing such lists
of countries, major cities, frequent person names
and major companies (extracted from the web), sig-
nificant improvements can be obtained (preliminary
tests show as much as 2.5 F-measure improvement
on a 80/20 split of the training data in Dutch).
</bodyText>
<sectionHeader confidence="0.934256" genericHeader="conclusions">
11. Conclusion
</sectionHeader>
<bodyText confidence="0.91968075">
This paper has presented and evaluated an ex-
tended bootstrapping model based on Cucerzan and
Yarowsky (1999) that uses a unified framework of
both entity internal and contextual evidence. Start-
</bodyText>
<table confidence="0.99922134375">
Spanish without POS information with POS information
Dev-set
Precision Recall F-meas. Precision Recall F-meas.
LOC 69.11 80.41 74.33 69.77 80.61 74.80
MISC 66.89 44.49 53.44 68.38 44.72 54.08
ORG 73.26 71.71 72.47 76.49 74.82 75.65
PER 85.39 83.22 84.29 86.07 83.96 85.00
Overall 75.08 74.13 74.60 78.82 75.62 76.22
Spanish without POS information with POS information
Test
Precision Recall F-meas. Precision Recall F-meas.
LOC 78.62 73.62 76.04 79.66 73.34 76.37
MISC 63.73 38.24 47.79 64.22 38.53 48.16
ORG 74.86 78.50 76.64 76.79 81.07 78.87
PER 80.63 87.21 83.79 82.57 88.30 85.34
Overall 76.62 74.96 75.78 78.19 76.14 77.15
Dutch without POS information with POS information
Dev-set
Precision Recall F-meas. Precision Recall F-meas.
LOC 73.30 70.38 71.81 76.87 73.32 75.05
MISC 64.08 57.64 60.69 68.16 63.14 65.55
ORG 67.34 53.76 59.79 70.63 55.96 62.45
PER 63.17 79.94 70.57 64.99 80.51 71.92
Overall 66.10 65.01 65.55 69.14 67.84 68.49
Dutch without POS information with POS information
Test
Precision Recall F-meas. Precision Recall F-meas.
LOC 73.65 77.56 75.55 77.72 80.54 79.11
MISC 70.10 57.29 63.05 74.67 62.34 67.95
ORG 69.78 62.14 65.74 72.12 64.88 68.31
PER 67.62 79.26 72.98 69.39 80.71 74.62
Overall 69.95 68.49 69.21 73.03 71.62 72.31
</table>
<tableCaption confidence="0.959853">
Table 1: Results on the development sets (files esp.testa and
ned.testa) and on the test sets (files esp.testb and ned.testb)
</tableCaption>
<bodyText confidence="0.99992025">
ing only with entity and context seeds extracted
from training data and the addition of part-of-speech
information, system performance exceeds 77 and 72
F-measure for Spanish and Dutch respectively.
</bodyText>
<sectionHeader confidence="0.944103" genericHeader="acknowledgments">
12. Acknowledgements
</sectionHeader>
<bodyText confidence="0.9935135">
This work was supported by NSF grant IIS-9985033
and ONR/MURI contract N00014-01-1-0685.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999723789473684">
S. Cucerzan and D. Yarowsky. 1999. Language independent
named entity recognition combining morphological and con-
textual evidence. In Proceedings of the Joint SIGDAT Confer-
ence on EMNLP and VLC 1999, pages 90–99.
S. Cucerzan and D. Yarowsky. 2002. Bootstrapping a multilin-
gual part-of-speech tagger in 1 person-day. In Proceedings of
CoNLL 2002
W. Daelemans, J. Zavrel, and S. Berck. 1996. Mbt: A memory-
based part of speech tagger-generator. In Proceedings of the
4th Workshop on Very Large Corpora, pages 14–27.
W. Gale, K. Church, and D. Yarowsky. 1992. One sense per dis-
course. In Proceedings of the 4th DARPA Speech and Natural
Language Workshop, pages 233–237.
S. M. Katz. 1996. Distribution of context words and phrases in
text and language modeling. Natural Language Engineering,
2(1):15–59.
K. Richmond, A. Smith, and E. Amitay. 1997. Detecting sub-
ject boundaries within text: a language independent statistical
approach. In Proceedings of EMNLP 1997, pages 47–54..
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.266929">
<title confidence="0.9778655">Language Independent NER using a Unified Model of Internal Contextual Evidence</title>
<author confidence="0.985879">Silviu Cucerzan</author>
<author confidence="0.985879">David</author>
<affiliation confidence="0.876566">Department of Computer Science Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.992625">Baltimore, MD 21218,</address>
<email confidence="0.999818">silviu@cs.jhu.edu</email>
<email confidence="0.999818">yarowsky@cs.jhu.edu</email>
<abstract confidence="0.991866">This paper investigates the use of a language independent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as independent evidence sources. Its bootstrapping process begins with only seed entities and seed contexts extracted from the provided annotated corpus.</abstract>
<note confidence="0.506294">F-measure exceeds 77 in Spanish and 72 in Dutch.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on EMNLP and VLC</booktitle>
<pages>90--99</pages>
<contexts>
<context position="946" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="135" endWordPosition="138">a language independent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as independent evidence sources. Its bootstrapping process begins with only seed entities and seed contexts extracted from the provided annotated corpus. F-measure exceeds 77 in Spanish and 72 in Dutch. 1. Introduction Our aim has been to build a maximally languageindependent system for named-entity recognition using minimal supervision or knowledge of the source language. The core model utilized, extended and evaluated here is based on Cucerzan and Yarowsky (1999). It assumes that only an entity exemplar list is provided as a bootstrapping seed set. For the particular task of CoNLL-2002, the seed entities are extracted from the provided annotated corpus. As a consequence, the seed examples may be ambiguous and the system must therefore handle seeds with probability distribution over entity classes rather than unambiguous seeds. Another consequence is that this approach of extracting only the entity seeds from the annotated text does not use the full potential of the training data, ignoring contextual information. For example, Bosnia appears labeled 9 t</context>
<context position="12326" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="2014" endWordPosition="2017">rs (e.g. is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). 9. Algorithm Structure The core algorithm can be divided into eight stages, which are summarized in Figure 5. The bootstrapping stage (5) uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached, as presented in Cucerzan and Yarowsky (1999). Soft boundary Topic boundary Topic boundary Soft boundary Topic boundary Topic boundary Topic boundary Positional similarity Other occurences of w Word position in the corpus is_B_candidate is_I_candidate is_E_candidate 1 Extract the entity (and context) seed sets from the annotated data 2 Read the text to be annotated and extract all entity-candidates 3 Extract the sets LC and RC of all contexts of entity candidates 4 Build the tries using all individual words and entity candidates, and all instances of the elements LC and RC from the text 5 Apply the bootstrapping procedure using the seed </context>
<context position="15019" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="2448" endWordPosition="2451"> classifications. PER LOC ORG MISC Precision 43 + 153 73+118 87+341 76+170 Recall 43+123 34+310 112+279 22+70 Because our system takes seed lists rather than annotated text as input, additional entity lists can be used by the system. By employing such lists of countries, major cities, frequent person names and major companies (extracted from the web), significant improvements can be obtained (preliminary tests show as much as 2.5 F-measure improvement on a 80/20 split of the training data in Dutch). 11. Conclusion This paper has presented and evaluated an extended bootstrapping model based on Cucerzan and Yarowsky (1999) that uses a unified framework of both entity internal and contextual evidence. StartSpanish without POS information with POS information Dev-set Precision Recall F-meas. Precision Recall F-meas. LOC 69.11 80.41 74.33 69.77 80.61 74.80 MISC 66.89 44.49 53.44 68.38 44.72 54.08 ORG 73.26 71.71 72.47 76.49 74.82 75.65 PER 85.39 83.22 84.29 86.07 83.96 85.00 Overall 75.08 74.13 74.60 78.82 75.62 76.22 Spanish without POS information with POS information Test Precision Recall F-meas. Precision Recall F-meas. LOC 78.62 73.62 76.04 79.66 73.34 76.37 MISC 63.73 38.24 47.79 64.22 38.53 48.16 ORG 74.86 </context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In Proceedings of the Joint SIGDAT Conference on EMNLP and VLC 1999, pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Bootstrapping a multilingual part-of-speech tagger in 1 person-day.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>S. Cucerzan and D. Yarowsky. 2002. Bootstrapping a multilingual part-of-speech tagger in 1 person-day. In Proceedings of CoNLL 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>S Berck</author>
</authors>
<title>Mbt: A memorybased part of speech tagger-generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<contexts>
<context position="11562" citStr="Daelemans et al., 1996" startWordPosition="1901" endWordPosition="1904">an represent the beginning of an entity, is_I_candidate tests if a word can be the end of an entity, and is_E_candidate tests if a word can be an internal part of an entity. These discriminators use simple heuristics based on capitalization, position in sentence, length of the word, usage of the word in the set of seed entities, and co-occurrence with uncapitalized instances of the same word. A string is considered an entity candidate if it has the structure shown in Figure 4. An extension of the system also makes use of Part-of-Speech (POS) tags. We used the provided POS annotation in Dutch (Daelemans et al., 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g. is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). 9. Algorithm Structure The core algorithm can be divided into eight stages, which are summarized in Figure 5. The bootstrapping stage (5) uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distrib</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, and S. Berck. 1996. Mbt: A memorybased part of speech tagger-generator. In Proceedings of the 4th Workshop on Very Large Corpora, pages 14–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th DARPA Speech and Natural Language Workshop,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="8469" citStr="Gale et al. (1992)" startWordPosition="1392" endWordPosition="1395"> pronoun use, then for LEFT CONTEXT PREFIX RIGHT CONTEXT SUFFIX ... ST A...a b ...z ... ... a , u t o a n r # A a # i ... c d ... . # ... . a . t s ... ... ... ... # # # H l o i z h p a r C h i ... i r RCT Entity candidate w Figure 3: Using contextual clues from all instances of an entity candidate in the corpus. Each instance is depicted as a disc with the diameter representing the confidence of the classification of that instance using word-internal and local contextual information. emphasis and clarity”. We use this property in conjunction with the one sense per discourse tendency noted by Gale et al. (1992). The later paradigm is not directly usable when analyzing a large corpus in which there are no document boundaries, like the one provided for Spanish. Therefore, a segmentation process needs to be employed, so that all the instances of a name in a segment have a high probability of belonging to the same class. Our approach is to consider a ’soft’ segmentation, which is worddependent and does not compute topic/document boundaries but regions for which the contextual information for all instances of a word can be used jointly when making a decision. This is viewed as an alternative to the class</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. One sense per discourse. In Proceedings of the 4th DARPA Speech and Natural Language Workshop, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Distribution of context words and phrases in text and language modeling.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7760" citStr="Katz (1996)" startWordPosition="1244" endWordPosition="1245"> In this way, effective smoothing can be realized for rare entities or contexts. A smoothing formula taking advantage of the distributional representation of uncertainty is presented below. For a sistring (i.e. the path in the trie is ) the general smoothing model for the conditional class probabilities is given by the recursive formula: (1) where is a normalization factor and are model parameters. 7. One Sense per Discourse Clearly, in many cases, the context for only one instance of an entity and the word-internal information is not enough to make a classification decision. But, as noted by Katz (1996), a newly introduced entity will be repeated, “if not for breaking the monotonous effect of pronoun use, then for LEFT CONTEXT PREFIX RIGHT CONTEXT SUFFIX ... ST A...a b ...z ... ... a , u t o a n r # A a # i ... c d ... . # ... . a . t s ... ... ... ... # # # H l o i z h p a r C h i ... i r RCT Entity candidate w Figure 3: Using contextual clues from all instances of an entity candidate in the corpus. Each instance is depicted as a disc with the diameter representing the confidence of the classification of that instance using word-internal and local contextual information. emphasis and clarit</context>
</contexts>
<marker>Katz, 1996</marker>
<rawString>S. M. Katz. 1996. Distribution of context words and phrases in text and language modeling. Natural Language Engineering, 2(1):15–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Richmond</author>
<author>A Smith</author>
<author>E Amitay</author>
</authors>
<title>Detecting subject boundaries within text: a language independent statistical approach.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>47--54</pages>
<contexts>
<context position="9240" citStr="Richmond et al. (1997)" startWordPosition="1523" endWordPosition="1526">h. Therefore, a segmentation process needs to be employed, so that all the instances of a name in a segment have a high probability of belonging to the same class. Our approach is to consider a ’soft’ segmentation, which is worddependent and does not compute topic/document boundaries but regions for which the contextual information for all instances of a word can be used jointly when making a decision. This is viewed as an alternative to the classical topic segmentation approach and can be used in conjunction with a language-independent segmentation system (Figure 3) like the one presented by Richmond et al. (1997). After estimating the class probability distributions for all instances of entity candidates in the corpus, a re-estimation step is employed. The probability of an entity class given an entity candidate at position is re-computed using the formula: (2) where are the positions of all instances of in the corpus, is the positional similarity, encoding the physical distance and topic (if topic or document boundary information exists), conf is the classification confidence of each instance (inverse proportional to the the , is a normalization factor. 8. Entity Identification / Multiple-Word Entiti</context>
</contexts>
<marker>Richmond, Smith, Amitay, 1997</marker>
<rawString>K. Richmond, A. Smith, and E. Amitay. 1997. Detecting subject boundaries within text: a language independent statistical approach. In Proceedings of EMNLP 1997, pages 47–54..</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>