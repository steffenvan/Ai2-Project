<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.944764">
ASSIGNING A SEMANTIC SCOPE TO OPERATORS
</title>
<author confidence="0.994153">
Massimo Poesio
</author>
<affiliation confidence="0.7528135">
University of Rochester, Department of Computer Science
Rochester, NY 14627-0226, USA
</affiliation>
<email confidence="0.998923">
poesio@cs.rochester.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998605">
I propose that the characteristics of the scope disambiguation
process observed in the literature can be explained in terms
of the way in which the model of the situation described
by a sentence is built. The model construction procedure I
present builds an event structure by identifying the situations
associated with the operators in the sentence and their mu-
tual dependency relations, as well as the relations between
these situations and other situations in the context. The pro-
cedure takes into account lexical semantics and the result of
various discourse interpretation procedures such as definite
description interpretation, and does not require a complete
disambiguation to take place.
</bodyText>
<sectionHeader confidence="0.847219" genericHeader="keywords">
THE PROBLEM
</sectionHeader>
<bodyText confidence="0.964431291666667">
Because new ways of obtaining semantically distinct inter-
pretations for sentences are continuously discovered, com-
ing to grips with ambiguity is becoming more and more
of a necessity for developers of natural language process-
ing systems, linguists and psychologists alike [9, 31, 7, 21.
In this paper, I am concerned with the scopal ambiguity of
operators1 [31, 33].
The attention of both psycholinguists and computational
linguists interested in ambiguity has concentrated on the
problem of combinatorial explosion. If the number of read-
ings of an utterance were to actually grow with the factorial
of the number of operators, even a simple sentence like (1),
with 4 operators (the modal &apos;should&apos;, tense, an indefinite and
a definite), would have 4! = 24 scopally different readings.
Two distinct questions thus must be answered: how can lis-
teners (and how should machines) deal with the combinato-
rial explosion of readings? Do we really use the brute-force
strategy of considering all of the available readings, and then
choose among them? And, if we do choose among several
readings, how is that done?
(1) We should hook up an engine to the boxcar.
To my knowledge, three positions on the problem of com-
binatorial explosion have been taken in the literature. Some
have argued that there is no problem: our brains contain
</bodyText>
<footnote confidence="0.944909">
1I use here the term operator as it is used by Heim [13], i.e., to
mean either quantifier or modal/tense operator.
</footnote>
<bodyText confidence="0.997547428571429">
more than enough machinery to process in parallel 4! in-
terpretations. It&apos;s unclear, however, whether this strategy
is feasible when larger numbers of readings are concerned.
A classical demonstration of the number of readings one
may have to consider is (2), which has 11! interpretations
if the standard treatment of quantification and modality is
assumed.
</bodyText>
<listItem confidence="0.772019666666667">
(2) You can fool most people on most of the issues most
of the time, but you can&apos;t fool everybody on every single
issue all of the time. [15]
</listItem>
<bodyText confidence="0.6639085">
Another position is that sentences like (1) are not semanti-
cally ambiguous, but vague. Consider for example (3):
</bodyText>
<listItem confidence="0.887864">
(3) Every kid climbed a tree.
</listItem>
<bodyText confidence="0.9996145">
Here, one of the readings (the one in which the indefinite
takes narrow scope) is entailed by the other (in which the
indefinite takes wide scope). The claim is that (3) is inter-
preted in the vaguest possible way, and the strongest reading,
if at all, is derived by pragmatic &apos;strengthening&apos; [25]. A dif-
ficulty with this approach is that a vaguest reading doesn&apos;t
always exist. The two readings of (4), for example, are dis-
tinct.
</bodyText>
<listItem confidence="0.94149">
(4) Few people speak many languages. [27]
</listItem>
<bodyText confidence="0.9598334375">
Finally, it has been proposed that the reason why listeners
do not seem to have problems in processing utterances like
(1) is because they do not disambiguate. They build a non-
disambiguated representation of the sentence and leave the
interpretation open. This strategy might be advantageous
for some kinds of applications&apos; and it has been argued that
a complete disambiguation never takes place [7].
No matter what processing strategy is chosen, the ques-
tion of how listeners choose one particular interpretation
cannot be ignored. All experimental work done on the sub-
ject of scopal ambiguity [20, 35, 26] indicates that subjects
do have preferred interpretations when confronted with tasks
which require understanding. In addition, sentences like (1),
(5) and (6) clearly have preferred interpretations. However,
the only answers to to this question that I have seen are based
on heuristics.3
</bodyText>
<footnote confidence="0.6764735">
2E.g., machine translation [2].
&apos;See [17] for an example of state-of-the-art techniques
</footnote>
<page confidence="0.994212">
78
</page>
<listItem confidence="0.988399">
(5) A girl took every chemistry course. [20]
(6) Each daughter of an admiral married a captain.
</listItem>
<bodyText confidence="0.9955881">
I present in this paper an hypothesis about interpretation that
accounts for facts about scope disambiguation that were pre-
viously explained in the literature by stipulating a number of
unmotivated principles. The proposal developed here is be-
ing applied to develop the module of the TRAINS-93 system
[1] that handles scope disambiguation and reference inter-
pretation. The goal of the TRAINS project is to develop a
conversationally proficient planning assistant. More details
about the project and the work presented here can be found
in [29].
</bodyText>
<sectionHeader confidence="0.993029" genericHeader="introduction">
SCOPE DISAMBIGUATION FACTORS
</sectionHeader>
<bodyText confidence="0.947249863636364">
Most proposals on scope disambiguation were developed to
account for the general preference of the leftmost quantified
phrase from taking wide scope in simple active sentences
like (7):
(7) Every kid climbed a tree.
Lakoff [27] proposed that this preference is due to the fact
that sentences are parsed from left to right; &amp;quot;every kid&amp;quot; takes
scope over &amp;quot;a tree&amp;quot; because it is processed first. (Kurtzman
and MacDonald called this the Left to Right principle.)
Ioup [20] argued instead that &amp;quot;...in natural language, or-
der has little to do with the determination of quantifier
scope.&amp;quot; ([20], p.37). The preferred reading of (8), for ex-
ample, is the one in which the NP &amp;quot;each child&amp;quot; takes wide
scope.
(8) I saw a picture of each child. [20]
According to Ioup, the relative scope of quantifiers is deter-
mined by the interaction of two factors. First of all, quan-
tifiers such as &amp;quot;each&amp;quot; or &amp;quot;the&amp;quot; have the inherent property of
taking wide scope over indefinites, which, in turn are lexi-
cally marked to take scope over plural quantifiers like &amp;quot;all.&amp;quot;
This hypothesis is motivated by contrasts such as those in
(9), and accounts for cases such as (8).4
</bodyText>
<listItem confidence="0.9936855">
(9) a. I saw a picture of each child.
b. I saw a picture of all the children.
</listItem>
<bodyText confidence="0.951327333333333">
Secondly, Ioup proposed that a hierarchy exists among
grammatical functions, such that listeners tend to attribute
to NPs in subject position wide scope over NPs in indirect
object position, which in turn tend to take wide scope over
NPs in object position. The hierarchy between grammatical
functions accounts for the preferred reading of (7).
Ioup also observed that NPs in topic position tend to take
wide scope. This is especially obvious in languages that
have a specific grammatical category for topic, like Japanese
or Korean. The Japanese sentence (10b) is ambiguous, but
the reading in which the NP in subject position, &amp;quot;most stu-
dents&amp;quot; takes scope over the NP in object position, &amp;quot;every
language,&amp;quot; is preferred. This preference is maintained if the
&apos;Van Lehn [35] and Hendrix [14] also studied the effect of
lexical preferences, or &apos;strengths&apos; as they are also called.
NP in object position is scrambled in sentence-initial posi-
tion, as in (10c) (another counterexample to Lakoff&apos;s left-
to-right principle). If, however, the NP is marked with the
topic-marking suffix &amp;quot;wa,&amp;quot; as in (10d), suddenly the pre-
ferred reading of the sentence becomes the one in which
&amp;quot;every language&amp;quot; takes wide scope.5
</bodyText>
<table confidence="0.548578857142857">
(10) a. Most students speak every language.
b. Hotondo-no gakusei-ga subete-no gengo-o hanasu
most-gen student-nom every language-acc speak
c. Subete-no gengo-o hotondo-no galcusei-ga hanasu
every language-acc most-gen student-nom speak
d. Subete-no gengo-wa hotondo-no galcusei-ga hanasu
every language-TOP most-gen student-nom speak
</table>
<bodyText confidence="0.99894925">
Several proposals attribute an important role to structural
factors in assigning a scope to operators. Jackendoff [21]
and Reinhart ([32], ch. 3 and 9) propose to account for the
preferred reading of (7) by means of a C-command principle
according to which a quantified expression is allowed to take
scope over another quantified expression only if the latter is
c-commanded by the former at surface structure.
Structural explanations (in the form of constraints on syn-
tactic movement) have also been proposed to explain the
constraint that prevents a quantifier to take scope outside
the clause in which it appears, first observed by May [28]
and called Scope Constraint by Heim [13]. This constraint
is exemplified by the contrast in (11): whereas (11a) has
a reading in which &amp;quot;every department&amp;quot; is allowed to take
wide scope over &amp;quot;a student,&amp;quot; this reading is not available for
(lib).
</bodyText>
<listItem confidence="0.981645666666667">
(11) a. A student from every department was at the party.
b. A student who was from every department was at the
party.
</listItem>
<bodyText confidence="0.999517">
Lexical semantics and commonsense knowledge also play
an important role in determining the scope of operators. The
contrast between the preferred readings of (12a) and (12b)
can only be explained in terms of lexical semantics:
</bodyText>
<listItem confidence="0.996696">
(12) a. A workstation serves many users.
b. A workstation can be found in many offices.
</listItem>
<bodyText confidence="0.865602230769231">
Kurtzman and MacDonald [26] set out to verify the empiri-
cal validity of several of these principles. The most crucial
result is that none of the principles they set to verify can
account for all the observed effects, and actually counterex-
amples to all of them—including the quantifier hierarchy—
can be found. No evidence for a Left-to-Right processing
principle was found. Kurtzman and MacDonald hypothesize
that &amp;quot;...processes that are not strictly dedicated to the inter-
pretation of scope relations may nonetheless influence the
interpretation of quantifier scope ambiguities.&amp;quot; ([26], p.22).
They conclude that &amp;quot;...the results leave open the question
of whether the building and selection of representations of
scope are mandatory processes&amp;quot; ([26], p.45).6
</bodyText>
<footnote confidence="0.9890608">
5Arguably, the closest thing to an explicit topic marker in En-
glish are certain uses of definite descriptions and the topicalization
construction; in both cases, the topically marked NP tends to take
wide scope.
6Their experiments are discussed in more detail in [29].
</footnote>
<page confidence="0.999418">
79
</page>
<sectionHeader confidence="0.9936" genericHeader="method">
OVERVIEW OF THE PROPOSAL
</sectionHeader>
<subsectionHeader confidence="0.9722955">
Scope Disambiguation as Construction of an Event
Structure
</subsectionHeader>
<bodyText confidence="0.9971373125">
It is commonly assumed in the psycholinguistic literature
on sentence interpretation that hearers interpret sentences
by constructing a model of the situation described by the
sentence [10, 22]. I propose that the scope assigned to the
operators contained in a sentence is determined by the char-
acteristics of the model construction procedure. The model
being constructed, which I call event structure, consists of a
set of situation descriptions, one for each operator, together
with dependency relations between them. The task of the
model construction procedure is to identify these situations
and to establish dependency relations. The scope assigned
by a hearer to an operator depends on the position of the
situation associated with that operator in the event structure.
For example, I propose that the scope assigned to quanti-
fiers depends on how their resource situation [3, 8] is iden-
tified. It is well-known that a sentence like (13):
</bodyText>
<equation confidence="0.741659">
(13) Everybody is asleep.
</equation>
<bodyText confidence="0.999086037037037">
is not interpreted as meaning that every single human be-
ing is asleep, but only that a certain contextually relevant
subset is. The process of identifying the set of individuals
over which an operator quantifies is usually called domain
restriction. In the case of, say, (7) whether &amp;quot;every kid&amp;quot; or &amp;quot;a
tree&amp;quot; takes wide scope depends on how the listener builds a
model of the sentence. If she starts by first identifying a situ-
ation containing the group of kids that &amp;quot;every&amp;quot; is quantifying
over, and then proceeds to &apos;build&apos; for each of these kids a
situation which contains a tree the kid is climbing, then &amp;quot;ev-
ery kid&amp;quot; will take wide scope. In other words, I propose that
a listener has a preferred reading for a sentence if she&apos;s able
to identify the resource situation of one or more of the oper-
ators in that sentence (&apos;to picture some objects in her mind&apos;),
and to hypothesize dependency relations between these sit-
uations. If this process cannot take place, the sentence is
perceived as &apos;ambiguous&apos; or &apos;hard to understand.&apos;
The less context is available, the more the establishment
of dependency relations between situations depends on the
order in which the model is built, i.e., on the order in which
the situations associated with the different operators and
events are identified. This order depends in part on which
NPs are perceived to be &apos;in topic,&apos; and in part on gen-
eral principles for building the conceptual representation of
events (see below). In addition, some operators (e.g., defi-
nite descriptions) impose constraints on their resource situ-
ation.
</bodyText>
<sectionHeader confidence="0.6764995" genericHeader="method">
A Model Construction Procedure: The DRT
Algorithm
</sectionHeader>
<bodyText confidence="0.980351793103448">
In order to make the intuition more concrete we need the
details of the model construction procedure. Ideally, one
would want to adopt an existing procedure and show that
the desired results fall out automatically. Unfortunately, the
model construction procedures presented in the psycholin-
guistic literature are not very detailed; often it&apos;s not even
clear what these researchers intend as a model. There is,
however, a discourse interpretation procedure that is speci-
fied in detail and has some of the characteristics of the model
construction procedure I have in mind; I&apos;m thinking of the
DRS construction algorithm [23, 24].
The DRS construction algorithm consists of a set of rules
that map discourses belonging to the language into certain
&amp;quot;interpretive structures&amp;quot;. The output structures are called
&amp;quot;Discourse Representation Structures&amp;quot; or &amp;quot;DRSs.&amp;quot; A DRS
is a pair consisting of a set of discourse referents and a set
of conditions (= predicates on the discourse referents). The
construction algorithm works by first adding the syntactic
structure of the sentence to the &apos;root&apos; DRS representing the
discourse up to that point, then applying the rules to the syn-
tactic structure, thus adding discourse referents and condi-
tions to the DRS. Consider how the algorithm is applied to
obtain an interpretation for (7):
(14) Every kid climbed the tree.
The initial interpretation of (14) is the tree shown in (15).
The DRS construction rule for definites and universal quan-
tification are as follows:
(Definite Descriptions) When a syntactic configuration
containing a definite NP is met in a DRS K,
</bodyText>
<listItem confidence="0.980085214285714">
1. Add a new discourse referent x to the root DRS,
2. Add a new condition to the root DRS representing the
restriction on the indefinite NP,
3. Replace the NP with x in the syntactic configuration.
(Universal Quantification) When a syntactic configura-
tion containing an NP with determiner &amp;quot;every&amp;quot; is met in a
DRS K,
1. Add a complex condition K1 —) K2 to K,
2. Add a new discourse referent x to Ki,
3. Add a new condition to K1 representing the restriction
on the indefinite NP,
4. Replace the NP with the discourse referent in the syn-
tactic configuration,
5. Move the syntactic configuration insider K2.
</listItem>
<bodyText confidence="0.9979755">
Both the rule for definites and the rule for universal quantifi-
cation are triggered by (15). Two hypotheses are obtained;
that obtained by applying first the rule for definite descrip-
tions is shown in (16). Both of these hypothesis contain op-
erators whose DRS construction rules haven&apos;t been applied
yet: this algorithm comes with a built-in notion of partial
</bodyText>
<figure confidence="0.998245">
NP
AA
Every kid
VP
V NP
ZNN
Da N&apos;
climbed a A
</figure>
<page confidence="0.973688">
80
</page>
<bodyText confidence="0.994633666666667">
hypothesis—a partial hypothesis is a DRS some of whose
operators still have to &apos;interpreted&apos; in the sense just men-
tioned.
</bodyText>
<equation confidence="0.786839">
(16)
</equation>
<bodyText confidence="0.999894">
The two partial hypotheses are made into complete hypothe-
ses by applying the remaining rules; the complete hypothesis
with the definite taking wide scope is shown in (17).
</bodyText>
<subsectionHeader confidence="0.891455">
Modifying the DRS Construction Algorithm
</subsectionHeader>
<bodyText confidence="0.999680133333333">
Because the DRS construction rules depend on syntactic pat-
terns, the role of structural factors in disambiguation can be
taken into account—and a lot of data about disambiguation
preferences can be explained without any further machin-
ery. The Scope Constraint, for example, is embedded in
the very semantics of DRT; and one can &apos;build in&apos; the con-
struction rules principles such as the c-command principle.
(Kamp and Reyle do just that in [24].) The limitations of
this approach are shown by examples in which the choice
of an interpretation does not depend on the structure, like
(12). Also, the rule for definites as just formulated is too re-
strictive: in cases like (18), for example, predicts the correct
reading for the definite NP &amp;quot;the meeting,&amp;quot; but the wrong one
for &amp;quot;the principal,&amp;quot; that, intuitively, takes narrow scope with
respect to &amp;quot;every school:&amp;quot;
</bodyText>
<listItem confidence="0.776714">
(18) Every school sent the principal to the meeting.
</listItem>
<bodyText confidence="0.999718269230769">
I propose that the role of lexical semantics, as well as the
data accounted for in the literature by introducing principles
such as the grammatical function hierarchy, the topic prin-
ciple, and the quantifier hierarchy, can be accounted for by
making the activation of the DRS construction rules depend
on factors other than the syntactic structure of the sentence.
The factors I propose to incorporate are (i) the semantics of
lexical items, (ii) the results of the interpretation of opera-
tors in context, and (iii) the way the representation of events
is built in memory.
In order to achieve this goal, I propose two main modi-
fications to the standard DRS construction algorithm. First
of all, I propose that the input to the algorithm is a logi-
cal form—a structure isomorphic to the s-structure, that car-
ries however information about the semantic interpretation
of lexical items. In this way, the role of semantic factors
in interpretation can be taken into account; in addition, a
semantic value can be assigned to a representation contain-
ing unresolved conditions or partial hypotheses. Secondly,
I propose to make the application of the DRS construction
rules depend on the identification of certain contextually de-
pendent elements of the interpretation. The ingredients of
the account thus include: a proposal about the input to the
model construction procedure; a notion of what an event
structure is; and an account of discourse interpretation. I
discuss these issues in turn in the next sections.
</bodyText>
<sectionHeader confidence="0.982621" genericHeader="method">
THE LOGICAL FORM
</sectionHeader>
<bodyText confidence="0.999971642857143">
As said above, the first difference between the interpretation
procedure proposed here and the DRS construction algorithm
illustrated above is that the rules I propose rely on semantical
and contextual factors. I propose to do this by adding to
standard DRT a new class of conditions, that I call &apos;logical
forms.&apos; Logical forms include semantic information about
the lexical items occurring in the sentence. The logical form
representation is the interface between the parser and the
model construction algorithm, and can be compositionally
obtained by a GPSG parser [11, 18] that couples a context-
free grammar with rules of semantic interpretation. I first
describe the language used to characterize the semantics of
lexical items, SEL (for Simple Episodic Logic), then the
syntax and interpretation of logical forms.
</bodyText>
<subsectionHeader confidence="0.963449">
Lexical Semantics in Simple Episodic Logic
</subsectionHeader>
<bodyText confidence="0.9990255">
I introduce SEL by presenting the truth conditions I propose
to assign to (18), repeated here for convenience:
</bodyText>
<listItem confidence="0.92889">
(18) Every school sent the principal to the meeting.
</listItem>
<bodyText confidence="0.995226">
The truth conditions usually assigned to (18) in a language
with restricted quantification, and ignoring tense, are shown
in (19); I propose instead to assign to (18) the interpretation
specified by (20).
</bodyText>
<figure confidence="0.831440111111111">
(19) (the m mEErING(m)
(V S SCHOOL(S)
(the p PRiNciPAL(p,$)]
sENT(s,p,m))))
(20) (the m [Si h MEETING(m)] A sHARED(spkr,hearer,h)
(V s [s*2 h scHoolN]
(the p [s*3 h PRINcEPAnp,$)]
A sHARED(spkr,hearer,S3)
sEtsrr(s,p,m))))
</figure>
<listItem confidence="0.718778">
(20) reads: there exists a unique m that is a meeting in a con-
textually specified resource situation and for all s&apos;s that
are schools in a contextually specified resource situation .§2
the unique p such that p is the principal of s participates torn.
</listItem>
<figure confidence="0.997028714285714">
TREE(X)
NP VP
ADid A A
Every kid climbed
TREE(x)
Km(y) every CLIMBED(y, X)
(17)
</figure>
<page confidence="0.992047">
81
</page>
<bodyText confidence="0.997187346153846">
The intent of the expression used for the quantifier restric-
tions in (20) is to make it explicit that the situations from
which the quantified elements are &apos;picked up&apos; need not be
the complete set of objects and relations at which the truth of
(20) is evaluated. This is accomplished by introducing into
the language an explicit relation h (&apos;supports&apos;) to represent
&apos;truth at a situation&apos; [8]. A statement of the form
[si h mEETING(x)]
evaluates to true in a situation s if the object—say, m—
assigned to the variable x is a meeting in the situation S1.
A situation is a set of objects and facts about these objects
[8, 18]. I assume a language which allows us to make state-
ments about situations, and an ontology in which situations
are objects in the universe. Episodic Logic provides such a
language and such an ontology [19, 181; where not otherwise
noted, the reader should assume that an expression of SEL
has the semantics of the identical expression in Episodic
Logic.
The restriction of the existential quantifier in (20) con-
tains a parameter s. Parameters are used in SEL to trans-
late anaphoric expressions of English. A parameter behaves
semantically as an open variable, a value for which has to
be provided by context.&apos;
I have assumed the following translations for the lexical
items &amp;quot;every,&amp;quot; &amp;quot;meeting,&amp;quot; and &amp;quot;sent&amp;quot; (I have again ignored
tense):
</bodyText>
<equation confidence="0.888979333333333">
&amp;quot;every&amp;quot; APAQ (V x P(x)] Q(x))
&amp;quot;meeting&amp;quot; MEETING
&amp;quot;sent&amp;quot; SENT
</equation>
<bodyText confidence="0.999498">
The semantics assigned to definite descriptions needs a bit
of an explanation. According to the location theory [12,
41 the major uses of definite NP&apos;s, as well as the contrast
between definites, indefinites, and demonstratives, can be
accounted for by stipulating that a speaker, when using a
definite article,
</bodyText>
<listItem confidence="0.9593078">
1. instructs the hearer to locate the referent in some shared
set of objects, and
2. refers to the totality of the objects/mass within this set that
satisfy the restriction.
I formalize this idea in [30] by associating to definite de-
scriptions the translation below. A situation is &apos;shared&apos; be-
tween x and y if every fact &amp;quot;tIf supported by that situation is
mutually believed by x and y (see [30] for details).
&amp;quot;the meeting&amp;quot; A P (the x: ([..5 h MEETING(x)1
SHARED(spkr,hearer,S)) P(x))
</listItem>
<subsectionHeader confidence="0.496889">
Syntax and Interpretation of the Logical Form
</subsectionHeader>
<bodyText confidence="0.9379881">
The translations seen above, together with the obvious
context-free rules, result in the following LF for (18) (I have
&apos;See [29] for details. The idea is to add to the parameters of
evaluation an anchoring function a that provides the values for
parameters, thus plays the role of &apos;context&apos; in Heim&apos;s proposal. The
reader should be aware that while the notation and terminology I
have adopted is borrowed from Situation Theory, parameters have
a different semantic interpretation there [8].
used here, and elsewhere in the paper, a linear notation to
save space):
</bodyText>
<equation confidence="0.7925402">
(21) [cp
[IP [NP Q (V s [.5*2 h scHoor.(s)] Q(s))]
[vp [vp [v, &apos;SENT
[Np Q (the p V3 h P8ThiciPAL(p,t)]
sHARED(spkr,hearer,s&apos;3)
Q(P)Ail
[PP &apos;TO
[NP &apos;A Q (the m h MEETiNG(m)]
sHARED(spicr,hearer,ssi)
Q(11)Allii
</equation>
<bodyText confidence="0.997175076923077">
I propose that expressions like (21) can appear as conditions
of DRSs. The syntax of LFs is as follows. Each internal
node of (21) is labeled with a phrase category; the leaves
are labeled with expressions of the form &apos;a, where a is an
expression of SEL (and has therefore a &apos;standard&apos; model the-
oretic denotation). I use the phrase structure system largely
adopted in the Government and Binding literature, accord-
ing to which the sentence is the maximal projection of an Infl
node and is therefore labeled IP [34]. I also assume the exis-
tence of a maximal projection of complementizer CP above
IP. Because I don&apos;t discuss relatives here, I use the following
simplified notation for NPs with determiners, such as &amp;quot;every
school&amp;quot;:
</bodyText>
<equation confidence="0.668963">
[NP Q (v x [s). scHooL(x)] Q(x))]
</equation>
<bodyText confidence="0.997736857142857">
LFs like (21) are usually treated in the natural language
processing literature as uninterpreted data structures from
which to &apos;extract&apos; the readings [16, 17]. However, it has
been recently proposed [31, 2, 33] that it is possible (and
indeed desirable) to assign a denotation to expressions like
(21). The reason is that in this way one can define a no-
tion of sound inference —that is, one can specify what can
and cannot properly be inferred from an expression like (21)
prior to disambiguation; and therefore, a notion of &apos;mono-
tone disambiguation.&apos; I do not assume disambiguation to
work monotonically, but I want to be able to treat expres-
sions like (21) as full-fledged conditions so that a DRS con-
taining a condition of this kind can be interpreted, and I need
to be able to characterize a disambiguation step as compati-
ble in the sense that it does not introduce any new readings.
To do this I need LFs to have an interpretation.
Were it not for the problem that more than one interpre-
tation can be associated to a single LF, one could easily de-
fine a recursive mapping EXT from logical forms to truth-
theoretical denotations (functions from situations to truth
values) in terms of the usual II II
</bodyText>
<equation confidence="0.7862278">
EXT(&apos;a) = Hail
EXT([v, a]) = EXT(a)
EXT([VP al) = EXT(a)
EXT([N, a]) = EXT(a)
EXT(iNp a 13]) = EXT(a)(EXT(f3))
EXTUT a /31) = EXT(a)(EXT((3))
if TYPE(EXT(a)) = (t1,t2)
and TYPE(EXT(13)) =
EXT(13)(EXT(a)) otherwise.
function, as follows:
</equation>
<page confidence="0.982087">
82
</page>
<bodyText confidence="0.999953666666667">
Once this is done, one can reformulate the semantics of
DRS in terms of situations and situations extensions instead
of embeddings and embedding extensions, and interpret all
conditions as functions from situations to truth values. (See
[29] for details.)
Matters get more complicated when expressions with
more than one reading like (21) are considered. Different
ways for assigning a denotation to expressions with more
than one interpretation have been proposed [2, 31]; my pro-
posal derives from [31]. I use a Cooper storage mechanism
[5] to define EXT in such a way as to allow for an LF to have
more than one &apos;indirect interpretation.&apos; Briefly, Cooper&apos;s
idea is to have a syntactic tree denote a set of sequences,
each sequence representing a distinct &apos;order of application&apos;
in computing the interpretation of the sentence. For exam-
ple, because in interpreting (22) one can either apply the
translation of tense immediately or wait, EXT maps (22) in
a set of two sequences, shown in (23).
</bodyText>
<equation confidence="0.720433333333333">
(22) [y, &apos;P [Np &apos;A, Q (det x R(x)) Q(x)] ]
EXT((22)) = { (A x (det x R(x))P(x) ),
(23) (P, A Q (det x R(x)) Q(x) ) }
</equation>
<bodyText confidence="0.8535782">
I omit here the definition of the EXT function implement-
ing Cooper storage, that is rather complex. For the current
purposes, it is enough to understand that EXT associates to
(21) a set of functions from situations to truth values, as in
(24).
</bodyText>
<equation confidence="0.698445571428571">
(24) EXT((21)) =
{the function denoted by
II (the m [s 1= mEETING(m)] A sHARED(spkr,hearer,..11)
(V s [Y2 = SCHOOL(S)]
(the p [5:3= PiuNciPAL(p,$)]
A sHARED(spkr,hearer,S3)
sENT(s,p,x)))) II,
</equation>
<bodyText confidence="0.575233">
the function denoted by
II (V s [s12 h SCHOOL(S)]
</bodyText>
<equation confidence="0.87247375">
(the m VI h mEETING(m)] A sHARED(spkr,hearer,..11)
(the p [4 h PR1NcIPAL(p,$)]
A sEARED(spla,hearer,.13)
sarT(s,p,x)))) II, etc. }
</equation>
<bodyText confidence="0.994822">
Having done this, we can say that a DRS condition like (21)
is verifies the current situation s if one of the functions de-
noted by (21) maps s into 1.
</bodyText>
<sectionHeader confidence="0.971959" genericHeader="method">
BUILDING EVENT STRUCTURES
</sectionHeader>
<bodyText confidence="0.999244">
Not all assertions in a narrative or conversation are going
to be about the same situation. In the conversations with
the TRAINS system, for example, the participants can dis-
cuss both the state of the world and the state of the plan
being developed. Maintaining this separation is crucial for
the proper interpretation of definite descriptions, for exam-
ple. The separation between the situations that are the topic
of different sentences is achieved by translating sentences as
situation descriptions. A situation description is a condition
of the form:
</bodyText>
<listItem confidence="0.68221">
(25) S:
</listItem>
<bodyText confidence="0.998803684210526">
whose intuitive interpretation is that (I) provides a partial
characterization of the situation s. The semantics of situa-
tion descriptions is defined as follows, using a semantics of
DRSs in terms of situation extensions, as discussed in the
previous section, and interpreting discourse markers as con-
stituents of situations:
The condition s:K is satisfied wrt the situation s&apos; if K
is satisfied wit the value assigned to s in s &apos; .
I also propose the following constraint on the model con-
struction rules:
Constraint on Interpretation : with the exception of the
discourse markers interpreted over situations and of the
situation descriptions, every discourse marker and condi-
tion has to be part of a situation descriptions.
Situation descriptions are added to the model by rules trig-
gered by an LF whose root is a CP node. The rules (now
shown for lack of space) delete the complementizer and its
whole projection, and introduce a situation structure. The
result is shown in (26).
</bodyText>
<equation confidence="0.571209">
(26)
</equation>
<bodyText confidence="0.999974833333333">
The constraint on discourse interpretation proposed above is
implemented by forcing the rules that build situation struc-
tures to be triggered before any other rule; this is done by
having every other rule being triggered by LFs whose root
node is an IP. The result of this constraint is that a discourse
model consists of a set of situation descriptions:
</bodyText>
<equation confidence="0.524849">
(27)
</equation>
<bodyText confidence="0.999950384615385">
The DRSs produced by the standard DRT algorithm are se-
mantically equivalent to the special case of a set of situation
descriptions all describing the same situation s.
Models like the one in (27) enable the formalization of
processes of resource situation identification like that de-
scribed in [30]. I illustrate how my rules for interpreting
operators differ from those of standard DRT, and how the in-
teraction between model construction rules and discourse in-
terpretation works, by means of the model construction rule
for definites. The rule MCR-DD is triggered by the config-
uration in (28), and results in the configuration in (29). The
notation used for the pattern indicates that this rule applies
to a definite NP in any position within a syntactic tree whose
</bodyText>
<page confidence="0.512248">
x
</page>
<figure confidence="0.998011833333333">
S
s: A
S: x
(I)
s&apos;: Y
&apos;If
</figure>
<page confidence="0.99527">
83
</page>
<bodyText confidence="0.945388">
maximal projection is an IP node, without any intervening
W node.
</bodyText>
<listItem confidence="0.675524">
(29) s:
</listItem>
<bodyText confidence="0.995072157894737">
The key observation is that the application of this rule, as
well as of any other NP rule, depends on the hearer&apos;s pre-
vious identification of a resource situation for the definite
description. The statement ANCHOR(&apos;S, s&apos;) constraining the
interpretation of S is added to the situation structure by the
processes that identify the referent of the definite descrip-
tion; I describe these processes in detail in [301.8
Finally, I propose that, when context is missing, a default
model construction procedure operates. It has been sug-
gested [6] that the conceptualization of events follows an
order reflected in the thematic hierarchy AGENT &lt; LOCA-
TION, SOURCE, GOAL &lt; THEME proposed to account for
phenomena like passivization [21]. Briefly, the idea is that
&apos;the normal procedure for building an event description&apos; is
to follow the order in the hierarchy: first identify the agent,
then the location, then the theme. This proposal can be for-
malized in the current framework by having rules that oper-
ate in case no other rule has, and that modify the model by
introducing a resource situation for an operator and estab-
lishing anchoring connections. These rules depend both on
the semantics of the verb and on the syntactic configuration.
The rule that identifies the AGENT, for example, is triggered
by the configuration in (30), and results in the configuration
in (31), that allows for the rule for the NP to operate in that
the resource situation of the operator has been anchored:
8A more conventional situation-theoretic framework is used
there, but the analysis carries over to the framework in this paper.
These rules can of course originate conflicts with the re-
sults of other discourse interpretation processes. I assume
the following conflict resolution rule: when two rules pro-
duce conflicting hypothesis, assume the result of the more
specific rule. In general, the discourse interpretation rules
are more specific than the default rules for constructing
events representations, so they will be preferred.
Although lack of space prevents me from giving exam-
ples, rules relating the construction of the model to lexical
semantics, such as those accounting for data like (12), can
also be formulated.
</bodyText>
<sectionHeader confidence="0.922871" genericHeader="method">
AN EXAMPLE
</sectionHeader>
<bodyText confidence="0.999171333333333">
We can now discuss in more detail the process of disam-
biguation of (18). I have presented the logical form for (18)
above, as (21).
</bodyText>
<listItem confidence="0.49671">
(18) Every school sent the principal to the meeting.
</listItem>
<bodyText confidence="0.999864833333333">
After identifying the situation descriptions, various interpre-
tation processes take place, like those performing definite
description interpretation described in [30]. These processes
generate hypotheses about the anchoring of resource situa-
tions. Without entering into details, I assume that the con-
text for (18) is provided by (32), that introduces into the
</bodyText>
<figure confidence="0.977924222222222">
P (det x QJ
P(x))
ANCHOR(&apos;.t, s&apos;)
s&apos;:
P(y)
IP
ZNN:
XX y YY
ANcHoR(&apos;S., s&apos;)
</figure>
<page confidence="0.992705">
84
</page>
<bodyText confidence="0.966678666666667">
model the situation description in (33), containing a group
of schools and a meeting.
(32) There was a meeting of the schools in the district.
</bodyText>
<equation confidence="0.507093">
(33)
</equation>
<bodyText confidence="0.998069142857143">
Given this context, the discourse interpretation processes
identify s as the resource situation for the NPs &amp;quot;every school&amp;quot;
and &amp;quot;the meeting.&amp;quot; However, no unique principal can be
identified in s. The activation of the model construction
rules for universal quantification and definite descriptions
results in the partial model in (34), in which gi and s&apos;2 have
been identified:
</bodyText>
<table confidence="0.953541315789474">
(34)
S s1
sl:
Y
s: MEETING (y)
ANCHOR(y, x)
S2
S2: Z --r
S:Z E S
S2 C THIS_SITUATION
S3
s3: IP 1--------.. VP PP
NP VP NP
&apos;SENT y
----,..............„ I
NP
A z_
&amp;quot;the principal&amp;quot; &apos;TO
s2 C S3
</table>
<bodyText confidence="0.999875882352941">
The model construction rule applied to the universal &amp;quot;ev-
ery school&amp;quot; introduces a complex condition K1 —4 K2 as
usual, but both the restriction and the nuclear scope include
situation descriptions. The situation description in the re-
striction, 52, is a subsituation of the situation at which the
restriction is evaluated (denoted by the indexical constant
THIS_SITUATION). The situation description in the nu-
clear scope, s3, is an extension of 52.
Now that a situation description for the resource situation
of the universal and a discourse marker for the school have
been introduced (52 and z, respectively), the rules for resolv-
ing the parametric component X of the interpretation of &amp;quot;the
principal&amp;quot; can apply. The result is that z is chosen as an-
tecedent of X, and s2 is chosen as the resource situation for
&amp;quot;the principal.&amp;quot; The model construction rule updates 53 ac-
cordingly; the resulting event structure is equivalent to the
interpretation of (21) specified by (20).
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="method">
ACCOUNTING FOR THE
DISAMBIGUATION DATA
</sectionHeader>
<bodyText confidence="0.999983260869565">
I briefly return here the disambiguation principles, to show
how the proposal just presented accounts for them. First of
all, I&apos;ll note that, under simple assumptions about the map-
ping between grammatical functions and theta-roles, there
is a striking resemblance between the grammatical function
hierarchy proposed by Ioup and the thematic hierarchy pro-
posed by Jackendoff to account for facts about passives and
reflexives. The facts accounted for by the grammatical func-
tion hierarchy principle can also be explained if we assume
that the description of an event is constructed by identify-
ing the filler of each thematic role in the order specified by
Jackendoff&apos;s thematic hierarchy.
Consider now the case of the other disambiguation fac-
tor proposed by Ioup, the lexically encoded preference for
certain operators to take wide scope. Definite descriptions
are the paradigmatic case of an operator that tends to take
wide scope. This preference can be explained in terms of
the model construction hypothesis as follows. The choice
of a resource situation for definite descriptions is restricted
by the constraint that this resource situation be either shared
among the conversational participants, or related to shared
knowledge by shared relations [12, 4]. In our dialogues, for
example, definite descriptions are usually interpreted with
respect to the &apos;situation&apos; corresponding to the current visual
scene, which is independent from other situations. It follows
that a definite description will be assigned narrow scope rel-
ative to another operator only if (i) the resource situation
of the definite is perceived to depend on this other resource
situation, and (ii) this dependency relation is known to be
shared.
As for the tendency for NPs in topic to take wide scope,
an element of a sentence is said to be in topic if it is consid-
ered to be part of the background information on which the
new information in the sentence depends. As the interpre-
tation of the &apos;new&apos; information in the sentence depends on
the background information, it is plausible to assume that,
in constructing a model for the sentence, the listener begins
by applying the model construction rules for the operators
perceived to be in topic (or explicitly marked as being in
topic, in the case of Japanese). The interpretation of the
operators not in topic, when determined at all, will depend
on the interpretation of the operators in topic, resulting in
the dependency relations between the related situations that
I have assumed to be the way scope is represented.
Finally, I&apos;ll note that, in the absence of contextual clues,
whether a completely disambiguated event structure is actu-
</bodyText>
<figure confidence="0.9862088">
X S
MEETING(X)
SCHOOL* (S)
PARTICIPATE(S, X)
s:
</figure>
<page confidence="0.98671">
85
</page>
<bodyText confidence="0.806455111111111">
ally constructed depends on how strong the model construc- 86 [18] C. H. Hwang. A Logical Approach to Narrative Understand-
tion rules are supposed to be; it&apos;s perfectly possible that the ing. PhD thesis, University of Alberta, 1992.
activation of these rules is controlled by additional factors, [19] C. H. Hwang and L. K. Schubert. Episodic logic: A situ-
such as the specific needs of a task to be performed. ational logic for natural language processing. In P. Aczel,
ACKNOWLEDGMENTS D. Israel, Y. Katagiri, and S. Peters, editors, Situation Theory
I wish to thank my advisor Len Schubert and James Allen, and its Applications, v.3. CSLI, 1993. To appear.
Howard Kurtzman, Peter Lasersohn, and Uwe Reyle for sev- [20] Georgette Ioup. Some universals for quantifier scope. In
eral suggestions, technical help, and constructive criticism. J. Kimball, editor, Syntaxand Semantics 4, pages 37-58. Aca-
This work was supported by the US Air Force - Rome Lab- demic Press, New York, 1975.
</bodyText>
<reference confidence="0.990601">
oratory Research Contact no. F30602-91-C-0010. [21] R. Jackendoff. Semantic Interpretation in Generative Gram-
References mar. MIT Press, 1972.
[1] J. E Allen and L.K. Schubert. The TRAINS project. TRAINS [22] P. Johnson-Laird. Mental Models. Harvard University Press,
Technical Note 91-1, University of Rochester, Department of 1983.
Computer Science, 1991. [23] H. Kamp. A theory of truth and semantic representation.
[2] H. Alshawi and R. Crouch. Monotonic semantic interpre- In J. Groenendijk, T. Janssen, and M. Stokhof, editors, For-
tation. In Proc. 30th. ACL, pages 32-39, University of mal Methods in the Study of Language.Mathematical Centre,
Delaware, 1992. Amsterdam, 1981.
[3] J. Barwise and J. Perry. Situations and Attitudes. The MIT [24] H. Kamp and U. Reyle. From discourse to logic. To appear.,
Press, 1983. 1993.
[4] H. H. Clark and C. R. Marshall. Definite reference and mu- [25] R. Kempson and A. Cormack. Ambiguity and quantification.
tual knowledge. In Elements of Discourse Understanding. Linguistics and Philosophy, 4(2):259-310,1981.
Cambridge University Press, 1981. [26] H. S. Kurtzman and M. C. MacDonald. Resolution of quan-
[5] Robin Cooper. Quantification and Syntactic Theory. D. Rei- tifier scope ambiguities. To appear., April 1992.
del, 1983. [27] G. Lakoff. Semantic interpretation in generative grammar. In
[6] W. Croft. Syntactic Categories and Grammatical Relations: D. A. Steinberg and L. A. Jakobovits, editors, Semantics: An
The cognitive organization of information. University of interdisciplinary reader in philosophy, linguistics, anthropol-
Chicago Press, 1991. ogy, and psychology. Cambridge University Press, 1971.
[7] Kees van Deemter. On the Composition of Meaning. PhD [28] R. May. The Grammar of Quantification. PhD thesis, MIT,
thesis, University of Amsterdam, 1991. 1977.
&apos; [8] K. Devlin. Logic and Information. Cambridge University [29] M. Poesio. Assigning a Scope to Operators in Dialogues.
Press, 1991. PhD thesis, University of Rochester, Department of Com-
[9] J.E. Fenstad, P.K. Halvorsen, T. Langholm, and J. van Ben- puter Science, 1993.
them. Situations, Language and Logic. D.Reidel, 1987. [30] M. Poesio. A situation-theoretic formalization of definite
[10] A. Gamham. On-line construction of representations of the description interpretation in plan elaboration dialogues. In
content of texts. Reproduced by Indiana University Linguis- P. Aczel, D. Israel, Y. Katagiri, and S. Peters, editors, Situ-
tics Club, 1982. ations Theory and its Applications, vol.3, chapter 12, pages
[11] G. Gazdar, E. Klein, G. Pullum, and L Sag. Generalized 343-378. CSLI, 1993. To appear.
Phrase Structure Grammar. Blackwell, 1985. [31] Massimo Poesio. Relational semantics and scope ambigu-
[12] J. A. Hawkins. Definiteness and Indefiniteness. Croom Helm, ity. In J. Barwise, J. M. Gawron, G. Plotkin, and S. Tutiya,
1978. editors, Situation Semantics and its Applications, vol.2, chap-
[13] I. Heim. The Semantics of Definite and Indefinite Noun ter 20, pages 469-497. CSLI, 1991.
Phrases. PhD thesis, University of Massachusetts at Amherst, [32] T. Reinhart. Anaphora and semantic interpretation. Croom
1982. Helm, 1983.
[14] 0.0. Hendrix. Semantic aspects of translation. In D. Walker, [33] U. Reyle. Dealing with ambiguities by underspecification:
editor, Understanding Spoken Language, pages 193-226. El- Construction, representation and deduction. Journal of Se-
sevier, 1978. mantics, 3,1993.
[15] J. Hobbs. An improper treatment of quantification in ordinary [34] T. Stowell. Origins of Phrase Structure. PhD thesis, MIT,
English. In Proc. ACL-83, pages 57-63, Cambridge, MA, 1981.
June 1983. [35] Kurt A. VanLehn. Determining the scope of English quan-
[16] J. R. Hobbs and S. M. Shieber. An algorithm for generating tifiers. Technical Report AI-TR-483, Artificial Intelligence
quantifier scopings. Computational Linguistics, 13(l-2):47- Laboratory, MIT, Cambridge, MA, 1978.
63, January-June 1987.
[17] Sven Hurum. Handling scope ambiguities using domain-
independent heuristics. Technical Report TR 88-12, Univer-
sity of Alberta, June 1988.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.022218">
<title confidence="0.999877">ASSIGNING A SEMANTIC SCOPE TO OPERATORS</title>
<author confidence="0.999971">Massimo Poesio</author>
<affiliation confidence="0.999882">University of Rochester, Department of Computer Science</affiliation>
<address confidence="0.999726">Rochester, NY 14627-0226, USA</address>
<email confidence="0.99873">poesio@cs.rochester.edu</email>
<abstract confidence="0.992557690253673">that the characteristics of the scope disambiguation process observed in the literature can be explained in terms of the way in which the model of the situation described by a sentence is built. The model construction procedure I builds an structure identifying the situations associated with the operators in the sentence and their mutual dependency relations, as well as the relations between these situations and other situations in the context. The procedure takes into account lexical semantics and the result of various discourse interpretation procedures such as definite description interpretation, and does not require a complete disambiguation to take place. THE PROBLEM Because new ways of obtaining semantically distinct interpretations for sentences are continuously discovered, coming to grips with ambiguity is becoming more and more of a necessity for developers of natural language processing systems, linguists and psychologists alike [9, 31, 7, 21. this paper, I am concerned with the of [31, 33]. The attention of both psycholinguists and computational linguists interested in ambiguity has concentrated on the problem of combinatorial explosion. If the number of readings of an utterance were to actually grow with the factorial of the number of operators, even a simple sentence like (1), with 4 operators (the modal &apos;should&apos;, tense, an indefinite and a definite), would have 4! = 24 scopally different readings. Two distinct questions thus must be answered: how can listeners (and how should machines) deal with the combinatorial explosion of readings? Do we really use the brute-force strategy of considering all of the available readings, and then choose among them? And, if we do choose among several readings, how is that done? (1) We should hook up an engine to the boxcar. To my knowledge, three positions on the problem of combinatorial explosion have been taken in the literature. Some have argued that there is no problem: our brains contain use here the term as it used by Heim [13], i.e., to mean either quantifier or modal/tense operator. more than enough machinery to process in parallel 4! interpretations. It&apos;s unclear, however, whether this strategy is feasible when larger numbers of readings are concerned. A classical demonstration of the number of readings one may have to consider is (2), which has 11! interpretations if the standard treatment of quantification and modality is assumed. (2) You can fool most people on most of the issues most of the time, but you can&apos;t fool everybody on every single issue all of the time. [15] Another position is that sentences like (1) are not semantiambiguous, but for example (3): (3) Every kid climbed a tree. Here, one of the readings (the one in which the indefinite takes narrow scope) is entailed by the other (in which the indefinite takes wide scope). The claim is that (3) is interpreted in the vaguest possible way, and the strongest reading, if at all, is derived by pragmatic &apos;strengthening&apos; [25]. A difficulty with this approach is that a vaguest reading doesn&apos;t always exist. The two readings of (4), for example, are distinct. (4) Few people speak many languages. [27] Finally, it has been proposed that the reason why listeners do not seem to have problems in processing utterances like (1) is because they do not disambiguate. They build a nondisambiguated representation of the sentence and leave the interpretation open. This strategy might be advantageous for some kinds of applications&apos; and it has been argued that a complete disambiguation never takes place [7]. No matter what processing strategy is chosen, the question of how listeners choose one particular interpretation cannot be ignored. All experimental work done on the subject of scopal ambiguity [20, 35, 26] indicates that subjects do have preferred interpretations when confronted with tasks which require understanding. In addition, sentences like (1), (5) and (6) clearly have preferred interpretations. However, the only answers to to this question that I have seen are based machine translation [2]. &apos;See [17] for an example of state-of-the-art techniques 78 (5) A girl took every chemistry course. [20] (6) Each daughter of an admiral married a captain. I present in this paper an hypothesis about interpretation that accounts for facts about scope disambiguation that were previously explained in the literature by stipulating a number of unmotivated principles. The proposal developed here is being applied to develop the module of the TRAINS-93 system [1] that handles scope disambiguation and reference interpretation. The goal of the TRAINS project is to develop a conversationally proficient planning assistant. More details about the project and the work presented here can be found in [29]. SCOPE DISAMBIGUATION FACTORS Most proposals on scope disambiguation were developed to account for the general preference of the leftmost quantified phrase from taking wide scope in simple active sentences like (7): (7) Every kid climbed a tree. Lakoff [27] proposed that this preference is due to the fact that sentences are parsed from left to right; &amp;quot;every kid&amp;quot; takes scope over &amp;quot;a tree&amp;quot; because it is processed first. (Kurtzman MacDonald called this the to Right principle.) Ioup [20] argued instead that &amp;quot;...in natural language, order has little to do with the determination of quantifier scope.&amp;quot; ([20], p.37). The preferred reading of (8), for example, is the one in which the NP &amp;quot;each child&amp;quot; takes wide scope. (8) I saw a picture of each child. [20] According to Ioup, the relative scope of quantifiers is determined by the interaction of two factors. First of all, quantifiers such as &amp;quot;each&amp;quot; or &amp;quot;the&amp;quot; have the inherent property of taking wide scope over indefinites, which, in turn are lexically marked to take scope over plural quantifiers like &amp;quot;all.&amp;quot; This hypothesis is motivated by contrasts such as those in and accounts for cases such as (9) a. I saw a picture of each child. b. I saw a picture of all the children. Secondly, Ioup proposed that a hierarchy exists among grammatical functions, such that listeners tend to attribute to NPs in subject position wide scope over NPs in indirect object position, which in turn tend to take wide scope over NPs in object position. The hierarchy between grammatical functions accounts for the preferred reading of (7). Ioup also observed that NPs in topic position tend to take wide scope. This is especially obvious in languages that have a specific grammatical category for topic, like Japanese or Korean. The Japanese sentence (10b) is ambiguous, but the reading in which the NP in subject position, &amp;quot;most students&amp;quot; takes scope over the NP in object position, &amp;quot;every language,&amp;quot; is preferred. This preference is maintained if the &apos;Van Lehn [35] and Hendrix [14] also studied the effect of lexical preferences, or &apos;strengths&apos; as they are also called. NP in object position is scrambled in sentence-initial position, as in (10c) (another counterexample to Lakoff&apos;s leftto-right principle). If, however, the NP is marked with the topic-marking suffix &amp;quot;wa,&amp;quot; as in (10d), suddenly the preferred reading of the sentence becomes the one in which language&amp;quot; takes wide a. students speak every language. b. Hotondo-no gakusei-ga subete-no gengo-o hanasu most-gen student-nom every language-acc speak c. Subete-no gengo-o hotondo-no galcusei-ga hanasu every language-acc most-gen student-nom speak d. Subete-no gengo-wa hotondo-no galcusei-ga hanasu every language-TOP most-gen student-nom speak Several proposals attribute an important role to structural factors in assigning a scope to operators. Jackendoff [21] and Reinhart ([32], ch. 3 and 9) propose to account for the reading of (7) by means of a principle according to which a quantified expression is allowed to take scope over another quantified expression only if the latter is c-commanded by the former at surface structure. Structural explanations (in the form of constraints on syntactic movement) have also been proposed to explain the constraint that prevents a quantifier to take scope outside the clause in which it appears, first observed by May [28] called Constraint Heim [13]. This constraint exemplified by the contrast in (11a) has a reading in which &amp;quot;every department&amp;quot; is allowed to take wide scope over &amp;quot;a student,&amp;quot; this reading is not available for (lib). (11) a. A student from every department was at the party. b. A student who was from every department was at the party. Lexical semantics and commonsense knowledge also play an important role in determining the scope of operators. The contrast between the preferred readings of (12a) and (12b) can only be explained in terms of lexical semantics: (12) a. A workstation serves many users. b. A workstation can be found in many offices. Kurtzman and MacDonald [26] set out to verify the empirical validity of several of these principles. The most crucial result is that none of the principles they set to verify can account for all the observed effects, and actually counterexamples to all of them—including the quantifier hierarchy— can be found. No evidence for a Left-to-Right processing principle was found. Kurtzman and MacDonald hypothesize that &amp;quot;...processes that are not strictly dedicated to the interpretation of scope relations may nonetheless influence the interpretation of quantifier scope ambiguities.&amp;quot; ([26], p.22). They conclude that &amp;quot;...the results leave open the question of whether the building and selection of representations of are mandatory processes&amp;quot; ([26], the closest thing to an explicit topic marker in English are certain uses of definite descriptions and the topicalization construction; in both cases, the topically marked NP tends to take wide scope. experiments are discussed in more detail in [29]. 79 OVERVIEW OF THE PROPOSAL Scope Disambiguation as Construction of an Event Structure It is commonly assumed in the psycholinguistic literature on sentence interpretation that hearers interpret sentences constructing a the situation described by the sentence [10, 22]. I propose that the scope assigned to the operators contained in a sentence is determined by the characteristics of the model construction procedure. The model constructed, which I call structure, of a of descriptions, for each operator, together with dependency relations between them. The task of the model construction procedure is to identify these situations and to establish dependency relations. The scope assigned by a hearer to an operator depends on the position of the situation associated with that operator in the event structure. For example, I propose that the scope assigned to quantidepends on how their situation 8] is identified. It is well-known that a sentence like (13): (13) Everybody is asleep. is not interpreted as meaning that every single human being is asleep, but only that a certain contextually relevant subset is. The process of identifying the set of individuals which an operator quantifies is usually called the case of, say, (7) whether &amp;quot;every kid&amp;quot; or &amp;quot;a tree&amp;quot; takes wide scope depends on how the listener builds a model of the sentence. If she starts by first identifying a situation containing the group of kids that &amp;quot;every&amp;quot; is quantifying over, and then proceeds to &apos;build&apos; for each of these kids a situation which contains a tree the kid is climbing, then &amp;quot;every kid&amp;quot; will take wide scope. In other words, I propose that a listener has a preferred reading for a sentence if she&apos;s able to identify the resource situation of one or more of the operators in that sentence (&apos;to picture some objects in her mind&apos;), and to hypothesize dependency relations between these situations. If this process cannot take place, the sentence is perceived as &apos;ambiguous&apos; or &apos;hard to understand.&apos; The less context is available, the more the establishment of dependency relations between situations depends on the order in which the model is built, i.e., on the order in which the situations associated with the different operators and events are identified. This order depends in part on which NPs are perceived to be &apos;in topic,&apos; and in part on general principles for building the conceptual representation of events (see below). In addition, some operators (e.g., definite descriptions) impose constraints on their resource situation. A Model Construction Procedure: The DRT Algorithm In order to make the intuition more concrete we need the details of the model construction procedure. Ideally, one would want to adopt an existing procedure and show that the desired results fall out automatically. Unfortunately, the model construction procedures presented in the psycholinguistic literature are not very detailed; often it&apos;s not even clear what these researchers intend as a model. There is, however, a discourse interpretation procedure that is specified in detail and has some of the characteristics of the model construction procedure I have in mind; I&apos;m thinking of the construction algorithm 24]. The DRS construction algorithm consists of a set of rules that map discourses belonging to the language into certain &amp;quot;interpretive structures&amp;quot;. The output structures are called &amp;quot;Discourse Representation Structures&amp;quot; or &amp;quot;DRSs.&amp;quot; A DRS a pair consisting of a set of referents a set (= on the discourse referents). The construction algorithm works by first adding the syntactic structure of the sentence to the &apos;root&apos; DRS representing the discourse up to that point, then applying the rules to the syntactic structure, thus adding discourse referents and conditions to the DRS. Consider how the algorithm is applied to obtain an interpretation for (7): (14) Every kid climbed the tree. The initial interpretation of (14) is the tree shown in (15). The DRS construction rule for definites and universal quantification are as follows: (Definite Descriptions) When a syntactic configuration containing a definite NP is met in a DRS K, 1. Add a new discourse referent x to the root DRS, 2. Add a new condition to the root DRS representing the restriction on the indefinite NP, 3. Replace the NP with x in the syntactic configuration. Quantification) a syntactic configuration containing an NP with determiner &amp;quot;every&amp;quot; is met in a DRS K, Add a condition —) K, 2. Add a new discourse referent x to Ki, Add a new condition to representing the restriction on the indefinite NP, 4. Replace the NP with the discourse referent in the syntactic configuration, Move the syntactic configuration insider Both the rule for definites and the rule for universal quantification are triggered by (15). Two hypotheses are obtained; that obtained by applying first the rule for definite descriptions is shown in (16). Both of these hypothesis contain operators whose DRS construction rules haven&apos;t been applied this algorithm comes with a built-in notion of NP AA Every kid VP V NP Da N&apos; a 80 hypothesis is a DRS some of whose operators still have to &apos;interpreted&apos; in the sense just mentioned. (16) The two partial hypotheses are made into complete hypotheses by applying the remaining rules; the complete hypothesis with the definite taking wide scope is shown in (17). Modifying the DRS Construction Algorithm Because the DRS construction rules depend on syntactic patterns, the role of structural factors in disambiguation can be taken into account—and a lot of data about disambiguation preferences can be explained without any further machinery. The Scope Constraint, for example, is embedded in the very semantics of DRT; and one can &apos;build in&apos; the construction rules principles such as the c-command principle. (Kamp and Reyle do just that in [24].) The limitations of this approach are shown by examples in which the choice of an interpretation does not depend on the structure, like (12). Also, the rule for definites as just formulated is too restrictive: in cases like (18), for example, predicts the correct for the definite meeting,&amp;quot; but the wrong one for &amp;quot;the principal,&amp;quot; that, intuitively, takes narrow scope with respect to &amp;quot;every school:&amp;quot; (18) Every school sent the principal to the meeting. I propose that the role of lexical semantics, as well as the data accounted for in the literature by introducing principles such as the grammatical function hierarchy, the topic principle, and the quantifier hierarchy, can be accounted for by making the activation of the DRS construction rules depend on factors other than the syntactic structure of the sentence. The factors I propose to incorporate are (i) the semantics of items, (ii) the results of the interpretation of operators in context, and (iii) the way the representation of events is built in memory. In order to achieve this goal, I propose two main modifications to the standard DRS construction algorithm. First all, I propose that the input to the algorithm is a logiform—a isomorphic to the s-structure, that carries however information about the semantic interpretation of lexical items. In this way, the role of semantic factors in interpretation can be taken into account; in addition, a semantic value can be assigned to a representation containing unresolved conditions or partial hypotheses. Secondly, I propose to make the application of the DRS construction rules depend on the identification of certain contextually dependent elements of the interpretation. The ingredients of the account thus include: a proposal about the input to the model construction procedure; a notion of what an event structure is; and an account of discourse interpretation. I discuss these issues in turn in the next sections. THE LOGICAL FORM As said above, the first difference between the interpretation procedure proposed here and the DRS construction algorithm illustrated above is that the rules I propose rely on semantical and contextual factors. I propose to do this by adding to standard DRT a new class of conditions, that I call &apos;logical forms.&apos; Logical forms include semantic information about the lexical items occurring in the sentence. The logical form representation is the interface between the parser and the model construction algorithm, and can be compositionally obtained by a GPSG parser [11, 18] that couples a contextfree grammar with rules of semantic interpretation. I first describe the language used to characterize the semantics of items, Simple Episodic Logic), then the syntax and interpretation of logical forms. Lexical Semantics in Simple Episodic Logic introduce presenting the truth conditions I propose to assign to (18), repeated here for convenience: (18) Every school sent the principal to the meeting. The truth conditions usually assigned to (18) in a language with restricted quantification, and ignoring tense, are shown in (19); I propose instead to assign to (18) the interpretation specified by (20). (19) (the m mEErING(m) (the p PRiNciPAL(p,$)] sENT(s,p,m)))) (the m [Si A (V s [s*2 h scHoolN] p h PRINcEPAnp,$)] sEtsrr(s,p,m)))) reads: there exists a unique m that is a meeting in a conspecified situation for all s&apos;s that schools in a contextually specified resource situation the unique p such that p is the principal of s participates torn. NP VP AA Every kid climbed TREE(x) Km(y) every CLIMBED(y, X) (17) 81 The intent of the expression used for the quantifier restrictions in (20) is to make it explicit that the situations from which the quantified elements are &apos;picked up&apos; need not be the complete set of objects and relations at which the truth of (20) is evaluated. This is accomplished by introducing into the language an explicit relation h (&apos;supports&apos;) to represent &apos;truth at a situation&apos; [8]. A statement of the form h evaluates to true in a situation s if the object—say, m— assigned to the variable x is a meeting in the situation S1. a set of objects and facts about these objects [8, 18]. I assume a language which allows us to make statements about situations, and an ontology in which situations are objects in the universe. Episodic Logic provides such a language and such an ontology [19, 181; where not otherwise the reader should assume that an expression of has the semantics of the identical expression in Episodic Logic. The restriction of the existential quantifier in (20) cona s. are used in translate anaphoric expressions of English. A parameter behaves semantically as an open variable, a value for which has to be provided by context.&apos; I have assumed the following translations for the lexical items &amp;quot;every,&amp;quot; &amp;quot;meeting,&amp;quot; and &amp;quot;sent&amp;quot; (I have again ignored tense): &amp;quot;every&amp;quot; APAQ (V x P(x)] Q(x)) &amp;quot;meeting&amp;quot; MEETING &amp;quot;sent&amp;quot; SENT The semantics assigned to definite descriptions needs a bit an explanation. According to the theory 41 the major uses of definite NP&apos;s, as well as the contrast between definites, indefinites, and demonstratives, can be accounted for by stipulating that a speaker, when using a definite article, 1. instructs the hearer to locate the referent in some shared set of objects, and refers to the the objects/mass within this set that satisfy the restriction. I formalize this idea in [30] by associating to definite descriptions the translation below. A situation is &apos;shared&apos; between x and y if every fact &amp;quot;tIf supported by that situation is mutually believed by x and y (see [30] for details). &amp;quot;the meeting&amp;quot; A P (the x: ([..5 h MEETING(x)1 SHARED(spkr,hearer,S)) P(x)) Syntax and Interpretation of the Logical Form The translations seen above, together with the obvious context-free rules, result in the following LF for (18) (I have &apos;See [29] for details. The idea is to add to the parameters of an function that provides the values for parameters, thus plays the role of &apos;context&apos; in Heim&apos;s proposal. The reader should be aware that while the notation and terminology I have adopted is borrowed from Situation Theory, parameters have a different semantic interpretation there [8]. used here, and elsewhere in the paper, a linear notation to save space): (21) [cp [IP [NP Q (V s [.5*2 h scHoor.(s)] Q(s))] [vp &apos;SENT Q p V3 sHARED(spkr,hearer,s&apos;3) Q(P)Ail [PP &apos;TO &apos;A Q m sHARED(spicr,hearer,ssi) Q(11)Allii I propose that expressions like (21) can appear as conditions of DRSs. The syntax of LFs is as follows. Each internal node of (21) is labeled with a phrase category; the leaves are labeled with expressions of the form &apos;a, where a is an of has therefore a &apos;standard&apos; model theoretic denotation). I use the phrase structure system largely adopted in the Government and Binding literature, according to which the sentence is the maximal projection of an Infl node and is therefore labeled IP [34]. I also assume the existence of a maximal projection of complementizer CP above IP. Because I don&apos;t discuss relatives here, I use the following simplified notation for NPs with determiners, such as &amp;quot;every school&amp;quot;: Q (v x [s). scHooL(x)] LFs like (21) are usually treated in the natural language processing literature as uninterpreted data structures from which to &apos;extract&apos; the readings [16, 17]. However, it has been recently proposed [31, 2, 33] that it is possible (and indeed desirable) to assign a denotation to expressions like (21). The reason is that in this way one can define a notion of sound inference —that is, one can specify what can and cannot properly be inferred from an expression like (21) prior to disambiguation; and therefore, a notion of &apos;monotone disambiguation.&apos; I do not assume disambiguation to work monotonically, but I want to be able to treat expressions like (21) as full-fledged conditions so that a DRS containing a condition of this kind can be interpreted, and I need be able to characterize a disambiguation step as compatithe sense that it does not introduce any new readings. To do this I need LFs to have an interpretation. Were it not for the problem that more than one interprecan be associated to a single LF, one could easily define a recursive mapping EXT from logical forms to truththeoretical denotations (functions from situations to truth values) in terms of the usual II II EXT(&apos;a) = Hail a]) = EXT([VP al) = EXT(a) a]) = 13]) = EXTUT a /31) = EXT(a)(EXT((3)) if TYPE(EXT(a)) = (t1,t2) and TYPE(EXT(13)) = EXT(13)(EXT(a)) otherwise. function, as follows: 82 Once this is done, one can reformulate the semantics of DRS in terms of situations and situations extensions instead of embeddings and embedding extensions, and interpret all conditions as functions from situations to truth values. (See [29] for details.) Matters get more complicated when expressions with more than one reading like (21) are considered. Different ways for assigning a denotation to expressions with more than one interpretation have been proposed [2, 31]; my proposal derives from [31]. I use a Cooper storage mechanism [5] to define EXT in such a way as to allow for an LF to have more than one &apos;indirect interpretation.&apos; Briefly, Cooper&apos;s is to have a syntactic tree denote a set of each sequence representing a distinct &apos;order of application&apos; in computing the interpretation of the sentence. For example, because in interpreting (22) one can either apply the translation of tense immediately or wait, EXT maps (22) in a set of two sequences, shown in (23). (22) &apos;P [Np &apos;A, Q (det x R(x)) Q(x)] ] EXT((22)) = { (A x (det x R(x))P(x) ), (23) (P, A Q (det x R(x)) Q(x) ) } I omit here the definition of the EXT function implementing Cooper storage, that is rather complex. For the current purposes, it is enough to understand that EXT associates to (21) a set of functions from situations to truth values, as in (24). (24) EXT((21)) = {the function denoted by (the m [s 1=mEETING(m)] A s = SCHOOL(S)] p PiuNciPAL(p,$)] sENT(s,p,x)))) II, the function denoted by (V s h m h A p [4 etc. } Having done this, we can say that a DRS condition like (21) is verifies the current situation s if one of the functions denoted by (21) maps s into 1. BUILDING EVENT STRUCTURES Not all assertions in a narrative or conversation are going to be about the same situation. In the conversations with the TRAINS system, for example, the participants can discuss both the state of the world and the state of the plan being developed. Maintaining this separation is crucial for the proper interpretation of definite descriptions, for example. The separation between the situations that are the topic of different sentences is achieved by translating sentences as descriptions. situation description is a condition of the form: (25) S: whose intuitive interpretation is that (I) provides a partial characterization of the situation s. The semantics of situation descriptions is defined as follows, using a semantics of in terms of as discussed in the previous section, and interpreting discourse markers as constituents of situations: The condition s:K is satisfied wrt the situation s&apos; if K is satisfied wit the value assigned to s in s &apos; . I also propose the following constraint on the model construction rules: on Interpretation : the exception of the discourse markers interpreted over situations and of the situation descriptions, every discourse marker and condition has to be part of a situation descriptions. Situation descriptions are added to the model by rules triggered by an LF whose root is a CP node. The rules (now shown for lack of space) delete the complementizer and its whole projection, and introduce a situation structure. The result is shown in (26). (26) The constraint on discourse interpretation proposed above is implemented by forcing the rules that build situation structures to be triggered before any other rule; this is done by having every other rule being triggered by LFs whose root node is an IP. The result of this constraint is that a discourse model consists of a set of situation descriptions: (27) The DRSs produced by the standard DRT algorithm are semantically equivalent to the special case of a set of situation descriptions all describing the same situation s. Models like the one in (27) enable the formalization of processes of resource situation identification like that described in [30]. I illustrate how my rules for interpreting operators differ from those of standard DRT, and how the interaction between model construction rules and discourse interpretation works, by means of the model construction rule for definites. The rule MCR-DD is triggered by the configuration in (28), and results in the configuration in (29). The notation used for the pattern indicates that this rule applies to a definite NP in any position within a syntactic tree whose x S (I) s&apos;: Y &apos;If 83 maximal projection is an IP node, without any intervening W node. (29) s: The key observation is that the application of this rule, as well as of any other NP rule, depends on the hearer&apos;s previous identification of a resource situation for the definite The statement the interpretation of S is added to the situation structure by the processes that identify the referent of the definite descrip- I describe these processes in detail in Finally, I propose that, when context is missing, a default model construction procedure operates. It has been suggested [6] that the conceptualization of events follows an reflected in the hierarchy &lt; LOCA- TION, SOURCE, GOAL &lt; THEME proposed to account for phenomena like passivization [21]. Briefly, the idea is that &apos;the normal procedure for building an event description&apos; is to follow the order in the hierarchy: first identify the agent, then the location, then the theme. This proposal can be formalized in the current framework by having rules that operate in case no other rule has, and that modify the model by introducing a resource situation for an operator and establishing anchoring connections. These rules depend both on the semantics of the verb and on the syntactic configuration. The rule that identifies the AGENT, for example, is triggered by the configuration in (30), and results in the configuration in (31), that allows for the rule for the NP to operate in that the resource situation of the operator has been anchored: more conventional situation-theoretic framework is used there, but the analysis carries over to the framework in this paper. These rules can of course originate conflicts with the results of other discourse interpretation processes. I assume the following conflict resolution rule: when two rules produce conflicting hypothesis, assume the result of the more specific rule. In general, the discourse interpretation rules are more specific than the default rules for constructing events representations, so they will be preferred. Although lack of space prevents me from giving examples, rules relating the construction of the model to lexical semantics, such as those accounting for data like (12), can also be formulated. AN EXAMPLE We can now discuss in more detail the process of disambiguation of (18). I have presented the logical form for (18) above, as (21). (18) Every school sent the principal to the meeting. After identifying the situation descriptions, various interpretation processes take place, like those performing definite description interpretation described in [30]. These processes generate hypotheses about the anchoring of resource situations. Without entering into details, I assume that the confor (18) is by (32), that introduces into the P (det x QJ P(x)) s&apos;: P(y) IP XX y YY 84 model the situation description in (33), containing a group of schools and a meeting. (32) There was a meeting of the schools in the district. (33) Given this context, the discourse interpretation processes identify s as the resource situation for the NPs &amp;quot;every school&amp;quot; and &amp;quot;the meeting.&amp;quot; However, no unique principal can be identified in s. The activation of the model construction rules for universal quantification and definite descriptions in the partial model in (34), in which and s&apos;2 have been identified: (34) sl: Y s: MEETING (y) S2 S2: Z --r S:Z E S THIS_SITUATION S3 s3: IP VP VP PP NP &apos;SENT NP ----,..............„ y NP I Az_ principal&amp;quot; s2 C S3 The model construction rule applied to the universal &amp;quot;evschool&amp;quot; introduces a complex condition —4 K2 usual, but both the restriction and the nuclear scope include situation descriptions. The situation description in the reis a subsituation of the situation at which the restriction is evaluated (denoted by the indexical constant situation description in the nuclear scope, s3, is an extension of 52. Now that a situation description for the resource situation of the universal and a discourse marker for the school have been introduced (52 and z, respectively), the rules for resolving the parametric component X of the interpretation of &amp;quot;the principal&amp;quot; can apply. The result is that z is chosen as anof X, and is chosen as the resource situation for &amp;quot;the principal.&amp;quot; The model construction rule updates 53 accordingly; the resulting event structure is equivalent to the interpretation of (21) specified by (20). ACCOUNTING FOR THE DISAMBIGUATION DATA I briefly return here the disambiguation principles, to show how the proposal just presented accounts for them. First of all, I&apos;ll note that, under simple assumptions about the mapping between grammatical functions and theta-roles, there is a striking resemblance between the grammatical function hierarchy proposed by Ioup and the thematic hierarchy proposed by Jackendoff to account for facts about passives and reflexives. The facts accounted for by the grammatical function hierarchy principle can also be explained if we assume that the description of an event is constructed by identifying the filler of each thematic role in the order specified by Jackendoff&apos;s thematic hierarchy. Consider now the case of the other disambiguation factor proposed by Ioup, the lexically encoded preference for certain operators to take wide scope. Definite descriptions the paradigmatic case of an operator that tends to take wide scope. This preference can be explained in terms of the model construction hypothesis as follows. The choice of a resource situation for definite descriptions is restricted by the constraint that this resource situation be either shared among the conversational participants, or related to shared knowledge by shared relations [12, 4]. In our dialogues, for example, definite descriptions are usually interpreted with respect to the &apos;situation&apos; corresponding to the current visual scene, which is independent from other situations. It follows that a definite description will be assigned narrow scope relative to another operator only if (i) the resource situation of the definite is perceived to depend on this other resource situation, and (ii) this dependency relation is known to be shared. As for the tendency for NPs in topic to take wide scope, an element of a sentence is said to be in topic if it is considered to be part of the background information on which the new information in the sentence depends. As the interpretation of the &apos;new&apos; information in the sentence depends on the background information, it is plausible to assume that, in constructing a model for the sentence, the listener begins by applying the model construction rules for the operators perceived to be in topic (or explicitly marked as being in topic, in the case of Japanese). The interpretation of the operators not in topic, when determined at all, will depend on the interpretation of the operators in topic, resulting in the dependency relations between the related situations that I have assumed to be the way scope is represented. Finally, I&apos;ll note that, in the absence of contextual clues, a completely disambiguated event structure is actu- X S MEETING(X)</abstract>
<note confidence="0.97420408">(S) PARTICIPATE(S, X) s: 85 ally constructed depends on how strong the model construc-tion rules are supposed to be; it&apos;s perfectly possible that the activation of these rules is controlled by additional factors, such as the specific needs of a task to be performed. 86 C. H. Hwang. Logical Approach to Narrative Understandthesis, University of Alberta, 1992. ACKNOWLEDGMENTS [19] C. H. Hwang and L. K. Schubert. Episodic logic: A situ-ational logic for natural language processing. In P. Aczel, Israel, Y. Katagiri, and S. Peters, editors, Theory its Applications, v.3. 1993. To appear. I wish to thank my advisor Len Schubert and James Allen, Howard Kurtzman, Peter Lasersohn, and Uwe Reyle for sev-eral suggestions, technical help, and constructive criticism. This work was supported by the US Air Force - Rome Lab-oratory Research Contact no. F30602-91-C-0010. [20] Georgette Ioup. Some universals for quantifier scope. In Kimball, editor, Semantics 4, 37-58. Aca-demic Press, New York, 1975. References R. Jackendoff. Interpretation in Generative Gram- Press, 1972. [1] J. E Allen and L.K. Schubert. The TRAINS project. TRAINS Technical Note 91-1, University of Rochester, Department of Computer Science, 1991. P. Johnson-Laird. Models. University Press, 1983. [2] H. Alshawi and R. Crouch. Monotonic semantic interpre- In 30th. ACL, 32-39, University of Delaware, 1992. [23] H. Kamp. A theory of truth and semantic representation. J. Groenendijk, T. Janssen, and M. Stokhof, editors, For- Methods in the Study of Language.Mathematical Amsterdam, 1981. J. Barwise and J. Perry. and Attitudes. MIT Press, 1983. [24] H. Kamp and U. Reyle. From discourse to logic. To appear., 1993. [4] H. H. Clark and C. R. Marshall. Definite reference and muknowledge. In of Discourse Understanding. Cambridge University Press, 1981. [25] R. Kempson and A. Cormack. Ambiguity and quantification. and Philosophy, Robin Cooper. and Syntactic Theory. D. Rei-del, 1983. [26] H. S. Kurtzman and M. C. MacDonald. Resolution of quan-tifier scope ambiguities. To appear., April 1992. W. Croft. Categories and Grammatical Relations: cognitive organization of information. of Chicago Press, 1991. [27] G. Lakoff. Semantic interpretation in generative grammar. In A. Steinberg and L. A. Jakobovits, editors, An interdisciplinary reader in philosophy, linguistics, anthropoland psychology. University Press, 1971. Kees van Deemter. the Composition of Meaning. thesis, University of Amsterdam, 1991. R. The Grammar of Quantification. thesis, MIT, 1977. [8] K. Devlin. and Information. University Press, 1991. M. Poesio. a Scope to Operators in Dialogues. PhD thesis, University of Rochester, Department of Com-puter Science, 1993. [9] J.E. Fenstad, P.K. Halvorsen, T. Langholm, and J. van Ben- Language and Logic. 1987. [30] M. Poesio. A situation-theoretic formalization of definite description interpretation in plan elaboration dialogues. In Aczel, D. Israel, Y. Katagiri, and S. Peters, editors, Situ- Theory and its Applications, vol.3, 12, pages 343-378. CSLI, 1993. To appear. [10] A. Gamham. On-line construction of representations of the content of texts. Reproduced by Indiana University Linguis-tics Club, 1982. [31] Massimo Poesio. Relational semantics and scope ambigu-ity. In J. Barwise, J. M. Gawron, G. Plotkin, and S. Tutiya, Semantics and its Applications, vol.2, chap-ter 20, pages 469-497. CSLI, 1991. G. Gazdar, E. Klein, G. Pullum, and L Sag. Structure Grammar. 1985. T. Reinhart. and semantic interpretation. Helm, 1983. J. A. Hawkins. and Indefiniteness. Helm, 1978. [33] U. Reyle. Dealing with ambiguities by underspecification: representation and deduction. of Se- I. Heim. Semantics of Definite and Indefinite Noun thesis, University of Massachusetts at Amherst, 1982. T. Stowell. of Phrase Structure. thesis, MIT, 1981. [14] 0.0. Hendrix. Semantic aspects of translation. In D. Walker, Spoken Language, 193-226. El-sevier, 1978. [35] Kurt A. VanLehn. Determining the scope of English quan-tifiers. Technical Report AI-TR-483, Artificial Intelligence Laboratory, MIT, Cambridge, MA, 1978. [15] J. Hobbs. An improper treatment of quantification in ordinary In ACL-83, 57-63, Cambridge, MA, June 1983. [16] J. R. Hobbs and S. M. Shieber. An algorithm for generating scopings. Linguistics, 13(l-2):47-63, January-June 1987. [17] Sven Hurum. Handling scope ambiguities using domain-independent heuristics. Technical Report TR 88-12, Univer-sity of Alberta, June 1988.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F30602-91-C-0010 R Jackendoff</author>
</authors>
<date></date>
<booktitle>Semantic Interpretation in Generative GramReferences</booktitle>
<publisher>MIT Press,</publisher>
<marker>Jackendoff, </marker>
<rawString> oratory Research Contact no. F30602-91-C-0010. [21] R. Jackendoff. Semantic Interpretation in Generative GramReferences mar. MIT Press, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Allen</author>
<author>L K Schubert</author>
</authors>
<title>The TRAINS project. TRAINS [22]</title>
<date>1983</date>
<journal>Computer Science,</journal>
<tech>Technical Note 91-1,</tech>
<publisher>Harvard University Press,</publisher>
<institution>University of Rochester, Department of</institution>
<note>[23]</note>
<contexts>
<context position="4823" citStr="[1]" startWordPosition="776" endWordPosition="776">erred interpretations. However, the only answers to to this question that I have seen are based on heuristics.3 2E.g., machine translation [2]. &apos;See [17] for an example of state-of-the-art techniques 78 (5) A girl took every chemistry course. [20] (6) Each daughter of an admiral married a captain. I present in this paper an hypothesis about interpretation that accounts for facts about scope disambiguation that were previously explained in the literature by stipulating a number of unmotivated principles. The proposal developed here is being applied to develop the module of the TRAINS-93 system [1] that handles scope disambiguation and reference interpretation. The goal of the TRAINS project is to develop a conversationally proficient planning assistant. More details about the project and the work presented here can be found in [29]. SCOPE DISAMBIGUATION FACTORS Most proposals on scope disambiguation were developed to account for the general preference of the leftmost quantified phrase from taking wide scope in simple active sentences like (7): (7) Every kid climbed a tree. Lakoff [27] proposed that this preference is due to the fact that sentences are parsed from left to right; &amp;quot;every </context>
</contexts>
<marker>[1]</marker>
<rawString>J. E Allen and L.K. Schubert. The TRAINS project. TRAINS [22] P. Johnson-Laird. Mental Models. Harvard University Press, Technical Note 91-1, University of Rochester, Department of 1983. Computer Science, 1991. [23] H. Kamp. A theory of truth and semantic representation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>R Crouch</author>
</authors>
<title>Monotonic semantic interpre- In</title>
<date>1992</date>
<booktitle>In Proc. 30th. ACL,</booktitle>
<pages>32--39</pages>
<editor>J. Groenendijk, T. Janssen, and M. Stokhof, editors, Fortation.</editor>
<location>Delaware,</location>
<contexts>
<context position="4362" citStr="[2]" startWordPosition="702" endWordPosition="702"> and it has been argued that a complete disambiguation never takes place [7]. No matter what processing strategy is chosen, the question of how listeners choose one particular interpretation cannot be ignored. All experimental work done on the subject of scopal ambiguity [20, 35, 26] indicates that subjects do have preferred interpretations when confronted with tasks which require understanding. In addition, sentences like (1), (5) and (6) clearly have preferred interpretations. However, the only answers to to this question that I have seen are based on heuristics.3 2E.g., machine translation [2]. &apos;See [17] for an example of state-of-the-art techniques 78 (5) A girl took every chemistry course. [20] (6) Each daughter of an admiral married a captain. I present in this paper an hypothesis about interpretation that accounts for facts about scope disambiguation that were previously explained in the literature by stipulating a number of unmotivated principles. The proposal developed here is being applied to develop the module of the TRAINS-93 system [1] that handles scope disambiguation and reference interpretation. The goal of the TRAINS project is to develop a conversationally proficient</context>
<context position="24057" citStr="[31, 2, 33]" startWordPosition="3925" endWordPosition="3927">ted in the Government and Binding literature, according to which the sentence is the maximal projection of an Infl node and is therefore labeled IP [34]. I also assume the existence of a maximal projection of complementizer CP above IP. Because I don&apos;t discuss relatives here, I use the following simplified notation for NPs with determiners, such as &amp;quot;every school&amp;quot;: [NP Q (v x [s). scHooL(x)] Q(x))] LFs like (21) are usually treated in the natural language processing literature as uninterpreted data structures from which to &apos;extract&apos; the readings [16, 17]. However, it has been recently proposed [31, 2, 33] that it is possible (and indeed desirable) to assign a denotation to expressions like (21). The reason is that in this way one can define a notion of sound inference —that is, one can specify what can and cannot properly be inferred from an expression like (21) prior to disambiguation; and therefore, a notion of &apos;monotone disambiguation.&apos; I do not assume disambiguation to work monotonically, but I want to be able to treat expressions like (21) as full-fledged conditions so that a DRS containing a condition of this kind can be interpreted, and I need to be able to characterize a disambiguation</context>
<context position="25763" citStr="[2, 31]" startWordPosition="4215" endWordPosition="4216">(f3)) EXTUT a /31) = EXT(a)(EXT((3)) if TYPE(EXT(a)) = (t1,t2) and TYPE(EXT(13)) = EXT(13)(EXT(a)) otherwise. function, as follows: 82 Once this is done, one can reformulate the semantics of DRS in terms of situations and situations extensions instead of embeddings and embedding extensions, and interpret all conditions as functions from situations to truth values. (See [29] for details.) Matters get more complicated when expressions with more than one reading like (21) are considered. Different ways for assigning a denotation to expressions with more than one interpretation have been proposed [2, 31]; my proposal derives from [31]. I use a Cooper storage mechanism [5] to define EXT in such a way as to allow for an LF to have more than one &apos;indirect interpretation.&apos; Briefly, Cooper&apos;s idea is to have a syntactic tree denote a set of sequences, each sequence representing a distinct &apos;order of application&apos; in computing the interpretation of the sentence. For example, because in interpreting (22) one can either apply the translation of tense immediately or wait, EXT maps (22) in a set of two sequences, shown in (23). (22) [y, &apos;P [Np &apos;A, Q (det x R(x)) Q(x)] ] EXT((22)) = { (A x (det x R(x))P(x)</context>
</contexts>
<marker>[2]</marker>
<rawString>H. Alshawi and R. Crouch. Monotonic semantic interpre- In J. Groenendijk, T. Janssen, and M. Stokhof, editors, Fortation. In Proc. 30th. ACL, pages 32-39, University of mal Methods in the Study of Language.Mathematical Centre, Delaware, 1992. Amsterdam, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>J Perry</author>
</authors>
<title>Situations and Attitudes.</title>
<date>1983</date>
<booktitle>The MIT [24]</booktitle>
<publisher>Press,</publisher>
<note>From discourse to logic. To appear.,</note>
<contexts>
<context position="11172" citStr="[3, 8]" startWordPosition="1790" endWordPosition="1791">ined by the characteristics of the model construction procedure. The model being constructed, which I call event structure, consists of a set of situation descriptions, one for each operator, together with dependency relations between them. The task of the model construction procedure is to identify these situations and to establish dependency relations. The scope assigned by a hearer to an operator depends on the position of the situation associated with that operator in the event structure. For example, I propose that the scope assigned to quantifiers depends on how their resource situation [3, 8] is identified. It is well-known that a sentence like (13): (13) Everybody is asleep. is not interpreted as meaning that every single human being is asleep, but only that a certain contextually relevant subset is. The process of identifying the set of individuals over which an operator quantifies is usually called domain restriction. In the case of, say, (7) whether &amp;quot;every kid&amp;quot; or &amp;quot;a tree&amp;quot; takes wide scope depends on how the listener builds a model of the sentence. If she starts by first identifying a situation containing the group of kids that &amp;quot;every&amp;quot; is quantifying over, and then proceeds to</context>
</contexts>
<marker>[3]</marker>
<rawString>J. Barwise and J. Perry. Situations and Attitudes. The MIT [24] H. Kamp and U. Reyle. From discourse to logic. To appear., Press, 1983. 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>C R Marshall</author>
</authors>
<title>Definite reference</title>
<date>1981</date>
<booktitle>In Elements of Discourse Understanding. Linguistics and Philosophy,</booktitle>
<pages>4--2</pages>
<publisher>Cambridge University Press,</publisher>
<note>[26]</note>
<contexts>
<context position="35802" citStr="[12, 4]" startWordPosition="5882" endWordPosition="5883">by Jackendoff&apos;s thematic hierarchy. Consider now the case of the other disambiguation factor proposed by Ioup, the lexically encoded preference for certain operators to take wide scope. Definite descriptions are the paradigmatic case of an operator that tends to take wide scope. This preference can be explained in terms of the model construction hypothesis as follows. The choice of a resource situation for definite descriptions is restricted by the constraint that this resource situation be either shared among the conversational participants, or related to shared knowledge by shared relations [12, 4]. In our dialogues, for example, definite descriptions are usually interpreted with respect to the &apos;situation&apos; corresponding to the current visual scene, which is independent from other situations. It follows that a definite description will be assigned narrow scope relative to another operator only if (i) the resource situation of the definite is perceived to depend on this other resource situation, and (ii) this dependency relation is known to be shared. As for the tendency for NPs in topic to take wide scope, an element of a sentence is said to be in topic if it is considered to be part of </context>
</contexts>
<marker>[4]</marker>
<rawString>H. H. Clark and C. R. Marshall. Definite reference and mu- [25] R. Kempson and A. Cormack. Ambiguity and quantification. tual knowledge. In Elements of Discourse Understanding. Linguistics and Philosophy, 4(2):259-310,1981. Cambridge University Press, 1981. [26] H. S. Kurtzman and M. C. MacDonald. Resolution of quan-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quantification</author>
<author>Syntactic Theory D Rei-</author>
</authors>
<title>tifier scope ambiguities.</title>
<date>1992</date>
<note>To appear.,</note>
<contexts>
<context position="25832" citStr="[5]" startWordPosition="4229" endWordPosition="4229">XT(13)) = EXT(13)(EXT(a)) otherwise. function, as follows: 82 Once this is done, one can reformulate the semantics of DRS in terms of situations and situations extensions instead of embeddings and embedding extensions, and interpret all conditions as functions from situations to truth values. (See [29] for details.) Matters get more complicated when expressions with more than one reading like (21) are considered. Different ways for assigning a denotation to expressions with more than one interpretation have been proposed [2, 31]; my proposal derives from [31]. I use a Cooper storage mechanism [5] to define EXT in such a way as to allow for an LF to have more than one &apos;indirect interpretation.&apos; Briefly, Cooper&apos;s idea is to have a syntactic tree denote a set of sequences, each sequence representing a distinct &apos;order of application&apos; in computing the interpretation of the sentence. For example, because in interpreting (22) one can either apply the translation of tense immediately or wait, EXT maps (22) in a set of two sequences, shown in (23). (22) [y, &apos;P [Np &apos;A, Q (det x R(x)) Q(x)] ] EXT((22)) = { (A x (det x R(x))P(x) ), (23) (P, A Q (det x R(x)) Q(x) ) } I omit here the definition of </context>
</contexts>
<marker>[5]</marker>
<rawString>Robin Cooper. Quantification and Syntactic Theory. D. Rei- tifier scope ambiguities. To appear., April 1992. del, 1983. [27] G. Lakoff. Semantic interpretation in generative grammar. In</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Croft</author>
</authors>
<title>Syntactic Categories and Grammatical Relations:</title>
<date>1991</date>
<booktitle>University of interdisciplinary reader in philosophy, linguistics, anthropolChicago Press,</booktitle>
<editor>D. A. Steinberg and L. A. Jakobovits, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="30517" citStr="[6]" startWordPosition="5025" endWordPosition="5025">ojection is an IP node, without any intervening W node. (29) s: The key observation is that the application of this rule, as well as of any other NP rule, depends on the hearer&apos;s previous identification of a resource situation for the definite description. The statement ANCHOR(&apos;S, s&apos;) constraining the interpretation of S is added to the situation structure by the processes that identify the referent of the definite description; I describe these processes in detail in [301.8 Finally, I propose that, when context is missing, a default model construction procedure operates. It has been suggested [6] that the conceptualization of events follows an order reflected in the thematic hierarchy AGENT &lt; LOCATION, SOURCE, GOAL &lt; THEME proposed to account for phenomena like passivization [21]. Briefly, the idea is that &apos;the normal procedure for building an event description&apos; is to follow the order in the hierarchy: first identify the agent, then the location, then the theme. This proposal can be formalized in the current framework by having rules that operate in case no other rule has, and that modify the model by introducing a resource situation for an operator and establishing anchoring connecti</context>
</contexts>
<marker>[6]</marker>
<rawString>W. Croft. Syntactic Categories and Grammatical Relations: D. A. Steinberg and L. A. Jakobovits, editors, Semantics: An The cognitive organization of information. University of interdisciplinary reader in philosophy, linguistics, anthropolChicago Press, 1991. ogy, and psychology. Cambridge University Press, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>On the Composition of Meaning.</title>
<date>1991</date>
<journal>PhD</journal>
<tech>PhD thesis,</tech>
<volume>28</volume>
<publisher>Press,</publisher>
<institution>MIT, thesis, University of Amsterdam,</institution>
<note>[8]</note>
<contexts>
<context position="3835" citStr="[7]" startWordPosition="622" endWordPosition="622">agmatic &apos;strengthening&apos; [25]. A difficulty with this approach is that a vaguest reading doesn&apos;t always exist. The two readings of (4), for example, are distinct. (4) Few people speak many languages. [27] Finally, it has been proposed that the reason why listeners do not seem to have problems in processing utterances like (1) is because they do not disambiguate. They build a nondisambiguated representation of the sentence and leave the interpretation open. This strategy might be advantageous for some kinds of applications&apos; and it has been argued that a complete disambiguation never takes place [7]. No matter what processing strategy is chosen, the question of how listeners choose one particular interpretation cannot be ignored. All experimental work done on the subject of scopal ambiguity [20, 35, 26] indicates that subjects do have preferred interpretations when confronted with tasks which require understanding. In addition, sentences like (1), (5) and (6) clearly have preferred interpretations. However, the only answers to to this question that I have seen are based on heuristics.3 2E.g., machine translation [2]. &apos;See [17] for an example of state-of-the-art techniques 78 (5) A girl t</context>
</contexts>
<marker>[7]</marker>
<rawString>Kees van Deemter. On the Composition of Meaning. PhD [28] R. May. The Grammar of Quantification. PhD thesis, MIT, thesis, University of Amsterdam, 1991. 1977. &apos; [8] K. Devlin. Logic and Information. Cambridge University [29] M. Poesio. Assigning a Scope to Operators in Dialogues. Press, 1991. PhD thesis, University of Rochester, Department of Com-</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Fenstad</author>
<author>P K Halvorsen</author>
<author>T Langholm</author>
<author>J van Ben-</author>
</authors>
<title>puter Science,</title>
<date>1993</date>
<journal>them. Situations, Language</journal>
<note>[30]</note>
<marker>[9]</marker>
<rawString>J.E. Fenstad, P.K. Halvorsen, T. Langholm, and J. van Ben- puter Science, 1993. them. Situations, Language and Logic. D.Reidel, 1987. [30] M. Poesio. A situation-theoretic formalization of definite</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Gamham</author>
</authors>
<title>On-line construction of representations of the description interpretation in plan elaboration dialogues. In content of texts. Reproduced by Indiana University Linguis-</title>
<booktitle>Situtics Club, 1982. ations Theory and its Applications, vol.3, chapter 12,</booktitle>
<pages>pages</pages>
<editor>P. Aczel, D. Israel, Y. Katagiri, and S. Peters, editors,</editor>
<contexts>
<context position="10480" citStr="[10, 22]" startWordPosition="1680" endWordPosition="1681">entations of scope are mandatory processes&amp;quot; ([26], p.45).6 5Arguably, the closest thing to an explicit topic marker in English are certain uses of definite descriptions and the topicalization construction; in both cases, the topically marked NP tends to take wide scope. 6Their experiments are discussed in more detail in [29]. 79 OVERVIEW OF THE PROPOSAL Scope Disambiguation as Construction of an Event Structure It is commonly assumed in the psycholinguistic literature on sentence interpretation that hearers interpret sentences by constructing a model of the situation described by the sentence [10, 22]. I propose that the scope assigned to the operators contained in a sentence is determined by the characteristics of the model construction procedure. The model being constructed, which I call event structure, consists of a set of situation descriptions, one for each operator, together with dependency relations between them. The task of the model construction procedure is to identify these situations and to establish dependency relations. The scope assigned by a hearer to an operator depends on the position of the situation associated with that operator in the event structure. For example, I p</context>
</contexts>
<marker>[10]</marker>
<rawString>A. Gamham. On-line construction of representations of the description interpretation in plan elaboration dialogues. In content of texts. Reproduced by Indiana University Linguis- P. Aczel, D. Israel, Y. Katagiri, and S. Peters, editors, Situtics Club, 1982. ations Theory and its Applications, vol.3, chapter 12, pages</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>L Sag</author>
</authors>
<title>Generalized 343-378. CSLI,</title>
<date>1993</date>
<publisher>Blackwell,</publisher>
<note>To appear. Phrase Structure Grammar.</note>
<contexts>
<context position="18838" citStr="[11, 18]" startWordPosition="3056" endWordPosition="3057"> next sections. THE LOGICAL FORM As said above, the first difference between the interpretation procedure proposed here and the DRS construction algorithm illustrated above is that the rules I propose rely on semantical and contextual factors. I propose to do this by adding to standard DRT a new class of conditions, that I call &apos;logical forms.&apos; Logical forms include semantic information about the lexical items occurring in the sentence. The logical form representation is the interface between the parser and the model construction algorithm, and can be compositionally obtained by a GPSG parser [11, 18] that couples a contextfree grammar with rules of semantic interpretation. I first describe the language used to characterize the semantics of lexical items, SEL (for Simple Episodic Logic), then the syntax and interpretation of logical forms. Lexical Semantics in Simple Episodic Logic I introduce SEL by presenting the truth conditions I propose to assign to (18), repeated here for convenience: (18) Every school sent the principal to the meeting. The truth conditions usually assigned to (18) in a language with restricted quantification, and ignoring tense, are shown in (19); I propose instead </context>
</contexts>
<marker>[11]</marker>
<rawString>G. Gazdar, E. Klein, G. Pullum, and L Sag. Generalized 343-378. CSLI, 1993. To appear. Phrase Structure Grammar. Blackwell, 1985. [31] Massimo Poesio. Relational semantics and scope ambigu-</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hawkins</author>
</authors>
<title>Definiteness and Indefiniteness. Croom Helm, ity.</title>
<date>1978</date>
<volume>2</volume>
<pages>chap-</pages>
<editor>In J. Barwise, J. M. Gawron, G. Plotkin, and S. Tutiya,</editor>
<contexts>
<context position="35802" citStr="[12, 4]" startWordPosition="5882" endWordPosition="5883">by Jackendoff&apos;s thematic hierarchy. Consider now the case of the other disambiguation factor proposed by Ioup, the lexically encoded preference for certain operators to take wide scope. Definite descriptions are the paradigmatic case of an operator that tends to take wide scope. This preference can be explained in terms of the model construction hypothesis as follows. The choice of a resource situation for definite descriptions is restricted by the constraint that this resource situation be either shared among the conversational participants, or related to shared knowledge by shared relations [12, 4]. In our dialogues, for example, definite descriptions are usually interpreted with respect to the &apos;situation&apos; corresponding to the current visual scene, which is independent from other situations. It follows that a definite description will be assigned narrow scope relative to another operator only if (i) the resource situation of the definite is perceived to depend on this other resource situation, and (ii) this dependency relation is known to be shared. As for the tendency for NPs in topic to take wide scope, an element of a sentence is said to be in topic if it is considered to be part of </context>
</contexts>
<marker>[12]</marker>
<rawString>J. A. Hawkins. Definiteness and Indefiniteness. Croom Helm, ity. In J. Barwise, J. M. Gawron, G. Plotkin, and S. Tutiya, 1978. editors, Situation Semantics and its Applications, vol.2, chap-</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Heim</author>
</authors>
<title>The Semantics of Definite and Indefinite Noun ter 20,</title>
<date>1991</date>
<journal>Amherst,</journal>
<tech>Phrases. PhD thesis,</tech>
<volume>32</volume>
<pages>469--497</pages>
<publisher>CSLI,</publisher>
<institution>University of Massachusetts at</institution>
<location>Helm,</location>
<contexts>
<context position="2262" citStr="[13]" startWordPosition="359" endWordPosition="359">eadings. Two distinct questions thus must be answered: how can listeners (and how should machines) deal with the combinatorial explosion of readings? Do we really use the brute-force strategy of considering all of the available readings, and then choose among them? And, if we do choose among several readings, how is that done? (1) We should hook up an engine to the boxcar. To my knowledge, three positions on the problem of combinatorial explosion have been taken in the literature. Some have argued that there is no problem: our brains contain 1I use here the term operator as it is used by Heim [13], i.e., to mean either quantifier or modal/tense operator. more than enough machinery to process in parallel 4! interpretations. It&apos;s unclear, however, whether this strategy is feasible when larger numbers of readings are concerned. A classical demonstration of the number of readings one may have to consider is (2), which has 11! interpretations if the standard treatment of quantification and modality is assumed. (2) You can fool most people on most of the issues most of the time, but you can&apos;t fool everybody on every single issue all of the time. [15] Another position is that sentences like (</context>
<context position="8534" citStr="[13]" startWordPosition="1375" endWordPosition="1375"> a scope to operators. Jackendoff [21] and Reinhart ([32], ch. 3 and 9) propose to account for the preferred reading of (7) by means of a C-command principle according to which a quantified expression is allowed to take scope over another quantified expression only if the latter is c-commanded by the former at surface structure. Structural explanations (in the form of constraints on syntactic movement) have also been proposed to explain the constraint that prevents a quantifier to take scope outside the clause in which it appears, first observed by May [28] and called Scope Constraint by Heim [13]. This constraint is exemplified by the contrast in (11): whereas (11a) has a reading in which &amp;quot;every department&amp;quot; is allowed to take wide scope over &amp;quot;a student,&amp;quot; this reading is not available for (lib). (11) a. A student from every department was at the party. b. A student who was from every department was at the party. Lexical semantics and commonsense knowledge also play an important role in determining the scope of operators. The contrast between the preferred readings of (12a) and (12b) can only be explained in terms of lexical semantics: (12) a. A workstation serves many users. b. A works</context>
</contexts>
<marker>[13]</marker>
<rawString>I. Heim. The Semantics of Definite and Indefinite Noun ter 20, pages 469-497. CSLI, 1991. Phrases. PhD thesis, University of Massachusetts at Amherst, [32] T. Reinhart. Anaphora and semantic interpretation. Croom 1982. Helm, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendrix</author>
</authors>
<title>Semantic aspects of translation. In</title>
<date>1978</date>
<journal>Journal of Sesevier,</journal>
<pages>193--226</pages>
<editor>D. Walker, [33] U. Reyle.</editor>
<contexts>
<context position="7101" citStr="[14]" startWordPosition="1158" endWordPosition="1158">tion, which in turn tend to take wide scope over NPs in object position. The hierarchy between grammatical functions accounts for the preferred reading of (7). Ioup also observed that NPs in topic position tend to take wide scope. This is especially obvious in languages that have a specific grammatical category for topic, like Japanese or Korean. The Japanese sentence (10b) is ambiguous, but the reading in which the NP in subject position, &amp;quot;most students&amp;quot; takes scope over the NP in object position, &amp;quot;every language,&amp;quot; is preferred. This preference is maintained if the &apos;Van Lehn [35] and Hendrix [14] also studied the effect of lexical preferences, or &apos;strengths&apos; as they are also called. NP in object position is scrambled in sentence-initial position, as in (10c) (another counterexample to Lakoff&apos;s leftto-right principle). If, however, the NP is marked with the topic-marking suffix &amp;quot;wa,&amp;quot; as in (10d), suddenly the preferred reading of the sentence becomes the one in which &amp;quot;every language&amp;quot; takes wide scope.5 (10) a. Most students speak every language. b. Hotondo-no gakusei-ga subete-no gengo-o hanasu most-gen student-nom every language-acc speak c. Subete-no gengo-o hotondo-no galcusei-ga ha</context>
</contexts>
<marker>[14]</marker>
<rawString>0.0. Hendrix. Semantic aspects of translation. In D. Walker, [33] U. Reyle. Dealing with ambiguities by underspecification: editor, Understanding Spoken Language, pages 193-226. El- Construction, representation and deduction. Journal of Sesevier, 1978. mantics, 3,1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>An improper treatment of quantification in ordinary [34]</title>
<date>1981</date>
<booktitle>In Proc. ACL-83,</booktitle>
<tech>PhD thesis,</tech>
<pages>57--63</pages>
<institution>MIT, English.</institution>
<location>Cambridge, MA,</location>
<note>[35] Kurt</note>
<contexts>
<context position="2820" citStr="[15]" startWordPosition="451" endWordPosition="451">e the term operator as it is used by Heim [13], i.e., to mean either quantifier or modal/tense operator. more than enough machinery to process in parallel 4! interpretations. It&apos;s unclear, however, whether this strategy is feasible when larger numbers of readings are concerned. A classical demonstration of the number of readings one may have to consider is (2), which has 11! interpretations if the standard treatment of quantification and modality is assumed. (2) You can fool most people on most of the issues most of the time, but you can&apos;t fool everybody on every single issue all of the time. [15] Another position is that sentences like (1) are not semantically ambiguous, but vague. Consider for example (3): (3) Every kid climbed a tree. Here, one of the readings (the one in which the indefinite takes narrow scope) is entailed by the other (in which the indefinite takes wide scope). The claim is that (3) is interpreted in the vaguest possible way, and the strongest reading, if at all, is derived by pragmatic &apos;strengthening&apos; [25]. A difficulty with this approach is that a vaguest reading doesn&apos;t always exist. The two readings of (4), for example, are distinct. (4) Few people speak many </context>
</contexts>
<marker>[15]</marker>
<rawString>J. Hobbs. An improper treatment of quantification in ordinary [34] T. Stowell. Origins of Phrase Structure. PhD thesis, MIT, English. In Proc. ACL-83, pages 57-63, Cambridge, MA, 1981. June 1983. [35] Kurt A. VanLehn. Determining the scope of English quan-</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>S M Shieber</author>
</authors>
<title>An algorithm for generating tifiers.</title>
<date>1978</date>
<booktitle>Artificial Intelligence quantifier scopings. Computational Linguistics, 13(l-2):47- Laboratory, MIT,</booktitle>
<tech>Technical Report AI-TR-483,</tech>
<pages>63</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="24005" citStr="[16, 17]" startWordPosition="3917" endWordPosition="3918">). I use the phrase structure system largely adopted in the Government and Binding literature, according to which the sentence is the maximal projection of an Infl node and is therefore labeled IP [34]. I also assume the existence of a maximal projection of complementizer CP above IP. Because I don&apos;t discuss relatives here, I use the following simplified notation for NPs with determiners, such as &amp;quot;every school&amp;quot;: [NP Q (v x [s). scHooL(x)] Q(x))] LFs like (21) are usually treated in the natural language processing literature as uninterpreted data structures from which to &apos;extract&apos; the readings [16, 17]. However, it has been recently proposed [31, 2, 33] that it is possible (and indeed desirable) to assign a denotation to expressions like (21). The reason is that in this way one can define a notion of sound inference —that is, one can specify what can and cannot properly be inferred from an expression like (21) prior to disambiguation; and therefore, a notion of &apos;monotone disambiguation.&apos; I do not assume disambiguation to work monotonically, but I want to be able to treat expressions like (21) as full-fledged conditions so that a DRS containing a condition of this kind can be interpreted, an</context>
</contexts>
<marker>[16]</marker>
<rawString>J. R. Hobbs and S. M. Shieber. An algorithm for generating tifiers. Technical Report AI-TR-483, Artificial Intelligence quantifier scopings. Computational Linguistics, 13(l-2):47- Laboratory, MIT, Cambridge, MA, 1978. 63, January-June 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hurum</author>
</authors>
<title>Handling scope ambiguities using domainindependent heuristics.</title>
<date>1988</date>
<tech>Technical Report TR 88-12,</tech>
<institution>University of Alberta,</institution>
<contexts>
<context position="4373" citStr="[17]" startWordPosition="704" endWordPosition="704">s been argued that a complete disambiguation never takes place [7]. No matter what processing strategy is chosen, the question of how listeners choose one particular interpretation cannot be ignored. All experimental work done on the subject of scopal ambiguity [20, 35, 26] indicates that subjects do have preferred interpretations when confronted with tasks which require understanding. In addition, sentences like (1), (5) and (6) clearly have preferred interpretations. However, the only answers to to this question that I have seen are based on heuristics.3 2E.g., machine translation [2]. &apos;See [17] for an example of state-of-the-art techniques 78 (5) A girl took every chemistry course. [20] (6) Each daughter of an admiral married a captain. I present in this paper an hypothesis about interpretation that accounts for facts about scope disambiguation that were previously explained in the literature by stipulating a number of unmotivated principles. The proposal developed here is being applied to develop the module of the TRAINS-93 system [1] that handles scope disambiguation and reference interpretation. The goal of the TRAINS project is to develop a conversationally proficient planning a</context>
<context position="24005" citStr="[16, 17]" startWordPosition="3917" endWordPosition="3918">). I use the phrase structure system largely adopted in the Government and Binding literature, according to which the sentence is the maximal projection of an Infl node and is therefore labeled IP [34]. I also assume the existence of a maximal projection of complementizer CP above IP. Because I don&apos;t discuss relatives here, I use the following simplified notation for NPs with determiners, such as &amp;quot;every school&amp;quot;: [NP Q (v x [s). scHooL(x)] Q(x))] LFs like (21) are usually treated in the natural language processing literature as uninterpreted data structures from which to &apos;extract&apos; the readings [16, 17]. However, it has been recently proposed [31, 2, 33] that it is possible (and indeed desirable) to assign a denotation to expressions like (21). The reason is that in this way one can define a notion of sound inference —that is, one can specify what can and cannot properly be inferred from an expression like (21) prior to disambiguation; and therefore, a notion of &apos;monotone disambiguation.&apos; I do not assume disambiguation to work monotonically, but I want to be able to treat expressions like (21) as full-fledged conditions so that a DRS containing a condition of this kind can be interpreted, an</context>
</contexts>
<marker>[17]</marker>
<rawString>Sven Hurum. Handling scope ambiguities using domainindependent heuristics. Technical Report TR 88-12, University of Alberta, June 1988.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>