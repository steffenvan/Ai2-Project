<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.9967915">
Competitive generative models with structure learning for NLP
classification tasks
</title>
<author confidence="0.97569">
Kristina Toutanova
</author>
<affiliation confidence="0.943013">
Microsoft Research
</affiliation>
<address confidence="0.909124">
Redmond, WA
</address>
<email confidence="0.998796">
kristout@microsoft.com
</email>
<sectionHeader confidence="0.993942" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999373523809524">
In this paper we show that generative
models are competitive with and some-
times superior to discriminative models,
when both kinds of models are allowed to
learn structures that are optimal for dis-
crimination. In particular, we compare
Bayesian Networks and Conditional log-
linear models on two NLP tasks. We ob-
serve that when the structure of the gen-
erative model encodes very strong inde-
pendence assumptions (a la Naive Bayes),
a discriminative model is superior, but
when the generative model is allowed to
weaken these independence assumptions
via learning a more complex structure, it
can achieve very similar or better perfor-
mance than a corresponding discrimina-
tive model. In addition, as structure learn-
ing for generative models is far more ef-
ficient, they may be preferable for some
tasks.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939614035088">
Discriminative models have become the models
of choice for NLP tasks, because of their ability
to easily incorporate non-independent features and
to more directly optimize classification accuracy.
State of the art models for many NLP tasks are ei-
ther fully discriminative or trained using discrim-
inative reranking (Collins, 2000). These include
models for part-of-speech tagging (Toutanova et
al., 2003), semantic-role labeling (Punyakanok et
al., 2005; Pradhan et al., 2005b) and Penn Tree-
bank parsing (Charniak and Johnson, 2005).
The superiority of discriminative models has
been shown on many tasks when the discrimina-
tive and generative models use exactly the same
model structure (Klein and Manning, 2002). How-
ever, the advantage of the discriminative mod-
els can be very slight (Johnson, 2001) and for
small training set sizes generative models can
be better because they need fewer training sam-
ples to converge to the optimal parameter setting
(Ng and Jordan, 2002). Additionally, many dis-
criminative models use a generative model as a
base model and add discriminative features with
reranking (Collins, 2000; Charniak and Johnson,
2005; Roark et al., 2004), or train discriminatively
a small set of weights for features which are gener-
atively estimated probabilities (Raina et al., 2004;
Och and Ney, 2002). Therefore it is important to
study generative models and to find ways of mak-
ing them better even when they are used only as
components of discriminative models.
Generative models may often perform poorly
due to making strong independence assumptions
about the joint distribution of features and classes.
To avoid this problem, generative models for
NLP tasks have often been manually designed
to achieve an appropriate representation of the
joint distribution, such as in the parsing models of
(Collins, 1997; Charniak, 2000). This shows that
when the generative models have a good model
structure, they can perform quite well.
In this paper, we look differently at compar-
ing generative and discriminative models. We ask
the question: given the same set of input features,
what is the best a generative model can do if it is
allowed to learn an optimal structure for the joint
distribution, and what is the best a discriminative
model can do if it is also allowed to learn an op-
timal structure. That is, we do not impose any in-
dependence assumptions on the generative or dis-
criminative models and let them learn the best rep-
resentation of the data they can.
Structure learning is very efficient for genera-
tive models in the form of directed graphical mod-
els (Bayesian Networks (Pearl, 1988)), since the
optimal parameters for such models can be esti-
mated in closed form. We compare Bayesian Net-
</bodyText>
<page confidence="0.974061">
576
</page>
<note confidence="0.857605">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576–584,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998329863636364">
works with structure learning to their closely re-
lated discriminative counterpart – conditional log-
linear models with structure learning. Our condi-
tional log-linear models can also be seen as Con-
ditional Random Fields (Lafferty et al., 2001), ex-
cept we do not have a structure on the labels, but
want to learn a structure on the features.
We compare the two kinds of models on two
NLP classification tasks – prepositional phrase at-
tachment and semantic role labelling. Our re-
sults show that the generative models are compet-
itive with or better than the discriminative mod-
els. When a small set of interpolation parame-
ters for the conditional probability tables are fit
discriminatively, the resulting hybrid generative-
discriminative models perform better than the gen-
erative only models and sometimes better than the
discriminative models.
In Section 2, we describe in detail the form of
the generative and discriminative models we study
and our structure search methodology. In Section
3 we present the results of our empirical study.
</bodyText>
<sectionHeader confidence="0.823173" genericHeader="method">
2 Model Classes and Methodology
2.1 Generative Models
</sectionHeader>
<bodyText confidence="0.99998608">
In classification tasks, given a training set of in-
stances D = {[xi, yi]}, where xi are the input
features for the i-th instance, and yi is its label,
the task is to learn a classifier that predicts the la-
bels of new examples. If X is the space of inputs
and Y is the space of labels, a classifier is a func-
tion f : X —* Y. A generative model is one that
models the joint probability of inputs and labels
PD(x, y) through a distribution Pθ(x, y), depen-
dent on some parameter vector 0. The classifier
based on this generative model chooses the most
likely label given an input according to the con-
ditionalized estimated joint distribution. The pa-
rameters 0 of the fitted distribution are usually es-
timated using the maximum joint likelihood esti-
mate, possibly with a prior.
We study generative models represented as
Bayesian Networks (Pearl, 1988), because their
parameters can be estimated extremely fast as the
maximizer of the joint likelihood is the closed
form relative frequency estimate. A Bayesian Net-
work is an acyclic directed graph over a set of
nodes. For every variable Z, let Pa(Z) denote the
set of parents of Z. The structure of the Bayesian
Network encodes the following set of indepen-
</bodyText>
<figureCaption confidence="0.997596">
Figure 1: Naive Bayes Bayesian Network
</figureCaption>
<bodyText confidence="0.973203590909091">
dence assumptions: every variable is conditionally
independent of its non-descendants given its par-
ents. For example, the structure of the Bayesian
Network model in Figure 1 encodes the indepen-
dence assumption that the input features are con-
ditionally independent given the class label.
Let the input be represented as a vector of m
nominal features. We define Bayesian Networks
over the m input variables X1, X2,. .. , Xm and
the class variable Y . In all networks, we add links
from the class variable Y to all input features.
In this way we have generative models which
estimate class-specific distributions over features
P(X|Y ) and a prior over labels P(Y ). Figure 1
shows a simple Bayesian Network of this form,
which is the well-known Naive Bayes model.
A specific joint distribution for a given Bayesian
Network (BN) is given by a set of condi-
tional probability tables (CPTs) which spec-
ify the distribution over each variable given its
parents P(Z|Pa(Z)). The joint distribution
P(Z1, Z2, ... , Zm) is given by:
</bodyText>
<equation confidence="0.958692">
�P(Z1, Z2, ... , Zm) = P(Zi|Pa(Zi))
i=1...m
</equation>
<bodyText confidence="0.999973666666667">
The parameters of a Bayesian Network model
given its graph structure are the values of
the conditional probabilities P(Zi|Pa(Zi)). If
the model is trained through maximizing the
joint likelihood of the data, the optimal pa-
rameters are the relative frequency estimates:
</bodyText>
<equation confidence="0.99549">
P�(Zi = v|Pa(Zi) = u) = count(Zi=v,Pa(Zi)=~u) Here
count(Pa(Zi)=u~)
</equation>
<bodyText confidence="0.9885165">
v denotes a value of Zi and i denotes a vector of
values for the parents of Zi.
Most often smoothing is applied to avoid zero
probability estimates. A simple form of smooth-
ing is add-α smoothing which is equivalent to a
Dirichlet prior. For NLP tasks it has been shown
that other smoothing methods are far superior to
add-α smoothing – see, for example, Goodman
</bodyText>
<equation confidence="0.9868835">
X1 X2 ...... Xrn
Y
</equation>
<page confidence="0.988013">
577
</page>
<bodyText confidence="0.999514842105263">
(2001). In particular, it is important to incorpo-
rate lower-order information based on subsets of
the conditioning information. Therefore we as-
sume a structural form of the conditional proba-
bility tables which implements a more sophisti-
cated type of smoothing – interpolated Witten-Bell
(Witten and Bell, 1991). This kind of smooth-
ing has also been used in the generative parser of
(Collins, 1997) and has been shown to have a rel-
atively good performance for language modeling
(Goodman, 2001).
To describe the form of the conditional proba-
bility tables, we introduce some notation. Let Z
denote a variable in the BN and Z1, Z2,..., Zk
denote the set of its parents. The probabil-
ity P(Z = z|Z1 = z1, Z2 = z2, . . . , Zk = zk) is estimated
using Witten-Bell smoothing as follows: (below
the tuple of values z1,z2,...,zk is denoted by
z1k).
</bodyText>
<equation confidence="0.999766">
PWs(z|z1k) = λ(z1k) x P�(z|z1k) + (1 − λ(z1k)) x PWs(z|z1k−1)
</equation>
<bodyText confidence="0.981414275">
In the above equation, P� is the relative fre-
quency estimator. The recursion is ended by inter-
polating with a uniform distribution vz , where Vz
is the vocabulary of values for the prediction vari-
able Z. We determine the interpolation back-off
order by looking at the number of values of each
variable. We apply the following rule: the variable
with the highest number of values observed in the
training set is backed off first, then the variable
with the next highest number of values, and so on.
Typically, the class variable will be backed-off last
according to this rule.
In Witten-Bell smoothing, the values of the in-
terpolation coefficients are as follows: λ(z1k) =
count(z1k) The weight of the
count(z1k)+dx z:count(z,z1k)&gt;0J.
relative frequency estimate based on a given con-
text increases if the context has been seen more
often in the training data and decreases if the con-
text has been seen with more different values for
the predicted variable z.
Looking at the form of our conditional proba-
bility tables, we can see that the major parame-
ters are estimated directly based on the counts of
the events in the training data. In addition, there
are interpolation parameters (denoted by d above),
which participate in computing the interpolation
weights λ. The d parameters are hyper-parameters
and we learn them on a development set of sam-
ples. We experimented with learning a single d
parameter which is shared by all CPTs and learn-
ing multiple d parameters – one for every type of
conditioning context in every CPT – i.e., each CPT
has as many d parameters as there are back-off lev-
els.
We place some restrictions on the Bayesian Net-
works learned, for closer correspondence with the
discriminative models and for tractability: Every
input variable node has the label node as a parent,
and at most three parents per variable are allowed.
</bodyText>
<subsectionHeader confidence="0.647239">
2.1.1 Structure Search Methodology
</subsectionHeader>
<bodyText confidence="0.999995170731708">
Our structure search method differs slightly
from previously proposed methods in the literature
(Heckerman, 1999; Pernkopf and Bilmes, 2005).
The search space is defined as follows. We start
with a Bayesian Network containing only the class
variable. We denote by CHOSEN the set of vari-
ables already in the network and by REMAINING
the set of unplaced variables. Initially, only the
class variable Y is in CHOSEN and all other vari-
ables are in REMAINING. Starting from the cur-
rent BN, the set of next candidate structures is de-
fined as follows: For every unplaced variable R
in REMAINING, and for every subset Sub of size
at most two from the already placed variables in
CHOSEN, consider adding R with parents Sub UY
to the current BN. Thus the number of candidate
structures for extending a current BN is on the or-
der of m3, where m is the number of variables.
We perform a greedy search. At each step, if the
best variable B with the best set of parents Pa(B)
improves the evaluation criterion, move B from
REMAINING to CHOSEN, and continue the search
until there are no variables in REMAINING or the
evaluation criterion can not be improved.
The evaluation criterion for BNs we use is clas-
sification accuracy on a development set of sam-
ples. Thus our structure search method is dis-
criminative, in the terminology of (Grossman and
Domingos, 2004; Pernkopf and Bilmes, 2005). It
is very easy to evaluate candidate BN structures.
The main parameters in the CPTs are estimated
via the relative frequency estimator on the training
set, as discussed in the previous section. We do not
fit the hyper-parameters d during structure search.
We fit these parameters only after we have se-
lected a final BN structure. Throughout the struc-
ture search, we use a fixed value of 1 for d for all
CPTs and levels of back-off. Therefore we are us-
ing generative parameter estimation and discrimi-
native structure search. See Section 4 for discus-
sion on how this method relates to previous work.
</bodyText>
<page confidence="0.98739">
578
</page>
<bodyText confidence="0.99988847826087">
Notice that the optimal parameters of the con-
ditional probability tables of variables already in
the current BN do not change at all when a new
variable is added, thus making update very ef-
ficient. After the stopping criterion is met, the
hyper-parameters of the resulting BN are fit on
the development set. As discussed in the previ-
ous subsection, we fit either a single or multiple
hyper-parameters d. The fitting criterion for the
generative Bayesian Networks is joint likelihood
of the development set of samples with a Gaussian
prior on the values log(d). 1
Additionally, we explore fitting the hyper-
parameters of the Bayesian Networks by opti-
mizing the conditional likelihood of the develop-
ment set of samples. In this case we call the
resulting models Hybrid Bayesian Network mod-
els, since they incorporate a number of discrimi-
natively trained parameters. Hybrid models have
been proposed before and shown to perform very
competitively (Raina et al., 2004; Och and Ney,
2002). In Section 3.2 we compare generative and
hybrid Bayesian Networks.
</bodyText>
<subsectionHeader confidence="0.988599">
2.2 Discriminative Models
</subsectionHeader>
<bodyText confidence="0.998136857142857">
Discriminative models learn a conditional distri-
bution Pθ(Y  |X) or discriminant functions that
discriminate between classes. Here we concen-
trate on conditional log-linear models. A sim-
ple example of such model is logistic regression,
which directly corresponds to Naive Bayes but is
trained to maximize the conditional likelihood. 2
To describe the form of models we study, let us
introduce some notation. We represent a tuple of
nominal variables (X1,X2,... ,Xm) as a vector of
0s and 1s in the following standard way: We map
the tuple of values of nominal variables to a vector
space with dimensionality the sum of possible val-
ues of all variables. There is a single dimension in
the vector space for every value of each input vari-
able Xi. The tuple (X1,X2,... ,Xm) is mapped to
a vector which has 1s in m places, which are the
corresponding dimensions for the values of each
variable Xi. We denote this mapping by 4b.
In logistic regression, the probability of a label
Y = y given input features 4b(X1, X2, ... , Xk) =
</bodyText>
<footnote confidence="0.880527833333333">
1Since the d parameters are positive we convert the prob-
lem to unconstrained optimization over parameters -y such
that d = ery.
2Logistic regression additionally does not have the sum to
one constraint on weights but it can be shown that this does
not increase the representational power of the model.
</footnote>
<equation confidence="0.736250333333333">
Y is estimated as:
exp h wy, xi
P(y|x) = Ey, exp h wy&apos;, x-)
</equation>
<bodyText confidence="0.999989857142857">
There is a parameter vector of feature weights
2Uy for each label y. We fit the parameters of the
log-linear model by maximizing the conditional
likelihood of the training set including a gaussian
prior on the parameters. The prior has mean 0 and
variance U2. The variance is a hyper-parameter,
which we optimize on a development set.
In addition to this simple logistic regression
model, as for the generative models, we consider
models with much richer structure. We consider
more complex mappings 4b, which incorporate
conjunctions of combinations of input variables.
We restrict the number of variables in the com-
binations to three, which directly corresponds to
our limit on number of parents in the Bayesian
Network structures. This is similar to consider-
ing polynomial kernels of up to degree three, but
is more general, because, for example, we can
add only some and not all bigram conjunctions
of variables. Structure search (or feature selec-
tion) for log-linear models has been done before
e.g. (Della Pietra et al., 1997; McCallum, 2003).
We devise our structure search methodology in a
way that corresponds as closely as possible to our
structure search for Bayesian Networks. The ex-
act hypothesis space considered is defined by the
search procedure for an optimal structure we ap-
ply, which we describe next.
</bodyText>
<subsectionHeader confidence="0.368069">
2.2.1 Structure Search Methodology
</subsectionHeader>
<bodyText confidence="0.999799764705882">
We start with an initial empty feature set and a
candidate feature set consisting of all input fea-
tures: CANDIDATES={X1,X2,...,Xm}. In the
course of the search, the set CANDIDATES may
contain feature conjunctions in addition to the ini-
tial input features. After a feature is selected from
the candidates set and added to the model, the fea-
ture is removed from CANDIDATES and all con-
junctions of that feature with all input features are
added to CANDIDATES. For example, if a fea-
ture conjunction hXi1,Xi2,...,XiJ is selected, all
of its expansions of the form hXi1,Xi2,...,Xi,�,Xii,
where Xi is not in the conjunction already, are
added to CANDIDATES.
We perform a greedy search and at each step
select the feature which maximizes the evaluation
criterion, add it to the model and extend the set
</bodyText>
<page confidence="0.992416">
579
</page>
<bodyText confidence="0.999886571428571">
CANDIDATES as described above. The evaluation
criterion for selecting features is classification ac-
curacy on a development set of samples, as for the
Bayesian Network structure search.
At each step, we evaluate all candidate fea-
tures. This is computationally expensive, because
it requires iterative re-estimation. In addition to
estimating weights for the new features, we re-
estimate the old parameters, since their optimal
values change. We did not preform search for the
hyper-parameter Q when evaluating models. We fit
Q by optimizing the development set accuracy af-
ter a model was selected. Note that our feature se-
lection algorithm adds an input variable or a vari-
able conjunction with all of its possible values in a
single step of the search. Therefore we are adding
hundreds or thousands of binary features at each
step, as opposed to only one as in (Della Pietra
et al., 1997). This is why we can afford to per-
form complete re-estimation of the parameters of
the model at each step.
</bodyText>
<sectionHeader confidence="0.999903" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999579">
3.1 Problems and Datasets
</subsectionHeader>
<bodyText confidence="0.999765230769231">
We study two classification problems – preposi-
tional phrase (PP) attachment, and semantic role
labeling.
Following most of the literature on preposi-
tional phrase attachment (e.g., (Hindle and Rooth,
1993; Collins and Brooks, 1995; Vanschoen-
winkel and Manderick, 2003)), we focus on the
most common configuration that leads to ambi-
guities: V NP PP. Here, we are given a verb
phrase with a following noun phrase and a prepo-
sitional phrase. The goal is to determine if the
PP should be attached to the verb or to the ob-
ject noun phrase. For example, in the sentence:
Never [hang]v [a painting]NP [with a peg]PP, the
prepositional phrase with a peg can either modify
the verb hang or the object noun phrase a painting.
Here, clearly, with a peg modifies the verb hang.
We follow the common practice in representing
the problem using only the head words of these
constituents and of the NP inside the PP. Thus the
example sentence is represented as the following
quadruple: [v:hang n1:painting p:with n2:peg].
Thus for the PP attachment task we have binary
labels Att, and four input variables – v, n1, p, n2.
We work with the standard dataset previously
used for this task by other researchers (Ratna-
</bodyText>
<table confidence="0.996982">
Task Training Devset Test
PP 20,801 4,039 3,097
SRL 173,514 5,115 9,272
</table>
<tableCaption confidence="0.8549205">
Table 1: Data sizes for the PP attachment and SRL
tasks.
</tableCaption>
<bodyText confidence="0.999360377777778">
parkhi et al., 1994; Collins and Brooks, 1995). It is
extracted from the the Penn Treebank Wall Street
Journal data (Ratnaparkhi et al., 1994). Table 1
shows summary statistics for the dataset.
The second task we concentrate on is semantic
role labeling in the context of PropBank (Palmer
et al., 2005). The PropBank corpus annotates
phrases which fill semantic roles for verbs on top
of Penn Treebank parse trees. The annotated roles
specify agent, patient, direction, etc. The labels
for semantic roles are grouped into two groups,
core argument labels and modifier argument la-
bels, which correspond approximately to the tradi-
tional distinction between arguments and adjuncts.
There has been plenty of work on machine
learning models for semantic role labeling, start-
ing with the work of Gildea and Jurafsky (2002),
and including CoNLL shared tasks (Carreras and
M`arquez, 2005). The most successful formulation
has been as learning to classify nodes in a syn-
tactic parse tree. The possible labels are NONE,
meaning that the corresponding phrase has no se-
mantic role and the set of core and modifier la-
bels. We concentrate on the subproblem of clas-
sification for core argument nodes. The problem
is, given that a node has a core argument label, de-
cide what the correct label is. Other researchers
have also looked at this subproblem (Gildea and
Jurafsky, 2002; Toutanova et al., 2005; Pradhan et
al., 2005a; Xue and Palmer, 2004).
Many features have been proposed for build-
ing models for semantic role labeling. Initially,
7 features were proposed by (Gildea and Juraf-
sky, 2002), and all following research has used
these features and some additional ones. These
are the features we use as well. Table 2 lists the
features. State-of-the-art models for the subprob-
lem of classification of core arguments addition-
ally use other features of individual nodes (Xue
and Palmer, 2004; Pradhan et al., 2005a), as well
as global features including the labels of other
nodes in parse tree. Nevertheless it is interesting
to see how well we can do with these 7 features
only.
We use the standard training, development, and
</bodyText>
<page confidence="0.994272">
580
</page>
<table confidence="0.908391375">
Feature Types (Gildea and Jurafsky, 2002)
PHRASE TYPE: Syntactic Category of node
PREDICATE LEMMA: Stemmed Verb
PATH: Path from node to predicate
POSITION: Before or after predicate?
VOICE: Active or passive relative to predicate
HEAD WORD OF PHRASE
SUB-CAT: CFG expansion of predicate’s parent
</table>
<tableCaption confidence="0.856326">
Table 2: Features for Semantic Role Labeling.
</tableCaption>
<bodyText confidence="0.999488142857143">
test sets from the February 2004 version of Prop-
bank. The training set consists of sections 2 to 21,
the development set is from section 24, and the test
set is from section 23. The number of samples is
listed in Table 1. As we can see, the training set
size is much larger compared to the PP attachment
training set.
</bodyText>
<subsectionHeader confidence="0.730926">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.99998703030303">
In line with previous work (Ng and Jordan, 2002;
Klein and Manning, 2002), we first compare Naive
Bayes and Logistic regression on the two NLP
tasks. This lets us see how they compare when the
generative model is making strong independence
assumptions and when the two kinds of models
have the same structure. Then we compare the
generative and discriminative models with learned
richer structures.
Table 3 shows the Naive Bayes/Logistic re-
gression results for PP attachment. We list re-
sults for several conditions of training the Naive
Bayes classifier, depending on whether it is trained
as strictly generative or as a hybrid model, and
whether a single or multiple hyper-parameters d
are trained. In the table, we see results for gen-
erative Naive Bayes, where the d parameters are
trained to maximize the joint likelihood of the de-
velopment set, and for Hybrid Naive Bayes, where
the hyper-parameters are trained to optimize the
conditional likelihood. The column H-Params (for
hyper-parameters) indicates whether a single or
multiple d parameters are learned.
Logistic regression is more fairly comparable
to Naive Bayes trained using a single hyper-
parameter, because it also uses a single hyper-
parameter Q trained on a development set. How-
ever, for the generative model it is very easy to
train multiple weights d since the likelihood of a
development set is differentiable with respect to
the parameters. For logistic regression, we may
want to choose different variances for the differ-
ent types of features but the search would be pro-
</bodyText>
<table confidence="0.999778">
Model H-params Test set acc
Naive Bayes 1 81.2
Naive Bayes 9 81.2
Logistic regression 1 82.6
Hybrid Naive Bayes 1 81.2
Hybrid Naive Bayes 9 81.5
</table>
<tableCaption confidence="0.970764">
Table 3: Naive Bayes and Logistic regression PP
attachment results.
</tableCaption>
<bodyText confidence="0.996956651162791">
hibitively expensive. Thus we think it is also fair
to fit multiple interpolation weights for the gener-
ative model and we show these results as well.
As we can see from the table, logistic regression
outperforms both Naive Bayes and Hybrid Naive
Bayes. The performance of Hybrid Naive Bayes
with multiple interpolation weights improves the
accuracy, but performance is still better for logis-
tic regression. This suggests that the strong in-
dependence assumptions are hurting the classifier.
According to McNemar’s test, logistic regression
is statistically significantly better than the Naive
Bayes models and than Hybrid Naive Bayes with a
single interpolation weight (p &lt; 0.025), but is not
significantly better than Hybrid Naive Bayes with
multiple interpolation parameters at level 0.05.
However, when both the generative and dis-
criminative models are allowed to learn optimal
structures, the generative model outperforms the
discriminative model. As seen from Table 4,
the Bayesian Network with a single interpolation
weight achieves an accuracy of 84.6%, whereas
the discriminative model performs at 83.8%. The
hybrid model with a single interpolation weight
does even better, achieving 85.0% accuracy. For
comparison, the model of Collins &amp; Brooks has
accuracy of 84.15% on this test set, and the high-
est result obtained through a discriminative model
with this feature set is 84.8%, using SVMs and a
polynomial kernel with multiple hyper-parameters
(Vanschoenwinkel and Manderick, 2003). The
Hybrid Bayes Nets are statistically significantly
better than the Log-linear model (p &lt; 0.05), and
the Bayes Nets are not significantly better than the
Log-linear model. All models from Table 4 are
significantly better than all models in Table 3.
For semantic role labelling classification of core
arguments, the results are listed in Tables 5 and
6. We can see that the difference in performance
between Naive Bayes with a single interpolation
parameter d – 83.3% and the performance of Lo-
gistic regression – 91.1%, is very large. This
shows that the independence assumptions are quite
</bodyText>
<page confidence="0.994072">
581
</page>
<table confidence="0.999838666666667">
Model H-params Test set acc
Bayes Net 1 84.6
Bayes Net 13 84.6
Log-linear model 1 83.8
Hybrid Bayes Net 1 85.0
Hybrid Bayes Net 13 84.8
</table>
<tableCaption confidence="0.961045">
Table 4: Bayesian Network and Conditional log-
linear model PP attachment results.
</tableCaption>
<table confidence="0.999924333333333">
Model H-params Test set acc
Naive Bayes 1 83.3
Naive Bayes 15 85.2
Logistic regression 1 91.1
Hybrid Naive Bayes 1 84.1
Hybrid Naive Bayes 15 86.5
</table>
<tableCaption confidence="0.9660605">
Table 5: Naive Bayes and Logistic regression SRL
classificaion results.
</tableCaption>
<bodyText confidence="0.976966514285714">
strong, and since many of the features are not
sparse lexical features and training data for them
is sufficient, the Naive Bayes model has no ad-
vantage over the discriminative logistic regression
model. The Hybrid Naive Bayes model with mul-
tiple interpolation weights does better than Naive
Bayes, performing at 86.5%. All differences be-
tween the classifiers in Table 5 are statistically sig-
nificant at level 0.01. Compared to the PP attach-
ment task, here we are getting more benefit from
multiple hyper-parameters, perhaps due to the di-
versity of the features for SRL: In SRL, we use
both sparse lexical features and non-sparse syntac-
tic ones, whereas all features for PP attachment are
lexical.
From Table 6 we can see that when we com-
pare general Bayesian Network structures to gen-
eral log-linear models, the performance gap be-
tween the generative and discriminative models
is much smaller. The Bayesian Network with a
single interpolation weight d has 93.5% accuracy
and the log-linear model has 93.9% accuracy. The
hybrid model with multiple interpolation weights
performs at 93.7%. All models in Table 6 are in
a statistical tie according to McNemar’s test, and
thus the log-linear model is not significantly bet-
ter than the Bayes Net models. We can see that
the generative model was able to learn a structure
with a set of independence assumptions which are
not as strong as the ones the Naive Bayes model
makes, thus resulting in a model with performance
competitive with the discriminative model.
Figures 2(a) and 2(b) show the Bayesian Net-
works learned for PP Attachment and Semantic
Role Labeling. Table 7 shows the conjunctions
</bodyText>
<table confidence="0.999736">
Model H-params Test set acc
Bayes Net 1 93.5
Bayes Net 20 93.6
Log-linear model 1 93.9
Hybrid Bayes Net 1 93.5
Hybrid Bayes Net 20 93.7
</table>
<tableCaption confidence="0.7384955">
Table 6: Bayesian Network and Conditional log-
linear model SRL classification results.
</tableCaption>
<table confidence="0.975123875">
PP Attachment Model
(P), (P,V), (P,N1), (P,N2)
(N1),(V), (P,N1,N2)
SRL Model
(PATH), (PATH,PLEMMA),(SUB-CAT),(PLEMMA)
(HW,PLEMMA),(PATH,PLEMMA,VOICE)
,(HW,PLEMMA,PTYPE),(SUB-CAT,PLEMMA)
(SUB-CAT,PLEMMA,POS),(HW)
</table>
<tableCaption confidence="0.9199045">
Table 7: Log-linear models learned for PP attach-
ment and SRL.
</tableCaption>
<bodyText confidence="0.995880826086957">
learned by the Log-linear models for PP attach-
ment and SRL.
We should note that it is much faster to do
structure search for the generative Bayesian Net-
work model, as compared to structure search for
the log-linear model. In our implementation, we
did not do any computation reuse between succes-
sive steps of structure search for the Bayesian Net-
work or log-linear models. Structure search took 2
hours for the Bayesian Network and 24 hours for
the log-linear model.
To put our results in the context of previous
work, other results on core arguments using the
same input features have been reported, the best
being 91.4% for an SVM with a degree 2 poly-
nomial kernel (Pradhan et al., 2005a).3 The
highest reported result for independent classifica-
tion of core arguments is 96.0% for a log-linear
model using more than 20 additional basic features
(Toutanova et al., 2005). Therefore our resulting
models with 93.5% and 93.9% accuracy compare
favorably to the SVM model with polynomial ker-
nel and show the importance of structure learning.
</bodyText>
<sectionHeader confidence="0.964709" genericHeader="conclusions">
4 Comparison to Related Work
</sectionHeader>
<bodyText confidence="0.975">
Previous work has compared generative and dis-
criminative models having the same structure,
such as the Naive Bayes and Logistic regression
models (Ng and Jordan, 2002; Klein and Man-
ning, 2002) and other models (Klein and Manning,
2002; Johnson, 2001).
</bodyText>
<footnote confidence="0.9836675">
3This result is on an older version of Propbank from July
2002.
</footnote>
<page confidence="0.992312">
582
</page>
<figure confidence="0.996974">
(a) Learned Bayesian Network
for PP attachment.
(b) Learned Bayesian Network
for SRL.
</figure>
<figureCaption confidence="0.9950465">
Figure 2: Learned Bayesian Network structures
for PP attachment and SRL.
</figureCaption>
<bodyText confidence="0.999797207317074">
Bayesian Networks with special structure of the
CPTs – e.g. decision trees, have been previously
studied in e.g. (Friedman and Goldszmidt, 1996),
but not for NLP tasks and not in comparison to dis-
criminative models. Studies comparing generative
and discriminative models with structure learn-
ing have been previously performed ((Pernkopf
and Bilmes, 2005) and (Grossman and Domingos,
2004)) for other, non-NLP domains. There are
several important algorithmic differences between
our work and that of (Pernkopf and Bilmes, 2005;
Grossman and Domingos, 2004). We detail the
differences here and perform an empirical evalua-
tion of the impact of some of these differences.
Form of the generative models. The genera-
tive models studied in that previous work do not
employ any special form of the conditional prob-
ability tables. Pernkopf and Bilmes (2005) use a
simple smoothing method: fixing the probability
of every event that has a zero relative frequency
estimate to a small fixed c. Thus the model does
not take into account information from lower or-
der distributions and has no hyper-parameters that
are being fit. Grossman and Domingos (2004) do
not employ a special form of the CPTs either and
do not mention any kind of smoothing used in the
generative model learning.
Form of the discriminative models. The
works (Pernkopf and Bilmes, 2005; Grossman
and Domingos, 2004) study Bayesian Networks
whose parameters are trained discriminatively (by
maximizing conditional likelihood), as represen-
tatives of discriminative models. We study more
general log-linear models, equivalent to Markov
Random Fields. Our models are more general
in that their parameters do not need to be inter-
pretable as probabilities (sum to 1 and between 0
and 1), and the structures do not need to corre-
spond to Bayes Net structures. For discriminative
classifiers, it is not important that their compo-
nent parameters be interpretable as probabilities;
thus this restriction is probably unnecessary. Like
for the generative models, another major differ-
ence is in the smoothing algorithms. We smooth
the models both by fitting a gaussian prior hyper-
parameter and by incorporating features of subsets
of cliques. Smoothing in (Pernkopf and Bilmes,
2005) is done by substituting zero-valued param-
eters with a small fixed c. Grossman and Domin-
gos (2004) employ early stopping using held-out
data which can achieve similar effects to smooth-
ing with a gaussian prior.
To evaluate the importance of the differences
between our algorithm and the ones presented in
these works, and to evaluate the importance of fit-
ting hyper-parameters for smoothing, we imple-
mented a modified version of our structure search.
The modifications were as follows. For Bayes
Net structure learning: (i) no Witten-Bell smooth-
ing is employed in the CPTs, and (ii) no backoffs
to lower-order distributions are considered. The
only smoothing remaining in the CPTs is an inter-
polation with a uniform distribution with a fixed
weight of a = .1. For discriminative log-linear
model structure learning: (i) the gaussian prior
was fixed to be very weak, serving only to keep the
weights away from infinity (a = 100) and (ii) the
conjunction selection was restricted to correspond
to a Bayes Net structure with no features for sub-
sets of feature conjunctions. Thus the only differ-
ence between the class of our modified discrimina-
tive log-linear models and the class of models con-
sidered in (Pernkopf and Bilmes, 2005; Grossman
and Domingos, 2004) is that we do not restrict the
parameters to be interpretable as probabilities.
The results shown in Table 8 summarize the re-
sults obtained by the modified algorithm on the
two tasks. Both the generative and discriminative
learners suffered a statistically significant (at level
.01) loss in performance. Notably, the log-linear
model for PP attachment performs worse than lo-
gistic regression with better smoothing.
</bodyText>
<figure confidence="0.993313272727273">
N1
Att
V
P
N2
PLem HW
Sub- Path
cat
Voice
Role
Pos
</figure>
<page confidence="0.993747">
583
</page>
<table confidence="0.999445625">
PP Attachment Results
Model H-params Test set acc
Bayes Net 0 82.8
Log-linear model 0 81.2
SRL Classification Results
Model H-params Test set acc
Bayes Net 0 92.5
Log-linear model 0 92.7
</table>
<tableCaption confidence="0.875906">
Table 8: Bayesian Network and Conditional log-
linear model: PP &amp; SRL classification results us-
</tableCaption>
<bodyText confidence="0.915539444444444">
ing minimal smoothing and no backoff to lower
order distributions.
In summary, our results showed that by learning
the structure for generative models, we can obtain
models which are competitive with or better than
corresponding discriminative models. We also
showed the importance of employing sophisti-
cated smoothing techniques in structure search al-
gorithms for natural language classification tasks.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999186650485437">
Xavier Carreras and Luis M`arquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. In
Proceedings ofACL.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings ofNAACL, pages 132–139.
Michael Collins and James Brooks. 1995. Prepositional at-
tachment through a backed-off model. In Proceedings of
the Third Workshop on Very Large Corpora, pages 27–38.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings ofACL, pages 16 –
23.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of ICML, pages 175–
182.
Stephen Della Pietra, Vincent J. Della Pietra, and John D.
Lafferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19(4):380–393.
Nir Friedman and Moises Goldszmidt. 1996. Learning
Bayesian networks with local structure. In Proceeding of
UAI, pages 252–262.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3):245–
288.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. In MSR Technical Report MSR-TR-2001-72.
Daniel Grossman and Pedro Domingos. 2004. Learning
bayesian network classifiers by maximizing conditional
likelihood. In Proceedings ofICML, pages 361—368.
David Heckerman. 1999. A tutorial on learning with bayesian
networks. In Learning in Graphical Models. MIT Press.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103–120.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proceedings ofACL.
Dan Klein and Christopher Manning. 2002. Conditional
structure versus conditional estimation in NLP models. In
Proceedings of EMNLP.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. 18th In-
ternational Conf. on Machine Learning, pages 282–289.
Morgan Kaufmann, San Francisco, CA.
Andrew McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI.
Andrew Ng and Michael Jordan. 2002. On discriminative vs.
generative classifiers: A comparison of logistic regression
and Naive Bayes. In NIPS 14.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings ofACL, pages 295–302.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic roles.
Computational Linguistics.
Judea Pearl. 1988. Probabilistic reasoning in intelligent
systems: Networks ofplausible inference. Morgan Kauf-
mann.
Franz Pernkopf and Jeff Bilmes. 2005. Discriminative versus
generative parameter and structure learning of bayesian
network classifiers. In Proceedings ofICML.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James Martin, and Dan Jurafsky. 2005a. Support
vector learning for semantic argument classification. Ma-
chine Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2005b. Semantic role labeling
using different syntactic views. In Proceedings ofACL.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005. The
necessity of syntactic parsing for semantic role labeling.
In Proceedings ofIJCAI.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid genera-
tive/discriminative models. In Sebastian Thrun, Lawrence
Saul, and Bernhard Sch¨olkopf, editors, Advances in Neu-
ral Information Processing Systems 16. MIT Press, Cam-
bridge, MA.
Adwait Ratnaparkhi, JeffReynar, and Salim Roukos. 1994. A
maximum entropy model for prepositional phrase attach-
ment. In Workshop on Human Language Technology.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling with
conditional random fields and the perceptron algorithm.
In Proceedings ofACL.
Kristina Toutanova, Dan Klein, and Christopher D. Manning.
2003. Feature-rich part-of-speech tagging with a cyclic
dependency network. In Proceedings of HLT-NAACL.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings ofACL.
Bram Vanschoenwinkel and Bernard Manderick. 2003. A
weighted polynomial information gain kernel for resolv-
ing prepositional phrase attachment ambiguities with sup-
port vector machines. In IJCAI.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE Transactions on Infor-
mation Theory, 37,4:1085–1094.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP.
</reference>
<page confidence="0.998578">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476144">
<title confidence="0.9966475">Competitive generative models with structure learning for classification tasks</title>
<author confidence="0.971883">Kristina</author>
<affiliation confidence="0.912835">Microsoft</affiliation>
<address confidence="0.550051">Redmond,</address>
<email confidence="0.999894">kristout@microsoft.com</email>
<abstract confidence="0.997039590909091">In this paper we show that generative models are competitive with and sometimes superior to discriminative models, when both kinds of models are allowed to learn structures that are optimal for discrimination. In particular, we compare Bayesian Networks and Conditional loglinear models on two NLP tasks. We observe that when the structure of the generative model encodes very strong indeassumptions la Bayes), a discriminative model is superior, but when the generative model is allowed to weaken these independence assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Luis M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1509" citStr="Charniak and Johnson, 2005" startWordPosition="224" endWordPosition="227"> generative models is far more efficient, they may be preferable for some tasks. 1 Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and J</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2819" citStr="Charniak, 2000" startWordPosition="433" endWordPosition="434">s which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. In this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best repr</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<contexts>
<context position="18631" citStr="Collins and Brooks, 1995" startWordPosition="3105" endWordPosition="3108">adds an input variable or a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Experiments 3.1 Problems and Datasets We study two classification problems – prepositional phrase (PP) attachment, and semantic role labeling. Following most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to determine if the PP should be attached to the verb or to the object noun phrase. For example, in the sentence: Never [hang]v [a painting]NP [with a peg]PP, the prepositional phrase with a peg can either modify the verb hang or the object noun phrase a painting. Here, clearly, with a peg modifies the verb hang. We follow the common practice in representing the problem using </context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional attachment through a backed-off model. In Proceedings of the Third Workshop on Very Large Corpora, pages 27–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2802" citStr="Collins, 1997" startWordPosition="431" endWordPosition="432">hts for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. In this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them le</context>
<context position="8389" citStr="Collins, 1997" startWordPosition="1372" endWordPosition="1373">oothing is add-α smoothing which is equivalent to a Dirichlet prior. For NLP tasks it has been shown that other smoothing methods are far superior to add-α smoothing – see, for example, Goodman X1 X2 ...... Xrn Y 577 (2001). In particular, it is important to incorporate lower-order information based on subsets of the conditioning information. Therefore we assume a structural form of the conditional probability tables which implements a more sophisticated type of smoothing – interpolated Witten-Bell (Witten and Bell, 1991). This kind of smoothing has also been used in the generative parser of (Collins, 1997) and has been shown to have a relatively good performance for language modeling (Goodman, 2001). To describe the form of the conditional probability tables, we introduce some notation. Let Z denote a variable in the BN and Z1, Z2,..., Zk denote the set of its parents. The probability P(Z = z|Z1 = z1, Z2 = z2, . . . , Zk = zk) is estimated using Witten-Bell smoothing as follows: (below the tuple of values z1,z2,...,zk is denoted by z1k). PWs(z|z1k) = λ(z1k) x P�(z|z1k) + (1 − λ(z1k)) x PWs(z|z1k−1) In the above equation, P� is the relative frequency estimator. The recursion is ended by interpol</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings ofACL, pages 16 – 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1307" citStr="Collins, 2000" startWordPosition="197" endWordPosition="198">ndence assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the opti</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John D Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="16245" citStr="Pietra et al., 1997" startWordPosition="2714" endWordPosition="2717">odels, we consider models with much richer structure. We consider more complex mappings 4b, which incorporate conjunctions of combinations of input variables. We restrict the number of variables in the combinations to three, which directly corresponds to our limit on number of parents in the Bayesian Network structures. This is similar to considering polynomial kernels of up to degree three, but is more general, because, for example, we can add only some and not all bigram conjunctions of variables. Structure search (or feature selection) for log-linear models has been done before e.g. (Della Pietra et al., 1997; McCallum, 2003). We devise our structure search methodology in a way that corresponds as closely as possible to our structure search for Bayesian Networks. The exact hypothesis space considered is defined by the search procedure for an optimal structure we apply, which we describe next. 2.2.1 Structure Search Methodology We start with an initial empty feature set and a candidate feature set consisting of all input features: CANDIDATES={X1,X2,...,Xm}. In the course of the search, the set CANDIDATES may contain feature conjunctions in addition to the initial input features. After a feature is </context>
<context position="18255" citStr="Pietra et al., 1997" startWordPosition="3045" endWordPosition="3048">ve, because it requires iterative re-estimation. In addition to estimating weights for the new features, we reestimate the old parameters, since their optimal values change. We did not preform search for the hyper-parameter Q when evaluating models. We fit Q by optimizing the development set accuracy after a model was selected. Note that our feature selection algorithm adds an input variable or a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Experiments 3.1 Problems and Datasets We study two classification problems – prepositional phrase (PP) attachment, and semantic role labeling. Following most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to </context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent J. Della Pietra, and John D. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nir Friedman</author>
<author>Moises Goldszmidt</author>
</authors>
<title>Learning Bayesian networks with local structure.</title>
<date>1996</date>
<booktitle>In Proceeding of UAI,</booktitle>
<pages>252--262</pages>
<contexts>
<context position="30623" citStr="Friedman and Goldszmidt, 1996" startWordPosition="5070" endWordPosition="5073">d Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SRL. Figure 2: Learned Bayesian Network structures for PP attachment and SRL. Bayesian Networks with special structure of the CPTs – e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure learning have been previously performed ((Pernkopf and Bilmes, 2005) and (Grossman and Domingos, 2004)) for other, non-NLP domains. There are several important algorithmic differences between our work and that of (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004). We detail the differences here and perform an empirical evaluation of the impact of some of these differences. Form of the generative models. The generative models studied in that previou</context>
</contexts>
<marker>Friedman, Goldszmidt, 1996</marker>
<rawString>Nir Friedman and Moises Goldszmidt. 1996. Learning Bayesian networks with local structure. In Proceeding of UAI, pages 252–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>288</pages>
<contexts>
<context position="20543" citStr="Gildea and Jurafsky (2002)" startWordPosition="3430" endWordPosition="3433">et. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts. There has been plenty of work on machine learning models for semantic role labeling, starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue an</context>
<context position="21886" citStr="Gildea and Jurafsky, 2002" startWordPosition="3655" endWordPosition="3658">es were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. These are the features we use as well. Table 2 lists the features. State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al., 2005a), as well as global features including the labels of other nodes in parse tree. Nevertheless it is interesting to see how well we can do with these 7 features only. We use the standard training, development, and 580 Feature Types (Gildea and Jurafsky, 2002) PHRASE TYPE: Syntactic Category of node PREDICATE LEMMA: Stemmed Verb PATH: Path from node to predicate POSITION: Before or after predicate? VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicate’s parent Table 2: Features for Semantic Role Labeling. test sets from the February 2004 version of Propbank. The training set consists of sections 2 to 21, the development set is from section 24, and the test set is from section 23. The number of samples is listed in Table 1. As we can see, the training set size is much larger compared to the PP attachm</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<booktitle>In MSR</booktitle>
<tech>Technical Report MSR-TR-2001-72.</tech>
<contexts>
<context position="8484" citStr="Goodman, 2001" startWordPosition="1388" endWordPosition="1389">shown that other smoothing methods are far superior to add-α smoothing – see, for example, Goodman X1 X2 ...... Xrn Y 577 (2001). In particular, it is important to incorporate lower-order information based on subsets of the conditioning information. Therefore we assume a structural form of the conditional probability tables which implements a more sophisticated type of smoothing – interpolated Witten-Bell (Witten and Bell, 1991). This kind of smoothing has also been used in the generative parser of (Collins, 1997) and has been shown to have a relatively good performance for language modeling (Goodman, 2001). To describe the form of the conditional probability tables, we introduce some notation. Let Z denote a variable in the BN and Z1, Z2,..., Zk denote the set of its parents. The probability P(Z = z|Z1 = z1, Z2 = z2, . . . , Zk = zk) is estimated using Witten-Bell smoothing as follows: (below the tuple of values z1,z2,...,zk is denoted by z1k). PWs(z|z1k) = λ(z1k) x P�(z|z1k) + (1 − λ(z1k)) x PWs(z|z1k−1) In the above equation, P� is the relative frequency estimator. The recursion is ended by interpolating with a uniform distribution vz , where Vz is the vocabulary of values for the prediction </context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling. In MSR Technical Report MSR-TR-2001-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Grossman</author>
<author>Pedro Domingos</author>
</authors>
<title>Learning bayesian network classifiers by maximizing conditional likelihood.</title>
<date>2004</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>361--368</pages>
<contexts>
<context position="12120" citStr="Grossman and Domingos, 2004" startWordPosition="2017" endWordPosition="2020">the current BN. Thus the number of candidate structures for extending a current BN is on the order of m3, where m is the number of variables. We perform a greedy search. At each step, if the best variable B with the best set of parents Pa(B) improves the evaluation criterion, move B from REMAINING to CHOSEN, and continue the search until there are no variables in REMAINING or the evaluation criterion can not be improved. The evaluation criterion for BNs we use is classification accuracy on a development set of samples. Thus our structure search method is discriminative, in the terminology of (Grossman and Domingos, 2004; Pernkopf and Bilmes, 2005). It is very easy to evaluate candidate BN structures. The main parameters in the CPTs are estimated via the relative frequency estimator on the training set, as discussed in the previous section. We do not fit the hyper-parameters d during structure search. We fit these parameters only after we have selected a final BN structure. Throughout the structure search, we use a fixed value of 1 for d for all CPTs and levels of back-off. Therefore we are using generative parameter estimation and discriminative structure search. See Section 4 for discussion on how this meth</context>
<context position="30867" citStr="Grossman and Domingos, 2004" startWordPosition="5106" endWordPosition="5109">nson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SRL. Figure 2: Learned Bayesian Network structures for PP attachment and SRL. Bayesian Networks with special structure of the CPTs – e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure learning have been previously performed ((Pernkopf and Bilmes, 2005) and (Grossman and Domingos, 2004)) for other, non-NLP domains. There are several important algorithmic differences between our work and that of (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004). We detail the differences here and perform an empirical evaluation of the impact of some of these differences. Form of the generative models. The generative models studied in that previous work do not employ any special form of the conditional probability tables. Pernkopf and Bilmes (2005) use a simple smoothing method: fixing the probability of every event that has a zero relative frequency estimate to a small fixed c. Thus th</context>
<context position="32803" citStr="Grossman and Domingos (2004)" startWordPosition="5413" endWordPosition="5417">babilities (sum to 1 and between 0 and 1), and the structures do not need to correspond to Bayes Net structures. For discriminative classifiers, it is not important that their component parameters be interpretable as probabilities; thus this restriction is probably unnecessary. Like for the generative models, another major difference is in the smoothing algorithms. We smooth the models both by fitting a gaussian prior hyperparameter and by incorporating features of subsets of cliques. Smoothing in (Pernkopf and Bilmes, 2005) is done by substituting zero-valued parameters with a small fixed c. Grossman and Domingos (2004) employ early stopping using held-out data which can achieve similar effects to smoothing with a gaussian prior. To evaluate the importance of the differences between our algorithm and the ones presented in these works, and to evaluate the importance of fitting hyper-parameters for smoothing, we implemented a modified version of our structure search. The modifications were as follows. For Bayes Net structure learning: (i) no Witten-Bell smoothing is employed in the CPTs, and (ii) no backoffs to lower-order distributions are considered. The only smoothing remaining in the CPTs is an interpolati</context>
</contexts>
<marker>Grossman, Domingos, 2004</marker>
<rawString>Daniel Grossman and Pedro Domingos. 2004. Learning bayesian network classifiers by maximizing conditional likelihood. In Proceedings ofICML, pages 361—368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Heckerman</author>
</authors>
<title>A tutorial on learning with bayesian networks.</title>
<date>1999</date>
<booktitle>In Learning in Graphical Models.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10885" citStr="Heckerman, 1999" startWordPosition="1799" endWordPosition="1800">ing a single d parameter which is shared by all CPTs and learning multiple d parameters – one for every type of conditioning context in every CPT – i.e., each CPT has as many d parameters as there are back-off levels. We place some restrictions on the Bayesian Networks learned, for closer correspondence with the discriminative models and for tractability: Every input variable node has the label node as a parent, and at most three parents per variable are allowed. 2.1.1 Structure Search Methodology Our structure search method differs slightly from previously proposed methods in the literature (Heckerman, 1999; Pernkopf and Bilmes, 2005). The search space is defined as follows. We start with a Bayesian Network containing only the class variable. We denote by CHOSEN the set of variables already in the network and by REMAINING the set of unplaced variables. Initially, only the class variable Y is in CHOSEN and all other variables are in REMAINING. Starting from the current BN, the set of next candidate structures is defined as follows: For every unplaced variable R in REMAINING, and for every subset Sub of size at most two from the already placed variables in CHOSEN, consider adding R with parents Su</context>
</contexts>
<marker>Heckerman, 1999</marker>
<rawString>David Heckerman. 1999. A tutorial on learning with bayesian networks. In Learning in Graphical Models. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="18605" citStr="Hindle and Rooth, 1993" startWordPosition="3101" endWordPosition="3104">ure selection algorithm adds an input variable or a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Experiments 3.1 Problems and Datasets We study two classification problems – prepositional phrase (PP) attachment, and semantic role labeling. Following most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to determine if the PP should be attached to the verb or to the object noun phrase. For example, in the sentence: Never [hang]v [a painting]NP [with a peg]PP, the prepositional phrase with a peg can either modify the verb hang or the object noun phrase a painting. Here, clearly, with a peg modifies the verb hang. We follow the common practice in repre</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Joint and conditional estimation of tagging and parsing models.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1777" citStr="Johnson, 2001" startWordPosition="269" endWordPosition="270">n accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making the</context>
<context position="30250" citStr="Johnson, 2001" startWordPosition="5013" endWordPosition="5014"> reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SRL. Figure 2: Learned Bayesian Network structures for PP attachment and SRL. Bayesian Networks with special structure of the CPTs – e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure learning have been previously performed ((Pernkopf and Bilmes, 2005) and (Grossman an</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>Mark Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in NLP models.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1689" citStr="Klein and Manning, 2002" startWordPosition="252" endWordPosition="255">ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 20</context>
<context position="22589" citStr="Klein and Manning, 2002" startWordPosition="3776" endWordPosition="3779">th from node to predicate POSITION: Before or after predicate? VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicate’s parent Table 2: Features for Semantic Role Labeling. test sets from the February 2004 version of Propbank. The training set consists of sections 2 to 21, the development set is from section 24, and the test set is from section 23. The number of samples is listed in Table 1. As we can see, the training set size is much larger compared to the PP attachment training set. 3.2 Results In line with previous work (Ng and Jordan, 2002; Klein and Manning, 2002), we first compare Naive Bayes and Logistic regression on the two NLP tasks. This lets us see how they compare when the generative model is making strong independence assumptions and when the two kinds of models have the same structure. Then we compare the generative and discriminative models with learned richer structures. Table 3 shows the Naive Bayes/Logistic regression results for PP attachment. We list results for several conditions of training the Naive Bayes classifier, depending on whether it is trained as strictly generative or as a hybrid model, and whether a single or multiple hyper</context>
<context position="30192" citStr="Klein and Manning, 2002" startWordPosition="5001" endWordPosition="5005">h a degree 2 polynomial kernel (Pradhan et al., 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SRL. Figure 2: Learned Bayesian Network structures for PP attachment and SRL. Bayesian Networks with special structure of the CPTs – e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure learning have been previous</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning. 2002. Conditional structure versus conditional estimation in NLP models. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="4117" citStr="Lafferty et al., 2001" startWordPosition="646" endWordPosition="649">nerative models in the form of directed graphical models (Bayesian Networks (Pearl, 1988)), since the optimal parameters for such models can be estimated in closed form. We compare Bayesian Net576 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576–584, Sydney, July 2006. c�2006 Association for Computational Linguistics works with structure learning to their closely related discriminative counterpart – conditional loglinear models with structure learning. Our conditional log-linear models can also be seen as Conditional Random Fields (Lafferty et al., 2001), except we do not have a structure on the labels, but want to learn a structure on the features. We compare the two kinds of models on two NLP classification tasks – prepositional phrase attachment and semantic role labelling. Our results show that the generative models are competitive with or better than the discriminative models. When a small set of interpolation parameters for the conditional probability tables are fit discriminatively, the resulting hybrid generativediscriminative models perform better than the generative only models and sometimes better than the discriminative models. In</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of UAI.</booktitle>
<contexts>
<context position="16262" citStr="McCallum, 2003" startWordPosition="2718" endWordPosition="2719">dels with much richer structure. We consider more complex mappings 4b, which incorporate conjunctions of combinations of input variables. We restrict the number of variables in the combinations to three, which directly corresponds to our limit on number of parents in the Bayesian Network structures. This is similar to considering polynomial kernels of up to degree three, but is more general, because, for example, we can add only some and not all bigram conjunctions of variables. Structure search (or feature selection) for log-linear models has been done before e.g. (Della Pietra et al., 1997; McCallum, 2003). We devise our structure search methodology in a way that corresponds as closely as possible to our structure search for Bayesian Networks. The exact hypothesis space considered is defined by the search procedure for an optimal structure we apply, which we describe next. 2.2.1 Structure Search Methodology We start with an initial empty feature set and a candidate feature set consisting of all input features: CANDIDATES={X1,X2,...,Xm}. In the course of the search, the set CANDIDATES may contain feature conjunctions in addition to the initial input features. After a feature is selected from the</context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>Andrew McCallum. 2003. Efficiently inducing features of conditional random fields. In Proceedings of UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes.</title>
<date>2002</date>
<booktitle>In NIPS 14.</booktitle>
<contexts>
<context position="1950" citStr="Ng and Jordan, 2002" startWordPosition="296" endWordPosition="299">for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. Generative models may often perform poorly due to making strong independence assumptions about </context>
<context position="22563" citStr="Ng and Jordan, 2002" startWordPosition="3772" endWordPosition="3775">Stemmed Verb PATH: Path from node to predicate POSITION: Before or after predicate? VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicate’s parent Table 2: Features for Semantic Role Labeling. test sets from the February 2004 version of Propbank. The training set consists of sections 2 to 21, the development set is from section 24, and the test set is from section 23. The number of samples is listed in Table 1. As we can see, the training set size is much larger compared to the PP attachment training set. 3.2 Results In line with previous work (Ng and Jordan, 2002; Klein and Manning, 2002), we first compare Naive Bayes and Logistic regression on the two NLP tasks. This lets us see how they compare when the generative model is making strong independence assumptions and when the two kinds of models have the same structure. Then we compare the generative and discriminative models with learned richer structures. Table 3 shows the Naive Bayes/Logistic regression results for PP attachment. We list results for several conditions of training the Naive Bayes classifier, depending on whether it is trained as strictly generative or as a hybrid model, and whether </context>
<context position="30166" citStr="Ng and Jordan, 2002" startWordPosition="4997" endWordPosition="5000"> 91.4% for an SVM with a degree 2 polynomial kernel (Pradhan et al., 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SRL. Figure 2: Learned Bayesian Network structures for PP attachment and SRL. Bayesian Networks with special structure of the CPTs – e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure l</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Ng and Michael Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes. In NIPS 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="2292" citStr="Och and Ney, 2002" startWordPosition="349" endWordPosition="352"> Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure,</context>
<context position="13735" citStr="Och and Ney, 2002" startWordPosition="2288" endWordPosition="2291"> single or multiple hyper-parameters d. The fitting criterion for the generative Bayesian Networks is joint likelihood of the development set of samples with a Gaussian prior on the values log(d). 1 Additionally, we explore fitting the hyperparameters of the Bayesian Networks by optimizing the conditional likelihood of the development set of samples. In this case we call the resulting models Hybrid Bayesian Network models, since they incorporate a number of discriminatively trained parameters. Hybrid models have been proposed before and shown to perform very competitively (Raina et al., 2004; Och and Ney, 2002). In Section 3.2 we compare generative and hybrid Bayesian Networks. 2.2 Discriminative Models Discriminative models learn a conditional distribution Pθ(Y |X) or discriminant functions that discriminate between classes. Here we concentrate on conditional log-linear models. A simple example of such model is logistic regression, which directly corresponds to Naive Bayes but is trained to maximize the conditional likelihood. 2 To describe the form of models we study, let us introduce some notation. We represent a tuple of nominal variables (X1,X2,... ,Xm) as a vector of 0s and 1s in the following</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings ofACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="20029" citStr="Palmer et al., 2005" startWordPosition="3350" endWordPosition="3353">us for the PP attachment task we have binary labels Att, and four input variables – v, n1, p, n2. We work with the standard dataset previously used for this task by other researchers (RatnaTask Training Devset Test PP 20,801 4,039 3,097 SRL 173,514 5,115 9,272 Table 1: Data sizes for the PP attachment and SRL tasks. parkhi et al., 1994; Collins and Brooks, 1995). It is extracted from the the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994). Table 1 shows summary statistics for the dataset. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts. There has been plenty of work on machine learning models for semantic role labeling, starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic reasoning in intelligent systems: Networks ofplausible inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3584" citStr="Pearl, 1988" startWordPosition="568" endWordPosition="569">nerative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can. Structure learning is very efficient for generative models in the form of directed graphical models (Bayesian Networks (Pearl, 1988)), since the optimal parameters for such models can be estimated in closed form. We compare Bayesian Net576 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576–584, Sydney, July 2006. c�2006 Association for Computational Linguistics works with structure learning to their closely related discriminative counterpart – conditional loglinear models with structure learning. Our conditional log-linear models can also be seen as Conditional Random Fields (Lafferty et al., 2001), except we do not have a structure on the labels, but want to lear</context>
<context position="5811" citStr="Pearl, 1988" startWordPosition="939" endWordPosition="940"> the space of inputs and Y is the space of labels, a classifier is a function f : X —* Y. A generative model is one that models the joint probability of inputs and labels PD(x, y) through a distribution Pθ(x, y), dependent on some parameter vector 0. The classifier based on this generative model chooses the most likely label given an input according to the conditionalized estimated joint distribution. The parameters 0 of the fitted distribution are usually estimated using the maximum joint likelihood estimate, possibly with a prior. We study generative models represented as Bayesian Networks (Pearl, 1988), because their parameters can be estimated extremely fast as the maximizer of the joint likelihood is the closed form relative frequency estimate. A Bayesian Network is an acyclic directed graph over a set of nodes. For every variable Z, let Pa(Z) denote the set of parents of Z. The structure of the Bayesian Network encodes the following set of indepenFigure 1: Naive Bayes Bayesian Network dence assumptions: every variable is conditionally independent of its non-descendants given its parents. For example, the structure of the Bayesian Network model in Figure 1 encodes the independence assumpt</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic reasoning in intelligent systems: Networks ofplausible inference. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Pernkopf</author>
<author>Jeff Bilmes</author>
</authors>
<title>Discriminative versus generative parameter and structure learning of bayesian network classifiers.</title>
<date>2005</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="10913" citStr="Pernkopf and Bilmes, 2005" startWordPosition="1801" endWordPosition="1804">rameter which is shared by all CPTs and learning multiple d parameters – one for every type of conditioning context in every CPT – i.e., each CPT has as many d parameters as there are back-off levels. We place some restrictions on the Bayesian Networks learned, for closer correspondence with the discriminative models and for tractability: Every input variable node has the label node as a parent, and at most three parents per variable are allowed. 2.1.1 Structure Search Methodology Our structure search method differs slightly from previously proposed methods in the literature (Heckerman, 1999; Pernkopf and Bilmes, 2005). The search space is defined as follows. We start with a Bayesian Network containing only the class variable. We denote by CHOSEN the set of variables already in the network and by REMAINING the set of unplaced variables. Initially, only the class variable Y is in CHOSEN and all other variables are in REMAINING. Starting from the current BN, the set of next candidate structures is defined as follows: For every unplaced variable R in REMAINING, and for every subset Sub of size at most two from the already placed variables in CHOSEN, consider adding R with parents Sub UY to the current BN. Thus</context>
<context position="12148" citStr="Pernkopf and Bilmes, 2005" startWordPosition="2021" endWordPosition="2024">er of candidate structures for extending a current BN is on the order of m3, where m is the number of variables. We perform a greedy search. At each step, if the best variable B with the best set of parents Pa(B) improves the evaluation criterion, move B from REMAINING to CHOSEN, and continue the search until there are no variables in REMAINING or the evaluation criterion can not be improved. The evaluation criterion for BNs we use is classification accuracy on a development set of samples. Thus our structure search method is discriminative, in the terminology of (Grossman and Domingos, 2004; Pernkopf and Bilmes, 2005). It is very easy to evaluate candidate BN structures. The main parameters in the CPTs are estimated via the relative frequency estimator on the training set, as discussed in the previous section. We do not fit the hyper-parameters d during structure search. We fit these parameters only after we have selected a final BN structure. Throughout the structure search, we use a fixed value of 1 for d for all CPTs and levels of back-off. Therefore we are using generative parameter estimation and discriminative structure search. See Section 4 for discussion on how this method relates to previous work.</context>
<context position="30833" citStr="Pernkopf and Bilmes, 2005" startWordPosition="5101" endWordPosition="5104">ls (Klein and Manning, 2002; Johnson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SRL. Figure 2: Learned Bayesian Network structures for PP attachment and SRL. Bayesian Networks with special structure of the CPTs – e.g. decision trees, have been previously studied in e.g. (Friedman and Goldszmidt, 1996), but not for NLP tasks and not in comparison to discriminative models. Studies comparing generative and discriminative models with structure learning have been previously performed ((Pernkopf and Bilmes, 2005) and (Grossman and Domingos, 2004)) for other, non-NLP domains. There are several important algorithmic differences between our work and that of (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004). We detail the differences here and perform an empirical evaluation of the impact of some of these differences. Form of the generative models. The generative models studied in that previous work do not employ any special form of the conditional probability tables. Pernkopf and Bilmes (2005) use a simple smoothing method: fixing the probability of every event that has a zero relative frequency es</context>
<context position="32705" citStr="Pernkopf and Bilmes, 2005" startWordPosition="5397" endWordPosition="5400">lds. Our models are more general in that their parameters do not need to be interpretable as probabilities (sum to 1 and between 0 and 1), and the structures do not need to correspond to Bayes Net structures. For discriminative classifiers, it is not important that their component parameters be interpretable as probabilities; thus this restriction is probably unnecessary. Like for the generative models, another major difference is in the smoothing algorithms. We smooth the models both by fitting a gaussian prior hyperparameter and by incorporating features of subsets of cliques. Smoothing in (Pernkopf and Bilmes, 2005) is done by substituting zero-valued parameters with a small fixed c. Grossman and Domingos (2004) employ early stopping using held-out data which can achieve similar effects to smoothing with a gaussian prior. To evaluate the importance of the differences between our algorithm and the ones presented in these works, and to evaluate the importance of fitting hyper-parameters for smoothing, we implemented a modified version of our structure search. The modifications were as follows. For Bayes Net structure learning: (i) no Witten-Bell smoothing is employed in the CPTs, and (ii) no backoffs to lo</context>
<context position="33932" citStr="Pernkopf and Bilmes, 2005" startWordPosition="5600" endWordPosition="5603">r-order distributions are considered. The only smoothing remaining in the CPTs is an interpolation with a uniform distribution with a fixed weight of a = .1. For discriminative log-linear model structure learning: (i) the gaussian prior was fixed to be very weak, serving only to keep the weights away from infinity (a = 100) and (ii) the conjunction selection was restricted to correspond to a Bayes Net structure with no features for subsets of feature conjunctions. Thus the only difference between the class of our modified discriminative log-linear models and the class of models considered in (Pernkopf and Bilmes, 2005; Grossman and Domingos, 2004) is that we do not restrict the parameters to be interpretable as probabilities. The results shown in Table 8 summarize the results obtained by the modified algorithm on the two tasks. Both the generative and discriminative learners suffered a statistically significant (at level .01) loss in performance. Notably, the log-linear model for PP attachment performs worse than logistic regression with better smoothing. N1 Att V P N2 PLem HW Sub- Path cat Voice Role Pos 583 PP Attachment Results Model H-params Test set acc Bayes Net 0 82.8 Log-linear model 0 81.2 SRL Cla</context>
</contexts>
<marker>Pernkopf, Bilmes, 2005</marker>
<rawString>Franz Pernkopf and Jeff Bilmes. 2005. Discriminative versus generative parameter and structure learning of bayesian network classifiers. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="1452" citStr="Pradhan et al., 2005" startWordPosition="215" endWordPosition="218">tive model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminati</context>
<context position="21134" citStr="Pradhan et al., 2005" startWordPosition="3531" endWordPosition="3534">of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004). Many features have been proposed for building models for semantic role labeling. Initially, 7 features were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. These are the features we use as well. Table 2 lists the features. State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al., 2005a), as well as global features including the labels of other nodes in parse tree. Nevertheless it is intere</context>
<context position="29620" citStr="Pradhan et al., 2005" startWordPosition="4912" endWordPosition="4915">d note that it is much faster to do structure search for the generative Bayesian Network model, as compared to structure search for the log-linear model. In our implementation, we did not do any computation reuse between successive steps of structure search for the Bayesian Network or log-linear models. Structure search took 2 hours for the Bayesian Network and 24 hours for the log-linear model. To put our results in the context of previous work, other results on core arguments using the same input features have been reported, the best being 91.4% for an SVM with a degree 2 polynomial kernel (Pradhan et al., 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James Martin, and Dan Jurafsky. 2005a. Support vector learning for semantic argument classification. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1452" citStr="Pradhan et al., 2005" startWordPosition="215" endWordPosition="218">tive model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminati</context>
<context position="21134" citStr="Pradhan et al., 2005" startWordPosition="3531" endWordPosition="3534">of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004). Many features have been proposed for building models for semantic role labeling. Initially, 7 features were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. These are the features we use as well. Table 2 lists the features. State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al., 2005a), as well as global features including the labels of other nodes in parse tree. Nevertheless it is intere</context>
<context position="29620" citStr="Pradhan et al., 2005" startWordPosition="4912" endWordPosition="4915">d note that it is much faster to do structure search for the generative Bayesian Network model, as compared to structure search for the log-linear model. In our implementation, we did not do any computation reuse between successive steps of structure search for the Bayesian Network or log-linear models. Structure search took 2 hours for the Bayesian Network and 24 hours for the log-linear model. To put our results in the context of previous work, other results on core arguments using the same input features have been reported, the best being 91.4% for an SVM with a degree 2 polynomial kernel (Pradhan et al., 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Daniel Jurafsky. 2005b. Semantic role labeling using different syntactic views. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCAI.</booktitle>
<contexts>
<context position="1430" citStr="Punyakanok et al., 2005" startWordPosition="211" endWordPosition="214"> corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base mode</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005. The necessity of syntactic parsing for semantic role labeling. In Proceedings ofIJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Yirong Shen</author>
<author>Andrew Y Ng</author>
<author>Andrew McCallum</author>
</authors>
<title>Classification with hybrid generative/discriminative models. In</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems 16.</booktitle>
<editor>Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2272" citStr="Raina et al., 2004" startWordPosition="345" endWordPosition="348">structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a g</context>
<context position="13715" citStr="Raina et al., 2004" startWordPosition="2284" endWordPosition="2287">ion, we fit either a single or multiple hyper-parameters d. The fitting criterion for the generative Bayesian Networks is joint likelihood of the development set of samples with a Gaussian prior on the values log(d). 1 Additionally, we explore fitting the hyperparameters of the Bayesian Networks by optimizing the conditional likelihood of the development set of samples. In this case we call the resulting models Hybrid Bayesian Network models, since they incorporate a number of discriminatively trained parameters. Hybrid models have been proposed before and shown to perform very competitively (Raina et al., 2004; Och and Ney, 2002). In Section 3.2 we compare generative and hybrid Bayesian Networks. 2.2 Discriminative Models Discriminative models learn a conditional distribution Pθ(Y |X) or discriminant functions that discriminate between classes. Here we concentrate on conditional log-linear models. A simple example of such model is logistic regression, which directly corresponds to Naive Bayes but is trained to maximize the conditional likelihood. 2 To describe the form of models we study, let us introduce some notation. We represent a tuple of nominal variables (X1,X2,... ,Xm) as a vector of 0s and</context>
</contexts>
<marker>Raina, Shen, Ng, McCallum, 2004</marker>
<rawString>Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew McCallum. 2004. Classification with hybrid generative/discriminative models. In Sebastian Thrun, Lawrence Saul, and Bernhard Sch¨olkopf, editors, Advances in Neural Information Processing Systems 16. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>JeffReynar</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="19869" citStr="Ratnaparkhi et al., 1994" startWordPosition="3323" endWordPosition="3326"> words of these constituents and of the NP inside the PP. Thus the example sentence is represented as the following quadruple: [v:hang n1:painting p:with n2:peg]. Thus for the PP attachment task we have binary labels Att, and four input variables – v, n1, p, n2. We work with the standard dataset previously used for this task by other researchers (RatnaTask Training Devset Test PP 20,801 4,039 3,097 SRL 173,514 5,115 9,272 Table 1: Data sizes for the PP attachment and SRL tasks. parkhi et al., 1994; Collins and Brooks, 1995). It is extracted from the the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994). Table 1 shows summary statistics for the dataset. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts. There has been plenty of work on machine learning models for se</context>
</contexts>
<marker>Ratnaparkhi, JeffReynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, JeffReynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2142" citStr="Roark et al., 2004" startWordPosition="325" endWordPosition="328">ity of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint d</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1381" citStr="Toutanova et al., 2003" startWordPosition="205" endWordPosition="208">achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative models can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discrimina</context>
</contexts>
<marker>Toutanova, Klein, Manning, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, and Christopher D. Manning. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="21112" citStr="Toutanova et al., 2005" startWordPosition="3527" endWordPosition="3530"> starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004). Many features have been proposed for building models for semantic role labeling. Initially, 7 features were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. These are the features we use as well. Table 2 lists the features. State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al., 2005a), as well as global features including the labels of other nodes in parse tree. Nev</context>
<context position="29803" citStr="Toutanova et al., 2005" startWordPosition="4940" endWordPosition="4943">did not do any computation reuse between successive steps of structure search for the Bayesian Network or log-linear models. Structure search took 2 hours for the Bayesian Network and 24 hours for the log-linear model. To put our results in the context of previous work, other results on core arguments using the same input features have been reported, the best being 91.4% for an SVM with a degree 2 polynomial kernel (Pradhan et al., 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al., 2005). Therefore our resulting models with 93.5% and 93.9% accuracy compare favorably to the SVM model with polynomial kernel and show the importance of structure learning. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001). 3This result is on an older version of Propbank from July 2002. 582 (a) Learned Bayesian Network for PP attachment. (b) Learned Bayesian Network for SR</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bram Vanschoenwinkel</author>
<author>Bernard Manderick</author>
</authors>
<title>A weighted polynomial information gain kernel for resolving prepositional phrase attachment ambiguities with support vector machines.</title>
<date>2003</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="18669" citStr="Vanschoenwinkel and Manderick, 2003" startWordPosition="3109" endWordPosition="3113">a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Experiments 3.1 Problems and Datasets We study two classification problems – prepositional phrase (PP) attachment, and semantic role labeling. Following most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to determine if the PP should be attached to the verb or to the object noun phrase. For example, in the sentence: Never [hang]v [a painting]NP [with a peg]PP, the prepositional phrase with a peg can either modify the verb hang or the object noun phrase a painting. Here, clearly, with a peg modifies the verb hang. We follow the common practice in representing the problem using only the head words of these constitue</context>
<context position="25763" citStr="Vanschoenwinkel and Manderick, 2003" startWordPosition="4279" endWordPosition="4282">lowed to learn optimal structures, the generative model outperforms the discriminative model. As seen from Table 4, the Bayesian Network with a single interpolation weight achieves an accuracy of 84.6%, whereas the discriminative model performs at 83.8%. The hybrid model with a single interpolation weight does even better, achieving 85.0% accuracy. For comparison, the model of Collins &amp; Brooks has accuracy of 84.15% on this test set, and the highest result obtained through a discriminative model with this feature set is 84.8%, using SVMs and a polynomial kernel with multiple hyper-parameters (Vanschoenwinkel and Manderick, 2003). The Hybrid Bayes Nets are statistically significantly better than the Log-linear model (p &lt; 0.05), and the Bayes Nets are not significantly better than the Log-linear model. All models from Table 4 are significantly better than all models in Table 3. For semantic role labelling classification of core arguments, the results are listed in Tables 5 and 6. We can see that the difference in performance between Naive Bayes with a single interpolation parameter d – 83.3% and the performance of Logistic regression – 91.1%, is very large. This shows that the independence assumptions are quite 581 Mod</context>
</contexts>
<marker>Vanschoenwinkel, Manderick, 2003</marker>
<rawString>Bram Vanschoenwinkel and Bernard Manderick. 2003. A weighted polynomial information gain kernel for resolving prepositional phrase attachment ambiguities with support vector machines. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Timothy C Bell</author>
</authors>
<title>The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>37--4</pages>
<contexts>
<context position="8302" citStr="Witten and Bell, 1991" startWordPosition="1354" endWordPosition="1357">of Zi. Most often smoothing is applied to avoid zero probability estimates. A simple form of smoothing is add-α smoothing which is equivalent to a Dirichlet prior. For NLP tasks it has been shown that other smoothing methods are far superior to add-α smoothing – see, for example, Goodman X1 X2 ...... Xrn Y 577 (2001). In particular, it is important to incorporate lower-order information based on subsets of the conditioning information. Therefore we assume a structural form of the conditional probability tables which implements a more sophisticated type of smoothing – interpolated Witten-Bell (Witten and Bell, 1991). This kind of smoothing has also been used in the generative parser of (Collins, 1997) and has been shown to have a relatively good performance for language modeling (Goodman, 2001). To describe the form of the conditional probability tables, we introduce some notation. Let Z denote a variable in the BN and Z1, Z2,..., Zk denote the set of its parents. The probability P(Z = z|Z1 = z1, Z2 = z2, . . . , Zk = zk) is estimated using Witten-Bell smoothing as follows: (below the tuple of values z1,z2,...,zk is denoted by z1k). PWs(z|z1k) = λ(z1k) x P�(z|z1k) + (1 − λ(z1k)) x PWs(z|z1k−1) In the abo</context>
</contexts>
<marker>Witten, Bell, 1991</marker>
<rawString>Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37,4:1085–1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="21158" citStr="Xue and Palmer, 2004" startWordPosition="3535" endWordPosition="3538">(2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004). Many features have been proposed for building models for semantic role labeling. Initially, 7 features were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. These are the features we use as well. Table 2 lists the features. State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al., 2005a), as well as global features including the labels of other nodes in parse tree. Nevertheless it is interesting to see how well we</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>