<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001431">
<title confidence="0.995193">
Interoperability and Knowledge Representation in Distributed Health
and Fitness Companion Dialogue System
</title>
<author confidence="0.996187">
Jaakko Hakulinen
</author>
<affiliation confidence="0.9984895">
Department of Computer Sciences
33014 University of Tampere, Finland
</affiliation>
<email confidence="0.997381">
jaakko.hakulinen@cs.uta.fi
</email>
<author confidence="0.982829">
Markku Turunen
</author>
<affiliation confidence="0.998285">
Department of Computer Sciences
33014 University of Tampere, Finland
</affiliation>
<email confidence="0.998369">
markku.turunen@cs.uta.fi
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852684210526">
As spoken dialogue systems move be-
yond task oriented dialogues and become
distributed in the pervasive computing
environments, their growing complexity
calls for more modular structures. When
different aspects of a single system can
be accessed with different interfaces,
knowledge representation and separation
of low level interaction modeling from
high level reasoning on domain level be-
comes important. In this paper, a model
utilizing a dialogue plan to communicate
information from domain level planner to
dialogue management and from there to a
separate mobile interface is presented.
The model enables each part of the sys-
tem handle the same information from
their own perspectives without contain-
ing overlapping logic.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994054466666667">
Most existing spoken dialogue systems pro-
vide a single interface to solve a well-defined
task, such as booking tickets or providing time-
table information. There are emerging areas that
differ dramatically from task-oriented systems.
In domain-oriented dialogues (Dybkjaer et al,
2004) the interaction with the system, typically
modeled as a conversation with a virtual human-
like character, can be the main motivation for the
interaction. These systems are often multimodal,
and may take place in pervasive computing envi-
ronments where various mobile, robotic, and
other untraditional interface are used to commu-
nicate with the system. For example, in the EU-
funded COMPANIONS-project (Wilks, 2007)
</bodyText>
<footnote confidence="0.92484475">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
Cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.998749636363636">
we are developing a conversational Health and
Fitness Companion that develops long-lasting
relationships with its users to support their
healthy living and eating habits via mobile and
physical agent interfaces. Such systems have dif-
ferent motivations for use compared to tradi-
tional task-based spoken dialogue systems. In-
stead of helping with a single, well defined task,
the system aims at building a long-term relation-
ship with its user and providing support on a
daily basis.
</bodyText>
<subsectionHeader confidence="0.996064">
1.1 Mobile and Physical Agent Interfaces
</subsectionHeader>
<bodyText confidence="0.999905">
New kinds of interfaces are used increasingly
often in conjunction with spoken dialogue tech-
nology. Speech suits mobile interfaces well be-
cause it can overcome the limited input and out-
put modalities of the small devices and can also
better support using during the moments when
their hand or eyes are busy. Physical agent inter-
faces, on the other hand, have been used in sys-
tems, which try to make dialogue systems more
part of people’s life. In many cases, they include
rich multimodal input and output while providing
a physical outlook for the agent. While naturalis-
tic human-like physical robots are under devel-
opment, especially in Japan, there is room for a
variety of different physical interface agents
ranging from completely abstract (e.g., simple
devices with lights and sound) to highly sophisti-
cated anthropomorphic apparatus. For example,
Marti and Schmandt (2005) used several toy ani-
mals, such as bunnies and squirrels, as physical
embodied agents for a conversational system.
Other example is an in-door guidance and recep-
tionist application involving a physical interface
agent that combines pointing gestures with con-
versational speech technology (Kainulainen et al.,
2005). Some physical agent technology has also
</bodyText>
<page confidence="0.998322">
24
</page>
<note confidence="0.721347">
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 24–31
Manchester, August 2008
</note>
<bodyText confidence="0.999602705882353">
been commercialized. For example, the wireless
NabaztagTM/tag rabbits
(http://www.nabaztag.com/) have been success
Both mobile use and physical agent interface
can support the goal of making a spoken dia-
logue system part of users’ everyday life and
building a meaningful relationship between the
system and the user. It has been found that mere
existence of a physical interface changes users’
attitude toward a system and having access to a
system throughout the day via a mobile interface
is likely to further support this relationship.
In this work, we have used the Nabaztag as a
multimodal physical interface to create a conver-
sational Health and Fitness Companion and a
mobile version interface for outdoor usage has
been implemented on Windows Mobile platform.
</bodyText>
<subsectionHeader confidence="0.978764">
1.2 Inter­component Communication and
Knowledge Representation Challenges
</subsectionHeader>
<bodyText confidence="0.99995934883721">
In systems, where multiple interfaces can be
used to access parts of the same functionality and
the system interacts with a user many times over
a long time period, modeling the interaction and
domain easily becomes complex. For example,
the system should model interaction history on a
longer timescale than a single session. With mul-
tiple interfaces, at least some such information
could be useful if they can be shared between the
interfaces. Furthermore, the system must include
a model capable of reasoning about the domain,
and learn from the user and his or her actions to
provide meaningful interaction, such as to pro-
vide reasonable guidance on user’s health and
progress as the user’s condition alters over time
in our case with the Health and Fitness Compan-
ion. Such reasoning should be concentrated on
one component, instead of duplicating the logic
to keep the system maintainable. Still, the infor-
mation must be communicated over different
interfaces and the component inside them. There-
fore, modularization of the system and appropri-
ate knowledge representation become vital.
On dialogue management level, a common
way to take some complexity away from the dia-
logue manager and limit its tasks more specifi-
cally to dialogue management is to separate do-
main specific processing, such as database que-
ries, into a back-end component. Many research-
ers have worked with separating generic dialogue
management processes from the domain specific
processes. Example solutions include shells
(Jönsson, 1991) and object oriented program-
ming methods (Salonen, et al., 2004, O’Neill, et
al., 2003). On the other hand, a simple back-end
and an active user community has emerged
around it.
interface, e.g., SQL queries, can be included as
configuration parameters (Pellon et al., 2000).
Since dialogue management is usually based on
state transition networks, form filling, or some
other clearly defined model, separating domain
specific processing to the back-end makes it pos-
sible to implemented dialogue management
purely with the selected model.
Health and Fitness Companion, as discussed in
the following, is based on a model where the
domain specific module is more than just a sim-
ple interface and includes active processing of
domain information, reasoning, learning, or other
complex processes. We call such a component
the cognitive model. While the task of a dialogue
manager is to maintain and update dialogue state,
the cognitive model reasons using the domain
level knowledge. In our case, we have two dia-
logue managers, one for the home system with a
physical interface agent and one for mobile sys-
tem (yet another is in development, but not con-
sidered here). The two handle somewhat separate
tasks but each provides input to another and the
cognitive model. Separation of the task between
the different parts is not trivial. For example,
managing dialogue level phenomena, such as
error handling and basic input processing, are
tasks clearly in the areas of respective dialogue
managers. However, cognitive modeling can help
in error handling by spotting input that seems
suspicious based on domain level information
and input parsing by providing information on
potential discussion topics. The solution we have
devised is to have the cognitive model produce a
dialogue plan for the dialogue management in
home system. The dialogue management in the
home system provides parsed user inputs to the
cognitive model and to the mobile system. The
mobile system provides similar input back to the
home system, which communicates it back to the
cognitive model.
In the following we describe the Health and
Fitness dialogue system in general. Then we dis-
cuss the mobile interface, the dialogue manager
of the home system and the cognitive model, be-
fore going into details on how the components
have been separated. The solution, which pro-
vides great flexibility for each, is discussed be-
fore conclusions.
</bodyText>
<page confidence="0.998571">
25
</page>
<sectionHeader confidence="0.873818" genericHeader="introduction">
2 Health and Fitness Companion
</sectionHeader>
<bodyText confidence="0.999572307692308">
The Health and Fitness Companion (H&amp;F) is a
conversational interface for supporting healthy
lifestyle. The companion plans each day together
with its user at home, and suggests healthy ac-
tivities, such as walking to work, when possible.
During the day, a mobile interface to the Com-
panion can be used to record various physical
activities, such as those walks to work. After-
wards, the user is able to report back to the com-
panion on the day, and get more advice and sup-
port. At this point information recorded by the
mobile system is automatically used by the sys-
tem.
</bodyText>
<figureCaption confidence="0.993534">
Figure 1: Health and Fitness Companion Sce-
</figureCaption>
<bodyText confidence="0.976557647058824">
nario.
As seen in Figure 1, H&amp;F home system uses a
Nabaztag/tag WLAN rabbit as a physical inter-
face. Nabaztag provides audio output and push-
to-talk input, and is able to move its ears and op-
erate four colored lights to signal, for example,
emotions. The mobile interface, as seen in figure
2, runs on a Window Mobile platform and uses
push-to-talk speech input, speech output and a
graphical interface with key and stylus input. The
graphics include Nabaztag graphics and the same
voice as in the home system is used for output to
help users associate the two interfaces. The mo-
bile Companion follows the user for physical
activities, such as jogging, and collects data on
the exercises and feeds this back into the main
system. While it includes a multimodal speech
interface, the main input modality for the mobile
Companion can be considered to be GPS posi-
tioning. It is used to collect information on user’s
exercise and provide feedback during the exer-
cise. It is also used as the detection for the com-
pletion of the exercises, which information is
then forwarded to the home system and the cog-
nitive model.
From technical viewpoint, H&amp;F is a multimo-
dal spoken dialogue system containing compo-
nents for speech recognition (ASR), natural lan-
guage understanding (NLU), dialogue manage-
ment (DM), natural language generation (NLG),
and speech synthesis (TTS). Furthermore, it in-
cludes a separate cognitive model (CM), which
works in close co-operation with DM of the
home system, as presented in the following sec-
tions. The dialogue system in the home system is
implemented using Java and Jaspis framework
(Turunen et al., 2005) with jNabServer
(http://www.cs.uta.fi/hci/spi/jnabserver/) for Na-
baztag connectivity. The cognitive model is im-
plemented in Lisp and integrated into the Jaspis
framework. The mobile interface is implemented
in Java with native C++ code for speech technol-
ogy components. It uses PART
(http://part.sourceforge.net/) for persistent stor-
age and HECL for scripting in dialogue manager
(http://sourceforge.net/projects/hecl).
GPS status bar
Avatar and text
output area
Exercise
status bar
</bodyText>
<figureCaption confidence="0.992391">
Figure 2: Mobile Companion Interface.
</figureCaption>
<bodyText confidence="0.913344">
- Good morning, anything interesting organized
for today?
- I’m going for a walk.
</bodyText>
<listItem confidence="0.819853">
- Is that walk before dinner?
- No, I think I’ll walk after I’ve eaten.
- OK, so you are going for a walk after dinner, is
that correct?
- Yes.
- Great, why don’t you cycle to work?
- Okay, I can do that.
</listItem>
<page confidence="0.958077">
26
</page>
<bodyText confidence="0.99997845">
For speech recognition and synthesis, H&amp;F
uses Loquendo ASR and TTS. Current recogni-
tion grammars for the home system, derived
from a WOZ data and extended using user test
data, have a vocabulary of 1090 words and a to-
tal of 436 grammar rules. Recognition grammars
are dynamically selected for each user input,
based on the dialogue state. The mobile interface
use mobile versions of Loquendo technology.
Due to the technological limitations, more chal-
lenging acoustic environment, potential physical
exhaustion of users, and more restricted domain,
the recognition grammars in the mobile interface
will remain significantly smaller than those of
the home system.
NLU is based on SISR semantic tags
(http://www.w3.org/TR/semantic-interpretation/)
embedded in the recognition grammars. In the
home system, where mixed initiative interaction
is possible, the tags provide parameters compati-
ble with predicates used to represent information
on the dialogue management level. Input parsing
unifies these parameters into full predicates
based on the current dialogue state. In mobile
system, more strict state based dialogue model-
ing can results in unambiguous output straight
from the SISR tags.
Natural language generation is a mixture of
canned strings and, in the home system, tree ad-
joining grammar based generation. In addition,
control messages for Nabaztag ears and lights
can be generated.
As discussed previously, distribution and co-
ordination of the different tasks between differ-
ent components can become rather complex in
systems such as H&amp;F without proper modeling
of interaction, domain, and reasoning compo-
nents. Next, we present a model which allows
flexible interaction between the cognitive model
and the dialogue management.
</bodyText>
<sectionHeader confidence="0.9557635" genericHeader="method">
3 Dialogue Management and Cognitive
Modeling
</sectionHeader>
<bodyText confidence="0.9996195625">
There is great consensus that components of a
dialogue system can be split into at least three
parts: an input module, which receives user input
and parses it into a logical form, dialogue man-
agement, which maintains and updates dialogue
state based on user input and generates output
requests, and an output module, which generates
natural language output to user based on the re-
quests. In the case of H&amp;F, we have also sepa-
rated a cognitive model (CM) from dialogue
manager (DM), as seen in Figure 3. We call this
module the cognitive model, because it contains
what can be considered higher level cognitive
processes of the system. Next, we present DM of
the home system, CM component, and the mo-
bile interface, focusing on their interaction.
</bodyText>
<figureCaption confidence="0.987286">
Figure 3: Information passed between the
</figureCaption>
<bodyText confidence="0.640944">
components.
</bodyText>
<subsectionHeader confidence="0.902043">
3.1 Cognitive Model Responsibilities
</subsectionHeader>
<bodyText confidence="0.99992025">
The task of CM is to model the domain, i.e.,
know what to recommend to the user, what to
ask from the user and what kind of feedback to
provide. CM in H&amp;F uses hierarchical task net-
works (HTNs) (Cavazza et al., 2008) as the
method of planning healthy daily activity for the
user. Part of a network can be seen in Figure 4.
In the current H&amp;F implementation, the planning
domain included 16 axioms and 111 methods, 49
operators, 42 semantic tags, 113 evaluation rules
and there are 17 different topics to be discussed
with the user.
</bodyText>
<figureCaption confidence="0.984544">
Figure 4: Hierachical Task Network.
</figureCaption>
<bodyText confidence="0.999577">
CM is aware of the meaning of the concepts
inside the system on a domain specific level. It
generates and updates a dialogue plan according
to the information received from the user. The
plan is forwarded to DM. Interaction level issues
are not directly visible to CM.
</bodyText>
<figure confidence="0.956738">
Cycling
Walking
Active Passive
N-Stop-
Bus
Travel
N-Stop-
Train
N-Stop-
Subway
</figure>
<page confidence="0.93903">
27
</page>
<subsectionHeader confidence="0.7504805">
3.2 Dialogue Management in the Home Sys-
tem
</subsectionHeader>
<bodyText confidence="0.99997286440678">
The task of DM is to maintain and update a
dialogue state. In the H&amp;F system, the dialogue
state includes a dialogue history tree (currently
linear), a stack of active dialogue topics, and cur-
rent user input, including ASR confidence scores
and N-best lists. In addition, two pools of items
that need to be confirmed are stored; one for
items to be confirmed individually and another
for those that can be confirmed together in one
question.
DM receives user inputs as predicates parsed
by the NLU component. If an utterance is suc-
cessfully parsed and matches the current dia-
logue plan (see Section 3.3), DM does not need
to know what the meaning of the input actually is.
It just takes care of confirmations and provides
the information to CM. When generating output
requests based on the plan, DM can also be un-
aware of the specific meaning of the plan items.
Overall, DM does not need to have the deep do-
main understanding CM specializes in.
DM, however, is aware of the relations of the
predicates on the topics level, i.e., it knows,
which predicates belong to each topic. This in-
formation is used primarily for parsing input.
DM also has understanding of the semantics of
the predicates which relates to interaction.
Namely, relations such as question – answer
pairs (suggestion – agreement, confirmation –
acceptance/rejection, etc.) are modeled.
On implementation level, dialogue manage-
ment is implemented as a collection of separate
small dialogue agents, following the principles of
the underlying Jaspis architecture. These agents
are small software components, each taking care
of a specific task and in each dialogue turn one
or more agents are selected by DM. In the cur-
rent H&amp;F prototype, there are over 30 dialogue
agents. There is a separate agent for each topic
that can occur in the plan. In practice, one topic
maps to a single plan item. These agents are all
instances of a single class with specific configu-
rations. Each agent handles all situations related
to its topic; when the topic is the first item of an
active plan, they produce related output and
when the user provides input matching to the
topic they forward that information back to the
cognitive model. In addition, topic specific
agents handle explicit topic switch requests from
the user (e.g., “let’s talk about lunch”) and also
take turn if the topic is found on top of the dia-
logue topic stack. A topic ends up in the stack
when it has not been finished and a new topic is
activated. The other agents found in the system
include one that generates a confirmation if the
ASR confidence score is too low, one that re-
peats the last system utterance when the user re-
quests it (“please repeat the last one”), and an
agent to handle ASR rejection errors.
</bodyText>
<subsectionHeader confidence="0.999747">
3.3 Mobile System
</subsectionHeader>
<bodyText confidence="0.999997304347826">
Mobile system is designed mainly to support us-
ers’ on their physical exercises and collected data
on them fro the home system. The mobile system
receives the day plan that the user has made with
the home system and it is used as basis when us-
ers activates the system. This way, the user does
not need to re-enter information such as the type
of an exercise. This is possible, however, with
simple spoken dialogue or by using the graphical
user interface. During the exercise, GPS informa-
tion is used by the system to provide feedback on
pace to the user using speech output. For dia-
logue management, the mobile system uses a
state based model, based on scripting. Since the
mobile system focuses on the physical exercises,
it is aware of the meaning of the predicates it
receives on that level. It knows more about run-
ning and walking than any other component. At
the same time, it ignores most of the day plan it
receives. For example, eating related plan items
are not relevant to the mobile system in any way
and are ignored (however, in the future we could
include the possibility to report on meals as well).
</bodyText>
<subsectionHeader confidence="0.995148">
3.4 Dialogue Plan and Day Plan
</subsectionHeader>
<bodyText confidence="0.99997575">
The communication between the dialogue
managers and CM is based on a dialogue plan
and a day plan. Various kinds of dialogue plans
(Larsson et al., 2000, Jullien and Marty, 1989)
have been used inside dialogue managers in the
past. A plan usually models what the system sees
as the optimal route to task completion.
In H&amp;F, CM provides a plan on how the cur-
rent task (planning a day, reporting on a day)
could proceed. The plan consists of items, which
are basically expressions on domain specific
propositional logic. Example 1 contains two
items from the start of a plan for planning the
day with the user in the morning. The first plan
item (QUERY-PLANNED-ACTIVITY) can be
realized as the question “Anything interesting
planned for today?” by the system.
As new information becomes available (from
the user), it forms a plan for the day or a report of
the day. DM provides this information to CM,
</bodyText>
<page confidence="0.996759">
28
</page>
<bodyText confidence="0.999297">
piece by piece as it becomes available. At the
same time, the information is uploaded into a
web server, where the mobile interface can ac-
cess it anytime.
As CM receives the information, it updates the
dialogue plan as necessary. Query type items,
whose information has been gathered, disappear
from the plan and new items may appear.
The messages sent to CM can add new infor-
mation (predicates) to CM state. DM can also
remove information from CM if previously en-
tered information is found to be untrue. Similarly,
information uploaded to the web server for mo-
bile use can be modified. The information in-
cludes statements on user’s condition (tired),
user’s commitments to the system (will walk to
work), user’s preferences (does not like cafete-
rias) and user’s reports on past activity (took a
taxi to work), which can be accomplishments or
failures of earlier commitments.
</bodyText>
<figure confidence="0.9893273125">
&lt;plan&gt;
&lt;plan-name&gt;Generate-Task-
Model-Questions&lt;/plan-name&gt;
&lt;plan-item&gt;
&lt;action&gt;QUERY-PLANNED-
ACTIVITY&lt;/action&gt;
&lt;/plan-item&gt;
&lt;plan-item&gt;
&lt;action&gt;SUGGEST-TRAVEL-
METHOD&lt;/action&gt;
&lt;param&gt;CYCLING-
TRAVEL&lt;/param&gt;
&lt;param&gt;HOME&lt;/param&gt;
&lt;param&gt;WORK&lt;/param&gt;
&lt;/plan-item&gt;
...
</figure>
<figureCaption confidence="0.980526">
Example 1: Start of a plan.
</figureCaption>
<bodyText confidence="0.993499961538462">
DM in the home system can follow the dia-
logue plan produced by CM step by step. Each
step usually maps to a single question, but can
naturally result in a longer dialogue if the user’s
answer is ambiguous or error management is
necessary, or if DM decides to split a single item
to multiple questions. For example, the two dia-
logue turn pairs seen in example 2 are the result
of a single plan item (QUERY-PLANNED-
ACTIVITY). Since the first user utterance does
not result in a complete, unambiguous predicate,
DM asks a clarification question. A single user
utterance can also result in multiple predicates
(e.g., will not take bus, has preference to walk-
ing).
When the mobile interface is activated, it
downloads the current day plan from the web
server and uses it as a basis for the dialogue it
has with the user. The exercise which will then
take place can be linked to an item in the day
plan, or it can be something new. As the exercise
is completed (or aborted), information in this is
uploaded to the web server. From there the DM
of the home system can download it. This infor-
mation is relevant to the DM when the user is
reporting on a day. The home system downloads
the information provided by the mobile system
and reports it back to CM when the dialogue plan
includes a related item. DM may also provide
some feedback to the user based on the informa-
tion. It is noteworthy, that CM does not need to
differentiate in any way, whether the information
on the exercise came from the mobile system or
was gathered in a dialogue with the home system.
( &lt;plan-item&gt;
&lt;action&gt;QUERY-PLANNED-
ACTIVITY&lt;/action&gt;
&lt;/plan-item&gt;)
S: Good morning. Anything in-
teresting organized for today?
U: I’m going jogging.
(&lt;pred&gt;
&lt;action&gt;PLANNED-
ACTIVITY&lt;/action&gt;
&lt;param&gt;ACTIVITY-
]OGGING&lt;/param&gt;
&lt;param&gt;unknownTime&lt;/param&gt;
&lt;/pred&gt; )
S: Is that jogging exercise
before dinner?
U: No, it’s after.
( &lt;pred&gt;
</bodyText>
<equation confidence="0.4628664">
&lt;action&gt;PLANNED-
ACTIVITY&lt;/action&gt;
&lt;param&gt;ACTIVITY-
]OGGING&lt;/param&gt;
&lt;param&gt;AFTER-DINNER&lt;/param&gt;
</equation>
<bodyText confidence="0.8343055">
&lt;/pred&gt; )
Example 2: A dialogue fragment and a corre-
sponding plan item and predicates, latter of
which is forwarded to the cognitive model and
the mobile interface.
Similarly, clarifications and confirmations are
not directly visible to CM. DM can confirm
items immediately (for example, when low con-
fidence is reported by the NLG component) or it
can delay confirmations to generate a single con-
</bodyText>
<page confidence="0.996764">
29
</page>
<bodyText confidence="0.999936838709677">
firmation for multiple items at an appropriate
moment.
Most importantly, when presenting questions
and suggestions to the user, DM is free to choose
any item in the plan, or even do something not
included in the plan at all. When information
from the mobile system is available, it can direct
where we start the dialogue from. DM could also
decide to do some small-talk to introduce sensi-
tive topics, which can be useful in managing the
user-system relationship (Bickmore and Picard,
2005). In the future, we see DM to have various
kinds of knowledge on the dialogue topics: it can
know how personal these topics are and how top-
ics are related to each other. It may also have
some topics of its own. The communication that
is not related to the domain does not reach CM at
any point.
CM can include additional annotation in the
plan. One such example is the importance of the
information. If information is marked important,
it is likely, but not certain, that DM will explic-
itly confirm it. It is also possible for CM to ex-
plicitly request a confirmation by generating a
separate plan item. For example, if a user reports
on having run much more than they are likely to
be capable of in their condition, CM can generate
a confirmation plan item. It is worth noting, that
DM cannot do reasoning on such level and there-
fore CM must participate in error handling in
such cases.
</bodyText>
<subsectionHeader confidence="0.989377">
3.5 Benefits of the Model
</subsectionHeader>
<bodyText confidence="0.9999852">
The presented model for interoperability be-
tween the mobile system, the DM of the home
system and CM has provided great flexibility for
each component. While the dialogue plan gener-
ated by CM provides a base for dialogue man-
agement, which, in most cases, is followed, DM
can deviate from it. DM can handle confirma-
tions as it pleases, add small talk, and process the
plan items in any order. The model also supports
mixed-initiative dialogues; while DM may fol-
low the plan, the user may discuss any topic. In
our current implementation, user input is parsed
first against the previous system output, next to
the current topic, and finally to the entire space
of known predicates. If needed, we can also
make parsing more detailed by parsing against
dialogue history and the current plan. This way,
the information produced by CM is used in input
parsing. The dialogue plan can be used in dy-
namic construction of recognition grammars to
support this on ASR grammar level.
Most importantly, all this is possible without
including domain specific knowledge. All such
information is kept exclusive in CM. Similarly,
CM does not need to know the interaction level
properties of the topics, such as recognition
grammars and natural language generation de-
tails. These are internal to their specific compo-
nents. The mobile system uses the same knowl-
edge representation as CM, but CM does not
need to be aware of its existence at all. Similarly,
the mobile system can use any part of the infor-
mation it receives, but is not forced to do any-
thing specific. DM just feed all the information
to it and lets it decide what to do with it. When
the mobile system provides information back to
the home system, DM handles this and CM can
ignore completely the fact that different parts of
the information it receives were generated using
different systems. Similarly, the mobile system
does not see any of the internals of the home sys-
tem.
On an implementation level, the model is in-
dependent of the mechanics of either DM or CM.
DM can be implemented using state transition
networks (a network per plan item), forms (form
per item), agent based model, like in the case of
mobile system, or any other suitable method.
Similarly, the plan does not tie CM to any spe-
cific implementation.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999983652173913">
When dialogue systems move beyond limited
task based domains and implement multimodal
interfaces in pervasive computing environment,
their complexity increases rapidly. Dialogue
management, which in most cases is handled
with well understood methods such as form fill-
ing or state transition networks, tends to grow
more complex. Therefore, a model to modularize
dialogue management and domain reasoning is
needed. At the same time, distributed systems
required various kinds of information to be
communicated with components and systems.
While traditional spoken dialogue systems
have been task-based, the Health and Fitness
Companions are part of the users’ life for a long
time, months, or even years. This requires that
they are part of life physically, i.e., interactions
can take place on mobile setting and in home
environment outside of traditional, task-based
computing devices. With the physical presence
of the interface agent and spoken, conversational
dialogue we aim at building social, emotional
relationships between the users and the Compan-
</bodyText>
<page confidence="0.993768">
30
</page>
<bodyText confidence="0.999962911111111">
ion. Such relationships should help us in motivat-
ing the users towards healthier lifestyle. The mo-
bility of the interface integrates the system into
the physical activities they aim at supporting us-
ers in.
We have presented a model, which separates
cognitive modeling from dialogue management
and enables flexible interoperability between the
two and also enables sharing the gathered knowl-
edge to the mobile part of the system and back.
This division, while similar to separation of a
back-end from dialogue management, draws the
line deeper into the area of interaction manage-
ment. The cognitive model processes domain
level information and generates dialogue plans.
The dialogue manager focuses only on interac-
tion level phenomena, such as initiative and error
management, and other meta-communication. In
order to enable flexible interaction, the plan pro-
vides a potential structure for the dialogue, but
the dialogue manager is free to handle things in
different order, and even add new topics. It can
also include input from a mobile interface of the
system without making this explicit to the cogni-
tive model. One example of flexibility is error
management; while the actual error correction is
the task of the dialogue manager, domain level
knowledge can reveal errors. Using the dialogue
plan, the cognitive model can provide such in-
formation to the dialogue manager without
knowledge on details of error management. The
model also enables user initiative topic shifts,
management of user-system relationship and
other novel issues relevant in domain-oriented
dialogue systems.
Overall, the model presented has enabled a
clear division and interoperability of the different
components handling separate parts of the inter-
action. The presented model has been imple-
mented in the Health and Fitness Companion
prototype, and it has enabled the cognitive model,
the dialogue manager, and the mobile interface to
be developed in parallel by different groups us-
ing various programming languages an integrated
system.
</bodyText>
<sectionHeader confidence="0.999524" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999323333333333">
This work is part of the EU-funded COM-
PANIONS-project (IST-34434). The Cognitive
Model has been developed by University of
Teesside, UK, while the mobile interface has
been implemented in Swedish Institute of Com-
puter Science.
</bodyText>
<sectionHeader confidence="0.996267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999979705882353">
Dybkjaer, L., Bernsen, N. O., Minker, W., Evaluation
and usability of multimodal spoken language dia-
logue systems, Speech Communication, 43, 1-2, ,
June 2004, pp. 33-54.
Wilks, Y., Is There Progress on Talking Sensibly to
Machines?, Science, 9 Nov 2007.
Marti, S. and Schmandt, C. Physical embodiments for
mobile communication agents. Proceedings of the
18th annual ACM symposium on User interface
software and technology: 231 – 240, 2005.
Kainulainen, A., Turunen, M., Hakulinen, J., Salonen,
E.-P., Prusi, P., and Helin, L. A Speech-based and
Auditory Ubiquitous Office Environment. Proceed-
ings of 10th International Conference on Speech
and Computer (SPECOM 2005): 231-234, 2005.
Jönsson, A. A Natural Language Shell and Tools for
Customizing the Dialogue in Natural Language In-
terfaces, Research Report, LiTH-IDA-R-91-10,
1991.
Salonen, E.-P., Hartikainen, M., Turunen, M., Haku-
linen J., Funk, J. A. Flexible Dialogue Manage-
ment Using Distributed and Dynamic Dialogue
Control. Proceedings of ICSLP 2004. pp. 197-200.
O&apos;Neill, I. Hanna, P. Liu, X., McTear, M., The
Queen&apos;s Communicator: An Object-Oriented Dia-
logue Manager, Eurospeech 2003, Geneva, Swit-
zerland (2003), pp. 593–596.
Pellom, B. Ward, W. Pradhan, S., The CU Communi-
cator: An Architecture for Dialogue Systems, Pro-
ceedings of ICSLP 2000, Beijing China, November
2000.
Turunen, M., Hakulinen, J., Räihä, K.-J., Salonen, E.-
P., Kainulainen, A., and Prusi, P. An architecture
and applications for speech-based accessibility sys-
tems. IBM Systems Journal, Vol. 44, No 3, 2005,
pp. 485-504.
Cavazza, M., Smith, C., Charlton, D., Zhang, L., Tu-
runen, M. and Hakulinen, J., A ‘Companion’ ECA
with Planning and Activity Modelling, Proceedings
of AAMAS08, 2008 (to appear).
Larsson, S. Ljunglöf, P. Cooper, R. Engdahl, E.,
Ericsson. S. GoDiS - an accommodating dialogue
system. ANLP / NAACL &apos;00 Workshop on Con-
versational Systems, May 2000.
Jullien C., Marty, J.-C. Plan revision in person-
machine dialogue, Proceedings of ACL‘89, Man-
chester, England, April 1989, pp.153-160.
Bickmore, T. W., Picard, R. W. Establishing and
maintaining long-term human-computer relation-
ships. ACM Trans. Computer-Human. Interaction
Vol. 12, No. 2. (June 2005), pp. 293-327.
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.648429">
<title confidence="0.9990355">Interoperability and Knowledge Representation in Distributed and Fitness Companion Dialogue System</title>
<author confidence="0.981572">Jaakko</author>
<affiliation confidence="0.999967">Department of Computer</affiliation>
<address confidence="0.99983">33014 University of Tampere, Finland</address>
<email confidence="0.845834">jaakko.hakulinen@cs.uta.fiMarkku</email>
<affiliation confidence="0.999873">Department of Computer</affiliation>
<address confidence="0.99907">33014 University of Tampere, Finland</address>
<email confidence="0.988121">markku.turunen@cs.uta.fi</email>
<abstract confidence="0.99829685">As spoken dialogue systems move beyond task oriented dialogues and become distributed in the pervasive computing environments, their growing complexity calls for more modular structures. When different aspects of a single system can be accessed with different interfaces, knowledge representation and separation of low level interaction modeling from high level reasoning on domain level becomes important. In this paper, a model utilizing a dialogue plan to communicate information from domain level planner to dialogue management and from there to a separate mobile interface is presented. The model enables each part of the system handle the same information from their own perspectives without containing overlapping logic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Dybkjaer</author>
<author>N O Bernsen</author>
<author>W Minker</author>
</authors>
<title>Evaluation and usability of multimodal spoken language dialogue systems,</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>43</volume>
<pages>1--2</pages>
<location></location>
<contexts>
<context position="1364" citStr="Dybkjaer et al, 2004" startWordPosition="185" endWordPosition="188"> In this paper, a model utilizing a dialogue plan to communicate information from domain level planner to dialogue management and from there to a separate mobile interface is presented. The model enables each part of the system handle the same information from their own perspectives without containing overlapping logic. 1 Introduction Most existing spoken dialogue systems provide a single interface to solve a well-defined task, such as booking tickets or providing timetable information. There are emerging areas that differ dramatically from task-oriented systems. In domain-oriented dialogues (Dybkjaer et al, 2004) the interaction with the system, typically modeled as a conversation with a virtual humanlike character, can be the main motivation for the interaction. These systems are often multimodal, and may take place in pervasive computing environments where various mobile, robotic, and other untraditional interface are used to communicate with the system. For example, in the EUfunded COMPANIONS-project (Wilks, 2007) © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported liCense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. we are de</context>
</contexts>
<marker>Dybkjaer, Bernsen, Minker, 2004</marker>
<rawString>Dybkjaer, L., Bernsen, N. O., Minker, W., Evaluation and usability of multimodal spoken language dialogue systems, Speech Communication, 43, 1-2, , June 2004, pp. 33-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Is There Progress on Talking Sensibly to</title>
<date>2007</date>
<journal>Machines?, Science,</journal>
<volume>9</volume>
<contexts>
<context position="1776" citStr="Wilks, 2007" startWordPosition="250" endWordPosition="251">ined task, such as booking tickets or providing timetable information. There are emerging areas that differ dramatically from task-oriented systems. In domain-oriented dialogues (Dybkjaer et al, 2004) the interaction with the system, typically modeled as a conversation with a virtual humanlike character, can be the main motivation for the interaction. These systems are often multimodal, and may take place in pervasive computing environments where various mobile, robotic, and other untraditional interface are used to communicate with the system. For example, in the EUfunded COMPANIONS-project (Wilks, 2007) © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported liCense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. we are developing a conversational Health and Fitness Companion that develops long-lasting relationships with its users to support their healthy living and eating habits via mobile and physical agent interfaces. Such systems have different motivations for use compared to traditional task-based spoken dialogue systems. Instead of helping with a single, well defined task, the system aims at building a long-term relation</context>
</contexts>
<marker>Wilks, 2007</marker>
<rawString>Wilks, Y., Is There Progress on Talking Sensibly to Machines?, Science, 9 Nov 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Marti</author>
<author>C Schmandt</author>
</authors>
<title>Physical embodiments for mobile communication agents.</title>
<date>2005</date>
<booktitle>Proceedings of the 18th annual ACM symposium on User interface software and technology: 231 – 240,</booktitle>
<contexts>
<context position="3355" citStr="Marti and Schmandt (2005)" startWordPosition="488" endWordPosition="491"> during the moments when their hand or eyes are busy. Physical agent interfaces, on the other hand, have been used in systems, which try to make dialogue systems more part of people’s life. In many cases, they include rich multimodal input and output while providing a physical outlook for the agent. While naturalistic human-like physical robots are under development, especially in Japan, there is room for a variety of different physical interface agents ranging from completely abstract (e.g., simple devices with lights and sound) to highly sophisticated anthropomorphic apparatus. For example, Marti and Schmandt (2005) used several toy animals, such as bunnies and squirrels, as physical embodied agents for a conversational system. Other example is an in-door guidance and receptionist application involving a physical interface agent that combines pointing gestures with conversational speech technology (Kainulainen et al., 2005). Some physical agent technology has also 24 Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 24–31 Manchester, August 2008 been commercialized. For example, the wireless NabaztagTM/tag rabbits (http://www.n</context>
</contexts>
<marker>Marti, Schmandt, 2005</marker>
<rawString>Marti, S. and Schmandt, C. Physical embodiments for mobile communication agents. Proceedings of the 18th annual ACM symposium on User interface software and technology: 231 – 240, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kainulainen</author>
<author>M Turunen</author>
<author>J Hakulinen</author>
<author>E-P Salonen</author>
<author>P Prusi</author>
<author>L Helin</author>
</authors>
<title>A Speech-based and Auditory Ubiquitous Office Environment.</title>
<date>2005</date>
<booktitle>Proceedings of 10th International Conference on Speech and Computer (SPECOM</booktitle>
<pages>231--234</pages>
<contexts>
<context position="3669" citStr="Kainulainen et al., 2005" startWordPosition="534" endWordPosition="537">istic human-like physical robots are under development, especially in Japan, there is room for a variety of different physical interface agents ranging from completely abstract (e.g., simple devices with lights and sound) to highly sophisticated anthropomorphic apparatus. For example, Marti and Schmandt (2005) used several toy animals, such as bunnies and squirrels, as physical embodied agents for a conversational system. Other example is an in-door guidance and receptionist application involving a physical interface agent that combines pointing gestures with conversational speech technology (Kainulainen et al., 2005). Some physical agent technology has also 24 Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 24–31 Manchester, August 2008 been commercialized. For example, the wireless NabaztagTM/tag rabbits (http://www.nabaztag.com/) have been success Both mobile use and physical agent interface can support the goal of making a spoken dialogue system part of users’ everyday life and building a meaningful relationship between the system and the user. It has been found that mere existence of a physical interface changes users’ att</context>
</contexts>
<marker>Kainulainen, Turunen, Hakulinen, Salonen, Prusi, Helin, 2005</marker>
<rawString>Kainulainen, A., Turunen, M., Hakulinen, J., Salonen, E.-P., Prusi, P., and Helin, L. A Speech-based and Auditory Ubiquitous Office Environment. Proceedings of 10th International Conference on Speech and Computer (SPECOM 2005): 231-234, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jönsson</author>
</authors>
<title>A Natural Language Shell and Tools for Customizing the Dialogue in Natural Language Interfaces,</title>
<date>1991</date>
<tech>Research Report, LiTH-IDA-R-91-10,</tech>
<contexts>
<context position="6227" citStr="Jönsson, 1991" startWordPosition="931" endWordPosition="932">l, the information must be communicated over different interfaces and the component inside them. Therefore, modularization of the system and appropriate knowledge representation become vital. On dialogue management level, a common way to take some complexity away from the dialogue manager and limit its tasks more specifically to dialogue management is to separate domain specific processing, such as database queries, into a back-end component. Many researchers have worked with separating generic dialogue management processes from the domain specific processes. Example solutions include shells (Jönsson, 1991) and object oriented programming methods (Salonen, et al., 2004, O’Neill, et al., 2003). On the other hand, a simple back-end and an active user community has emerged around it. interface, e.g., SQL queries, can be included as configuration parameters (Pellon et al., 2000). Since dialogue management is usually based on state transition networks, form filling, or some other clearly defined model, separating domain specific processing to the back-end makes it possible to implemented dialogue management purely with the selected model. Health and Fitness Companion, as discussed in the following, i</context>
</contexts>
<marker>Jönsson, 1991</marker>
<rawString>Jönsson, A. A Natural Language Shell and Tools for Customizing the Dialogue in Natural Language Interfaces, Research Report, LiTH-IDA-R-91-10, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E-P Salonen</author>
<author>M Hartikainen</author>
<author>M Turunen</author>
<author>J Hakulinen</author>
<author>J A Funk</author>
</authors>
<title>Flexible Dialogue Management Using Distributed and Dynamic Dialogue Control.</title>
<date>2004</date>
<booktitle>Proceedings of ICSLP</booktitle>
<pages>197--200</pages>
<contexts>
<context position="6290" citStr="Salonen, et al., 2004" startWordPosition="939" endWordPosition="942">nterfaces and the component inside them. Therefore, modularization of the system and appropriate knowledge representation become vital. On dialogue management level, a common way to take some complexity away from the dialogue manager and limit its tasks more specifically to dialogue management is to separate domain specific processing, such as database queries, into a back-end component. Many researchers have worked with separating generic dialogue management processes from the domain specific processes. Example solutions include shells (Jönsson, 1991) and object oriented programming methods (Salonen, et al., 2004, O’Neill, et al., 2003). On the other hand, a simple back-end and an active user community has emerged around it. interface, e.g., SQL queries, can be included as configuration parameters (Pellon et al., 2000). Since dialogue management is usually based on state transition networks, form filling, or some other clearly defined model, separating domain specific processing to the back-end makes it possible to implemented dialogue management purely with the selected model. Health and Fitness Companion, as discussed in the following, is based on a model where the domain specific module is more tha</context>
</contexts>
<marker>Salonen, Hartikainen, Turunen, Hakulinen, Funk, 2004</marker>
<rawString>Salonen, E.-P., Hartikainen, M., Turunen, M., Hakulinen J., Funk, J. A. Flexible Dialogue Management Using Distributed and Dynamic Dialogue Control. Proceedings of ICSLP 2004. pp. 197-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Hanna O&apos;Neill</author>
<author>P Liu</author>
<author>X McTear</author>
<author>M</author>
</authors>
<title>The Queen&apos;s Communicator: An Object-Oriented Dialogue Manager,</title>
<date>2003</date>
<pages>593--596</pages>
<location>Eurospeech</location>
<marker>O&apos;Neill, Liu, McTear, M, 2003</marker>
<rawString>O&apos;Neill, I. Hanna, P. Liu, X., McTear, M., The Queen&apos;s Communicator: An Object-Oriented Dialogue Manager, Eurospeech 2003, Geneva, Switzerland (2003), pp. 593–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ward Pellom</author>
<author>W Pradhan</author>
<author>S</author>
</authors>
<title>The CU Communicator: An Architecture for Dialogue Systems,</title>
<date>2000</date>
<booktitle>Proceedings of ICSLP 2000,</booktitle>
<location>Beijing China,</location>
<marker>Pellom, Pradhan, S, 2000</marker>
<rawString>Pellom, B. Ward, W. Pradhan, S., The CU Communicator: An Architecture for Dialogue Systems, Proceedings of ICSLP 2000, Beijing China, November 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turunen</author>
<author>J Hakulinen</author>
<author>K-J Räihä</author>
<author>E-P Salonen</author>
<author>A Kainulainen</author>
<author>P Prusi</author>
</authors>
<title>An architecture and applications for speech-based accessibility systems.</title>
<date>2005</date>
<journal>IBM Systems Journal,</journal>
<volume>44</volume>
<pages>485--504</pages>
<contexts>
<context position="10903" citStr="Turunen et al., 2005" startWordPosition="1702" endWordPosition="1705">letion of the exercises, which information is then forwarded to the home system and the cognitive model. From technical viewpoint, H&amp;F is a multimodal spoken dialogue system containing components for speech recognition (ASR), natural language understanding (NLU), dialogue management (DM), natural language generation (NLG), and speech synthesis (TTS). Furthermore, it includes a separate cognitive model (CM), which works in close co-operation with DM of the home system, as presented in the following sections. The dialogue system in the home system is implemented using Java and Jaspis framework (Turunen et al., 2005) with jNabServer (http://www.cs.uta.fi/hci/spi/jnabserver/) for Nabaztag connectivity. The cognitive model is implemented in Lisp and integrated into the Jaspis framework. The mobile interface is implemented in Java with native C++ code for speech technology components. It uses PART (http://part.sourceforge.net/) for persistent storage and HECL for scripting in dialogue manager (http://sourceforge.net/projects/hecl). GPS status bar Avatar and text output area Exercise status bar Figure 2: Mobile Companion Interface. - Good morning, anything interesting organized for today? - I’m going for a wa</context>
</contexts>
<marker>Turunen, Hakulinen, Räihä, Salonen, Kainulainen, Prusi, 2005</marker>
<rawString>Turunen, M., Hakulinen, J., Räihä, K.-J., Salonen, E.-P., Kainulainen, A., and Prusi, P. An architecture and applications for speech-based accessibility systems. IBM Systems Journal, Vol. 44, No 3, 2005, pp. 485-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cavazza</author>
<author>C Smith</author>
<author>D Charlton</author>
<author>L Zhang</author>
<author>M Turunen</author>
<author>J Hakulinen</author>
</authors>
<date>2008</date>
<booktitle>A ‘Companion’ ECA with Planning and Activity Modelling, Proceedings of AAMAS08,</booktitle>
<note>(to appear).</note>
<contexts>
<context position="14552" citStr="Cavazza et al., 2008" startWordPosition="2279" endWordPosition="2282">a cognitive model (CM) from dialogue manager (DM), as seen in Figure 3. We call this module the cognitive model, because it contains what can be considered higher level cognitive processes of the system. Next, we present DM of the home system, CM component, and the mobile interface, focusing on their interaction. Figure 3: Information passed between the components. 3.1 Cognitive Model Responsibilities The task of CM is to model the domain, i.e., know what to recommend to the user, what to ask from the user and what kind of feedback to provide. CM in H&amp;F uses hierarchical task networks (HTNs) (Cavazza et al., 2008) as the method of planning healthy daily activity for the user. Part of a network can be seen in Figure 4. In the current H&amp;F implementation, the planning domain included 16 axioms and 111 methods, 49 operators, 42 semantic tags, 113 evaluation rules and there are 17 different topics to be discussed with the user. Figure 4: Hierachical Task Network. CM is aware of the meaning of the concepts inside the system on a domain specific level. It generates and updates a dialogue plan according to the information received from the user. The plan is forwarded to DM. Interaction level issues are not dir</context>
</contexts>
<marker>Cavazza, Smith, Charlton, Zhang, Turunen, Hakulinen, 2008</marker>
<rawString>Cavazza, M., Smith, C., Charlton, D., Zhang, L., Turunen, M. and Hakulinen, J., A ‘Companion’ ECA with Planning and Activity Modelling, Proceedings of AAMAS08, 2008 (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S</author>
</authors>
<title>GoDiS - an accommodating dialogue system.</title>
<date>2000</date>
<booktitle>ANLP / NAACL &apos;00 Workshop on Conversational Systems,</booktitle>
<marker>S, 2000</marker>
<rawString>Larsson, S. Ljunglöf, P. Cooper, R. Engdahl, E., Ericsson. S. GoDiS - an accommodating dialogue system. ANLP / NAACL &apos;00 Workshop on Conversational Systems, May 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jullien</author>
<author>J-C Marty</author>
</authors>
<title>Plan revision in personmachine dialogue,</title>
<date>1989</date>
<booktitle>Proceedings of ACL‘89,</booktitle>
<pages>153--160</pages>
<location>Manchester, England,</location>
<contexts>
<context position="19372" citStr="Jullien and Marty, 1989" startWordPosition="3121" endWordPosition="3124">the physical exercises, it is aware of the meaning of the predicates it receives on that level. It knows more about running and walking than any other component. At the same time, it ignores most of the day plan it receives. For example, eating related plan items are not relevant to the mobile system in any way and are ignored (however, in the future we could include the possibility to report on meals as well). 3.4 Dialogue Plan and Day Plan The communication between the dialogue managers and CM is based on a dialogue plan and a day plan. Various kinds of dialogue plans (Larsson et al., 2000, Jullien and Marty, 1989) have been used inside dialogue managers in the past. A plan usually models what the system sees as the optimal route to task completion. In H&amp;F, CM provides a plan on how the current task (planning a day, reporting on a day) could proceed. The plan consists of items, which are basically expressions on domain specific propositional logic. Example 1 contains two items from the start of a plan for planning the day with the user in the morning. The first plan item (QUERY-PLANNED-ACTIVITY) can be realized as the question “Anything interesting planned for today?” by the system. As new information b</context>
</contexts>
<marker>Jullien, Marty, 1989</marker>
<rawString>Jullien C., Marty, J.-C. Plan revision in personmachine dialogue, Proceedings of ACL‘89, Manchester, England, April 1989, pp.153-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Bickmore</author>
<author>R W Picard</author>
</authors>
<title>Establishing and maintaining long-term human-computer relationships.</title>
<date>2005</date>
<journal>ACM Trans. Computer-Human. Interaction</journal>
<volume>12</volume>
<pages>293--327</pages>
<contexts>
<context position="24124" citStr="Bickmore and Picard, 2005" startWordPosition="3894" endWordPosition="3897">immediately (for example, when low confidence is reported by the NLG component) or it can delay confirmations to generate a single con29 firmation for multiple items at an appropriate moment. Most importantly, when presenting questions and suggestions to the user, DM is free to choose any item in the plan, or even do something not included in the plan at all. When information from the mobile system is available, it can direct where we start the dialogue from. DM could also decide to do some small-talk to introduce sensitive topics, which can be useful in managing the user-system relationship (Bickmore and Picard, 2005). In the future, we see DM to have various kinds of knowledge on the dialogue topics: it can know how personal these topics are and how topics are related to each other. It may also have some topics of its own. The communication that is not related to the domain does not reach CM at any point. CM can include additional annotation in the plan. One such example is the importance of the information. If information is marked important, it is likely, but not certain, that DM will explicitly confirm it. It is also possible for CM to explicitly request a confirmation by generating a separate plan ite</context>
</contexts>
<marker>Bickmore, Picard, 2005</marker>
<rawString>Bickmore, T. W., Picard, R. W. Establishing and maintaining long-term human-computer relationships. ACM Trans. Computer-Human. Interaction Vol. 12, No. 2. (June 2005), pp. 293-327.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>