<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001039">
<title confidence="0.855467">
Man* vs. Machine: A Case Study in Base Noun Phrase Learning
</title>
<author confidence="0.961812">
Eric Brill and Grace Ngai
</author>
<affiliation confidence="0.8523575">
Department of Computer Science
The Johns Hopkins University
</affiliation>
<address confidence="0.521338">
Baltimore, MD 21218, USA
</address>
<email confidence="0.711835">
Email: , gyn}@cs . jhu . edu
</email>
<sectionHeader confidence="0.99146" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982285714286">
A great deal of work has been done demonstrat-
ing the ability of machine learning algorithms to
automatically extract linguistic knowledge from
annotated corpora. Very little work has gone
into quantifying the difference in ability at this
task between a person and a machine. This pa-
per is a first step in that direction.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999478814814815">
Machine learning has been very successful at
solving many problems in the field of natural
language processing. It has been amply demon-
strated that a wide assortment of machine learn-
ing algorithms are quite effective at extracting
linguistic information from manually annotated
corpora.
Among the machine learning algorithms stud-
ied, rule based systems have proven effective
on many natural language processing tasks,
including part-of-speech tagging (Brill, 1995;
Ramshaw and Marcus, 1994), spelling correc-
tion (Mangu and Brill, 1997), word-sense dis-
ambiguation (Gale et al., 1992), message un-
derstanding (Day et al., 1997), discourse tag-
ging (Samuel et al., 1998), accent restoration
(Yarowsky, 1994), prepositional-phrase attach-
ment (Brill and Resnik, 1994) and base noun
phrase identification (Ramshaw and Marcus, In
Press; Cardie and Pierce, 1998; Veenstra, 1998;
Argamon et al., 1998). Many of these rule based
systems learn a short list of simple rules (typ-
ically on the order of 50-300) which are easily
understood by humans.
Since these rule-based systems achieve good
performance while learning a small list of sim-
ple rules, it raises the question of whether peo-
</bodyText>
<subsectionHeader confidence="0.46373">
*and Woman.
</subsectionHeader>
<bodyText confidence="0.999883058823529">
ple could also derive an effective rule list man-
ually from an annotated corpus. In this pa-
per we explore how quickly and effectively rel-
atively untrained people can extract linguistic
generalities from a corpus as compared to a ma-
chine. There are a number of reasons for doing
this. We would like to understand the relative
strengths and weaknesses of humans versus ma-
chines in hopes of marrying their complemen-
tary strengths to create even more accurate sys-
tems. Also, since people can use their meta-
knowledge to generalize from a small number of
examples, it is possible that a person could de-
rive effective linguistic knowledge from a much
smaller training corpus than that needed by a
machine. A person could also potentially learn
more powerful representations than a machine,
thereby achieving higher accuracy.
In this paper we describe experiments we per-
formed to ascertain how well humans, given
an annotated training set, can generate rules
for base noun phrase chunking. Much previous
work has been done on this problem and many
different methods have been used: Church&apos;s
PARTS (1988) program uses a Markov model;
Bourigault (1992) uses heuristics along with a
grammar; Voutilainen&apos;s NPTool (1993) uses a
lexicon combined with a constraint grammar;
Juteson and Katz (1995) use repeated phrases;
Veenstra (1998), Argamon, Dagan &amp; Kry-
molowski(1998) and Daelemans, van den Bosch
&amp; Zavrel (1999) use memory-based systems;
Ramshaw &amp; Marcus (In Press) and Cardie &amp;
Pierce (1998) use rule-based systems.
</bodyText>
<sectionHeader confidence="0.977623" genericHeader="method">
2 Learning Base Noun Phrases by
Machine
</sectionHeader>
<bodyText confidence="0.997855333333333">
We used the base noun phrase system of
Ramshaw and Marcus (R&amp;M) as the machine
learning system with which to compare the hu-
</bodyText>
<page confidence="0.999268">
65
</page>
<bodyText confidence="0.997896480769231">
man learners. It is difficult to compare different
machine learning approaches to base NP anno-
tation, since different definitions of base NP are
used in many of the papers, but the R&amp;M sys-
tem is the best of those that have been tested
on the Penn Treebank.1
To train their system, R&amp;M used a 200k-word
chunk of the Penn Treebank Parsed Wall Street
Journal (Marcus et al., 1993) tagged using a
transformation-based tagger (Brill, 1995) and
extracted base noun phrases from its parses by
selecting noun phrases that contained no nested
noun phrases and further processing the data
with some heuristics (like treating the posses-
sive marker as the first word of a new base
noun phrase) to flatten the recursive struc-
ture of the parse. They cast the problem as
a transformation-based tagging problem, where
each word is to be labelled with a chunk struc-
ture tag from the set II, 0, Bl, where words
marked &amp;quot;I&amp;quot; are inside some base NP chunk,
those marked &amp;quot;0&amp;quot; are not part of any base NP,
and those marked &amp;quot;B&amp;quot; denote the first word
of a base NP which immediately succeeds an-
other base NP. The training corpus is first run
through a part-of-speech tagger. Then, as a
baseline annotation, each word is labelled with
the most common chunk structure tag for its
part-of-speech tag.
After the baseline is achieved, transforma-
tion rules fitting a set of rule templates are
then learned to improve the &amp;quot;tagging accuracy&amp;quot;
of the training set. These templates take into
consideration the word, part-of-speech tag and
chunk structure tag of the current word and all
words within a window of 3 to either side of it.
Applying a rule to a word changes the chunk
structure tag of a word and in effect alters the
boundaries of the base NP chunks in the sen-
tence.
An example of a rule learned by the R&amp;M sys-
tem is: change a chunk structure tag of a word
from I to B if the word is a determiner, the next
word is a noun, and the two previous words both
have chunk structure tags of I. In other words,
a determiner in this context is likely to begin a
noun phrase. The R&amp;M system learns a total
INVe would like to thank Lance Ramshaw for pro-
viding us with the base-NP-annotated training and test
corpora that were used in the R&amp;M system, as well as
the rules learned by this system.
of 500 rules.
</bodyText>
<sectionHeader confidence="0.965255" genericHeader="method">
3 Manual Rule Acquisition
</sectionHeader>
<bodyText confidence="0.975519225806452">
R&amp;M framed the base NP annotation problem
as a word tagging problem. We chose instead
to use regular expressions on words and part of
speech tags to characterize the NPs, as well as
the context surrounding the NPs, because this
is both a more powerful representational lan-
guage and more intuitive to a person. A person
can more easily consider potential phrases as a
sequence of words and tags, rather than looking
at each individual word and deciding whether it
is part of a phrase or not. The rule actions we
allow are:2
Add Add a base NP (bracket a se-
quence of words as a base NP)
Kill Delete a base NP (remove a pair
of parentheses)
Transform Transform a base NP (move
one or both parentheses to ex-
tend/contract a base NP)
Merge Merge two base NPs
As an example, we consider an actual rule
from our experiments:
Bracket all sequences of words of: one
determiner (DT), zero or more adjec-
tives (JJ, JJR, JJS), and one or more
nouns (NN, NNP, NNS, NNPS), if
they are followed by a verb (VB, VBD,
VBG, VBN, VBP, VBZ).
In our language, the rule is written thus:3
A
(* . )
</bodyText>
<listItem confidence="0.812866">
({1} t=DT) (* t=JJ [RS] ?) (+ t=NNP?S?)
({1} t=VB [DGNPZ] ?)
</listItem>
<bodyText confidence="0.9998685">
The first line denotes the action, in this case,
Add a bracketing. The second line defines the
context preceding the sequence we want to have
bracketed — in this case, we do not care what
this sequence is. The third line defines the se-
quence which we want bracketed, and the last
</bodyText>
<footnote confidence="0.9970792">
2The rule types we have chosen are similar to those
used by Vilain and Day (1996) in transformation-based
parsing, but are more powerful.
3A full description of the rule language can be found
at http://n1p.cs.jhu.eduk,,baseNP/manual.
</footnote>
<page confidence="0.829724">
66
</page>
<equation confidence="0.970089666666667">
s{(([ \s_]+__DT\s+) ([\s_]+__JJ [RS] \s+)*
( [^ \s _]+__NNp?s?\s+)+) ([\s_] +VB [DGNPZ] \s+)}
( $1 ) $5 1g
</equation>
<bodyText confidence="0.99996953125">
line defines the context following the bracketed
sequence.
Internally, the software then translates this
rule into the more unwieldy Pen l regular expres-
sion:
The base NP annotation system created by
the humans is essentially a transformation-
based system with hand-written rules. The user
manually creates an ordered list of rules. A
rule list can be edited by adding a rule at any
position, deleting a rule, or modifying a rule.
The user begins with an empty rule list. Rules
are derived by studying the training corpus
and NPs that the rules have not yet bracketed,
as well as NPs that the rules have incorrectly
bracketed. Whenever the rule list is edited, the
efficacy of the changes can be checked by run-
ning the new rule list on the training set and
seeing how the modified rule list compares to
the unmodified list. Based on this feedback,
the user decides whether to accept or reject
the changes that were made. One nice prop-
erty of transformation-based learning is that in
appending a rule to the end of a rule list, the
user need not be concerned about how that rule
may interact with other rules on the list. This
is much easier than writing a CFG, for instance,
where rules interact in a way that may not be
readily apparent to a human rule writer.
To make it easy for people to study the train-
ing set, word sequences are presented in one of
four colors indicating that they:
</bodyText>
<listItem confidence="0.994575583333333">
1. are not part of an NP either in the truth or
in the output of the person&apos;s rule set
2. consist of an NP both in the truth and in
the output of the person&apos;s rule set (i.e. they
constitute a base NP that the person&apos;s rules
correctly annotated)
3. consist of an NP in the truth but not in the
output of the person&apos;s rule set (i.e. they
constitute a recall error)
4. consist of an NP in the output of the per-
son&apos;s rule set but not in the truth (i.e. they
constitute a precision error)
</listItem>
<bodyText confidence="0.966553833333333">
The actual system is located at
http://n1p. cs . jhu.eduk-dbasenpichunking.
A screenshot of this system is shown in figure
4. The correct base NPs are enclosed in paren-
theses and those annotated by the human&apos;s
rules in brackets.
</bodyText>
<sectionHeader confidence="0.983337" genericHeader="method">
4 Experimental Set-Up and Results
</sectionHeader>
<bodyText confidence="0.993139659090909">
The experiment of writing rule lists for base NP
annotation was assigned as a homework set to
a group of 11 undergraduate and graduate stu-
dents in an introductory natural language pro-
cessing course.4
The corpus that the students were given from
which to derive and validate rules is a 25k word
subset of the R&amp;M training set, approximately
the size of the full R&amp;M training set. The
reason we used a downsized training set was
that we believed humans could generalize better
from less data, and we thought that it might be
possible to meet or surpass R&amp;M&apos;s results with
a much smaller training set.
Figure 1 shows the final precision, recall, F-
measure and precision+recall numbers on the
training and test corpora for the students.
There was very little difference in performance
on the training set compared to the test set.
This indicates that people, unlike machines,
seem immune to overtraining. The time the
students spent on the problem ranged from less
than 3 hours to almost 10 hours, with an av-
erage of about 6 hours. While it was certainly
the case that the students with the worst results
spent the least amount of time on the prob-
lem, it was not true that those with the best
results spent the most time — indeed, the av-
erage amount of time spent by the top three
students was a little less than the overall aver-
age — slightly over 5 hours. On average, peo-
ple achieved 90% of their final performance after
half of the total time they spent in rule writing.
The number of rules in the final rule lists also
varied, from as few as 16 rules to as many as 61
rules, with an average of 35.6 rules. Again, the
average number for the top three subjects was
a little under the average for everybody: 30.3
rules.
4These 11 students were a subset of the entire class.
Students were given an option of participating in this ex-
periment or doing a much more challenging final project.
Thus, as a population, they tended to be the less moti-
vated students.
</bodyText>
<page confidence="0.995001">
67
</page>
<table confidence="0.999819142857143">
TRAINING SET (25K Words) TEST SET
Precision Recall F-Measure 11 Precision Recall F-Measure P42-R
-1-ff-
Student 1 87.8% 88.6% 88.2 88.2 88.0% 88.8% 88.4 88.4
Student 2 88.1% 88.2% 88.2 88.2 88.2% 87.9% 88.0 88.1
Student 3 88.6% 87.6% 88.1 88.2 88.3% 87.8% 88.0 88.1
Student 4 88.0% 87.2% 87.6 87.6 86.9% 85.9% 86.4 86.4
Student 5 86.2% 86.8% 86.5 86.5 85.8% 85.8% 85.8 85.8
Student 6 86.0% 87.1% 86.6 86.6 85.8% 87.1% 86.4 86.5
Student 7 84.9% 86.7% 85.8 85.8 85.3% 87.3% 86.3 86.3
Student 8 83.6% 86.0% 84.8 84.8 83.1% 85.7% 84.4 84.4
Student 9 83.9% 85.0% 84.4 84.5 83.5% 84.8% 84.1 84.2
Student 10 82.8% 84.5% 83.6 83.7 83.3% 84.4% 83.8 83.8
Student 11 84.8% 78.8% 81.7 81.8 84.0% 77.4% 80.6 80.7
</table>
<figureCaption confidence="0.994835">
Figure 1: P/R results of test subjects on training and test corpora
</figureCaption>
<bodyText confidence="0.999973909090909">
In the beginning, we believed that the stu-
dents would be able to match or better the
R&amp;M system&apos;s results, which are shown in fig-
ure 2. It can be seen that when the same train-
ing corpus is used, the best students do achieve
performances which are close to the R&amp;M sys-
tem&apos;s - on average, the top 3 students&apos; per-
formances come within 0.5% precision and 1.1%
recall of the machine&apos;s. In the following section,
we will examine the output of both the manual
and automatic systems for differences.
</bodyText>
<sectionHeader confidence="0.977384" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999964298245614">
Before we started the analysis of the test set,
we hypothesized that the manually derived sys-
tems would have more difficulty with potential
rules that are effective, but fix only a very small
number of mistakes in the training set.
The distribution of noun phrase types, iden-
tified by their part of speech sequence, roughly
obeys Zipf&apos;s Law (Zipf, 1935): there is a large
tail of noun phrase types that occur very infre-
quently in the corpus. Assuming there is not a
rule that can generalize across a large number
of these low-frequency noun phrases, the only
way noun phrases in the tail of the distribution
can be learned is by learning low-count rules: in
other words, rules that will only positively af-
fect a small number of instances in the training
corpus.
Van der Dosch and Daelemans (1998) show
that not ignoring the low count instances is of-
ten crucial to performance in machine learning
systems for natural language. Do the human-
written rules suffer from failing to learn these
infrequent phrases?
To explore the hypothesis that a primary dif-
ference between the accuracy of human and ma-
chine is the machine&apos;s ability to capture the low
frequency noun phrases, we observed how the
accuracy of noun phrase annotation of both hu-
man and machine derived rules is affected by
the frequency of occurrence of the noun phrases
in the training corpus. We reduced each base
NP in the test set to its POS tag sequence as
assigned by the POS tagger. For each POS tag
sequence, we then counted the number of times
it appeared in the training set and the recall
achieved on the test set.
The plot of the test set recall vs. the number
of appearances in the training set of each tag
sequence for the machine and the mean of the
top 3 students is shown in figure 3. For instance,
for base NPs in the test set with tag sequences
that appeared 5 times in the training corpus,
the students achieved an average recall of 63.6%
while the machine achieved a recall of 83.5%.
For base NPs with tag sequences that appear
less than 6 times in the training set, the machine
outperforms the students by a recall of 62.8%
vs. 54.8%. However, for the rest of the base
NPs - those that appear 6 or more times -
the performances of the machine and students
are almost identical: 93.7% for the machine vs.
93.5% for the 3 students, a difference that is not
statistically significant.
The recall graph clearly shows that for the
top 3 students, performance is comparable to
the machine&apos;s on all but the low frequency con-
stituents. This can be explained by the human&apos;s
</bodyText>
<page confidence="0.998501">
68
</page>
<table confidence="0.992131666666667">
Training set size(words) Precision Recall F-Measure EP
25k 88.7% 89.3% 89.0 89.0
200k 91.8% 92.3% 92.0 92.1
</table>
<figureCaption confidence="0.916832">
Figure 2: P/R results of the R&amp;M system on test corpus
</figureCaption>
<figure confidence="0.9909672">
- &amp;quot;Madline
--Students
••• *.2
.L
Number of Appearances In Training Set
</figure>
<figureCaption confidence="0.999959">
Figure 3: Test Set Recall vs. Frequency of Appearances in Training Set.
</figureCaption>
<bodyText confidence="0.882368730769231">
reluctance or inability to write a rule that will
only capture a small number of new base NPs in
the training set. Whereas a machine can easily
learn a few hundred rules, each of which makes
a very small improvement to accuracy, this is a
tedious task for a person, and a task which ap-
parently none of our human subjects was willing
or able to take on.
There is one anomalous point in figure 3. For
base NPs with POS tag sequences that appear
3 times in the training set, there is a large de-
crease in recall for the machine, but a large
increase in recall for the students. When we
looked at the POS tag sequences in question and
their corresponding base NPs, we found that
this was caused by one single POS tag sequence
— that of two successive numbers (CD). The
test set happened to include many sentences
containing sequences of the type:
. . . ( CD CD ) TO ( CD CD )
as in:
( International/NNP Paper/NNP )
fell/VBD ( 1/CD 1/CD ) to/TO (
51/CD 1/CD )...
while the training set had none. The machine
ended up bracketing the entire sequence
</bodyText>
<note confidence="0.400662">
1/CD 1/CD to/TO 51/CD 1/CD
</note>
<footnote confidence="0.641334">
as a base NP. None of the students, however,
made this mistake.
</footnote>
<page confidence="0.999236">
69
</page>
<sectionHeader confidence="0.986848" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999801040816327">
In this paper we have described research we un-
dertook in an attempt to ascertain how people
can perform compared to a machine at learning
linguistic information from an annotated cor-
pus, and more importantly to begin to explore
the differences in learning behavior between hu-
man and machine. Although people did not
match the performance of the machine-learned
annotator, it is interesting that these &amp;quot;language
novices&amp;quot;, with almost no training, were able to
come fairly close, learning a small number of
powerful rules in a short amount of time on a
small training set. This challenges the claim
that machine learning offers portability advan-
tages over manual rule writing, seeing that rel-
atively unmotivated people can near-match the
best machine performance on this task in so lit-
tle time at a labor cost of approximately USS40.
We plan to take this work in a number of di-
rections. First, we will further explore whether
people can meet or beat the machine&apos;s accuracy
at this task. We have identified one major weak-
ness of human rule writers: capturing informa-
tion about low frequency events. It is possible
that by providing the person with sufficiently
powerful corpus analysis tools to aide in rule
writing, we could overcome this problem.
We ran all of our human experiments on a
fixed training corpus size. It would be interest-
ing to compare how human performance varies
as a function of training corpus size with how
machine performance varies.
There are many ways to combine human
corpus-based knowledge extraction with ma-
chine learning. One possibility would be to com-
bine the human and machine outputs. Another
would be to have the human start with the out-
put of the machine and then learn rules to cor-
rect the machine&apos;s mistakes. We could also have
a hybrid system where the person writes rules
with the help of machine learning. For instance,
the machine could propose a set of rules and
the person could choose the best one. We hope
that by further studying both human and ma-
chine knowledge acquisition from corpora, we
can devise learning strategies that successfully
combine the two approaches, and by doing so,
further improve our ability to extract useful lin-
guistic information from online resources.
</bodyText>
<sectionHeader confidence="0.955023" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996082">
The authors would like to thank Ryan Brown,
Mike Harmon, John Henderson and David
Yarowsky for their valuable feedback regarding
this work. This work was partly funded by NSF
grant IRI-9502312.
</bodyText>
<sectionHeader confidence="0.989767" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998237697674419">
S. Argamon, I. Dagan, and Y. Krymolowski.
1998. A memory-based approach to learning
shallow language patterns. In Proceedings of
the 17th International Conference on Compu-
tational Linguistics, pages 67-73. COLING-
ACL.
D. Bourigault. 1992. Surface grammatical anal-
ysis for the extraction of terminological noun
phrases. In Proceedings of the 30th Annual
Meeting of the Association of Computational
Linguistics, pages 977-981. Association of
Computational Linguistics.
E. Brill and P. Resnik. 1994. A rule-based
approach to prepositional phrase attachment
disambiguation. In Proceedings of the fif-
teenth International Conference on Compu-
tational Linguistics (COLING-1994).
E. Brill. 1995. Transformation-based error-
driven learning and natural language process-
ing: A case study in past of speech tagging.
Computational Linguistics, December.
C. Cardie and D. Pierce. 1998. Error-driven
pruning of treebank gramars for base noun
phrase identification. In Proceedings of the
36th Annual Meeting of the Association of.
Computational Linguistics, pages 218-224.
Association of Computational Linguistics.
K. Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text.
In Proceedings of the Second Conference on
Applied Natural Language Processing, pages
136-143. Association of Computational Lin-
guistics.
W. Daelemans, A. van den Bosch, and J. Zavrel.
1999. Forgetting exceptions is harmful in lan-
guage learning. In Machine Learning, spe-
cial issue on natural language learning, vol-
ume 11, pages 11-43. to appear.
D. Day, J. Aberdeen, L. Hirschman,
R. Kozierok, P. Robinson, and M. Vi-
lain. 1997. Mixed-initiative development
of language processing systems. In Fifth
Conference on Applied Natural Language
</reference>
<page confidence="0.984068">
70
</page>
<figure confidence="0.873493311111111">
View Gu Cometzanic4ete
Nrteacipct Bass NP hapet-rhur
riukctive rules and me:MP:wren Log
rEntire corpus roirChanged lines only Wrecision errors only kiRecall arson only
RdOrcP on refgc
Rules so far:
(Reload frame ON EVERY ITERATION to molten:tit that
contents are up to date)
Type in your rules in the box below.
Thanks for your partipation and gond luck!
# existential/pronoun rule
A
( . )
(ill t-(EX I PRP IWP MT))
)
* determiner+adjecti ve+noun
A
(* • )
(fll t-(DT)) (* t.(CDIJJ(IISRIVOG)) (+ t-NNS?)
(* )
* POS+adjectives+nouns
A
(*•)
t=POS) t-(LJER5V1VBN I VSG)) (4-
(* &apos;
----iflfwffNffIP4fff#141*Bsrr,,
t =NNW)
Mee
(1 ThebT third-largm til thriftNN inaUwIlooN])ieINUF1ortoNNJp
Ric0Nr4P] allwiusuidvaD ([it F D eMPeCtIV82 [REF return t4 hr])
°Ira ([prolitabilityNN I) lam ([thejyr thirdji quarter NN I) whenwRg
([it) reports ,(operatingvao resultsvu) (IthisDT week N N )
Sentence 499:
([PonceNHp FederalNHp] )saidvBD athe,7 dividend N r41) WU vim
sus pended&amp;quot; N int N (Ian ticipa H N] ) of, N (triore, [stringent„
capiudNN requirements NNs] ) under/N [ (tiseD7 FinancialNNp
wtiomNNP Ref°TnINNP] [RecmtTYNNP] andec
[EnforcementNNp ACtNNpl ( [1989 cD1 ) ,
Sentence 500 :
( [AL, labor manage cot gaup isvaz prepannsno ([aD 7
revisedveN Lny-outm form [ (Unitedwir Airlines NN?s
FdreniNN1 [UALN„ C.orp.„pl )([thatwurDwouldmm transferv3
[Illai°ritYNN ov(itert hiPi•ii41) KIT° (remPloressNsl)whileus
kgvinitvtio n&apos;ollierr NN ) ill IN ( [PU blic hlruhnNsi).,
accordirm,..„-jo7:-:
</figure>
<figureCaption confidence="0.999277">
Figure 4: Screenshot of base NP chunking system
</figureCaption>
<reference confidence="0.999369595238095">
Processing, pages 348-355. Association for
Computational Linguistics, March.
W. Gale, K. Church, and D. Yarowsky. 1992.
One sense per discourse. In Proceedings of
the 4th DARPA Speech and Natural Language
Workship, pages 233-237.
J. Juteson and S. Katz. 1995. Technical ter-
minology: Some linguistic properties and an
algorithm for identification in text. Natural
Language Engineering, 1:9-27.
L. Mangu and E. Brill. 1997. Automatic rule
acquisition for spelling correction. In Pro-
ceedings of the Fourteenth International Con-
ference on Machine Learning, Nashville, Ten-
nessee.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313-330.
L. Ramshaw and M. Marcus. 1994. Exploring
the statistical derivation of transformational
rule sequences for part-of-speech tagging. In
The Balancing Act: Proceedings of the ACL
Workshop on Combining Symbolic and Sta-
tistical Approaches to Language, New Mexico
State University, July.
L. Ramshaw and M. Marcus. In Press. Text
chunking using transformation-based learn-
ing. In Natural Language Processing Using
Very large Corpora. Kluwer.
K. Samuel, S. Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In Proceed-
ings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics, vol-
ume 2. Association of Computational Linguis-
tics.
A. van der Dosch and W. Daelemans. 1998.
Do not forget: Pull memory in memory-
based learning of word pronunciation. In New
Methods in Language Processing, pages 195-
204. Computational Natural Language Learn-
</reference>
<page confidence="0.98019">
71
</page>
<reference confidence="0.998158153846154">
ing.
J. Veenstra. 1998. Fast NP chunking
using memory-based learning techniques.
In BENELEARN-98: Proceedings of the
Eighth Belgian-Dutch Conference on Ma-
chine Learning, Wageningen, the Nether-
lands.
M. Vilain and D. Day. 1996. Finite-state
parsing by rule sequences. In International
Conference on Computational Linguistics,
Copenhagen, Denmark, August. The Interna-
tional Committee on Computational Linguis-
tics.
A Voutilainen. 1993. NPTool, a detector of
English noun phrases. In Proceedings of the
Workshop on Very Large Corpora, pages 48-
57. Association for Computational Linguis-
tics.
D. Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to ac-
cent restoration in Spanish and French. In
Proceedings of the 32nd Annual Meeting of
the Association for Computational Linguis-
tics, pages 88-95, Las Cruces, NM.
G. Zipf. 1935. The Psycho-Biology of Language.
Houghton Mifflin.
</reference>
<page confidence="0.998718">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000256">
<title confidence="0.993045">Man* vs. Machine: A Case Study in Base Noun Phrase Learning</title>
<author confidence="0.999398">Eric Brill</author>
<author confidence="0.999398">Grace Ngai</author>
<affiliation confidence="0.9998765">Department of Computer Science The Johns Hopkins University</affiliation>
<address confidence="0.99999">Baltimore, MD 21218, USA</address>
<email confidence="0.771739">gyn}@cs.jhu.edu</email>
<abstract confidence="0.993305401486989">A great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora. Very little work has gone into quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction. Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simrules, it raises the question of whether peo- *and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly and effectively relatively untrained people can extract linguistic generalities from a corpus as compared to a machine. There are a number of reasons for doing this. We would like to understand the relative strengths and weaknesses of humans versus machines in hopes of marrying their complementary strengths to create even more accurate systems. Also, since people can use their metaknowledge to generalize from a small number of examples, it is possible that a person could derive effective linguistic knowledge from a much smaller training corpus than that needed by a machine. A person could also potentially learn more powerful representations than a machine, thereby achieving higher accuracy. In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking. Much previous work has been done on this problem and many different methods have been used: Church&apos;s PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski(1998) and Daelemans, van den Bosch &amp; Zavrel (1999) use memory-based systems; Ramshaw &amp; Marcus (In Press) and Cardie &amp; Pierce (1998) use rule-based systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine system with which to compare the hu- 65 man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&amp;M system is the best of those that have been tested the Penn To train their system, R&amp;M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal (Marcus et al., 1993) tagged using a transformation-based tagger (Brill, 1995) and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics (like treating the possessive marker as the first word of a new base noun phrase) to flatten the recursive structure of the parse. They cast the problem as a transformation-based tagging problem, where each word is to be labelled with a chunk structure tag from the set II, 0, Bl, where words marked &amp;quot;I&amp;quot; are inside some base NP chunk, those marked &amp;quot;0&amp;quot; are not part of any base NP, those marked the first word of a base NP which immediately succeeds another base NP. The training corpus is first run through a part-of-speech tagger. Then, as a baseline annotation, each word is labelled with the most common chunk structure tag for its part-of-speech tag. After the baseline is achieved, transformation rules fitting a set of rule templates are then learned to improve the &amp;quot;tagging accuracy&amp;quot; of the training set. These templates take into consideration the word, part-of-speech tag and chunk structure tag of the current word and all words within a window of 3 to either side of it. Applying a rule to a word changes the chunk structure tag of a word and in effect alters the boundaries of the base NP chunks in the sentence. An example of a rule learned by the R&amp;M syschange a chunk structure tag of a word from I to B if the word is a determiner, the next word is a noun, and the two previous words both chunk structure tags of I. other words, a determiner in this context is likely to begin a noun phrase. The R&amp;M system learns a total would like to thank Lance Ramshaw for providing us with the base-NP-annotated training and test corpora that were used in the R&amp;M system, as well as the rules learned by this system. of 500 rules. 3 Manual Rule Acquisition R&amp;M framed the base NP annotation problem as a word tagging problem. We chose instead to use regular expressions on words and part of speech tags to characterize the NPs, as well as the context surrounding the NPs, because this is both a more powerful representational language and more intuitive to a person. A person can more easily consider potential phrases as a sequence of words and tags, rather than looking at each individual word and deciding whether it is part of a phrase or not. The rule actions we a base NP (bracket a sequence of words as a base NP) Kill Delete a base NP (remove a pair of parentheses) Transform Transform a base NP (move one or both parentheses to extend/contract a base NP) Merge Merge two base NPs As an example, we consider an actual rule from our experiments: Bracket all sequences of words of: one determiner (DT), zero or more adjec- JJR, and one or more nouns (NN, NNP, NNS, NNPS), if are followed by a verb VBD, VBN, VBP, our language, the rule is written A (* . ) ({1} t=DT) (* t=JJ [RS] ?) (+ t=NNP?S?) ({1} t=VB [DGNPZ] ?) The first line denotes the action, in this case, Add a bracketing. The second line defines the context preceding the sequence we want to have bracketed — in this case, we do not care what this sequence is. The third line defines the sequence which we want bracketed, and the last rule types we have chosen are similar to those used by Vilain and Day (1996) in transformation-based parsing, but are more powerful. full description of the rule language can be found 66 s{(([ \s_]+__DT\s+) ([\s_]+__JJ [RS] \s+)* ( [^ \s _]+__NNp?s?\s+)+) ([\s_] +VB [DGNPZ] \s+)} $1 ) $5 line defines the context following the bracketed sequence. Internally, the software then translates this rule into the more unwieldy Pen l regular expression: The base NP annotation system created by the humans is essentially a transformationbased system with hand-written rules. The user manually creates an ordered list of rules. A rule list can be edited by adding a rule at any position, deleting a rule, or modifying a rule. The user begins with an empty rule list. Rules are derived by studying the training corpus and NPs that the rules have not yet bracketed, as well as NPs that the rules have incorrectly bracketed. Whenever the rule list is edited, the efficacy of the changes can be checked by running the new rule list on the training set and seeing how the modified rule list compares to the unmodified list. Based on this feedback, the user decides whether to accept or reject the changes that were made. One nice property of transformation-based learning is that in appending a rule to the end of a rule list, the user need not be concerned about how that rule may interact with other rules on the list. This is much easier than writing a CFG, for instance, where rules interact in a way that may not be readily apparent to a human rule writer. To make it easy for people to study the training set, word sequences are presented in one of four colors indicating that they: 1. are not part of an NP either in the truth or in the output of the person&apos;s rule set 2. consist of an NP both in the truth and in the output of the person&apos;s rule set (i.e. they constitute a base NP that the person&apos;s rules correctly annotated) 3. consist of an NP in the truth but not in the output of the person&apos;s rule set (i.e. they constitute a recall error) 4. consist of an NP in the output of the person&apos;s rule set but not in the truth (i.e. they constitute a precision error) The actual system is located at . jhu.eduk-dbasenpichunking. A screenshot of this system is shown in figure 4. The correct base NPs are enclosed in parentheses and those annotated by the human&apos;s rules in brackets. 4 Experimental Set-Up and Results The experiment of writing rule lists for base NP annotation was assigned as a homework set to a group of 11 undergraduate and graduate students in an introductory natural language pro- The corpus that the students were given from which to derive and validate rules is a 25k word subset of the R&amp;M training set, approximately the size of the full R&amp;M training set. The reason we used a downsized training set was that we believed humans could generalize better from less data, and we thought that it might be possible to meet or surpass R&amp;M&apos;s results with a much smaller training set. Figure 1 shows the final precision, recall, Fmeasure and precision+recall numbers on the training and test corpora for the students. There was very little difference in performance on the training set compared to the test set. This indicates that people, unlike machines, seem immune to overtraining. The time the students spent on the problem ranged from less than 3 hours to almost 10 hours, with an average of about 6 hours. While it was certainly the case that the students with the worst results spent the least amount of time on the problem, it was not true that those with the best results spent the most time — indeed, the average amount of time spent by the top three students was a little less than the overall average — slightly over 5 hours. On average, people achieved 90% of their final performance after half of the total time they spent in rule writing. The number of rules in the final rule lists also varied, from as few as 16 rules to as many as 61 rules, with an average of 35.6 rules. Again, the average number for the top three subjects was a little under the average for everybody: 30.3 rules. 11 students were a subset of the entire class. Students were given an option of participating in this experiment or doing a much more challenging final project. Thus, as a population, they tended to be the less motivated students.</abstract>
<note confidence="0.9074335625">67 TRAINING SET (25K Words) TEST SET Precision Recall F-Measure 11 Precision Recall F-Measure -1-ff- Student 1 87.8% 88.6% 88.2 88.2 88.0% 88.8% 88.4 88.4 Student 2 88.1% 88.2% 88.2 88.2 88.2% 87.9% 88.0 88.1 Student 3 88.6% 87.6% 88.1 88.2 88.3% 87.8% 88.0 88.1 Student 4 88.0% 87.2% 87.6 87.6 86.9% 85.9% 86.4 86.4 Student 5 86.2% 86.8% 86.5 86.5 85.8% 85.8% 85.8 85.8 Student 6 86.0% 87.1% 86.6 86.6 85.8% 87.1% 86.4 86.5 Student 7 84.9% 86.7% 85.8 85.8 85.3% 87.3% 86.3 86.3 Student 8 83.6% 86.0% 84.8 84.8 83.1% 85.7% 84.4 84.4 Student 9 83.9% 85.0% 84.4 84.5 83.5% 84.8% 84.1 84.2 Student 10 82.8% 84.5% 83.6 83.7 83.3% 84.4% 83.8 83.8 Student 11 84.8% 78.8% 81.7 81.8 84.0% 77.4% 80.6 80.7 Figure 1: P/R results of test subjects on training and test corpora</note>
<abstract confidence="0.99897465408805">In the beginning, we believed that the students would be able to match or better the R&amp;M system&apos;s results, which are shown in figure 2. It can be seen that when the same training corpus is used, the best students do achieve performances which are close to the R&amp;M system&apos;s on average, the top 3 students&apos; performances come within 0.5% precision and 1.1% recall of the machine&apos;s. In the following section, we will examine the output of both the manual and automatic systems for differences. 5 Analysis Before we started the analysis of the test set, we hypothesized that the manually derived systems would have more difficulty with potential rules that are effective, but fix only a very small number of mistakes in the training set. The distribution of noun phrase types, identified by their part of speech sequence, roughly obeys Zipf&apos;s Law (Zipf, 1935): there is a large tail of noun phrase types that occur very infrequently in the corpus. Assuming there is not a rule that can generalize across a large number of these low-frequency noun phrases, the only way noun phrases in the tail of the distribution can be learned is by learning low-count rules: in other words, rules that will only positively affect a small number of instances in the training corpus. Van der Dosch and Daelemans (1998) show that not ignoring the low count instances is often crucial to performance in machine learning systems for natural language. Do the humanwritten rules suffer from failing to learn these infrequent phrases? To explore the hypothesis that a primary difference between the accuracy of human and machine is the machine&apos;s ability to capture the low frequency noun phrases, we observed how the accuracy of noun phrase annotation of both human and machine derived rules is affected by the frequency of occurrence of the noun phrases in the training corpus. We reduced each base NP in the test set to its POS tag sequence as assigned by the POS tagger. For each POS tag sequence, we then counted the number of times it appeared in the training set and the recall achieved on the test set. The plot of the test set recall vs. the number of appearances in the training set of each tag sequence for the machine and the mean of the top 3 students is shown in figure 3. For instance, for base NPs in the test set with tag sequences that appeared 5 times in the training corpus, the students achieved an average recall of 63.6% while the machine achieved a recall of 83.5%. For base NPs with tag sequences that appear less than 6 times in the training set, the machine outperforms the students by a recall of 62.8% vs. 54.8%. However, for the rest of the base NPs those that appear 6 or more times the performances of the machine and students are almost identical: 93.7% for the machine vs. 93.5% for the 3 students, a difference that is not statistically significant. The recall graph clearly shows that for the top 3 students, performance is comparable to the machine&apos;s on all but the low frequency constituents. This can be explained by the human&apos;s 68 Training set size(words) Precision Recall F-Measure EP 25k 88.7% 89.3% 89.0 89.0 200k 91.8% 92.3% 92.0 92.1 Figure 2: P/R results of the R&amp;M system on test corpus - &amp;quot;Madline - .L Number of Appearances In Training Set Figure 3: Test Set Recall vs. Frequency of Appearances in Training Set. reluctance or inability to write a rule that will only capture a small number of new base NPs in the training set. Whereas a machine can easily learn a few hundred rules, each of which makes a very small improvement to accuracy, this is a tedious task for a person, and a task which apparently none of our human subjects was willing or able to take on. There is one anomalous point in figure 3. For base NPs with POS tag sequences that appear 3 times in the training set, there is a large decrease in recall for the machine, but a large increase in recall for the students. When we looked at the POS tag sequences in question and their corresponding base NPs, we found that this was caused by one single POS tag sequence — that of two successive numbers (CD). The test set happened to include many sentences containing sequences of the type: . . ( CD ) TO ( CD CD ) as in: ( International/NNP Paper/NNP ) fell/VBD ( 1/CD 1/CD ) to/TO ( 51/CD 1/CD )... while the training set had none. The machine ended up bracketing the entire sequence 1/CD 1/CD to/TO 51/CD 1/CD as a base NP. None of the students, however, made this mistake. 69 6 Conclusions and Future Work In this paper we have described research we undertook in an attempt to ascertain how people can perform compared to a machine at learning linguistic information from an annotated corpus, and more importantly to begin to explore the differences in learning behavior between human and machine. Although people did not match the performance of the machine-learned annotator, it is interesting that these &amp;quot;language novices&amp;quot;, with almost no training, were able to come fairly close, learning a small number of powerful rules in a short amount of time on a small training set. This challenges the claim that machine learning offers portability advantages over manual rule writing, seeing that relatively unmotivated people can near-match the best machine performance on this task in so little time at a labor cost of approximately USS40. We plan to take this work in a number of directions. First, we will further explore whether people can meet or beat the machine&apos;s accuracy at this task. We have identified one major weakness of human rule writers: capturing information about low frequency events. It is possible that by providing the person with sufficiently powerful corpus analysis tools to aide in rule writing, we could overcome this problem. We ran all of our human experiments on a fixed training corpus size. It would be interesting to compare how human performance varies as a function of training corpus size with how machine performance varies. There are many ways to combine human corpus-based knowledge extraction with machine learning. One possibility would be to combine the human and machine outputs. Another would be to have the human start with the output of the machine and then learn rules to correct the machine&apos;s mistakes. We could also have a hybrid system where the person writes rules with the help of machine learning. For instance, the machine could propose a set of rules and the person could choose the best one. We hope that by further studying both human and machine knowledge acquisition from corpora, we can devise learning strategies that successfully combine the two approaches, and by doing so, further improve our ability to extract useful linguistic information from online resources.</abstract>
<note confidence="0.88804905">Acknowledgements The authors would like to thank Ryan Brown, Mike Harmon, John Henderson and David Yarowsky for their valuable feedback regarding this work. This work was partly funded by NSF grant IRI-9502312. References S. Argamon, I. Dagan, and Y. Krymolowski. 1998. A memory-based approach to learning language patterns. In of the 17th International Conference on Compu- Linguistics, 67-73. COLING- ACL. D. Bourigault. 1992. Surface grammatical analysis for the extraction of terminological noun In of the 30th Annual Meeting of the Association of Computational 977-981. Association of Computational Linguistics. E. Brill and P. Resnik. 1994. A rule-based</note>
<abstract confidence="0.93695128">approach to prepositional phrase attachment In of the fifteenth International Conference on Computational Linguistics (COLING-1994). E. Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in past of speech tagging. Linguistics, C. Cardie and D. Pierce. 1998. Error-driven pruning of treebank gramars for base noun identification. In of the Annual Meeting of the Association Linguistics, 218-224. Association of Computational Linguistics. K. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. of the Second Conference on Natural Language Processing, 136-143. Association of Computational Linguistics. W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in lanlearning. In Learning, speissue on natural language learning, volume 11, pages 11-43. to appear. D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, P. Robinson, and M. Vilain. 1997. Mixed-initiative development language processing systems. In Conference on Applied Natural Language 70 Gu Bass NP riukctive rules and me:MP:wren Log rEntire corpus roirChanged lines only Wrecision errors only kiRecall arson only RdOrcP on refgc Rules so far: (Reload frame ON EVERY ITERATION to molten:tit that contents are up to date) Type in your rules in the box below. Thanks for your partipation and gond luck! A ( . ) (ill t-(EX I PRP IWP MT)) ) * determiner+adjecti ve+noun A (* • ) (fll t-(DT)) (* t.(CDIJJ(IISRIVOG)) (+ t-NNS?) (* ) * POS+adjectives+nouns A (*•) t-(LJER5V1VBN I VSG)) (* &apos; t =NNW) Mee third-largm ([it D return t4 I) quarter whenwRg reports week N) Sentence 499: dividend NWU vim pended&amp;quot; N(Ian ticipa ) of, N(triore, [stringent„ requirements ) [ Ref°TnINNP] [RecmtTYNNP] ) , Sentence 500 : manage cot prepannsno ([aD 7 [ Airlines NN?s C.orp.„pl hiPi•ii41) KIT° (remPloressNsl)whileus n&apos;ollierr ) illIN ( blic hlruhnNsi)., Figure 4: Screenshot of base NP chunking system 348-355. Association for Computational Linguistics, March. W. Gale, K. Church, and D. Yarowsky. 1992. sense per discourse. In of the 4th DARPA Speech and Natural Language 233-237. J. Juteson and S. Katz. 1995. Technical terminology: Some linguistic properties and an for identification in text. Engineering, L. Mangu and E. Brill. 1997. Automatic rule for spelling correction. In Proceedings of the Fourteenth International Conon Machine Learning, Tennessee. M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of The Penn Treebank. L. Ramshaw and M. Marcus. 1994. Exploring the statistical derivation of transformational sequences for part-of-speech tagging. The Balancing Act: Proceedings of the ACL Workshop on Combining Symbolic and Sta- Approaches to Language, Mexico State University, July. L. Ramshaw and M. Marcus. In Press. Text chunking using transformation-based learn- In Language Processing Using large Corpora. K. Samuel, S. Carberry, and K. Vijay- Shanker. 1998. Dialogue act tagging with learning. In Proceedings of the 36th Annual Meeting of the Asfor Computational Linguistics, volume 2. Association of Computational Linguistics. A. van der Dosch and W. Daelemans. 1998. Do not forget: Pull memory in memorylearning of word pronunciation. In in Language Processing, 195- Computational Natural Language Learn- 71 ing. J. Veenstra. 1998. Fast NP chunking using memory-based learning techniques. Proceedings of the Eighth Belgian-Dutch Conference on Ma- Learning, the Netherlands. M. Vilain and D. Day. 1996. Finite-state by rule sequences. In</abstract>
<affiliation confidence="0.903749">Conference on Computational Linguistics,</affiliation>
<address confidence="0.845557">Copenhagen, Denmark, August. The Interna-</address>
<note confidence="0.6858546875">tional Committee on Computational Linguistics. Voutilainen. 1993. detector of noun phrases. In of the on Very Large Corpora, 48- 57. Association for Computational Linguistics. D. Yarowsky. 1994. Decision lists for lexiambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguis- Las Cruces, NM. Zipf. 1935. of Language. Houghton Mifflin. 72</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow language patterns.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>67--73</pages>
<publisher>COLINGACL.</publisher>
<contexts>
<context position="1436" citStr="Argamon et al., 1998" startWordPosition="218" endWordPosition="221">nnotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly and effectively relatively untrained people can extract linguistic generalities from a corpus as compared to a machine. There are a number of reasons for doing this. We would like to unders</context>
</contexts>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1998. A memory-based approach to learning shallow language patterns. In Proceedings of the 17th International Conference on Computational Linguistics, pages 67-73. COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Surface grammatical analysis for the extraction of terminological noun phrases.</title>
<date>1992</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>977--981</pages>
<contexts>
<context position="2871" citStr="Bourigault (1992)" startWordPosition="460" endWordPosition="461"> from a small number of examples, it is possible that a person could derive effective linguistic knowledge from a much smaller training corpus than that needed by a machine. A person could also potentially learn more powerful representations than a machine, thereby achieving higher accuracy. In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking. Much previous work has been done on this problem and many different methods have been used: Church&apos;s PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski(1998) and Daelemans, van den Bosch &amp; Zavrel (1999) use memory-based systems; Ramshaw &amp; Marcus (In Press) and Cardie &amp; Pierce (1998) use rule-based systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine learning system with which to compare the hu65 man learners. It is difficult to compare different machine learning </context>
</contexts>
<marker>Bourigault, 1992</marker>
<rawString>D. Bourigault. 1992. Surface grammatical analysis for the extraction of terminological noun phrases. In Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics, pages 977-981. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the fifteenth International Conference on Computational Linguistics (COLING-1994).</booktitle>
<contexts>
<context position="1306" citStr="Brill and Resnik, 1994" startWordPosition="198" endWordPosition="201">rated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly and effectively relatively untrained people can extract lin</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>E. Brill and P. Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. In Proceedings of the fifteenth International Conference on Computational Linguistics (COLING-1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based errordriven learning and natural language processing: A case study in past of speech tagging.</title>
<date>1995</date>
<booktitle>Computational Linguistics,</booktitle>
<contexts>
<context position="1009" citStr="Brill, 1995" startWordPosition="157" endWordPosition="158">nto quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction. 1 Introduction Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based system</context>
<context position="3837" citStr="Brill, 1995" startWordPosition="621" endWordPosition="622">ed systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine learning system with which to compare the hu65 man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&amp;M system is the best of those that have been tested on the Penn Treebank.1 To train their system, R&amp;M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal (Marcus et al., 1993) tagged using a transformation-based tagger (Brill, 1995) and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics (like treating the possessive marker as the first word of a new base noun phrase) to flatten the recursive structure of the parse. They cast the problem as a transformation-based tagging problem, where each word is to be labelled with a chunk structure tag from the set II, 0, Bl, where words marked &amp;quot;I&amp;quot; are inside some base NP chunk, those marked &amp;quot;0&amp;quot; are not part of any base NP, and those marked &amp;quot;B&amp;quot; denote the first word of a base </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based errordriven learning and natural language processing: A case study in past of speech tagging. Computational Linguistics, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>D Pierce</author>
</authors>
<title>Error-driven pruning of treebank gramars for base noun phrase identification.</title>
<date>1998</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association of. Computational Linguistics,</booktitle>
<pages>218--224</pages>
<contexts>
<context position="1397" citStr="Cardie and Pierce, 1998" startWordPosition="212" endWordPosition="215">ng linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly and effectively relatively untrained people can extract linguistic generalities from a corpus as compared to a machine. There are a number of reasons </context>
<context position="3212" citStr="Cardie &amp; Pierce (1998)" startWordPosition="510" endWordPosition="513">rformed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking. Much previous work has been done on this problem and many different methods have been used: Church&apos;s PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski(1998) and Daelemans, van den Bosch &amp; Zavrel (1999) use memory-based systems; Ramshaw &amp; Marcus (In Press) and Cardie &amp; Pierce (1998) use rule-based systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine learning system with which to compare the hu65 man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&amp;M system is the best of those that have been tested on the Penn Treebank.1 To train their system, R&amp;M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal (Marcus et al., 1993) tagged using a transformation-b</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>C. Cardie and D. Pierce. 1998. Error-driven pruning of treebank gramars for base noun phrase identification. In Proceedings of the 36th Annual Meeting of the Association of. Computational Linguistics, pages 218-224. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<marker>Church, 1988</marker>
<rawString>K. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>In Machine Learning, special issue on natural language learning,</booktitle>
<volume>11</volume>
<pages>11--43</pages>
<note>to appear.</note>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. In Machine Learning, special issue on natural language learning, volume 11, pages 11-43. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Day</author>
<author>J Aberdeen</author>
<author>L Hirschman</author>
<author>R Kozierok</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mixed-initiative development of language processing systems.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>348--355</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="1170" citStr="Day et al., 1997" startWordPosition="180" endWordPosition="183"> learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule </context>
</contexts>
<marker>Day, Aberdeen, Hirschman, Kozierok, Robinson, Vilain, 1997</marker>
<rawString>D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, P. Robinson, and M. Vilain. 1997. Mixed-initiative development of language processing systems. In Fifth Conference on Applied Natural Language Processing, pages 348-355. Association for Computational Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th DARPA Speech and Natural Language Workship,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="1128" citStr="Gale et al., 1992" startWordPosition="173" endWordPosition="176">p in that direction. 1 Introduction Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. One sense per discourse. In Proceedings of the 4th DARPA Speech and Natural Language Workship, pages 233-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Juteson</author>
<author>S Katz</author>
</authors>
<title>Technical terminology: Some linguistic properties and an algorithm for identification in text. Natural Language Engineering,</title>
<date>1995</date>
<contexts>
<context position="3012" citStr="Juteson and Katz (1995)" startWordPosition="479" endWordPosition="482"> corpus than that needed by a machine. A person could also potentially learn more powerful representations than a machine, thereby achieving higher accuracy. In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking. Much previous work has been done on this problem and many different methods have been used: Church&apos;s PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski(1998) and Daelemans, van den Bosch &amp; Zavrel (1999) use memory-based systems; Ramshaw &amp; Marcus (In Press) and Cardie &amp; Pierce (1998) use rule-based systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine learning system with which to compare the hu65 man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&amp;M system is the best of th</context>
</contexts>
<marker>Juteson, Katz, 1995</marker>
<rawString>J. Juteson and S. Katz. 1995. Technical terminology: Some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1:9-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning,</booktitle>
<location>Nashville, Tennessee.</location>
<contexts>
<context position="1081" citStr="Mangu and Brill, 1997" startWordPosition="166" endWordPosition="169">n a person and a machine. This paper is a first step in that direction. 1 Introduction Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, </context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>L. Mangu and E. Brill. 1997. Automatic rule acquisition for spelling correction. In Proceedings of the Fourteenth International Conference on Machine Learning, Nashville, Tennessee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="3780" citStr="Marcus et al., 1993" startWordPosition="612" endWordPosition="615">mshaw &amp; Marcus (In Press) and Cardie &amp; Pierce (1998) use rule-based systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine learning system with which to compare the hu65 man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&amp;M system is the best of those that have been tested on the Penn Treebank.1 To train their system, R&amp;M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal (Marcus et al., 1993) tagged using a transformation-based tagger (Brill, 1995) and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics (like treating the possessive marker as the first word of a new base noun phrase) to flatten the recursive structure of the parse. They cast the problem as a transformation-based tagging problem, where each word is to be labelled with a chunk structure tag from the set II, 0, Bl, where words marked &amp;quot;I&amp;quot; are inside some base NP chunk, those marked &amp;quot;0&amp;quot; are not part of any base </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging.</title>
<date>1994</date>
<booktitle>In The Balancing Act: Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language,</booktitle>
<institution>Mexico State University,</institution>
<location>New</location>
<contexts>
<context position="1036" citStr="Ramshaw and Marcus, 1994" startWordPosition="159" endWordPosition="162">ng the difference in ability at this task between a person and a machine. This paper is a first step in that direction. 1 Introduction Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance </context>
</contexts>
<marker>Ramshaw, Marcus, 1994</marker>
<rawString>L. Ramshaw and M. Marcus. 1994. Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging. In The Balancing Act: Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language, New Mexico State University, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>In Press. Text chunking using transformation-based learning.</title>
<booktitle>In Natural Language Processing Using Very large Corpora.</booktitle>
<publisher>Kluwer.</publisher>
<marker>Ramshaw, Marcus, </marker>
<rawString>L. Ramshaw and M. Marcus. In Press. Text chunking using transformation-based learning. In Natural Language Processing Using Very large Corpora. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Samuel</author>
<author>S Carberry</author>
<author>K VijayShanker</author>
</authors>
<title>Dialogue act tagging with transformation-based learning.</title>
<date>1998</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<contexts>
<context position="1211" citStr="Samuel et al., 1998" startWordPosition="187" endWordPosition="190">solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule list manually from an annotated corpus. I</context>
</contexts>
<marker>Samuel, Carberry, VijayShanker, 1998</marker>
<rawString>K. Samuel, S. Carberry, and K. VijayShanker. 1998. Dialogue act tagging with transformation-based learning. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, volume 2. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A van der Dosch</author>
<author>W Daelemans</author>
</authors>
<title>Do not forget: Pull memory in memorybased learning of word pronunciation.</title>
<date>1998</date>
<journal>Computational Natural Language Learning.</journal>
<booktitle>In New Methods in Language Processing,</booktitle>
<pages>195--204</pages>
<marker>van der Dosch, Daelemans, 1998</marker>
<rawString>A. van der Dosch and W. Daelemans. 1998. Do not forget: Pull memory in memorybased learning of word pronunciation. In New Methods in Language Processing, pages 195-204. Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
</authors>
<title>Fast NP chunking using memory-based learning techniques.</title>
<date>1998</date>
<booktitle>In BENELEARN-98: Proceedings of the Eighth Belgian-Dutch Conference on Machine Learning,</booktitle>
<location>Wageningen, the Netherlands.</location>
<contexts>
<context position="1413" citStr="Veenstra, 1998" startWordPosition="216" endWordPosition="217"> from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly and effectively relatively untrained people can extract linguistic generalities from a corpus as compared to a machine. There are a number of reasons for doing this. </context>
<context position="3050" citStr="Veenstra (1998)" startWordPosition="486" endWordPosition="487">n could also potentially learn more powerful representations than a machine, thereby achieving higher accuracy. In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking. Much previous work has been done on this problem and many different methods have been used: Church&apos;s PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski(1998) and Daelemans, van den Bosch &amp; Zavrel (1999) use memory-based systems; Ramshaw &amp; Marcus (In Press) and Cardie &amp; Pierce (1998) use rule-based systems. 2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&amp;M) as the machine learning system with which to compare the hu65 man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&amp;M system is the best of those that have been tested on the Penn </context>
</contexts>
<marker>Veenstra, 1998</marker>
<rawString>J. Veenstra. 1998. Fast NP chunking using memory-based learning techniques. In BENELEARN-98: Proceedings of the Eighth Belgian-Dutch Conference on Machine Learning, Wageningen, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>D Day</author>
</authors>
<title>Finite-state parsing by rule sequences.</title>
<date>1996</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="7193" citStr="Vilain and Day (1996)" startWordPosition="1243" endWordPosition="1246">zero or more adjectives (JJ, JJR, JJS), and one or more nouns (NN, NNP, NNS, NNPS), if they are followed by a verb (VB, VBD, VBG, VBN, VBP, VBZ). In our language, the rule is written thus:3 A (* . ) ({1} t=DT) (* t=JJ [RS] ?) (+ t=NNP?S?) ({1} t=VB [DGNPZ] ?) The first line denotes the action, in this case, Add a bracketing. The second line defines the context preceding the sequence we want to have bracketed — in this case, we do not care what this sequence is. The third line defines the sequence which we want bracketed, and the last 2The rule types we have chosen are similar to those used by Vilain and Day (1996) in transformation-based parsing, but are more powerful. 3A full description of the rule language can be found at http://n1p.cs.jhu.eduk,,baseNP/manual. 66 s{(([ \s_]+__DT\s+) ([\s_]+__JJ [RS] \s+)* ( [^ \s _]+__NNp?s?\s+)+) ([\s_] +VB [DGNPZ] \s+)} ( $1 ) $5 1g line defines the context following the bracketed sequence. Internally, the software then translates this rule into the more unwieldy Pen l regular expression: The base NP annotation system created by the humans is essentially a transformationbased system with hand-written rules. The user manually creates an ordered list of rules. A rul</context>
</contexts>
<marker>Vilain, Day, 1996</marker>
<rawString>M. Vilain and D. Day. 1996. Finite-state parsing by rule sequences. In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
</authors>
<title>NPTool, a detector of English noun phrases.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Voutilainen, 1993</marker>
<rawString>A Voutilainen. 1993. NPTool, a detector of English noun phrases. In Proceedings of the Workshop on Very Large Corpora, pages 48-57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="1248" citStr="Yarowsky, 1994" startWordPosition="193" endWordPosition="194">ral language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo*and Woman. ple could also derive an effective rule list manually from an annotated corpus. In this paper we explore how quickly a</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 88-95, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zipf</author>
</authors>
<title>The Psycho-Biology of Language.</title>
<date>1935</date>
<publisher>Houghton Mifflin.</publisher>
<contexts>
<context position="13170" citStr="Zipf, 1935" startWordPosition="2320" endWordPosition="2321">e close to the R&amp;M system&apos;s - on average, the top 3 students&apos; performances come within 0.5% precision and 1.1% recall of the machine&apos;s. In the following section, we will examine the output of both the manual and automatic systems for differences. 5 Analysis Before we started the analysis of the test set, we hypothesized that the manually derived systems would have more difficulty with potential rules that are effective, but fix only a very small number of mistakes in the training set. The distribution of noun phrase types, identified by their part of speech sequence, roughly obeys Zipf&apos;s Law (Zipf, 1935): there is a large tail of noun phrase types that occur very infrequently in the corpus. Assuming there is not a rule that can generalize across a large number of these low-frequency noun phrases, the only way noun phrases in the tail of the distribution can be learned is by learning low-count rules: in other words, rules that will only positively affect a small number of instances in the training corpus. Van der Dosch and Daelemans (1998) show that not ignoring the low count instances is often crucial to performance in machine learning systems for natural language. Do the humanwritten rules s</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>G. Zipf. 1935. The Psycho-Biology of Language. Houghton Mifflin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>