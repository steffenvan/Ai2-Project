<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.997266">
Joint Syntactic and Semantic Parsing with
Combinatory Categorial Grammar
</title>
<author confidence="0.992541">
Jayant Krishnamurthy
</author>
<affiliation confidence="0.991004">
Carnegie Mellon University
</affiliation>
<address confidence="0.8863515">
5000 Forbes Avenue
Pittsburgh, PA 15213
</address>
<email confidence="0.999586">
jayantk@cs.cmu.edu
</email>
<sectionHeader confidence="0.994817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989428571429">
We present an approach to training a joint
syntactic and semantic parser that com-
bines syntactic training information from
CCGbank with semantic training informa-
tion from a knowledge base via distant su-
pervision. The trained parser produces a
full syntactic parse of any sentence, while
simultaneously producing logical forms
for portions of the sentence that have a se-
mantic representation within the parser’s
predicate vocabulary. We demonstrate our
approach by training a parser whose se-
mantic representation contains 130 pred-
icates from the NELL ontology. A seman-
tic evaluation demonstrates that this parser
produces logical forms better than both
comparable prior work and a pipelined
syntax-then-semantics approach. A syn-
tactic evaluation on CCGbank demon-
strates that the parser’s dependency F-
score is within 2.5% of state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964529411765">
Integrating syntactic parsing with semantics has
long been a goal of natural language processing
and is expected to improve both syntactic and se-
mantic processing. For example, semantics could
help predict the differing prepositional phrase at-
tachments in “I caught the butterfly with the net”
and “I caught the butterfly with the spots.” A joint
analysis could also avoid propagating syntactic
parsing errors into semantic processing, thereby
improving performance.
We suggest that a large populated knowledge
base should play a key role in syntactic and se-
mantic parsing: in training the parser, in resolv-
ing syntactic ambiguities when the trained parser
is applied to new text, and in its output semantic
representation. Using semantic information from
the knowledge base at training and test time will
</bodyText>
<note confidence="0.85426925">
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213
</note>
<email confidence="0.985544">
tom.mitchell@cmu.edu
</email>
<bodyText confidence="0.999878951219512">
ideally improve the parser’s ability to solve diffi-
cult syntactic parsing problems, as in the exam-
ples above. A semantic representation tied to a
knowledge base allows for powerful inference op-
erations – such as identifying the possible entity
referents of a noun phrase – that cannot be per-
formed with shallower representations (e.g., frame
semantics (Baker et al., 1998) or a direct conver-
sion of syntax to logic (Bos, 2005)).
This paper presents an approach to training a
joint syntactic and semantic parser using a large
background knowledge base. Our parser produces
a full syntactic parse of every sentence, and fur-
thermore produces logical forms for portions of
the sentence that have a semantic representation
within the parser’s predicate vocabulary. For ex-
ample, given a phrase like “my favorite town in
California,” our parser will assign a logical form
like Ax.CITY(x) ∧ LOCATEDIN(x, CALIFORNIA)
to the “town in California” portion. Additionally,
the parser uses predicate and entity type informa-
tion during parsing to select a syntactic parse.
Our parser is trained by combining a syntactic
parsing task with a distantly-supervised relation
extraction task. Syntactic information is provided
by CCGbank, a conversion of the Penn Treebank
into the CCG formalism (Hockenmaier and Steed-
man, 2002a). Semantics are learned by training
the parser to extract knowledge base relation in-
stances from a corpus of unlabeled sentences, in
a distantly-supervised training regime. This ap-
proach uses the knowledge base to avoid expen-
sive manual labeling of individual sentence se-
mantics. By optimizing the parser to perform both
tasks simultaneously, we train a parser that pro-
duces accurate syntactic and semantic analyses.
We demonstrate our approach by training a joint
syntactic and semantic parser, which we call ASP.
ASP produces a full syntactic analysis of every
sentence while simultaneously producing logical
forms containing any of 61 category and 69 re-
</bodyText>
<page confidence="0.950252">
1188
</page>
<note confidence="0.8296685">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1188–1198,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999478">
lation predicates from NELL. Experiments with
ASP demonstrate that jointly analyzing syntax
and semantics improves semantic parsing perfor-
mance over comparable prior work and a pipelined
syntax-then-semantics approach. ASP’s syntactic
parsing performance is within 2.5% of state-of-
the-art; however, we also find that incorporating
semantic information reduces syntactic parsing ac-
curacy by ∼ 0.5%.
</bodyText>
<sectionHeader confidence="0.996378" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.999964111111111">
This paper combines two lines of prior work:
broad coverage syntactic parsing with CCG and
semantic parsing.
Broad coverage syntactic parsing with CCG has
produced both resources and successful parsers.
These parsers are trained and evaluated using
CCGbank (Hockenmaier and Steedman, 2002a),
an automatic conversion of the Penn Treebank
into the CCG formalism. Several broad cover-
age parsers have been trained using this resource
(Hockenmaier and Steedman, 2002b; Hocken-
maier, 2003b). The parsing model in this paper is
loosely based on C&amp;C (Clark and Curran, 2007b;
Clark and Curran, 2007a), a discriminative log-
linear model for statistical parsing. Some work
has also attempted to automatically derive logi-
cal meaning representations directly from syntac-
tic CCG parses (Bos, 2005; Lewis and Steedman,
2013). However, these approaches to semantics do
not ground the text to beliefs in a knowledge base.
Meanwhile, work on semantic parsing has fo-
cused on producing semantic parsers for answer-
ing simple natural language questions (Zelle and
Mooney, 1996; Ge and Mooney, 2005; Wong and
Mooney, 2006; Wong and Mooney, 2007; Lu et
al., 2008; Kate and Mooney, 2006; Zettlemoyer
and Collins, 2005; Kwiatkowski et al., 2011). This
line of work has typically used a corpus of sen-
tences with annotated logical forms to train the
parser. Recent work has relaxed the requisite su-
pervision conditions (Clarke et al., 2010; Liang et
al., 2011), but has still focused on simple ques-
tions. Finally, some work has looked at applying
semantic parsing to answer queries against large
knowledge bases, such as YAGO (Yahya et al.,
2012) and Freebase (Cai and Yates, 2013b; Cai
and Yates, 2013a; Kwiatkowski et al., 2013; Be-
rant et al., 2013). Although this work considers
a larger number (thousands) of predicates than we
do, none of these systems are capable of parsing
open-domain text. Our approach is most closely
related to the distantly-supervised approach of Kr-
ishnamurthy and Mitchell (2012).
The parser presented in this paper can be viewed
as a combination of both a broad coverage syn-
tactic parser and a semantic parser trained using
distant supervision. Combining these two lines
of work has synergistic effects – for example, our
parser is capable of semantically analyzing con-
junctions and relative clauses based on the syn-
tactic annotation of these categories in CCGbank.
This synergy gives our parser a richer semantic
representation than previous work, while simulta-
neously enabling broad coverage.
</bodyText>
<sectionHeader confidence="0.991875" genericHeader="method">
3 Parser Design
</sectionHeader>
<bodyText confidence="0.999711166666667">
This section describes the Combinatory Categorial
Grammar (CCG) parsing model used by ASP. The
input to the parser is a part-of-speech tagged sen-
tence, and the output is a syntactic CCG parse tree,
along with zero or more logical forms representing
the semantics of subspans of the sentence. These
logical forms are constructed using category and
relation predicates from a broad coverage knowl-
edge base. The parser also outputs a collection of
dependency structures summarizing the sentence’s
predicate-argument structure. Figure 1 illustrates
ASP’s input/output specification.
</bodyText>
<subsectionHeader confidence="0.9987">
3.1 Knowledge Base
</subsectionHeader>
<bodyText confidence="0.999860928571428">
The parser uses category and relation predicates
from a broad coverage knowledge base both to
construct logical forms and to parametrize the
parsing model. The knowledge base is assumed
to have two kinds of ontological structure: a gen-
eralization/subsumption hierarchy and argument
type constraints. This paper uses NELL’s ontology
(Carlson et al., 2010), which, for example, speci-
fies that the category ORGANIZATION is a general-
ization of SPORTSTEAM, and that both arguments
to the LOCATEDIN relation must have type LOCA-
TION. These type constraints are enforced during
parsing. Throughout this paper, predicate names
are shown in SMALLCAPS.
</bodyText>
<subsectionHeader confidence="0.996907">
3.2 Syntax
</subsectionHeader>
<bodyText confidence="0.9043112">
ASP uses a lexicalized and semantically-
typed Combinatory Categorial Grammar
(CCG) (Steedman, 1996). Most gram-
matical information in CCG is encoded in
a lexicon A, containing entries such as:
</bodyText>
<page confidence="0.857832">
1189
</page>
<equation confidence="0.952951866666667">
area / NN that / WDT includes / VBZ beautiful / JJ London / NNP
N
λx.LOCATION(x)
(N1\N1)/(S[dcl]\NP1)2
λf.λg.λz.g(z) ∧ f(λy.y = z)
(S[dcl]\NP1)/NP2
λf.λg.∃x, y.g(x) ∧ f(y)
∧ LOCATEDIN(y, x)
N1/N1 N
λf.f λx.M(x, “london”, CITY)
N : λx.M(x, “london”, CITY)
(S[dcl]\NP1) :
λg.∃x, y.g(x) ∧ M(y, “london”, CITY) ∧ LOCATEDIN(y, x)
N1\N1 : λg.λz.∃x, y.g(z) ∧ x = z ∧ M(y, “london”, CITY) ∧ LOCATEDIN(y, x)
N : λz.∃x, y.LOCATION(z) ∧ x = z ∧ M(y, “london”, CITY) ∧ LOCATEDIN(y, x)
</equation>
<table confidence="0.997007857142857">
word Head index syntactic category arg. num. Argument index
POS semantic type word POS semantic type
that WDT — 1 (N1\N1)/(S\NP1)2 1 area NN LOCATION 0
that WDT — 1 (N1\N1)/(S\NP1)2 2 includes VBZ LOCATEDIN−1 2
includes VBZ LOCATEDIN−1 2 (S[dcl]\NP1)/NP2 1 area NN LOCATION 0
includes VBZ LOCATEDIN−1 2 (S[dcl]\NP1)/NP2 2 ENTITY:CITY NNP CITY 4
beautiful JJ — 3 N1/N1 1 ENTITY:CITY NNP CITY 4
</table>
<figureCaption confidence="0.998368">
Figure 1: Example input and output for ASP. Given a POS-tagged sentence, the parser produces a CCG
syntactic tree and logical form (top), and a collection of dependency structures (bottom).
</figureCaption>
<equation confidence="0.9994892">
person := N : PERSON: λx.PERSON(x)
London := N : CITY: λx.M(x, “london”, CITY)
great := N1/N1 : — : λf.λx.f(x)
bought := (S[dcl]\NP1)/NP2 : ACQUIRED :
λf.λg.∃x, y.f(y) ∧ g(x) ∧ ACQUIRED(x, y)
</equation>
<bodyText confidence="0.999302404761905">
Each lexicon entry maps a word to a syntactic
category, semantic type, and logical form. CCG
has two kinds of syntactic categories: atomic and
functional. Atomic categories include N for noun
and S for sentence. Functional categories are
functions constructed recursively from atomic cat-
egories; these categories are denoted using slashes
to separate the category’s argument type from its
return type. The argument type appears on the
right side of the slash, and the return type on the
left. The direction of slash determines where the
argument must appear – / means an argument on
the right, and \ means an argument on the left.
Syntactic categories in ASP are annotated with
two additional kinds of information. First, atomic
categories may have associated syntactic features
given in square brackets. These features are used
in CCGbank to distinguish variants of atomic syn-
tactic categories, e.g., S[dcl] denotes a declara-
tive sentence. Second, each category is anno-
tated with head and dependency information us-
ing subscripts. These subscripts are used to pop-
ulate predicate-argument dependencies (described
below), and to pass head information using unifi-
cation. For example, the head of the parse in Fig-
ure 1 is “area,” due to the coindexing of the argu-
ment and return categories in the category N1\N1.
In addition to the syntactic category, each lexi-
con entry has a semantic type and a logical form.
The semantic type is a category or relation pred-
icate that concisely represents the word’s seman-
tics. The semantic type is used to enforce type
constraints during parsing and to include seman-
tics in the parser’s parametrization. The logi-
cal form gives the full semantics of the word in
lambda calculus. The parser also allows lexicon
entries with the semantic type “—”, representing
words whose semantics cannot be expressed using
predicates from the ontology.
Parsing in CCG combines adjacent categories
using a small number of combinators, such as
function application:
</bodyText>
<equation confidence="0.792122">
X/Y : f Y : g =⇒ X : f(g)
Y : g X\Y : f =⇒ X : f(g)
</equation>
<bodyText confidence="0.999969714285714">
The first rule states that the category X/Y can
be applied to the category Y , returning category
X, and that the logical form f is applied to g to
produce the logical form for the returned category.
Head words and semantic types are also propa-
gated to the returned category based on the anno-
tated head-passing markup.
</bodyText>
<subsectionHeader confidence="0.999617">
3.3 Dependency Structures
</subsectionHeader>
<bodyText confidence="0.999735083333333">
Parsing a sentence produces a collection of depen-
dency structures which summarize the predicate-
argument structure of the sentence. Dependency
structures are 10-tuples, of the form:
&lt; head word, head POS, head semantic type, head word
index, head word syntactic category, argument number, ar-
gument word, argument POS, argument semantic type, argu-
ment word index &gt;
A dependency structure captures a relationship
between a head word and its argument. During
parsing, whenever a subscripted argument of a
syntactic category is filled, a dependency structure
</bodyText>
<page confidence="0.950647">
1190
</page>
<bodyText confidence="0.99996375">
is created between the head of the applied func-
tion and its argument. For example, in Figure 1,
the first application fills argument 1 of “beautiful”
with “London,” creating a dependency structure.
</bodyText>
<subsectionHeader confidence="0.997139">
3.4 Logical Forms
</subsectionHeader>
<bodyText confidence="0.999987813953489">
ASP performs a best-effort semantic analysis of
every parsed sentence, producing logical forms for
subspans of the sentence when possible. Logical
forms are designed so that the meaning of a sen-
tence is a universally- and existentially-quantified
conjunction of predicates with partially shared ar-
guments. This representation allows the parser to
produce semantic analyses for a reasonable subset
of language, including prepositions, verbs, nouns,
relative clauses, and conjunctions.
Figure 1 shows a representative sample of a log-
ical form produced by ASP. Generally, the parser
produces a lambda calculus statement with sev-
eral existentially-quantified variables ranging over
entities in the knowledge base. The only excep-
tion to this rule is conjunctions, which are rep-
resented using a scoped universal quantifier over
the conjoined predicates. Entity mentions appear
in logical forms via a special mention predicate,
M, instead of as database constants. For exam-
ple, “London” appears as M(x, “london”, CITY),
instead of as a constant like LONDON. The mean-
ing of this mention predicate is that x is an en-
tity which can be called “london” and belongs to
the CITY category. This representation propagates
uncertainty about entity references into the logical
form where background knowledge can be used
for disambiguation. For example, “London, Eng-
land” is assigned a logical form that disambiguates
“London” to a “London” located in “England.”1
Lexicon entries without a semantic type are au-
tomatically assigned logical forms based on their
head passing markup. For example, in Figure 1,
the adjective “beautiful” is assigned Af.f. This
approach allows a logical form to be derived for
most sentences, but (somewhat counterintuitively)
can lose interesting logical forms from constituent
subspans. For example, the preposition “in” has
syntactic category (N1\N1)/N2, which results in
the logical form Af.Ag.g. This logical form dis-
cards any information present in the argument f.
We avoid this problem by extracting a logical form
from every subtree of the CCG parse.
</bodyText>
<construct confidence="0.6771275">
1Specifically, λx.∃y.CITYLOCATEDINCOUNTRY(x, y) ∧
M(x, “london”, CITY) ∧ M(y, “england”, COUNTRY)
</construct>
<subsectionHeader confidence="0.977368">
3.5 Parametrization
</subsectionHeader>
<bodyText confidence="0.994768043478261">
The parser F is trained as a discriminative linear
model of the following form:
F(E, d, t|s; θ) = θT φ(d, t, s)
Given a parameter vector θ and a sentence s, the
parser produces a score for a syntactic parse tree
t, a collection of dependency structures d and a
logical form E. The score depends on features of
the parse produced by the feature function φ.
φ contains four classes of features: lexicon
features, combinator features, dependency fea-
tures and dependency distance features (Table 1).
These features are based on those of C&amp;C (Clark
and Curran, 2007b), modified to include seman-
tic types. The features are designed to share syn-
tactic information about a word across its distinct
semantic realizations in order to transfer syntactic
information from CCGbank to semantic parsing.
The parser also includes a hard type-checking
constraint to ensure that logical forms are well-
typed. This constraint states that dependency
structures with a head semantic type only accept
arguments that (1) have a semantic type, and (2)
are within the domain/range of the head type.
</bodyText>
<sectionHeader confidence="0.983323" genericHeader="method">
4 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9998814">
This section describes the training procedure for
ASP. Training is performed by minimizing a joint
objective function combining a syntactic parsing
task and a distantly-supervised relation extraction
task. The input training data includes:
</bodyText>
<listItem confidence="0.998916833333333">
1. A collection L of sentences si with annotated
syntactic trees ti (e.g., CCGbank).
2. A corpus of sentences S (e.g., Wikipedia).
3. A knowledge base K (e.g., NELL), contain-
ing relation instances r(e1, e2) ∈ K.
4. A CCG lexicon A (see Section 5.2).
</listItem>
<bodyText confidence="0.999866076923077">
Given these resources, the algorithm described
in this section produces parameters θ for a se-
mantic parser. Our parameter estimation proce-
dure constructs a joint objective function O(θ) that
decomposes into syntactic and semantic compo-
nents: O(θ) = Osyn(θ) + Osem(θ). The syntac-
tic component Osyn is a standard syntactic pars-
ing objective constructed using the syntactic re-
source L. The semantic component Osem is a
distantly-supervised relation extraction task based
on the semantic constraint from Krishnamurthy
and Mitchell (2012). These components are de-
scribed in more detail in the following sections.
</bodyText>
<page confidence="0.910373">
1191
</page>
<table confidence="0.999863307692308">
Lexicon features: word, POS := X : t : f Dependency Features: &lt; hw, hp, ht, hi, s, n, aw, ap, at, ai &gt;
Word/syntactic category word, X Predicate-Argument Indicator &lt; hw, —, ht, —, s, n, aw, —, at, — &gt;
POS/syntactic category POS, X Word-Word Indicator &lt; hw, —, —, —, s, n, aw, —, —, — &gt;
Word semantics word, X, t Predicate-POS Indicator &lt; hw, —, ht, —, s, n, —, ap, —, — &gt;
Combinator features: X Y Y Z or X Y Z Word-POS Indicator &lt; hw, —, —, —, s, n, —, ap, —, — &gt;
Binary combinator indicator X Y Y Z POS-Argument Indicator &lt; —, hp, —, —, s, n, aw, —, at, — &gt;
Unary combinator indicator XYZ POS-Word Indicator &lt; —, hp, —, —, s, n, aw, —, —, — &gt;
Root syntactic category Z POS-POS Indicator &lt; —, hp, —, —, s, n, —, ap, —, — &gt;
Dependency Distance Features:
Token distance hw, ht, —, s, n, d d = Number of tokens between hi and ai: 0, 1, 2 or more.
Token distance word backoff hw, —, s, n, d d = Number of tokens between hi and ai: 0, 1, 2 or more.
Token distance POS backoff —, —, hp, s, n, d d = Number of tokens between hi and ai: 0, 1, 2 or more.
(The above distance features are repeated using the number of intervening verbs and punctuation marks.)
</table>
<tableCaption confidence="0.823155666666667">
Table 1: Listing of parser feature templates used in the feature function φ. Each feature template repre-
sents a class of indicator features that fire during parsing when lexicon entries are used, combinators are
applied, or dependency structures are instantiated.
</tableCaption>
<subsectionHeader confidence="0.990265">
4.1 Syntactic Objective
</subsectionHeader>
<bodyText confidence="0.9990882">
The syntactic objective is the structured percep-
tron objective instantiated for a syntactic parsing
task. This objective encourages the parser to accu-
rately reproduce the syntactic parses in the anno-
tated corpus L = {(si, ti)}ni=1:
</bodyText>
<equation confidence="0.988183">
n
Osyn(θ) =
i=1
Γ(`∗, d∗, ti|si; θ)|+
</equation>
<bodyText confidence="0.9999675">
The first term in the above expression represents
the best CCG parse of the sentence si according to
the current model. The second term is the best
parse of si whose syntactic tree equals the true
syntactic tree ti. In the above equation  |· |+ de-
notes the positive part of the expression. Minimiz-
ing this objective therefore finds parameters θ that
reproduce the annotated syntactic trees.
</bodyText>
<subsectionHeader confidence="0.99335">
4.2 Semantic Objective
</subsectionHeader>
<bodyText confidence="0.998738933333333">
The semantic objective corresponds to a distantly-
supervised relation extraction task that constrains
the logical forms produced by the semantic parser.
Distant supervision is provided by the following
constraint: every relation instance r(e1, e2) ∈ K
must be expressed by at least one sentence in
S(e1,e2), the set of sentences that mention both e1
and e2 (Hoffmann et al., 2011). If this constraint
is empirically true and sufficiently constrains the
parser’s logical forms, then optimizing the seman-
tic objective produces an accurate semantic parser.
A training example in the semantic objective
consists of the set of sentences mentioning a pair
of entities, S(e1,e2) = {s1,s2, ...}, paired with a
binary vector representing the set of relations that
the two entities participate in, y(e1,e2). The distant
supervision constraint Ψ forces the logical forms
predicted for the sentences to entail the relations
in y(e1,e2). Ψ is a deterministic OR constraint that
checks whether each logical form entails the re-
lation instance r(e1, e2), deterministically setting
yr = 1 if any logical form entails the instance and
yr = 0 otherwise.
Let (`, d, t) represent a collection of seman-
tic parses for the sentences S = S(e1,e2). Let
Γ(`, d, t|S; θ) = E|S|
i=1 Γ(`i, di, ti|si; θ) represent
the total weight assigned by the parser to a collec-
tion of parses for the sentences S. For the pair of
entities (e1, e2), the semantic objective is:
</bodyText>
<subsectionHeader confidence="0.995765">
4.3 Optimization
</subsectionHeader>
<bodyText confidence="0.999989222222222">
Training minimizes the joint objective using the
structured perceptron algorithm, which can be
viewed as the stochastic subgradient method
(Ratliff et al., 2006) applied to the objective
O(θ). We initialize the parameters to zero, i.e.,
θ0 = 0. On each iteration, we sample either a
syntactic example (si, ti) or a semantic example
(S(e1,e2),y(e1,e2)). If a syntactic example is sam-
pled, we apply the following parameter update:
</bodyText>
<equation confidence="0.953229">
ˆ`, ˆd, tˆ ←arg max
`,d,t
`∗, d∗ ← arg max Γ(`, d, ti|si; θt)
`,d
θt+1 ← θt + φ(d∗, ti, si) − φ(ˆd, ˆt, si)
</equation>
<bodyText confidence="0.999571666666667">
This update moves the parameters toward the fea-
tures of the best parse with the correct syntactic
derivation, φ(d∗, ti, si). If a semantic example is
</bodyText>
<equation confidence="0.964334777777778">
Γ(ˆ`, ˆd, ˆt|si; θ) −
 |max
ˆ`, ˆd,ˆt
max
`*,d*
Osem(θ) =  |max Γ(ˆ`, ˆd,ˆt|S; θ) − max
0&apos;ˆt �*,d*,t*
(Ψ(y(e1,e2),`∗, d∗,t∗) + Γ(`∗, d∗, t∗|S; θ))|+
Γ(`, d, t|si; θt)
</equation>
<page confidence="0.920416">
1192
</page>
<table confidence="0.999695833333333">
Labeled Dependencies Unlabeled Dependencies Coverage
P R F P R F
ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63
ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63
C&amp;C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63
(Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83
</table>
<tableCaption confidence="0.9366725">
Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using
precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies.
</tableCaption>
<bodyText confidence="0.922804">
sampled, we instead apply the following update:
</bodyText>
<equation confidence="0.996563">
F(`, d, t|S(e1,e2); Bt)
F(`, d, t|S(e1,e2); Bt)
+ Ψ(y(e1,e2), `, d, t)
Bt+1 ← Bt + 0(d*, t*, S(e1,e2))
− 0( ˆd,ˆt, S(e1,e2))
</equation>
<bodyText confidence="0.9993325">
This update moves the parameters toward the fea-
tures of the best set of parses that satisfy the distant
supervision constraint. Training outputs the aver-
age of each iteration’s parameters, B¯ = 1 Etn Bt.
</bodyText>
<equation confidence="0.677633">
n =1
</equation>
<bodyText confidence="0.999971888888889">
In practice, we train the parser by performing a
single pass over the examples in the data set.
All of the maximizations above can be per-
formed exactly using a CKY-style chart parsing
algorithm, except for the last one. This maxi-
mization is intractable due to the coupling between
logical forms in ` caused by enforcing the dis-
tant supervision constraint. We approximate this
maximization in two steps. First, we perform a
beam search to produce a list of candidate parses
for each sentence s E S(e1,e2). We then extract
relation instances from each parse and apply the
greedy inference algorithm from Hoffmann et al.,
(2011) to identify the best set of parses that satisfy
the distant supervision constraint. The procedure
skips any examples with sentences that cannot be
parsed (due to beam search failures) or where the
distant supervision constraint cannot be satisfied.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999993625">
The experiments below evaluate ASP’s syntactic
and semantic parsing ability. The parser is trained
on CCGbank and a corpus of Wikipedia sentences,
using NELL’s predicate vocabulary. The syntactic
analyses of the trained parser are evaluated against
CCGbank, and its logical forms are evaluated on
an information extraction task and against an an-
notated test set of Wikipedia sentences.
</bodyText>
<subsectionHeader confidence="0.99652">
5.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.99999124">
The data sets for the evaluation consist of CCG-
bank, a corpus of dependency-parsed Wikipedia
sentences, and a logical knowledge base derived
from NELL and Freebase. Sections 02-21 of
CCGbank were used for training, Section 00 for
validation, and Section 23 for the final results. The
knowledge base’s predicate vocabulary is taken
from NELL, and its instances are taken from Free-
base using a manually-constructed mapping be-
tween Freebase and NELL. Using Freebase rela-
tion instances produces cleaner training data than
NELL’s automatically-extracted instances.
Using the relation instances and Wikipedia sen-
tences, we constructed a data set for distantly-
supervised relation extraction. We identified men-
tions of entities in each sentence using simple
string matching, then aggregated these sentences
by entity pair. 20% of the entity pairs were set
aside for validation. In the remaining training
data, we downsampled entity pairs that did not
participate in at least one relation. We further
eliminated sentences containing more than 30 to-
kens. The resulting training corpus contains 25k
entity pairs (half of which participate in a relation),
41k sentences, and 71 distinct relation predicates.
</bodyText>
<subsectionHeader confidence="0.996246">
5.2 Grammar Construction
</subsectionHeader>
<bodyText confidence="0.9996322">
The grammar for ASP contains the annotated lex-
icon entries and grammar rules in Sections 02-21
of CCGbank, and additional semantic entries pro-
duced using a set of dependency parse heuristics.
The lexicon A contains all words that occur at
least 20 times in CCGbank. Rare words are re-
placed by their part of speech. The head pass-
ing and dependency markup was generated using
the rules of the C&amp;C parser (Clark and Curran,
2007b). These lexicon entries are also annotated
with logical forms capturing their head passing re-
lationship. For example, the adjective category
N1/N1 is annotated with the logical form Af.f.
These entries are all assigned semantic type —.
We augment this lexicon with additional entries
</bodyText>
<figure confidence="0.849437125">
ˆ`, ˆd,ˆt ← arg max
f,d,t
`*, d*, t* ← arg max
f,d,t
1193
Extracted Logical Form
Sentence
St. John, a Mexican-American born in San Francisco, Califor-
</figure>
<bodyText confidence="0.988720166666667">
nia, her family comes from Zacatecas, Mexico.
The capital and largest city of Laos is Vientiane and other major
cities include Luang Prabang, Savannakhet and Pakse.
Gellar next played a lead role in James Toback ’s critically
unsuccessful independent “Harvard Man” (2001), where she
played the daughter of a mobster.
</bodyText>
<equation confidence="0.953891375">
ax.∃y, z.M(x, “st. john”) ∧ M(y, “san francisco”) ∧
PERSONBORNINLOCATION(x, y) ∧
CITYLOCATEDINSTATE(y, z) ∧ M(z, “california”)
∃x, y.M(x, “vientiane”) ∧ CITY(x) ∧
CITYCAPITALOFCOUNTRY(x, y) ∧ M(y, “laos”)
ax.∃y.M(y, “james toback”) ∧
DIRECTORDIRECTEDMOVIE(y, x) ∧
M(x, “harvard man”)
</equation>
<figureCaption confidence="0.9924325">
Figure 2: Logical forms produced by ASP for sentences in the information extraction corpus. Each
logical form is extracted from the underlined sentence portion.
</figureCaption>
<figure confidence="0.975592">
0 300 600 900
</figure>
<figureCaption confidence="0.869418">
Figure 3: Logical form precision as a function of
the expected number of correct extracted logical
forms. ASP extracts more correct logical forms
because it jointly analyzes syntax and semantics.
</figureCaption>
<bodyText confidence="0.999954272727273">
mapping words to logical forms with NELL pred-
icates. These entries are instantiated using a set
of dependency parse patterns, listed in an online
appendix.2 These patterns are applied to the train-
ing corpus, heuristically identifying verbs, prepo-
sitions, and possessives that express relations, and
nouns that express categories. The patterns also
include special cases for forms of “to be.” This
process generates ∼4000 entries (not counting en-
tity names), representing 69 relations and 61 cate-
gories from NELL. Section 3.2 shows several lex-
icon entries generated by this process.
The parser’s combinators include function ap-
plication, composition, and crossed composition,
as well as several binary and unary type-changing
rules that occur in CCGbank. All combinators
were restricted to only apply to categories that
combine in Sections 02-21. Finally, the grammar
includes a number of heuristically-instantiated bi-
nary rules of the form , N → N\N that instanti-
ate a relation between adjacent nouns. These rules
capture appositives and some other constructions.
</bodyText>
<subsectionHeader confidence="0.995876">
5.3 Supertagging
</subsectionHeader>
<bodyText confidence="0.99789925">
Parsing in practice can be slow because the
parser’s lexicalized grammar permits a large num-
ber of parses for a sentence. We improve parser
performance by performing supertagging (Banga-
</bodyText>
<footnote confidence="0.802543">
2http://rtw.ml.cmu.edu/acl2014_asp/
</footnote>
<bodyText confidence="0.9999713">
lore and Joshi, 1999; Clark and Curran, 2004).
We trained a logistic regression classifier to pre-
dict the syntactic category of each token in a sen-
tence from features of the surrounding tokens and
POS tags. Subsequent parsing is restricted to only
consider categories whose probability is within a
factor of α of the highest-scoring category. The
parser uses a backoff strategy, first attempting to
parse with the supertags from α = 0.01, backing
off to α = 0.001 if the initial parsing attempt fails.
</bodyText>
<subsectionHeader confidence="0.992037">
5.4 Syntactic Evaluation
</subsectionHeader>
<bodyText confidence="0.999938433333333">
The syntactic evaluation measures ASP’s ability
to reproduce the predicate-argument dependencies
in CCGbank. As in previous work, our evalu-
ation uses labeled and unlabeled dependencies.
Labeled dependencies are dependency structures
with both words and semantic types removed,
leaving two word indexes, a syntactic category,
and an argument number. Unlabeled dependen-
cies further eliminate the syntactic category and
argument number, leaving a pair of word indexes.
Performance is measured using precision, recall,
and F-measure against the annotated dependency
structures in CCGbank. Precision is the fraction
of predicted dependencies which are in CCGbank,
recall is the fraction of CCGbank dependencies
produced by the parser, and F-measure is the har-
monic mean of precision and recall.
For comparison, we also trained a syntactic ver-
sion of our parser, ASP-SYN, using only the CCG-
bank lexicon and grammar. Comparing against
this parser lets us measure the effect of the rela-
tion extraction task on syntactic parsing.
Table 2 shows the results of our evaluation.
For comparison, we include results for two ex-
isting syntactic CCG parsers: C&amp;C, the current
state-of-the-art CCG parser (Clark and Curran,
2007b), and the next best system (Hockenmaier,
2003a). Both ASP and ASP-SYN perform rea-
sonably well, within 2.5% of the performance of
C&amp;C at the same coverage level. However, ASP-
</bodyText>
<figure confidence="0.993796444444444">
1.0
0.8
0.6
ASP
PIPELINE
K&amp;M-2012
0
0.4
0.2
</figure>
<page confidence="0.986619">
1194
</page>
<table confidence="0.9950404">
Logical Form Extraction Extraction
Accuracy Precision Recall
ASP 0.28 0.90 0.32
K&amp;M-2012 0.14 1.00 0.06
PIPELINE 0.2 0.63 0.17
</table>
<tableCaption confidence="0.983134">
Table 3: Logical form accuracy and extraction pre-
</tableCaption>
<bodyText confidence="0.9858165">
cision/recall on the annotated test set. The high
extraction recall for ASP shows that it produces
more complete logical forms than either baseline.
SYN outperforms ASP by around 0.5%, suggesting
that ASP’s additional semantic knowledge slightly
hurts syntactic parsing performance. This perfor-
mance loss appears to be largely due to poor en-
tity mention detection, as we found that not us-
ing entity mention lexicon entries at test time im-
proves ASP’s labeled and unlabeled F-scores by
0.3% on Section 00. The knowledge base contains
many infrequently-mentioned entities with com-
mon names; these entities contribute incorrect se-
mantic type information that confuses the parser.
</bodyText>
<subsectionHeader confidence="0.997447">
5.5 Semantic Evaluation
</subsectionHeader>
<bodyText confidence="0.9999802">
We performed two semantic evaluations to bet-
ter understand ASP’s ability to construct logical
forms. The first evaluation emphasizes precision
over recall, and the second evaluation accurately
measures recall using a manually labeled test set.
</bodyText>
<subsectionHeader confidence="0.93003">
5.5.1 Baselines
</subsectionHeader>
<bodyText confidence="0.999982615384615">
For comparison, we also trained two base-
line models. The first baseline, PIPELINE, is
a pipelined syntax-then-semantics approach de-
signed to mimic Boxer (Bos, 2005). This base-
line first syntactically parses each sentence using
ASP-SYN, then produces a semantic analysis by
assigning a logical form to each word. We train
this baseline using the semantic objective (Section
4.2) while holding fixed the syntactic parse of each
sentence. Note that, unlike Boxer, this baseline
learns which logical form to assign each word, and
its logical forms contain NELL predicates.
The second baseline, K&amp;M-2012, is the ap-
proach of Krishnamurthy and Mitchell (2012),
representing the state-of-the-art in distantly-
supervised semantic parsing. This approach trains
a semantic parser by combining distant seman-
tic supervision with syntactic supervision from
dependency parses. The best performing vari-
ant of this system also uses dependency parses
at test time to constrain the interpretation of
test sentences – hence, this system also uses a
pipelined syntax-then-semantics approach. To im-
prove comparability, we reimplemented this ap-
proach using our parsing model, which has richer
features than were used in their paper.
</bodyText>
<subsubsectionHeader confidence="0.480683">
5.5.2 Information Extraction Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999979936170213">
The information extraction evaluation uses each
system to extract logical forms from a large cor-
pus of sentences, then measures the fraction of
extracted logical forms that are correct. The test
set consists of 8.5k sentences sampled from the
held-out Wikipedia sentences. Each system was
run on this data set, extracting all logical forms
from each sentence that entailed at least one cat-
egory or relation instance. We ranked these ex-
tractions using the parser’s inside chart score, then
manually annotated a sample of 250 logical forms
from each system for correctness. Logical forms
were marked correct if all category and relation
instances entailed by the logical form were ex-
pressed by the sentence. Note that a correct logical
form need not entail all of the relations expressed
by the sentence, reflecting an emphasis on preci-
sion over recall. Figure 2 shows some example
logical forms produced by ASP in the evaluation.
The annotated sample of logical forms allows
us to estimate precision for each system as a func-
tion of the number of correct extractions (Figure
3). The number of correct extractions is directly
proportional to recall, and was estimated from the
total number of extractions and precision at each
rank in the sample. All three systems initially
have high precision, implying that their extracted
logical forms express facts found in the sentence.
However, ASP produces 3 times more correct log-
ical forms than either baseline because it jointly
analyzes syntax and semantics. The baselines suf-
fer from reduced recall because they depend on re-
ceiving an accurate syntactic parse as input; syn-
tactic parsing errors cause these systems to fail.
Examining the incorrect logical forms produced
by ASP reveals that incorrect mention detection is
by far the most common source of mistakes. Ap-
proximately 50% of errors are caused by marking
common nouns as entity mentions (e.g., marking
“coin” as a COMPANY). These errors occur be-
cause the knowledge base contains many infre-
quently mentioned entities with relatively com-
mon names. Another 30% of errors are caused by
assigning an incorrect type to a common proper
noun (e.g, marking “Bolivia” as a CITY). This
analysis suggests that performing entity linking
before parsing could significantly reduce errors.
</bodyText>
<page confidence="0.984974">
1195
</page>
<table confidence="0.880735578947368">
Sentence: De Niro and Joe Pesci in “Goodfellas” offered a virtuoso display of the director’s bravura cinematic
technique and reestablished, enhanced, and consolidated his reputation.
Annotation:
LF: Ax.`dp E {Ad.M(d, “de niro”), Aj.M(j, “joe pesci”)}3y.p(x) n STARREDINMOVIE(x, y) n M(y, “goodfellas”)
Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)
Prediction:
LF: Ax.`dp E {Ad.M(d, “de niro”), Aj.M(j, “joe pesci”)}3y.p(x) n STARREDINMOVIE(x, y) n M(y, “goodfellas”)
Instances: STARREDINMOVIE(de niro, goodfellas), STARREDINMOVIE(joe pesci, goodfellas)
Logical form accuracy: 1 / 1 Extraction Precision: 2 / 2 Extraction Recall: 2 / 2
Sentence: In addition to the University of Illinois, Champaign is also home to Parkland College.
Annotation:
LF: 3c, p.M(c, “champaign”) n CITY(c) n M(p, “parkland college”) n UNIVERSITYINCITY(p, c)
Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign)
Prediction:
LF 1: Ax.3yM(y, “illinois”) n M(x, “university”) n CITYLOCATEDINSTATE(x, y)
LF 2: 3c, p.M(c, “champaign”) n CITY(c) n M(p, “parkland college”) n UNIVERSITYINCITY(p, c)
Instances: CITY(champaign), UNIVERSITYINCITY(parkland college, champaign),
CITYLOCATEDINSTATE(university, illinois)
Logical form accuracy: 1 / 1 Extraction Precision: 2 / 3 Extraction Recall: 2 / 2
</table>
<figureCaption confidence="0.617153">
Figure 4: Two test examples with ASP’s predictions and error calculations. The annotated logical forms
are for the italicized sentence spans, while the extracted logical forms are for the underlined spans.
</figureCaption>
<subsubsectionHeader confidence="0.530616">
5.5.3 Annotated Sentence Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999984142857143">
A limitation of the previous evaluation is that it
does not measure the completeness of predicted
logical forms, nor estimate what portion of sen-
tences are left unanalyzed. We conducted a second
evaluation to measure these quantities.
The data for this evaluation consists of sen-
tences annotated with logical forms for subspans.
We manually annotated Wikipedia sentences from
the held-out set with logical forms for the largest
subspans for which a logical form existed. To
avoid trivial cases, we only annotated logical
forms containing at least one category or relation
predicate and at least one mention. We also chose
not to annotate mentions of entities that are not in
the knowledge base, as no system would be able
to correctly identify them. The corpus contains 97
sentences with 100 annotated logical forms.
We measured performance using two met-
rics: logical form accuracy, and extraction preci-
sion/recall. Logical form accuracy examines the
predicted logical form for the smallest subspan of
the sentence containing the annotated span, and
marks this prediction correct if it exactly matches
the annotation. A limitation of this metric is that it
does not assign partial credit to logical forms that
are close to, but do not exactly match, the anno-
tation. The extraction metric assigns partial credit
by computing the precision and recall of the cat-
egory and relation instances entailed by the pre-
dicted logical form, using those entailed by the an-
notated logical form as the gold standard. Figure
4 shows the computation of both error metrics on
two examples from the test corpus.
Table 3 shows the results of the annotated sen-
tence evaluation. ASP outperforms both baselines
in logical form accuracy and extraction recall, sug-
gesting that it produces more complete analyses
than either baseline. The extraction precision of
90% suggests that ASP rarely extracts incorrect in-
formation. Precision is higher in this evaluation
because every sentence in the data set has at least
one correct extraction.
</bodyText>
<sectionHeader confidence="0.999683" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999984111111111">
We present an approach to training a joint syntac-
tic and semantic parser. Our parser ASP produces
a full syntactic parse of any sentence, while simul-
taneously producing logical forms for sentence
spans that have a semantic representation within
its predicate vocabulary. The parser is trained
by jointly optimizing performance on a syntac-
tic parsing task and a distantly-supervised rela-
tion extraction task. Experimental results demon-
strate that jointly analyzing syntax and semantics
triples the number of extracted logical forms over
approaches that first analyze syntax, then seman-
tics. However, we also find that incorporating se-
mantics slightly reduces syntactic parsing perfor-
mance. Poor entity mention detection is a major
source of error in both cases, suggesting that fu-
ture work should consider integrating entity link-
ing with joint syntactic and semantic parsing.
</bodyText>
<sectionHeader confidence="0.998032" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99279">
This work was supported in part by DARPA under
award FA8750-13-2-0005. We additionally thank
Jamie Callan and Chris R´e’s Hazy group for col-
lecting and processing the Wikipedia corpus.
</bodyText>
<page confidence="0.991747">
1196
</page>
<sectionHeader confidence="0.980249" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999687698113208">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th International Conference on Com-
putational Linguistics - Volume 1.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237–265.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing.
Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In In Proceedings of Sixth In-
ternational Workshop on Computational Semantics
IWCS-6.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale Semantic Parsing via Schema Matching and
Lexicon Extension. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Qingqing Cai and Alexander Yates. 2013b. Semantic
Parsing Freebase: Towards Open-domain Semantic
Parsing. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intel-
ligence.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics.
Stephen Clark and James R. Curran. 2007a. Per-
ceptron training for a wide-coverage lexicalized-
grammar parser. In Proceedings of the Workshop on
Deep Linguistic Processing.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
the world’s response. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the Ninth Conference on
Computational Natural Language Learning.
Julia Hockenmaier and Mark Steedman. 2002a.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proceedings of Third Interna-
tional Conference on Language Resources and Eval-
uation.
Julia Hockenmaier and Mark Steedman. 2002b. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics.
Julia Hockenmaier. 2003a. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier. 2003b. Parsing with generative
models of predicate-argument structure. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceedings
of the Conference.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Association for Compu-
tational Linguistics, Portland, Oregon. Association
for Computational Linguistics.
</reference>
<page confidence="0.874585">
1197
</page>
<reference confidence="0.998591388888889">
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.
Zinkevich. 2006. (online) subgradient methods
for structured prediction. Artificial Intelligence and
Statistics.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press, Cambridge, MA, USA.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for
the web of data. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the thirteenth na-
tional conference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: struc-
tured classification with probabilistic categorial
grammars. In UAI ’05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence.
</reference>
<page confidence="0.995381">
1198
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725708">
<title confidence="0.9482245">Joint Syntactic and Semantic Parsing Combinatory Categorial Grammar</title>
<author confidence="0.872543">Jayant</author>
<affiliation confidence="0.99456">Carnegie Mellon</affiliation>
<address confidence="0.9875385">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999652">jayantk@cs.cmu.edu</email>
<abstract confidence="0.995158863636364">We present an approach to training a joint syntactic and semantic parser that combines syntactic training information from CCGbank with semantic training information from a knowledge base via distant supervision. The trained parser produces a full syntactic parse of any sentence, while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parser’s predicate vocabulary. We demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the NELL ontology. A semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach. A syntactic evaluation on CCGbank demonstrates that the parser’s dependency Fscore is within 2.5% of state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics -</booktitle>
<volume>1</volume>
<contexts>
<context position="2327" citStr="Baker et al., 1998" startWordPosition="344" endWordPosition="347">lied to new text, and in its output semantic representation. Using semantic information from the knowledge base at training and test time will Tom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 tom.mitchell@cmu.edu ideally improve the parser’s ability to solve difficult syntactic parsing problems, as in the examples above. A semantic representation tied to a knowledge base allows for powerful inference operations – such as identifying the possible entity referents of a noun phrase – that cannot be performed with shallower representations (e.g., frame semantics (Baker et al., 1998) or a direct conversion of syntax to logic (Bos, 2005)). This paper presents an approach to training a joint syntactic and semantic parser using a large background knowledge base. Our parser produces a full syntactic parse of every sentence, and furthermore produces logical forms for portions of the sentence that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like Ax.CITY(x) ∧ LOCATEDIN(x, CALIFORNIA) to the “town in California” portion. Additionally, the parser us</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th International Conference on Computational Linguistics - Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6254" citStr="Berant et al., 2013" startWordPosition="949" endWordPosition="953">g and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semantically analyzing conjunctions and relative clauses based on the syntact</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Towards wide-coverage semantic interpretation. In</title>
<date>2005</date>
<booktitle>In Proceedings of Sixth International Workshop on Computational Semantics IWCS-6.</booktitle>
<contexts>
<context position="2381" citStr="Bos, 2005" startWordPosition="357" endWordPosition="358">ing semantic information from the knowledge base at training and test time will Tom M. Mitchell Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 tom.mitchell@cmu.edu ideally improve the parser’s ability to solve difficult syntactic parsing problems, as in the examples above. A semantic representation tied to a knowledge base allows for powerful inference operations – such as identifying the possible entity referents of a noun phrase – that cannot be performed with shallower representations (e.g., frame semantics (Baker et al., 1998) or a direct conversion of syntax to logic (Bos, 2005)). This paper presents an approach to training a joint syntactic and semantic parser using a large background knowledge base. Our parser produces a full syntactic parse of every sentence, and furthermore produces logical forms for portions of the sentence that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like Ax.CITY(x) ∧ LOCATEDIN(x, CALIFORNIA) to the “town in California” portion. Additionally, the parser uses predicate and entity type information during parsin</context>
<context position="5313" citStr="Bos, 2005" startWordPosition="794" endWordPosition="795">ources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision co</context>
<context position="31463" citStr="Bos, 2005" startWordPosition="5026" endWordPosition="5027">dge base contains many infrequently-mentioned entities with common names; these entities contribute incorrect semantic type information that confuses the parser. 5.5 Semantic Evaluation We performed two semantic evaluations to better understand ASP’s ability to construct logical forms. The first evaluation emphasizes precision over recall, and the second evaluation accurately measures recall using a manually labeled test set. 5.5.1 Baselines For comparison, we also trained two baseline models. The first baseline, PIPELINE, is a pipelined syntax-then-semantics approach designed to mimic Boxer (Bos, 2005). This baseline first syntactically parses each sentence using ASP-SYN, then produces a semantic analysis by assigning a logical form to each word. We train this baseline using the semantic objective (Section 4.2) while holding fixed the syntactic parse of each sentence. Note that, unlike Boxer, this baseline learns which logical form to assign each word, and its logical forms contain NELL predicates. The second baseline, K&amp;M-2012, is the approach of Krishnamurthy and Mitchell (2012), representing the state-of-the-art in distantlysupervised semantic parsing. This approach trains a semantic par</context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>Johan Bos. 2005. Towards wide-coverage semantic interpretation. In In Proceedings of Sixth International Workshop on Computational Semantics IWCS-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Largescale Semantic Parsing via Schema Matching and Lexicon Extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6183" citStr="Cai and Yates, 2013" startWordPosition="937" endWordPosition="940">elle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semant</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013a. Largescale Semantic Parsing via Schema Matching and Lexicon Extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Semantic Parsing Freebase: Towards Open-domain Semantic Parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="6183" citStr="Cai and Yates, 2013" startWordPosition="937" endWordPosition="940">elle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semant</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013b. Semantic Parsing Freebase: Towards Open-domain Semantic Parsing. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7997" citStr="Carlson et al., 2010" startWordPosition="1214" endWordPosition="1217">ng category and relation predicates from a broad coverage knowledge base. The parser also outputs a collection of dependency structures summarizing the sentence’s predicate-argument structure. Figure 1 illustrates ASP’s input/output specification. 3.1 Knowledge Base The parser uses category and relation predicates from a broad coverage knowledge base both to construct logical forms and to parametrize the parsing model. The knowledge base is assumed to have two kinds of ontological structure: a generalization/subsumption hierarchy and argument type constraints. This paper uses NELL’s ontology (Carlson et al., 2010), which, for example, specifies that the category ORGANIZATION is a generalization of SPORTSTEAM, and that both arguments to the LOCATEDIN relation must have type LOCATION. These type constraints are enforced during parsing. Throughout this paper, predicate names are shown in SMALLCAPS. 3.2 Syntax ASP uses a lexicalized and semanticallytyped Combinatory Categorial Grammar (CCG) (Steedman, 1996). Most grammatical information in CCG is encoded in a lexicon A, containing entries such as: 1189 area / NN that / WDT includes / VBZ beautiful / JJ London / NNP N λx.LOCATION(x) (N1\N1)/(S[dcl]\NP1)2 λf</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="28247" citStr="Clark and Curran, 2004" startWordPosition="4527" endWordPosition="4530"> rules that occur in CCGbank. All combinators were restricted to only apply to categories that combine in Sections 02-21. Finally, the grammar includes a number of heuristically-instantiated binary rules of the form , N → N\N that instantiate a relation between adjacent nouns. These rules capture appositives and some other constructions. 5.3 Supertagging Parsing in practice can be slow because the parser’s lexicalized grammar permits a large number of parses for a sentence. We improve parser performance by performing supertagging (Banga2http://rtw.ml.cmu.edu/acl2014_asp/ lore and Joshi, 1999; Clark and Curran, 2004). We trained a logistic regression classifier to predict the syntactic category of each token in a sentence from features of the surrounding tokens and POS tags. Subsequent parsing is restricted to only consider categories whose probability is within a factor of α of the highest-scoring category. The parser uses a backoff strategy, first attempting to parse with the supertags from α = 0.01, backing off to α = 0.001 if the initial parsing attempt fails. 5.4 Syntactic Evaluation The syntactic evaluation measures ASP’s ability to reproduce the predicate-argument dependencies in CCGbank. As in pre</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Perceptron training for a wide-coverage lexicalizedgrammar parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Deep Linguistic Processing.</booktitle>
<contexts>
<context position="5096" citStr="Clark and Curran, 2007" startWordPosition="760" endWordPosition="763">educes syntactic parsing accuracy by ∼ 0.5%. 2 Prior Work This paper combines two lines of prior work: broad coverage syntactic parsing with CCG and semantic parsing. Broad coverage syntactic parsing with CCG has produced both resources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Ze</context>
<context position="15638" citStr="Clark and Curran, 2007" startWordPosition="2451" endWordPosition="2454">london”, CITY) ∧ M(y, “england”, COUNTRY) 3.5 Parametrization The parser F is trained as a discriminative linear model of the following form: F(E, d, t|s; θ) = θT φ(d, t, s) Given a parameter vector θ and a sentence s, the parser produces a score for a syntactic parse tree t, a collection of dependency structures d and a logical form E. The score depends on features of the parse produced by the feature function φ. φ contains four classes of features: lexicon features, combinator features, dependency features and dependency distance features (Table 1). These features are based on those of C&amp;C (Clark and Curran, 2007b), modified to include semantic types. The features are designed to share syntactic information about a word across its distinct semantic realizations in order to transfer syntactic information from CCGbank to semantic parsing. The parser also includes a hard type-checking constraint to ensure that logical forms are welltyped. This constraint states that dependency structures with a head semantic type only accept arguments that (1) have a semantic type, and (2) are within the domain/range of the head type. 4 Parameter Estimation This section describes the training procedure for ASP. Training </context>
<context position="21892" citStr="Clark and Curran, 2007" startWordPosition="3531" endWordPosition="3534">date: ˆ`, ˆd, tˆ ←arg max `,d,t `∗, d∗ ← arg max Γ(`, d, ti|si; θt) `,d θt+1 ← θt + φ(d∗, ti, si) − φ(ˆd, ˆt, si) This update moves the parameters toward the features of the best parse with the correct syntactic derivation, φ(d∗, ti, si). If a semantic example is Γ(ˆ`, ˆd, ˆt|si; θ) − |max ˆ`, ˆd,ˆt max `*,d* Osem(θ) = |max Γ(ˆ`, ˆd,ˆt|S; θ) − max 0&apos;ˆt �*,d*,t* (Ψ(y(e1,e2),`∗, d∗,t∗) + Γ(`∗, d∗, t∗|S; θ))|+ Γ(`, d, t|si; θt) 1192 Labeled Dependencies Unlabeled Dependencies Coverage P R F P R F ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63 ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63 C&amp;C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63 (Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83 Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies. sampled, we instead apply the following update: F(`, d, t|S(e1,e2); Bt) F(`, d, t|S(e1,e2); Bt) + Ψ(y(e1,e2), `, d, t) Bt+1 ← Bt + 0(d*, t*, S(e1,e2)) − 0( ˆd,ˆt, S(e1,e2)) This update moves the parameters toward the features of the best set of parses that satisfy the distant supervision constraint. Training output</context>
<context position="25493" citStr="Clark and Curran, 2007" startWordPosition="4109" endWordPosition="4112">more than 30 tokens. The resulting training corpus contains 25k entity pairs (half of which participate in a relation), 41k sentences, and 71 distinct relation predicates. 5.2 Grammar Construction The grammar for ASP contains the annotated lexicon entries and grammar rules in Sections 02-21 of CCGbank, and additional semantic entries produced using a set of dependency parse heuristics. The lexicon A contains all words that occur at least 20 times in CCGbank. Rare words are replaced by their part of speech. The head passing and dependency markup was generated using the rules of the C&amp;C parser (Clark and Curran, 2007b). These lexicon entries are also annotated with logical forms capturing their head passing relationship. For example, the adjective category N1/N1 is annotated with the logical form Af.f. These entries are all assigned semantic type —. We augment this lexicon with additional entries ˆ`, ˆd,ˆt ← arg max f,d,t `*, d*, t* ← arg max f,d,t 1193 Extracted Logical Form Sentence St. John, a Mexican-American born in San Francisco, California, her family comes from Zacatecas, Mexico. The capital and largest city of Laos is Vientiane and other major cities include Luang Prabang, Savannakhet and Pakse. </context>
<context position="29938" citStr="Clark and Curran, 2007" startWordPosition="4789" endWordPosition="4792">ecision is the fraction of predicted dependencies which are in CCGbank, recall is the fraction of CCGbank dependencies produced by the parser, and F-measure is the harmonic mean of precision and recall. For comparison, we also trained a syntactic version of our parser, ASP-SYN, using only the CCGbank lexicon and grammar. Comparing against this parser lets us measure the effect of the relation extraction task on syntactic parsing. Table 2 shows the results of our evaluation. For comparison, we include results for two existing syntactic CCG parsers: C&amp;C, the current state-of-the-art CCG parser (Clark and Curran, 2007b), and the next best system (Hockenmaier, 2003a). Both ASP and ASP-SYN perform reasonably well, within 2.5% of the performance of C&amp;C at the same coverage level. However, ASP1.0 0.8 0.6 ASP PIPELINE K&amp;M-2012 0 0.4 0.2 1194 Logical Form Extraction Extraction Accuracy Precision Recall ASP 0.28 0.90 0.32 K&amp;M-2012 0.14 1.00 0.06 PIPELINE 0.2 0.63 0.17 Table 3: Logical form accuracy and extraction precision/recall on the annotated test set. The high extraction recall for ASP shows that it produces more complete logical forms than either baseline. SYN outperforms ASP by around 0.5%, suggesting that</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007a. Perceptron training for a wide-coverage lexicalizedgrammar parser. In Proceedings of the Workshop on Deep Linguistic Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="5096" citStr="Clark and Curran, 2007" startWordPosition="760" endWordPosition="763">educes syntactic parsing accuracy by ∼ 0.5%. 2 Prior Work This paper combines two lines of prior work: broad coverage syntactic parsing with CCG and semantic parsing. Broad coverage syntactic parsing with CCG has produced both resources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Ze</context>
<context position="15638" citStr="Clark and Curran, 2007" startWordPosition="2451" endWordPosition="2454">london”, CITY) ∧ M(y, “england”, COUNTRY) 3.5 Parametrization The parser F is trained as a discriminative linear model of the following form: F(E, d, t|s; θ) = θT φ(d, t, s) Given a parameter vector θ and a sentence s, the parser produces a score for a syntactic parse tree t, a collection of dependency structures d and a logical form E. The score depends on features of the parse produced by the feature function φ. φ contains four classes of features: lexicon features, combinator features, dependency features and dependency distance features (Table 1). These features are based on those of C&amp;C (Clark and Curran, 2007b), modified to include semantic types. The features are designed to share syntactic information about a word across its distinct semantic realizations in order to transfer syntactic information from CCGbank to semantic parsing. The parser also includes a hard type-checking constraint to ensure that logical forms are welltyped. This constraint states that dependency structures with a head semantic type only accept arguments that (1) have a semantic type, and (2) are within the domain/range of the head type. 4 Parameter Estimation This section describes the training procedure for ASP. Training </context>
<context position="21892" citStr="Clark and Curran, 2007" startWordPosition="3531" endWordPosition="3534">date: ˆ`, ˆd, tˆ ←arg max `,d,t `∗, d∗ ← arg max Γ(`, d, ti|si; θt) `,d θt+1 ← θt + φ(d∗, ti, si) − φ(ˆd, ˆt, si) This update moves the parameters toward the features of the best parse with the correct syntactic derivation, φ(d∗, ti, si). If a semantic example is Γ(ˆ`, ˆd, ˆt|si; θ) − |max ˆ`, ˆd,ˆt max `*,d* Osem(θ) = |max Γ(ˆ`, ˆd,ˆt|S; θ) − max 0&apos;ˆt �*,d*,t* (Ψ(y(e1,e2),`∗, d∗,t∗) + Γ(`∗, d∗, t∗|S; θ))|+ Γ(`, d, t|si; θt) 1192 Labeled Dependencies Unlabeled Dependencies Coverage P R F P R F ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63 ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63 C&amp;C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63 (Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83 Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies. sampled, we instead apply the following update: F(`, d, t|S(e1,e2); Bt) F(`, d, t|S(e1,e2); Bt) + Ψ(y(e1,e2), `, d, t) Bt+1 ← Bt + 0(d*, t*, S(e1,e2)) − 0( ˆd,ˆt, S(e1,e2)) This update moves the parameters toward the features of the best set of parses that satisfy the distant supervision constraint. Training output</context>
<context position="25493" citStr="Clark and Curran, 2007" startWordPosition="4109" endWordPosition="4112">more than 30 tokens. The resulting training corpus contains 25k entity pairs (half of which participate in a relation), 41k sentences, and 71 distinct relation predicates. 5.2 Grammar Construction The grammar for ASP contains the annotated lexicon entries and grammar rules in Sections 02-21 of CCGbank, and additional semantic entries produced using a set of dependency parse heuristics. The lexicon A contains all words that occur at least 20 times in CCGbank. Rare words are replaced by their part of speech. The head passing and dependency markup was generated using the rules of the C&amp;C parser (Clark and Curran, 2007b). These lexicon entries are also annotated with logical forms capturing their head passing relationship. For example, the adjective category N1/N1 is annotated with the logical form Af.f. These entries are all assigned semantic type —. We augment this lexicon with additional entries ˆ`, ˆd,ˆt ← arg max f,d,t `*, d*, t* ← arg max f,d,t 1193 Extracted Logical Form Sentence St. John, a Mexican-American born in San Francisco, California, her family comes from Zacatecas, Mexico. The capital and largest city of Laos is Vientiane and other major cities include Luang Prabang, Savannakhet and Pakse. </context>
<context position="29938" citStr="Clark and Curran, 2007" startWordPosition="4789" endWordPosition="4792">ecision is the fraction of predicted dependencies which are in CCGbank, recall is the fraction of CCGbank dependencies produced by the parser, and F-measure is the harmonic mean of precision and recall. For comparison, we also trained a syntactic version of our parser, ASP-SYN, using only the CCGbank lexicon and grammar. Comparing against this parser lets us measure the effect of the relation extraction task on syntactic parsing. Table 2 shows the results of our evaluation. For comparison, we include results for two existing syntactic CCG parsers: C&amp;C, the current state-of-the-art CCG parser (Clark and Curran, 2007b), and the next best system (Hockenmaier, 2003a). Both ASP and ASP-SYN perform reasonably well, within 2.5% of the performance of C&amp;C at the same coverage level. However, ASP1.0 0.8 0.6 ASP PIPELINE K&amp;M-2012 0 0.4 0.2 1194 Logical Form Extraction Extraction Accuracy Precision Recall ASP 0.28 0.90 0.32 K&amp;M-2012 0.14 1.00 0.06 PIPELINE 0.2 0.63 0.17 Table 3: Logical form accuracy and extraction precision/recall on the annotated test set. The high extraction recall for ASP shows that it produces more complete logical forms than either baseline. SYN outperforms ASP by around 0.5%, suggesting that</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5942" citStr="Clarke et al., 2010" startWordPosition="896" endWordPosition="899">d Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5606" citStr="Ge and Mooney, 2005" startWordPosition="839" endWordPosition="842">, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of Third International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="3260" citStr="Hockenmaier and Steedman, 2002" startWordPosition="488" endWordPosition="492">ntence that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like Ax.CITY(x) ∧ LOCATEDIN(x, CALIFORNIA) to the “town in California” portion. Additionally, the parser uses predicate and entity type information during parsing to select a syntactic parse. Our parser is trained by combining a syntactic parsing task with a distantly-supervised relation extraction task. Syntactic information is provided by CCGbank, a conversion of the Penn Treebank into the CCG formalism (Hockenmaier and Steedman, 2002a). Semantics are learned by training the parser to extract knowledge base relation instances from a corpus of unlabeled sentences, in a distantly-supervised training regime. This approach uses the knowledge base to avoid expensive manual labeling of individual sentence semantics. By optimizing the parser to perform both tasks simultaneously, we train a parser that produces accurate syntactic and semantic analyses. We demonstrate our approach by training a joint syntactic and semantic parser, which we call ASP. ASP produces a full syntactic analysis of every sentence while simultaneously produ</context>
<context position="4820" citStr="Hockenmaier and Steedman, 2002" startWordPosition="716" endWordPosition="719">intly analyzing syntax and semantics improves semantic parsing performance over comparable prior work and a pipelined syntax-then-semantics approach. ASP’s syntactic parsing performance is within 2.5% of state-ofthe-art; however, we also find that incorporating semantic information reduces syntactic parsing accuracy by ∼ 0.5%. 2 Prior Work This paper combines two lines of prior work: broad coverage syntactic parsing with CCG and semantic parsing. Broad coverage syntactic parsing with CCG has produced both resources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a </context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of Third International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3260" citStr="Hockenmaier and Steedman, 2002" startWordPosition="488" endWordPosition="492">ntence that have a semantic representation within the parser’s predicate vocabulary. For example, given a phrase like “my favorite town in California,” our parser will assign a logical form like Ax.CITY(x) ∧ LOCATEDIN(x, CALIFORNIA) to the “town in California” portion. Additionally, the parser uses predicate and entity type information during parsing to select a syntactic parse. Our parser is trained by combining a syntactic parsing task with a distantly-supervised relation extraction task. Syntactic information is provided by CCGbank, a conversion of the Penn Treebank into the CCG formalism (Hockenmaier and Steedman, 2002a). Semantics are learned by training the parser to extract knowledge base relation instances from a corpus of unlabeled sentences, in a distantly-supervised training regime. This approach uses the knowledge base to avoid expensive manual labeling of individual sentence semantics. By optimizing the parser to perform both tasks simultaneously, we train a parser that produces accurate syntactic and semantic analyses. We demonstrate our approach by training a joint syntactic and semantic parser, which we call ASP. ASP produces a full syntactic analysis of every sentence while simultaneously produ</context>
<context position="4820" citStr="Hockenmaier and Steedman, 2002" startWordPosition="716" endWordPosition="719">intly analyzing syntax and semantics improves semantic parsing performance over comparable prior work and a pipelined syntax-then-semantics approach. ASP’s syntactic parsing performance is within 2.5% of state-ofthe-art; however, we also find that incorporating semantic information reduces syntactic parsing accuracy by ∼ 0.5%. 2 Prior Work This paper combines two lines of prior work: broad coverage syntactic parsing with CCG and semantic parsing. Broad coverage syntactic parsing with CCG has produced both resources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a </context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002b. Generative models for statistical parsing with combinatory categorial grammar. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="5013" citStr="Hockenmaier, 2003" startWordPosition="746" endWordPosition="748">ate-ofthe-art; however, we also find that incorporating semantic information reduces syntactic parsing accuracy by ∼ 0.5%. 2 Prior Work This paper combines two lines of prior work: broad coverage syntactic parsing with CCG and semantic parsing. Broad coverage syntactic parsing with CCG has produced both resources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong </context>
<context position="21955" citStr="Hockenmaier, 2003" startWordPosition="3542" endWordPosition="3543">`,d θt+1 ← θt + φ(d∗, ti, si) − φ(ˆd, ˆt, si) This update moves the parameters toward the features of the best parse with the correct syntactic derivation, φ(d∗, ti, si). If a semantic example is Γ(ˆ`, ˆd, ˆt|si; θ) − |max ˆ`, ˆd,ˆt max `*,d* Osem(θ) = |max Γ(ˆ`, ˆd,ˆt|S; θ) − max 0&apos;ˆt �*,d*,t* (Ψ(y(e1,e2),`∗, d∗,t∗) + Γ(`∗, d∗, t∗|S; θ))|+ Γ(`, d, t|si; θt) 1192 Labeled Dependencies Unlabeled Dependencies Coverage P R F P R F ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63 ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63 C&amp;C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63 (Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83 Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies. sampled, we instead apply the following update: F(`, d, t|S(e1,e2); Bt) F(`, d, t|S(e1,e2); Bt) + Ψ(y(e1,e2), `, d, t) Bt+1 ← Bt + 0(d*, t*, S(e1,e2)) − 0( ˆd,ˆt, S(e1,e2)) This update moves the parameters toward the features of the best set of parses that satisfy the distant supervision constraint. Training outputs the average of each iteration’s parameters, B¯ = 1 Etn Bt. n </context>
<context position="29985" citStr="Hockenmaier, 2003" startWordPosition="4798" endWordPosition="4799">ich are in CCGbank, recall is the fraction of CCGbank dependencies produced by the parser, and F-measure is the harmonic mean of precision and recall. For comparison, we also trained a syntactic version of our parser, ASP-SYN, using only the CCGbank lexicon and grammar. Comparing against this parser lets us measure the effect of the relation extraction task on syntactic parsing. Table 2 shows the results of our evaluation. For comparison, we include results for two existing syntactic CCG parsers: C&amp;C, the current state-of-the-art CCG parser (Clark and Curran, 2007b), and the next best system (Hockenmaier, 2003a). Both ASP and ASP-SYN perform reasonably well, within 2.5% of the performance of C&amp;C at the same coverage level. However, ASP1.0 0.8 0.6 ASP PIPELINE K&amp;M-2012 0 0.4 0.2 1194 Logical Form Extraction Extraction Accuracy Precision Recall ASP 0.28 0.90 0.32 K&amp;M-2012 0.14 1.00 0.06 PIPELINE 0.2 0.63 0.17 Table 3: Logical form accuracy and extraction precision/recall on the annotated test set. The high extraction recall for ASP shows that it produces more complete logical forms than either baseline. SYN outperforms ASP by around 0.5%, suggesting that ASP’s additional semantic knowledge slightly h</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003a. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics -</booktitle>
<volume>1</volume>
<contexts>
<context position="5013" citStr="Hockenmaier, 2003" startWordPosition="746" endWordPosition="748">ate-ofthe-art; however, we also find that incorporating semantic information reduces syntactic parsing accuracy by ∼ 0.5%. 2 Prior Work This paper combines two lines of prior work: broad coverage syntactic parsing with CCG and semantic parsing. Broad coverage syntactic parsing with CCG has produced both resources and successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong </context>
<context position="21955" citStr="Hockenmaier, 2003" startWordPosition="3542" endWordPosition="3543">`,d θt+1 ← θt + φ(d∗, ti, si) − φ(ˆd, ˆt, si) This update moves the parameters toward the features of the best parse with the correct syntactic derivation, φ(d∗, ti, si). If a semantic example is Γ(ˆ`, ˆd, ˆt|si; θ) − |max ˆ`, ˆd,ˆt max `*,d* Osem(θ) = |max Γ(ˆ`, ˆd,ˆt|S; θ) − max 0&apos;ˆt �*,d*,t* (Ψ(y(e1,e2),`∗, d∗,t∗) + Γ(`∗, d∗, t∗|S; θ))|+ Γ(`, d, t|si; θt) 1192 Labeled Dependencies Unlabeled Dependencies Coverage P R F P R F ASP 85.58 85.31 85.44 91.75 91.46 91.60 99.63 ASP-SYN 86.06 85.84 85.95 92.13 91.89 92.01 99.63 C&amp;C (Clark and Curran, 2007b) 88.34 86.96 87.64 93.74 92.28 93.00 99.63 (Hockenmaier, 2003a) 84.3 84.6 84.4 91.8 92.2 92.0 99.83 Table 2: Syntactic parsing results for Section 23 of CCGbank. Parser performance is measured using precision (P), recall (R) and F-measure (F) of labeled and unlabeled dependencies. sampled, we instead apply the following update: F(`, d, t|S(e1,e2); Bt) F(`, d, t|S(e1,e2); Bt) + Ψ(y(e1,e2), `, d, t) Bt+1 ← Bt + 0(d*, t*, S(e1,e2)) − 0( ˆd,ˆt, S(e1,e2)) This update moves the parameters toward the features of the best set of parses that satisfy the distant supervision constraint. Training outputs the average of each iteration’s parameters, B¯ = 1 Etn Bt. n </context>
<context position="29985" citStr="Hockenmaier, 2003" startWordPosition="4798" endWordPosition="4799">ich are in CCGbank, recall is the fraction of CCGbank dependencies produced by the parser, and F-measure is the harmonic mean of precision and recall. For comparison, we also trained a syntactic version of our parser, ASP-SYN, using only the CCGbank lexicon and grammar. Comparing against this parser lets us measure the effect of the relation extraction task on syntactic parsing. Table 2 shows the results of our evaluation. For comparison, we include results for two existing syntactic CCG parsers: C&amp;C, the current state-of-the-art CCG parser (Clark and Curran, 2007b), and the next best system (Hockenmaier, 2003a). Both ASP and ASP-SYN perform reasonably well, within 2.5% of the performance of C&amp;C at the same coverage level. However, ASP1.0 0.8 0.6 ASP PIPELINE K&amp;M-2012 0 0.4 0.2 1194 Logical Form Extraction Extraction Accuracy Precision Recall ASP 0.28 0.90 0.32 K&amp;M-2012 0.14 1.00 0.06 PIPELINE 0.2 0.63 0.17 Table 3: Logical form accuracy and extraction precision/recall on the annotated test set. The high extraction recall for ASP shows that it produces more complete logical forms than either baseline. SYN outperforms ASP by around 0.5%, suggesting that ASP’s additional semantic knowledge slightly h</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003b. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke S Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of</booktitle>
<contexts>
<context position="19777" citStr="Hoffmann et al., 2011" startWordPosition="3169" endWordPosition="3172">actic tree equals the true syntactic tree ti. In the above equation |· |+ denotes the positive part of the expression. Minimizing this objective therefore finds parameters θ that reproduce the annotated syntactic trees. 4.2 Semantic Objective The semantic objective corresponds to a distantlysupervised relation extraction task that constrains the logical forms produced by the semantic parser. Distant supervision is provided by the following constraint: every relation instance r(e1, e2) ∈ K must be expressed by at least one sentence in S(e1,e2), the set of sentences that mention both e1 and e2 (Hoffmann et al., 2011). If this constraint is empirically true and sufficiently constrains the parser’s logical forms, then optimizing the semantic objective produces an accurate semantic parser. A training example in the semantic objective consists of the set of sentences mentioning a pair of entities, S(e1,e2) = {s1,s2, ...}, paired with a binary vector representing the set of relations that the two entities participate in, y(e1,e2). The distant supervision constraint Ψ forces the logical forms predicted for the sentences to entail the relations in y(e1,e2). Ψ is a deterministic OR constraint that checks whether </context>
<context position="23183" citStr="Hoffmann et al., (2011)" startWordPosition="3748" endWordPosition="3751"> practice, we train the parser by performing a single pass over the examples in the data set. All of the maximizations above can be performed exactly using a CKY-style chart parsing algorithm, except for the last one. This maximization is intractable due to the coupling between logical forms in ` caused by enforcing the distant supervision constraint. We approximate this maximization in two steps. First, we perform a beam search to produce a list of candidate parses for each sentence s E S(e1,e2). We then extract relation instances from each parse and apply the greedy inference algorithm from Hoffmann et al., (2011) to identify the best set of parses that satisfy the distant supervision constraint. The procedure skips any examples with sentences that cannot be parsed (due to beam search failures) or where the distant supervision constraint cannot be satisfied. 5 Experiments The experiments below evaluate ASP’s syntactic and semantic parsing ability. The parser is trained on CCGbank and a corpus of Wikipedia sentences, using NELL’s predicate vocabulary. The syntactic analyses of the trained parser are evaluated against CCGbank, and its logical forms are evaluated on an information extraction task and agai</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S. Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.</booktitle>
<contexts>
<context position="5692" citStr="Kate and Mooney, 2006" startWordPosition="855" endWordPosition="858">C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a large</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="6512" citStr="Krishnamurthy and Mitchell (2012)" startWordPosition="988" endWordPosition="992">laxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semantically analyzing conjunctions and relative clauses based on the syntactic annotation of these categories in CCGbank. This synergy gives our parser a richer semantic representation than previous work, while simultaneously enabling broad coverage. 3 Parser Design This section describes the Combinatory Categorial Grammar (CCG) par</context>
<context position="17197" citStr="Krishnamurthy and Mitchell (2012)" startWordPosition="2692" endWordPosition="2695"> K (e.g., NELL), containing relation instances r(e1, e2) ∈ K. 4. A CCG lexicon A (see Section 5.2). Given these resources, the algorithm described in this section produces parameters θ for a semantic parser. Our parameter estimation procedure constructs a joint objective function O(θ) that decomposes into syntactic and semantic components: O(θ) = Osyn(θ) + Osem(θ). The syntactic component Osyn is a standard syntactic parsing objective constructed using the syntactic resource L. The semantic component Osem is a distantly-supervised relation extraction task based on the semantic constraint from Krishnamurthy and Mitchell (2012). These components are described in more detail in the following sections. 1191 Lexicon features: word, POS := X : t : f Dependency Features: &lt; hw, hp, ht, hi, s, n, aw, ap, at, ai &gt; Word/syntactic category word, X Predicate-Argument Indicator &lt; hw, —, ht, —, s, n, aw, —, at, — &gt; POS/syntactic category POS, X Word-Word Indicator &lt; hw, —, —, —, s, n, aw, —, —, — &gt; Word semantics word, X, t Predicate-POS Indicator &lt; hw, —, ht, —, s, n, —, ap, —, — &gt; Combinator features: X Y Y Z or X Y Z Word-POS Indicator &lt; hw, —, —, —, s, n, —, ap, —, — &gt; Binary combinator indicator X Y Y Z POS-Argument Indicat</context>
<context position="31951" citStr="Krishnamurthy and Mitchell (2012)" startWordPosition="5100" endWordPosition="5103">also trained two baseline models. The first baseline, PIPELINE, is a pipelined syntax-then-semantics approach designed to mimic Boxer (Bos, 2005). This baseline first syntactically parses each sentence using ASP-SYN, then produces a semantic analysis by assigning a logical form to each word. We train this baseline using the semantic objective (Section 4.2) while holding fixed the syntactic parse of each sentence. Note that, unlike Boxer, this baseline learns which logical form to assign each word, and its logical forms contain NELL predicates. The second baseline, K&amp;M-2012, is the approach of Krishnamurthy and Mitchell (2012), representing the state-of-the-art in distantlysupervised semantic parsing. This approach trains a semantic parser by combining distant semantic supervision with syntactic supervision from dependency parses. The best performing variant of this system also uses dependency parses at test time to constrain the interpretation of test sentences – hence, this system also uses a pipelined syntax-then-semantics approach. To improve comparability, we reimplemented this approach using our parsing model, which has richer features than were used in their paper. 5.5.2 Information Extraction Evaluation The</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M. Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5750" citStr="Kwiatkowski et al., 2011" startWordPosition="863" endWordPosition="866">a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of the</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6232" citStr="Kwiatkowski et al., 2013" startWordPosition="945" endWordPosition="948">Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for example, our parser is capable of semantically analyzing conjunctions and relative clause</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined distributional and logical semantics.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--179</pages>
<contexts>
<context position="5340" citStr="Lewis and Steedman, 2013" startWordPosition="796" endWordPosition="799">successful parsers. These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 20</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined distributional and logical semantics. Transactions of the Association for Computational Linguistics, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="5963" citStr="Liang et al., 2011" startWordPosition="900" endWordPosition="903">wever, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the Association for Computational Linguistics, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5669" citStr="Lu et al., 2008" startWordPosition="851" endWordPosition="854">osely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan D Ratliff</author>
<author>J Andrew Bagnell</author>
<author>Martin A Zinkevich</author>
</authors>
<title>(online) subgradient methods for structured prediction. Artificial Intelligence and Statistics.</title>
<date>2006</date>
<contexts>
<context position="21008" citStr="Ratliff et al., 2006" startWordPosition="3370" endWordPosition="3373">al form entails the relation instance r(e1, e2), deterministically setting yr = 1 if any logical form entails the instance and yr = 0 otherwise. Let (`, d, t) represent a collection of semantic parses for the sentences S = S(e1,e2). Let Γ(`, d, t|S; θ) = E|S| i=1 Γ(`i, di, ti|si; θ) represent the total weight assigned by the parser to a collection of parses for the sentences S. For the pair of entities (e1, e2), the semantic objective is: 4.3 Optimization Training minimizes the joint objective using the structured perceptron algorithm, which can be viewed as the stochastic subgradient method (Ratliff et al., 2006) applied to the objective O(θ). We initialize the parameters to zero, i.e., θ0 = 0. On each iteration, we sample either a syntactic example (si, ti) or a semantic example (S(e1,e2),y(e1,e2)). If a syntactic example is sampled, we apply the following parameter update: ˆ`, ˆd, tˆ ←arg max `,d,t `∗, d∗ ← arg max Γ(`, d, ti|si; θt) `,d θt+1 ← θt + φ(d∗, ti, si) − φ(ˆd, ˆt, si) This update moves the parameters toward the features of the best parse with the correct syntactic derivation, φ(d∗, ti, si). If a semantic example is Γ(ˆ`, ˆd, ˆt|si; θ) − |max ˆ`, ˆd,ˆt max `*,d* Osem(θ) = |max Γ(ˆ`, ˆd,ˆt|</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2006</marker>
<rawString>Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. 2006. (online) subgradient methods for structured prediction. Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="8394" citStr="Steedman, 1996" startWordPosition="1275" endWordPosition="1276"> the parsing model. The knowledge base is assumed to have two kinds of ontological structure: a generalization/subsumption hierarchy and argument type constraints. This paper uses NELL’s ontology (Carlson et al., 2010), which, for example, specifies that the category ORGANIZATION is a generalization of SPORTSTEAM, and that both arguments to the LOCATEDIN relation must have type LOCATION. These type constraints are enforced during parsing. Throughout this paper, predicate names are shown in SMALLCAPS. 3.2 Syntax ASP uses a lexicalized and semanticallytyped Combinatory Categorial Grammar (CCG) (Steedman, 1996). Most grammatical information in CCG is encoded in a lexicon A, containing entries such as: 1189 area / NN that / WDT includes / VBZ beautiful / JJ London / NNP N λx.LOCATION(x) (N1\N1)/(S[dcl]\NP1)2 λf.λg.λz.g(z) ∧ f(λy.y = z) (S[dcl]\NP1)/NP2 λf.λg.∃x, y.g(x) ∧ f(y) ∧ LOCATEDIN(y, x) N1/N1 N λf.f λx.M(x, “london”, CITY) N : λx.M(x, “london”, CITY) (S[dcl]\NP1) : λg.∃x, y.g(x) ∧ M(y, “london”, CITY) ∧ LOCATEDIN(y, x) N1\N1 : λg.λz.∃x, y.g(z) ∧ x = z ∧ M(y, “london”, CITY) ∧ LOCATEDIN(y, x) N : λz.∃x, y.LOCATION(z) ∧ x = z ∧ M(y, “london”, CITY) ∧ LOCATEDIN(y, x) word Head index syntactic cat</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Mark Steedman. 1996. Surface Structure and Interpretation. The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL.</booktitle>
<contexts>
<context position="5629" citStr="Wong and Mooney, 2006" startWordPosition="843" endWordPosition="846"> 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5652" citStr="Wong and Mooney, 2007" startWordPosition="847" endWordPosition="850">del in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 201</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural language questions for the web of data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="6149" citStr="Yahya et al., 2012" startWordPosition="931" endWordPosition="934">mple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predicates than we do, none of these systems are capable of parsing open-domain text. Our approach is most closely related to the distantly-supervised approach of Krishnamurthy and Mitchell (2012). The parser presented in this paper can be viewed as a combination of both a broad coverage syntactic parser and a semantic parser trained using distant supervision. Combining these two lines of work has synergistic effects – for exampl</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural language questions for the web of data. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the thirteenth national conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5585" citStr="Zelle and Mooney, 1996" startWordPosition="835" endWordPosition="838">Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). The parsing model in this paper is loosely based on C&amp;C (Clark and Curran, 2007b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b;</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the thirteenth national conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="5723" citStr="Zettlemoyer and Collins, 2005" startWordPosition="859" endWordPosition="862">07b; Clark and Curran, 2007a), a discriminative loglinear model for statistical parsing. Some work has also attempted to automatically derive logical meaning representations directly from syntactic CCG parses (Bos, 2005; Lewis and Steedman, 2013). However, these approaches to semantics do not ground the text to beliefs in a knowledge base. Meanwhile, work on semantic parsing has focused on producing semantic parsers for answering simple natural language questions (Zelle and Mooney, 1996; Ge and Mooney, 2005; Wong and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011). This line of work has typically used a corpus of sentences with annotated logical forms to train the parser. Recent work has relaxed the requisite supervision conditions (Clarke et al., 2010; Liang et al., 2011), but has still focused on simple questions. Finally, some work has looked at applying semantic parsing to answer queries against large knowledge bases, such as YAGO (Yahya et al., 2012) and Freebase (Cai and Yates, 2013b; Cai and Yates, 2013a; Kwiatkowski et al., 2013; Berant et al., 2013). Although this work considers a larger number (thousands) of predica</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: structured classification with probabilistic categorial grammars. In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>