<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027687">
<title confidence="0.9951315">
New Parameterizations and Features for PSCFG-Based Machine
Translation
</title>
<author confidence="0.996551">
Andreas Zollmann Stephan Vogel
</author>
<affiliation confidence="0.986041">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.998545">
{zollmann,vogel}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975656">
We propose several improvements to the
hierarchical phrase-based MT model of
Chiang (2005) and its syntax-based exten-
sion by Zollmann and Venugopal (2006).
We add a source-span variance model
that, for each rule utilized in a prob-
abilistic synchronous context-free gram-
mar (PSCFG) derivation, gives a confi-
dence estimate in the rule based on the
number of source words spanned by the
rule and its substituted child rules, with
the distributions of these source span sizes
estimated during training time.
We further propose different methods of
combining hierarchical and syntax-based
PSCFG models, by merging the grammars
as well as by interpolating the translation
models.
Finally, we compare syntax-augmented
MT, which extracts rules based on target-
side syntax, to a corresponding variant
based on source-side syntax, and experi-
ment with a model extension that jointly
takes source and target syntax into ac-
count.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991376744186">
The Probabilistic Synchronous Context Free
Grammar (PSCFG) formalism suggests an intu-
itive approach to model the long-distance and lex-
ically sensitive reordering phenomena that often
occur across language pairs considered for statis-
tical machine translation. As in monolingual pars-
ing, nonterminal symbols in translation rules are
used to generalize beyond purely lexical opera-
tions. Labels on these nonterminal symbols are
often used to enforce syntactic constraints in the
generation of bilingual sentences and imply con-
ditional independence assumptions in the statis-
tical translation model. Several techniques have
been recently proposed to automatically iden-
tify and estimate parameters for PSCFGs (or re-
lated synchronous grammars) from parallel cor-
pora (Galley et al., 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al., 2006; Marcu et
al., 2006).
In this work, we propose several improvements
to the hierarchical phrase-based MT model of
Chiang (2005) and its syntax-based extension by
Zollmann and Venugopal (2006). We add a source
span variance model that, for each rule utilized
in a probabilistic synchronous context-free gram-
mar (PSCFG) derivation, gives a confidence es-
timate in the rule based on the number of source
words spanned by the rule and its substituted child
rules, with the distributions of these source span
sizes estimated during training (i.e., rule extrac-
tion) time.
We further propose different methods of com-
bining hierarchical and syntax-based PSCFG
models, by merging the grammars as well as by
interpolating the translation models.
Finally, we compare syntax-augmented MT,
which extracts rules based on target-side syntax,
to a corresponding variant based on source-side
syntax, and experiment with a model extension
based on source and target syntax.
We evaluate the different models on the
NIST large resource Chinese-to-English transla-
tion task.
</bodyText>
<page confidence="0.976298">
110
</page>
<note confidence="0.9831775">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 110–117,
COLING 2010, Beijing, August 2010.
</note>
<sectionHeader confidence="0.997267" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999969444444445">
Chiang et al. (2008) introduce structural dis-
tortion features into a hierarchical phrase-based
model, aimed at modeling nonterminal reordering
given source span length, by estimating for each
possible source span length E a Bernoulli distribu-
tion p(R|E) where R takes value one if reorder-
ing takes place and zero otherwise. Maximum-
likelihood estimation of the distribution amounts
to simply counting the relative frequency of non-
terminal reorderings over all extracted rule in-
stances that incurred a substitution of span length
E. In a more fine-grained approach they add a sep-
arate binary feature (R, E) for each combination of
reordering truth value R and span length E (where
all E &gt; 10 are merged into a single value), and
then tune the feature weights discriminatively on a
development set. Our approach differs from Chi-
ang et al. (2008) in that we estimate one source
span length distribution for each substitution site
of each grammar rule, resulting in unique distri-
butions for each rule, estimated from all instances
of the rule in the training data. This enables our
model to condition reordering range on the in-
dividual rules used in a derivation, and even al-
lows to distinguish between two rules r1 and r2
that both reorder arguments with identical mean
span lengths E, but where the span lengths encoun-
tered in extracted instances of r1 are all close to E,
whereas span length instances for r2 vary widely.
Chen and Eisele (2010) propose a hypbrid ap-
proach between hierarchical phrase based MT
and a rule based MT system, reporting improve-
ment over each individual model on an English-
to-German translation task. Essentially, the rule
based system is converted to a single-nonterminal
PSCFG, and hence can be combined with the
hierarchical model, another single-nonterminal
PSCFG, by taking the union of the rule sets
and augmenting the feature vectors, adding zero-
values for rules that only exist in one of the two
grammars. We face the challenge of combining
the single-nonterminal hierarchical grammar with
a multi-nonterminal syntax-augmented grammar.
Thus one hierarchical rule typically corresponds
to many syntax-augmented rules. The SAMT sys-
tem used by Zollmann et al. (2008) adds hierar-
chical rules separately to the syntax-augmented
grammar, resulting in a backbone grammar of
well-estimated hierarchical rules supporting the
sparser syntactic rules. They allow the model
preference between hierarchical and syntax rules
to be learned from development data by adding
an indicator feature to all rules, which is one
for hierarchical rules and zero for syntax rules.
However, no empirical comparison is given be-
tween the purely syntax-augmented and the hy-
brid grammar. We aim to fill this gap by experi-
menting with both models, and further refine the
hybrid approach by adding interpolated probabil-
ity models to the syntax rules.
Chiang (2010) augments a hierarchical phrase-
based MT model with binary syntax features rep-
resenting the source and target syntactic con-
stituents of a given rule’s instantiations during
training, thus taking source and target syntax
into account while avoiding the data-sparseness
and decoding-complexity problems of multi-
nonterminal PSCFG models. In our approach, the
source- and target-side syntax directly determines
the grammar, resulting in a nonterminal set de-
rived from the labels underlying the source- and
target-language treebanks.
</bodyText>
<sectionHeader confidence="0.995534" genericHeader="method">
3 PSCFG-based translation
</sectionHeader>
<bodyText confidence="0.99978125">
Given a source language sentence f, statistical
machine translation defines the translation task as
selecting the most likely target translation e under
a model P(e|f), i.e.:
</bodyText>
<equation confidence="0.89799">
=
m
ˆe(f) arg max P(e|f) = argmax hi(e,f)Ai
e e i=1
</equation>
<bodyText confidence="0.999899583333333">
where the arg max operation denotes a search
through a structured space of translation outputs
in the target language, hi(e, f) are bilingual fea-
tures of e and f and monolingual features of
e, and weights Ai are typically trained discrim-
inatively to maximize translation quality (based
on automatic metrics) on held out data, e.g., us-
ing minimum-error-rate training (MERT) (Och,
2003).
In PSCFG-based systems, the search space is
structured by automatically extracted rules that
model both translation and re-ordering operations.
</bodyText>
<page confidence="0.993727">
111
</page>
<bodyText confidence="0.998480823529412">
Most large scale systems approximate the search
above by simply searching for the most likely
derivation of rules, rather than searching for the
most likely translated output. There are efficient
algorithms to perform this search (Kasami, 1965;
Chappelier and Rajman, 1998) that have been ex-
tended to efficiently integrate n-gram language
model features (Chiang, 2007; Venugopal et al.,
2007; Huang and Chiang, 2007; Zollmann et al.,
2008; Petrov et al., 2008).
In this work we experiment with PSCFGs
that have been automatically learned from word-
aligned parallel corpora. PSCFGs are defined by a
source terminal set (source vocabulary) TS, a tar-
get terminal set (target vocabulary) TT, a shared
nonterminal set N and rules of the form: X →
(ry, α, w) where
</bodyText>
<listItem confidence="0.9499645">
• X E N is a labeled nonterminal referred to as
the left-hand-side of the rule.
• ry E (N U TS)* is the source side of the rule.
• α E (N U TT)* is the target side of the rule.
• w E [0, oc) is a non-negative real-valued
weight assigned to the rule; in our model, w is
the exponential function of the inner product of
features h and weights A.
</listItem>
<subsectionHeader confidence="0.99922">
3.1 Hierarchical phrase-based MT
</subsectionHeader>
<bodyText confidence="0.996854916666666">
Building upon the success of phrase-based meth-
ods, Chiang (2005) presents a PSCFG model of
translation that uses the bilingual phrase pairs
of phrase-based MT as starting point to learn
hierarchical rules. For each training sentence
pair’s set of extracted phrase pairs, the set of in-
duced PSCFG rules can be generated as follows:
First, each phrase pair is assigned a generic X-
nonterminal as left-hand-side, making it an initial
rule. We can now recursively generalize each al-
ready obtained rule (initial or including nontermi-
nals)
</bodyText>
<equation confidence="0.946759">
N → f1 ... fm/e1 ... en
</equation>
<bodyText confidence="0.882174">
for which there is an initial rule
</bodyText>
<equation confidence="0.95027">
M → fi ... fu/ej ... ev
</equation>
<bodyText confidence="0.9998235">
where 1 &lt; i &lt; u &lt; m and 1 &lt; j &lt; v &lt; n, to
obtain a new rule
</bodyText>
<equation confidence="0.815703">
fi−11Xkf1/e1 1
Xk vN →+1
</equation>
<bodyText confidence="0.99960647368421">
where e.g. fi−1
1 is short-hand for f1 ... fi−1, and
where k is an index for the nonterminal X that
indicates the one-to-one correspondence between
the new X tokens on the two sides (it is not in
the space of word indices like i, j, u, v, m, n). The
recursive form of this generalization operation al-
lows the generation of rules with multiple nonter-
minal pairs.
Chiang (2005) uses features analogous to the
ones used in phrase-based translation: a lan-
guage model neg-log probability, a ‘rule given
source-side’ neg-log-probability, a ‘rule given
target-side’ neg-log-probability, source- and tar-
get conditioned ‘lexical’ neg-log-probabilities
based on word-to-word co-occurrences (Koehn et
al., 2003), as well as rule, target word, and glue
operation counters. We follow Venugopal and
Zollmann (2009) to further add a rareness penalty,
</bodyText>
<equation confidence="0.82002">
1/ count(r)
</equation>
<bodyText confidence="0.999959">
where count(r) is the occurrence count of rule
r in the training corpus, allowing the system to
learn penalization of low-frequency rules, as well
as three indicator features firing if the rule has
one, two unswapped, and two swapped nontermi-
nal pairs, respectively.1
</bodyText>
<subsectionHeader confidence="0.999579">
3.2 Syntax Augmented MT
</subsectionHeader>
<bodyText confidence="0.999925470588235">
Syntax Augmented MT (SAMT) (Zollmann and
Venugopal, 2006) extends Chiang (2005) to in-
clude nonterminal symbols from target language
phrase structure parse trees. Each target sentence
in the training corpus is parsed with a stochas-
tic parser to produce constituent labels for target
spans. Phrase pairs (extracted from a particular
sentence pair) are assigned left-hand-side nonter-
minal symbols based on the target side parse tree
constituent spans.
Phrase pairs whose target side corresponds to
a constituent span are assigned that constituent’s
label as their left-hand-side nonterminal. If the
target side of the phrase pair is not spanned by
a single constituent in the corresponding parse
tree, we use the labels of subsuming, subsumed,
and neighboring parse tree constituents to assign
</bodyText>
<footnote confidence="0.870079666666667">
1Penalization or reward of purely-lexical rules can be in-
directly learned by trading off these features with the rule
counter feature.
</footnote>
<page confidence="0.994611">
112
</page>
<bodyText confidence="0.9999809">
an extended label of the form C1 + C2, C1/C2,
or C2 C1 (the latter two being motivated from
the operations in combinatory categorial gram-
mar (CCG) (Steedman, 2000)), indicating that the
phrase pair’s target side spans two adjacent syn-
tactic categories (e.g., she went: NP+VB), a par-
tial syntactic category C1 missing a C2 at the right
(e.g., the great: NP/NN), or a partial C1 missing
a C2 at the left (e.g., great wall: WZ NP), respec-
tively. The label assignment is attempted in the or-
der just described, i.e., assembling labels based on
‘+’ concatenation of two subsumed constituents is
preferred, as smaller constituents tend to be more
accurately labeled. If no label is assignable by ei-
ther of these three methods, a default label ‘FAIL’
is assigned.
In addition to the features used in hierarchical
phrase-based MT, SAMT introduces a relative-
frequency estimated probability of the rule given
its left-hand-side nonterminal.
</bodyText>
<sectionHeader confidence="0.9693995" genericHeader="method">
4 Modeling Source Span Length of
PSCFG Rule Substitution Sites
</sectionHeader>
<bodyText confidence="0.999936083333333">
Extracting a rule with k right-hand-side nonter-
minal pairs, i.e., substitution sites, (from now on
called order-k rule) by the method described in
Section 3 involves k + 1 phrase pairs: one phrase
pair used as initial rule and k phrase pairs that are
sub phrase pairs of the first and replaced by non-
terminal pairs. Conversely, during translation, ap-
plying this rule amounts to combining k hypothe-
ses from k different chart cells, each represented
by a source span and a nonterminal, to form a new
hypothesis and file it into a chart cell. Intuitively,
we want the source span lengths of these k + 1
chart cells to be close to the source side lengths of
the k+1 phrase pairs from the training corpus that
were involved in extracting the rule. Of course,
each rule generally was extracted from multiple
training corpus locations, with different involved
phrase pairs of different lengths. We therefore
model k + 1 source span length distributions for
each order-k rule in the grammar.
Ignoring the discreteness of source span length
for the sake of easier estimation, we assume the
distribution to be log-normal. This is motivated
by the fact that source span length is positive and
that we expect its deviation between instances of
the same rule to be greater for long phrase pairs
than for short ones.
We can now add kˆ + 1 features to the transla-
tion framework, where kˆ is the maximum num-
ber of PSCFG rule nonterminal pairs, in our case
two. Each feature is computed during translation
time. Ideally, it should represent the probabil-
ity of the hypothesized rule given the respective
chart cell span length. However, as each com-
peting rule underlies a different distribution, this
would require a Bayesian setting, in which priors
over distributions are specified. In this prelimi-
nary work we take a simpler approach: Based on
the rule’s span distribution, we compute the prob-
ability that a span length no likelier than the one
encountered was generated from the distribution.
This probability thus yields a confidence estimate
for the rule. More formally, let µ be the mean and
u the standard deviation of the logarithm of the
span length random variable X concerned, and let
x be the span length encountered during decoding.
Then the computed confidence estimate is given
by
</bodyText>
<equation confidence="0.9705305">
P ( |ln(X) − µ |?  |ln(x) − µ|)
= 2 * Z (−( |ln(x) − µ|)/u)
</equation>
<bodyText confidence="0.999944176470588">
where Z is the cumulative density function of the
normal distribution with mean zero and variance
one.
The confidence estimate is one if the encoun-
tered span length is equal to the mean of the dis-
tribution, and decreases as the encountered span
length deviates further from the mean. The sever-
ity of that decline is determined by the distribution
variance: the higher the variance, the less a devia-
tion from the mean is penalized.
Mean and variance of log source span length are
sufficient statistics of the log-normal distribution.
As we extract rules in a distributed fashion, we
use a straightforward parallelization of the online
algorithm of Welford (1962) and its improvement
by West (1979) to compute the sample variance
over all instances of a rule.
</bodyText>
<page confidence="0.999139">
113
</page>
<sectionHeader confidence="0.6533975" genericHeader="method">
5 Merging a Hierarchical and a
Syntax-Based Model
</sectionHeader>
<bodyText confidence="0.99999406122449">
While syntax-based grammars allow for more re-
fined statistical models and guide the search by
constraining substitution possibilitites in a gram-
mar derivation, grammar sizes tend to be much
greater than for hierarchical grammars. Therefore
the average occurrence count of a syntax rule is
much lower than that of a hierarchical rule, and
thus estimated probabilitites are less reliable.
We propose to augment the syntax-based “rule
given source side” and “rule given target side” dis-
tributions by hierarchical counterparts obtained by
marginalizing over the left-hand-side and right-
hand-side rule nonterminals. For example, the
hierarchical equivalent of the “rule given source
side” probability is obtained by summing occur-
rence counts over all rules that have the same
source and target terminals and substitution posi-
tions but possibly differ in the left- and/or right-
hand side nonterminal labels, divided by the sum
of occurrence counts of all rules that have the
same source side terminals and source side substi-
tution positions. Similarly, an alternative rareness
penalty based on the combined frequency of all
rules with the same terminals and substitution po-
sitions is obtained.
Using these syntax and hierarchical features
side by side amounts to interpolation of the re-
spective probability models in log-space, with
minimum-error-rate training (MERT) determining
the optimal interpolation coefficient. We also add
respective models interpolated with coefficient .5
in probability-space as additional features to the
system.
We further experiment with adding hierarchical
rules separately to the syntax-augmented gram-
mar, as proposed in Zollmann et al. (2008), with
the respective syntax-specific features set to zero.
A ‘hierarchical-indicator’ feature is added to all
rules, which is one for hierarchical rules and zero
for syntax rules, allowing the joint model to trade
of hierarchical against syntactic rules. During
translation, the hierarchical and syntax worlds are
bridged by glue rules, which allow monotonic
concatenation of hierarchical and syntactic partial
sentence hypotheses. We separate the glue feature
used in hierarchical and syntax-augmented trans-
lation into a glue feature that only fires when a hi-
erarchical rule is glued, and a distinct glue feature
firing when gluing a syntax-augmented rule.
</bodyText>
<note confidence="0.600653">
6 Extension of SAMT to a bilingually
parsed corpus
</note>
<bodyText confidence="0.99884545">
Syntax-based MT models have been proposed
both based on target-side syntactic annotations
(Galley et al., 2004; Zollmann and Venugopal,
2006) as well source-side annotations (Liu et al.,
2006). Syntactic annotations for both source and
target language are available for popular language
pairs such as Chinese-English. In this case, our
grammar extraction procedure can be easily ex-
tended to impose both source and target con-
straints on the eligible substitutions simultane-
ously.
Let Nf be the nonterminal label that would be
assigned to a given initial rule when utilizing the
source-side parse tree, and Ne the assigned label
according to the target-side parse. Then our bilin-
gual model assigns ‘Nf + Ne’ to the initial rule.
The extraction of complex rules proceeds as be-
fore. The number of nonterminals in this model,
based on a source-model label set of size s and a
target label set of size t, is thus given by st.
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999992722222222">
We evaluate our approaches by comparing trans-
lation quality according to the IBM-BLEU (Pap-
ineni et al., 2002) metric on the NIST Chinese-
to-English translation task using MT04 as devel-
opment set to train the model parameters A, and
MT05, MT06 and MT08 as test sets.
We perform PSCFG rule extraction and de-
coding using the open-source “SAMT” system
(Venugopal and Zollmann, 2009), using the pro-
vided implementations for the hierarchical and
syntax-augmented grammars. For all systems, we
use the bottom-up chart parsing decoder imple-
mented in the SAMT toolkit with a reordering
limit of 15 source words, and correspondingly ex-
tract rules from initial phrase pairs of maximum
source length 15. All rules have at most two non-
terminal symbols, which must be non-consecutive
on the source side, and rules must contain at least
</bodyText>
<page confidence="0.997193">
114
</page>
<bodyText confidence="0.999701820512821">
one source-side terminal symbol.
For parameter tuning, we use the Lo-
regularized minimum-error-rate training tool pro-
vided by the SAMT toolkit.
The parallel training data comprises of 9.6M
sentence pairs (206M Chinese Words, 228M En-
glish words). The source and target language
parses for the syntax-augmented grammar were
generated by the Stanford parser (Klein and Man-
ning, 2003).
The results are given in Table 1. The source
span models (indicated by +span) achieve small
test set improvements of 0.15 BLEU points on av-
erage for the hierarchical and 0.26 BLEU points
for the syntax-augmented system, but these are
not statistically significant.
Augmenting a syntax-augmented grammar
with hierarchical features (“Syntax+hiermodels”)
results in average test set improvements of 0.5
BLEU points. These improvements are not sta-
tistically significant either, but persist across all
three test sets. This demonstrates the benefit of
more reliable feature estimation. Further aug-
menting the hierarchical rules to the grammar
(“Syntax+hiermodels+hierrules”) does not yield
additional improvements.
The use of bilingual syntactic parses (‘Syn-
tax/src&amp;tgt’) turns out detrimental to translation
quality. We assume this is due to the huge number
of nonterminals in these grammars and the great
amount of badly-estimated low-occurrence-count
rules. Perhaps merging this grammar with a regu-
lar syntax-augmented grammar could yield better
results.
We also experimented with a source-parse
based model (‘Syntax/src’). While not being able
to match translation quality of its target-based
counterpart, the model still outperforms the hier-
archical system on all test sets.
</bodyText>
<sectionHeader confidence="0.98444" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998087">
We proposed several improvements to the hierar-
chical phrase-based MT model of Chiang (2005)
and its syntax-based extension by Zollmann and
Venugopal (2006). We added a source span length
model that, for each rule utilized in a probabilis-
tic synchronous context-free grammar (PSCFG)
derivation, gives a confidence estimate in the rule
based on the number of source words spanned by
the rule and its substituted child rules, resulting in
small improvements for hierarchical phrase-based
as well as syntax-augmented MT.
We further demonstrated the utility of combin-
ing hierarchical and syntax-based PSCFG models
and grammars.
Finally, we compared syntax-augmented MT,
which extracts rules based on target-side syntax,
to a corresponding variant based on source-side
syntax, showing that target syntax is more ben-
efitial, and unsuccessfully experimented with a
model extension that jointly takes source and tar-
get syntax into account.
Hierarchical phrase-based MT suffers from
spurious ambiguity: A single translation for a
given source sentence can usually be accom-
plished by many different PSCFG derivations.
This problem is exacerbated by syntax-augmented
MT with its thousands of nonterminals, and made
even worse by its joint source-and-target exten-
sion. Future research should apply the work of
Blunsom et al. (2008) and Blunsom and Osborne
(2008), who marginalize over derivations to find
the most probable translation rather than the most
probable derivation, to these multi-nonterminal
grammars.
All source code underlying this work is avail-
able under the GNU Lesser General Public Li-
cense as part of the ‘SAMT’ system at:
www.cs.cmu.edu/˜zollmann/samt
</bodyText>
<sectionHeader confidence="0.995211" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999935571428572">
This work is in part supported by NSF un-
der the Cluster Exploratory program (grant NSF
0844507), and in part by the US DARPA GALE
program. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of NSF or DARPA.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9644525">
Blunsom, Phil and Miles Osborne. 2008. Probabilistic
inference for machine translation. In EMNLP ’08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 215–
223, Morristown, NJ, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.99607">
115
</page>
<table confidence="0.999943888888889">
Dev (MT04) MT05 MT06 MT08 TestAvg Time
Hierarchical 38.63 36.51 33.26 25.77 31.85 14.3
Hier+span 39.03 36.44 33.29 26.26 32.00 16.7
Syntax 39.17 37.17 33.87 26.81 32.62 59
Syntax+hiermodels 39.61 37.74 34.30 27.30 33.11 68.4
Syntax+hiermodels+hierrules 39.69 37.56 34.66 26.93 33.05 34.6
Syntax+span+hiermodels+hierrules 39.81 38.02 34.50 27.41 33.31 39.6
Syntax/src+span+hiermodels+hierrules 39.62 37.25 33.99 26.44 32.56 20.1
Syntax/src&amp;tgt+span+hiermodels+hierrules 39.15 36.92 33.70 26.24 32.29 17.5
</table>
<tableCaption confidence="0.89469">
Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length) for
different systems on Chinese-English NIST-large translation tasks. ‘TestAvg’ shows the average score over the three test sets.
‘Time’ is the average decoding time per sentence in seconds on one CPU.
</tableCaption>
<reference confidence="0.999420933333333">
Blunsom, Phil, Trevor Cohn, and Miles Osborne.
2008. A discriminative latent variable model for
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Brown, Peter F., Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2).
Chappelier, J.C. and M. Rajman. 1998. A general-
ized CYK algorithm for parsing stochastic CFG. In
Proceedings of Tabulation in Parsing and Deduction
(TAPD), pages 133–137, Paris.
Chen, Yu and Andreas Eisele. 2010. Hierarchical hy-
brid translation between english and german. In
Hansen, Viggo and Francois Yvon, editors, Pro-
ceedings of the 14th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
90–97. EAMT, EAMT, 5.
Chiang, David, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 224–233, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Chiang, David. 2007. Hierarchical phrase based trans-
lation. Computational Linguistics, 33(2).
Chiang, David. 2010. Learning to translate with
source and target syntax. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1443–1452, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Galley, Michael, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics Confer-
ence (HLT/NAACL).
Huang, Liang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Kasami, T. 1965. An efficient recognition
and syntax-analysis algorithm for context-free lan-
guages. Technical report, Air Force Cambridge Re-
search Lab.
Klein, Dan and Christoper Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics.
Marcu, Daniel, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical ma-
chine translation with syntactified target language
phrases. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
</reference>
<page confidence="0.989663">
116
</page>
<reference confidence="0.99975588372093">
Och, Franz J. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Petrov, Slav, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
Steedman, Mark. 2000. The Syntactic Process. MIT
Press.
Venugopal, Ashish and Andreas Zollmann. 2009.
Grammar based statistical MT on Hadoop: An end-
to-end toolkit for large scale PSCFG based MT.
The Prague Bulletin of Mathematical Linguistics,
91:67–78.
Venugopal, Ashish, Andreas Zollmann, and Stephan
Vogel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Welford, B. P. 1962. Note on a method for calculating
corrected sums of squares and products. Techno-
metrics, 4(3):419–420.
West, D. H. D. 1979. Updating mean and variance
estimates: an improved method. Commun. ACM,
22(9):532–535.
Zollmann, Andreas and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proceedings of the Workshop on Sta-
tistical Machine Translation, HLT/NAACL.
Zollmann, Andreas, Ashish Venugopal, Franz J. Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of the Conference on
Computational Linguistics (COLING).
</reference>
<page confidence="0.998134">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.609250">
<title confidence="0.999313">New Parameterizations and Features for PSCFG-Based Machine Translation</title>
<author confidence="0.998152">Andreas Zollmann Stephan</author>
<affiliation confidence="0.954619">Language Technologies School of Computer Carnegie Mellon</affiliation>
<abstract confidence="0.988418192307692">We propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source-span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training time. We further propose different methods of combining hierarchical and syntax-based PSCFG models, by merging the grammars as well as by interpolating the translation models. Finally, we compare syntax-augmented MT, which extracts rules based on targetside syntax, to a corresponding variant based on source-side syntax, and experiment with a model extension that jointly takes source and target syntax into account.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Miles Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>215--223</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="22590" citStr="Blunsom and Osborne (2008)" startWordPosition="3583" endWordPosition="3586">responding variant based on source-side syntax, showing that target syntax is more benefitial, and unsuccessfully experimented with a model extension that jointly takes source and target syntax into account. Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of Blunsom et al. (2008) and Blunsom and Osborne (2008), who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi-nonterminal grammars. All source code underlying this work is available under the GNU Lesser General Public License as part of the ‘SAMT’ system at: www.cs.cmu.edu/˜zollmann/samt Acknowledgements This work is in part supported by NSF under the Cluster Exploratory program (grant NSF 0844507), and in part by the US DARPA GALE program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily r</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>Blunsom, Phil and Miles Osborne. 2008. Probabilistic inference for machine translation. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 215– 223, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="22559" citStr="Blunsom et al. (2008)" startWordPosition="3578" endWordPosition="3581">rget-side syntax, to a corresponding variant based on source-side syntax, showing that target syntax is more benefitial, and unsuccessfully experimented with a model extension that jointly takes source and target syntax into account. Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of Blunsom et al. (2008) and Blunsom and Osborne (2008), who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi-nonterminal grammars. All source code underlying this work is available under the GNU Lesser General Public License as part of the ‘SAMT’ system at: www.cs.cmu.edu/˜zollmann/samt Acknowledgements This work is in part supported by NSF under the Cluster Exploratory program (grant NSF 0844507), and in part by the US DARPA GALE program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the a</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Blunsom, Phil, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Chappelier</author>
<author>M Rajman</author>
</authors>
<title>A generalized CYK algorithm for parsing stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of Tabulation in Parsing and Deduction (TAPD),</booktitle>
<pages>133--137</pages>
<location>Paris.</location>
<contexts>
<context position="7659" citStr="Chappelier and Rajman, 1998" startWordPosition="1169" endWordPosition="1172">eatures of e, and weights Ai are typically trained discriminatively to maximize translation quality (based on automatic metrics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Chappelier, J.C. and M. Rajman. 1998. A generalized CYK algorithm for parsing stochastic CFG. In Proceedings of Tabulation in Parsing and Deduction (TAPD), pages 133–137, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Chen</author>
<author>Andreas Eisele</author>
</authors>
<title>Hierarchical hybrid translation between english and german.</title>
<date>2010</date>
<booktitle>Proceedings of the 14th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>90--97</pages>
<editor>In Hansen, Viggo and Francois Yvon, editors,</editor>
<contexts>
<context position="4650" citStr="Chen and Eisele (2010)" startWordPosition="712" endWordPosition="715">m Chiang et al. (2008) in that we estimate one source span length distribution for each substitution site of each grammar rule, resulting in unique distributions for each rule, estimated from all instances of the rule in the training data. This enables our model to condition reordering range on the individual rules used in a derivation, and even allows to distinguish between two rules r1 and r2 that both reorder arguments with identical mean span lengths E, but where the span lengths encountered in extracted instances of r1 are all close to E, whereas span length instances for r2 vary widely. Chen and Eisele (2010) propose a hypbrid approach between hierarchical phrase based MT and a rule based MT system, reporting improvement over each individual model on an Englishto-German translation task. Essentially, the rule based system is converted to a single-nonterminal PSCFG, and hence can be combined with the hierarchical model, another single-nonterminal PSCFG, by taking the union of the rule sets and augmenting the feature vectors, adding zerovalues for rules that only exist in one of the two grammars. We face the challenge of combining the single-nonterminal hierarchical grammar with a multi-nonterminal </context>
</contexts>
<marker>Chen, Eisele, 2010</marker>
<rawString>Chen, Yu and Andreas Eisele. 2010. Hierarchical hybrid translation between english and german. In Hansen, Viggo and Francois Yvon, editors, Proceedings of the 14th Annual Conference of the European Association for Machine Translation, pages 90–97. EAMT, EAMT, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="3228" citStr="Chiang et al. (2008)" startWordPosition="474" endWordPosition="477">ng hierarchical and syntax-based PSCFG models, by merging the grammars as well as by interpolating the translation models. Finally, we compare syntax-augmented MT, which extracts rules based on target-side syntax, to a corresponding variant based on source-side syntax, and experiment with a model extension based on source and target syntax. We evaluate the different models on the NIST large resource Chinese-to-English translation task. 110 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 110–117, COLING 2010, Beijing, August 2010. 2 Related work Chiang et al. (2008) introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length, by estimating for each possible source span length E a Bernoulli distribution p(R|E) where R takes value one if reordering takes place and zero otherwise. Maximumlikelihood estimation of the distribution amounts to simply counting the relative frequency of nonterminal reorderings over all extracted rule instances that incurred a substitution of span length E. In a more fine-grained approach they add a separate binary feature (R, E) for each combin</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>Chiang, David, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224–233, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1948" citStr="Chiang, 2005" startWordPosition="280" endWordPosition="281">en occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extra</context>
<context position="8585" citStr="Chiang (2005)" startWordPosition="1341" endWordPosition="1342">fined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a non-negative real-valued weight assigned to the rule; in our model, w is the exponential function of the inner product of features h and weights A. 3.1 Hierarchical phrase-based MT Building upon the success of phrase-based methods, Chiang (2005) presents a PSCFG model of translation that uses the bilingual phrase pairs of phrase-based MT as starting point to learn hierarchical rules. For each training sentence pair’s set of extracted phrase pairs, the set of induced PSCFG rules can be generated as follows: First, each phrase pair is assigned a generic Xnonterminal as left-hand-side, making it an initial rule. We can now recursively generalize each already obtained rule (initial or including nonterminals) N → f1 ... fm/e1 ... en for which there is an initial rule M → fi ... fu/ej ... ev where 1 &lt; i &lt; u &lt; m and 1 &lt; j &lt; v &lt; n, to obtain</context>
<context position="10441" citStr="Chiang (2005)" startWordPosition="1657" endWordPosition="1658">oned ‘lexical’ neg-log-probabilities based on word-to-word co-occurrences (Koehn et al., 2003), as well as rule, target word, and glue operation counters. We follow Venugopal and Zollmann (2009) to further add a rareness penalty, 1/ count(r) where count(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.1 3.2 Syntax Augmented MT Syntax Augmented MT (SAMT) (Zollmann and Venugopal, 2006) extends Chiang (2005) to include nonterminal symbols from target language phrase structure parse trees. Each target sentence in the training corpus is parsed with a stochastic parser to produce constituent labels for target spans. Phrase pairs (extracted from a particular sentence pair) are assigned left-hand-side nonterminal symbols based on the target side parse tree constituent spans. Phrase pairs whose target side corresponds to a constituent span are assigned that constituent’s label as their left-hand-side nonterminal. If the target side of the phrase pair is not spanned by a single constituent in the corres</context>
<context position="21332" citStr="Chiang (2005)" startWordPosition="3397" endWordPosition="3398"> turns out detrimental to translation quality. We assume this is due to the huge number of nonterminals in these grammars and the great amount of badly-estimated low-occurrence-count rules. Perhaps merging this grammar with a regular syntax-augmented grammar could yield better results. We also experimented with a source-parse based model (‘Syntax/src’). While not being able to match translation quality of its target-based counterpart, the model still outperforms the hierarchical system on all test sets. 8 Conclusion We proposed several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We added a source span length model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, resulting in small improvements for hierarchical phrase-based as well as syntax-augmented MT. We further demonstrated the utility of combining hierarchical and syntax-based PSCFG models and grammars. Finally, we compared syntax-augmented MT, which extracts rules based</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="7753" citStr="Chiang, 2007" startWordPosition="1185" endWordPosition="1186">automatic metrics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a non-negative </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, David. 2007. Hierarchical phrase based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<location>Uppsala,</location>
<contexts>
<context position="6071" citStr="Chiang (2010)" startWordPosition="932" endWordPosition="933">nted grammar, resulting in a backbone grammar of well-estimated hierarchical rules supporting the sparser syntactic rules. They allow the model preference between hierarchical and syntax rules to be learned from development data by adding an indicator feature to all rules, which is one for hierarchical rules and zero for syntax rules. However, no empirical comparison is given between the purely syntax-augmented and the hybrid grammar. We aim to fill this gap by experimenting with both models, and further refine the hybrid approach by adding interpolated probability models to the syntax rules. Chiang (2010) augments a hierarchical phrasebased MT model with binary syntax features representing the source and target syntactic constituents of a given rule’s instantiations during training, thus taking source and target syntax into account while avoiding the data-sparseness and decoding-complexity problems of multinonterminal PSCFG models. In our approach, the source- and target-side syntax directly determines the grammar, resulting in a nonterminal set derived from the labels underlying the source- and target-language treebanks. 3 PSCFG-based translation Given a source language sentence f, statistica</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>Chiang, David. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452, Uppsala,</rawString>
</citation>
<citation valid="false">
<authors>
<author>July Sweden</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>Sweden, </marker>
<rawString>Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context position="1934" citStr="Galley et al., 2004" startWordPosition="276" endWordPosition="279">ng phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.</context>
<context position="17923" citStr="Galley et al., 2004" startWordPosition="2863" endWordPosition="2866">odel to trade of hierarchical against syntactic rules. During translation, the hierarchical and syntax worlds are bridged by glue rules, which allow monotonic concatenation of hierarchical and syntactic partial sentence hypotheses. We separate the glue feature used in hierarchical and syntax-augmented translation into a glue feature that only fires when a hierarchical rule is glued, and a distinct glue feature firing when gluing a syntax-augmented rule. 6 Extension of SAMT to a bilingually parsed corpus Syntax-based MT models have been proposed both based on target-side syntactic annotations (Galley et al., 2004; Zollmann and Venugopal, 2006) as well source-side annotations (Liu et al., 2006). Syntactic annotations for both source and target language are available for popular language pairs such as Chinese-English. In this case, our grammar extraction procedure can be easily extended to impose both source and target constraints on the eligible substitutions simultaneously. Let Nf be the nonterminal label that would be assigned to a given initial rule when utilizing the source-side parse tree, and Ne the assigned label according to the target-side parse. Then our bilingual model assigns ‘Nf + Ne’ to t</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, Michael, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7801" citStr="Huang and Chiang, 2007" startWordPosition="1191" endWordPosition="1194">.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a non-negative real-valued weight assigned to the rule; in our </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Huang, Liang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax-analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical report,</tech>
<institution>Air Force Cambridge Research Lab.</institution>
<contexts>
<context position="7629" citStr="Kasami, 1965" startWordPosition="1167" endWordPosition="1168"> monolingual features of e, and weights Ai are typically trained discriminatively to maximize translation quality (based on automatic metrics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry </context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, T. 1965. An efficient recognition and syntax-analysis algorithm for context-free languages. Technical report, Air Force Cambridge Research Lab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christoper Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="19951" citStr="Klein and Manning, 2003" startWordPosition="3195" endWordPosition="3199">t of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least 114 one source-side terminal symbol. For parameter tuning, we use the Loregularized minimum-error-rate training tool provided by the SAMT toolkit. The parallel training data comprises of 9.6M sentence pairs (206M Chinese Words, 228M English words). The source and target language parses for the syntax-augmented grammar were generated by the Stanford parser (Klein and Manning, 2003). The results are given in Table 1. The source span models (indicated by +span) achieve small test set improvements of 0.15 BLEU points on average for the hierarchical and 0.26 BLEU points for the syntax-augmented system, but these are not statistically significant. Augmenting a syntax-augmented grammar with hierarchical features (“Syntax+hiermodels”) results in average test set improvements of 0.5 BLEU points. These improvements are not statistically significant either, but persist across all three test sets. This demonstrates the benefit of more reliable feature estimation. Further augmentin</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Christoper Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context position="9922" citStr="Koehn et al., 2003" startWordPosition="1573" endWordPosition="1576">nonterminal X that indicates the one-to-one correspondence between the new X tokens on the two sides (it is not in the space of word indices like i, j, u, v, m, n). The recursive form of this generalization operation allows the generation of rules with multiple nonterminal pairs. Chiang (2005) uses features analogous to the ones used in phrase-based translation: a language model neg-log probability, a ‘rule given source-side’ neg-log-probability, a ‘rule given target-side’ neg-log-probability, source- and target conditioned ‘lexical’ neg-log-probabilities based on word-to-word co-occurrences (Koehn et al., 2003), as well as rule, target word, and glue operation counters. We follow Venugopal and Zollmann (2009) to further add a rareness penalty, 1/ count(r) where count(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.1 3.2 Syntax Augmented MT Syntax Augmented MT (SAMT) (Zollmann and Venugopal, 2006) extends Chiang (2005) to include nonterminal symbols from target language phrase structure parse trees</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1996" citStr="Liu et al., 2006" startWordPosition="286" endWordPosition="289">r statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time. We further propose different method</context>
<context position="18005" citStr="Liu et al., 2006" startWordPosition="2875" endWordPosition="2878">rchical and syntax worlds are bridged by glue rules, which allow monotonic concatenation of hierarchical and syntactic partial sentence hypotheses. We separate the glue feature used in hierarchical and syntax-augmented translation into a glue feature that only fires when a hierarchical rule is glued, and a distinct glue feature firing when gluing a syntax-augmented rule. 6 Extension of SAMT to a bilingually parsed corpus Syntax-based MT models have been proposed both based on target-side syntactic annotations (Galley et al., 2004; Zollmann and Venugopal, 2006) as well source-side annotations (Liu et al., 2006). Syntactic annotations for both source and target language are available for popular language pairs such as Chinese-English. In this case, our grammar extraction procedure can be easily extended to impose both source and target constraints on the eligible substitutions simultaneously. Let Nf be the nonterminal label that would be assigned to a given initial rule when utilizing the source-side parse tree, and Ne the assigned label according to the target-side parse. Then our bilingual model assigns ‘Nf + Ne’ to the initial rule. The extraction of complex rules proceeds as before. The number of</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2017" citStr="Marcu et al., 2006" startWordPosition="290" endWordPosition="293">ine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time. We further propose different methods of combining hierar</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7236" citStr="Och, 2003" startWordPosition="1110" endWordPosition="1111">iven a source language sentence f, statistical machine translation defines the translation task as selecting the most likely target translation e under a model P(e|f), i.e.: = m ˆe(f) arg max P(e|f) = argmax hi(e,f)Ai e e i=1 where the arg max operation denotes a search through a structured space of translation outputs in the target language, hi(e, f) are bilingual features of e and f and monolingual features of e, and weights Ai are typically trained discriminatively to maximize translation quality (based on automatic metrics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz J. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="18856" citStr="Papineni et al., 2002" startWordPosition="3019" endWordPosition="3023">constraints on the eligible substitutions simultaneously. Let Nf be the nonterminal label that would be assigned to a given initial rule when utilizing the source-side parse tree, and Ne the assigned label according to the target-side parse. Then our bilingual model assigns ‘Nf + Ne’ to the initial rule. The extraction of complex rules proceeds as before. The number of nonterminals in this model, based on a source-model label set of size s and a target label set of size t, is thus given by st. 7 Experiments We evaluate our approaches by comparing translation quality according to the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chineseto-English translation task using MT04 as development set to train the model parameters A, and MT05, MT06 and MT08 as test sets. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7846" citStr="Petrov et al., 2008" startWordPosition="1199" endWordPosition="1202">Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a non-negative real-valued weight assigned to the rule; in our model, w is the exponential function of the i</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Petrov, Slav, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11455" citStr="Steedman, 2000" startWordPosition="1817" endWordPosition="1818">e corresponds to a constituent span are assigned that constituent’s label as their left-hand-side nonterminal. If the target side of the phrase pair is not spanned by a single constituent in the corresponding parse tree, we use the labels of subsuming, subsumed, and neighboring parse tree constituents to assign 1Penalization or reward of purely-lexical rules can be indirectly learned by trading off these features with the rule counter feature. 112 an extended label of the form C1 + C2, C1/C2, or C2 C1 (the latter two being motivated from the operations in combinatory categorial grammar (CCG) (Steedman, 2000)), indicating that the phrase pair’s target side spans two adjacent syntactic categories (e.g., she went: NP+VB), a partial syntactic category C1 missing a C2 at the right (e.g., the great: NP/NN), or a partial C1 missing a C2 at the left (e.g., great wall: WZ NP), respectively. The label assignment is attempted in the order just described, i.e., assembling labels based on ‘+’ concatenation of two subsumed constituents is preferred, as smaller constituents tend to be more accurately labeled. If no label is assignable by either of these three methods, a default label ‘FAIL’ is assigned. In addi</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Steedman, Mark. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
</authors>
<title>Grammar based statistical MT on Hadoop: An endto-end toolkit for large scale PSCFG based MT. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2009</date>
<pages>91--67</pages>
<contexts>
<context position="10022" citStr="Venugopal and Zollmann (2009)" startWordPosition="1589" endWordPosition="1592"> two sides (it is not in the space of word indices like i, j, u, v, m, n). The recursive form of this generalization operation allows the generation of rules with multiple nonterminal pairs. Chiang (2005) uses features analogous to the ones used in phrase-based translation: a language model neg-log probability, a ‘rule given source-side’ neg-log-probability, a ‘rule given target-side’ neg-log-probability, source- and target conditioned ‘lexical’ neg-log-probabilities based on word-to-word co-occurrences (Koehn et al., 2003), as well as rule, target word, and glue operation counters. We follow Venugopal and Zollmann (2009) to further add a rareness penalty, 1/ count(r) where count(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.1 3.2 Syntax Augmented MT Syntax Augmented MT (SAMT) (Zollmann and Venugopal, 2006) extends Chiang (2005) to include nonterminal symbols from target language phrase structure parse trees. Each target sentence in the training corpus is parsed with a stochastic parser to produce constitu</context>
<context position="19124" citStr="Venugopal and Zollmann, 2009" startWordPosition="3065" endWordPosition="3068">model assigns ‘Nf + Ne’ to the initial rule. The extraction of complex rules proceeds as before. The number of nonterminals in this model, based on a source-model label set of size s and a target label set of size t, is thus given by st. 7 Experiments We evaluate our approaches by comparing translation quality according to the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chineseto-English translation task using MT04 as development set to train the model parameters A, and MT05, MT06 and MT08 as test sets. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least 114 one source-side terminal symbol. For parameter tuning, we use the Loregularized minimum-error-rate training tool provided by the SAMT toolkit. The paral</context>
</contexts>
<marker>Venugopal, Zollmann, 2009</marker>
<rawString>Venugopal, Ashish and Andreas Zollmann. 2009. Grammar based statistical MT on Hadoop: An endto-end toolkit for large scale PSCFG based MT. The Prague Bulletin of Mathematical Linguistics, 91:67–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephan Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</booktitle>
<contexts>
<context position="7777" citStr="Venugopal et al., 2007" startWordPosition="1187" endWordPosition="1190">ics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a non-negative real-valued weight assig</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Venugopal, Ashish, Andreas Zollmann, and Stephan Vogel. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Conference (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B P Welford</author>
</authors>
<title>Note on a method for calculating corrected sums of squares and products.</title>
<date>1962</date>
<journal>Technometrics,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="15284" citStr="Welford (1962)" startWordPosition="2468" endWordPosition="2469">he normal distribution with mean zero and variance one. The confidence estimate is one if the encountered span length is equal to the mean of the distribution, and decreases as the encountered span length deviates further from the mean. The severity of that decline is determined by the distribution variance: the higher the variance, the less a deviation from the mean is penalized. Mean and variance of log source span length are sufficient statistics of the log-normal distribution. As we extract rules in a distributed fashion, we use a straightforward parallelization of the online algorithm of Welford (1962) and its improvement by West (1979) to compute the sample variance over all instances of a rule. 113 5 Merging a Hierarchical and a Syntax-Based Model While syntax-based grammars allow for more refined statistical models and guide the search by constraining substitution possibilitites in a grammar derivation, grammar sizes tend to be much greater than for hierarchical grammars. Therefore the average occurrence count of a syntax rule is much lower than that of a hierarchical rule, and thus estimated probabilitites are less reliable. We propose to augment the syntax-based “rule given source side</context>
</contexts>
<marker>Welford, 1962</marker>
<rawString>Welford, B. P. 1962. Note on a method for calculating corrected sums of squares and products. Technometrics, 4(3):419–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H D West</author>
</authors>
<title>Updating mean and variance estimates: an improved method.</title>
<date>1979</date>
<journal>Commun. ACM,</journal>
<volume>22</volume>
<issue>9</issue>
<contexts>
<context position="15319" citStr="West (1979)" startWordPosition="2474" endWordPosition="2475">and variance one. The confidence estimate is one if the encountered span length is equal to the mean of the distribution, and decreases as the encountered span length deviates further from the mean. The severity of that decline is determined by the distribution variance: the higher the variance, the less a deviation from the mean is penalized. Mean and variance of log source span length are sufficient statistics of the log-normal distribution. As we extract rules in a distributed fashion, we use a straightforward parallelization of the online algorithm of Welford (1962) and its improvement by West (1979) to compute the sample variance over all instances of a rule. 113 5 Merging a Hierarchical and a Syntax-Based Model While syntax-based grammars allow for more refined statistical models and guide the search by constraining substitution possibilitites in a grammar derivation, grammar sizes tend to be much greater than for hierarchical grammars. Therefore the average occurrence count of a syntax rule is much lower than that of a hierarchical rule, and thus estimated probabilitites are less reliable. We propose to augment the syntax-based “rule given source side” and “rule given target side” dist</context>
</contexts>
<marker>West, 1979</marker>
<rawString>West, D. H. D. 1979. Updating mean and variance estimates: an improved method. Commun. ACM, 22(9):532–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL.</booktitle>
<contexts>
<context position="1978" citStr="Zollmann and Venugopal, 2006" startWordPosition="282" endWordPosition="285">s language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time. We further propos</context>
<context position="10419" citStr="Zollmann and Venugopal, 2006" startWordPosition="1652" endWordPosition="1655">robability, source- and target conditioned ‘lexical’ neg-log-probabilities based on word-to-word co-occurrences (Koehn et al., 2003), as well as rule, target word, and glue operation counters. We follow Venugopal and Zollmann (2009) to further add a rareness penalty, 1/ count(r) where count(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.1 3.2 Syntax Augmented MT Syntax Augmented MT (SAMT) (Zollmann and Venugopal, 2006) extends Chiang (2005) to include nonterminal symbols from target language phrase structure parse trees. Each target sentence in the training corpus is parsed with a stochastic parser to produce constituent labels for target spans. Phrase pairs (extracted from a particular sentence pair) are assigned left-hand-side nonterminal symbols based on the target side parse tree constituent spans. Phrase pairs whose target side corresponds to a constituent span are assigned that constituent’s label as their left-hand-side nonterminal. If the target side of the phrase pair is not spanned by a single con</context>
<context position="17954" citStr="Zollmann and Venugopal, 2006" startWordPosition="2867" endWordPosition="2870">archical against syntactic rules. During translation, the hierarchical and syntax worlds are bridged by glue rules, which allow monotonic concatenation of hierarchical and syntactic partial sentence hypotheses. We separate the glue feature used in hierarchical and syntax-augmented translation into a glue feature that only fires when a hierarchical rule is glued, and a distinct glue feature firing when gluing a syntax-augmented rule. 6 Extension of SAMT to a bilingually parsed corpus Syntax-based MT models have been proposed both based on target-side syntactic annotations (Galley et al., 2004; Zollmann and Venugopal, 2006) as well source-side annotations (Liu et al., 2006). Syntactic annotations for both source and target language are available for popular language pairs such as Chinese-English. In this case, our grammar extraction procedure can be easily extended to impose both source and target constraints on the eligible substitutions simultaneously. Let Nf be the nonterminal label that would be assigned to a given initial rule when utilizing the source-side parse tree, and Ne the assigned label according to the target-side parse. Then our bilingual model assigns ‘Nf + Ne’ to the initial rule. The extraction</context>
<context position="21396" citStr="Zollmann and Venugopal (2006)" startWordPosition="3404" endWordPosition="3407">e assume this is due to the huge number of nonterminals in these grammars and the great amount of badly-estimated low-occurrence-count rules. Perhaps merging this grammar with a regular syntax-augmented grammar could yield better results. We also experimented with a source-parse based model (‘Syntax/src’). While not being able to match translation quality of its target-based counterpart, the model still outperforms the hierarchical system on all test sets. 8 Conclusion We proposed several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We added a source span length model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, resulting in small improvements for hierarchical phrase-based as well as syntax-augmented MT. We further demonstrated the utility of combining hierarchical and syntax-based PSCFG models and grammars. Finally, we compared syntax-augmented MT, which extracts rules based on target-side syntax, to a corresponding variant based on sour</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Zollmann, Andreas and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Franz J Och</author>
<author>Jay Ponte</author>
</authors>
<title>A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="5403" citStr="Zollmann et al. (2008)" startWordPosition="826" endWordPosition="829">dual model on an Englishto-German translation task. Essentially, the rule based system is converted to a single-nonterminal PSCFG, and hence can be combined with the hierarchical model, another single-nonterminal PSCFG, by taking the union of the rule sets and augmenting the feature vectors, adding zerovalues for rules that only exist in one of the two grammars. We face the challenge of combining the single-nonterminal hierarchical grammar with a multi-nonterminal syntax-augmented grammar. Thus one hierarchical rule typically corresponds to many syntax-augmented rules. The SAMT system used by Zollmann et al. (2008) adds hierarchical rules separately to the syntax-augmented grammar, resulting in a backbone grammar of well-estimated hierarchical rules supporting the sparser syntactic rules. They allow the model preference between hierarchical and syntax rules to be learned from development data by adding an indicator feature to all rules, which is one for hierarchical rules and zero for syntax rules. However, no empirical comparison is given between the purely syntax-augmented and the hybrid grammar. We aim to fill this gap by experimenting with both models, and further refine the hybrid approach by addin</context>
<context position="7824" citStr="Zollmann et al., 2008" startWordPosition="1195" endWordPosition="1198">-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS, a target terminal set (target vocabulary) TT, a shared nonterminal set N and rules of the form: X → (ry, α, w) where • X E N is a labeled nonterminal referred to as the left-hand-side of the rule. • ry E (N U TS)* is the source side of the rule. • α E (N U TT)* is the target side of the rule. • w E [0, oc) is a non-negative real-valued weight assigned to the rule; in our model, w is the exponen</context>
<context position="17103" citStr="Zollmann et al. (2008)" startWordPosition="2740" endWordPosition="2743">an alternative rareness penalty based on the combined frequency of all rules with the same terminals and substitution positions is obtained. Using these syntax and hierarchical features side by side amounts to interpolation of the respective probability models in log-space, with minimum-error-rate training (MERT) determining the optimal interpolation coefficient. We also add respective models interpolated with coefficient .5 in probability-space as additional features to the system. We further experiment with adding hierarchical rules separately to the syntax-augmented grammar, as proposed in Zollmann et al. (2008), with the respective syntax-specific features set to zero. A ‘hierarchical-indicator’ feature is added to all rules, which is one for hierarchical rules and zero for syntax rules, allowing the joint model to trade of hierarchical against syntactic rules. During translation, the hierarchical and syntax worlds are bridged by glue rules, which allow monotonic concatenation of hierarchical and syntactic partial sentence hypotheses. We separate the glue feature used in hierarchical and syntax-augmented translation into a glue feature that only fires when a hierarchical rule is glued, and a distinc</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>Zollmann, Andreas, Ashish Venugopal, Franz J. Och, and Jay Ponte. 2008. A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT. In Proceedings of the Conference on Computational Linguistics (COLING).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>