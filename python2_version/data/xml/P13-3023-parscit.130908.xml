<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002227">
<title confidence="0.987512">
Robust Multilingual Statistical Morphological Generation Models
</title>
<author confidence="0.955222">
Ond-rej Dušek and Filip Jur-cí-cek
</author>
<affiliation confidence="0.884375">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.601133">
Malostranské námˇestí 25, CZ-11800 Praha, Czech Republic
</address>
<email confidence="0.991894">
{odusek,jurcicek}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904352941177">
We present a novel method of statisti-
cal morphological generation, i.e. the pre-
diction of inflected word forms given
lemma, part-of-speech and morphological
features, aimed at robustness to unseen in-
puts. Our system uses a trainable classifier
to predict “edit scripts” that are then used
to transform lemmas into inflected word
forms. Suffixes of lemmas are included as
features to achieve robustness. We evalu-
ate our system on 6 languages with a vary-
ing degree of morphological richness. The
results show that the system is able to learn
most morphological phenomena and gen-
eralize to unseen inputs, producing sig-
nificantly better results than a dictionary-
based baseline.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962242545454545">
Surface realization is an integral part of all natu-
ral language generation (NLG) systems, albeit of-
ten implemented in a very simple manner, such
as filling words into ready hand-written templa-
tes. More sophisticated methods use hand-written
grammars (Gatt and Reiter, 2009), possibly in
combination with a statistical reranker (Langkilde
and Knight, 1998). Existing NLG systems are
very often applied to languages with little mor-
phology, such as English, where a small set of
hand-written rules or the direct use of word forms
in the symbolic representation or templates is usu-
ally sufficient, and so the main focus of these sys-
tems lies on syntax and word order.
However, this approach poses a problem in lan-
guages with a complex morphology. Avoiding
inflection, i.e. ensuring that a word lemma will
keep its base form at all times, often leads to
very unnatural results (see Figure 1). Some gen-
erators use a hand-made morphological dictionary
word inserted to avoid
inflecting the name
</bodyText>
<figure confidence="0.932376909090909">
ě é
Toto se líbí -----------
uživateli Jana -- Nováková.--
This is liked by user masculine (name)feminine
dative nominative
Děkujeme, Jan Novák , vaše hlasování
e u
Thank you, (name)nominative bylo vytvořeno.
your poll has been created
name left uninflected
(correct form: vocative)
</figure>
<figureCaption confidence="0.999643">
Figure 1: Unnatural language resulting from tem-
plates with no inflection.
</figureCaption>
<bodyText confidence="0.999266392857143">
The sentences are taken from the Czech translations of Face-
book and Doodle, which use simple templates to generate
personalized texts. Corrections to make the text fluent are
shown in red.
for inflection (Ptáˇcek and Žabokrtský, 2006) or a
dictionary learned from automatically tagged data
(Toutanova et al., 2008). That gives good results,
but reaching sufficient coverage with a hand-made
dictionary is a very demanding task and even using
extreme amounts of automatically annotated data
will not generalize beyond the word forms already
encountered in the corpus. Hand-written rules can
become overly complex and are not easily adapt-
able for a different language.
Therefore, the presented method relies on a sta-
tistical approach that learns to predict morpholog-
ical inflection from annotated data. As a result,
such approach is more robust, i.e. capable of gen-
eralizing to unseen inputs, and easily portable to
different languages.
An attempt to implement statistical morpholog-
ical generation has already been made by Bohnet
et al. (2010). However, their morphology genera-
tion was only a component of a complex genera-
tion system. Therefore, no deep analysis of the ca-
pabilities of the methods has been performed. In
addition, their method did not attempt to general-
ize beyond seen inputs. In this paper, we propose
</bodyText>
<page confidence="0.962246">
158
</page>
<note confidence="0.59327">
Proceedings of the ACL Student Research Workshop, pages 158–164,
</note>
<page confidence="0.471019">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<figure confidence="0.9926185">
Teller NN Gen,Sg,Masc
(plate) Tellers
Oma NN Nom,Pl,Fem
(grandma) Omas
</figure>
<bodyText confidence="0.999486181818182">
several improvements and provide a detailed eval-
uation of a statistical morphological inflection sys-
tem, including more languages into the evaluation
and focusing on robustness to unseen inputs.
The paper is structured as follows: first, we
explain the problem of morphological generation
(Section 2), then give an account of our system
(Section 3). Section 4 provides a detailed evalua-
tion of the performance of our system in different
languages. We then compare our system to related
works in Section 5. Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.992689" genericHeader="method">
2 The Problem of Morphological
Realization
</sectionHeader>
<bodyText confidence="0.999858777777778">
The problem of morphological surface realization
is inverse to part-of-speech tagging and lemma-
tization (or stemming): given a lemma/stem of
a word and its part-of-speech and morphological
properties, the system should output the correctly
inflected form of the word. An example is given
in Figure 2. This does not include generating aux-
iliary words (such as be → will be), which are as-
sumed to be already generated.
</bodyText>
<equation confidence="0.986404666666667">
word + NNS words
Wort + NN Neut,Pl,Dat Wörtern
be +VBZ is
ser V gen=c,num=s,person=3,
+
mood=indicative,tense=present es
</equation>
<figureCaption confidence="0.91559">
Figure 2: The task of morphological generation
(examples for English, German, and Spanish).
</figureCaption>
<bodyText confidence="0.99971275">
While this problem can be solved by a set of
rules to a great extent for languages with little mor-
phology such as English (Minnen et al., 2001),
it becomes much more complicated in languages
with a complex nominal case system or multiple
synthetic verbal inflection patterns, such as Ger-
man or Czech. Figure 3 shows an example of am-
biguity in these languages.
This research aims to create a system that is
easy to train from morphologically annotated data,
yet able to infer and apply more general rules and
generate forms unseen in the training corpus.
</bodyText>
<sectionHeader confidence="0.991949" genericHeader="method">
3 Our Morphological Generation Setup
</sectionHeader>
<bodyText confidence="0.9998942">
Similarly to Bohnet et al. (2010), our system is
based on the prediction of edit scripts (diffs) be-
tween the lemma and the target word form (see
Section 3.1), which are then used to derive the tar-
get word form from the lemma. This allows the
</bodyText>
<figure confidence="0.963374333333333">
Herr NN Nom,Pl,Masc
(sir) Herren
Mann NN Nom,Pl,Masc
(man) Männer
stroj N Gen=I,Cas=7,Num=S
strojem
(machine)
kost N Gen=F,Cas=3,Num=P
(bone) kostem
</figure>
<figureCaption confidence="0.9994005">
Figure 3: Morphological ambiguity in German
and Czech.
</figureCaption>
<bodyText confidence="0.999642375">
The same inflection pattern is used to express multiple mor-
phological properties (left) and multiple patterns may express
the same property (right).
system to operate even on previously unseen lem-
mas. The employed classifier and features are de-
scribed in Sections 3.2 and 3.3. Section 3.4 then
gives an overview of the whole morphological in-
flection process.
</bodyText>
<subsectionHeader confidence="0.999649">
3.1 Lemma-Form Edit Scripts
</subsectionHeader>
<bodyText confidence="0.999700083333333">
Our system uses lemma-form edit scripts based
on the Levenshtein string distance metric (Lev-
enshtein, 1966): the dynamic programming algo-
rithm used to compute the distance can be adapted
to produce diffs on characters, i.e. a mapping from
the source string (lemma) to the target string (word
form) that indicates which characters were added,
replaced or removed.
We use the distance from the end of the word to
indicate the position of a particular change, same
as Bohnet et al. (2010). We have added several
enhancements to this general scenario:
</bodyText>
<listItem confidence="0.995761705882353">
• Our system treats separately changes at the
beginning of the word, since they are usually
independent of the word length and always
occur at the beginning, such as the prefix ge-
for past participles in German or ne- for nega-
tion in Czech.
• Adjacent changes in the string are joined to-
gether to produce a total lower number of
more complex changes.
• If the Levenshtein edit script indicates a re-
moval of letters from the beginning of the
word, we treat the target word form as irreg-
ular, i.e. as if the whole word changed.
• In our setup, the edit scripts need not be
treated as atomic, which allows to train sep-
arate classification models for word changes
that are orthogonal (cf. Section 3.4).
</listItem>
<figure confidence="0.993873285714286">
N Gen=M,Cas=2,Num=S
pána
kůfl N Gen=M,Cas= 2,Num=S
koně
(horse)
pán
(sir)
</figure>
<page confidence="0.98537">
159
</page>
<bodyText confidence="0.9907265">
An example of the edit scripts generated by our
system is shown in Figure 4.
</bodyText>
<figure confidence="0.97941675">
do doing &gt;0-ing
llegar llegó &gt;2-ó
Mann Männer &gt;0-er,3:1-ä
jenž jež &gt;2:1-
mantenir mantindran &gt;0-an,2:1-d,4:1-i
sparen gespart &gt;2-t,&lt;ge
vědět nevfme &gt;4-íme,&lt;ne
be is *is
</figure>
<figureCaption confidence="0.995506">
Figure 4: Example edit scripts generated by our
system.
</figureCaption>
<bodyText confidence="0.999883136363636">
The changes are separated by commas. “&gt;” denotes a change
at the end of the word, “N:” denotes a change at the N-th
character from the end. The number of deleted characters
and their replacement follows in both cases. “&lt;” marks ad-
ditions to the beginning of a word (regardless of its length).
“*” marks irregular forms where the whole word is replaced.
Our diffs are case-insensitive since we believe
that letter-casing and morphology are distinct phe-
nomena and should be treated separately. Case-
insensitivity, along with merging adjacent changes
and the possibility to split models, causes a de-
crease in the number of different edit scripts, thus
simplifying the task for the classifier.
In our preliminary experiments on Czech, we
also explored the possibility of using different dis-
tance metrics for the edit scripts, such as vari-
ous settings of the Needleman-Wunsch algorithm
(Needleman and Wunsch, 1970) or the longest
common subsequence1 post-edited with regular
expressions to lower the total number of changes.
However, this did not have any noticeable impact
on the performance of the models.
</bodyText>
<subsectionHeader confidence="0.999854">
3.2 Used Statistical Models
</subsectionHeader>
<bodyText confidence="0.999964076923077">
We use the multi-class logistic regression classi-
fier from the LibLINEAR package2 (Fan et al.,
2008) for the prediction of edit scripts. We use
L1-regularization since it yields models that are
smaller in size and the resulting trained weights
indicate the important features in a straightforward
way. This direct influence on features (similar to
keyword spotting) allows for a simple interpreta-
tion of the learned models. We examined various
settings of the regularization cost and the termina-
tion criterion (See Section 4.1).
We have also experimented with support vec-
tor machines from the LibSVM package (Chang
</bodyText>
<footnote confidence="0.999167">
1We used the Perl implementation of this algorithm from
https://metacpan.org/module/String::Diff.
2We use it via the Python wrapper in the Scikit-Learn li-
brary (http://scikit-learn.org).
</footnote>
<bodyText confidence="0.999101833333333">
and Lin, 2011), but the logistic regression clas-
sifier proved to be better suited to this task, pro-
viding a higher edit script accuracy on the devel-
opment set for German and Czech (when feature
concatenation is used, cf. Section 3.3), while also
requiring less CPU time and RAM to train.
</bodyText>
<subsectionHeader confidence="0.95348">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999488666666667">
While the complete set of features varies across
languages given their specifics, most of the fea-
tures are common to all languages:
</bodyText>
<listItem confidence="0.9997602">
• lemma of the word in question,
• coarse and fine-grained part-of-speech tag,
• morphological features (e.g. case, gender,
tense etc., tagset-dependent), and
• suf�xes of the lemma of up to 4 characters.
</listItem>
<bodyText confidence="0.99932296969697">
Since morphological changes usually occur near
the end of the word, they mostly depend just on
that part of the word and not on e.g. prefixes or
previous parts of a compound. Therefore, using
suffixes allows the classifier to generalize to un-
known words.
In addition, as we use a linear classifier, we have
found the concatenation of various morphologi-
cal features, such as number, gender, and case in
nouns or tense and person in verbs, to be very ben-
eficial. We created new features by concatenating
all possible subsets of morphological features, as
long as all their values were non-empty (to prevent
from creating duplicate values). To avoid com-
binatorial explosion, we resorted to concatenating
only case, number, and gender for Czech and ex-
cluding the postype feature from concatenation
for Spanish and Catalan.
We also employ the properties of adjacent
words in the sentence as features in our models
for the individual languages (see Section 4). These
are used mainly to model congruency (is vs. are in
English, different adjectival declension after defi-
nite and indefinite article in German) or article vo-
calization (l’ vs. el in Catalan). The congruency
information could be obtained more reliably from
elsewhere in a complete NLG system (e.g. features
from the syntactic realizer), which would probably
result in a performance gain, but lies beyond the
scope of this paper.
No feature pruning was needed in our setup as
our classifier was able to handle the large amount
of features (100,000s, language-dependent).
</bodyText>
<page confidence="0.982303">
160
</page>
<subsectionHeader confidence="0.926605">
3.4 Overall Schema of the Predictor
</subsectionHeader>
<bodyText confidence="0.9998796">
After an examination of the training data, we de-
cided to use a separate model for the changes that
occur at the beginning of the word since they tend
to be much simpler than and not very dependent on
the changes towards the end of the word (e.g. the
usages of the Czech negation prefix ne- or the Ger-
man infinitive prefix zu- are quite self-contained
phenomena).
The final word inflection prediction schema
looks as follows:
</bodyText>
<listItem confidence="0.957470222222222">
1. Using the statistical model described in Sec-
tion 3.2, predict an edit script (cf. Section 3.1)
for changes at the end or in the middle of the
word.3
2. Predict an edit script for the possible addition
of a prefix using a separate model.
3. Apply the edit scripts predicted by the pre-
vious steps as rules to generate the final in-
flected word form.
</listItem>
<sectionHeader confidence="0.994894" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999970095238095">
We evaluate our morphological generation setup
on all of the languages included in the CoNLL
2009 Shared Task data sets except Chinese (which,
as an isolating language, lacks morphology almost
altogether): English, German, Spanish, Catalan,
Japanese, and Czech. We use the CoNLL 2009
data sets (Hajiˇc et al., 2009) with gold-standard
morphology annotation for all our experiments
(see Table 1 for a detailed overview).
We give a discussion of the overall performance
of our system in all the languages in Section 4.1.
We focus on Czech in the detailed analysis of the
generalization power of our system in Section 4.2
since Czech has the most complicated morphology
of all these languages. In addition, the morpho-
logical annotation provided in the CoNLL 2009
Czech data set is more detailed than in the other
languages, which eliminates the need for addi-
tional syntactic features (cf. Section 3.3). We also
provide a detailed performance overview on En-
glish for comparison.
</bodyText>
<subsectionHeader confidence="0.995274">
4.1 Overall Performance
</subsectionHeader>
<bodyText confidence="0.9996245">
The performance of our system in the best set-
tings for the individual languages measured on the
</bodyText>
<footnote confidence="0.986679">
3Completely irregular forms (see Section 3.1) are also
predicted by this step.
</footnote>
<bodyText confidence="0.987746294117647">
CoNLL 2009 evaluation test sets is shown in Ta-
ble 2. We used the classifier and features described
in Sections 3.2 and 3.3 (additional features for the
individual languages are listed in the table). We
used two models as described in Section 3.4 for
all languages but English, where no changes at the
beginning of the word were found in the training
data set and a single model was sufficient. We per-
formed a grid search for the best parameters of the
first model4 and used the same parameters for both
models.5
One can see from the results in Table 2 that
the system is able to predict the majority of word
forms correctly and performs well even on data
unseen in the training set.
When manually inspecting the errors produced
by the system, we observed that in some cases the
system in fact assigned a form synonymous to the
one actually occurring in the test set, such as not
instead of n’t in English or také instead of taky
(both meaning also) in Czech. However, most er-
rors are caused by the selection of a more frequent
rule, even if incorrect given the actual morpholog-
ical features. We believe that this could possibly
be mitigated by using features combining lemma
suffixes and morphological categories, or features
from the syntactic context.
The lower score for German is caused partly by
the lack of syntactic features for the highly am-
biguous adjective inflection and partly by a some-
what problematic lemmatization of punctuation
(all punctuation has the lemma “_” and the part-
of-speech tag only distinguishes terminal, comma-
like and other characters).
</bodyText>
<subsectionHeader confidence="0.993658">
4.2 Generalization Power
</subsectionHeader>
<bodyText confidence="0.999521111111111">
To measure the ability of our system to generalize
to previously unseen inputs, we compare it against
a baseline that uses a dictionary collected from the
same data and leaves unseen forms intact. The per-
formance of our system on unseen forms is shown
in Table 2 for all languages. A comparison with
the dictionary baseline for varying training data
sizes in English and Czech is given in Table 3.
It is visible from Table 3 that our approach
</bodyText>
<footnote confidence="0.968297571428571">
4We always used L1-norm and primal form and modi-
fied the termination criterion tol and regularization strength
C. The best values found on the development data sets for the
individual languages are listed in Table 2.
5As the changes at the beginning of words are much sim-
pler, changing parameters did not have a significant influence
on the performance of the second model.
</footnote>
<page confidence="0.992673">
161
</page>
<table confidence="0.999329875">
Language Data set sizes Eval In Eval (%) UnkF
Train Dev -Punct InflF
English 958,167 33,368 57,676 85.93 15.14 1.80
German 648,677 32,033 31,622 87.24 45.12 8.69
Spanish 427,442 50,368 50,630 85.42 29.96 6.16
Catalan 390,302 53,015 53,355 86.75 31.89 6.28
Japanese 112,555 6,589 13,615 87.34 10.73 6.43
Czech 652,544 87,988 92,663 85.50 42.98 7.68
</table>
<tableCaption confidence="0.634955">
Table 1: The CoNLL 2009 data sets: Sizes and properties
The data set sizes give the number of words (tokens) in the individual sets. The right column shows the percentage of data in
the evaluation set: -Punct = excluding punctuation tokens, InflF = only forms that differ from the lemma (i.e. have a non-empty
edit script), UnkF = forms unseen in the training set.
</tableCaption>
<table confidence="0.999951625">
Language Additional features Best parameters Rule (%) Total Form accuracy (%) UnkF
accuracy -Punc InflF
English W-1/LT C=10, tol=1e-3 99.56 99.56 99.49 97.76 98.26
German W-1/LT, MC C=10, tol=1e-3 96.66 / 99.91 96.46 98.01 92.64 89.63
Spanish MC C=100, tol=1e-3 99.05 / 99.98 99.01 98.86 97.10 91.11
Catalan W+1/C1, MC C=10, tol=1e-3 98.91 / 99.86 98.72 98.53 96.49 94.24
Japanese MC C=100, tol=1e-3 99.94 / 100.0 99.94 99.93 99.59 99.54
Czech MC C=100, tol=1e-3 99.45 / 99.99 99.45 99.35 98.81 95.93
</table>
<tableCaption confidence="0.999807">
Table 2: The overall performance of our system in different languages.
</tableCaption>
<bodyText confidence="0.922692551724138">
The additional features include: MC = concatenation of morphological features (see Section 3.3), W-1/LT = lemma and part-
of-speech tag of the previous word, W+1/C1 = first character of the following word.
Rule (edit script) accuracy is given for the prediction of changes at the end or in the middle and at the beginning of the word,
respectively.
The form accuracy field shows the percentage of correctly predicted (lowercased) target word forms: Total = on the whole
evaluation set; -Punct, InflF, UnkF = on subsets as defined in Table 1.
maintains a significantly6 higher accuracy when
compared to the baseline for all training data
sizes. It is capable of reaching high performance
even with relatively small amounts of training in-
stances. The overall performance difference be-
comes smaller as the training data grow; how-
ever, performance on unseen inputs and relative
error reduction show a different trend: the im-
provement stays stable. The relative error reduc-
tion decreases slightly for English where unknown
word forms are more likely to be base forms of
unknown lemmas, but keeps increasing for Czech
where unknown word forms are more likely to re-
quire inflection (the accuracy reached by the base-
line method on unknown forms equals the percent-
age of base forms among the unknown forms).
Though the number of unseen word forms is de-
clining with increasing amounts of training data,
which plays in favor of the dictionary method, un-
seen inputs will still occur and may become very
frequent for out-of-domain data. Our system is
therefore beneficial – at least as a back-off for un-
seen forms – even if a large-coverage morpholog-
</bodyText>
<footnote confidence="0.985392">
6Significance at the 99% level has been assessed using
paired bootstrap resampling (Koehn, 2004).
</footnote>
<bodyText confidence="0.99204">
ical dictionary is available.
We observed upon manual inspection that the
suffix features were among the most prominent
for the prediction of many edit scripts, which indi-
cates their usefulness; e.g. LemmaSuffix1=e is
a strong feature (along with POS_Tag=VBD) for
the edit script &gt;0d in English.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999844">
Statistical morphological realizers are very rare
since most NLG systems are either fully based
on hand-written grammars, including morpholog-
ical rules (Bateman et al., 2005; Gatt and Reiter,
2009; Lavoie and Rambow, 1997), or employ sta-
tistical methods only as a post-processing step to
select the best one of several variants generated
by a rule-based system (Langkilde and Knight,
1998; Langkilde-Geary, 2002) or to guide the de-
cision among the rules during the generation pro-
cess (Belz, 2008). While there are fully statistical
surface realizers (Angeli et al., 2010; Mairesse et
al., 2010), they operate in a phrase-based fashion
on word forms with no treatment of morphology.
Morphological generation in machine translation
tends to use dictionaries – hand-written (Žabokrt-
</bodyText>
<page confidence="0.993893">
162
</page>
<table confidence="0.999732333333333">
Train Czech English
data
part
Unseen Dict. acc. Our sys. acc. Error Unseen Dict acc. Our sys. acc. Error
forms Total UnkF Total UnkF reduct. forms Total UnkF Total UnkF reduct.
0.1 63.94 62.00 41.54 76.92 64.43 39.27 27.77 89.18 78.73 95.02 93.14 53.91
0.5 51.38 66.78 38.65 88.73 78.83 66.08 19.96 91.34 76.33 97.89 95.56 75.64
1 45.36 69.43 36.97 92.23 83.60 74.60 14.69 92.76 73.95 98.28 95.27 76.19
5 31.11 77.29 35.56 96.63 90.36 85.17 6.82 96.21 75.73 99.05 97.13 74.96
10 24.72 80.97 33.88 97.83 92.45 88.61 4.66 97.31 77.13 99.34 97.76 75.44
20 17.35 85.69 32.47 98.72 94.28 91.02 3.10 98.09 78.52 99.46 97.57 71.65
30 14.17 87.92 31.85 98.95 94.56 91.34 2.46 98.40 79.79 99.48 97.63 67.75
50 11.06 90.34 31.62 99.20 95.25 91.69 1.76 98.69 80.53 99.54 98.04 64.81
75 9.01 91.91 31.54 99.34 95.60 91.89 1.35 98.86 82.23 99.55 98.17 60.61
100 7.68 92.88 30.38 99.45 95.93 92.21 1.12 98.94 82.53 99.56 98.26 58.85
</table>
<tableCaption confidence="0.999975">
Table 3: Comparison of our system with a dictionary baseline on different training data sizes.
</tableCaption>
<bodyText confidence="0.998724685714286">
All numbers are percentages. The accuracy of both methods is given for the whole evaluation set (Total) and for word forms
unseen in the training set (UnkF). Error reduct. shows the relative error reduction of our method in comparison to the baseline
on the whole evaluation set.
ský et al., 2008), learnt from data (Toutanova et
al., 2008), or a combination thereof (Popel and
Žabokrtský, 2009).
The only statistical morphological generator
known to us is that of Bohnet et al. (2010), em-
ployed as a part of a support-vector-machines-
based surface realizer from semantic structures.
They apply their system to a subset of CoNLL
2009 data sets and their results (morphological ac-
curacy of 97.8% for English, 97.49% for German
and 98.48% for Spanish) seem to indicate that our
system performs better for English, slightly bet-
ter for Spanish and slightly worse for German, but
the numbers may not be directly comparable to our
results as it is unclear whether the authors use the
original data set or the output of the previous steps
of their system for evaluation and whether they in-
clude punctuation and/or capitalization.
Since the morphological generator of Bohnet et
al. (2010) is only a part of a larger system, they
do not provide a thorough analysis of the results.
While their system also predicts edit scripts de-
rived from Levenshtein distance, their edit script
representation seems less efficient than ours. They
report using about 1500 and 2500 different scripts
for English and German, respectively, disregard-
ing scripts occurring only once in the training data.
However, our representation only yields 154 En-
glish and 1785 German7 edit scripts with no prun-
ing. Along with the independent models for the
beginning of the word, this simplifies the task
for the classifier. In addition to features used by
</bodyText>
<footnote confidence="0.700678">
7We get this number when counting the edit scripts as
atomic; they divide into 1735 changes at the end or in the
middle of the words and 18 changes at the beginning.
</footnote>
<bodyText confidence="0.991848">
Bohnet et al. (2010), our system includes the suf-
fix features to generalize to unseen inputs.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.992952571428571">
We have presented a fully trainable morphologi-
cal generation system aimed at robustness to pre-
viously unseen inputs, based on logistic regression
and Levenshtein distance edit scripts between the
lemma and the target word form. The results from
the evaluation on six different languages from the
CoNLL 2009 data sets indicate that the system is
able to learn most morphological rules correctly
and is able to cope with previously unseen input,
performing significantly better than a dictionary
learned from the same amount of data. The sys-
tem is freely available for download at:
http://ufal.mff.cuni.cz/~odusek/flect
In future, we plan to integrate our generator
into a semantic NLG scenario, as well as a sim-
pler template-based system, and evaluate it on
further languages. We also consider employ-
ing transformation-based learning (Brill, 1995) for
prediction to make better use of the possibility of
splitting the edit scripts and applying the morpho-
logical changes one-by-one.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998681125">
This research was partly funded by the Ministry of
Education, Youth and Sports of the Czech Repub-
lic under the grant agreement LK11221 and core
research funding of Charles University in Prague.
The authors would like to thank Matˇej Korvas and
Martin Popel for helpful comments on the draft
and David Marek, Ondˇrej Plátek and Lukáš Žilka
for discussions.
</bodyText>
<page confidence="0.998671">
163
</page>
<sectionHeader confidence="0.990236" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999694695652174">
G. Angeli, P. Liang, and D. Klein. 2010. A simple
domain-independent probabilistic approach to gen-
eration. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, page 502–512.
J. A. Bateman, I. Kruijff-Korbayová, and G.-J. Krui-
jff. 2005. Multilingual resource sharing across
both related and unrelated languages: An imple-
mented, open-source framework for practical natu-
ral language generation. Research on Language and
Computation, 3(2-3):191–219.
A. Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431–455.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence genera-
tion with a stochastic multi-level realizer. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, page 98–106.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational lin-
guistics, 21(4):543–565.
C. C. Chang and C. J. Lin. 2011. LIBSVM: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3):27.
R. E Fan, K. W Chang, C. J Hsieh, X. R Wang, and
C. J Lin. 2008. LIBLINEAR: a library for large lin-
ear classification. The Journal of Machine Learning
Research, 9:1871–1874.
A. Gatt and E. Reiter. 2009. SimpleNLG: a realisation
engine for practical applications. In Proceedings of
the 12th European Workshop on Natural Language
Generation, page 90–93.
J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara,
M. A Martí, L. Màrquez, A. Meyers, J. Nivre,
S. Padó, J. Štˇepánek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, page 1–18.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, volume 4, page 388–395.
I. Langkilde and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1, page 704–710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the 12th Inter-
national Natural Language Generation Workshop,
page 17–24.
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the fifth conference on Applied natural language
processing, page 265–268.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707.
F. Mairesse, M. Gaši´c, F. Jurˇcíˇcek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2010. Phrase-based sta-
tistical language generation using graphical models
and active learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, page 1552–1561.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207–223.
S. B. Needleman and C. D. Wunsch. 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443–453.
M. Popel and Z. Žabokrtský. 2009. Improv-
ing English-Czech tectogrammatical MT. The
Prague Bulletin of Mathematical Linguistics, 92(-
1):115–134.
J. Ptáˇcek and Z. Žabokrtský. 2006. Synthesis of
Czech sentences from tectogrammatical trees. In
Text, Speech and Dialogue.
K. Toutanova, H. Suzuki, and A. Ruopp. 2008. Ap-
plying morphology generation models to machine
translation. In Proc. of ACL, volume 8.
Z. Žabokrtský, J. Ptáˇcek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
page 167–170. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998514">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.627346">
<title confidence="0.995514">Robust Multilingual Statistical Morphological Generation Models</title>
<author confidence="0.716379">Dušek</author>
<affiliation confidence="0.971273">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.970751">Malostranské námˇestí 25, CZ-11800 Praha, Czech</address>
<email confidence="0.96901">odusek@ufal.mff.cuni.cz</email>
<email confidence="0.96901">jurcicek@ufal.mff.cuni.cz</email>
<abstract confidence="0.995861277777778">We present a novel method of statistical morphological generation, i.e. the prediction of inflected word forms given lemma, part-of-speech and morphological features, aimed at robustness to unseen inputs. Our system uses a trainable classifier to predict “edit scripts” that are then used to transform lemmas into inflected word forms. Suffixes of lemmas are included as features to achieve robustness. We evaluate our system on 6 languages with a varying degree of morphological richness. The results show that the system is able to learn most morphological phenomena and generalize to unseen inputs, producing significantly better results than a dictionarybased baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Angeli</author>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>A simple domain-independent probabilistic approach to generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>502--512</pages>
<contexts>
<context position="20568" citStr="Angeli et al., 2010" startWordPosition="3350" endWordPosition="3353">r the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Žabokrt162 Train Czech English data part Unseen Dict. acc. Our sys. acc. Error Unseen Dict acc. Our sys. acc. Error forms Total UnkF Total UnkF reduct. forms Total UnkF Total UnkF reduct. 0.1 63.94 62.00 41.54 76.92 64.43 39.27 27.77 89.18 78.73 95.02 93.14 53.91 0.5 51.38 66.78 38.65 88.73 78.83 66.08 19.96 91.34 76.33 97.89 95.56 75.64 1 45.36 69.43 36.97 92.23 83.60 74.60 14.69 92.76 73.95 98</context>
</contexts>
<marker>Angeli, Liang, Klein, 2010</marker>
<rawString>G. Angeli, P. Liang, and D. Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, page 502–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bateman</author>
<author>I Kruijff-Korbayová</author>
<author>G-J Kruijff</author>
</authors>
<title>Multilingual resource sharing across both related and unrelated languages: An implemented, open-source framework for practical natural language generation.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<pages>3--2</pages>
<contexts>
<context position="20171" citStr="Bateman et al., 2005" startWordPosition="3286" endWordPosition="3289">s – even if a large-coverage morpholog6Significance at the 99% level has been assessed using paired bootstrap resampling (Koehn, 2004). ical dictionary is available. We observed upon manual inspection that the suffix features were among the most prominent for the prediction of many edit scripts, which indicates their usefulness; e.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Ž</context>
</contexts>
<marker>Bateman, Kruijff-Korbayová, Kruijff, 2005</marker>
<rawString>J. A. Bateman, I. Kruijff-Korbayová, and G.-J. Kruijff. 2005. Multilingual resource sharing across both related and unrelated languages: An implemented, open-source framework for practical natural language generation. Research on Language and Computation, 3(2-3):191–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="20494" citStr="Belz, 2008" startWordPosition="3341" endWordPosition="3342">.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Žabokrt162 Train Czech English data part Unseen Dict. acc. Our sys. acc. Error Unseen Dict acc. Our sys. acc. Error forms Total UnkF Total UnkF reduct. forms Total UnkF Total UnkF reduct. 0.1 63.94 62.00 41.54 76.92 64.43 39.27 27.77 89.18 78.73 95.02 93.14 53.91 0.5 51.38 66.78 38.65 88.73 78.83 66.08 19.96 91.34 76.33 97</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>A. Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>L Wanner</author>
<author>S Mille</author>
<author>A Burga</author>
</authors>
<title>Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>98--106</pages>
<contexts>
<context position="3384" citStr="Bohnet et al. (2010)" startWordPosition="517" endWordPosition="520"> task and even using extreme amounts of automatically annotated data will not generalize beyond the word forms already encountered in the corpus. Hand-written rules can become overly complex and are not easily adaptable for a different language. Therefore, the presented method relies on a statistical approach that learns to predict morphological inflection from annotated data. As a result, such approach is more robust, i.e. capable of generalizing to unseen inputs, and easily portable to different languages. An attempt to implement statistical morphological generation has already been made by Bohnet et al. (2010). However, their morphology generation was only a component of a complex generation system. Therefore, no deep analysis of the capabilities of the methods has been performed. In addition, their method did not attempt to generalize beyond seen inputs. In this paper, we propose 158 Proceedings of the ACL Student Research Workshop, pages 158–164, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Teller NN Gen,Sg,Masc (plate) Tellers Oma NN Nom,Pl,Fem (grandma) Omas several improvements and provide a detailed evaluation of a statistical morphological inflection sys</context>
<context position="5718" citStr="Bohnet et al. (2010)" startWordPosition="892" endWordPosition="895">e this problem can be solved by a set of rules to a great extent for languages with little morphology such as English (Minnen et al., 2001), it becomes much more complicated in languages with a complex nominal case system or multiple synthetic verbal inflection patterns, such as German or Czech. Figure 3 shows an example of ambiguity in these languages. This research aims to create a system that is easy to train from morphologically annotated data, yet able to infer and apply more general rules and generate forms unseen in the training corpus. 3 Our Morphological Generation Setup Similarly to Bohnet et al. (2010), our system is based on the prediction of edit scripts (diffs) between the lemma and the target word form (see Section 3.1), which are then used to derive the target word form from the lemma. This allows the Herr NN Nom,Pl,Masc (sir) Herren Mann NN Nom,Pl,Masc (man) Männer stroj N Gen=I,Cas=7,Num=S strojem (machine) kost N Gen=F,Cas=3,Num=P (bone) kostem Figure 3: Morphological ambiguity in German and Czech. The same inflection pattern is used to express multiple morphological properties (left) and multiple patterns may express the same property (right). system to operate even on previously u</context>
<context position="7003" citStr="Bohnet et al. (2010)" startWordPosition="1102" endWordPosition="1105">in Sections 3.2 and 3.3. Section 3.4 then gives an overview of the whole morphological inflection process. 3.1 Lemma-Form Edit Scripts Our system uses lemma-form edit scripts based on the Levenshtein string distance metric (Levenshtein, 1966): the dynamic programming algorithm used to compute the distance can be adapted to produce diffs on characters, i.e. a mapping from the source string (lemma) to the target string (word form) that indicates which characters were added, replaced or removed. We use the distance from the end of the word to indicate the position of a particular change, same as Bohnet et al. (2010). We have added several enhancements to this general scenario: • Our system treats separately changes at the beginning of the word, since they are usually independent of the word length and always occur at the beginning, such as the prefix gefor past participles in German or ne- for negation in Czech. • Adjacent changes in the string are joined together to produce a total lower number of more complex changes. • If the Levenshtein edit script indicates a removal of letters from the beginning of the word, we treat the target word form as irregular, i.e. as if the whole word changed. • In our set</context>
<context position="22280" citStr="Bohnet et al. (2010)" startWordPosition="3638" endWordPosition="3641">2.21 1.12 98.94 82.53 99.56 98.26 58.85 Table 3: Comparison of our system with a dictionary baseline on different training data sizes. All numbers are percentages. The accuracy of both methods is given for the whole evaluation set (Total) and for word forms unseen in the training set (UnkF). Error reduct. shows the relative error reduction of our method in comparison to the baseline on the whole evaluation set. ský et al., 2008), learnt from data (Toutanova et al., 2008), or a combination thereof (Popel and Žabokrtský, 2009). The only statistical morphological generator known to us is that of Bohnet et al. (2010), employed as a part of a support-vector-machinesbased surface realizer from semantic structures. They apply their system to a subset of CoNLL 2009 data sets and their results (morphological accuracy of 97.8% for English, 97.49% for German and 98.48% for Spanish) seem to indicate that our system performs better for English, slightly better for Spanish and slightly worse for German, but the numbers may not be directly comparable to our results as it is unclear whether the authors use the original data set or the output of the previous steps of their system for evaluation and whether they includ</context>
<context position="23796" citStr="Bohnet et al. (2010)" startWordPosition="3893" endWordPosition="3896">ion seems less efficient than ours. They report using about 1500 and 2500 different scripts for English and German, respectively, disregarding scripts occurring only once in the training data. However, our representation only yields 154 English and 1785 German7 edit scripts with no pruning. Along with the independent models for the beginning of the word, this simplifies the task for the classifier. In addition to features used by 7We get this number when counting the edit scripts as atomic; they divide into 1735 changes at the end or in the middle of the words and 18 changes at the beginning. Bohnet et al. (2010), our system includes the suffix features to generalize to unseen inputs. 6 Conclusions and Further Work We have presented a fully trainable morphological generation system aimed at robustness to previously unseen inputs, based on logistic regression and Levenshtein distance edit scripts between the lemma and the target word form. The results from the evaluation on six different languages from the CoNLL 2009 data sets indicate that the system is able to learn most morphological rules correctly and is able to cope with previously unseen input, performing significantly better than a dictionary l</context>
</contexts>
<marker>Bohnet, Wanner, Mille, Burga, 2010</marker>
<rawString>B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Proceedings of the 23rd International Conference on Computational Linguistics, page 98–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational linguistics,</journal>
<pages>21--4</pages>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<marker>Chang, Lin, 2011</marker>
<rawString>C. C. Chang and C. J. Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fan</author>
<author>K W Chang</author>
<author>C J Hsieh</author>
<author>X R Wang</author>
<author>C J Lin</author>
</authors>
<title>LIBLINEAR: a library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9374" citStr="Fan et al., 2008" startWordPosition="1497" endWordPosition="1500">er of different edit scripts, thus simplifying the task for the classifier. In our preliminary experiments on Czech, we also explored the possibility of using different distance metrics for the edit scripts, such as various settings of the Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) or the longest common subsequence1 post-edited with regular expressions to lower the total number of changes. However, this did not have any noticeable impact on the performance of the models. 3.2 Used Statistical Models We use the multi-class logistic regression classifier from the LibLINEAR package2 (Fan et al., 2008) for the prediction of edit scripts. We use L1-regularization since it yields models that are smaller in size and the resulting trained weights indicate the important features in a straightforward way. This direct influence on features (similar to keyword spotting) allows for a simple interpretation of the learned models. We examined various settings of the regularization cost and the termination criterion (See Section 4.1). We have also experimented with support vector machines from the LibSVM package (Chang 1We used the Perl implementation of this algorithm from https://metacpan.org/module/S</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R. E Fan, K. W Chang, C. J Hsieh, X. R Wang, and C. J Lin. 2008. LIBLINEAR: a library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>E Reiter</author>
</authors>
<title>SimpleNLG: a realisation engine for practical applications.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation,</booktitle>
<pages>90--93</pages>
<contexts>
<context position="1271" citStr="Gatt and Reiter, 2009" startWordPosition="182" endWordPosition="185">ixes of lemmas are included as features to achieve robustness. We evaluate our system on 6 languages with a varying degree of morphological richness. The results show that the system is able to learn most morphological phenomena and generalize to unseen inputs, producing significantly better results than a dictionarybased baseline. 1 Introduction Surface realization is an integral part of all natural language generation (NLG) systems, albeit often implemented in a very simple manner, such as filling words into ready hand-written templates. More sophisticated methods use hand-written grammars (Gatt and Reiter, 2009), possibly in combination with a statistical reranker (Langkilde and Knight, 1998). Existing NLG systems are very often applied to languages with little morphology, such as English, where a small set of hand-written rules or the direct use of word forms in the symbolic representation or templates is usually sufficient, and so the main focus of these systems lies on syntax and word order. However, this approach poses a problem in languages with a complex morphology. Avoiding inflection, i.e. ensuring that a word lemma will keep its base form at all times, often leads to very unnatural results (</context>
<context position="20194" citStr="Gatt and Reiter, 2009" startWordPosition="3290" endWordPosition="3293">verage morpholog6Significance at the 99% level has been assessed using paired bootstrap resampling (Koehn, 2004). ical dictionary is available. We observed upon manual inspection that the suffix features were among the most prominent for the prediction of many edit scripts, which indicates their usefulness; e.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Žabokrt162 Train Czech E</context>
</contexts>
<marker>Gatt, Reiter, 2009</marker>
<rawString>A. Gatt and E. Reiter. 2009. SimpleNLG: a realisation engine for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation, page 90–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Martí</author>
<author>L Màrquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Padó</author>
<author>J Štˇepánek</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martí, Màrquez, Meyers, Nivre, Padó, Štˇepánek, 2009</marker>
<rawString>J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M. A Martí, L. Màrquez, A. Meyers, J. Nivre, S. Padó, J. Štˇepánek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, page 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<pages>388--395</pages>
<contexts>
<context position="19685" citStr="Koehn, 2004" startWordPosition="3214" endWordPosition="3215">nown word forms are more likely to require inflection (the accuracy reached by the baseline method on unknown forms equals the percentage of base forms among the unknown forms). Though the number of unseen word forms is declining with increasing amounts of training data, which plays in favor of the dictionary method, unseen inputs will still occur and may become very frequent for out-of-domain data. Our system is therefore beneficial – at least as a back-off for unseen forms – even if a large-coverage morpholog6Significance at the 99% level has been assessed using paired bootstrap resampling (Koehn, 2004). ical dictionary is available. We observed upon manual inspection that the suffix features were among the most prominent for the prediction of many edit scripts, which indicates their usefulness; e.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, volume 4, page 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational LinguisticsVolume 1,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1353" citStr="Langkilde and Knight, 1998" startWordPosition="193" endWordPosition="196">r system on 6 languages with a varying degree of morphological richness. The results show that the system is able to learn most morphological phenomena and generalize to unseen inputs, producing significantly better results than a dictionarybased baseline. 1 Introduction Surface realization is an integral part of all natural language generation (NLG) systems, albeit often implemented in a very simple manner, such as filling words into ready hand-written templates. More sophisticated methods use hand-written grammars (Gatt and Reiter, 2009), possibly in combination with a statistical reranker (Langkilde and Knight, 1998). Existing NLG systems are very often applied to languages with little morphology, such as English, where a small set of hand-written rules or the direct use of word forms in the symbolic representation or templates is usually sufficient, and so the main focus of these systems lies on syntax and word order. However, this approach poses a problem in languages with a complex morphology. Avoiding inflection, i.e. ensuring that a word lemma will keep its base form at all times, often leads to very unnatural results (see Figure 1). Some generators use a hand-made morphological dictionary word inser</context>
<context position="20386" citStr="Langkilde and Knight, 1998" startWordPosition="3321" endWordPosition="3324"> suffix features were among the most prominent for the prediction of many edit scripts, which indicates their usefulness; e.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Žabokrt162 Train Czech English data part Unseen Dict. acc. Our sys. acc. Error Unseen Dict acc. Our sys. acc. Error forms Total UnkF Total UnkF reduct. forms Total UnkF Total UnkF reduct. 0.1 63.94 62.00 41.54 76.92 </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational LinguisticsVolume 1, page 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proceedings of the 12th International Natural Language Generation Workshop,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="20410" citStr="Langkilde-Geary, 2002" startWordPosition="3325" endWordPosition="3326">the most prominent for the prediction of many edit scripts, which indicates their usefulness; e.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Žabokrt162 Train Czech English data part Unseen Dict. acc. Our sys. acc. Error Unseen Dict acc. Our sys. acc. Error forms Total UnkF Total UnkF reduct. forms Total UnkF Total UnkF reduct. 0.1 63.94 62.00 41.54 76.92 64.43 39.27 27.77 89.18 </context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator. In Proceedings of the 12th International Natural Language Generation Workshop, page 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
<author>O Rambow</author>
</authors>
<title>A fast and portable realizer for text generation systems.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>265--268</pages>
<contexts>
<context position="20220" citStr="Lavoie and Rambow, 1997" startWordPosition="3294" endWordPosition="3297">icance at the 99% level has been assessed using paired bootstrap resampling (Koehn, 2004). ical dictionary is available. We observed upon manual inspection that the suffix features were among the most prominent for the prediction of many edit scripts, which indicates their usefulness; e.g. LemmaSuffix1=e is a strong feature (along with POS_Tag=VBD) for the edit script &gt;0d in English. 5 Related Work Statistical morphological realizers are very rare since most NLG systems are either fully based on hand-written grammars, including morphological rules (Bateman et al., 2005; Gatt and Reiter, 2009; Lavoie and Rambow, 1997), or employ statistical methods only as a post-processing step to select the best one of several variants generated by a rule-based system (Langkilde and Knight, 1998; Langkilde-Geary, 2002) or to guide the decision among the rules during the generation process (Belz, 2008). While there are fully statistical surface realizers (Angeli et al., 2010; Mairesse et al., 2010), they operate in a phrase-based fashion on word forms with no treatment of morphology. Morphological generation in machine translation tends to use dictionaries – hand-written (Žabokrt162 Train Czech English data part Unseen Di</context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>B. Lavoie and O. Rambow. 1997. A fast and portable realizer for text generation systems. In Proceedings of the fifth conference on Applied natural language processing, page 265–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="6625" citStr="Levenshtein, 1966" startWordPosition="1038" endWordPosition="1040">Cas=7,Num=S strojem (machine) kost N Gen=F,Cas=3,Num=P (bone) kostem Figure 3: Morphological ambiguity in German and Czech. The same inflection pattern is used to express multiple morphological properties (left) and multiple patterns may express the same property (right). system to operate even on previously unseen lemmas. The employed classifier and features are described in Sections 3.2 and 3.3. Section 3.4 then gives an overview of the whole morphological inflection process. 3.1 Lemma-Form Edit Scripts Our system uses lemma-form edit scripts based on the Levenshtein string distance metric (Levenshtein, 1966): the dynamic programming algorithm used to compute the distance can be adapted to produce diffs on characters, i.e. a mapping from the source string (lemma) to the target string (word form) that indicates which characters were added, replaced or removed. We use the distance from the end of the word to indicate the position of a particular change, same as Bohnet et al. (2010). We have added several enhancements to this general scenario: • Our system treats separately changes at the beginning of the word, since they are usually independent of the word length and always occur at the beginning, s</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mairesse</author>
<author>M Gaši´c</author>
<author>F Jurˇcíˇcek</author>
<author>S Keizer</author>
<author>B Thomson</author>
<author>K Yu</author>
<author>S Young</author>
</authors>
<title>Phrase-based statistical language generation using graphical models and active learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1552--1561</pages>
<marker>Mairesse, Gaši´c, Jurˇcíˇcek, Keizer, Thomson, Yu, Young, 2010</marker>
<rawString>F. Mairesse, M. Gaši´c, F. Jurˇcíˇcek, S. Keizer, B. Thomson, K. Yu, and S. Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, page 1552–1561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="5237" citStr="Minnen et al., 2001" startWordPosition="812" endWordPosition="815">ts part-of-speech and morphological properties, the system should output the correctly inflected form of the word. An example is given in Figure 2. This does not include generating auxiliary words (such as be → will be), which are assumed to be already generated. word + NNS words Wort + NN Neut,Pl,Dat Wörtern be +VBZ is ser V gen=c,num=s,person=3, + mood=indicative,tense=present es Figure 2: The task of morphological generation (examples for English, German, and Spanish). While this problem can be solved by a set of rules to a great extent for languages with little morphology such as English (Minnen et al., 2001), it becomes much more complicated in languages with a complex nominal case system or multiple synthetic verbal inflection patterns, such as German or Czech. Figure 3 shows an example of ambiguity in these languages. This research aims to create a system that is easy to train from morphologically annotated data, yet able to infer and apply more general rules and generate forms unseen in the training corpus. 3 Our Morphological Generation Setup Similarly to Bohnet et al. (2010), our system is based on the prediction of edit scripts (diffs) between the lemma and the target word form (see Section</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Needleman</author>
<author>C D Wunsch</author>
</authors>
<title>A general method applicable to the search for similarities in the amino acid sequence of two proteins.</title>
<date>1970</date>
<journal>Journal of molecular biology,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="9052" citStr="Needleman and Wunsch, 1970" startWordPosition="1447" endWordPosition="1450">ts length). “*” marks irregular forms where the whole word is replaced. Our diffs are case-insensitive since we believe that letter-casing and morphology are distinct phenomena and should be treated separately. Caseinsensitivity, along with merging adjacent changes and the possibility to split models, causes a decrease in the number of different edit scripts, thus simplifying the task for the classifier. In our preliminary experiments on Czech, we also explored the possibility of using different distance metrics for the edit scripts, such as various settings of the Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) or the longest common subsequence1 post-edited with regular expressions to lower the total number of changes. However, this did not have any noticeable impact on the performance of the models. 3.2 Used Statistical Models We use the multi-class logistic regression classifier from the LibLINEAR package2 (Fan et al., 2008) for the prediction of edit scripts. We use L1-regularization since it yields models that are smaller in size and the resulting trained weights indicate the important features in a straightforward way. This direct influence on features (similar to keyword spotting) allows for a</context>
</contexts>
<marker>Needleman, Wunsch, 1970</marker>
<rawString>S. B. Needleman and C. D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 48(3):443–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popel</author>
<author>Z Žabokrtský</author>
</authors>
<date>2009</date>
<booktitle>Improving English-Czech tectogrammatical MT. The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>92--1</pages>
<contexts>
<context position="22190" citStr="Popel and Žabokrtský, 2009" startWordPosition="3623" endWordPosition="3626">.91 31.54 99.34 95.60 91.89 1.35 98.86 82.23 99.55 98.17 60.61 100 7.68 92.88 30.38 99.45 95.93 92.21 1.12 98.94 82.53 99.56 98.26 58.85 Table 3: Comparison of our system with a dictionary baseline on different training data sizes. All numbers are percentages. The accuracy of both methods is given for the whole evaluation set (Total) and for word forms unseen in the training set (UnkF). Error reduct. shows the relative error reduction of our method in comparison to the baseline on the whole evaluation set. ský et al., 2008), learnt from data (Toutanova et al., 2008), or a combination thereof (Popel and Žabokrtský, 2009). The only statistical morphological generator known to us is that of Bohnet et al. (2010), employed as a part of a support-vector-machinesbased surface realizer from semantic structures. They apply their system to a subset of CoNLL 2009 data sets and their results (morphological accuracy of 97.8% for English, 97.49% for German and 98.48% for Spanish) seem to indicate that our system performs better for English, slightly better for Spanish and slightly worse for German, but the numbers may not be directly comparable to our results as it is unclear whether the authors use the original data set </context>
</contexts>
<marker>Popel, Žabokrtský, 2009</marker>
<rawString>M. Popel and Z. Žabokrtský. 2009. Improving English-Czech tectogrammatical MT. The Prague Bulletin of Mathematical Linguistics, 92(-1):115–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ptáˇcek</author>
<author>Z Žabokrtský</author>
</authors>
<title>Synthesis of Czech sentences from tectogrammatical trees.</title>
<date>2006</date>
<booktitle>In Text, Speech and Dialogue.</booktitle>
<marker>Ptáˇcek, Žabokrtský, 2006</marker>
<rawString>J. Ptáˇcek and Z. Žabokrtský. 2006. Synthesis of Czech sentences from tectogrammatical trees. In Text, Speech and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H Suzuki</author>
<author>A Ruopp</author>
</authors>
<title>Applying morphology generation models to machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<volume>8</volume>
<contexts>
<context position="2657" citStr="Toutanova et al., 2008" startWordPosition="404" endWordPosition="407">ováková.-- This is liked by user masculine (name)feminine dative nominative Děkujeme, Jan Novák , vaše hlasování e u Thank you, (name)nominative bylo vytvořeno. your poll has been created name left uninflected (correct form: vocative) Figure 1: Unnatural language resulting from templates with no inflection. The sentences are taken from the Czech translations of Facebook and Doodle, which use simple templates to generate personalized texts. Corrections to make the text fluent are shown in red. for inflection (Ptáˇcek and Žabokrtský, 2006) or a dictionary learned from automatically tagged data (Toutanova et al., 2008). That gives good results, but reaching sufficient coverage with a hand-made dictionary is a very demanding task and even using extreme amounts of automatically annotated data will not generalize beyond the word forms already encountered in the corpus. Hand-written rules can become overly complex and are not easily adaptable for a different language. Therefore, the presented method relies on a statistical approach that learns to predict morphological inflection from annotated data. As a result, such approach is more robust, i.e. capable of generalizing to unseen inputs, and easily portable to </context>
<context position="22135" citStr="Toutanova et al., 2008" startWordPosition="3615" endWordPosition="3618">91.69 1.76 98.69 80.53 99.54 98.04 64.81 75 9.01 91.91 31.54 99.34 95.60 91.89 1.35 98.86 82.23 99.55 98.17 60.61 100 7.68 92.88 30.38 99.45 95.93 92.21 1.12 98.94 82.53 99.56 98.26 58.85 Table 3: Comparison of our system with a dictionary baseline on different training data sizes. All numbers are percentages. The accuracy of both methods is given for the whole evaluation set (Total) and for word forms unseen in the training set (UnkF). Error reduct. shows the relative error reduction of our method in comparison to the baseline on the whole evaluation set. ský et al., 2008), learnt from data (Toutanova et al., 2008), or a combination thereof (Popel and Žabokrtský, 2009). The only statistical morphological generator known to us is that of Bohnet et al. (2010), employed as a part of a support-vector-machinesbased surface realizer from semantic structures. They apply their system to a subset of CoNLL 2009 data sets and their results (morphological accuracy of 97.8% for English, 97.49% for German and 98.48% for Spanish) seem to indicate that our system performs better for English, slightly better for Spanish and slightly worse for German, but the numbers may not be directly comparable to our results as it is</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>K. Toutanova, H. Suzuki, and A. Ruopp. 2008. Applying morphology generation models to machine translation. In Proc. of ACL, volume 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Žabokrtský</author>
<author>J Ptáˇcek</author>
<author>P Pajas</author>
</authors>
<title>TectoMT: highly modular MT system with tectogrammatics used as transfer layer.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>167--170</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Žabokrtský, Ptáˇcek, Pajas, 2008</marker>
<rawString>Z. Žabokrtský, J. Ptáˇcek, and P. Pajas. 2008. TectoMT: highly modular MT system with tectogrammatics used as transfer layer. In Proceedings of the Third Workshop on Statistical Machine Translation, page 167–170. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>