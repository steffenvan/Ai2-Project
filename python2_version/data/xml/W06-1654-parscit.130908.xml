<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000203">
<title confidence="0.992612">
Random Indexing using Statistical Weight Functions
</title>
<author confidence="0.998134">
James Gorman and James R. Curran
</author>
<affiliation confidence="0.998637">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.667177">
NSW 2006, Australia
</address>
<email confidence="0.998836">
{jgorman2,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999663857142857">
Random Indexing is a vector space tech-
nique that provides an efficient and scal-
able approximation to distributional simi-
larity problems. We present experiments
showing Random Indexing to be poor at
handling large volumes of data and evalu-
ate the use of weighting functions for im-
proving the performance of Random In-
dexing. We find that Random Index is ro-
bust for small data sets, but performance
degrades because of the influence high fre-
quency attributes in large data sets. The
use of appropriate weight functions im-
proves this significantly.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999914052631579">
Synonymy relations between words have been
used to inform many Natural Language Processing
(NLP) tasks. While these relations can be extracted
from manually created resources such as thesauri
(e.g. Roget’s Thesaurus) and lexical databases
(e.g. WordNet, Fellbaum, 1998), it is often ben-
eficial to extract these relationships from a corpus
representative of the task.
Manually created resources are expensive and
time-consuming to create, and tend to suffer from
problems of bias, inconsistency, and limited cover-
age. These problems may result in an inappropri-
ate vocabulary, where some terms are not present
or an unbalanced set of synonyms. In a medical
context it is more likely that administration will re-
fer to the giving of medicine than to paper work,
whereas in a business context the converse is more
likely.
The most common method for automatically
creating these resources uses distributional simi-
larity and is based on the distributional hypoth-
esis that similar words appear in similar con-
texts. Terms are described by collating informa-
tion about their occurrence in a corpus into vec-
tors. These context vectors are then compared for
similarity. Existing approaches differ primarily in
their definition of context, e.g. the surrounding
words or the entire document, and their choice of
distance metric for calculating similarity between
the context vectors representing each term.
In this paper, we analyse the use of Random In-
dexing (Kanerva et al., 2000) for semantic similar-
ity measurement. Random Indexing is an approxi-
mation technique proposed as an alternative to La-
tent Semantic Analysis (LSA, Landauer and Du-
mais, 1997). Random Indexing is more scalable
and allows for the incremental learning of context
information.
Curran and Moens (2002) found that dramati-
cally increasing the volume of raw input data for
distributional similarity tasks increases the accu-
racy of synonyms extracted. Random Indexing
performs poorly on these volumes of data. Noting
that in many NLP tasks, including distributional
similarity, statistical weighting is used to improve
performance, we modify the Random Indexing al-
gorithm to allow for weighted contexts.
We test the performance of the original and our
modified system using existing evaluation metrics.
We further evaluate against bilingual lexicon ex-
traction using distributional similarity (Sahlgren
and Karlgren, 2005). The paper concludes with
a more detailed analysis of Random Indexing in
terms of both task and corpus composition. We
find that Random Index is robust for small cor-
pora, but larger corpora require that the contexts
be weighted to maintain accuracy.
</bodyText>
<page confidence="0.975525">
457
</page>
<note confidence="0.9592185">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457–464,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.960831" genericHeader="introduction">
2 Random Indexing
</sectionHeader>
<bodyText confidence="0.999769204545455">
Random Indexing is an approximating technique
proposed by Kanerva et al. (2000) as an alternative
to Singular Value Decomposition (SVD) for Latent
Semantic Analysis (LSA, Landauer and Dumais,
1997). In LSA, it is assumed that there is some
underlying dimensionality in the data, so that the
attributes of two or more terms that have similar
meanings can be folded onto a single axis.
Sahlgren (2005) criticise LSA for being both
computationally inefficient and requiring the for-
mation of a full co-occurrence matrix and its de-
composition before any similarity measurements
can be made. Random Indexing avoids both these
by creating a short index vector for each unique
context, and producing the context vector for each
term by summing index vectors for each context
as it is read, allowing an incremental building of
the context space.
Hecht-Nielsen (1994) observed that there are
many more nearly orthogonal directions in high-
dimensional space than there are truly orthogo-
nal directions. The random index vectors are
nearly-orthogonal, resulting in an approximate
description of the context space. The approx-
imation comes from the Johnson-Lindenstrauss
lemma (Johnson and Lindenstrauss, 1984), which
states that if we project points in a vector space
into a randomly selected subspace of sufficiently
high dimensionality, the distances between the
points are approximately preserved. Random Pro-
jection (Papadimitriou et al., 1998) and Random
Mapping (Kaski, 1998) are similar techniques that
use this lemma. Achlioptas (2001) showed that
most zero-mean distributions with unit variance,
including very simple ones like that used in Ran-
dom Indexing, produce a mapping that satisfies
the lemma. The following description of Ran-
dom Indexing is taken from Sahlgren (2005) and
Sahlgren and Karlgren (2005).
We allocate a d length index vector to each
unique context as is it found. These vectors con-
sist of a large number of 0s and a small number
(E) of ±1s. Each element is allocated one of these
values with the following probability:
</bodyText>
<equation confidence="0.939656">
+1 with probability E/2d
0 with probability d��
d
−1 with probability E/d2
</equation>
<bodyText confidence="0.992167705882353">
Context vectors are generated on-the-fly. As the
corpus is scanned, for each term encountered, its
contexts are extracted. For each new context, an
index vector is produced for it as above. The con-
text vector is the sum of the index vectors of all
the contexts in which the term appears.
The context vector for a term t appearing in one
each in the contexts c1 = [1, 0, 0, −1] and c2 =
[0, 1, 0, −1] would be [1, 1, 0, −2]. If the context
c1 encountered again, no new index vector would
be generated and the existing index vector for c1
would be added to the existing context vector to
produce a new context vector for t of [2, 1, 0, −3].
The distance between these context vectors can
then be measured using any vector space distance
measure. Sahlgren and Karlgren (2005) use the
cosine measure:
</bodyText>
<equation confidence="0.99779875">
Pd i=1 ~ui~vi
qPd qPd
i=1 ~u2 i=1 ~v2
i i
</equation>
<bodyText confidence="0.999647">
Random Indexing allows for incremental sam-
pling. This means that the entire data set need not
be sampled before similarity between terms can be
measured. It also means that additional context
information can be added at any time without in-
validating the information already produced. This
is not feasible with most other word-space mod-
els. The approach used by Grefenstette (1994) and
Curran (2004) requires the re-computation of all
non-linear weights if new data is added, although
some of these weights can be approximated when
adding new data incrementally. Similarly, new
data can be folded into a reduced LSA space, but
there is no guarantee that the original smoothing
will apply correctly to the new data (Sahlgren,
2005).
</bodyText>
<sectionHeader confidence="0.997953" genericHeader="method">
3 Weights
</sectionHeader>
<bodyText confidence="0.999091071428571">
Our initial experiments using Random Indexing
to extract synonymy relations produced worse re-
sults than those using full vector measures, such as
JACCARD (Curran, 2004), when the full vector is
weighted. We experiment using weight functions
with Random Indexing.
Only a linear weighting scheme can be applied
while maintaining incremental sampling. While
incremental sampling is part of the rationale be-
hind its development, it is not required for Ran-
dom Indexing to work as a dimensionality reduc-
tion technique.
To this end, we revise Random Indexing to en-
able us to use weight functions. For each unique
</bodyText>
<equation confidence="0.946969487804878">
⎧
⎨⎪
⎪⎩
cos(θ(u,v)) = u· v
|~u ||~v |=
458
IDENTITY 1.0 FREQ f(w, r, w0)
f(w,r,w0) f(w,r,w0)
RELFREQ TF-IDF
f(w,∗,∗) n(∗,r,w0)
log2(f(w,r,w0)+1)
TF-IDF†
N(r,w0)
log
log( p(w,r,w0)
MI p(w,∗,∗)p(∗,r,w0))
2(1+
n(∗,r,w0) )
p(w,r,w0)−p(∗,r,w0)p(w,∗,∗) log2(f(w,r,w0)+1)
TTEST GREF94
log2(n(∗,r,w0)+1)
�p(∗,r,w0)p(w,∗,∗)
− log(n(∗,r,w0)
LIN98B )
CHI2 cf. Manning and Sch¨utze (1999) LR cf. Manning and Sch¨utze (1999)
2p(w,r,w0 )
DICE p(w,∗,∗)+p(∗,r,w0)
Table 1: Weight Functions Evaluated
f�w,, )  ����0�∈���∗�∗� f(w, r, w0)
p�w, , )  ����∗�∗�
��∗�∗�∗�
n�w, , )  |�w, , )|
N�  |{w|n�w, , ) &gt; �}|
also evaluated with an extra
r,
+
1) factor to promote the influence of higher fre-
quency attributes, indicated by a LOG
loge(f(w,
w0)
suffix. Al-
</equation>
<bodyText confidence="0.9980285">
ternative functions are marked with a dagger.
The context vector of each term w is thus:
</bodyText>
<equation confidence="0.847410555555556">
w�= � (r, ~w0) wgt(w, r, w0)
����0�∈���∗�∗�
~
where (r, w0) is the index vector of the context
log( f(w,r,w0)f(∗,r∗)
LIN98A f(∗,r,w0)f(w,r,∗) )
Nw
per term, which since a
m is also O(dnm).
</equation>
<bodyText confidence="0.95286525">
Following the notation of Curran (2004), a
relation is defined as a tuple (w, r,
where
w is a term, which occurs in some grammatical re-
lation rwith another word
in some sentence.
We refer to the tuple (r,
as an attribute of w.
For example, (dog, direct-obj, walk) indicates that
dog was the direct object of walk in a sentence.
An as

</bodyText>
<equation confidence="0.9950416">
con-
text
w0)
w0
w0)
</equation>
<bodyText confidence="0.9799">
terisk indicates the set of all existing val-
ues of that component in the tuple.
</bodyText>
<equation confidence="0.963735">
�w, , )  {�r, w0)|�w, r, w0)}
</equation>
<bodyText confidence="0.9490535">
The frequency of a tuple, that is the number of
times a word appears in a context is f(w, r, w0).
f(w, , ) is the instance or token frequency of the
contexts in which w appears. n(w, , ) is the type
frequency. This is the number of attributes of w.
The weights functions we evaluate are
those from Curran
w0).
(2004) and are given in Table 1.
context attribute, a d length index vector will be
generated. The context vector of a term w is then
created by the weighted sum of each of its at-
tributes. The results of the original Random In-
dexing algorithm are reproduced using frequency
weighting (FREQ).
Weights are generated using the frequency dis-
tribution of each term and its contexts. This in-
creases the overhead, as we must store the context
attributes for each term. Rather than the context
vector being generated by adding each individual
context, it is generated by adding each the index
vector for each unique context multiplied by its
weight.
The time to calculate the weight of all attributes
of all terms is negligible. The original technique
scales to O(dnm) in construction, for n terms and
m unique attributes. Our new technique scales to
O(d(a + nm)) for anon-zero context attributes
</bodyText>
<page confidence="0.980313">
459
</page>
<bodyText confidence="0.9622505">
Most experiments limited weights to the positive
range; those evaluated with an unrestricted range
are marked with a ± suffix. Some weights were
(r,
</bodyText>
<sectionHeader confidence="0.970213" genericHeader="method">
4 Semantic Similarity
</sectionHeader>
<bodyText confidence="0.97188494117647">
The first use of Random Indexing was to measure
semantic similarity using distributional similarity.
Kanerva et al. (2000) used Random Indexing to
find the best synonym match in Test of English
as a Foreign Language (TOEFL).
was used
by Landauer and Dumais (1997), who reported an
accuracy 36% using un-normalised vectors, which
was improved to 64% using LSA. Kanerva et al.
(2000) produced an accuracy of
the
same type of document based contexts and Ran-
dom Indexing, which improved to
using
narrow context windows. Karlgren and Sahlgren
(2001) improved this to 72% using lemmatisation
an
</bodyText>
<sectionHeader confidence="0.420527" genericHeader="method">
TOEFL
</sectionHeader>
<bodyText confidence="0.827929">
48–51% using
62–70%
d POS tagging.
</bodyText>
<subsectionHeader confidence="0.991262">
4.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999986379310345">
Measuring distributional similarity first requires
the extraction of context information for each of
the vocabulary terms from raw text. The contexts
for each term are collected together and counted,
producing a vector of context attributes and their
frequencies in the corpus. These terms are then
compared for similarity using a nearest-neighbour
search based on distance calculations between the
statistical descriptions of their contexts.
The simplest algorithm for finding synonyms is
a k-nearest-neighbour search, which involves pair-
wise vector comparison of the context vector of
the target term with the context vector of every
other term in the vocabulary.
We use two types of context extraction to pro-
duce both high and low quality context descrip-
tions. The high quality contexts were extracted
from grammatical relations extracted using the
SEXTANT relation extractor (Grefenstette, 1994)
and are lemmatised. This is the same data used in
Curran (2004).
The low quality contexts were extracted taking
a window of one word to the left and right of the
target term. The context is marked as to whether
it preceded or followed the term. Curran (2004)
found this extraction technique to provided rea-
sonable results on the non-speech portion of the
BNC when the data was lemmatised. We do not
lemmatise, which produces noisier data.
</bodyText>
<subsectionHeader confidence="0.991244">
4.2 Bilingual Lexicon Acquisition
</subsectionHeader>
<bodyText confidence="0.999698631578947">
A variation on the extraction of synonymy rela-
tions, is the extraction of bilingual lexicons. This
is the task of finding for a word in one language
words of a similar meaning in a second language.
The results of this can be used to aid manual con-
struction of resources or directly aid translation.
This task was first approached as a distribu-
tional similarity-like problem by Brown et al.
(1988). Their approach uses aligned corpora in
two or more languages: the source language, from
which we are translating, and the target language,
to which we are translating. For a each aligned
segment, they measure co-occurrence scores be-
tween each word in the source segment and each
word in the target segment. These co-occurrence
scores are used to measure the similarity between
source and target language terms
Sahlgren and Karlgren’s approach models the
problem as a distributional similarity problem us-
</bodyText>
<table confidence="0.9981174">
Source Context Target
Language Language
aaabbc I xxyzzz
bcc II wxy
aab III xzz
</table>
<tableCaption confidence="0.99763">
Table 2: Paragraph Aligned Corpora
</tableCaption>
<bodyText confidence="0.999915947368421">
ing the paragraph as context. In Table 2, the source
language is limited to the words a, b and c and the
target language to the words x, y and z. Three para-
graphs in each of these languages are presented as
pairs of translations labelled as a context: aaabbc
is translated as xxyzzz and labelled context I. The
frequency weighted context vector for a is {I:3,
III:2} and for x is {I:2, II:1, III:1}.
A translation candidate for a term in the source
language is found by measuring the similarity be-
tween its context vector and the context vectors of
each of the terms in the target language. The most
similar target language term is the most likely
translation candidate.
Sahlgren and Karlgren (2005) use Random In-
dexing to produce the context vectors for the
source and target languages. We re-implement
their system and apply weighting functions in an
attempt to achieve improved results.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999973636363637">
For the experiments extracting synonymy rela-
tions, high quality contexts were extracted from
the non-speech portion of the British National
Corpus (BNC) as described above. This represents
90% of the BNC, or 90 million words.
Comparisons between low frequency terms are
less accurate than between high frequency terms
as there is less evidence describing them (Cur-
ran and Moens, 2002). This is compounded in
randomised vector techniques because the ran-
domised nature of the representation means that
a low frequency term may have a similar context
vector to a high frequency term while not sharing
many contexts. A frequency cut-off of 100 was
found to balance this inaccuracy with the reduc-
tion in vocabulary size. This reduces the original
246,046 word vocabulary to 14,862 words. Exper-
iments showed d = 1000 and E = 10 to provide a
balance between speed and accuracy.
Low quality contexts were extracted from por-
tions of the entire of the BNC. These formed cor-
pora of 100,000, 500,000, 1 million, 5 million, 10
</bodyText>
<page confidence="0.997682">
460
</page>
<bodyText confidence="0.999864375">
million, 50 million and 100 million words, cho-
sen from random documents. This allowed us test
the effect of both corpus size and context qual-
ity. This produced vocabularies of between 10,380
and 522,163 words in size. Because of the size
of the smallest corpora meant that a high cutoff
would remove to many terms for a fair test, a cut-
off of 5 was applied. The values d = 1000 and
</bodyText>
<equation confidence="0.626357">
c = 6 were used.
</equation>
<bodyText confidence="0.999861923076923">
For our experiments in bilingual lexicon acqui-
sition we follow Sahlgren and Karlgren (2005).
We use the Spanish-Swedish and the English-
German portions of the Europarl corpora (Koehn,
2005).1 These consist of 37,379 aligned para-
graphs in Spanish–Swedish and 45,556 in English-
German. The text was lemmatised using Con-
nexor Machinese (Tapanainen and J¨avinen, 1997)2
producing vocabularies of 42,671 terms of Span-
ish, 100,891 terms of Swedish, 40,181 terms of
English and 70,384 terms of German. We use
d = 600 and c = 6 and apply a frequency cut-
off of 100.
</bodyText>
<sectionHeader confidence="0.997788" genericHeader="method">
6 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.99998584">
The simplest method for evaluation is the direct
comparison of extracted synonyms with a man-
ually created gold standard (Grefenstette, 1994).
To reduce the problem of limited coverage, our
evaluation of the extraction of synonyms combines
three electronic thesauri: the Macquarie, Roget’s
and Moby thesauri.
We follow Curran (2004) and use two perfor-
mance measures: direct matches (DIRECT) and
inverse rank (INVR). DIRECT is the number of
returned synonyms found in the gold standard.
INVR is the sum of the inverse rank of each match-
ing synonym, e.g. matches at ranks 3, 5 and 28
give an inverse rank score of 13 + 51 + 128. With
at most 100 matching synonyms, the maximum
INVR is 5.187. This more fine grained as it incor-
porates the both the number of matches and their
ranking.
The same 300 single word nouns were used for
evaluation as used by Curran (2004) for his large
scale evaluation. These were chosen randomly
from WordNet such that they covered a range over
the following properties: frequency, number of
senses, specificity and concreteness. On average
each evaluation term had 301 gold-standard syn-
</bodyText>
<footnote confidence="0.9999085">
1http://www.statmt.org/europarl/
2http://www.connexor.com/
</footnote>
<table confidence="0.999925190476191">
Weight DIRECT INVR
FREQ 2.87 0.94
IDENTITY 3.18 0.95
RELFREQ 2.87 0.94
TF-IDF 0.30 0.07
TF-IDF† 3.92 1.39
MI 1.52 0.54
MILOG 3.38 1.39
MI± 1.87 0.65
MILOG± 3.49 1.41
TTEST 1.06 0.52
TTESTLOG 1.53 0.62
TTEST± 1.06 0.52
TTESTLOG± 1.52 0.61
GREF94 2.82 0.86
LIN98A 1.52 0.50
LIN98B 2.95 0.84
CHI2 0.46 0.25
DICE 3.32 1.11
DICELOG 2.56 0.81
LR 1.96 0.58
</table>
<tableCaption confidence="0.999944">
Table 3: Evaluation of synonym extraction
</tableCaption>
<bodyText confidence="0.987274733333333">
onyms. For each of these terms, the closest 100
terms and their similarity scores were extracted.
For the evaluation of bilingual lexicon acqui-
sition we use two online lexical resources used
by Sahlgren and Karlgren (2005) as gold stan-
dards: Lexin’s online Swedish-Spanish lexicon3
and TU Chemnitz’ online English-German dic-
tionary.4 Each of the elements in a compound
or multi-word expression is treated as a poten-
tial translation. The German abblendlicht (low beam
light) is treated as a translation candidate for low,
beam and light separately.
Low coverage is more of problem than in our
thesaurus task as we have not used combined re-
sources. There are an average of 19 translations
for each of the 3,403 Spanish terms and 197 trans-
lations for each of the 4,468 English terms. The
English-German translation count is skewed by
the presence of connectives in multi-word expres-
sions, such as of and on, producing mistranslations.
Sahlgren and Karlgren (2005) provide good com-
mentary on the evaluation of this task.
Spanish and English are used as the source lan-
guages. The 200 closest terms in the target lan-
guage are found for all terms in both the source
vocabulary and the gold-standards.
We measure the DIRECT score and INVR as
above. In addition we measure the precision of the
closest translation candidate, as used in Sahlgren
and Karlgren (2005).
</bodyText>
<footnote confidence="0.9999255">
3http://lexin.nada.kth.se/sve-spa.shtml
4http://dict.tu-chemnitz.de/
</footnote>
<page confidence="0.996019">
461
</page>
<table confidence="0.999977954545454">
Weight English-German INVR Spanish-Swedish INVR
DIRECT Precision DIRECT Precision
FREQ 6.1 58% 0.97 0.8 47% 0.53
IDENTITY 6.0 58% 0.91 0.8 47% 0.53
RELFREQ 6.1 58% 0.97 0.8 47% 0.53
TF-IDF 4.9 53% 0.84 0.8 43% 0.50
TF-IDF† 6.3 58% 0.94 0.8 47% 0.53
MI 2.3 58% 0.76 0.8 48% 0.56
MILOG 2.1 58% 0.76 0.8 49% 0.56
MI± 4.6 57% 0.86 0.8 46% 0.53
MILOG± 4.6 57% 0.87 0.8 47% 0.54
TTEST 2.1 57% 0.75 0.8 48% 0.56
TTESTLOG 1.9 56% 0.72 0.8 46% 0.54
TTEST± 4.3 57% 0.85 0.8 45% 0.53
TTESTLOG± 4.0 56% 0.80 0.8 46% 0.53
GREF94 6.1 58% 0.95 0.8 48% 0.54
LIN98A 4.0 59% 0.82 0.8 48% 0.56
LIN98B 5.9 58% 0.91 0.8 48% 0.54
CHI2 3.1 50% 0.71 0.7 41% 0.48
DICE 5.7 58% 0.95 0.8 47% 0.53
DICELOG 4.7 57% 0.90 0.8 46% 0.52
LR 4.5 57% 0.86 0.8 47% 0.54
</table>
<tableCaption confidence="0.999855">
Table 4: Evaluation of bilingual lexicon extraction
</tableCaption>
<sectionHeader confidence="0.999624" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999862709677419">
Table 3 shows the results for the experiments ex-
tracting synonymy. The basic Random Indexing
algorithm (FREQ) produces a DIRECT score of
2.87, and an INVR of 0.94. It is interesting that
the only other linear weight, IDENTITY, produces
more accurate results. This shows high frequency,
low information contexts reduce the accuracy of
Random Indexing. IDENTITY removes this effect
by ignoring frequency, but does not address the
information aspect. A more accurate weight will
consider the information provided by a context in
its weighting.
There was a large variance in the effective-
ness of the other weights and most proved to be
detrimental to Random Indexing. TF-IDF was the
worst, reducing the DIRECT score to 0.30 and the
INVR to 0.07. TF-IDF†, which is a log-weighted
alternative to TF-IDF, produced very good results.
With the exception of DICELOG, adding an
additional log factor improved performance (TF-
IDF†, MILOG and TTESTLOG). Unrestricted
ranges improved the MI family, but made no dif-
ference to TTEST. Grefenstette’s variation on
TF-IDF (GREF94) does not perform as well as
TF-IDF†, and Lin’s variations on MI± (LIN98A,
LIN98B) do not perform as well as MILOG±.
MILOG± had a higher INVR than TF-IDF†, but
a lower DIRECT score, indicating that it forces
more correct results to the top of the results list,
but also forces some correct results further down
so that they no longer appear in the top 100.
</bodyText>
<table confidence="0.9991714">
Weight BNC LARGE
DIRECT INVR DIRECT INVR
FREQ 8.9 0.93 7.2 0.85
TF-IDF† 11.8 1.39 12.5 1.50
MILOG± 10.5 1.41 13.8 1.75
</table>
<tableCaption confidence="0.9355045">
Table 5: Evaluation of Random Indexing using a
very large corpus
</tableCaption>
<bodyText confidence="0.999561105263158">
The effect of high frequency contexts is in-
creased further as we increase the size of the cor-
pus. Table 5 presents results using the 2 billion
word corpus used by Curran (2004). This consists
of the non-speech portion of the BNC, the Reuter’s
Corpus Volume 1 and most of the English news
holdings of the LDC in 2003. Contexts were ex-
tracted as presented in Section 4. A frequency cut-
off of 100 was applied and the values d = 1000
and E = 5 for FREQ and E = 10 for the improved
weights were used.
We see that the very large corpus has reduced
the accuracy of frequency weighted Random In-
dexing. In contrast, our two top performers have
both substantially increased in accuracy, present-
ing a 75–100% improvment in performance over
FREQ. MILOG± is more accurate than TF-IDF†
for both measures of accuracy now, indicating it is
a better weight function for very large data sets.
</bodyText>
<subsectionHeader confidence="0.998806">
7.1 Bilingual Lexicon Acquisition
</subsectionHeader>
<bodyText confidence="0.9953905">
When the same function were applied to the bilin-
gual lexicon acquisition task we see substantially
different results: neither the improvement nor the
extremely poor results are found (Table 4).
</bodyText>
<page confidence="0.998783">
462
</page>
<figure confidence="0.974551">
INVR
0 20 40 60 80 100
Corpus Size (millions of words)
</figure>
<figureCaption confidence="0.956407">
Figure 1: Random Indexing using window-based
context
</figureCaption>
<bodyText confidence="0.999947428571429">
In the English-German corpora we replicate
Sahlgren and Karlgren’s (2005) results, with a pre-
cision of 58%. This has a DIRECT score of 6.1 and
an INVR of 0.97. The only weight to make an im-
provement is TF-IDF†, which has a DIRECT score
of 6.3, but a lower INVR and all weights perform
worse in at least one measure.
Our results for the Spanish-Swedish corpora
show similar results. Our accuracy is down from
that in Sahlgren and Karlgren (2005). This is ex-
plained by our application of the frequency cut-off
to both the source and target languages. There are
more weights with higher accuracies, and fewer
with significantly lower accuracies.
</bodyText>
<subsectionHeader confidence="0.986568">
7.2 Smaller Corpora
</subsectionHeader>
<bodyText confidence="0.999980787878788">
The absence of a substantial improvement in bilin-
gual lexicon acquisition requires further investiga-
tion. Three main factors differ between our mono-
lingual and bilingual experiments: that we are
smoothing a homogeneous data set in our mono-
lingual experiments and a heterogeneous data set
in our bilingual experiments; we are using local
grammatical contexts in our monolingual experi-
ments and paragraph contexts in our bilingual ex-
periments; and, the volume of raw data used in our
monolingual experiments is many times that used
in our bilingual experiments.
Figure 1 presents results for corpora extracted
from the BNC using the window-based context.
Results are shown for the original Random Index-
ing (FREQ) and using IDENTITY, MILOG± and
TF-IDF†, as well as for the full vector measure-
ment using JACCARD measure and the TTEST±
weight (Curran, 2004). Of the Random Index-
ing results FREQ produces the lowest overall re-
sults. It performs better than MILOG± for very
small corpora, but produces near constant results
for greater corpus sizes. Curran and Moens (2002)
found that increasing the volume of input data in-
creased the accuracy of results generated using a
full vector space model. Without weighting, Ran-
dom Indexing fails this, but after weighting is ap-
plied Curran and Moens’ results are confirmed.
The quality of context extracted influences how
weights perform individually, but Random In-
dexing using weights still outperforms not using
weights. The relative performance of MILOG±
has been reduced when compared with TF-IDF†,
but is still greater then FREQ.
Gorman and Curran (2006) showed Random In-
dexing to be much faster than full vector space
techniques, but with a 46–56% reduction in accu-
racy compared to using JACCARD and TTEST±.
Using the MI± weight kept the improvement in
speed but with only a 10–18% reduction in accu-
racy. When JACCARD and TTEST± are used with
our low quality contexts they perform consistently
worse that Random Indexing. This indicates Ran-
dom Indexing is stable in the presence of noisy
data. It would be interesting to further compare
these results to those produced by LSA.
The results we have presented have shown that
applying weights to Random Indexing can im-
prove its performance for thesaurus extraction
tasks. This improvement is dependent on the vol-
ume of raw data used to generate the context in-
formation. It is less dependent on the quality of
contexts extracted.
What we have not shown is whether this extends
to the extraction of bilingual lexicons. The bilin-
gual corpora have 12-16 million words per lan-
guage, and for this sized corpora we already see
substantial improvement with corpora as small as
5 million words (Figure 1). It may be that ex-
tracting paragraph-level contexts is not well suited
to weighting, or that the heterogeneous nature of
the aligned corpora reduces the meaningfulness of
weighting. There is also the question as to whether
it can be applied to all languages. There is a lack of
freely available large-scale multi-lingual resources
that makes this difficult to examine.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998674">
We have applied weighting functions to the vec-
tor space approximation Random Indexing. For
large data sets we found a significant improvement
</bodyText>
<figure confidence="0.9984539375">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
1.1
1
FREQ
IDENTITY
TF-IDF†
MILOG
JACCARD
</figure>
<page confidence="0.99971">
463
</page>
<bodyText confidence="0.999980952380952">
when weights were applied. For smaller data sets
we found that Random Indexing was sufficiently
robust that weighting had at most a minor effect.
Our weighting schemes removed the possibil-
ity of incremental learning of the term space. An
interesting direction would be the development of
algorithms that allowed the incremental applica-
tion of weights, perhaps by re-weighting vectors
when a new context is learned.
Other areas left open for investigation are the in-
teraction between Random Indexing, weights and
the type of context extracted, the use of large-
scale bilingual corpora, the acquisition of lexi-
cons for non-Indo-European languages and across
language family boundaries, and the difference in
effect term and paragraph/document contexts for
thesaurus extraction.
We have demonstrated that the accuracy of Ran-
dom Indexing can be improved by applying weight
functions, increasing accuracy by up to 50% on the
BNC and 100% on a 2 billion word corpus.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998772833333333">
We would like to thank Magnus Sahlgren for gen-
erously supplying his training and evaluation data
and our reviewers for their helpful feedback and
corrections. This work has been supported by
the Australian Research Council under Discovery
Project DP0453131.
</bodyText>
<sectionHeader confidence="0.999527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859945205479">
Dimitris Achlioptas. 2001. Database-friendly random pro-
jections. In Symposium on Principles of Database Sys-
tems, pages 274–281, Santa Barbara, CA, USA, 21–23
May.
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, Robert L. Mer-
cer, and Paul S. Roossin. 1988. A statistical approach
to language translation. In Proceedings of the 12th Con-
ference on Computational linguistics, pages 71–76, Bu-
dapest, Hungry, 22–27 August.
James R. Curran and Marc Moens. 2002. Scaling con-
text space. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, pages
231–238, Philadelphia, PA, USA, 7–12 July.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Christiane Fellbaum, editor. 1998. WordNet: an electronic
lexical database. The MIT Press, Cambridge, MA, USA.
James Gorman and James R. Curran. 2006. Scaling distri-
butional similarity to large corpora. In Proceedings of the
44th Annual Meeting ofthe Association for Computational
Linguistics, Sydney, Australia, 17–21 July. To appear.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Robert Hecht-Nielsen. 1994. Context vectors: general pur-
pose approximate meaning representations self-organized
from raw data. Computational Intelligence: Imitating
Life, pages 43–56.
William B. Johnson and Joram Lindenstrauss. 1984. Exten-
sions to Lipshitz mapping into Hilbert space. Contempo-
rary mathematics, 26:189–206.
Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000.
Random indexing of text samples for latent semantic anal-
ysis. In Proceedings of the 22nd Annual Conference of the
Cognitive Science Society, page 1036, Philadelphia, PA,
USA, 13–15 August.
Jussi Karlgren and Magnus Sahlgren. 2001. From words to
understanding. In Y. Uesaka, P. Kanerva, and H Asoh, ed-
itors, Foundations of Real-World Intelligence, pages 294–
308. CSLI Publications, Stanford, CA, USA.
Samuel Kaski. 1998. Dimensionality reduction by random
mapping: Fast similarity computation for clustering. In
Proceedings of the International Joint Conference on Neu-
ral Networks, pages 413–418. Piscataway, NJ, USA, 31
July–4 August.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit X, Phuket, Thai-
land, 12–16 September.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution
to plato’s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211–240, April.
Christopher D. Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing. The
MIT Press, Cambridge, MA, USA.
Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Ragha-
van, and Santosh Vempala. 1998. Latent semantic index-
ing: A probabilistic analysis. In Proceedings of the 17th
ACM Symposium on the Principle of Database Systems,
pages 159–168, Seattle, WA, USA, 2–4 June.
Magnus Sahlgren and Jussi Karlgren. 2005. Automatic bilin-
gual lexicon acquisition using random indexing of parallel
corpora. Journal of Natural Language Engineering, Spe-
cial Issue on Parallel Texts, 11(3), June.
Magnus Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Termi-
nology and Knowledge Engineering, Copenhagen, Den-
mark, 16 August.
Pasi Tapanainen and Timo J¨avinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing, pages 64–71,
31 March–3 April.
</reference>
<page confidence="0.999517">
464
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.506550">
<title confidence="0.999825">Random Indexing using Statistical Weight Functions</title>
<author confidence="0.999677">R Gorman</author>
<affiliation confidence="0.986898">School of Information University of</affiliation>
<address confidence="0.514498">NSW 2006,</address>
<abstract confidence="0.9990616">Random Indexing is a vector space technique that provides an efficient and scalable approximation to distributional similarity problems. We present experiments showing Random Indexing to be poor at handling large volumes of data and evaluate the use of weighting functions for improving the performance of Random Indexing. We find that Random Index is robust for small data sets, but performance degrades because of the influence high frequency attributes in large data sets. The use of appropriate weight functions improves this significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections. In</title>
<date>2001</date>
<booktitle>Symposium on Principles of Database Systems,</booktitle>
<pages>274--281</pages>
<location>Santa Barbara, CA, USA, 21–23</location>
<contexts>
<context position="5121" citStr="Achlioptas (2001)" startWordPosition="781" endWordPosition="782">directions in highdimensional space than there are truly orthogonal directions. The random index vectors are nearly-orthogonal, resulting in an approximate description of the context space. The approximation comes from the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kaski, 1998) are similar techniques that use this lemma. Achlioptas (2001) showed that most zero-mean distributions with unit variance, including very simple ones like that used in Random Indexing, produce a mapping that satisfies the lemma. The following description of Random Indexing is taken from Sahlgren (2005) and Sahlgren and Karlgren (2005). We allocate a d length index vector to each unique context as is it found. These vectors consist of a large number of 0s and a small number (E) of ±1s. Each element is allocated one of these values with the following probability: +1 with probability E/2d 0 with probability d�� d −1 with probability E/d2 Context vectors ar</context>
</contexts>
<marker>Achlioptas, 2001</marker>
<rawString>Dimitris Achlioptas. 2001. Database-friendly random projections. In Symposium on Principles of Database Systems, pages 274–281, Santa Barbara, CA, USA, 21–23 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th Conference on Computational linguistics,</booktitle>
<pages>71--76</pages>
<location>Budapest, Hungry, 22–27</location>
<contexts>
<context position="13192" citStr="Brown et al. (1988)" startWordPosition="2146" endWordPosition="2149">n (2004) found this extraction technique to provided reasonable results on the non-speech portion of the BNC when the data was lemmatised. We do not lemmatise, which produces noisier data. 4.2 Bilingual Lexicon Acquisition A variation on the extraction of synonymy relations, is the extraction of bilingual lexicons. This is the task of finding for a word in one language words of a similar meaning in a second language. The results of this can be used to aid manual construction of resources or directly aid translation. This task was first approached as a distributional similarity-like problem by Brown et al. (1988). Their approach uses aligned corpora in two or more languages: the source language, from which we are translating, and the target language, to which we are translating. For a each aligned segment, they measure co-occurrence scores between each word in the source segment and each word in the target segment. These co-occurrence scores are used to measure the similarity between source and target language terms Sahlgren and Karlgren’s approach models the problem as a distributional similarity problem usSource Context Target Language Language aaabbc I xxyzzz bcc II wxy aab III xzz Table 2: Paragra</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Mercer, Roossin, 1988</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, Robert L. Mercer, and Paul S. Roossin. 1988. A statistical approach to language translation. In Proceedings of the 12th Conference on Computational linguistics, pages 71–76, Budapest, Hungry, 22–27 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Marc Moens</author>
</authors>
<title>Scaling context space.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>231--238</pages>
<location>Philadelphia, PA, USA, 7–12</location>
<contexts>
<context position="2521" citStr="Curran and Moens (2002)" startWordPosition="387" endWordPosition="390">red for similarity. Existing approaches differ primarily in their definition of context, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term. In this paper, we analyse the use of Random Indexing (Kanerva et al., 2000) for semantic similarity measurement. Random Indexing is an approximation technique proposed as an alternative to Latent Semantic Analysis (LSA, Landauer and Dumais, 1997). Random Indexing is more scalable and allows for the incremental learning of context information. Curran and Moens (2002) found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted. Random Indexing performs poorly on these volumes of data. Noting that in many NLP tasks, including distributional similarity, statistical weighting is used to improve performance, we modify the Random Indexing algorithm to allow for weighted contexts. We test the performance of the original and our modified system using existing evaluation metrics. We further evaluate against bilingual lexicon extraction using distributional similarity (Sahlgren and</context>
<context position="15099" citStr="Curran and Moens, 2002" startWordPosition="2460" endWordPosition="2464">date. Sahlgren and Karlgren (2005) use Random Indexing to produce the context vectors for the source and target languages. We re-implement their system and apply weighting functions in an attempt to achieve improved results. 5 Experiments For the experiments extracting synonymy relations, high quality contexts were extracted from the non-speech portion of the British National Corpus (BNC) as described above. This represents 90% of the BNC, or 90 million words. Comparisons between low frequency terms are less accurate than between high frequency terms as there is less evidence describing them (Curran and Moens, 2002). This is compounded in randomised vector techniques because the randomised nature of the representation means that a low frequency term may have a similar context vector to a high frequency term while not sharing many contexts. A frequency cut-off of 100 was found to balance this inaccuracy with the reduction in vocabulary size. This reduces the original 246,046 word vocabulary to 14,862 words. Experiments showed d = 1000 and E = 10 to provide a balance between speed and accuracy. Low quality contexts were extracted from portions of the entire of the BNC. These formed corpora of 100,000, 500,</context>
<context position="25042" citStr="Curran and Moens (2002)" startWordPosition="4151" endWordPosition="4154">nd, the volume of raw data used in our monolingual experiments is many times that used in our bilingual experiments. Figure 1 presents results for corpora extracted from the BNC using the window-based context. Results are shown for the original Random Indexing (FREQ) and using IDENTITY, MILOG± and TF-IDF†, as well as for the full vector measurement using JACCARD measure and the TTEST± weight (Curran, 2004). Of the Random Indexing results FREQ produces the lowest overall results. It performs better than MILOG± for very small corpora, but produces near constant results for greater corpus sizes. Curran and Moens (2002) found that increasing the volume of input data increased the accuracy of results generated using a full vector space model. Without weighting, Random Indexing fails this, but after weighting is applied Curran and Moens’ results are confirmed. The quality of context extracted influences how weights perform individually, but Random Indexing using weights still outperforms not using weights. The relative performance of MILOG± has been reduced when compared with TF-IDF†, but is still greater then FREQ. Gorman and Curran (2006) showed Random Indexing to be much faster than full vector space techni</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James R. Curran and Marc Moens. 2002. Scaling context space. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 231–238, Philadelphia, PA, USA, 7–12 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6940" citStr="Curran (2004)" startWordPosition="1105" endWordPosition="1106">[2, 1, 0, −3]. The distance between these context vectors can then be measured using any vector space distance measure. Sahlgren and Karlgren (2005) use the cosine measure: Pd i=1 ~ui~vi qPd qPd i=1 ~u2 i=1 ~v2 i i Random Indexing allows for incremental sampling. This means that the entire data set need not be sampled before similarity between terms can be measured. It also means that additional context information can be added at any time without invalidating the information already produced. This is not feasible with most other word-space models. The approach used by Grefenstette (1994) and Curran (2004) requires the re-computation of all non-linear weights if new data is added, although some of these weights can be approximated when adding new data incrementally. Similarly, new data can be folded into a reduced LSA space, but there is no guarantee that the original smoothing will apply correctly to the new data (Sahlgren, 2005). 3 Weights Our initial experiments using Random Indexing to extract synonymy relations produced worse results than those using full vector measures, such as JACCARD (Curran, 2004), when the full vector is weighted. We experiment using weight functions with Random Inde</context>
<context position="8955" citStr="Curran (2004)" startWordPosition="1432" endWordPosition="1433"> 1: Weight Functions Evaluated f�w,, )  ����0�∈���∗�∗� f(w, r, w0) p�w, , )  ����∗�∗� ��∗�∗�∗� n�w, , )  |�w, , )| N�  |{w|n�w, , ) &gt; �}| also evaluated with an extra r, + 1) factor to promote the influence of higher frequency attributes, indicated by a LOG loge(f(w, w0) suffix. Alternative functions are marked with a dagger. The context vector of each term w is thus: w�= � (r, ~w0) wgt(w, r, w0) ����0�∈���∗�∗� ~ where (r, w0) is the index vector of the context log( f(w,r,w0)f(∗,r∗) LIN98A f(∗,r,w0)f(w,r,∗) ) Nw per term, which since a m is also O(dnm). Following the notation of Curran (2004), a relation is defined as a tuple (w, r, where w is a term, which occurs in some grammatical relation rwith another word in some sentence. We refer to the tuple (r, as an attribute of w. For example, (dog, direct-obj, walk) indicates that dog was the direct object of walk in a sentence. An as  context w0) w0 w0) terisk indicates the set of all existing values of that component in the tuple. �w, , )  {�r, w0)|�w, r, w0)} The frequency of a tuple, that is the number of times a word appears in a context is f(w, r, w0). f(w, , ) is the instance or token frequency of the contexts in which w</context>
<context position="12386" citStr="Curran (2004)" startWordPosition="2009" endWordPosition="2010">earch based on distance calculations between the statistical descriptions of their contexts. The simplest algorithm for finding synonyms is a k-nearest-neighbour search, which involves pairwise vector comparison of the context vector of the target term with the context vector of every other term in the vocabulary. We use two types of context extraction to produce both high and low quality context descriptions. The high quality contexts were extracted from grammatical relations extracted using the SEXTANT relation extractor (Grefenstette, 1994) and are lemmatised. This is the same data used in Curran (2004). The low quality contexts were extracted taking a window of one word to the left and right of the target term. The context is marked as to whether it preceded or followed the term. Curran (2004) found this extraction technique to provided reasonable results on the non-speech portion of the BNC when the data was lemmatised. We do not lemmatise, which produces noisier data. 4.2 Bilingual Lexicon Acquisition A variation on the extraction of synonymy relations, is the extraction of bilingual lexicons. This is the task of finding for a word in one language words of a similar meaning in a second la</context>
<context position="17040" citStr="Curran (2004)" startWordPosition="2795" endWordPosition="2796">lemmatised using Connexor Machinese (Tapanainen and J¨avinen, 1997)2 producing vocabularies of 42,671 terms of Spanish, 100,891 terms of Swedish, 40,181 terms of English and 70,384 terms of German. We use d = 600 and c = 6 and apply a frequency cutoff of 100. 6 Evaluation Measures The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard (Grefenstette, 1994). To reduce the problem of limited coverage, our evaluation of the extraction of synonyms combines three electronic thesauri: the Macquarie, Roget’s and Moby thesauri. We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR). DIRECT is the number of returned synonyms found in the gold standard. INVR is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 give an inverse rank score of 13 + 51 + 128. With at most 100 matching synonyms, the maximum INVR is 5.187. This more fine grained as it incorporates the both the number of matches and their ranking. The same 300 single word nouns were used for evaluation as used by Curran (2004) for his large scale evaluation. These were chosen randomly from WordN</context>
<context position="22274" citStr="Curran (2004)" startWordPosition="3688" endWordPosition="3689">l as MILOG±. MILOG± had a higher INVR than TF-IDF†, but a lower DIRECT score, indicating that it forces more correct results to the top of the results list, but also forces some correct results further down so that they no longer appear in the top 100. Weight BNC LARGE DIRECT INVR DIRECT INVR FREQ 8.9 0.93 7.2 0.85 TF-IDF† 11.8 1.39 12.5 1.50 MILOG± 10.5 1.41 13.8 1.75 Table 5: Evaluation of Random Indexing using a very large corpus The effect of high frequency contexts is increased further as we increase the size of the corpus. Table 5 presents results using the 2 billion word corpus used by Curran (2004). This consists of the non-speech portion of the BNC, the Reuter’s Corpus Volume 1 and most of the English news holdings of the LDC in 2003. Contexts were extracted as presented in Section 4. A frequency cutoff of 100 was applied and the values d = 1000 and E = 5 for FREQ and E = 10 for the improved weights were used. We see that the very large corpus has reduced the accuracy of frequency weighted Random Indexing. In contrast, our two top performers have both substantially increased in accuracy, presenting a 75–100% improvment in performance over FREQ. MILOG± is more accurate than TF-IDF† for </context>
<context position="24828" citStr="Curran, 2004" startWordPosition="4118" endWordPosition="4119">lingual experiments and a heterogeneous data set in our bilingual experiments; we are using local grammatical contexts in our monolingual experiments and paragraph contexts in our bilingual experiments; and, the volume of raw data used in our monolingual experiments is many times that used in our bilingual experiments. Figure 1 presents results for corpora extracted from the BNC using the window-based context. Results are shown for the original Random Indexing (FREQ) and using IDENTITY, MILOG± and TF-IDF†, as well as for the full vector measurement using JACCARD measure and the TTEST± weight (Curran, 2004). Of the Random Indexing results FREQ produces the lowest overall results. It performs better than MILOG± for very small corpora, but produces near constant results for greater corpus sizes. Curran and Moens (2002) found that increasing the volume of input data increased the accuracy of results generated using a full vector space model. Without weighting, Random Indexing fails this, but after weighting is applied Curran and Moens’ results are confirmed. The quality of context extracted influences how weights perform individually, but Random Indexing using weights still outperforms not using we</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gorman</author>
<author>James R Curran</author>
</authors>
<title>Scaling distributional similarity to large corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia,</location>
<note>To appear.</note>
<contexts>
<context position="25571" citStr="Gorman and Curran (2006)" startWordPosition="4234" endWordPosition="4237">l corpora, but produces near constant results for greater corpus sizes. Curran and Moens (2002) found that increasing the volume of input data increased the accuracy of results generated using a full vector space model. Without weighting, Random Indexing fails this, but after weighting is applied Curran and Moens’ results are confirmed. The quality of context extracted influences how weights perform individually, but Random Indexing using weights still outperforms not using weights. The relative performance of MILOG± has been reduced when compared with TF-IDF†, but is still greater then FREQ. Gorman and Curran (2006) showed Random Indexing to be much faster than full vector space techniques, but with a 46–56% reduction in accuracy compared to using JACCARD and TTEST±. Using the MI± weight kept the improvement in speed but with only a 10–18% reduction in accuracy. When JACCARD and TTEST± are used with our low quality contexts they perform consistently worse that Random Indexing. This indicates Random Indexing is stable in the presence of noisy data. It would be interesting to further compare these results to those produced by LSA. The results we have presented have shown that applying weights to Random Ind</context>
</contexts>
<marker>Gorman, Curran, 2006</marker>
<rawString>James Gorman and James R. Curran. 2006. Scaling distributional similarity to large corpora. In Proceedings of the 44th Annual Meeting ofthe Association for Computational Linguistics, Sydney, Australia, 17–21 July. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="6922" citStr="Grefenstette (1994)" startWordPosition="1102" endWordPosition="1103">context vector for t of [2, 1, 0, −3]. The distance between these context vectors can then be measured using any vector space distance measure. Sahlgren and Karlgren (2005) use the cosine measure: Pd i=1 ~ui~vi qPd qPd i=1 ~u2 i=1 ~v2 i i Random Indexing allows for incremental sampling. This means that the entire data set need not be sampled before similarity between terms can be measured. It also means that additional context information can be added at any time without invalidating the information already produced. This is not feasible with most other word-space models. The approach used by Grefenstette (1994) and Curran (2004) requires the re-computation of all non-linear weights if new data is added, although some of these weights can be approximated when adding new data incrementally. Similarly, new data can be folded into a reduced LSA space, but there is no guarantee that the original smoothing will apply correctly to the new data (Sahlgren, 2005). 3 Weights Our initial experiments using Random Indexing to extract synonymy relations produced worse results than those using full vector measures, such as JACCARD (Curran, 2004), when the full vector is weighted. We experiment using weight function</context>
<context position="12322" citStr="Grefenstette, 1994" startWordPosition="1997" endWordPosition="1998">ese terms are then compared for similarity using a nearest-neighbour search based on distance calculations between the statistical descriptions of their contexts. The simplest algorithm for finding synonyms is a k-nearest-neighbour search, which involves pairwise vector comparison of the context vector of the target term with the context vector of every other term in the vocabulary. We use two types of context extraction to produce both high and low quality context descriptions. The high quality contexts were extracted from grammatical relations extracted using the SEXTANT relation extractor (Grefenstette, 1994) and are lemmatised. This is the same data used in Curran (2004). The low quality contexts were extracted taking a window of one word to the left and right of the target term. The context is marked as to whether it preceded or followed the term. Curran (2004) found this extraction technique to provided reasonable results on the non-speech portion of the BNC when the data was lemmatised. We do not lemmatise, which produces noisier data. 4.2 Bilingual Lexicon Acquisition A variation on the extraction of synonymy relations, is the extraction of bilingual lexicons. This is the task of finding for </context>
<context position="16849" citStr="Grefenstette, 1994" startWordPosition="2767" endWordPosition="2768">e the Spanish-Swedish and the EnglishGerman portions of the Europarl corpora (Koehn, 2005).1 These consist of 37,379 aligned paragraphs in Spanish–Swedish and 45,556 in EnglishGerman. The text was lemmatised using Connexor Machinese (Tapanainen and J¨avinen, 1997)2 producing vocabularies of 42,671 terms of Spanish, 100,891 terms of Swedish, 40,181 terms of English and 70,384 terms of German. We use d = 600 and c = 6 and apply a frequency cutoff of 100. 6 Evaluation Measures The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard (Grefenstette, 1994). To reduce the problem of limited coverage, our evaluation of the extraction of synonyms combines three electronic thesauri: the Macquarie, Roget’s and Moby thesauri. We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR). DIRECT is the number of returned synonyms found in the gold standard. INVR is the sum of the inverse rank of each matching synonym, e.g. matches at ranks 3, 5 and 28 give an inverse rank score of 13 + 51 + 128. With at most 100 matching synonyms, the maximum INVR is 5.187. This more fine grained as it incorporates the both </context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Hecht-Nielsen</author>
</authors>
<title>Context vectors: general purpose approximate meaning representations self-organized from raw data. Computational Intelligence: Imitating Life,</title>
<date>1994</date>
<pages>43--56</pages>
<contexts>
<context position="4451" citStr="Hecht-Nielsen (1994)" startWordPosition="685" endWordPosition="686">me underlying dimensionality in the data, so that the attributes of two or more terms that have similar meanings can be folded onto a single axis. Sahlgren (2005) criticise LSA for being both computationally inefficient and requiring the formation of a full co-occurrence matrix and its decomposition before any similarity measurements can be made. Random Indexing avoids both these by creating a short index vector for each unique context, and producing the context vector for each term by summing index vectors for each context as it is read, allowing an incremental building of the context space. Hecht-Nielsen (1994) observed that there are many more nearly orthogonal directions in highdimensional space than there are truly orthogonal directions. The random index vectors are nearly-orthogonal, resulting in an approximate description of the context space. The approximation comes from the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kask</context>
</contexts>
<marker>Hecht-Nielsen, 1994</marker>
<rawString>Robert Hecht-Nielsen. 1994. Context vectors: general purpose approximate meaning representations self-organized from raw data. Computational Intelligence: Imitating Life, pages 43–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Johnson</author>
<author>Joram Lindenstrauss</author>
</authors>
<title>Extensions to Lipshitz mapping into Hilbert space. Contemporary mathematics,</title>
<date>1984</date>
<pages>26--189</pages>
<contexts>
<context position="4788" citStr="Johnson and Lindenstrauss, 1984" startWordPosition="730" endWordPosition="733">asurements can be made. Random Indexing avoids both these by creating a short index vector for each unique context, and producing the context vector for each term by summing index vectors for each context as it is read, allowing an incremental building of the context space. Hecht-Nielsen (1994) observed that there are many more nearly orthogonal directions in highdimensional space than there are truly orthogonal directions. The random index vectors are nearly-orthogonal, resulting in an approximate description of the context space. The approximation comes from the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kaski, 1998) are similar techniques that use this lemma. Achlioptas (2001) showed that most zero-mean distributions with unit variance, including very simple ones like that used in Random Indexing, produce a mapping that satisfies the lemma. The following description of Random Indexing is taken from Sahlgren (2005) and Sahlgren and Karlgre</context>
</contexts>
<marker>Johnson, Lindenstrauss, 1984</marker>
<rawString>William B. Johnson and Joram Lindenstrauss. 1984. Extensions to Lipshitz mapping into Hilbert space. Contemporary mathematics, 26:189–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristoferson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1036</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="2228" citStr="Kanerva et al., 2000" startWordPosition="342" endWordPosition="345">tomatically creating these resources uses distributional similarity and is based on the distributional hypothesis that similar words appear in similar contexts. Terms are described by collating information about their occurrence in a corpus into vectors. These context vectors are then compared for similarity. Existing approaches differ primarily in their definition of context, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term. In this paper, we analyse the use of Random Indexing (Kanerva et al., 2000) for semantic similarity measurement. Random Indexing is an approximation technique proposed as an alternative to Latent Semantic Analysis (LSA, Landauer and Dumais, 1997). Random Indexing is more scalable and allows for the incremental learning of context information. Curran and Moens (2002) found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted. Random Indexing performs poorly on these volumes of data. Noting that in many NLP tasks, including distributional similarity, statistical weighting is used to i</context>
<context position="3673" citStr="Kanerva et al. (2000)" startWordPosition="558" endWordPosition="561">gual lexicon extraction using distributional similarity (Sahlgren and Karlgren, 2005). The paper concludes with a more detailed analysis of Random Indexing in terms of both task and corpus composition. We find that Random Index is robust for small corpora, but larger corpora require that the contexts be weighted to maintain accuracy. 457 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457–464, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Random Indexing Random Indexing is an approximating technique proposed by Kanerva et al. (2000) as an alternative to Singular Value Decomposition (SVD) for Latent Semantic Analysis (LSA, Landauer and Dumais, 1997). In LSA, it is assumed that there is some underlying dimensionality in the data, so that the attributes of two or more terms that have similar meanings can be folded onto a single axis. Sahlgren (2005) criticise LSA for being both computationally inefficient and requiring the formation of a full co-occurrence matrix and its decomposition before any similarity measurements can be made. Random Indexing avoids both these by creating a short index vector for each unique context, a</context>
<context position="10885" citStr="Kanerva et al. (2000)" startWordPosition="1775" endWordPosition="1778">ding each the index vector for each unique context multiplied by its weight. The time to calculate the weight of all attributes of all terms is negligible. The original technique scales to O(dnm) in construction, for n terms and m unique attributes. Our new technique scales to O(d(a + nm)) for anon-zero context attributes 459 Most experiments limited weights to the positive range; those evaluated with an unrestricted range are marked with a ± suffix. Some weights were (r, 4 Semantic Similarity The first use of Random Indexing was to measure semantic similarity using distributional similarity. Kanerva et al. (2000) used Random Indexing to find the best synonym match in Test of English as a Foreign Language (TOEFL). was used by Landauer and Dumais (1997), who reported an accuracy 36% using un-normalised vectors, which was improved to 64% using LSA. Kanerva et al. (2000) produced an accuracy of the same type of document based contexts and Random Indexing, which improved to using narrow context windows. Karlgren and Sahlgren (2001) improved this to 72% using lemmatisation an TOEFL 48–51% using 62–70% d POS tagging. 4.1 Distributional Similarity Measuring distributional similarity first requires the extract</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, page 1036, Philadelphia, PA, USA, 13–15 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Magnus Sahlgren</author>
</authors>
<title>From words to understanding.</title>
<date>2001</date>
<booktitle>Foundations of Real-World Intelligence,</booktitle>
<pages>294--308</pages>
<editor>In Y. Uesaka, P. Kanerva, and H Asoh, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="11307" citStr="Karlgren and Sahlgren (2001)" startWordPosition="1845" endWordPosition="1848">icted range are marked with a ± suffix. Some weights were (r, 4 Semantic Similarity The first use of Random Indexing was to measure semantic similarity using distributional similarity. Kanerva et al. (2000) used Random Indexing to find the best synonym match in Test of English as a Foreign Language (TOEFL). was used by Landauer and Dumais (1997), who reported an accuracy 36% using un-normalised vectors, which was improved to 64% using LSA. Kanerva et al. (2000) produced an accuracy of the same type of document based contexts and Random Indexing, which improved to using narrow context windows. Karlgren and Sahlgren (2001) improved this to 72% using lemmatisation an TOEFL 48–51% using 62–70% d POS tagging. 4.1 Distributional Similarity Measuring distributional similarity first requires the extraction of context information for each of the vocabulary terms from raw text. The contexts for each term are collected together and counted, producing a vector of context attributes and their frequencies in the corpus. These terms are then compared for similarity using a nearest-neighbour search based on distance calculations between the statistical descriptions of their contexts. The simplest algorithm for finding synony</context>
</contexts>
<marker>Karlgren, Sahlgren, 2001</marker>
<rawString>Jussi Karlgren and Magnus Sahlgren. 2001. From words to understanding. In Y. Uesaka, P. Kanerva, and H Asoh, editors, Foundations of Real-World Intelligence, pages 294– 308. CSLI Publications, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Kaski</author>
</authors>
<title>Dimensionality reduction by random mapping: Fast similarity computation for clustering.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Joint Conference on Neural Networks,</booktitle>
<pages>413--418</pages>
<location>Piscataway, NJ, USA, 31 July–4</location>
<contexts>
<context position="5059" citStr="Kaski, 1998" startWordPosition="772" endWordPosition="773">994) observed that there are many more nearly orthogonal directions in highdimensional space than there are truly orthogonal directions. The random index vectors are nearly-orthogonal, resulting in an approximate description of the context space. The approximation comes from the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kaski, 1998) are similar techniques that use this lemma. Achlioptas (2001) showed that most zero-mean distributions with unit variance, including very simple ones like that used in Random Indexing, produce a mapping that satisfies the lemma. The following description of Random Indexing is taken from Sahlgren (2005) and Sahlgren and Karlgren (2005). We allocate a d length index vector to each unique context as is it found. These vectors consist of a large number of 0s and a small number (E) of ±1s. Each element is allocated one of these values with the following probability: +1 with probability E/2d 0 with</context>
</contexts>
<marker>Kaski, 1998</marker>
<rawString>Samuel Kaski. 1998. Dimensionality reduction by random mapping: Fast similarity computation for clustering. In Proceedings of the International Joint Conference on Neural Networks, pages 413–418. Piscataway, NJ, USA, 31 July–4 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="16320" citStr="Koehn, 2005" startWordPosition="2679" endWordPosition="2680">llion, 5 million, 10 460 million, 50 million and 100 million words, chosen from random documents. This allowed us test the effect of both corpus size and context quality. This produced vocabularies of between 10,380 and 522,163 words in size. Because of the size of the smallest corpora meant that a high cutoff would remove to many terms for a fair test, a cutoff of 5 was applied. The values d = 1000 and c = 6 were used. For our experiments in bilingual lexicon acquisition we follow Sahlgren and Karlgren (2005). We use the Spanish-Swedish and the EnglishGerman portions of the Europarl corpora (Koehn, 2005).1 These consist of 37,379 aligned paragraphs in Spanish–Swedish and 45,556 in EnglishGerman. The text was lemmatised using Connexor Machinese (Tapanainen and J¨avinen, 1997)2 producing vocabularies of 42,671 terms of Spanish, 100,891 terms of Swedish, 40,181 terms of English and 70,384 terms of German. We use d = 600 and c = 6 and apply a frequency cutoff of 100. 6 Evaluation Measures The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold standard (Grefenstette, 1994). To reduce the problem of limited coverage, our evaluation of the extr</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit X, Phuket, Thailand, 12–16 September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="2399" citStr="Landauer and Dumais, 1997" startWordPosition="368" endWordPosition="372"> are described by collating information about their occurrence in a corpus into vectors. These context vectors are then compared for similarity. Existing approaches differ primarily in their definition of context, e.g. the surrounding words or the entire document, and their choice of distance metric for calculating similarity between the context vectors representing each term. In this paper, we analyse the use of Random Indexing (Kanerva et al., 2000) for semantic similarity measurement. Random Indexing is an approximation technique proposed as an alternative to Latent Semantic Analysis (LSA, Landauer and Dumais, 1997). Random Indexing is more scalable and allows for the incremental learning of context information. Curran and Moens (2002) found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted. Random Indexing performs poorly on these volumes of data. Noting that in many NLP tasks, including distributional similarity, statistical weighting is used to improve performance, we modify the Random Indexing algorithm to allow for weighted contexts. We test the performance of the original and our modified system using existing </context>
<context position="3791" citStr="Landauer and Dumais, 1997" startWordPosition="575" endWordPosition="578"> more detailed analysis of Random Indexing in terms of both task and corpus composition. We find that Random Index is robust for small corpora, but larger corpora require that the contexts be weighted to maintain accuracy. 457 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457–464, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Random Indexing Random Indexing is an approximating technique proposed by Kanerva et al. (2000) as an alternative to Singular Value Decomposition (SVD) for Latent Semantic Analysis (LSA, Landauer and Dumais, 1997). In LSA, it is assumed that there is some underlying dimensionality in the data, so that the attributes of two or more terms that have similar meanings can be folded onto a single axis. Sahlgren (2005) criticise LSA for being both computationally inefficient and requiring the formation of a full co-occurrence matrix and its decomposition before any similarity measurements can be made. Random Indexing avoids both these by creating a short index vector for each unique context, and producing the context vector for each term by summing index vectors for each context as it is read, allowing an inc</context>
<context position="11026" citStr="Landauer and Dumais (1997)" startWordPosition="1800" endWordPosition="1803">erms is negligible. The original technique scales to O(dnm) in construction, for n terms and m unique attributes. Our new technique scales to O(d(a + nm)) for anon-zero context attributes 459 Most experiments limited weights to the positive range; those evaluated with an unrestricted range are marked with a ± suffix. Some weights were (r, 4 Semantic Similarity The first use of Random Indexing was to measure semantic similarity using distributional similarity. Kanerva et al. (2000) used Random Indexing to find the best synonym match in Test of English as a Foreign Language (TOEFL). was used by Landauer and Dumais (1997), who reported an accuracy 36% using un-normalised vectors, which was improved to 64% using LSA. Kanerva et al. (2000) produced an accuracy of the same type of document based contexts and Random Indexing, which improved to using narrow context windows. Karlgren and Sahlgren (2001) improved this to 72% using lemmatisation an TOEFL 48–51% using 62–70% d POS tagging. 4.1 Distributional Similarity Measuring distributional similarity first requires the extraction of context information for each of the vocabulary terms from raw text. The contexts for each term are collected together and counted, pro</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos H Papadimitriou</author>
<author>Hisao Tamaki</author>
<author>Prabhakar Raghavan</author>
<author>Santosh Vempala</author>
</authors>
<title>Latent semantic indexing: A probabilistic analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th ACM Symposium on the Principle of Database Systems,</booktitle>
<pages>159--168</pages>
<location>Seattle, WA, USA,</location>
<contexts>
<context position="5026" citStr="Papadimitriou et al., 1998" startWordPosition="765" endWordPosition="768"> building of the context space. Hecht-Nielsen (1994) observed that there are many more nearly orthogonal directions in highdimensional space than there are truly orthogonal directions. The random index vectors are nearly-orthogonal, resulting in an approximate description of the context space. The approximation comes from the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kaski, 1998) are similar techniques that use this lemma. Achlioptas (2001) showed that most zero-mean distributions with unit variance, including very simple ones like that used in Random Indexing, produce a mapping that satisfies the lemma. The following description of Random Indexing is taken from Sahlgren (2005) and Sahlgren and Karlgren (2005). We allocate a d length index vector to each unique context as is it found. These vectors consist of a large number of 0s and a small number (E) of ±1s. Each element is allocated one of these values with the following probability</context>
</contexts>
<marker>Papadimitriou, Tamaki, Raghavan, Vempala, 1998</marker>
<rawString>Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. 1998. Latent semantic indexing: A probabilistic analysis. In Proceedings of the 17th ACM Symposium on the Principle of Database Systems, pages 159–168, Seattle, WA, USA, 2–4 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
<author>Jussi Karlgren</author>
</authors>
<title>Automatic bilingual lexicon acquisition using random indexing of parallel corpora.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering, Special Issue on Parallel Texts,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="3137" citStr="Sahlgren and Karlgren, 2005" startWordPosition="476" endWordPosition="479">Moens (2002) found that dramatically increasing the volume of raw input data for distributional similarity tasks increases the accuracy of synonyms extracted. Random Indexing performs poorly on these volumes of data. Noting that in many NLP tasks, including distributional similarity, statistical weighting is used to improve performance, we modify the Random Indexing algorithm to allow for weighted contexts. We test the performance of the original and our modified system using existing evaluation metrics. We further evaluate against bilingual lexicon extraction using distributional similarity (Sahlgren and Karlgren, 2005). The paper concludes with a more detailed analysis of Random Indexing in terms of both task and corpus composition. We find that Random Index is robust for small corpora, but larger corpora require that the contexts be weighted to maintain accuracy. 457 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457–464, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Random Indexing Random Indexing is an approximating technique proposed by Kanerva et al. (2000) as an alternative to Singular Value Decomposition (SVD) for Lat</context>
<context position="5396" citStr="Sahlgren and Karlgren (2005)" startWordPosition="822" endWordPosition="825">Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kaski, 1998) are similar techniques that use this lemma. Achlioptas (2001) showed that most zero-mean distributions with unit variance, including very simple ones like that used in Random Indexing, produce a mapping that satisfies the lemma. The following description of Random Indexing is taken from Sahlgren (2005) and Sahlgren and Karlgren (2005). We allocate a d length index vector to each unique context as is it found. These vectors consist of a large number of 0s and a small number (E) of ±1s. Each element is allocated one of these values with the following probability: +1 with probability E/2d 0 with probability d�� d −1 with probability E/d2 Context vectors are generated on-the-fly. As the corpus is scanned, for each term encountered, its contexts are extracted. For each new context, an index vector is produced for it as above. The context vector is the sum of the index vectors of all the contexts in which the term appears. The c</context>
<context position="14510" citStr="Sahlgren and Karlgren (2005)" startWordPosition="2369" endWordPosition="2372">d to the words a, b and c and the target language to the words x, y and z. Three paragraphs in each of these languages are presented as pairs of translations labelled as a context: aaabbc is translated as xxyzzz and labelled context I. The frequency weighted context vector for a is {I:3, III:2} and for x is {I:2, II:1, III:1}. A translation candidate for a term in the source language is found by measuring the similarity between its context vector and the context vectors of each of the terms in the target language. The most similar target language term is the most likely translation candidate. Sahlgren and Karlgren (2005) use Random Indexing to produce the context vectors for the source and target languages. We re-implement their system and apply weighting functions in an attempt to achieve improved results. 5 Experiments For the experiments extracting synonymy relations, high quality contexts were extracted from the non-speech portion of the British National Corpus (BNC) as described above. This represents 90% of the BNC, or 90 million words. Comparisons between low frequency terms are less accurate than between high frequency terms as there is less evidence describing them (Curran and Moens, 2002). This is c</context>
<context position="16223" citStr="Sahlgren and Karlgren (2005)" startWordPosition="2662" endWordPosition="2665">ty contexts were extracted from portions of the entire of the BNC. These formed corpora of 100,000, 500,000, 1 million, 5 million, 10 460 million, 50 million and 100 million words, chosen from random documents. This allowed us test the effect of both corpus size and context quality. This produced vocabularies of between 10,380 and 522,163 words in size. Because of the size of the smallest corpora meant that a high cutoff would remove to many terms for a fair test, a cutoff of 5 was applied. The values d = 1000 and c = 6 were used. For our experiments in bilingual lexicon acquisition we follow Sahlgren and Karlgren (2005). We use the Spanish-Swedish and the EnglishGerman portions of the Europarl corpora (Koehn, 2005).1 These consist of 37,379 aligned paragraphs in Spanish–Swedish and 45,556 in EnglishGerman. The text was lemmatised using Connexor Machinese (Tapanainen and J¨avinen, 1997)2 producing vocabularies of 42,671 terms of Spanish, 100,891 terms of Swedish, 40,181 terms of English and 70,384 terms of German. We use d = 600 and c = 6 and apply a frequency cutoff of 100. 6 Evaluation Measures The simplest method for evaluation is the direct comparison of extracted synonyms with a manually created gold sta</context>
<context position="18494" citStr="Sahlgren and Karlgren (2005)" startWordPosition="3037" endWordPosition="3040">connexor.com/ Weight DIRECT INVR FREQ 2.87 0.94 IDENTITY 3.18 0.95 RELFREQ 2.87 0.94 TF-IDF 0.30 0.07 TF-IDF† 3.92 1.39 MI 1.52 0.54 MILOG 3.38 1.39 MI± 1.87 0.65 MILOG± 3.49 1.41 TTEST 1.06 0.52 TTESTLOG 1.53 0.62 TTEST± 1.06 0.52 TTESTLOG± 1.52 0.61 GREF94 2.82 0.86 LIN98A 1.52 0.50 LIN98B 2.95 0.84 CHI2 0.46 0.25 DICE 3.32 1.11 DICELOG 2.56 0.81 LR 1.96 0.58 Table 3: Evaluation of synonym extraction onyms. For each of these terms, the closest 100 terms and their similarity scores were extracted. For the evaluation of bilingual lexicon acquisition we use two online lexical resources used by Sahlgren and Karlgren (2005) as gold standards: Lexin’s online Swedish-Spanish lexicon3 and TU Chemnitz’ online English-German dictionary.4 Each of the elements in a compound or multi-word expression is treated as a potential translation. The German abblendlicht (low beam light) is treated as a translation candidate for low, beam and light separately. Low coverage is more of problem than in our thesaurus task as we have not used combined resources. There are an average of 19 translations for each of the 3,403 Spanish terms and 197 translations for each of the 4,468 English terms. The English-German translation count is s</context>
<context position="23757" citStr="Sahlgren and Karlgren (2005)" startWordPosition="3947" endWordPosition="3950">her the improvement nor the extremely poor results are found (Table 4). 462 INVR 0 20 40 60 80 100 Corpus Size (millions of words) Figure 1: Random Indexing using window-based context In the English-German corpora we replicate Sahlgren and Karlgren’s (2005) results, with a precision of 58%. This has a DIRECT score of 6.1 and an INVR of 0.97. The only weight to make an improvement is TF-IDF†, which has a DIRECT score of 6.3, but a lower INVR and all weights perform worse in at least one measure. Our results for the Spanish-Swedish corpora show similar results. Our accuracy is down from that in Sahlgren and Karlgren (2005). This is explained by our application of the frequency cut-off to both the source and target languages. There are more weights with higher accuracies, and fewer with significantly lower accuracies. 7.2 Smaller Corpora The absence of a substantial improvement in bilingual lexicon acquisition requires further investigation. Three main factors differ between our monolingual and bilingual experiments: that we are smoothing a homogeneous data set in our monolingual experiments and a heterogeneous data set in our bilingual experiments; we are using local grammatical contexts in our monolingual expe</context>
</contexts>
<marker>Sahlgren, Karlgren, 2005</marker>
<rawString>Magnus Sahlgren and Jussi Karlgren. 2005. Automatic bilingual lexicon acquisition using random indexing of parallel corpora. Journal of Natural Language Engineering, Special Issue on Parallel Texts, 11(3), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering,</booktitle>
<location>Copenhagen,</location>
<contexts>
<context position="3993" citStr="Sahlgren (2005)" startWordPosition="613" endWordPosition="614">accuracy. 457 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 457–464, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Random Indexing Random Indexing is an approximating technique proposed by Kanerva et al. (2000) as an alternative to Singular Value Decomposition (SVD) for Latent Semantic Analysis (LSA, Landauer and Dumais, 1997). In LSA, it is assumed that there is some underlying dimensionality in the data, so that the attributes of two or more terms that have similar meanings can be folded onto a single axis. Sahlgren (2005) criticise LSA for being both computationally inefficient and requiring the formation of a full co-occurrence matrix and its decomposition before any similarity measurements can be made. Random Indexing avoids both these by creating a short index vector for each unique context, and producing the context vector for each term by summing index vectors for each context as it is read, allowing an incremental building of the context space. Hecht-Nielsen (1994) observed that there are many more nearly orthogonal directions in highdimensional space than there are truly orthogonal directions. The rando</context>
<context position="5363" citStr="Sahlgren (2005)" startWordPosition="819" endWordPosition="820"> lemma (Johnson and Lindenstrauss, 1984), which states that if we project points in a vector space into a randomly selected subspace of sufficiently high dimensionality, the distances between the points are approximately preserved. Random Projection (Papadimitriou et al., 1998) and Random Mapping (Kaski, 1998) are similar techniques that use this lemma. Achlioptas (2001) showed that most zero-mean distributions with unit variance, including very simple ones like that used in Random Indexing, produce a mapping that satisfies the lemma. The following description of Random Indexing is taken from Sahlgren (2005) and Sahlgren and Karlgren (2005). We allocate a d length index vector to each unique context as is it found. These vectors consist of a large number of 0s and a small number (E) of ±1s. Each element is allocated one of these values with the following probability: +1 with probability E/2d 0 with probability d�� d −1 with probability E/d2 Context vectors are generated on-the-fly. As the corpus is scanned, for each term encountered, its contexts are extracted. For each new context, an index vector is produced for it as above. The context vector is the sum of the index vectors of all the contexts</context>
<context position="7271" citStr="Sahlgren, 2005" startWordPosition="1159" endWordPosition="1160">milarity between terms can be measured. It also means that additional context information can be added at any time without invalidating the information already produced. This is not feasible with most other word-space models. The approach used by Grefenstette (1994) and Curran (2004) requires the re-computation of all non-linear weights if new data is added, although some of these weights can be approximated when adding new data incrementally. Similarly, new data can be folded into a reduced LSA space, but there is no guarantee that the original smoothing will apply correctly to the new data (Sahlgren, 2005). 3 Weights Our initial experiments using Random Indexing to extract synonymy relations produced worse results than those using full vector measures, such as JACCARD (Curran, 2004), when the full vector is weighted. We experiment using weight functions with Random Indexing. Only a linear weighting scheme can be applied while maintaining incremental sampling. While incremental sampling is part of the rationale behind its development, it is not required for Random Indexing to work as a dimensionality reduction technique. To this end, we revise Random Indexing to enable us to use weight functions</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, Copenhagen, Denmark, 16 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨avinen</author>
</authors>
<title>A non-projective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<marker>Tapanainen, J¨avinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨avinen. 1997. A non-projective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71, 31 March–3 April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>