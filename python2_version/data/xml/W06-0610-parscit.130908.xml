<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.950513">
On Distance between Deep Syntax and Semantic Representation
</title>
<author confidence="0.95888">
V´aclav Nov´ak
</author>
<affiliation confidence="0.9029625">
Institute of Formal and Applied Linguistics
Charles University
</affiliation>
<address confidence="0.522635">
Praha, Czech Republic
</address>
<email confidence="0.996574">
novak@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996894">
We present a comparison of two for-
malisms for representing natural language
utterances, namely deep syntactical Tec-
togrammatical Layer of Functional Gen-
erative Description (FGD) and a seman-
tic formalism, MultiNet. We discuss the
possible position of MultiNet in the FGD
framework and present a preliminary map-
ping of representational means of these
two formalisms.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988893096774194">
The Prague Dependency Treebank 2.0 (PDT 2.0)
described in Sgall et al. (2004) contains a large
amount of Czech texts with complex and inter-
linked morphological (2 million words), syntactic
(1.5M words), and complex semantic (tectogram-
matical) annotation (0.8M words); in addition,
certain properties of sentence information struc-
ture and coreference relations are annotated at the
semantic level.
The theoretical basis of the treebank lies in the
Functional Generative Description (FGD) of lan-
guage system by Sgall et al. (1986).
PDT 2.0 is based on the long-standing Praguian
linguistic tradition, adapted for the current
computational-linguistics research needs. The
corpus itself is embedded into the latest annotation
technology. Software tools for corpus search, an-
notation, and language analysis are included. Ex-
tensive documentation (in English) is provided as
well.
An example of a tectogrammatical tree from
PDT 2.0 is given in figure 1. Function words are
removed, their function is preserved in node at-
tributes (grammatemes), information structure is
annotated in terms of topic-focus articulation, and
every node receives detailed semantic label corre-
sponding to its function in the utterance (e.g., ad-
dressee, from where, how often, ... ). The square
node indicates an obligatory but missing valent.
The tree represents the following sentence:
This year he tries to return to politics.
</bodyText>
<figure confidence="0.9948359">
(1)
t-ln94200-123-p12s3
root
letos
t TWHEN.basic
adv.denot.ngrad.nneg
politika
f DIR3.basic
n.denot
fem.sg
</figure>
<figureCaption confidence="0.99963">
Figure 1: Tectogrammatical tree of sentence (1)
</figureCaption>
<subsectionHeader confidence="0.952438">
1.1 MultiNet
</subsectionHeader>
<bodyText confidence="0.986467">
The representational means of Multilayered Ex-
tended Semantic Networks (MultiNet), which are
</bodyText>
<figure confidence="0.995129423076923">
Letos
CC�C!
�
o
X V
n´avrat
V
politiky.
V
do
V
se snaˇzi
snažit_se enunc
.
f PRED
v decl disp0 ind
. .
proc.it0.res0.sim
#PersPron
t ACT
n.pron.def.pers
anim.sg.3.basic
nAvrat
f PAT
n.denot
inan.sg
</figure>
<page confidence="0.979986">
78
</page>
<note confidence="0.7073655">
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 78–85,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.994150813559322">
described in Helbig (2006), provide a universally
applicable formalism for treatment of semantic
phenomena of natural language. To this end, they
offer distinct advantages over the use of the clas-
sical predicate calculus and its derivatives. The
knowledge representation paradigm and semantic
formalism MultiNet is used as a common back-
bone for all aspects of natural language process-
ing (be they theoretical or practical ones). It is
continually used for the development of intelligent
information and communication systems and for
natural language interfaces to the Internet. Within
this framework, it is subject to permanent practical
evaluation and further development.
The semantic representation of natural language
expressions by means of MultiNet is mainly in-
dependent of the considered language. In con-
trast, the syntactic constructs used in different
languages to describe the same content are ob-
viously not identical. To bridge the gap be-
tween different languages we can employ the deep
syntactico-semantic representation available in the
FGD framework.
An example of a MultiNet structure is given in
figure 2. The figure represents the following dis-
course:
Max gave his brother several apples.
This was a generous gift.
Four of them were rotten.
(2)
MultiNet is not explicitly model-theoretical and
the extensional level is created only in those situ-
ations where the natural language expressions re-
quire it. It can be seen that the overall structure
of the representation is not a tree unlike in Tec-
togrammatical representation (TR). The layer in-
formation is hidden except for the most important
QUANT and CARD values. These attributes con-
vey information that is important with respect to
the content of the sentence. TR lacks attributes
distinguishing intensional and extensional infor-
mation and there are no relations like SUBM de-
noting relation between a set and its subset.
Note that the MultiNet representation crosses
the sentence boundaries. First, the structure repre-
senting a sentence is created and then this structure
is assimilated into the existing representation.
In contrast to CLASSIC (Brachman et al., 1991)
and other KL-ONE networks, MultiNet contains a
predefined final set of relation types, encapsula-
tion of concepts, and attribute layers concerning
cardinality of objects mentioned in discourse.
In Section 2, we describe our motivation for ex-
tending the annotation in FGD to an even deeper
level. Section 3 lists the MultiNet structural coun-
terparts of tectogrammatical means. We discuss
the related work in Section 4. Section 5 deals with
various evaluation techniques and we conclude in
Section 6.
</bodyText>
<sectionHeader confidence="0.990805" genericHeader="introduction">
2 FGD layers
</sectionHeader>
<bodyText confidence="0.998981428571429">
PDT 2.0 contains three layers of information about
the text (as described in Hajiˇc (1998)):
Morphosyntactic Tagging. This layer represents
the text in the original linear word order with
a tag assigned unambiguously to each word
form occurence, much like the Brown corpus
does.
</bodyText>
<subsectionHeader confidence="0.632308">
Syntactic Dependency Annotation. It contains
</subsectionHeader>
<bodyText confidence="0.999381">
the (unambiguous) dependency representa-
tion of every sentence, with features describ-
ing the morphosyntactic properties, the syn-
tactic function, and the lexical unit itself. All
words from the sentence appear in its repre-
sentation.
</bodyText>
<subsectionHeader confidence="0.622503">
Tectogrammatical Representation (TR). At
</subsectionHeader>
<bodyText confidence="0.999356090909091">
this level of description, we annotate every
(autosemantic non-auxiliary) lexical unit
with its tectogrammatical function, position
in the scale of the communicative dynamism
and its grammatemes (similar to the mor-
phosyntactic tag, but only for categories
which cannot be derived from the word’s
function, like number for nouns, but not its
case).
There are several reasons why TR may not be
sufficient in a question answering system or MT:
</bodyText>
<listItem confidence="0.977664375">
1. The syntactic functors Actor and Patient dis-
allow creating inference rules for cognitive
roles like Affected object or State carrier. For
example, the axiom stating that an affected
object is changed by the event ((v AFF o) —*
(v SUBS change.2.1)) can not be used
in the TR framework.
2. There is no information about sorts of con-
</listItem>
<bodyText confidence="0.7718">
cepts represented by TR nodes. Sorts (the
upper conceptual ontology) are an important
source of constraints for MultiNet relations.
Every relation has its signature which in turn
</bodyText>
<page confidence="0.998842">
79
</page>
<figureCaption confidence="0.998456">
Figure 2: MultiNet representation of example discourse (2)
</figureCaption>
<bodyText confidence="0.926139">
reduces ambiguity in the process of text anal-
ysis and inferencing.
</bodyText>
<listItem confidence="0.899262125">
3. Lexemes of TR have no hierarchy which lim-
its especially the search for an answer in a
question answering system. In TR there is
no counterpart of SUB, SUBR, and SUBS
MultiNet relations which connect subordi-
nate concepts to superordinate ones and indi-
vidual object representatves to corresponding
generic concepts.
4. In TR, each sentence is isolated from the
rest of the text, except for coreference arrows
heading to preceding sentences. This, in ef-
fect, disallows inferences combining knowl-
edge from multiple sentences in one infer-
ence rule.
5. Nodes in TR always correspond to a word
or a group of words in the surface form of
sentence or to a deleted obligatory valency
of another node. There are no means for
representing knowledge generated during the
inference process, if the knowledge doesn’t
have a form of TR. For example, consider ax-
iom of temporal precedence transitivity (3):
(a ANTE b) n (b ANTE c) —* (a ANTE c)
(3)
</listItem>
<bodyText confidence="0.971453153846154">
In TR, we can not add an edge denoting
(a ANTE c). We would have to include a
proposition like “a precedes c” as a whole
new clause.
For all these reasons we need to extend our text
annotation to a form suitable to more advanced
tasks. It is shown in Helbig (2006) that MultiNet
is capable to solve all the above mentioned issues.
Helbig (1986) describes a procedure for auto-
matic translation of natural language utterances
into MultiNet structures used in WOCADI tool for
German. WOCADI uses no theoretical intermedi-
ate structures and relies heavily on semantically
annotated dictionary (HagenLex, see Hartrumpf et
al. (2003)).
In our approach, we want to take advantage of
existing tools for conversions between layers in
FGD. By combining several simpler procedures
for translation between adjacent layers, we can im-
prove the robustness of the whole procedure and
the modularity of the software tools. Moreover,
the process is divided to logical steps correspond-
ing to theoretically sound and well defined struc-
tures. On the other hand, such a multistage pro-
cessing is susceptible to accumulation of errors
made by individual components.
</bodyText>
<sectionHeader confidence="0.969282" genericHeader="method">
3 Structural Similarities
</sectionHeader>
<subsectionHeader confidence="0.9984">
3.1 Nodes and Concepts
</subsectionHeader>
<bodyText confidence="0.999988272727273">
If we look at examples of TR and MultiNet struc-
tures, at first sight we can see that the nodes of
TR mostly correspond to concepts in MultiNet.
However, there is a major difference: TR does not
include the concept encapsulation. The encapsu-
lation in MultiNet serves for distinguishing def-
initional knowledge from assertional knowledge
about given node, e.g., in the sentence “The old
man is sleeping”, the connection to old will be in
the definitional part of man, while the connection
to the state is sleeping belongs to the assertional
</bodyText>
<page confidence="0.981772">
80
</page>
<bodyText confidence="0.978851818181818">
part of the concept representing the man. In TR,
these differences in content are represented by dif-
ferences in Topic-Focus Articulation (TFA) of cor-
responding words.
There are also TR nodes that correspond to no
MultiNet concept (typically, the node representing
the verb “be”) and TR nodes corresponding to a
whole subnetwork, e.g., Fred in the sentence “Fred
is going home.”, where the TR node representing
Fred corresponds to the subnetwork&apos; in figure 3.
first name
</bodyText>
<figureCaption confidence="0.992958">
Figure 3: The MultiNet subnetwork correspond-
ing to TR node representing Fred
</figureCaption>
<subsectionHeader confidence="0.999847">
3.2 Edges, relations and functions
</subsectionHeader>
<bodyText confidence="0.99982725">
An edge of TR between nodes that have their
conceptual counterparts in MultiNet always corre-
sponds to one or more relations and possibly also
some functions. In general, it can be said that
MultiNet representation of a text contains signif-
icantly more connections (either as relations, or as
functions) than TR, and some of them correspond
to TR edges.
</bodyText>
<subsectionHeader confidence="0.9849465">
3.3 Functors and types of relations and
functions
</subsectionHeader>
<bodyText confidence="0.9998832">
There are 67 functor types in TR (see Hajiˇcov´a
et al. (2000) for description), which correspond to
94 relation types and 19 function types in Multi-
Net (Helbig, 2006). The mapping of TR functions
to MultiNet is given in table 1:
</bodyText>
<sectionHeader confidence="0.98326575" genericHeader="method">
TR functor MultiNet counterpart
ACMP ASSOC
ACT AFF, AGT, BENF, CSTR, EXP,
ADDR MEXP, SCAR
ADVS ORNT
AIM SUBST, OPPOS
PURP
APP ASSOC, ATTCH
</sectionHeader>
<bodyText confidence="0.86408925">
continued...
&apos;In fact the concept representing the man is the concept
G01, i.e. only one vertex. However, the whole network cor-
responds to the TR node representing Fred.
</bodyText>
<figure confidence="0.962860701030928">
continued...
fred
VAL
G01
human SUB
VIA
PURP
MOD
MODL
TR functor
MultiNet counterpart
*ITMS, MODL
PROP except for sentential com-
plements
COND
CONFR
COND
OPPOS
*IMTS-I, *TUPL
OPPOS
CONC
*COMP
METH, JUST, CIRC, CONF
CAUS, JUST, GOAL
*MODP, *OP
ORIGL, ORIG
DIRCL, ELMT
*ALTN2, *VEL2
MCONT, PROP, RSLT
QMOD
AVRT
NAME
LOC
MANN
MAT
MEANS
LOC,LEXT
MANNR, METH
ORIGM
MODE, INSTR
*OP, TEMP
AVRT, INIT, ORIGM, ORIGL,
ORIG
PARTL
MODL
AFF, ATTR, BENF, ELMT,
GOAL, OBJ, PARS, PROP,
SSPE, VAL
REAS, OPPOS
CAUS, GOAL
CONF
CAUS, GOAL
*DIFF
MODL
PROP, ATTR
SUBST
PAT
EQU, NAME
MODL
AGT, ORIG
BENF
CAUS, JUST
CONC
APPS
ATT
AUTH
BEN
CAUS
CNCS
CM
CONJ
CONTRA
CONTRD
CPR
CRIT
CSQ
DIFF
DIR1
DIR2
DIR3
DISJ
EFF
EXT
HER
ID
INTT
OPER
ORIG
COMPL
PREC
REAS
REG
RESL
RESTR
RHEM
RSTR
SUBS
</figure>
<page confidence="0.270212">
81
</page>
<table confidence="0.562104875">
TR functor
MultiNet counterpart
MultiNet
TR counterpart
TFHL
TFRWH
THL
THO
TOWH
TPAR
TSIN
TTILL
TWHEN
DUR
TEMP
DUR
QUANT layer
SUBST, TEMP
TEMP, DUR
STRT
FIN
TEMP
PAT, EFF
see PARS, ORIGM, *ELMT,
*SUBM and TEMP
MANN, CRIT
ACT
see INSTR, METH and
MANNR
MOD, ATT, PARTL, RHEM
ID, APPS
PAT
MCONT
MERO
METH
MEXP
MODE
MODL
NAME
OBJ
</table>
<tableCaption confidence="0.821722666666667">
Table 1: Mapping of TR functors to MultiNet
There are also TR functors with no appropriate
MultiNet counterpart: CPHR, DENOM, DPHR,
FPHR, GRAD, INTF, PAR, PRED and VOCAT
Table 2 shows the mapping from MultiNet rela-
tions to TR functors:
</tableCaption>
<figure confidence="0.893800428571429">
CONTRA
ORIG, DIR1, AUTH
DIR1
ORIG
ADDR
COMPL, RSTR
OPPOS
ORIG
ORIGL
ORIGM
ORNT
PROP
PROPR
COMPL, RSTR
</figure>
<sectionHeader confidence="0.933895491803279" genericHeader="method">
Relations:
AFF
AGT
ANTE
ARG1/2/3
ASSOC
ATTCH
ATTR
AVRT
BENF
CAUS
CIRC
CONC
COND
CONF
CSTR
CTXT
DIRCL
DUR
ELMT
EXP
FIN
GOAL
IMPL
INIT
INSTR
JUST
LEXT
LOC
MANNR
PURP
QMOD
REAS
RPRS
RSLT
SCAR
SITU
SOURC
SSPE
STRT
SUBST
SUPPL
TEMP
VAL
VIA
Functions:
*ALTN1
*ALTN1
*COMP
*DIFF
*INTSC
*ITMS
*MODP
*MODQ
*MODS
*NON
*ORD
*PMOD
*QUANT
AIM
RSTR
</sectionHeader>
<bodyText confidence="0.2233735">
see CAUS, JUST and IMPL
LOC, MANN
PAT, EFF
ACT
see CIRC and CTXT
see INIT, ORIG, ORIGL,
</bodyText>
<sectionHeader confidence="0.871914666666667" genericHeader="method">
ORIGM and AVRT
PAT
TSIN
SUBS
PAT
TWHEN
RSTR, PAT
DIR2
CONJ
DISJ
CPR, grammateme DEGCMP
RESTR
CONJ
CONJ
MANN
RHEM
MANNR
grammateme NEGATION
grammateme NUMERTYPE
RSTR
MAT, RSTR
</sectionHeader>
<figure confidence="0.972888111111111">
PAT, DIR1
ACT
TWHEN
ACT, PAT, .. .
ACMP, APP
APP
RSTR
ORIG, ADDR, DIR1
BEN
CAUS, RESL, REAS, GOAL
CRIT
CNCS
COND
REG, CRIT
ACT
REG
DIR3
TFHL, PAR, THL
DIR3, DIR1
ACT
TTILL
see RSLT, DIRCL and PURP
CAUS
ORIG
MEANS
CAUS
LOC
LOC
MANN
MultiNet
TR counterpart
continued...
continued...
82
TR counterpart
grammateme DEGCMP
6. INDEFTYPE corresponds to QUANT and
VARIA layer attributes.
MultiNet
*SUPL
*TUPL
CONJ
*UNION
*VEL1
*VEL2
</figure>
<tableCaption confidence="0.993686">
Table 2: Mapping of MultiNet relations to TR
</tableCaption>
<bodyText confidence="0.999716214285714">
There are also MultiNet relations and functions
with no counterpart in TR (stars at the begin-
ning denote a function): ANLG, ANTO, CHEA,
CHPA, CHPE, CHPS, CHSA CHSP, CNVRS,
COMPL, CONTR, CORR, DISTG, DPND, EQU,
EXT, HSIT, MAJ, MIN, PARS, POSS, PRED0,
PRED, PREDR, PREDS, SETOF, SUB, SYNO,
VALR, *FLPJ and *OP.
From the tables 1 and 2, we can conclude that
although the mapping is not one to one, the prepro-
cessing of the input text to TR highly reduces the
problem of the appropriate text to MultiNet trans-
formation. However, it is not clear how to solve
the remaining ambiguity.
</bodyText>
<subsectionHeader confidence="0.996979">
3.4 Grammatemes and layer information
</subsectionHeader>
<bodyText confidence="0.999882166666667">
TR has at its disposal 15 grammatemes, which
can be conceived as node attributes. Note that
not all grammatemes are applicable to all nodes.
The grammatemes in TR roughly correspond to
layer information in MultiNet, but also to specific
MultiNet relations.
</bodyText>
<listItem confidence="0.905777888888889">
1. NUMBER. This TR grammateme is trans-
formed to QUANT, CARD, and ETYPE at-
tributes in MultiNet.
2. GENDER. This syntactical information is not
transformed to the semantic representation
with the exception of occurences where the
grammateme distinguishes the gender of an
animal or a person and where MultiNet uses
SUB relation with appropriate concepts.
3. PERSON. This verbal grammateme is re-
flected in cognitive roles connected to the
event or state and is semantically superfluous.
4. POLITENESS has no structural counterpart
in MultiNet. It can be represented in the con-
ceptual hierarchy of SUB relation.
5. NUMERTYPE distinguishing e.g. “three”
from “third” and “one third” is transformed to
corresponding number and also to the manner
this number is connected to the network.
7. NEGATION is transformed to both FACT
layer attribute and *NON function combined
with modality relation.
8. DEGCMP corresponds to *COMP and
*SUPL functions.
9. VERBMOD: imp value is represented by
MODL relation to imperative, cdn value is
ambiguous not only with respect to facticity
of the condition but also with regard to other
criteria distinguishing CAUS, IMPL, JUST
and COND relatinos which can all result in
a sentence with cdn verb. Also the FACT
layer attribute of several concepts is affected
by this value.
10. DEONTMOD corresponds to MODL rela-
tion.
11. DISPMOD is semantically superfluous.
</listItem>
<bodyText confidence="0.921908615384615">
12. ASPECT has no direct counterpart in Multi-
Net. It can be represented by the interplay
of temporal specification and RSLT relation
connecting an action to its result.
13. TENSE is represented by relations ANTE,
TEMP, DUR, STRT, and FIN.
14. RESULTATIVE has no direct counterpart
and must be expressed using the RSLT rela-
tion.
15. ITERATIVENESS should be represented by
a combination of DUR and TEMP rela-
tions where some of temporal concepts have
QUANT layer information set to several.
</bodyText>
<subsectionHeader confidence="0.982272">
3.5 TFA, quantifiers, and encapsulation
</subsectionHeader>
<bodyText confidence="0.95356325">
In TR, the information structure of every utterance
is annotated in terms of Topic-Focus Articulation
(TFA):
1. Every autosemantic word is marked c, t, or
f for contrastive topic, topic, or focus, re-
spectively. The values can distinguish which
part of the sentence belongs to topic and
which part to focus.
</bodyText>
<listItem confidence="0.461338">
2. There is an ordering of all nodes according to
communicative dynamism (CD). Nodes with
lower values of CD belong to topic and nodes
</listItem>
<figure confidence="0.672036333333333">
CONJ
CONJ
DISJ
</figure>
<page confidence="0.996045">
83
</page>
<bodyText confidence="0.993448483870968">
with greater values to focus. In this way, the
degree of “aboutness” is distinguished even
inside topic and focus of sentences.
MultiNet, on the other hand, doesn’t contain
any representational means devoted directly to
representation of information structure. Neverthe-
less, the differences in the content of sentences dif-
fering only in TFA can be represented in MultiNet
by other means. The TFA differences can be re-
flected in these categories:
• Relations connecting the topic of sentence
with the remaining concepts in the sentence
are usually a part of definitional knowledge
about the concepts in the topic, while the re-
lations going to the focus belong to the asser-
tional part of knowledge about the concepts
in focus. In other words, TFA can be reflected
in different values of K TYPE attribute.
• TFA has an effect on the identification of
presuppositions (Peregrin, 1995a) and allega-
tions (Hajiˇcov´a, 1984). In case of presuppo-
sition, we need to know about them in the
process of assimilation of new information
into the existing network in order to detect
presupposition failures. In case of allegation,
there is a difference in FACT attribute of the
allegation.
• The TFA has an influence on the scope of
quantifiers (Peregrin, 1995b; Hajiˇcov´a et al.,
1998). This information is fully transformed
into the quantifier scopes in MultiNet.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999954">
There are various approaches trying to analyze
text to a semantic representation. Some of them
use layered approach and others use only a sin-
gle tool to directly produce the target struc-
ture. For German, there is the above mentioned
WOCADI parser to MultiNet, for English, there
is a Discourse Representation Theory (DRT) ana-
lyzer (Bos, 2005), and for Czech there is a Trans-
parent Intensional Logic analyzer (Hor´ak, 2001).
The layered approaches: DeepThought
project (Callmeier et al., 2004) can combine
output of various tools into one representation.
It would be even possible to incorporate TR and
MultiNet into this framework. Meaning-Text
Theory (Bolshakov and Gelbukh, 2000) uses
an approach similar to Functional Generative
Description (ˇZabokrtsk´y, 2005) but it also has no
layer corresponding to MultiNet.
There were attempts to analyze the seman-
tics of TR, namely in question answering system
TIBAQ (Jirk˚u and Hajiˇc, 1982), which used TR di-
rectly as the semantic representation, and Kruijff-
Korbayov´a (1998), who tried to transform the TFA
information in TR into the DRT framework.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999736">
It is a still open question how to evaluate systems
for semantic representation. Basically, three ap-
proaches are used in similar projects:
First, the coverage of the system may serve as a
basis for evaluation. This criterion is used in sev-
eral systems (Bos, 2005; Hor´ak, 2001; Callmeier
et al., 2004). However, this criterion is far from
ideal, because it’s not applicable to robust systems
and can not tell anything about the quality of re-
sulting representation.
Second, the consistency of the semantic repre-
sentation serves as an evaluation criterion in Bos
(2005). It is a desired state to have a consistent
representation of texts, but there is no guarantee
that a consistent semantic representation is in any
sense also a good one.
Third, the performance in an application
(e.g., question answering system) is another cri-
terion used for evaluating a semantic representa-
tion (Hartrumpf, 2005). A problem in this kind
of evaluation is that we can not separate the eval-
uation of the formalism itself from the evaluation
of the automatic processing tools. This problem
becomes even bigger in a multilayered approach
like FGD or MTT, where the overall performance
depends on all participating transducers as well as
on the quality of the theoretical description. How-
ever, from the user point of view, this is so far
the most reliable form of semantic representation
evaluation.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999992333333333">
We have presented an outline of a procedure that
enables us to transform syntactical (tectogrammat-
ical) structures into a fully equipped knowledge
representation framework. We have compared
the structural properties of TR and MultiNet and
found both similarities and differences suggest-
ing which parts of such a task are more difficult
and which are rather technical. The comparison
shows that for applications requiring understand-
</bodyText>
<page confidence="0.992436">
84
</page>
<bodyText confidence="0.998335666666667">
ing of texts (e.g., question answering system) it is
desirable to further analyze TR into another layer
of knowledge representation.
</bodyText>
<sectionHeader confidence="0.974827" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999532">
This work was supported by Czech Academy
of Science grant 1ET201120505 and by Czech
Ministry of Education, Youth and Sports project
LC536. The views expressed are not necessarily
endorsed by the sponsors. We also thank anony-
mous reviewers for improvements in the final ver-
sion.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857634408602">
Igor Bolshakov and Alexander Gelbukh. 2000. The
Meaning-Text Model: Thirty Years After. Interna-
tional Forum on Information and Documentation,
1:10–16.
Johan Bos. 2005. Towards Wide-Coverage Se-
mantic Interpretation. In Proceedings of Sixth In-
ternational Workshop on Computational Semantics
IWCS-6, pages 42–53.
Ronald J. Brachman, Deborah L. McGuinness, Pe-
ter F. Patel-Schneider, Lori Alperin Resnick, and
Alex Borgida. 1991. Living with CLASSIC: When
and How to Use a KL-ONE-like Language. In John
Sowa, editor, Principles of Semantic Networks: Ex-
plorations in the representation of knowledge, pages
401–456. Morgan-Kaufmann, San Mateo, Califor-
nia.
Ulrich Callmeier, Andreas Eisele, Ulrich Sch¨afer, and
Melanie Siegel. 2004. The DeepThought Core Ar-
chitecture Framework. In Proceedings of LREC,
May.
Jan Hajiˇc. 1998. Building a Syntactically Anno-
tated Corpus: The Prague Dependency Treebank. In
E. Hajiˇcov´a, editor, Issues of Valency and Meaning.
Studies in Honour ofJarmila Panevov´a, pages 106–
132. Karolinum, Charles University Press, Prague,
Czech Republic.
Eva Hajiˇcov´a, Jarmila Panevov´a, and Petr Sgall.
2000. A Manual for Tectogrammatic Tagging of
the Prague Dependency Treebank. Technical Re-
port TR-2000-09, ´UFAL MFF UK, Prague, Czech
Republic. in Czech.
Eva Hajiˇcov´a, Petr Sgall, and Barbara Partee. 1998.
Topic-Focus Articulation, Tripartite Structures, and
Semantic Content. Kluwer, Dordrecht.
Eva Hajiˇcov´a. 1984. Presupposition and Allegation
Revisited. Journal ofPragmatics, 8:155–167.
Sven Hartrumpf, Hermann Helbig, and Rainer Oss-
wald. 2003. The Semantically Based Computer
Lexicon HaGenLex – Structure and Technological
Environment. Traitement automatique des langues,
44(2):81–105.
Sven Hartrumpf. 2005. University of hagen at qa@clef
2005: Extending knowledge and deepening linguis-
tic processing for question answering. In Carol
Peters, editor, Results of the CLEF 2005 Cross-
Language System Evaluation Campaign, Work-
ing Notes for the CLEF 2005 Workshop, Wien,
¨Osterreich. Centromedia.
Hermann Helbig. 1986. Syntactic-Semantic Analy-
sis of Natural Language by a New Word-Class Con-
trolled Functional Analysis. Computers and Artifi-
cialInteligence, 5(1):53–59.
Hermann Helbig. 2006. Knowledge Representation
and the Semantics of Natural Language. Springer-
Verlag, Berlin Heidelberg.
Aleˇs Hor´ak. 2001. The Normal Translation Algorithm
in Transparent Intensional Logic for Czech. Ph.D.
thesis, Faculty of Informatics, Masaryk University,
Brno, Czech Republic.
Petr Jirk˚u and Jan Hajiˇc. 1982. Inferencing and search
for an answer in TIBAQ. In Proceedings of the 9th
conference on Computational linguistics – Volume
2, pages 139–141, Prague, Czechoslovakia.
Ivana Kruijff-Korbayov´a. 1998. The Dynamic Po-
tential of Topic and Focus: A Praguian Approach
to Discourse Representation Theory. Ph.D. thesis,
´UFAL, MFF UK, Prague, Czech Republic.
Jaroslav Peregrin. 1995a. Topic, Focus and the Logic
of Language. In Sprachtheoretische Grundlagen f¨ur
die Computerlinguistik (Proceedings of the Goettin-
gen Focus Workshop, 17. DGfS), Heidelberg. IBM
Deutschland.
Jaroslav Peregrin. 1995b. Topic-Focus Articulation
as Generalized Quantification. In P. Bosch and
R. van der Sandt, editors, Proceedings of “Focus and
natural language processing”, pages 49–57, Heidel-
berg. IBM Deutschland.
Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing company,
Dodrecht, Boston, London.
Petr Sgall, Jarmila Panevov´a, and Eva Hajiˇcov´a. 2004.
Deep Syntactic Annotation: Tectogrammatical Rep-
resentation and Beyond. In A. Meyers, editor, Pro-
ceedings of the HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, pages 32–38, Boston,
Massachusetts, USA. Association for Computa-
tional Linguistics.
Zdenˇek ˇZabokrtsk´y. 2005. Resemblances between
Meaning-Text Theory and Functional Generative
Description. In Proceedings of the 2nd Interna-
tional Conference of Meaning-Text Theory, pages
549–557.
</reference>
<page confidence="0.999699">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584454">
<title confidence="0.999846">On Distance between Deep Syntax and Semantic Representation</title>
<author confidence="0.920931">V´aclav</author>
<affiliation confidence="0.805171">Institute of Formal and Applied Charles</affiliation>
<address confidence="0.78727">Praha, Czech</address>
<email confidence="0.944421">novak@ufal.mff.cuni.cz</email>
<abstract confidence="0.998907181818182">We present a comparison of two formalisms for representing natural language namely deep syntactical Tec- Layer Functional Generative Description (FGD) and a semanformalism, We discuss the possible position of MultiNet in the FGD framework and present a preliminary mapping of representational means of these two formalisms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Igor Bolshakov</author>
<author>Alexander Gelbukh</author>
</authors>
<title>The Meaning-Text Model: Thirty Years After.</title>
<date>2000</date>
<booktitle>International Forum on Information and Documentation,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="18901" citStr="Bolshakov and Gelbukh, 2000" startWordPosition="3059" endWordPosition="3062">o a semantic representation. Some of them use layered approach and others use only a single tool to directly produce the target structure. For German, there is the above mentioned WOCADI parser to MultiNet, for English, there is a Discourse Representation Theory (DRT) analyzer (Bos, 2005), and for Czech there is a Transparent Intensional Logic analyzer (Hor´ak, 2001). The layered approaches: DeepThought project (Callmeier et al., 2004) can combine output of various tools into one representation. It would be even possible to incorporate TR and MultiNet into this framework. Meaning-Text Theory (Bolshakov and Gelbukh, 2000) uses an approach similar to Functional Generative Description (ˇZabokrtsk´y, 2005) but it also has no layer corresponding to MultiNet. There were attempts to analyze the semantics of TR, namely in question answering system TIBAQ (Jirk˚u and Hajiˇc, 1982), which used TR directly as the semantic representation, and KruijffKorbayov´a (1998), who tried to transform the TFA information in TR into the DRT framework. 5 Evaluation It is a still open question how to evaluate systems for semantic representation. Basically, three approaches are used in similar projects: First, the coverage of the system</context>
</contexts>
<marker>Bolshakov, Gelbukh, 2000</marker>
<rawString>Igor Bolshakov and Alexander Gelbukh. 2000. The Meaning-Text Model: Thirty Years After. International Forum on Information and Documentation, 1:10–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Towards Wide-Coverage Semantic Interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of Sixth International Workshop on Computational Semantics IWCS-6,</booktitle>
<pages>42--53</pages>
<contexts>
<context position="18562" citStr="Bos, 2005" startWordPosition="3011" endWordPosition="3012">e of allegation, there is a difference in FACT attribute of the allegation. • The TFA has an influence on the scope of quantifiers (Peregrin, 1995b; Hajiˇcov´a et al., 1998). This information is fully transformed into the quantifier scopes in MultiNet. 4 Related Work There are various approaches trying to analyze text to a semantic representation. Some of them use layered approach and others use only a single tool to directly produce the target structure. For German, there is the above mentioned WOCADI parser to MultiNet, for English, there is a Discourse Representation Theory (DRT) analyzer (Bos, 2005), and for Czech there is a Transparent Intensional Logic analyzer (Hor´ak, 2001). The layered approaches: DeepThought project (Callmeier et al., 2004) can combine output of various tools into one representation. It would be even possible to incorporate TR and MultiNet into this framework. Meaning-Text Theory (Bolshakov and Gelbukh, 2000) uses an approach similar to Functional Generative Description (ˇZabokrtsk´y, 2005) but it also has no layer corresponding to MultiNet. There were attempts to analyze the semantics of TR, namely in question answering system TIBAQ (Jirk˚u and Hajiˇc, 1982), whic</context>
<context position="19896" citStr="Bos (2005)" startWordPosition="3221" endWordPosition="3222">in TR into the DRT framework. 5 Evaluation It is a still open question how to evaluate systems for semantic representation. Basically, three approaches are used in similar projects: First, the coverage of the system may serve as a basis for evaluation. This criterion is used in several systems (Bos, 2005; Hor´ak, 2001; Callmeier et al., 2004). However, this criterion is far from ideal, because it’s not applicable to robust systems and can not tell anything about the quality of resulting representation. Second, the consistency of the semantic representation serves as an evaluation criterion in Bos (2005). It is a desired state to have a consistent representation of texts, but there is no guarantee that a consistent semantic representation is in any sense also a good one. Third, the performance in an application (e.g., question answering system) is another criterion used for evaluating a semantic representation (Hartrumpf, 2005). A problem in this kind of evaluation is that we can not separate the evaluation of the formalism itself from the evaluation of the automatic processing tools. This problem becomes even bigger in a multilayered approach like FGD or MTT, where the overall performance de</context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>Johan Bos. 2005. Towards Wide-Coverage Semantic Interpretation. In Proceedings of Sixth International Workshop on Computational Semantics IWCS-6, pages 42–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald J Brachman</author>
<author>Deborah L McGuinness</author>
<author>Peter F Patel-Schneider</author>
<author>Lori Alperin Resnick</author>
<author>Alex Borgida</author>
</authors>
<title>Living with CLASSIC: When and How to Use a KL-ONE-like Language. In</title>
<date>1991</date>
<booktitle>Principles of Semantic Networks: Explorations in the representation of knowledge,</booktitle>
<pages>401--456</pages>
<editor>John Sowa, editor,</editor>
<publisher>Morgan-Kaufmann,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="4732" citStr="Brachman et al., 1991" startWordPosition="708" endWordPosition="711">rammatical representation (TR). The layer information is hidden except for the most important QUANT and CARD values. These attributes convey information that is important with respect to the content of the sentence. TR lacks attributes distinguishing intensional and extensional information and there are no relations like SUBM denoting relation between a set and its subset. Note that the MultiNet representation crosses the sentence boundaries. First, the structure representing a sentence is created and then this structure is assimilated into the existing representation. In contrast to CLASSIC (Brachman et al., 1991) and other KL-ONE networks, MultiNet contains a predefined final set of relation types, encapsulation of concepts, and attribute layers concerning cardinality of objects mentioned in discourse. In Section 2, we describe our motivation for extending the annotation in FGD to an even deeper level. Section 3 lists the MultiNet structural counterparts of tectogrammatical means. We discuss the related work in Section 4. Section 5 deals with various evaluation techniques and we conclude in Section 6. 2 FGD layers PDT 2.0 contains three layers of information about the text (as described in Hajiˇc (199</context>
</contexts>
<marker>Brachman, McGuinness, Patel-Schneider, Resnick, Borgida, 1991</marker>
<rawString>Ronald J. Brachman, Deborah L. McGuinness, Peter F. Patel-Schneider, Lori Alperin Resnick, and Alex Borgida. 1991. Living with CLASSIC: When and How to Use a KL-ONE-like Language. In John Sowa, editor, Principles of Semantic Networks: Explorations in the representation of knowledge, pages 401–456. Morgan-Kaufmann, San Mateo, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
<author>Andreas Eisele</author>
<author>Ulrich Sch¨afer</author>
<author>Melanie Siegel</author>
</authors>
<title>The DeepThought Core Architecture Framework.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<marker>Callmeier, Eisele, Sch¨afer, Siegel, 2004</marker>
<rawString>Ulrich Callmeier, Andreas Eisele, Ulrich Sch¨afer, and Melanie Siegel. 2004. The DeepThought Core Architecture Framework. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Building a Syntactically Annotated Corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>Issues of Valency and Meaning. Studies in Honour ofJarmila Panevov´a,</booktitle>
<pages>106--132</pages>
<editor>In E. Hajiˇcov´a, editor,</editor>
<publisher>Karolinum, Charles University Press,</publisher>
<location>Prague, Czech Republic.</location>
<marker>Hajiˇc, 1998</marker>
<rawString>Jan Hajiˇc. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. In E. Hajiˇcov´a, editor, Issues of Valency and Meaning. Studies in Honour ofJarmila Panevov´a, pages 106– 132. Karolinum, Charles University Press, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
<author>Petr Sgall</author>
</authors>
<title>A Manual for Tectogrammatic Tagging of the Prague Dependency Treebank.</title>
<date>2000</date>
<tech>Technical Report TR-2000-09, ´UFAL MFF</tech>
<location>UK, Prague, Czech Republic.</location>
<note>in Czech.</note>
<marker>Hajiˇcov´a, Panevov´a, Sgall, 2000</marker>
<rawString>Eva Hajiˇcov´a, Jarmila Panevov´a, and Petr Sgall. 2000. A Manual for Tectogrammatic Tagging of the Prague Dependency Treebank. Technical Report TR-2000-09, ´UFAL MFF UK, Prague, Czech Republic. in Czech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Barbara Partee</author>
</authors>
<title>Topic-Focus Articulation, Tripartite Structures, and Semantic Content.</title>
<date>1998</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Hajiˇcov´a, Sgall, Partee, 1998</marker>
<rawString>Eva Hajiˇcov´a, Petr Sgall, and Barbara Partee. 1998. Topic-Focus Articulation, Tripartite Structures, and Semantic Content. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
</authors>
<title>Presupposition and Allegation Revisited.</title>
<date>1984</date>
<journal>Journal ofPragmatics,</journal>
<pages>8--155</pages>
<marker>Hajiˇcov´a, 1984</marker>
<rawString>Eva Hajiˇcov´a. 1984. Presupposition and Allegation Revisited. Journal ofPragmatics, 8:155–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
<author>Hermann Helbig</author>
<author>Rainer Osswald</author>
</authors>
<date>2003</date>
<booktitle>The Semantically Based Computer Lexicon HaGenLex – Structure and Technological Environment. Traitement automatique des langues,</booktitle>
<pages>44--2</pages>
<contexts>
<context position="8483" citStr="Hartrumpf et al. (2003)" startWordPosition="1318" endWordPosition="1321">TR, we can not add an edge denoting (a ANTE c). We would have to include a proposition like “a precedes c” as a whole new clause. For all these reasons we need to extend our text annotation to a form suitable to more advanced tasks. It is shown in Helbig (2006) that MultiNet is capable to solve all the above mentioned issues. Helbig (1986) describes a procedure for automatic translation of natural language utterances into MultiNet structures used in WOCADI tool for German. WOCADI uses no theoretical intermediate structures and relies heavily on semantically annotated dictionary (HagenLex, see Hartrumpf et al. (2003)). In our approach, we want to take advantage of existing tools for conversions between layers in FGD. By combining several simpler procedures for translation between adjacent layers, we can improve the robustness of the whole procedure and the modularity of the software tools. Moreover, the process is divided to logical steps corresponding to theoretically sound and well defined structures. On the other hand, such a multistage processing is susceptible to accumulation of errors made by individual components. 3 Structural Similarities 3.1 Nodes and Concepts If we look at examples of TR and Mul</context>
</contexts>
<marker>Hartrumpf, Helbig, Osswald, 2003</marker>
<rawString>Sven Hartrumpf, Hermann Helbig, and Rainer Osswald. 2003. The Semantically Based Computer Lexicon HaGenLex – Structure and Technological Environment. Traitement automatique des langues, 44(2):81–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
</authors>
<title>University of hagen at qa@clef 2005: Extending knowledge and deepening linguistic processing for question answering.</title>
<date>2005</date>
<booktitle>Results of the CLEF 2005 CrossLanguage System Evaluation Campaign, Working Notes for the CLEF 2005 Workshop,</booktitle>
<editor>In Carol Peters, editor,</editor>
<location>Wien, ¨Osterreich. Centromedia.</location>
<contexts>
<context position="20226" citStr="Hartrumpf, 2005" startWordPosition="3274" endWordPosition="3275">lmeier et al., 2004). However, this criterion is far from ideal, because it’s not applicable to robust systems and can not tell anything about the quality of resulting representation. Second, the consistency of the semantic representation serves as an evaluation criterion in Bos (2005). It is a desired state to have a consistent representation of texts, but there is no guarantee that a consistent semantic representation is in any sense also a good one. Third, the performance in an application (e.g., question answering system) is another criterion used for evaluating a semantic representation (Hartrumpf, 2005). A problem in this kind of evaluation is that we can not separate the evaluation of the formalism itself from the evaluation of the automatic processing tools. This problem becomes even bigger in a multilayered approach like FGD or MTT, where the overall performance depends on all participating transducers as well as on the quality of the theoretical description. However, from the user point of view, this is so far the most reliable form of semantic representation evaluation. 6 Conclusion We have presented an outline of a procedure that enables us to transform syntactical (tectogrammatical) s</context>
</contexts>
<marker>Hartrumpf, 2005</marker>
<rawString>Sven Hartrumpf. 2005. University of hagen at qa@clef 2005: Extending knowledge and deepening linguistic processing for question answering. In Carol Peters, editor, Results of the CLEF 2005 CrossLanguage System Evaluation Campaign, Working Notes for the CLEF 2005 Workshop, Wien, ¨Osterreich. Centromedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Helbig</author>
</authors>
<title>Syntactic-Semantic Analysis of Natural Language by a New Word-Class Controlled Functional Analysis.</title>
<date>1986</date>
<journal>Computers and ArtificialInteligence,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="8201" citStr="Helbig (1986)" startWordPosition="1280" endWordPosition="1281">ry valency of another node. There are no means for representing knowledge generated during the inference process, if the knowledge doesn’t have a form of TR. For example, consider axiom of temporal precedence transitivity (3): (a ANTE b) n (b ANTE c) —* (a ANTE c) (3) In TR, we can not add an edge denoting (a ANTE c). We would have to include a proposition like “a precedes c” as a whole new clause. For all these reasons we need to extend our text annotation to a form suitable to more advanced tasks. It is shown in Helbig (2006) that MultiNet is capable to solve all the above mentioned issues. Helbig (1986) describes a procedure for automatic translation of natural language utterances into MultiNet structures used in WOCADI tool for German. WOCADI uses no theoretical intermediate structures and relies heavily on semantically annotated dictionary (HagenLex, see Hartrumpf et al. (2003)). In our approach, we want to take advantage of existing tools for conversions between layers in FGD. By combining several simpler procedures for translation between adjacent layers, we can improve the robustness of the whole procedure and the modularity of the software tools. Moreover, the process is divided to log</context>
</contexts>
<marker>Helbig, 1986</marker>
<rawString>Hermann Helbig. 1986. Syntactic-Semantic Analysis of Natural Language by a New Word-Class Controlled Functional Analysis. Computers and ArtificialInteligence, 5(1):53–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Helbig</author>
</authors>
<title>Knowledge Representation and the Semantics of Natural Language.</title>
<date>2006</date>
<publisher>SpringerVerlag,</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="2622" citStr="Helbig (2006)" startWordPosition="384" endWordPosition="385">WHEN.basic adv.denot.ngrad.nneg politika f DIR3.basic n.denot fem.sg Figure 1: Tectogrammatical tree of sentence (1) 1.1 MultiNet The representational means of Multilayered Extended Semantic Networks (MultiNet), which are Letos CC�C! � o X V n´avrat V politiky. V do V se snaˇzi snažit_se enunc . f PRED v decl disp0 ind . . proc.it0.res0.sim #PersPron t ACT n.pron.def.pers anim.sg.3.basic nAvrat f PAT n.denot inan.sg 78 Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 78–85, Sydney, July 2006. c�2006 Association for Computational Linguistics described in Helbig (2006), provide a universally applicable formalism for treatment of semantic phenomena of natural language. To this end, they offer distinct advantages over the use of the classical predicate calculus and its derivatives. The knowledge representation paradigm and semantic formalism MultiNet is used as a common backbone for all aspects of natural language processing (be they theoretical or practical ones). It is continually used for the development of intelligent information and communication systems and for natural language interfaces to the Internet. Within this framework, it is subject to permanen</context>
<context position="8121" citStr="Helbig (2006)" startWordPosition="1267" endWordPosition="1268">ord or a group of words in the surface form of sentence or to a deleted obligatory valency of another node. There are no means for representing knowledge generated during the inference process, if the knowledge doesn’t have a form of TR. For example, consider axiom of temporal precedence transitivity (3): (a ANTE b) n (b ANTE c) —* (a ANTE c) (3) In TR, we can not add an edge denoting (a ANTE c). We would have to include a proposition like “a precedes c” as a whole new clause. For all these reasons we need to extend our text annotation to a form suitable to more advanced tasks. It is shown in Helbig (2006) that MultiNet is capable to solve all the above mentioned issues. Helbig (1986) describes a procedure for automatic translation of natural language utterances into MultiNet structures used in WOCADI tool for German. WOCADI uses no theoretical intermediate structures and relies heavily on semantically annotated dictionary (HagenLex, see Hartrumpf et al. (2003)). In our approach, we want to take advantage of existing tools for conversions between layers in FGD. By combining several simpler procedures for translation between adjacent layers, we can improve the robustness of the whole procedure a</context>
<context position="10740" citStr="Helbig, 2006" startWordPosition="1689" endWordPosition="1690">ng Fred 3.2 Edges, relations and functions An edge of TR between nodes that have their conceptual counterparts in MultiNet always corresponds to one or more relations and possibly also some functions. In general, it can be said that MultiNet representation of a text contains significantly more connections (either as relations, or as functions) than TR, and some of them correspond to TR edges. 3.3 Functors and types of relations and functions There are 67 functor types in TR (see Hajiˇcov´a et al. (2000) for description), which correspond to 94 relation types and 19 function types in MultiNet (Helbig, 2006). The mapping of TR functions to MultiNet is given in table 1: TR functor MultiNet counterpart ACMP ASSOC ACT AFF, AGT, BENF, CSTR, EXP, ADDR MEXP, SCAR ADVS ORNT AIM SUBST, OPPOS PURP APP ASSOC, ATTCH continued... &apos;In fact the concept representing the man is the concept G01, i.e. only one vertex. However, the whole network corresponds to the TR node representing Fred. continued... fred VAL G01 human SUB VIA PURP MOD MODL TR functor MultiNet counterpart *ITMS, MODL PROP except for sentential complements COND CONFR COND OPPOS *IMTS-I, *TUPL OPPOS CONC *COMP METH, JUST, CIRC, CONF CAUS, JUST, GO</context>
</contexts>
<marker>Helbig, 2006</marker>
<rawString>Hermann Helbig. 2006. Knowledge Representation and the Semantics of Natural Language. SpringerVerlag, Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleˇs Hor´ak</author>
</authors>
<title>The Normal Translation Algorithm in Transparent Intensional Logic for Czech.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Faculty of Informatics, Masaryk University,</institution>
<location>Brno, Czech Republic.</location>
<marker>Hor´ak, 2001</marker>
<rawString>Aleˇs Hor´ak. 2001. The Normal Translation Algorithm in Transparent Intensional Logic for Czech. Ph.D. thesis, Faculty of Informatics, Masaryk University, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Jirk˚u</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Inferencing and search for an answer in TIBAQ.</title>
<date>1982</date>
<booktitle>In Proceedings of the 9th conference on Computational linguistics –</booktitle>
<volume>2</volume>
<pages>139--141</pages>
<location>Prague, Czechoslovakia.</location>
<marker>Jirk˚u, Hajiˇc, 1982</marker>
<rawString>Petr Jirk˚u and Jan Hajiˇc. 1982. Inferencing and search for an answer in TIBAQ. In Proceedings of the 9th conference on Computational linguistics – Volume 2, pages 139–141, Prague, Czechoslovakia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayov´a</author>
</authors>
<title>The Dynamic Potential of Topic and Focus: A Praguian Approach to Discourse Representation Theory.</title>
<date>1998</date>
<booktitle>Ph.D. thesis, ´UFAL, MFF UK,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Kruijff-Korbayov´a, 1998</marker>
<rawString>Ivana Kruijff-Korbayov´a. 1998. The Dynamic Potential of Topic and Focus: A Praguian Approach to Discourse Representation Theory. Ph.D. thesis, ´UFAL, MFF UK, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaroslav Peregrin</author>
</authors>
<title>Topic, Focus and the Logic of Language.</title>
<date>1995</date>
<booktitle>In Sprachtheoretische Grundlagen f¨ur die Computerlinguistik (Proceedings of the Goettingen Focus Workshop, 17. DGfS),</booktitle>
<publisher>IBM Deutschland.</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="17733" citStr="Peregrin, 1995" startWordPosition="2875" endWordPosition="2876">rtheless, the differences in the content of sentences differing only in TFA can be represented in MultiNet by other means. The TFA differences can be reflected in these categories: • Relations connecting the topic of sentence with the remaining concepts in the sentence are usually a part of definitional knowledge about the concepts in the topic, while the relations going to the focus belong to the assertional part of knowledge about the concepts in focus. In other words, TFA can be reflected in different values of K TYPE attribute. • TFA has an effect on the identification of presuppositions (Peregrin, 1995a) and allegations (Hajiˇcov´a, 1984). In case of presupposition, we need to know about them in the process of assimilation of new information into the existing network in order to detect presupposition failures. In case of allegation, there is a difference in FACT attribute of the allegation. • The TFA has an influence on the scope of quantifiers (Peregrin, 1995b; Hajiˇcov´a et al., 1998). This information is fully transformed into the quantifier scopes in MultiNet. 4 Related Work There are various approaches trying to analyze text to a semantic representation. Some of them use layered approa</context>
</contexts>
<marker>Peregrin, 1995</marker>
<rawString>Jaroslav Peregrin. 1995a. Topic, Focus and the Logic of Language. In Sprachtheoretische Grundlagen f¨ur die Computerlinguistik (Proceedings of the Goettingen Focus Workshop, 17. DGfS), Heidelberg. IBM Deutschland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaroslav Peregrin</author>
</authors>
<title>Topic-Focus Articulation as Generalized Quantification.</title>
<date>1995</date>
<booktitle>Proceedings of “Focus and natural language processing”,</booktitle>
<pages>49--57</pages>
<editor>In P. Bosch and R. van der Sandt, editors,</editor>
<publisher>IBM Deutschland.</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="17733" citStr="Peregrin, 1995" startWordPosition="2875" endWordPosition="2876">rtheless, the differences in the content of sentences differing only in TFA can be represented in MultiNet by other means. The TFA differences can be reflected in these categories: • Relations connecting the topic of sentence with the remaining concepts in the sentence are usually a part of definitional knowledge about the concepts in the topic, while the relations going to the focus belong to the assertional part of knowledge about the concepts in focus. In other words, TFA can be reflected in different values of K TYPE attribute. • TFA has an effect on the identification of presuppositions (Peregrin, 1995a) and allegations (Hajiˇcov´a, 1984). In case of presupposition, we need to know about them in the process of assimilation of new information into the existing network in order to detect presupposition failures. In case of allegation, there is a difference in FACT attribute of the allegation. • The TFA has an influence on the scope of quantifiers (Peregrin, 1995b; Hajiˇcov´a et al., 1998). This information is fully transformed into the quantifier scopes in MultiNet. 4 Related Work There are various approaches trying to analyze text to a semantic representation. Some of them use layered approa</context>
</contexts>
<marker>Peregrin, 1995</marker>
<rawString>Jaroslav Peregrin. 1995b. Topic-Focus Articulation as Generalized Quantification. In P. Bosch and R. van der Sandt, editors, Proceedings of “Focus and natural language processing”, pages 49–57, Heidelberg. IBM Deutschland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<date>1986</date>
<booktitle>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D. Reidel Publishing company, Dodrecht,</booktitle>
<location>Boston, London.</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D. Reidel Publishing company, Dodrecht, Boston, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
</authors>
<title>Deep Syntactic Annotation: Tectogrammatical Representation and Beyond.</title>
<date>2004</date>
<booktitle>Proceedings of the HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation,</booktitle>
<pages>32--38</pages>
<editor>In A. Meyers, editor,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA.</location>
<marker>Sgall, Panevov´a, Hajiˇcov´a, 2004</marker>
<rawString>Petr Sgall, Jarmila Panevov´a, and Eva Hajiˇcov´a. 2004. Deep Syntactic Annotation: Tectogrammatical Representation and Beyond. In A. Meyers, editor, Proceedings of the HLT-NAACL 2004 Workshop: Frontiers in Corpus Annotation, pages 32–38, Boston, Massachusetts, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Resemblances between Meaning-Text Theory and Functional Generative Description.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Conference of Meaning-Text Theory,</booktitle>
<pages>549--557</pages>
<marker>ˇZabokrtsk´y, 2005</marker>
<rawString>Zdenˇek ˇZabokrtsk´y. 2005. Resemblances between Meaning-Text Theory and Functional Generative Description. In Proceedings of the 2nd International Conference of Meaning-Text Theory, pages 549–557.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>