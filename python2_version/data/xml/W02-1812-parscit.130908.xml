<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000292">
<title confidence="0.9996725">
A Word Segmentation Method With Dynamic Adapting To Text
Using Inductive Learning
</title>
<author confidence="0.958315">
Zhongjian Wang
</author>
<affiliation confidence="0.6334105">
Hokuto System Co.,LTD
Oyachi Higashi1-3-23, Atubetuku, Sapporo, 004-0041 Japan
</affiliation>
<email confidence="0.992885">
wang@hscnet.co.jp
</email>
<author confidence="0.986986">
Kenji Araki
</author>
<affiliation confidence="0.995734">
Graduate School of Engineering, Hokkaido University
</affiliation>
<address confidence="0.946993">
N13-W8, Kita-ku, Sapporo, 060-8628 Japan
</address>
<email confidence="0.998087">
araki@media.eng.hokudai.ac.jp
</email>
<author confidence="0.986945">
Koji Tochinai
</author>
<affiliation confidence="0.8396025">
Graduate School of Business Administration, Hokkai-Gakuen University,
Asahimachi 4-1-40, Toyohira-ku, Sapporo, 062-8605 Japan
</affiliation>
<sectionHeader confidence="0.992848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998175">
We have proposed a method of word segmen-
tation for non-segmented language using Induc-
tive Learning. This method uses only surface
information of a text, so that it has an ad-
vantage that is entirely not dependent on any
specific language. In this method, we consider
that a character string of appearing frequently
in a text has a high possibility as a word. The
method predicts unknown words by recursively
extracting common character strings. With the
proposed method, the segmentation results can
adapt to different users and fields. To evaluate
effectivety for Chinese word segmentation and
adaptability for different fields, we have done
the evaluation experiment with Chinese text of
the two fields.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="keywords">
1 Instruction
</sectionHeader>
<bodyText confidence="0.999985022222222">
In NLP applications, word segmentation of non-
segmented language is a very necessary initial
stage(Sun et al., 1998). In the other hands,
with the development of the Internet and popu-
larization of computers, a large amount of text
information in different languages on the In-
ternet are increasing explosively, so it is nec-
essary to develop a common method to deal
with multi-language(Yamasita and Matsumoto,
2000). Furthermore, the standard of word seg-
mentation is dependent on a user and destina-
tion of use(Sproat et al., 1996), so that it is nec-
essary that word segmentation can adapt users,
can deal with multi languages.
In our method, we extract recursively a
common character string that occur frequently
in text and call it a common part. When
some common parts contain still same charac-
ter strings, furthermore we extract the same
character string as high dimensional common
parts and the remain parts is called different
parts. The high dimensional common parts
maybe have higher possibility as words because
it is extracted by multi steps. Those extracted
common parts and different parts are called
WS(Word Segment), and classified into some
ranks according to extracting condition. The
proposed method segments a non-segmented
sentence into words using the ranks of WS in
order of the higher value of the certainty de-
grees as words. When there are multiple seg-
mentation candidates, the system gets a list of
segmentable candidates, and picks a correct seg-
mentation candidate from the list by using a
value of LEF (Likelihood Evaluation Function,
Section 2.1) and so on. In addition, it is not
necessary to prepare a dictionary and any word
segmentation rules beforehand. A dictionary of
adapting to the user or the field is generated
with increasing of processed text. Because only
surface information of a text is used, it is possi-
ble the method is used to deal with general non-
segmented language. Here Inductive Learning
is the procedure to extract recursively WS by
multi steps(Araki et al., 1995).
</bodyText>
<sectionHeader confidence="0.971169" genericHeader="introduction">
2 Algrithm
</sectionHeader>
<figureCaption confidence="0.872671666666667">
Fig. 1 shows the outline of the proposed
method.
(1) Input sentences are segmented by word
candidates that were acquired in the dictionary
so far.
Figure 1: Outline
</figureCaption>
<bodyText confidence="0.989597125">
(2) For the remaining part of the charac-
ter strings that are unsegmented by the known
words, the system predicts unknown words by
extracting WS using Inductive Learning.
The system extracts WS as word candidates.
This process is based on the supposition that a
common character string of appearing repeat-
edly in text has high probability as a word.
</bodyText>
<listItem confidence="0.721111375">
(3) The user judges whether the results of the
word segmentation is correct or not. If there are
errors in the result, the user will correct errors.
(4) The system compares the proofread re-
sults with the segmentation result to update
the information in the dictionary. Through this
procedure, the certainty of WS as a word is con-
firmed and increased.
</listItem>
<bodyText confidence="0.9952675">
Here, the WS those are used in correct seg-
mentation are called CW (Correct Word).
</bodyText>
<subsectionHeader confidence="0.998255">
2.1 Segmentation by Known Words
</subsectionHeader>
<bodyText confidence="0.990935090909091">
Input sentence and then the system segments
it into words by registered CW and WS that
the system has got by using Inductive Learning
until that time.
(1) In the first step, the system compares the
registered CW or WS in the dictionary with
the character string in the input sentence from
the beginning of the sentence, and finds out
the same character strings with the registered
words. The system repeats this comparison pro-
cess until the end of the sentence is reached.
A list of segmentation candidate is established.
Then the system segments the sentence into
words.
(2) In the second step, however, for the char-
acter strings of multiple segmentations, we use
the registered candidates in order of their ranks
in the dictionary(Section 2.3). When there are
more than one word candidate with the same
rank, we decide the correct segmentation from
the list of segmentation candidates by the value
of LEF. We define LEF as follows:
</bodyText>
<equation confidence="0.997784">
FR + αC5 − QE5 + -yLE
LEF = (1)
FR + C5 − E5 + LE
</equation>
<bodyText confidence="0.990583909090909">
Where: FR, CS, ES and LE are the fre-
quency of CW or WS appearing in the text, the
frequency of the correct segmentation, the fre-
quency of the erroneous segmentation and the
length of CW or WS respectively. α, Q and -y are
coefficients. The optimum coefficients of LEF
are decided by the preliminary experiments us-
ing Greedy method, α=10, Q=1 and -y=5.
The word that has the maximum value of
LEF is decided as the correct segmentation can-
didate.
</bodyText>
<listItem confidence="0.98722">
(3) When LEF value of the set of possible
segmentations is equal to each other, the cor-
rect segmentation candidate is decided by the
word candidate that the value of ES is mini-
mum, the value of CS is maximum, the value of
FR is maximum, the value of LE is the longest
or the location of segmentation is the leftmost
in a sentence in turn.
</listItem>
<subsectionHeader confidence="0.968846">
2.2 Prediction for Unknown Words
</subsectionHeader>
<bodyText confidence="0.999911444444445">
Fig. 2 shows an example of a non-segmented
sentence. In this example, every character rep-
resents a Chinese character, so we use this ex-
ample to express a general sentence of non-
segmented language to present the proposed
method. Those words that are not registered in
the dictionary are predicted by using Inductive
Learning. After the sentences were segmented
by known words, which have been registered in
the dictionary, the unsegmented part of char-
acter string will be used to extract WS. The
prediction method is to find the common char-
acter string in text. The extraction procedure
is carried out as Fig. 3 shows: the extraction
of common parts, sift out the common part of
the most possibility as a word, the re-extraction
of common parts and the extraction of different
parts.
</bodyText>
<figure confidence="0.999596368421053">
Segment by known words
Input sentences
Dictionary
Recursively
prediction
Segment by a update dictionary
No
Prediction over
Yes
Segmentation
results
Proofreading
Corrected results
Feedback processing
Output segmentation results
Predict unknown words using
Inductive Learning
αβχδκϕEφγOπµτγβπαβχδ
OπµτγβηΨcφγτγβαζθ.
</figure>
<figureCaption confidence="0.950204">
Figure 2: An example of non-segmented sen-
tence.
</figureCaption>
<subsubsectionHeader confidence="0.505862">
2.2.1 Extraction of a Common Part
</subsubsectionHeader>
<bodyText confidence="0.844727116279069">
A common part in non-segmented text is ex-
tracted by two steps:
(1) When a character string appears in text
frequently, we call it a common character string.
If the common character string consists of more
than two characters, we extract it as a word can-
didate and call it common part and represent it
by S1(Segment one). Here, we use length, fre-
quency and location of S1 in the sentence to sift
out it, to get the S1 of the most possible as a
word. At this step, we acquired S1 from the
sentence that is shown in Fig. 2: “αβχδ”,“Eφγ”
and “Oπµτγβ”.
(2) When the character string appears in the
sentence only one times but meanwhile it is
included in other extracted common part and
made up by more than two characters, we also
extract it as a word candidate. For example in
Fig. 2: “τγβ” is included in “Oπµτγβ”. There-
fore “τγβ” is extracted and belongs to S1.
2.2.2 Extraction of a High Dimensional
Common Part and a Different
Part
The extracted S1 at 2.2.1 may still include a
common character string. At this situation, the
common character string can be re-extracted
moreover from the extracted S1. We consider
it has a higher probability as a word that re-
extracted common parts at this procedure. The
conditions of re-extraction are presented as fol-
lows:
(1) The common part can be re-extracted
from the extracted S1 when it includes a com-
mon character string that is more than two
characters. For example, “Oπµτγβ” contains
“τγβ” which can be extracted from “Oπµτγβ”,
so “Oπµτγβ(S1)” is equal to “Oπµ(S2)” +
“τγβ(S3)”.
The part of re-extraction is called high di-
mensional common part and represented by S2
(Segment two). The part of remain is called
different part and represented by S3 (Segment
three). The S1 is deleted from the dictionary
</bodyText>
<table confidence="0.9514445">
Unsegmented Character String
Extraction of common parts
Sift out common parts of the
most possibility as words
Get S1
Extraction of high dimension
common parts and different parts
Get S2 and S3
</table>
<figureCaption confidence="0.993135">
Figure 3: WS extraction procedure
</figureCaption>
<bodyText confidence="0.995675652173913">
when it is divided into S2 and S3.
(2) Furthermore one character can also be ex-
tracted as a word candidate when both sides
of it are extracted as a word candidate or
both sides were segmented by known words.
Like “π” in “Oπµτγβπαβχδ” is surrounded by
“Oπµτγβ” and “αβχδ”, and “π” is extracted
as a word candidate belonging to S2.
The extraction procedure is carried out re-
peatedly until the new WS can not be extracted
and the input can not be segmented.
2.3 Segmentation by a Update
Dictionary
The extracted WS are classified to “S1”, “S2”,
and “S3”. Those WS that are confirmed
as a word by proofreading process are called
“CW” (Correct Word). Furthermore, the
FR(appearing FRequency), CS(Correct Seg-
mentation frequency), ES(Erroneous Segmen-
tation frequency), LE(LEength) and rank of a
word candidate are rigestered simultaneously.
Word Segmentation is carried out by the up-
date dictionary as 2.1.
</bodyText>
<subsectionHeader confidence="0.539555">
2.4 Feedback Process
</subsectionHeader>
<bodyText confidence="0.999466272727273">
After the system segments the sentence into
words, the results are judged whether they are
correct or not by the user. Then the user cor-
rects the errors if there are errors in the results.
The system updates the rank of the registered
CW and WS in the dictionary by comparing the
corrected results with the segmentation results.
And the system increases the priority degree of
the words that were used in correct segmenta-
tion and decreases the priority degree of words
that were used in erroneous segmentations. The
</bodyText>
<tableCaption confidence="0.997834">
Table 1: Experimental results
</tableCaption>
<table confidence="0.9990006">
Fields Economics Engineering Average
words 92,085 70,017 162,102
CSR[%] 87.50 90.80 89.44
ESR[%] 5.40 5.60 5.45
USR[%] 7.10 3.60 5.11
</table>
<listItem confidence="0.97894247826087">
feedback process is described in detail as follows:
(1) For the Correct Segmentation Results:
• When the result of segmentation is correct,
the value of FR and CS of a word that is
used to segment are added one.
• If the rank of the words does not belong to
CW, it is changed to CW.
(2) For the Erroneous Segmentation Results:
• If the dictionary does not has the correct
words, the system registers the words in the
dictionary. In this case, their FRs are 1,
their ranks are CW.
• If the dictionary has the correct words, the
system adds one to the value of FR for a
word and changes the value of CL to CW
if it does not belong to CW.
• If the reason of erroneous segmentation is
that the erroneous word was used, then the
ES of erroneous word is added one.
(3) For the Unsegmented Parts:
• The system registers the words in the dic-
tionary, as FR of the words equal 1 and
rank equal CW.
</listItem>
<sectionHeader confidence="0.997202" genericHeader="method">
3 Evaluation Experiments
</sectionHeader>
<subsectionHeader confidence="0.99993">
3.1 Experimental Data And Procedure
</subsectionHeader>
<bodyText confidence="0.999846666666667">
To evaluate the adaptability of the proposed
method for different fields and the effectivity for
Chinese word segmentation. We use the Chi-
nese text of two specialized fields from Sinica
Corpusi: the economics contains 92,085 words
and the engineering contains 70,017 words. To-
tal words is 162,102. The economics consists of
the text of economic system, economic policy
and economic theory. The engineering consists
of the text of electronics, communication engi-
neering, machine engineering and nuclear indus-
try.
</bodyText>
<footnote confidence="0.4514135">
lhttp : //www.sinica.edu.tw/ftms − bin/kiwi.sh
Number of Words
</footnote>
<figureCaption confidence="0.999156">
Figure 4: The changes of segmentation rates
</figureCaption>
<bodyText confidence="0.9999232">
In order to confirm the adaptability of pro-
posed method to user, we let the initial dic-
tionary empty. We input a paragraph about
hundred words one times and two fields text in
turns.
</bodyText>
<subsectionHeader confidence="0.999357">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999168">
The results of experiment are shown in Table
1. Fig. 4 shows the change of CSR, ESR and
USR. In our method, the correct segmentation
number is the number of correct segmentation
that is judged by a user. The unsegmentation
number is the number when all unsegmented
strings are segmented correctly. The erroneous
segmentation number is the number that sub-
tracts the number of correct segmentation and
unsegmentation from the number of all words
in the input text. To evaluate the experiment
result, we use these formulas of CSR (Correct
Segmentation Rate), ESR (Erroneous Segmen-
tation Rate) and USR (Unsegmented Rate) as
follows:
</bodyText>
<subsectionHeader confidence="0.581418">
Correct segmentation number
</subsectionHeader>
<figure confidence="0.964043375">
CSR[%] = x 100 (2)
Total number of words
Erroneous segmentation number
ESR[%] = x 100 (3)
Total number of words
Unsegmentation number
USR[%] = x 100 (4)
Total number of words
</figure>
<sectionHeader confidence="0.99688" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.996546">
4.1 Adaptability To Different Fields
</subsectionHeader>
<bodyText confidence="0.99502825">
Fig. 4 shows the experimental results of two
fields. When the text is changed to different
domain, because appearance of some new words
of different fields, the correct segmentation rate
</bodyText>
<figure confidence="0.9997185">
0 2 4 6 8 10 12 14 16
(x10,000)
100
90
70
60
40
30
20
80
50
10
0
Correct Segmentation Rate
Erroneous Segmentation Rate
Economics Engineering
Unsegmentation Rate
Segmentation Rate(%)
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Number of words (x10,000)
</figure>
<figureCaption confidence="0.999991">
Figure 5: The ability to predict unknown words
</figureCaption>
<bodyText confidence="0.985978428571428">
is fall down temporary. However with increasing
of processed sentence, the correct segmentation
rate goes on increasing quickly.
We may consider that the proposed method
has adaptability for different fields. Sometimes
the correct segmentation rate is a little lower
because the domain of text is a little difference,
for example: the economics consists of the text
of economic system, economic policy and eco-
nomic theory and so on.
4.2 Evaluation of Ability for Predicting
Unknown Words
We use 50,000 words to discuss the predicting
ability of proposed method for unknown words.
</bodyText>
<equation confidence="0.9726565">
CWN
Precision[%] = TWN × 100 (5)
CWN
Recall[%] = TUN × 100 (6)
</equation>
<bodyText confidence="0.997705545454545">
Where, CWN is the number of words that are
predicted correctly. TWN is the total number
of words that are predicted. TUN is the total
number of unknown words.
The precision and recall are shown in Fig. 5.
The average precision is 26.0%. The average
recall is 31.0%. With increasing of registered
words in the dictionary, prediction effect for
unknown words is becoming well, after 40,000
words are processed the precision and the recall
are 85.0%, 40.0% respectively.
</bodyText>
<subsectionHeader confidence="0.811555">
4.3 Analysis of Erroneous
Segmentation
</subsectionHeader>
<bodyText confidence="0.999932571428572">
We select 1,000 words from the beginning of the
experimental date and the end of the experi-
mental date respectively, to analysis the reason
of an erroneous segmentation. At the begin-
ning, ESR that is because of unregistered words
is 18.0%, but after 16,000 words are processed,
ESR that is because of unregistered words is
0.9%. However ESR that is caused by ambigu-
ity goes on increasing from 1.6% to 7.0%. ESR
caused by ambiguity is increasing with increas-
ing of registered word in the dictionary. Am-
biguous segmentation is still a difficult problem,
so that it is necessary to improve the ability to
deal with ambiguity.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973055555555">
The experiment results show the prediction
ability for unknown words by using Inductive
Learning. The experiment results of two fields
shown the proposed method can adapt to dif-
ferent fields text. In this paper, the emphasis is
to evaluate the adaptivity of the method to dif-
ferent user and fields. About comparison with
other existed methods will be done in the future.
The proposed method may be used to
computer-aided acquisition of language re-
source. The experimental results show our
proposed method has ability of learning, pre-
dictability for unknown words and effectivity
for Chinese word segmentation. For the future
works, we plan to improve the ability of dealing
with segmentation ambiguity, and use this pro-
posed method for Chinese morphological anal-
ysis.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999129052631579">
Kenji Araki, Yoshio Momouchi, and Koji Tochi-
nai. 1995. Evaluation for adaptability of
kana-kanji translation of non-segmentation
japanese kana sentences using inductive
learning. PACLING-II, pages 1–7.
Richard Sproat, Chilin Shih, William Gale,
and Nancy Chang. 1996. A stochastic finite-
state word-segmentation algorithm for Chi-
nese. Association for Computational Linguis-
tics, 22(3):377–404.
Maosong Sun, Dayang Shen, and Benjamin K
Tsou. 1998. Chinese word segmentation
without using lexicon and hand-crafted train-
ing data. 17th International Conference on
Computational Linguistics, pages 1265–1271.
T. Yamasita and Y. Matsumoto. 2000. Journal
of natural language processing(in japanese).
Framework for Language Independent Mor-
phological Analysis, 7(3):39–56.
</reference>
<figure confidence="0.999111083333333">
Precosion
Recall
Precision &amp; Recall 90
80
70
60
50
40
30
20
10
0
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.94650425">A Word Segmentation Method With Dynamic Adapting To Text Using Inductive Learning Zhongjian Hokuto System</title>
<address confidence="0.768099">Oyachi Higashi1-3-23, Atubetuku, Sapporo, 004-0041</address>
<email confidence="0.973245">wang@hscnet.co.jp</email>
<author confidence="0.450976">Kenji</author>
<affiliation confidence="0.98833">Graduate School of Engineering, Hokkaido</affiliation>
<address confidence="0.988028">N13-W8, Kita-ku, Sapporo, 060-8628</address>
<email confidence="0.95956">araki@media.eng.hokudai.ac.jp</email>
<author confidence="0.837567">Koji Tochinai</author>
<affiliation confidence="0.389583">Graduate School of Business Administration, Hokkai-Gakuen</affiliation>
<address confidence="0.223091">Asahimachi 4-1-40, Toyohira-ku, Sapporo, 062-8605 Japan</address>
<abstract confidence="0.993637611940298">We have proposed a method of word segmentation for non-segmented language using Inductive Learning. This method uses only surface information of a text, so that it has an advantage that is entirely not dependent on any specific language. In this method, we consider that a character string of appearing frequently in a text has a high possibility as a word. The method predicts unknown words by recursively extracting common character strings. With the proposed method, the segmentation results can adapt to different users and fields. To evaluate effectivety for Chinese word segmentation and adaptability for different fields, we have done the evaluation experiment with Chinese text of the two fields. 1 Instruction In NLP applications, word segmentation of nonsegmented language is a very necessary initial stage(Sun et al., 1998). In the other hands, with the development of the Internet and popularization of computers, a large amount of text information in different languages on the Internet are increasing explosively, so it is necessary to develop a common method to deal with multi-language(Yamasita and Matsumoto, 2000). Furthermore, the standard of word segmentation is dependent on a user and destination of use(Sproat et al., 1996), so that it is necessary that word segmentation can adapt users, can deal with multi languages. In our method, we extract recursively a common character string that occur frequently in text and call it a common part. When some common parts contain still same character strings, furthermore we extract the same character string as high dimensional common parts and the remain parts is called different parts. The high dimensional common parts maybe have higher possibility as words because it is extracted by multi steps. Those extracted common parts and different parts are called WS(Word Segment), and classified into some ranks according to extracting condition. The proposed method segments a non-segmented sentence into words using the ranks of WS in order of the higher value of the certainty degrees as words. When there are multiple segmentation candidates, the system gets a list of segmentable candidates, and picks a correct segmentation candidate from the list by using a value of LEF (Likelihood Evaluation Function, and so on. In addition, it is not necessary to prepare a dictionary and any word segmentation rules beforehand. A dictionary of adapting to the user or the field is generated with increasing of processed text. Because only surface information of a text is used, it is possible the method is used to deal with general nonsegmented language. Here Inductive Learning is the procedure to extract recursively WS by multi steps(Araki et al., 1995). 2 Algrithm Fig. 1 shows the outline of the proposed method. (1) Input sentences are segmented by word candidates that were acquired in the dictionary so far. Figure 1: Outline (2) For the remaining part of the character strings that are unsegmented by the known words, the system predicts unknown words by extracting WS using Inductive Learning. The system extracts WS as word candidates. This process is based on the supposition that a common character string of appearing repeatedly in text has high probability as a word. (3) The user judges whether the results of the word segmentation is correct or not. If there are errors in the result, the user will correct errors. (4) The system compares the proofread results with the segmentation result to update the information in the dictionary. Through this procedure, the certainty of WS as a word is confirmed and increased. Here, the WS those are used in correct segmentation are called CW (Correct Word). 2.1 Segmentation by Known Words Input sentence and then the system segments it into words by registered CW and WS that the system has got by using Inductive Learning until that time. (1) In the first step, the system compares the registered CW or WS in the dictionary with the character string in the input sentence from the beginning of the sentence, and finds out the same character strings with the registered words. The system repeats this comparison process until the end of the sentence is reached. A list of segmentation candidate is established. Then the system segments the sentence into words. (2) In the second step, however, for the character strings of multiple segmentations, we use the registered candidates in order of their ranks the dictionary(Section When there are more than one word candidate with the same rank, we decide the correct segmentation from the list of segmentation candidates by the value of LEF. We define LEF as follows: Where: FR, CS, ES and LE are the frequency of CW or WS appearing in the text, the frequency of the correct segmentation, the frequency of the erroneous segmentation and the of CW or WS respectively. coefficients. The optimum coefficients of LEF are decided by the preliminary experiments us- Greedy method, and The word that has the maximum value of LEF is decided as the correct segmentation candidate. (3) When LEF value of the set of possible segmentations is equal to each other, the correct segmentation candidate is decided by the word candidate that the value of ES is minimum, the value of CS is maximum, the value of FR is maximum, the value of LE is the longest or the location of segmentation is the leftmost in a sentence in turn. 2.2 Prediction for Unknown Words Fig. 2 shows an example of a non-segmented sentence. In this example, every character represents a Chinese character, so we use this example to express a general sentence of nonsegmented language to present the proposed method. Those words that are not registered in the dictionary are predicted by using Inductive Learning. After the sentences were segmented by known words, which have been registered in the dictionary, the unsegmented part of character string will be used to extract WS. The prediction method is to find the common character string in text. The extraction procedure is carried out as Fig. 3 shows: the extraction of common parts, sift out the common part of the most possibility as a word, the re-extraction of common parts and the extraction of different parts. Segment by known words Input sentences Dictionary Recursively prediction Segment by a update dictionary No Prediction over Yes Segmentation results Proofreading Corrected results Feedback processing Output segmentation results Predict unknown words using Inductive Learning Figure 2: An example of non-segmented sentence. 2.2.1 Extraction of a Common Part A common part in non-segmented text is extracted by two steps: (1) When a character string appears in text frequently, we call it a common character string. If the common character string consists of more than two characters, we extract it as a word candidate and call it common part and represent it by S1(Segment one). Here, we use length, frequency and location of S1 in the sentence to sift out it, to get the S1 of the most possible as a word. At this step, we acquired S1 from the that is shown in Fig. 2: (2) When the character string appears in the sentence only one times but meanwhile it is included in other extracted common part and made up by more than two characters, we also extract it as a word candidate. For example in 2: is included in Thereis extracted and belongs to S1. 2.2.2 Extraction of a High Dimensional Common Part and a Different Part extracted S1 at still include a common character string. At this situation, the common character string can be re-extracted moreover from the extracted S1. We consider it has a higher probability as a word that reextracted common parts at this procedure. The conditions of re-extraction are presented as follows: (1) The common part can be re-extracted from the extracted S1 when it includes a common character string that is more than two For example, contains which can be extracted from is equal to + The part of re-extraction is called high dimensional common part and represented by S2 (Segment two). The part of remain is called different part and represented by S3 (Segment three). The S1 is deleted from the dictionary Unsegmented Character String Extraction of common parts Sift out common parts of the most possibility as words Get S1 Extraction of high common parts and different parts Get S2 and S3 Figure 3: WS extraction procedure when it is divided into S2 and S3. (2) Furthermore one character can also be extracted as a word candidate when both sides of it are extracted as a word candidate or both sides were segmented by known words. in is surrounded by and and is extracted as a word candidate belonging to S2. The extraction procedure is carried out repeatedly until the new WS can not be extracted and the input can not be segmented. 2.3 Segmentation by a Update Dictionary The extracted WS are classified to “S1”, “S2”, and “S3”. Those WS that are confirmed as a word by proofreading process are called “CW” (Correct Word). Furthermore, the FR(appearing FRequency), CS(Correct Segmentation frequency), ES(Erroneous Segmentation frequency), LE(LEength) and rank of a word candidate are rigestered simultaneously. Word Segmentation is carried out by the updictionary as 2.4 Feedback Process After the system segments the sentence into words, the results are judged whether they are correct or not by the user. Then the user corrects the errors if there are errors in the results. The system updates the rank of the registered CW and WS in the dictionary by comparing the corrected results with the segmentation results. And the system increases the priority degree of the words that were used in correct segmentation and decreases the priority degree of words that were used in erroneous segmentations. The Table 1: Experimental results Fields Economics Engineering Average words 92,085 70,017 162,102 CSR[%] 87.50 90.80 89.44 ESR[%] 5.40 5.60 5.45 USR[%] 7.10 3.60 5.11 feedback process is described in detail as follows: (1) For the Correct Segmentation Results: • When the result of segmentation is correct, the value of FR and CS of a word that is used to segment are added one. • If the rank of the words does not belong to CW, it is changed to CW. (2) For the Erroneous Segmentation Results: • If the dictionary does not has the correct words, the system registers the words in the dictionary. In this case, their FRs are 1, their ranks are CW. • If the dictionary has the correct words, the system adds one to the value of FR for a word and changes the value of CL to CW if it does not belong to CW. • If the reason of erroneous segmentation is that the erroneous word was used, then the ES of erroneous word is added one. (3) For the Unsegmented Parts: • The system registers the words in the dictionary, as FR of the words equal 1 and rank equal CW. 3 Evaluation Experiments 3.1 Experimental Data And Procedure To evaluate the adaptability of the proposed method for different fields and the effectivity for Chinese word segmentation. We use the Chinese text of two specialized fields from Sinica the economics contains 92,085 words and the engineering contains 70,017 words. Total words is 162,102. The economics consists of the text of economic system, economic policy and economic theory. The engineering consists of the text of electronics, communication engineering, machine engineering and nuclear industry. Number of Words Figure 4: The changes of segmentation rates In order to confirm the adaptability of proposed method to user, we let the initial dictionary empty. We input a paragraph about hundred words one times and two fields text in turns. 3.2 Experimental Results The results of experiment are shown in Table 1. Fig. 4 shows the change of CSR, ESR and USR. In our method, the correct segmentation number is the number of correct segmentation that is judged by a user. The unsegmentation number is the number when all unsegmented strings are segmented correctly. The erroneous segmentation number is the number that subtracts the number of correct segmentation and unsegmentation from the number of all words in the input text. To evaluate the experiment result, we use these formulas of CSR (Correct Segmentation Rate), ESR (Erroneous Segmentation Rate) and USR (Unsegmented Rate) as follows: Correct segmentation number = Total number of words Erroneous segmentation number = Total number of words Unsegmentation number = Total number of words 4 Discussion 4.1 Adaptability To Different Fields Fig. 4 shows the experimental results of two fields. When the text is changed to different domain, because appearance of some new words of different fields, the correct segmentation rate</abstract>
<phone confidence="0.321859">0 2 4 6 8 10 12 14 16</phone>
<note confidence="0.89937775">(x10,000) 100 90 70 60 40 30 20 80 50 10 0</note>
<title confidence="0.7955954">Correct Segmentation Rate Erroneous Segmentation Rate Economics Engineering Unsegmentation Rate Segmentation Rate(%)</title>
<phone confidence="0.404524">0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5</phone>
<abstract confidence="0.980534986111111">Number of words (x10,000) Figure 5: The ability to predict unknown words is fall down temporary. However with increasing of processed sentence, the correct segmentation rate goes on increasing quickly. We may consider that the proposed method has adaptability for different fields. Sometimes the correct segmentation rate is a little lower because the domain of text is a little difference, for example: the economics consists of the text of economic system, economic policy and economic theory and so on. 4.2 Evaluation of Ability for Predicting Unknown Words We use 50,000 words to discuss the predicting ability of proposed method for unknown words. CWN = (5) CWN = (6) Where, CWN is the number of words that are predicted correctly. TWN is the total number of words that are predicted. TUN is the total number of unknown words. The precision and recall are shown in Fig. 5. The average precision is 26.0%. The average recall is 31.0%. With increasing of registered words in the dictionary, prediction effect for unknown words is becoming well, after 40,000 words are processed the precision and the recall are 85.0%, 40.0% respectively. 4.3 Analysis of Erroneous Segmentation We select 1,000 words from the beginning of the experimental date and the end of the experimental date respectively, to analysis the reason of an erroneous segmentation. At the beginning, ESR that is because of unregistered words is 18.0%, but after 16,000 words are processed, ESR that is because of unregistered words is 0.9%. However ESR that is caused by ambiguity goes on increasing from 1.6% to 7.0%. ESR caused by ambiguity is increasing with increasing of registered word in the dictionary. Ambiguous segmentation is still a difficult problem, so that it is necessary to improve the ability to deal with ambiguity. 5 Conclusion The experiment results show the prediction ability for unknown words by using Inductive Learning. The experiment results of two fields shown the proposed method can adapt to different fields text. In this paper, the emphasis is to evaluate the adaptivity of the method to different user and fields. About comparison with other existed methods will be done in the future. The proposed method may be used to computer-aided acquisition of language resource. The experimental results show our proposed method has ability of learning, predictability for unknown words and effectivity for Chinese word segmentation. For the future works, we plan to improve the ability of dealing with segmentation ambiguity, and use this proposed method for Chinese morphological analysis. References Kenji Araki, Yoshio Momouchi, and Koji Tochinai. 1995. Evaluation for adaptability of kana-kanji translation of non-segmentation japanese kana sentences using inductive pages 1–7.</abstract>
<author confidence="0.965993">Richard Sproat</author>
<author confidence="0.965993">Chilin Shih</author>
<author confidence="0.965993">William Gale</author>
<abstract confidence="0.758478909090909">and Nancy Chang. 1996. A stochastic finitestate word-segmentation algorithm for Chifor Computational Linguis- 22(3):377–404. Maosong Sun, Dayang Shen, and Benjamin K Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted traindata. International Conference on pages 1265–1271. T. Yamasita and Y. Matsumoto. 2000. Journal of natural language processing(in japanese).</abstract>
<note confidence="0.8366444">Framework for Language Independent Mor- 7(3):39–56. Precosion Recall Precision &amp; Recall 90 80 70 60 50 40 30 20 10 0</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenji Araki</author>
<author>Yoshio Momouchi</author>
<author>Koji Tochinai</author>
</authors>
<title>Evaluation for adaptability of kana-kanji translation of non-segmentation japanese kana sentences using inductive learning.</title>
<date>1995</date>
<pages>1--7</pages>
<publisher>PACLING-II,</publisher>
<contexts>
<context position="3208" citStr="Araki et al., 1995" startWordPosition="496" endWordPosition="499"> the system gets a list of segmentable candidates, and picks a correct segmentation candidate from the list by using a value of LEF (Likelihood Evaluation Function, Section 2.1) and so on. In addition, it is not necessary to prepare a dictionary and any word segmentation rules beforehand. A dictionary of adapting to the user or the field is generated with increasing of processed text. Because only surface information of a text is used, it is possible the method is used to deal with general nonsegmented language. Here Inductive Learning is the procedure to extract recursively WS by multi steps(Araki et al., 1995). 2 Algrithm Fig. 1 shows the outline of the proposed method. (1) Input sentences are segmented by word candidates that were acquired in the dictionary so far. Figure 1: Outline (2) For the remaining part of the character strings that are unsegmented by the known words, the system predicts unknown words by extracting WS using Inductive Learning. The system extracts WS as word candidates. This process is based on the supposition that a common character string of appearing repeatedly in text has high probability as a word. (3) The user judges whether the results of the word segmentation is corre</context>
</contexts>
<marker>Araki, Momouchi, Tochinai, 1995</marker>
<rawString>Kenji Araki, Yoshio Momouchi, and Koji Tochinai. 1995. Evaluation for adaptability of kana-kanji translation of non-segmentation japanese kana sentences using inductive learning. PACLING-II, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finitestate word-segmentation algorithm for Chinese. Association for Computational Linguistics,</title>
<date>1996</date>
<contexts>
<context position="1724" citStr="Sproat et al., 1996" startWordPosition="251" endWordPosition="254">s, we have done the evaluation experiment with Chinese text of the two fields. 1 Instruction In NLP applications, word segmentation of nonsegmented language is a very necessary initial stage(Sun et al., 1998). In the other hands, with the development of the Internet and popularization of computers, a large amount of text information in different languages on the Internet are increasing explosively, so it is necessary to develop a common method to deal with multi-language(Yamasita and Matsumoto, 2000). Furthermore, the standard of word segmentation is dependent on a user and destination of use(Sproat et al., 1996), so that it is necessary that word segmentation can adapt users, can deal with multi languages. In our method, we extract recursively a common character string that occur frequently in text and call it a common part. When some common parts contain still same character strings, furthermore we extract the same character string as high dimensional common parts and the remain parts is called different parts. The high dimensional common parts maybe have higher possibility as words because it is extracted by multi steps. Those extracted common parts and different parts are called WS(Word Segment), </context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finitestate word-segmentation algorithm for Chinese. Association for Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Dayang Shen</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>17th International Conference on Computational Linguistics,</booktitle>
<pages>1265--1271</pages>
<contexts>
<context position="1312" citStr="Sun et al., 1998" startWordPosition="183" endWordPosition="186">y specific language. In this method, we consider that a character string of appearing frequently in a text has a high possibility as a word. The method predicts unknown words by recursively extracting common character strings. With the proposed method, the segmentation results can adapt to different users and fields. To evaluate effectivety for Chinese word segmentation and adaptability for different fields, we have done the evaluation experiment with Chinese text of the two fields. 1 Instruction In NLP applications, word segmentation of nonsegmented language is a very necessary initial stage(Sun et al., 1998). In the other hands, with the development of the Internet and popularization of computers, a large amount of text information in different languages on the Internet are increasing explosively, so it is necessary to develop a common method to deal with multi-language(Yamasita and Matsumoto, 2000). Furthermore, the standard of word segmentation is dependent on a user and destination of use(Sproat et al., 1996), so that it is necessary that word segmentation can adapt users, can deal with multi languages. In our method, we extract recursively a common character string that occur frequently in te</context>
</contexts>
<marker>Sun, Shen, Tsou, 1998</marker>
<rawString>Maosong Sun, Dayang Shen, and Benjamin K Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted training data. 17th International Conference on Computational Linguistics, pages 1265–1271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yamasita</author>
<author>Y Matsumoto</author>
</authors>
<title>Journal of natural language processing(in japanese). Framework for Language Independent Morphological Analysis,</title>
<date>2000</date>
<contexts>
<context position="1609" citStr="Yamasita and Matsumoto, 2000" startWordPosition="231" endWordPosition="234">pt to different users and fields. To evaluate effectivety for Chinese word segmentation and adaptability for different fields, we have done the evaluation experiment with Chinese text of the two fields. 1 Instruction In NLP applications, word segmentation of nonsegmented language is a very necessary initial stage(Sun et al., 1998). In the other hands, with the development of the Internet and popularization of computers, a large amount of text information in different languages on the Internet are increasing explosively, so it is necessary to develop a common method to deal with multi-language(Yamasita and Matsumoto, 2000). Furthermore, the standard of word segmentation is dependent on a user and destination of use(Sproat et al., 1996), so that it is necessary that word segmentation can adapt users, can deal with multi languages. In our method, we extract recursively a common character string that occur frequently in text and call it a common part. When some common parts contain still same character strings, furthermore we extract the same character string as high dimensional common parts and the remain parts is called different parts. The high dimensional common parts maybe have higher possibility as words bec</context>
</contexts>
<marker>Yamasita, Matsumoto, 2000</marker>
<rawString>T. Yamasita and Y. Matsumoto. 2000. Journal of natural language processing(in japanese). Framework for Language Independent Morphological Analysis, 7(3):39–56.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>