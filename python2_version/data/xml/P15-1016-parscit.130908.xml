<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000914">
<title confidence="0.982213">
Compositional Vector Space Models for Knowledge Base Completion
</title>
<author confidence="0.998362">
Arvind Neelakantan, Benjamin Roth, Andrew McCallum
</author>
<affiliation confidence="0.9988175">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<address confidence="0.918677">
Amherst, MA, 01003
</address>
<email confidence="0.999314">
{arvind,beroth,mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999785576923077">
Knowledge base (KB) completion adds
new facts to a KB by making inferences
from existing facts, for example by infer-
ring with high likelihood nationality(X,Y)
from bornIn(X,Y). Most previous methods
infer simple one-hop relational synonyms
like this, or use as evidence a multi-hop re-
lational path treated as an atomic feature,
like bornIn(X,Z) → containedIn(Z,Y). This
paper presents an approach that reasons
about conjunctions of multi-hop relations
non-atomically, composing the implica-
tions of a path using a recurrent neural
network (RNN) that takes as inputs vec-
tor embeddings of the binary relation in
the path. Not only does this allow us
to generalize to paths unseen at training
time, but also, with a single high-capacity
RNN, to predict new relation types not
seen when the compositional model was
trained (zero-shot learning). We assem-
ble a new dataset of over 52M relational
triples, and show that our method im-
proves over a traditional classifier by 11%,
and a method leveraging pre-trained em-
beddings by 7%.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999777519230769">
Constructing large knowledge bases (KBs) sup-
ports downstream reasoning about resolved enti-
ties and their relations, rather than the noisy tex-
tual evidence surrounding their natural language
mentions. For this reason KBs have been of in-
creasing interest in both industry and academia
(Bollacker et al., 2008; Suchanek et al., 2007;
Carlson et al., 2010). Such KBs typically con-
tain many millions of facts, most of them (en-
tity1,relation,entity2) “triples” (also known as bi-
nary relations) such as (Barack Obama, presi-
dentOf, USA) and (Brad Pitt, marriedTo, Angelina
Jolie).
However, even the largest KBs are woefully in-
complete (Min et al., 2013), missing many impor-
tant facts, and therefore damaging their usefulness
in downstream tasks. Ironically, these missing
facts can frequently be inferred from other facts al-
ready in the KB, thus representing a sort of incon-
sistency that can be repaired by the application of
an automated process. The addition of new triples
by leveraging existing triples is typically known as
KB completion.
Early work on this problem focused on learn-
ing symbolic rules. For example, Schoenmack-
ers et al. (2010) learns Horn clauses predictive of
new binary relations by exhausitively exploring re-
lational paths of increasing length, and selecting
those surpassing an accuracy threshold. (A “path”
is a sequence of triples in which the second entity
of each triple matches the first entity of the next
triple.) Lao et al. (2011) introduced the Path Rank-
ing Algorithm (PRA), which greatly improves ef-
ficiency and robustness by replacing exhaustive
search with random walks, and using unique paths
as features in a per-target-relation binary classifier.
A typical predictive feature learned by PRA is that
CountryOflleadquarters(X, Y) is implied by Is-
BasedIn(X,A) and StateLocatedIn(A, B) and Coun-
tryLocatedIn(B, Y). Given IsBasedIn(Microsoft,
Seattle), StateLocatedIn(Seattle, Washington) and
CountryLocatedIn(Washington, USA), we can in-
fer the fact CountryOflleadquarters(Microsoft,
USA) using the predictive feature. In later work,
Lao et al. (2012) greatly increase available raw
material for paths by augmenting KB-schema rela-
tions with relations defined by the text connecting
mentions of entities in a large corpus (also known
as OpenIE relations (Banko et al., 2007)).
However, these symbolic methods can produce
many millions of distinct paths, each of which is
categorically distinct, treated by PRA as a dis-
</bodyText>
<page confidence="0.980011">
156
</page>
<note confidence="0.978099333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 156–166,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998515862745098">
tinct feature. (See Figure 1.) Even putting aside
the OpenIE relations, this limits the applicability
of these methods to modern KBs that have thou-
sands of relation types, since the number of dis-
tinct paths increases rapidly with the number of re-
lation types. If textually-defined OpenIE relations
are included, the problem is obviously far more
severe.
Better generalization can be gained by operat-
ing on embedded vector representations of rela-
tions, in which vector similarity can be interpreted
as semantic similarity. For example, Bordes et al.
(2013) learn low-dimensional vector representa-
tions of entities and KB relations, such that vector
differences between two entities should be close
to the vectors associated with their relations. This
approach can find relation synonyms, and thus per-
form a kind of one-to-one, non-path-based relation
prediction for KB completion. Similarly Nickel
et al. (2011) and Socher et al. (2013a) perform
KB completion by learning embeddings of rela-
tions, but based on matrices or tensors. Universal
schema (Riedel et al., 2013) learns to perform rela-
tion prediction cast as matrix completion (likewise
using vector embeddings), but predicts textually-
defined OpenIE relations as well as KB relations,
and embeds entity-pairs in addition to individual
entities. Like all of the above, it also reasons
about individual relations, not the evidence of a
connected path of relations.
This paper proposes an approach combining the
advantages of (a) reasoning about conjunctions of
relations connected in a path, and (b) generaliza-
tion through vector embeddings, and (c) reasoning
non-atomically and compositionally about the el-
ements of the path, for further generalization.
Our method uses recurrent neural networks
(RNNs) (Werbos, 1990) to compose the semantics
of relations in an arbitrary-length path. At each
path-step it consumes both the vector embedding
of the next relation, and the vector representing the
path-so-far, then outputs a composed vector (rep-
resenting the extended path-so-far), which will be
the input to the next step. After consuming a path,
the RNN should output a vector in the semantic
neighborhood of the relation between the first and
last entity of the path. For example, after con-
suming the relation vectors along the path Melinda
Gates → Bill Gates → Microsoft → Seattle, our
method produces a vector very close to the rela-
tion livesIn.
</bodyText>
<figure confidence="0.48273">
CountryOfHeadquarters
</figure>
<figureCaption confidence="0.992755">
Figure 1: Semantically similar paths connecting entity pair
(Microsoft, USA).
</figureCaption>
<bodyText confidence="0.996929380952381">
Our compositional approach allow us at test
time to make predictions from paths that were un-
seen during training, because of the generaliza-
tion provided by vector neighborhoods, and be-
cause they are composed in non-atomic fashion.
This allows our model to seamlessly perform in-
ference on many millions of paths in the KB graph.
In most of our experiments, we learn a separate
RNN for predicting each relation type, but alterna-
tively, by learning a single high-capacity composi-
tion function for all relation types, our method can
perform zero-shot learning—predicting new rela-
tion types for which the composition function was
never explicitly trained.
Related to our work, new versions of PRA
(Gardner et al., 2013; Gardner et al., 2014) use
pre-trained vector representations of relations to
alleviate its feature explosion problem—but the
core mechanism continues to be a classifier based
on atomic-path features. In the 2013 work many
paths are collapsed by clustering paths accord-
ing to their relations’ embeddings, and substitut-
ing cluster ids for the original relation types. In
the 2014 work unseen paths are mapped to nearby
paths seen at training time, where nearness is mea-
sured using the embeddings. Neither is able to per-
form zero-shot learning since there must be a clas-
sifer for each predicted relation type. Furthermore
their pre-trained vectors do not have the opportu-
nity to be tuned to the KB completion task because
the two sub-tasks are completely disentangled.
An additional contribution of our work is a
new large-scale data set of over 52 million triples,
and its preprocessing for purposes of path-based
KB completion (can be downloaded from http:
//iesl.cs.umass.edu/downloads/
inferencerules/release.tar.gz). The
dataset is build from the combination of Freebase
(Bollacker et al., 2008) and Google’s entity
linking in ClueWeb (Orr et al., 2013). Rather than
Gardner’s 1000 distinct paths per relation type, we
have over 2 million. Rather than Gardner’s 200
</bodyText>
<table confidence="0.932485142857143">
IsBasedIn StateLocatedIn CountryLocatedIn
USA
Microsoft Seattle Washington
headquartered in in the U.S. state of state part of
headquarters located in located in the state of state in the NW region of
founded in beautiful city in located in country
based in in state democratic state in
</table>
<page confidence="0.987561">
157
</page>
<figureCaption confidence="0.951918">
Figure 2: Vector Representations of the paths are computed
by applying the composition function recursively.
</figureCaption>
<bodyText confidence="0.998100235294118">
entity pairs, we use over 10k. All experimental
comparisons below are performed on this new
data set.
On this challenging large-scale dataset our com-
positional method outperforms PRA (Lao et al.,
2012), and Cluster PRA (Gardner et al., 2013) by
11% and 7% respectively. A further contribution
of our work is a new, surprisingly strong baseline
method using classifiers of path bigram features,
which beats PRA and Cluster PRA, and statisti-
cally ties our compositional method. Our analysis
shows that our method has substantially different
strengths than the new baseline, and the combi-
nation of the two yields a 15% improvement over
Gardner et al. (2013). We also show that our zero-
shot model is indeed capable of predicting new un-
seen relation types.
</bodyText>
<sectionHeader confidence="0.971098" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999923">
We give background on PRA which we use to ob-
tain a set of paths connecting the entity pairs and
the RNN model which we employ to model the
composition function.
</bodyText>
<subsectionHeader confidence="0.998532">
2.1 Path Ranking Algorithm
</subsectionHeader>
<bodyText confidence="0.999865916666667">
Since it is impractical to exhaustively obtain the
set of all paths connecting an entity pair in the
large KB graph, we use PRA (Lao et al., 2011)
to obtain a set of paths connecting the entity pairs.
Given a training set of entity pairs for a relation,
PRA heuristically finds a set of paths by perform-
ing random walks from the source and target nodes
keeping the most common paths. We use PRA to
find millions of distinct paths per relation type. We
do not use the random walk probabilities given by
PRA since using it did not yield improvements in
our experiments.
</bodyText>
<subsectionHeader confidence="0.999534">
2.2 Recurrent Neural Networks
</subsectionHeader>
<bodyText confidence="0.999897647058823">
Recurrent neural network (RNN) (Werbos, 1990)
is a neural network that constructs vector repre-
sentation for sequences (of any length). For exam-
ple, a RNN model can be used to construct vec-
tor representations for phrases or sentences (of any
length) in natural language by applying a compo-
sition function (Mikolov et al., 2010; Sutskever
et al., 2014; Vinyals et al., 2014). The vector
representation of a phrase (w1, w2) consisting of
words w1 and w2 is given by f(W [v(w1); v(w2)])
where v(w) E Rd is the vector representation of
w, f is an element-wise non linearity function,
[a; b] represents the concatenation two vectors a
and b along with a bias term, and W E Rd×2∗d+1
is the composition matrix. This operation can
be repeated to construct vector representations of
longer phrases.
</bodyText>
<sectionHeader confidence="0.992355" genericHeader="method">
3 Recurrent Neural Networks for KB
Completion
</sectionHeader>
<bodyText confidence="0.999960037037037">
This paper proposes a RNN model for KB comple-
tion that reasons on the paths connecting an entity
pair to predict missing relation types. The vec-
tor representations of the paths (of any length) in
the KB graph are computed by applying the com-
position function recursively as shown in Figure
2. To compute the vector representations for the
higher nodes in the tree, the composition function
consumes the vector representation of the node’s
two children nodes and outputs a new vector of the
same dimension. Predictions about missing rela-
tion types are made by comparing the vector repre-
sentation of the path with the vector representation
of the relation using the sigmoid function.
We represent each binary relation using a d-
dimensional real valued vector. We model com-
position using recurrent neural networks (Werbos,
1990). We learn a separate composition matrix for
every relation that is predicted.
Let vr(S) E Rd be the vector representation of
relation S and vp(7r) E Rd be the vector represen-
tation of path 7r. vp(7r) denotes the relation vec-
tor if path 7r is of length one. To predict relation
S = CountryOfHeadquarters, the vector represen-
tation of the path 7r = IsBasedIn → StateLocate-
dIn containing two relations IsBasedIn and State-
LocatedIn is computed by (Figure 2),
</bodyText>
<equation confidence="0.7240805">
vp(7r) =
f(Wδ[vr(IsBasedIn); vr(StateLocatedIn)])
</equation>
<figure confidence="0.930780333333333">
CountryOfHeadquarters
~
Compositon
Compositon
IsBasedIn StateLocatedIn CountryLocatedIn
Microsoft Seattle Washington USA
</figure>
<page confidence="0.98613">
158
</page>
<bodyText confidence="0.999853111111111">
where f = sigmoid is the element-wise non-
linearity function, Wδ E Rd∗2d+1 is the compo-
sition matrix for δ = CountryOfHeadquarters and
[a; b] represents the concatenation of two vectors
a E Rd, b E Rd along with a bias feature to get a
new vector [a; b] E R2d+1.
The vector representation of the path H = Is-
BasedIn —* StateLocatedIn —* CountryLocatedIn
in Figure 2 is computed similarly by,
</bodyText>
<equation confidence="0.989416">
vp(H) =
f(Wδ[vp(π); vr( CountryLocatedIn)])
</equation>
<bodyText confidence="0.9999612">
where vp(π) is the vector representation of path Is-
BasedIn —* StateLocatedIn. While computing the
vector representation of a path we always traverse
left to right, composing the relation vector in the
right with the accumulated path vector in the left1.
This makes our model a recurrent neural network
(Werbos, 1990).
Finally, we make a prediction regarding Coun-
tryOfHeadquarters(Microsoft, USA) using the
path H = IsBasedIn —* StateLocatedIn —* Coun-
tryLocatedIn by comparing the vector represen-
tation of the path (vp(H)) with the vector repre-
sentation of the relation CountryOfHeadquarters
(vr(CountryOfHeadquarters)) using the sigmoid
function.
</bodyText>
<subsectionHeader confidence="0.999784">
3.1 Model Training
</subsectionHeader>
<bodyText confidence="0.999798388888889">
We train the model with the existing facts in a
KB using them as positive examples and nega-
tive examples are obtained by treating the unob-
served instances as negative examples (Mintz et
al., 2009; Lao et al., 2011; Riedel et al., 2013; Bor-
des et al., 2013). Unlike in previous work that use
RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy
and Cardie, 2014), a challenge with using them
for our task is that among the set of paths connect-
ing an entity pair, we do not observe which of the
path(s) is predictive of a relation. We select the
path that is closest to the relation type to be pre-
dicted in the vector space. This not only allows
for faster training (compared to marginalization)
but also gives improved performance. This tech-
nique has been successfully used in models other
than RNNs previously (Weston et al., 2013; Nee-
lakantan et al., 2014).
</bodyText>
<footnote confidence="0.765105666666667">
1we did not get significant improvements when we tried
more sophisticated ordering schemes for computing the path
representations.
</footnote>
<construct confidence="0.3735825">
Algorithm 1 Training Algorithm of RNN model for rela-
tion δ
</construct>
<listItem confidence="0.99909005">
1: Input: Aδ = A+δ U A−δ , Φδ, number of itera-
tions T, mini-batch size B
2: Initialize vr, Wδ randomly
3: for t = 1,2,...,T do
4: Vvr = 0, VWδ = 0 and b = 0
5: for λ = (γ, δ) E Aδ do
6: µλ = arg maxπ∈Φδ(γ) vp(π).vr(δ)
7: Accumulate gradients to Vvr, VWδ
8: using path µλ.
9: b = b + 1
10: ifb=Bthen
11: Gradient Update for vr, Wδ
12: Vvr = 0, VWδ = 0 and b = 0
13: end if
14: end for
15: if b &gt; 0 then
16: Gradient Update for vr, Wδ
17: end if
18: end for
19: Output: vr, Wδ
</listItem>
<bodyText confidence="0.99981">
We assume that we are given a KB (for exam-
ple, Freebase enriched with SVO triples) contain-
ing a set of entity pairs F, set of relations Δ and
a set of observed facts A+ where bλ = (γ, δ) E
A+(γ E F, δ E Δ) indicates a positive fact that
entity pair γ is in relation δ. Let Φδ(γ) denote the
set of paths connecting entity pair γ given by PRA
for predicting relation δ.
In our task, we only observe the set of paths
connecting an entity pair but we do not observe
which of the path(s) is predictive of the fact. We
treat this as a latent variable (µλ for the fact λ)
and we assign µλ the path whose vector represen-
tation has maximum dot product with the vector
representation of the relation to be predicted. For
example, µλ for the fact λ = (γ, δ) E A+ is given
by,
</bodyText>
<equation confidence="0.949357">
µλ = arg max vp(π).vr(δ)
π∈Φδ(γ)
</equation>
<bodyText confidence="0.999611125">
During training, we assign µλ using the current
parameter estimates. We use the same procedure
to assign µλ for unobserved facts that are used as
negative examples during training.
We train a separate RNN model for predicting
each relation and the parameters of the model for
predicting relation δ E Δ are Θ = {vr(ω)bω E
Δ, Wδ}. Given a training set consisting of posi-
</bodyText>
<page confidence="0.995196">
159
</page>
<bodyText confidence="0.964783625">
tive (A+δ ) and negative (Aa ) instances2 for relation
6, the parameters are trained to maximize the log
likelihood of the training set with L-2 regulariza-
tion.
for example, the vector representation of the path
7r = IsBasedIn —* StateLocatedIn containing two
relations IsBasedIn and StateLocatedIn is com-
puted by (Figure 2),
</bodyText>
<table confidence="0.5438615">
�O* = arg max P(yλ = 1; O)+
O λ=(γ,δ)EA+δ P(yλ = 0; O) − ρ11O112
�
λ=(γ,δ)EA−δ
</table>
<bodyText confidence="0.999594">
where yλ is a binary random variable which takes
the value 1 if the fact A is true and 0 otherwise, and
the probability of a fact P(yλ = 1; O) is given by,
</bodyText>
<equation confidence="0.922527666666667">
P(yλ = 1; O) = sigmoid(vp(pλ).vr(6))
where pλ = arg max vp(7r).vr(6)
πEΦδ(γ)
</equation>
<bodyText confidence="0.9973144">
and P(yλ = 0; O) = 1 − P(yλ = 1; O). The
relation vectors and the composition matrix are
initialized randomly. We train the network us-
ing backpropagation through structure (Goller and
K¨uchler, 1996).
</bodyText>
<sectionHeader confidence="0.993616" genericHeader="method">
4 Zero-shot KB Completion
</sectionHeader>
<bodyText confidence="0.999267576923077">
The KB completion task involves predicting facts
on thousands of relations types and it is highly de-
sirable that a method can infer facts about relation
types without directly training for them. Given the
vector representation of the relations, we show that
our model described in the previous section is ca-
pable of predicting relational facts without explic-
itly training for the target (or test) relation types
(zero-shot learning).
In zero-shot or zero-data learning (Larochelle et
al., 2008; Palatucci et al., 2009), some labels or
classes are not available during training the model
and only a description of those classes are given
at prediction time. We make two modifications to
the model described in the previous section, (1)
learn a general composition matrix, and (2) fix re-
lation vectors with pre-trained vectors, so that we
can predict relations that are unseen during train-
ing. This ability of the model to generalize to un-
seen relations is beyond the capabilities of all pre-
vious methods for KB inference (Schoenmackers
et al., 2010; Lao et al., 2011; Gardner et al., 2013;
Gardner et al., 2014).
We learn a general composition matrix for all
relations instead of learning a separate composi-
tion matrix for every relation to be predicted. So,
</bodyText>
<footnote confidence="0.8496145">
2we sub-sample a portion of the set of all unobserved in-
stances.
</footnote>
<equation confidence="0.9889105">
vp(7r) =
f(W[vr(IsBasedIn); vr(StateLocatedIn)])
</equation>
<bodyText confidence="0.999932777777778">
where W E Rd*2d+1 is the general composition
matrix.
We initialize the vector representations of the
binary relations (vr) using the representations
learned in Riedel et al. (2013) and do not update
them during training. The relation vectors are not
updated because at prediction time we would be
predicting relation types which are never seen dur-
ing training and hence their vectors would never
get updated. We learn only the general composi-
tion matrix in this model. We train a single model
for a set of relation types by replacing the sigmoid
function with a softmax function while computing
probabilities and the parameters of the composi-
tion matrix are learned using the available train-
ing data containing instances of few relations. The
other aspects of the model remain unchanged.
To predict facts whose relation types are unseen
during training, we compute the vector represen-
tation of the path using the general composition
matrix and compute the probability of the fact us-
ing the pre-trained relation vector. For example,
using the vector representation of the path II = Is-
BasedIn —* StateLocatedIn —* CountryLocatedIn
in Figure 2, we can predict any relation irrespec-
tive of whether they are seen at training by com-
paring it with the pre-trained relation vectors.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999748">
The hyperparameters of all the models were tuned
on the same held-out development data. All the
neural network models are trained for 150 itera-
tions using 50 dimensional relation vectors, and
we set the L2-regularizer and learning rate to
0.0001 and 0.1 respectively. We halved the learn-
ing rate after every 60 iterations and use mini-
batches of size 20. The neural networks and the
classifiers were optimized using AdaGrad (Duchi
et al., 2011).
</bodyText>
<subsectionHeader confidence="0.946465">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9989675">
We ran experiments on Freebase (Bollacker et al.,
2008) enriched with information from ClueWeb.
</bodyText>
<page confidence="0.967108">
160
</page>
<table confidence="0.999662222222222">
Entities 18M
Freebase triples 40M
ClueWeb triples 12M
Relations 25,994
Relation types tested 46
Avg. paths/relation 2.3M
Avg. training facts/relation 6638
Avg. positive test instances/relation 3492
Avg. negative test instances/relation 43,160
</table>
<tableCaption confidence="0.999897">
Table 1: Statistics of our dataset.
</tableCaption>
<bodyText confidence="0.999452333333333">
We use the publicly available entity links to Free-
base in the ClueWeb dataset (Orr et al., 2013).
Hence, we create nodes only for Freebase enti-
ties in our KB graph. We remove facts containing
/type/object/type as they do not give useful pre-
dictive information for our task. We get triples
from ClueWeb by considering sentences that con-
tain two entities linked to Freebase. We extract the
phrase between the two entities and treat them as
the relation types. For phrases that are of length
greater than four we keep only the first and last
two words. This helps us to avoid the time con-
suming step of dependency parsing the sentence
to get the relation type. These triples are similar to
facts obtained by OpenIE (Banko et al., 2007). To
reduce noise, we select relation types that occur at
least 50 times. We evaluate on 46 relation types in
Freebase that have the most number of instances.
The methods are evaluated on a subset of facts in
Freebase that were hidden during training. Table
1 shows important statistics of our dataset.
</bodyText>
<subsectionHeader confidence="0.997165">
5.2 Predictive Paths
</subsectionHeader>
<bodyText confidence="0.9999604">
Table 2 shows predictive paths for 4 relations
learned by the RNN model. The high quality of
unseen paths is indicative of the fact that the RNN
model is able to generalize to paths that are never
seen during training.
</bodyText>
<subsectionHeader confidence="0.726654">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.991375133333334">
Using our dataset, we compare the performance of
the following methods:
PRA Classifier is the method in Lao et al. (2012)
which trains a logistic regression classifier by cre-
ating a feature for every path type.
Cluster PRA Classifier is the method in Gard-
ner et al. (2013) which replaces relation types from
ClueWeb triples with their cluster membership in
the KB graph before the path finding step. Af-
ter this step, their method proceeds in exactly the
same manner as Lao et al. (2012) training a logis-
tic regression classifier by creating a feature for
every path type. We use pre-trained relation vec-
tors from Riedel et al. (2013) and use k-means
clustering to cluster the relation types to 25 clus-
ters as done in Gardner et al. (2013).
Composition-Add uses a simple element-wise ad-
dition followed by sigmoid non-linearity as the
composition function similar to Yang et al. (2014).
RNN-random is the supervised RNN model de-
scribed in section 3 with the relation vectors ini-
tialized randomly.
RNN is the supervised RNN model described in
section 3 with the relation vectors initialized using
the method in Riedel et al. (2013).
PRA Classifier-b is our simple extension to the
method in Lao et al. (2012) which additionally
uses bigrams in the path as features. We add a
special start and stop symbol to the path before
computing the bigram features.
Cluster PRA Classifier-b is our simple extension
to the method in Gardner et al. (2013) which ad-
ditionally uses bigram features computed as previ-
ously described.
RNN + PRA Classifier combines the predictions
of RNN and PRA Classifier. We combine the pre-
dictions by assigning the score of a fact as the sum
of their rank in the two models after sorting them
in ascending order.
RNN + PRA Classifier-b combines the predictions
of RNN and PRA Classifier-b using the technique
described previously.
Table 3 shows the results of our experiments.
The method described in Gardner et al. (2014) is
not included in the table since the publicly avail-
able implementation does not scale to our large
dataset. First, we show that it is better to train the
models using all the path types instead of using
only the top 1, 000 path types as done in previous
work (Gardner et al., 2013; Gardner et al., 2014).
We can see that the RNN model performs signif-
icantly better than the baseline methods of Lao et
al. (2012) and Gardner et al. (2013). The perfor-
mance of the RNN model is not affected by initial-
ization since using random vectors and pre-trained
vectors results in similar performance.
A surprising result is the impressive perfor-
mance of our simple extension to the classifier
approach. After the addition of bigram features,
the naive PRA method is as effective as the Clus-
</bodyText>
<page confidence="0.997079">
161
</page>
<table confidence="0.999889866666667">
Relation: /book/written work/original language/ (book “x” written in language “y”)
Seen paths:
/book/written work/previous in series → /book/written work/author → /people/person/nationality → /people/person/nationality−1
→ /people/person/languages
/book/written work/author → /people/ethnicity/people−1 → /people/ethnicity/languages spoken
Unseen paths:
”in”−1 - ”writer”−1 → /people/person/nationality−1 → /people/person/languages
/book/written work/author → addresses → /people/person/nationality−1 → /people/person/languages
Relation: /people/person/place of birth/ (person “x” born in place “y”)
Seen paths:
“was,born,in” → /location/mailing address/citytown−1 → /location/mailing address/state province region
“from” → /location/location/contains−1
Unseen paths:
“born,in” → /location/location/contains → “near”−1
“was,born,in” → commonly,known,as−1
Relation: /geography/river/cities/ (river “x” flows through or borders “y”)
Seen paths:
“at” → /location/location/contains−1
“meets,the” → /transportation/bridge/body of water spanned−1 → /location/location/contains−1 → “in”
Unseen paths:
/geography/lake/outflow−1 → /location/location/contains−1
/geography/lake/outflow−1 → /location/location/contains−1 → “near”
Relation: /people/family/members/ (person “y” part offamily “x”)
Seen paths:
/royalty/monarch/royal line−1 → /people/person/children → /royalty/monarch/royal line
→ /royalty/royal line/monarchs from this line
/royalty/royal line/monarchs from this line → /people/person/parents−1 → /people/person/parents−1 → /people/person/parents−1
Unseen paths:
/royalty/monarch/royal line−1 → “leader”−1 → “king” → “was,married,to”−1
“of,the”−1 → “but,also,of” → “married” → “defended”−1
</table>
<tableCaption confidence="0.990978333333333">
Table 2: Predictive paths, according to the RNN model, for 4 target relations. Two examples of seen and
unseen paths are shown for each target relation. Inverse relations are marked by −1, i.e, r(x, y) =⇒
r−1(y, x), ∀(x, y) ∈ r. Relations within quotes are OpenIE (textual) relation types.
</tableCaption>
<table confidence="0.995421416666667">
train with train with
top 1000 paths all paths
Method MAP MAP
PRA Classifier 43.46 51.31
Cluster PRA Classifier 46.26 53.23
Composition-Add 40.23 45.37
RNN-random 45.52 56.91
RNN 46.61 56.95
PRA Classifier-b 48.09 58.13
Cluster PRA Classifier-b 48.72 58.02
RNN + PRA Classifier 49.92 58.42
RNN + PRA Classifier-b 51.94 61.17
</table>
<tableCaption confidence="0.998805">
Table 3: Results comparing different methods on 46 types. All the methods perform better when trained
</tableCaption>
<bodyText confidence="0.989063428571429">
using all the paths than training using the top 1, 000 paths. When training with all the paths, RNN
performs significantly (p &lt; 0.005) better than PRA Classifier and Cluster PRA Classifier. The small
difference in performance between RNN and both PRA Classifier-b and Cluster PRA Classifier-b is not
statistically significant. The best results are obtained by combining the predictions of RNN with PRA
Classifier-b which performs significantly (p &lt; 10−5) better than both PRA Classifier-b and Cluster PRA
Classifier-b.
ter PRA method. The small difference in perfor-
mance between RNN and both PRA Classifier-b
and Cluster PRA Classifier-b is not statistically
significant. We conjecture that our method has
substantially different strengths than the new base-
line. While the classifier with bigram features has
an ability to accurately memorize important local
structure, the RNN model generalizes better to un-
</bodyText>
<page confidence="0.991551">
162
</page>
<table confidence="0.9957305">
train with train with
top 1000 paths all paths
Method MAP MAP
RNN 43.82 50.10
zero-shot 19.28 20.61
Random 7.59
</table>
<tableCaption confidence="0.997126">
Table 4: Results comparing the zero-shot model
</tableCaption>
<bodyText confidence="0.945244454545454">
with supervised RNN and a random baseline on
10 types. RNN is the fully supervised model de-
scribed in section 3 while zero-shot is the model
described in section 4. The zero-shot model with-
out explicitly training for the target relation types
achieves impressive results by performing signifi-
cantly (p &lt; 0.05) better than a random baseline.
seen paths that are very different from the paths
seen is training. Empirically, combining the pre-
dictions of RNN and PRA Classifier-b achieves a
statistically significant gain over PRA Classifier-b.
</bodyText>
<subsectionHeader confidence="0.975026">
5.3.1 Zero-shot
</subsectionHeader>
<bodyText confidence="0.99998205882353">
Table 4 shows the results of the zero-shot model
described in section 4 compared with the fully su-
pervised RNN model (section 3) and a baseline
that produces a random ordering of the test facts.
We evaluate on randomly selected 10 (out of 46)
relation types, hence for the fully supervised ver-
sion we train 10 RNNs, one for each relation type.
For evaluating the zero-shot model, we randomly
split the relations into two sets of equal size and
train a zero-shot model on one set and test on the
other set. So, in this case we have two RNNs
making predictions on relation types that they have
never seen during training. As expected, the fully
supervised RNN outperforms the zero-shot model
by a large margin but the zero-shot model with-
out using any direct supervision clearly performs
much better than a random baseline.
</bodyText>
<subsectionHeader confidence="0.77942">
5.3.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999977375">
To investigate whether the performance of the
RNNs were affected by multiple local optima is-
sues, we combined the predictions of five different
RNNs trained using all the paths. Apart from RNN
and RNN-random, we trained three more RNNs
with different random initialization and the perfor-
mance of the three RNNs individually are 57.09,
57.11 and 56.91. The performance of the ensem-
ble is 59.16 and their performance stopped im-
proving after using three RNNs. So, this indicates
that even though multiple local optima affects the
performance, it is likely not the only issue since
the performance of the ensemble is still less than
the performance of RNN + PRA Classifier-b.
We suspect the RNN model does not capture
some of the important local structure as well as
the classifier using bigram features. To overcome
this drawback, in future work, we plan to explore
compositional models that have a longer memory
(Hochreiter and Schmidhuber, 1997; Cho et al.,
2014; Mikolov et al., 2014). We also plan to in-
clude vector representations for the entities and
develop models that address the issue of polysemy
in verb phrases (Cheng et al., 2014).
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.991830916666667">
KB Completion includes methods such as Lin
and Pantel (2001), Yates and Etzioni (2007) and
Berant et al. (2011) that learn inference rules of
length one. Schoenmackers et al. (2010) learn
general inference rules by considering the set of
all paths in the KB and selecting paths that sat-
isfy a certain precision threshold. Their method
does not scale well to modern KBs and also de-
pends on carefully tuned thresholds. Lao et al.
(2011) train a simple logistic regression classifier
with NELL KB paths as features to perform KB
completion while Gardner et al. (2013) and Gard-
ner et al. (2014) extend it by using pre-trained re-
lation vectors to overcome feature sparsity. Re-
cently, Yang et al. (2014) learn inference rules us-
ing simple element-wise addition or multiplication
as the composition function.
Compositional Vector Space Models have been
developed to represent phrases and sentences in
natural language as vectors (Mitchell and Lap-
ata, 2008; Baroni and Zamparelli, 2010; Yesse-
nalina and Cardie, 2011). Neural networks have
been successfully used to learn vector representa-
tions of phrases using the vector representations
of the words in that phrase. Recurrent neural net-
works have been used for many tasks such as lan-
guage modeling (Mikolov et al., 2010), machine
translation (Sutskever et al., 2014) and parsing
(Vinyals et al., 2014). Recursive neural networks,
a more general version of the recurrent neural net-
works have been used for many tasks like pars-
ing (Socher et al., 2011), sentiment classification
(Socher et al., 2012; Socher et al., 2013c; Irsoy
and Cardie, 2014), question answering (Iyyer et
al., 2014) and natural language logical semantics
(Bowman et al., 2014). Our overall approach is
</bodyText>
<page confidence="0.997836">
163
</page>
<bodyText confidence="0.999890272727273">
similar to RNNs with attention (Bahdanau et al.,
2014; Graves, 2013) since we select a path among
the set of paths connecting the entity pair to make
the final prediction.
Zero-shot or zero-data learning was introduced
in Larochelle et al. (2008) for character recogni-
tion and drug discovery. Palatucci et al. (2009)
perform zero-shot learning for neural decoding
while there has been plenty of work in this direc-
tion for image recognition (Socher et al., 2013b;
Frome et al., 2013; Norouzi et al., 2014).
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999934428571429">
We develop a compositional vector space
model for knowledge base completion using
recurrent neural networks. In our challeng-
ing large-scale dataset available at http:
//iesl.cs.umass.edu/downloads/
inferencerules/release.tar.gz,
our method outperforms two baseline methods
and performs competitively with a modified
stronger baseline. The best results are obtained
by combining the predictions of our model with
the predictions of the modified baseline which
achieves a 15% improvement over Gardner et
al. (2013). We also show that our model has the
ability to perform zero-shot inference.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972266666667">
We thank Matt Gardner for releasing the PRA
code, and for answering numerous question about
the code and data. We also thank the Stanford
NLP group for releasing the neural networks code.
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by
DARPA under agreement number FA8750-13-2-
0020, in part by an award from Google, and in
part by NSF grant #CNS-0958392. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Governmental purposes notwithstand-
ing any copyright notation thereon. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.998922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998588428571429">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In ArXiv.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Inter-
national Joint Conference on Artificial Intelligence.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Empirical Methods in Natural Language Process-
ing.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Association for Computational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the ACM SIG-
MOD International Conference on Management of
Data.
Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-
Dur´an, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems.
Samuel R. Bowman, Christopher Potts, and Christo-
pher D Manning. 2014. Recursive neural networks
for learning logical semantics. In CoRR.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and A. 2010. Toward
an architecture for never-ending language learning.
In In AAAI.
Cheng, Jianpeng Kartsaklis, and Edward Grefenstette.
2014. Investigating the role of prior disambiguation
in deep-learning compositional models of meaning.
In In Learning Semantics workshop NIPS.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. In Workshop on Syntax, Semantics and
Structure in Statistical Translation.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. In Journal of Machine
Learning Research.
Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. Devise: A deep visual-
semantic embedding model. In Neural Information
Processing Systems.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom M. Mitchell. 2013. Improving learning
and inference in a large knowledge-base using la-
tent syntactic cues. In Empirical Methods in Natural
Language Processing.
</reference>
<page confidence="0.993486">
164
</page>
<reference confidence="0.999557899999999">
Matt Gardner, Partha Talukdar, Jayant Krishnamurthy,
and Tom Mitchell. 2014. Incorporating vector space
similarity in random walk inference over knowledge
bases. In Empirical Methods in Natural Language
Processing.
Christoph Goller and Andreas K¨uchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Trans-
actions on Neural Networks.
Alex Graves. 2013. Generating sequences with recur-
rent neural networks. In ArXiv.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. In Neural Computation.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Neural Information Processing Systems.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Empirical Methods in Natural Language
Processing.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Conference on Empirical Meth-
ods in Natural Language Processing.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio.
2008. Zero-data learning of new tasks. In National
Conference on Artificial Intelligence.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In International Con-
ference on Knowledge Discovery and Data Mining.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In An-
nual Conference of the International Speech Com-
munication Association.
Tomas Mikolov, Armand Joulin, Sumit Chopra,
Micha¨el Mathieu, and Marc’Aurelio Ranzato. 2014.
Learning longer memory in recurrent neural net-
works. In CoRR.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In HLT-NAACL, pages 777–782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Association for Com-
putational Linguistics and International Joint Con-
ference on Natural Language Processing.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Association for
Computational Linguistics.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Empirical Methods in Nat-
ural Language Processing.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In International
Conference on Machine Learning.
Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome,
Greg Corrado, and Jeffrey Dean. 2014. Zero-shot
learning by convex combination of semantic em-
beddings. In International Conference on Learning
Representations.
Dave Orr, Amarnag Subramanya, Evgeniy
Gabrilovich, and Michael Ringgaard. 2013.
11 billion clues in 800 million documents: A web
research corpus annotated with freebase concepts.
http://googleresearch.blogspot.com/2013/07/11-
billion-clues-in-800-million.html.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
HLT-NAACL.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Empirical Methods in Nat-
ural Language Processing.
Richard Socher, Cliff Chiung-Yu Lin, Christopher D.
Manning, and Andrew Y. Ng. 2011. Parsing natu-
ral scenes and natural language with recursive neural
networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013a. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems.
Richard Socher, Milind Ganjoo, Christopher D Man-
ning, and Andrew Ng. 2013b. Zero-shot learning
through cross-modal transfer. In Neural Information
Processing Systems.
</reference>
<page confidence="0.984019">
165
</page>
<reference confidence="0.999381342857143">
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013c. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Conference on Empirical Methods in
Natural Language Processing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web.
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information Pro-
cessing Systems.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2014. Gram-
mar as a foreign language. In CoRR.
Paul Werbos. 1990. Backpropagation through time:
what it does and how to do it. In IEEE.
Jason Weston, Ron Weiss, and Hector Yee. 2013.
Nonlinear latent factorization by embedding multi-
ple user interests. In ACM International Conference
on Recommender Systems.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Embedding entities and
relations for learning and inference in knowledge
bases. In CoRR.
Alexander Yates and Oren Etzioni. 2007. Unsuper-
vised resolution of objects and relations on the web.
In North American Chapter of the Association for
Computational Linguistics.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Empirical Methods in Natural Language Process-
ing.
</reference>
<page confidence="0.998746">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.583917">
<title confidence="0.997902">Compositional Vector Space Models for Knowledge Base Completion</title>
<author confidence="0.997001">Arvind Neelakantan</author>
<author confidence="0.997001">Benjamin Roth</author>
<author confidence="0.997001">Andrew</author>
<affiliation confidence="0.999989">Department of Computer University of Massachusetts,</affiliation>
<address confidence="0.996732">Amherst, MA,</address>
<abstract confidence="0.998196846153846">Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferwith high likelihood Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, This paper presents an approach that reasons about conjunctions of multi-hop relations composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained em-</abstract>
<note confidence="0.61415">beddings by 7%.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2014</date>
<booktitle>In ArXiv.</booktitle>
<contexts>
<context position="32693" citStr="Bahdanau et al., 2014" startWordPosition="5287" endWordPosition="5290">phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. In ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="3587" citStr="Banko et al., 2007" startWordPosition="539" endWordPosition="542"> typical predictive feature learned by PRA is that CountryOflleadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOflleadquarters(Microsoft, USA) using the predictive feature. In later work, Lao et al. (2012) greatly increase available raw material for paths by augmenting KB-schema relations with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations (Banko et al., 2007)). However, these symbolic methods can produce many millions of distinct paths, each of which is categorically distinct, treated by PRA as a dis156 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 156–166, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tinct feature. (See Figure 1.) Even putting aside the OpenIE relations, this limits the applicability of these methods to modern KBs that have thousands of relation types, since the number of</context>
<context position="21608" citStr="Banko et al., 2007" startWordPosition="3553" endWordPosition="3556">e nodes only for Freebase entities in our KB graph. We remove facts containing /type/object/type as they do not give useful predictive information for our task. We get triples from ClueWeb by considering sentences that contain two entities linked to Freebase. We extract the phrase between the two entities and treat them as the relation types. For phrases that are of length greater than four we keep only the first and last two words. This helps us to avoid the time consuming step of dependency parsing the sentence to get the relation type. These triples are similar to facts obtained by OpenIE (Banko et al., 2007). To reduce noise, we select relation types that occur at least 50 times. We evaluate on 46 relation types in Freebase that have the most number of instances. The methods are evaluated on a subset of facts in Freebase that were hidden during training. Table 1 shows important statistics of our dataset. 5.2 Predictive Paths Table 2 shows predictive paths for 4 relations learned by the RNN model. The high quality of unseen paths is indicative of the fact that the RNN model is able to generalize to paths that are never seen during training. 5.3 Results Using our dataset, we compare the performance</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="31898" citStr="Baroni and Zamparelli, 2010" startWordPosition="5159" endWordPosition="5162">well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Car</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31032" citStr="Berant et al. (2011)" startWordPosition="5018" endWordPosition="5021">RNN + PRA Classifier-b. We suspect the RNN model does not capture some of the important local structure as well as the classifier using bigram features. To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014). We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inferen</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM SIGMOD International Conference on Management of Data.</booktitle>
<contexts>
<context position="1594" citStr="Bollacker et al., 2008" startWordPosition="236" endWordPosition="239">-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%. 1 Introduction Constructing large knowledge bases (KBs) supports downstream reasoning about resolved entities and their relations, rather than the noisy textual evidence surrounding their natural language mentions. For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010). Such KBs typically contain many millions of facts, most of them (entity1,relation,entity2) “triples” (also known as binary relations) such as (Barack Obama, presidentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie). However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the app</context>
<context position="8312" citStr="Bollacker et al., 2008" startWordPosition="1269" endWordPosition="1272">sing the embeddings. Neither is able to perform zero-shot learning since there must be a classifer for each predicted relation type. Furthermore their pre-trained vectors do not have the opportunity to be tuned to the KB completion task because the two sub-tasks are completely disentangled. An additional contribution of our work is a new large-scale data set of over 52 million triples, and its preprocessing for purposes of path-based KB completion (can be downloaded from http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz). The dataset is build from the combination of Freebase (Bollacker et al., 2008) and Google’s entity linking in ClueWeb (Orr et al., 2013). Rather than Gardner’s 1000 distinct paths per relation type, we have over 2 million. Rather than Gardner’s 200 IsBasedIn StateLocatedIn CountryLocatedIn USA Microsoft Seattle Washington headquartered in in the U.S. state of state part of headquarters located in located in the state of state in the NW region of founded in beautiful city in located in country based in in state democratic state in 157 Figure 2: Vector Representations of the paths are computed by applying the composition function recursively. entity pairs, we use over 10k</context>
<context position="20552" citStr="Bollacker et al., 2008" startWordPosition="3380" endWordPosition="3383">pective of whether they are seen at training by comparing it with the pre-trained relation vectors. 5 Experiments The hyperparameters of all the models were tuned on the same held-out development data. All the neural network models are trained for 150 iterations using 50 dimensional relation vectors, and we set the L2-regularizer and learning rate to 0.0001 and 0.1 respectively. We halved the learning rate after every 60 iterations and use minibatches of size 20. The neural networks and the classifiers were optimized using AdaGrad (Duchi et al., 2011). 5.1 Data We ran experiments on Freebase (Bollacker et al., 2008) enriched with information from ClueWeb. 160 Entities 18M Freebase triples 40M ClueWeb triples 12M Relations 25,994 Relation types tested 46 Avg. paths/relation 2.3M Avg. training facts/relation 6638 Avg. positive test instances/relation 3492 Avg. negative test instances/relation 43,160 Table 1: Statistics of our dataset. We use the publicly available entity links to Freebase in the ClueWeb dataset (Orr et al., 2013). Hence, we create nodes only for Freebase entities in our KB graph. We remove facts containing /type/object/type as they do not give useful predictive information for our task. We</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD International Conference on Management of Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto Garc´ıaDur´an</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<marker>Bordes, Usunier, Garc´ıaDur´an, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto Garc´ıaDur´an, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
<author>Christopher Potts</author>
<author>Christopher D Manning</author>
</authors>
<title>Recursive neural networks for learning logical semantics.</title>
<date>2014</date>
<journal>In CoRR.</journal>
<contexts>
<context position="32610" citStr="Bowman et al., 2014" startWordPosition="5273" endWordPosition="5276"> representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for k</context>
</contexts>
<marker>Bowman, Potts, Manning, 2014</marker>
<rawString>Samuel R. Bowman, Christopher Potts, and Christopher D Manning. 2014. Recursive neural networks for learning logical semantics. In CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka</author>
<author>A</author>
</authors>
<title>Toward an architecture for never-ending language learning. In</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="1640" citStr="Carlson et al., 2010" startWordPosition="244" endWordPosition="247">t seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%. 1 Introduction Constructing large knowledge bases (KBs) supports downstream reasoning about resolved entities and their relations, rather than the noisy textual evidence surrounding their natural language mentions. For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010). Such KBs typically contain many millions of facts, most of them (entity1,relation,entity2) “triples” (also known as binary relations) such as (Barack Obama, presidentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie). However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the application of an automated process. The addition</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Hruschka, A, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, and A. 2010. Toward an architecture for never-ending language learning. In In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianpeng Kartsaklis Cheng</author>
<author>Edward Grefenstette</author>
</authors>
<title>Investigating the role of prior disambiguation in deep-learning compositional models of meaning.</title>
<date>2014</date>
<booktitle>In In Learning Semantics workshop NIPS.</booktitle>
<marker>Cheng, Grefenstette, 2014</marker>
<rawString>Cheng, Jianpeng Kartsaklis, and Edward Grefenstette. 2014. Investigating the role of prior disambiguation in deep-learning compositional models of meaning. In In Learning Semantics workshop NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoder–decoder approaches.</title>
<date>2014</date>
<booktitle>In Workshop on Syntax, Semantics and Structure in Statistical Translation.</booktitle>
<marker>Cho, van Merrienboer, Bahdanau, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder–decoder approaches. In Workshop on Syntax, Semantics and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>In Journal of Machine Learning Research.</journal>
<contexts>
<context position="20486" citStr="Duchi et al., 2011" startWordPosition="3369" endWordPosition="3372">ountryLocatedIn in Figure 2, we can predict any relation irrespective of whether they are seen at training by comparing it with the pre-trained relation vectors. 5 Experiments The hyperparameters of all the models were tuned on the same held-out development data. All the neural network models are trained for 150 iterations using 50 dimensional relation vectors, and we set the L2-regularizer and learning rate to 0.0001 and 0.1 respectively. We halved the learning rate after every 60 iterations and use minibatches of size 20. The neural networks and the classifiers were optimized using AdaGrad (Duchi et al., 2011). 5.1 Data We ran experiments on Freebase (Bollacker et al., 2008) enriched with information from ClueWeb. 160 Entities 18M Freebase triples 40M ClueWeb triples 12M Relations 25,994 Relation types tested 46 Avg. paths/relation 2.3M Avg. training facts/relation 6638 Avg. positive test instances/relation 3492 Avg. negative test instances/relation 43,160 Table 1: Statistics of our dataset. We use the publicly available entity links to Freebase in the ClueWeb dataset (Orr et al., 2013). Hence, we create nodes only for Freebase entities in our KB graph. We remove facts containing /type/object/type </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. In Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Gregory S Corrado</author>
<author>Jonathon Shlens</author>
<author>Samy Bengio</author>
<author>Jeffrey Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>Devise: A deep visualsemantic embedding model.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="33121" citStr="Frome et al., 2013" startWordPosition="5358" endWordPosition="5361">2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-scale dataset available at http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz, our method outperforms two baseline methods and performs competitively with a modified stronger baseline. The best results are obtained by combining the predictions of our model with the predictions of the modified baseline which achieves a 15% improvement over Gardner et al. (2013). We also show that our model has the ability to p</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visualsemantic embedding model. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Pratim Talukdar</author>
<author>Bryan Kisiel</author>
<author>Tom M Mitchell</author>
</authors>
<title>Improving learning and inference in a large knowledge-base using latent syntactic cues.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7213" citStr="Gardner et al., 2013" startWordPosition="1100" endWordPosition="1103">een during training, because of the generalization provided by vector neighborhoods, and because they are composed in non-atomic fashion. This allows our model to seamlessly perform inference on many millions of paths in the KB graph. In most of our experiments, we learn a separate RNN for predicting each relation type, but alternatively, by learning a single high-capacity composition function for all relation types, our method can perform zero-shot learning—predicting new relation types for which the composition function was never explicitly trained. Related to our work, new versions of PRA (Gardner et al., 2013; Gardner et al., 2014) use pre-trained vector representations of relations to alleviate its feature explosion problem—but the core mechanism continues to be a classifier based on atomic-path features. In the 2013 work many paths are collapsed by clustering paths according to their relations’ embeddings, and substituting cluster ids for the original relation types. In the 2014 work unseen paths are mapped to nearby paths seen at training time, where nearness is measured using the embeddings. Neither is able to perform zero-shot learning since there must be a classifer for each predicted relati</context>
<context position="9124" citStr="Gardner et al., 2013" startWordPosition="1398" endWordPosition="1401">edIn CountryLocatedIn USA Microsoft Seattle Washington headquartered in in the U.S. state of state part of headquarters located in located in the state of state in the NW region of founded in beautiful city in located in country based in in state democratic state in 157 Figure 2: Vector Representations of the paths are computed by applying the composition function recursively. entity pairs, we use over 10k. All experimental comparisons below are performed on this new data set. On this challenging large-scale dataset our compositional method outperforms PRA (Lao et al., 2012), and Cluster PRA (Gardner et al., 2013) by 11% and 7% respectively. A further contribution of our work is a new, surprisingly strong baseline method using classifiers of path bigram features, which beats PRA and Cluster PRA, and statistically ties our compositional method. Our analysis shows that our method has substantially different strengths than the new baseline, and the combination of the two yields a 15% improvement over Gardner et al. (2013). We also show that our zeroshot model is indeed capable of predicting new unseen relation types. 2 Background We give background on PRA which we use to obtain a set of paths connecting t</context>
<context position="18469" citStr="Gardner et al., 2013" startWordPosition="3040" endWordPosition="3043">g (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-trained vectors, so that we can predict relations that are unseen during training. This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014). We learn a general composition matrix for all relations instead of learning a separate composition matrix for every relation to be predicted. So, 2we sub-sample a portion of the set of all unobserved instances. vp(7r) = f(W[vr(IsBasedIn); vr(StateLocatedIn)]) where W E Rd*2d+1 is the general composition matrix. We initialize the vector representations of the binary relations (vr) using the representations learned in Riedel et al. (2013) and do not update them during training. The relation vectors are not updated because at prediction time we would be predicting relatio</context>
<context position="22435" citStr="Gardner et al. (2013)" startWordPosition="3697" endWordPosition="3701">in Freebase that were hidden during training. Table 1 shows important statistics of our dataset. 5.2 Predictive Paths Table 2 shows predictive paths for 4 relations learned by the RNN model. The high quality of unseen paths is indicative of the fact that the RNN model is able to generalize to paths that are never seen during training. 5.3 Results Using our dataset, we compare the performance of the following methods: PRA Classifier is the method in Lao et al. (2012) which trains a logistic regression classifier by creating a feature for every path type. Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et</context>
<context position="24384" citStr="Gardner et al., 2013" startWordPosition="4034" endWordPosition="4037"> predictions by assigning the score of a fact as the sum of their rank in the two models after sorting them in ascending order. RNN + PRA Classifier-b combines the predictions of RNN and PRA Classifier-b using the technique described previously. Table 3 shows the results of our experiments. The method described in Gardner et al. (2014) is not included in the table since the publicly available implementation does not scale to our large dataset. First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014). We can see that the RNN model performs significantly better than the baseline methods of Lao et al. (2012) and Gardner et al. (2013). The performance of the RNN model is not affected by initialization since using random vectors and pre-trained vectors results in similar performance. A surprising result is the impressive performance of our simple extension to the classifier approach. After the addition of bigram features, the naive PRA method is as effective as the Clus161 Relation: /book/written work/original language/ (book “x” written in language “y”) Seen paths: /bo</context>
<context position="31485" citStr="Gardner et al. (2013)" startWordPosition="5095" endWordPosition="5098">emy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recur</context>
<context position="33671" citStr="Gardner et al. (2013)" startWordPosition="5433" endWordPosition="5436">rection for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-scale dataset available at http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz, our method outperforms two baseline methods and performs competitively with a modified stronger baseline. The best results are obtained by combining the predictions of our model with the predictions of the modified baseline which achieves a 15% improvement over Gardner et al. (2013). We also show that our model has the ability to perform zero-shot inference. Acknowledgments We thank Matt Gardner for releasing the PRA code, and for answering numerous question about the code and data. We also thank the Stanford NLP group for releasing the neural networks code. This work was supported in part by the Center for Intelligent Information Retrieval, in part by DARPA under agreement number FA8750-13-2- 0020, in part by an award from Google, and in part by NSF grant #CNS-0958392. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwi</context>
</contexts>
<marker>Gardner, Talukdar, Kisiel, Mitchell, 2013</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom M. Mitchell. 2013. Improving learning and inference in a large knowledge-base using latent syntactic cues. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Talukdar</author>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Incorporating vector space similarity in random walk inference over knowledge bases.</title>
<date>2014</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7236" citStr="Gardner et al., 2014" startWordPosition="1104" endWordPosition="1107">ecause of the generalization provided by vector neighborhoods, and because they are composed in non-atomic fashion. This allows our model to seamlessly perform inference on many millions of paths in the KB graph. In most of our experiments, we learn a separate RNN for predicting each relation type, but alternatively, by learning a single high-capacity composition function for all relation types, our method can perform zero-shot learning—predicting new relation types for which the composition function was never explicitly trained. Related to our work, new versions of PRA (Gardner et al., 2013; Gardner et al., 2014) use pre-trained vector representations of relations to alleviate its feature explosion problem—but the core mechanism continues to be a classifier based on atomic-path features. In the 2013 work many paths are collapsed by clustering paths according to their relations’ embeddings, and substituting cluster ids for the original relation types. In the 2014 work unseen paths are mapped to nearby paths seen at training time, where nearness is measured using the embeddings. Neither is able to perform zero-shot learning since there must be a classifer for each predicted relation type. Furthermore th</context>
<context position="18492" citStr="Gardner et al., 2014" startWordPosition="3044" endWordPosition="3047">2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-trained vectors, so that we can predict relations that are unseen during training. This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014). We learn a general composition matrix for all relations instead of learning a separate composition matrix for every relation to be predicted. So, 2we sub-sample a portion of the set of all unobserved instances. vp(7r) = f(W[vr(IsBasedIn); vr(StateLocatedIn)]) where W E Rd*2d+1 is the general composition matrix. We initialize the vector representations of the binary relations (vr) using the representations learned in Riedel et al. (2013) and do not update them during training. The relation vectors are not updated because at prediction time we would be predicting relation types which are never</context>
<context position="24101" citStr="Gardner et al. (2014)" startWordPosition="3980" endWordPosition="3983">ore computing the bigram features. Cluster PRA Classifier-b is our simple extension to the method in Gardner et al. (2013) which additionally uses bigram features computed as previously described. RNN + PRA Classifier combines the predictions of RNN and PRA Classifier. We combine the predictions by assigning the score of a fact as the sum of their rank in the two models after sorting them in ascending order. RNN + PRA Classifier-b combines the predictions of RNN and PRA Classifier-b using the technique described previously. Table 3 shows the results of our experiments. The method described in Gardner et al. (2014) is not included in the table since the publicly available implementation does not scale to our large dataset. First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014). We can see that the RNN model performs significantly better than the baseline methods of Lao et al. (2012) and Gardner et al. (2013). The performance of the RNN model is not affected by initialization since using random vectors and pre-trained vectors results in similar performance. A surpri</context>
<context position="31511" citStr="Gardner et al. (2014)" startWordPosition="5100" endWordPosition="5104"> et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have </context>
</contexts>
<marker>Gardner, Talukdar, Krishnamurthy, Mitchell, 2014</marker>
<rawString>Matt Gardner, Partha Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014. Incorporating vector space similarity in random walk inference over knowledge bases. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas K¨uchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure. In</title>
<date>1996</date>
<journal>IEEE Transactions on Neural Networks.</journal>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>Christoph Goller and Andreas K¨uchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In IEEE Transactions on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In ArXiv.</booktitle>
<contexts>
<context position="32708" citStr="Graves, 2013" startWordPosition="5291" endWordPosition="5292">l networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-scale dataset a</context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. In ArXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>In Neural Computation.</booktitle>
<contexts>
<context position="30707" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="4961" endWordPosition="4964">individually are 57.09, 57.11 and 56.91. The performance of the ensemble is 59.16 and their performance stopped improving after using three RNNs. So, this indicates that even though multiple local optima affects the performance, it is likely not the only issue since the performance of the ensemble is still less than the performance of RNN + PRA Classifier-b. We suspect the RNN model does not capture some of the important local structure as well as the classifier using bigram features. To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014). We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. In Neural Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="14184" citStr="Irsoy and Cardie, 2014" startWordPosition="2241" endWordPosition="2244">he path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn by comparing the vector representation of the path (vp(H)) with the vector representation of the relation CountryOfHeadquarters (vr(CountryOfHeadquarters)) using the sigmoid function. 3.1 Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014). 1we did not get significant improvements when we tried more sophisticated ordering schemes for computi</context>
<context position="32508" citStr="Irsoy and Cardie, 2014" startWordPosition="5258" endWordPosition="5261">parelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan Boyd-Graber</author>
<author>Leonardo Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<date>2014</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014. A neural network for factoid question answering over paragraphs. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2743" citStr="Lao et al. (2011)" startWordPosition="422" endWordPosition="425">representing a sort of inconsistency that can be repaired by the application of an automated process. The addition of new triples by leveraging existing triples is typically known as KB completion. Early work on this problem focused on learning symbolic rules. For example, Schoenmackers et al. (2010) learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of increasing length, and selecting those surpassing an accuracy threshold. (A “path” is a sequence of triples in which the second entity of each triple matches the first entity of the next triple.) Lao et al. (2011) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryOflleadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOflleadquarters(Microsoft, USA) using the predictive feature. In later w</context>
<context position="9982" citStr="Lao et al., 2011" startWordPosition="1548" endWordPosition="1551">alysis shows that our method has substantially different strengths than the new baseline, and the combination of the two yields a 15% improvement over Gardner et al. (2013). We also show that our zeroshot model is indeed capable of predicting new unseen relation types. 2 Background We give background on PRA which we use to obtain a set of paths connecting the entity pairs and the RNN model which we employ to model the composition function. 2.1 Path Ranking Algorithm Since it is impractical to exhaustively obtain the set of all paths connecting an entity pair in the large KB graph, we use PRA (Lao et al., 2011) to obtain a set of paths connecting the entity pairs. Given a training set of entity pairs for a relation, PRA heuristically finds a set of paths by performing random walks from the source and target nodes keeping the most common paths. We use PRA to find millions of distinct paths per relation type. We do not use the random walk probabilities given by PRA since using it did not yield improvements in our experiments. 2.2 Recurrent Neural Networks Recurrent neural network (RNN) (Werbos, 1990) is a neural network that constructs vector representation for sequences (of any length). For example, </context>
<context position="14037" citStr="Lao et al., 2011" startWordPosition="2214" endWordPosition="2217">s our model a recurrent neural network (Werbos, 1990). Finally, we make a prediction regarding CountryOfHeadquarters(Microsoft, USA) using the path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn by comparing the vector representation of the path (vp(H)) with the vector representation of the relation CountryOfHeadquarters (vr(CountryOfHeadquarters)) using the sigmoid function. 3.1 Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Wes</context>
<context position="18447" citStr="Lao et al., 2011" startWordPosition="3036" endWordPosition="3039"> zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-trained vectors, so that we can predict relations that are unseen during training. This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014). We learn a general composition matrix for all relations instead of learning a separate composition matrix for every relation to be predicted. So, 2we sub-sample a portion of the set of all unobserved instances. vp(7r) = f(W[vr(IsBasedIn); vr(StateLocatedIn)]) where W E Rd*2d+1 is the general composition matrix. We initialize the vector representations of the binary relations (vr) using the representations learned in Riedel et al. (2013) and do not update them during training. The relation vectors are not updated because at prediction time we would</context>
<context position="31355" citStr="Lao et al. (2011)" startWordPosition="5074" endWordPosition="5077">al., 2014). We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have bee</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="3365" citStr="Lao et al. (2012)" startWordPosition="504" endWordPosition="507">oduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryOflleadquarters(X, Y) is implied by IsBasedIn(X,A) and StateLocatedIn(A, B) and CountryLocatedIn(B, Y). Given IsBasedIn(Microsoft, Seattle), StateLocatedIn(Seattle, Washington) and CountryLocatedIn(Washington, USA), we can infer the fact CountryOflleadquarters(Microsoft, USA) using the predictive feature. In later work, Lao et al. (2012) greatly increase available raw material for paths by augmenting KB-schema relations with relations defined by the text connecting mentions of entities in a large corpus (also known as OpenIE relations (Banko et al., 2007)). However, these symbolic methods can produce many millions of distinct paths, each of which is categorically distinct, treated by PRA as a dis156 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 156–166, Beijing, China, July 26-31, 2015. c�2015 Association</context>
<context position="9084" citStr="Lao et al., 2012" startWordPosition="1391" endWordPosition="1394">n Gardner’s 200 IsBasedIn StateLocatedIn CountryLocatedIn USA Microsoft Seattle Washington headquartered in in the U.S. state of state part of headquarters located in located in the state of state in the NW region of founded in beautiful city in located in country based in in state democratic state in 157 Figure 2: Vector Representations of the paths are computed by applying the composition function recursively. entity pairs, we use over 10k. All experimental comparisons below are performed on this new data set. On this challenging large-scale dataset our compositional method outperforms PRA (Lao et al., 2012), and Cluster PRA (Gardner et al., 2013) by 11% and 7% respectively. A further contribution of our work is a new, surprisingly strong baseline method using classifiers of path bigram features, which beats PRA and Cluster PRA, and statistically ties our compositional method. Our analysis shows that our method has substantially different strengths than the new baseline, and the combination of the two yields a 15% improvement over Gardner et al. (2013). We also show that our zeroshot model is indeed capable of predicting new unseen relation types. 2 Background We give background on PRA which we u</context>
<context position="22284" citStr="Lao et al. (2012)" startWordPosition="3671" endWordPosition="3674">east 50 times. We evaluate on 46 relation types in Freebase that have the most number of instances. The methods are evaluated on a subset of facts in Freebase that were hidden during training. Table 1 shows important statistics of our dataset. 5.2 Predictive Paths Table 2 shows predictive paths for 4 relations learned by the RNN model. The high quality of unseen paths is indicative of the fact that the RNN model is able to generalize to paths that are never seen during training. 5.3 Results Using our dataset, we compare the performance of the following methods: PRA Classifier is the method in Lao et al. (2012) which trains a logistic regression classifier by creating a feature for every path type. Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gard</context>
<context position="24515" citStr="Lao et al. (2012)" startWordPosition="4058" endWordPosition="4061">PRA Classifier-b combines the predictions of RNN and PRA Classifier-b using the technique described previously. Table 3 shows the results of our experiments. The method described in Gardner et al. (2014) is not included in the table since the publicly available implementation does not scale to our large dataset. First, we show that it is better to train the models using all the path types instead of using only the top 1, 000 path types as done in previous work (Gardner et al., 2013; Gardner et al., 2014). We can see that the RNN model performs significantly better than the baseline methods of Lao et al. (2012) and Gardner et al. (2013). The performance of the RNN model is not affected by initialization since using random vectors and pre-trained vectors results in similar performance. A surprising result is the impressive performance of our simple extension to the classifier approach. After the addition of bigram features, the naive PRA method is as effective as the Clus161 Relation: /book/written work/original language/ (book “x” written in language “y”) Seen paths: /book/written work/previous in series → /book/written work/author → /people/person/nationality → /people/person/nationality−1 → /peopl</context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W. Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Larochelle</author>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
</authors>
<title>Zero-data learning of new tasks.</title>
<date>2008</date>
<booktitle>In National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="17875" citStr="Larochelle et al., 2008" startWordPosition="2939" endWordPosition="2942">zed randomly. We train the network using backpropagation through structure (Goller and K¨uchler, 1996). 4 Zero-shot KB Completion The KB completion task involves predicting facts on thousands of relations types and it is highly desirable that a method can infer facts about relation types without directly training for them. Given the vector representation of the relations, we show that our model described in the previous section is capable of predicting relational facts without explicitly training for the target (or test) relation types (zero-shot learning). In zero-shot or zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-trained vectors, so that we can predict relations that are unseen during training. This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gard</context>
<context position="32886" citStr="Larochelle et al. (2008)" startWordPosition="5319" endWordPosition="5322">14). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-scale dataset available at http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz, our method outperforms two baseline methods and performs competitively with a modified stronger ba</context>
</contexts>
<marker>Larochelle, Erhan, Bengio, 2008</marker>
<rawString>Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. 2008. Zero-data learning of new tasks. In National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="30981" citStr="Lin and Pantel (2001)" startWordPosition="5009" endWordPosition="5012"> the ensemble is still less than the performance of RNN + PRA Classifier-b. We suspect the RNN model does not capture some of the important local structure as well as the classifier using bigram features. To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014). We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature s</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Annual Conference of the International Speech Communication Association.</booktitle>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Annual Conference of the International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Armand Joulin</author>
<author>Sumit Chopra</author>
<author>Micha¨el Mathieu</author>
<author>Marc’Aurelio Ranzato</author>
</authors>
<title>Learning longer memory in recurrent neural networks.</title>
<date>2014</date>
<booktitle>In CoRR.</booktitle>
<marker>Mikolov, Joulin, Chopra, Micha¨el Mathieu, Ranzato, 2014</marker>
<rawString>Tomas Mikolov, Armand Joulin, Sumit Chopra, Micha¨el Mathieu, and Marc’Aurelio Ranzato. 2014. Learning longer memory in recurrent neural networks. In CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>777--782</pages>
<contexts>
<context position="1933" citStr="Min et al., 2013" startWordPosition="291" endWordPosition="294"> bases (KBs) supports downstream reasoning about resolved entities and their relations, rather than the noisy textual evidence surrounding their natural language mentions. For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010). Such KBs typically contain many millions of facts, most of them (entity1,relation,entity2) “triples” (also known as binary relations) such as (Barack Obama, presidentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie). However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the application of an automated process. The addition of new triples by leveraging existing triples is typically known as KB completion. Early work on this problem focused on learning symbolic rules. For example, Schoenmackers et al. (2010) learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of inc</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In HLT-NAACL, pages 777–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="14019" citStr="Mintz et al., 2009" startWordPosition="2210" endWordPosition="2213">the left1. This makes our model a recurrent neural network (Werbos, 1990). Finally, we make a prediction regarding CountryOfHeadquarters(Microsoft, USA) using the path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn by comparing the vector representation of the path (vp(H)) with the vector representation of the relation CountryOfHeadquarters (vr(CountryOfHeadquarters)) using the sigmoid function. 3.1 Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RN</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Association for Computational Linguistics.</booktitle>
<contexts>
<context position="31869" citStr="Mitchell and Lapata, 2008" startWordPosition="5154" endWordPosition="5158">heir method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="14680" citStr="Neelakantan et al., 2014" startWordPosition="2329" endWordPosition="2333">013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014). 1we did not get significant improvements when we tried more sophisticated ordering schemes for computing the path representations. Algorithm 1 Training Algorithm of RNN model for relation δ 1: Input: Aδ = A+δ U A−δ , Φδ, number of iterations T, mini-batch size B 2: Initialize vr, Wδ randomly 3: for t = 1,2,...,T do 4: Vvr = 0, VWδ = 0 and b = 0 5: for λ = (γ, δ) E Aδ do 6: µλ = arg maxπ∈Φδ(γ) vp(π).vr(δ) 7: Accumulate gradients to Vvr, VWδ 8: using path µλ. 9: b = b + 1 10: ifb=Bthen 11: Gradient Update for vr, Wδ 12: Vvr = 0, VWδ = 0 and b = 0 13: end if 14: end for 15: if b &gt; 0 then 16: Gr</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>A three-way model for collective learning on multi-relational data.</title>
<date>2011</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4906" citStr="Nickel et al. (2011)" startWordPosition="737" endWordPosition="740">lations are included, the problem is obviously far more severe. Better generalization can be gained by operating on embedded vector representations of relations, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunction</context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2011</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Norouzi</author>
<author>Tomas Mikolov</author>
<author>Samy Bengio</author>
<author>Yoram Singer</author>
<author>Jonathon Shlens</author>
<author>Andrea Frome</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Zero-shot learning by convex combination of semantic embeddings.</title>
<date>2014</date>
<booktitle>In International Conference on Learning Representations.</booktitle>
<contexts>
<context position="33144" citStr="Norouzi et al., 2014" startWordPosition="5362" endWordPosition="5365">ering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-scale dataset available at http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz, our method outperforms two baseline methods and performs competitively with a modified stronger baseline. The best results are obtained by combining the predictions of our model with the predictions of the modified baseline which achieves a 15% improvement over Gardner et al. (2013). We also show that our model has the ability to perform zero-shot infere</context>
</contexts>
<marker>Norouzi, Mikolov, Bengio, Singer, Shlens, Frome, Corrado, Dean, 2014</marker>
<rawString>Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg Corrado, and Jeffrey Dean. 2014. Zero-shot learning by convex combination of semantic embeddings. In International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Orr</author>
<author>Amarnag Subramanya</author>
<author>Evgeniy Gabrilovich</author>
<author>Michael Ringgaard</author>
</authors>
<title>11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts.</title>
<date>2013</date>
<note>http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html.</note>
<contexts>
<context position="8370" citStr="Orr et al., 2013" startWordPosition="1279" endWordPosition="1282">ng since there must be a classifer for each predicted relation type. Furthermore their pre-trained vectors do not have the opportunity to be tuned to the KB completion task because the two sub-tasks are completely disentangled. An additional contribution of our work is a new large-scale data set of over 52 million triples, and its preprocessing for purposes of path-based KB completion (can be downloaded from http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz). The dataset is build from the combination of Freebase (Bollacker et al., 2008) and Google’s entity linking in ClueWeb (Orr et al., 2013). Rather than Gardner’s 1000 distinct paths per relation type, we have over 2 million. Rather than Gardner’s 200 IsBasedIn StateLocatedIn CountryLocatedIn USA Microsoft Seattle Washington headquartered in in the U.S. state of state part of headquarters located in located in the state of state in the NW region of founded in beautiful city in located in country based in in state democratic state in 157 Figure 2: Vector Representations of the paths are computed by applying the composition function recursively. entity pairs, we use over 10k. All experimental comparisons below are performed on this</context>
<context position="20972" citStr="Orr et al., 2013" startWordPosition="3441" endWordPosition="3444">terations and use minibatches of size 20. The neural networks and the classifiers were optimized using AdaGrad (Duchi et al., 2011). 5.1 Data We ran experiments on Freebase (Bollacker et al., 2008) enriched with information from ClueWeb. 160 Entities 18M Freebase triples 40M ClueWeb triples 12M Relations 25,994 Relation types tested 46 Avg. paths/relation 2.3M Avg. training facts/relation 6638 Avg. positive test instances/relation 3492 Avg. negative test instances/relation 43,160 Table 1: Statistics of our dataset. We use the publicly available entity links to Freebase in the ClueWeb dataset (Orr et al., 2013). Hence, we create nodes only for Freebase entities in our KB graph. We remove facts containing /type/object/type as they do not give useful predictive information for our task. We get triples from ClueWeb by considering sentences that contain two entities linked to Freebase. We extract the phrase between the two entities and treat them as the relation types. For phrases that are of length greater than four we keep only the first and last two words. This helps us to avoid the time consuming step of dependency parsing the sentence to get the relation type. These triples are similar to facts obt</context>
</contexts>
<marker>Orr, Subramanya, Gabrilovich, Ringgaard, 2013</marker>
<rawString>Dave Orr, Amarnag Subramanya, Evgeniy Gabrilovich, and Michael Ringgaard. 2013. 11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts. http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey Hinton</author>
<author>Tom Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="17900" citStr="Palatucci et al., 2009" startWordPosition="2943" endWordPosition="2946">e network using backpropagation through structure (Goller and K¨uchler, 1996). 4 Zero-shot KB Completion The KB completion task involves predicting facts on thousands of relations types and it is highly desirable that a method can infer facts about relation types without directly training for them. Given the vector representation of the relations, we show that our model described in the previous section is capable of predicting relational facts without explicitly training for the target (or test) relation types (zero-shot learning). In zero-shot or zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-trained vectors, so that we can predict relations that are unseen during training. This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014). We lea</context>
<context position="32956" citStr="Palatucci et al. (2009)" startWordPosition="5330" endWordPosition="5333"> neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (Socher et al., 2013b; Frome et al., 2013; Norouzi et al., 2014). 7 Conclusion We develop a compositional vector space model for knowledge base completion using recurrent neural networks. In our challenging large-scale dataset available at http: //iesl.cs.umass.edu/downloads/ inferencerules/release.tar.gz, our method outperforms two baseline methods and performs competitively with a modified stronger baseline. The best results are obtained by combining the predictions of </context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom Mitchell. 2009. Zero-shot learning with semantic output codes. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="5064" citStr="Riedel et al., 2013" startWordPosition="763" endWordPosition="766">ns, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected in a path, and (b) generalization through vector embeddings, and (c) reasoning non-atomically and compositionally about the elements </context>
<context position="14058" citStr="Riedel et al., 2013" startWordPosition="2218" endWordPosition="2221">rrent neural network (Werbos, 1990). Finally, we make a prediction regarding CountryOfHeadquarters(Microsoft, USA) using the path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn by comparing the vector representation of the path (vp(H)) with the vector representation of the relation CountryOfHeadquarters (vr(CountryOfHeadquarters)) using the sigmoid function. 3.1 Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Nee</context>
<context position="18934" citStr="Riedel et al. (2013)" startWordPosition="3112" endWordPosition="3115">to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014). We learn a general composition matrix for all relations instead of learning a separate composition matrix for every relation to be predicted. So, 2we sub-sample a portion of the set of all unobserved instances. vp(7r) = f(W[vr(IsBasedIn); vr(StateLocatedIn)]) where W E Rd*2d+1 is the general composition matrix. We initialize the vector representations of the binary relations (vr) using the representations learned in Riedel et al. (2013) and do not update them during training. The relation vectors are not updated because at prediction time we would be predicting relation types which are never seen during training and hence their vectors would never get updated. We learn only the general composition matrix in this model. We train a single model for a set of relation types by replacing the sigmoid function with a softmax function while computing probabilities and the parameters of the composition matrix are learned using the available training data containing instances of few relations. The other aspects of the model remain unc</context>
<context position="22796" citStr="Riedel et al. (2013)" startWordPosition="3760" endWordPosition="3763">aset, we compare the performance of the following methods: PRA Classifier is the method in Lao et al. (2012) which trains a logistic regression classifier by creating a feature for every path type. Cluster PRA Classifier is the method in Gardner et al. (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al. (2014). RNN-random is the supervised RNN model described in section 3 with the relation vectors initialized randomly. RNN is the supervised RNN model described in section 3 with the relation vectors initialized using the method in Riedel et al. (2013). PRA Classifier-b is our simple extension to the method in Lao et al. (2012) which additionally uses big</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Jesse Davis</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2427" citStr="Schoenmackers et al. (2010)" startWordPosition="370" endWordPosition="374">residentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie). However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the application of an automated process. The addition of new triples by leveraging existing triples is typically known as KB completion. Early work on this problem focused on learning symbolic rules. For example, Schoenmackers et al. (2010) learns Horn clauses predictive of new binary relations by exhausitively exploring relational paths of increasing length, and selecting those surpassing an accuracy threshold. (A “path” is a sequence of triples in which the second entity of each triple matches the first entity of the next triple.) Lao et al. (2011) introduced the Path Ranking Algorithm (PRA), which greatly improves efficiency and robustness by replacing exhaustive search with random walks, and using unique paths as features in a per-target-relation binary classifier. A typical predictive feature learned by PRA is that CountryO</context>
<context position="18429" citStr="Schoenmackers et al., 2010" startWordPosition="3032" endWordPosition="3035">t learning). In zero-shot or zero-data learning (Larochelle et al., 2008; Palatucci et al., 2009), some labels or classes are not available during training the model and only a description of those classes are given at prediction time. We make two modifications to the model described in the previous section, (1) learn a general composition matrix, and (2) fix relation vectors with pre-trained vectors, so that we can predict relations that are unseen during training. This ability of the model to generalize to unseen relations is beyond the capabilities of all previous methods for KB inference (Schoenmackers et al., 2010; Lao et al., 2011; Gardner et al., 2013; Gardner et al., 2014). We learn a general composition matrix for all relations instead of learning a separate composition matrix for every relation to be predicted. So, 2we sub-sample a portion of the set of all unobserved instances. vp(7r) = f(W[vr(IsBasedIn); vr(StateLocatedIn)]) where W E Rd*2d+1 is the general composition matrix. We initialize the vector representations of the binary relations (vr) using the representations learned in Riedel et al. (2013) and do not update them during training. The relation vectors are not updated because at predic</context>
<context position="31102" citStr="Schoenmackers et al. (2010)" startWordPosition="5029" endWordPosition="5032">re some of the important local structure as well as the classifier using bigram features. To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014). We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the c</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, Davis, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and Jesse Davis. 2010. Learning first-order horn clauses from web text. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff Chiung-Yu Lin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="14139" citStr="Socher et al., 2011" startWordPosition="2233" endWordPosition="2236">tryOfHeadquarters(Microsoft, USA) using the path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn by comparing the vector representation of the path (vp(H)) with the vector representation of the relation CountryOfHeadquarters (vr(CountryOfHeadquarters)) using the sigmoid function. 3.1 Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014). 1we did not get significant improvements when we tried mo</context>
<context position="32414" citStr="Socher et al., 2011" startWordPosition="5244" endWordPosition="5247">ses and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while ther</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff Chiung-Yu Lin, Christopher D. Manning, and Andrew Y. Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="32461" citStr="Socher et al., 2012" startWordPosition="5250" endWordPosition="5253"> (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="4930" citStr="Socher et al. (2013" startWordPosition="742" endWordPosition="745"> problem is obviously far more severe. Better generalization can be gained by operating on embedded vector representations of relations, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected</context>
<context position="32482" citStr="Socher et al., 2013" startWordPosition="5254" endWordPosition="5257">, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (S</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013a. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="4930" citStr="Socher et al. (2013" startWordPosition="742" endWordPosition="745"> problem is obviously far more severe. Better generalization can be gained by operating on embedded vector representations of relations, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected</context>
<context position="32482" citStr="Socher et al., 2013" startWordPosition="5254" endWordPosition="5257">, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (S</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. 2013b. Zero-shot learning through cross-modal transfer. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="4930" citStr="Socher et al. (2013" startWordPosition="742" endWordPosition="745"> problem is obviously far more severe. Better generalization can be gained by operating on embedded vector representations of relations, in which vector similarity can be interpreted as semantic similarity. For example, Bordes et al. (2013) learn low-dimensional vector representations of entities and KB relations, such that vector differences between two entities should be close to the vectors associated with their relations. This approach can find relation synonyms, and thus perform a kind of one-to-one, non-path-based relation prediction for KB completion. Similarly Nickel et al. (2011) and Socher et al. (2013a) perform KB completion by learning embeddings of relations, but based on matrices or tensors. Universal schema (Riedel et al., 2013) learns to perform relation prediction cast as matrix completion (likewise using vector embeddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected</context>
<context position="32482" citStr="Socher et al., 2013" startWordPosition="5254" endWordPosition="5257">, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Larochelle et al. (2008) for character recognition and drug discovery. Palatucci et al. (2009) perform zero-shot learning for neural decoding while there has been plenty of work in this direction for image recognition (S</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013c. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web.</booktitle>
<contexts>
<context position="1617" citStr="Suchanek et al., 2007" startWordPosition="240" endWordPosition="243">t new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%. 1 Introduction Constructing large knowledge bases (KBs) supports downstream reasoning about resolved entities and their relations, rather than the noisy textual evidence surrounding their natural language mentions. For this reason KBs have been of increasing interest in both industry and academia (Bollacker et al., 2008; Suchanek et al., 2007; Carlson et al., 2010). Such KBs typically contain many millions of facts, most of them (entity1,relation,entity2) “triples” (also known as binary relations) such as (Barack Obama, presidentOf, USA) and (Brad Pitt, marriedTo, Angelina Jolie). However, even the largest KBs are woefully incomplete (Min et al., 2013), missing many important facts, and therefore damaging their usefulness in downstream tasks. Ironically, these missing facts can frequently be inferred from other facts already in the KB, thus representing a sort of inconsistency that can be repaired by the application of an automate</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="10783" citStr="Sutskever et al., 2014" startWordPosition="1686" endWordPosition="1689">the source and target nodes keeping the most common paths. We use PRA to find millions of distinct paths per relation type. We do not use the random walk probabilities given by PRA since using it did not yield improvements in our experiments. 2.2 Recurrent Neural Networks Recurrent neural network (RNN) (Werbos, 1990) is a neural network that constructs vector representation for sequences (of any length). For example, a RNN model can be used to construct vector representations for phrases or sentences (of any length) in natural language by applying a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014). The vector representation of a phrase (w1, w2) consisting of words w1 and w2 is given by f(W [v(w1); v(w2)]) where v(w) E Rd is the vector representation of w, f is an element-wise non linearity function, [a; b] represents the concatenation two vectors a and b along with a bias term, and W E Rd×2∗d+1 is the composition matrix. This operation can be repeated to construct vector representations of longer phrases. 3 Recurrent Neural Networks for KB Completion This paper proposes a RNN model for KB completion that reasons on the paths connecting an entity pair to predict m</context>
<context position="32230" citStr="Sutskever et al., 2014" startWordPosition="5212" endWordPosition="5215"> al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Lukasz Kaiser</author>
<author>Terry Koo</author>
<author>Slav Petrov</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Grammar as a foreign language. In CoRR.</title>
<date>2014</date>
<contexts>
<context position="10806" citStr="Vinyals et al., 2014" startWordPosition="1690" endWordPosition="1693">des keeping the most common paths. We use PRA to find millions of distinct paths per relation type. We do not use the random walk probabilities given by PRA since using it did not yield improvements in our experiments. 2.2 Recurrent Neural Networks Recurrent neural network (RNN) (Werbos, 1990) is a neural network that constructs vector representation for sequences (of any length). For example, a RNN model can be used to construct vector representations for phrases or sentences (of any length) in natural language by applying a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014). The vector representation of a phrase (w1, w2) consisting of words w1 and w2 is given by f(W [v(w1); v(w2)]) where v(w) E Rd is the vector representation of w, f is an element-wise non linearity function, [a; b] represents the concatenation two vectors a and b along with a bias term, and W E Rd×2∗d+1 is the composition matrix. This operation can be repeated to construct vector representations of longer phrases. 3 Recurrent Neural Networks for KB Completion This paper proposes a RNN model for KB completion that reasons on the paths connecting an entity pair to predict missing relation types. </context>
<context position="32265" citStr="Vinyals et al., 2014" startWordPosition="5218" endWordPosition="5221">ng simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering (Iyyer et al., 2014) and natural language logical semantics (Bowman et al., 2014). Our overall approach is 163 similar to RNNs with attention (Bahdanau et al., 2014; Graves, 2013) since we select a path among the set of paths connecting the entity pair to make the final prediction. Zero-shot or zero-data learning was introduced in Lar</context>
</contexts>
<marker>Vinyals, Kaiser, Koo, Petrov, Sutskever, Hinton, 2014</marker>
<rawString>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2014. Grammar as a foreign language. In CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Werbos</author>
</authors>
<title>Backpropagation through time: what it does and how to do it.</title>
<date>1990</date>
<booktitle>In IEEE.</booktitle>
<contexts>
<context position="5768" citStr="Werbos, 1990" startWordPosition="869" endWordPosition="870">eddings), but predicts textuallydefined OpenIE relations as well as KB relations, and embeds entity-pairs in addition to individual entities. Like all of the above, it also reasons about individual relations, not the evidence of a connected path of relations. This paper proposes an approach combining the advantages of (a) reasoning about conjunctions of relations connected in a path, and (b) generalization through vector embeddings, and (c) reasoning non-atomically and compositionally about the elements of the path, for further generalization. Our method uses recurrent neural networks (RNNs) (Werbos, 1990) to compose the semantics of relations in an arbitrary-length path. At each path-step it consumes both the vector embedding of the next relation, and the vector representing the path-so-far, then outputs a composed vector (representing the extended path-so-far), which will be the input to the next step. After consuming a path, the RNN should output a vector in the semantic neighborhood of the relation between the first and last entity of the path. For example, after consuming the relation vectors along the path Melinda Gates → Bill Gates → Microsoft → Seattle, our method produces a vector very</context>
<context position="10479" citStr="Werbos, 1990" startWordPosition="1636" endWordPosition="1637">austively obtain the set of all paths connecting an entity pair in the large KB graph, we use PRA (Lao et al., 2011) to obtain a set of paths connecting the entity pairs. Given a training set of entity pairs for a relation, PRA heuristically finds a set of paths by performing random walks from the source and target nodes keeping the most common paths. We use PRA to find millions of distinct paths per relation type. We do not use the random walk probabilities given by PRA since using it did not yield improvements in our experiments. 2.2 Recurrent Neural Networks Recurrent neural network (RNN) (Werbos, 1990) is a neural network that constructs vector representation for sequences (of any length). For example, a RNN model can be used to construct vector representations for phrases or sentences (of any length) in natural language by applying a composition function (Mikolov et al., 2010; Sutskever et al., 2014; Vinyals et al., 2014). The vector representation of a phrase (w1, w2) consisting of words w1 and w2 is given by f(W [v(w1); v(w2)]) where v(w) E Rd is the vector representation of w, f is an element-wise non linearity function, [a; b] represents the concatenation two vectors a and b along with</context>
<context position="12093" citStr="Werbos, 1990" startWordPosition="1907" endWordPosition="1908">e computed by applying the composition function recursively as shown in Figure 2. To compute the vector representations for the higher nodes in the tree, the composition function consumes the vector representation of the node’s two children nodes and outputs a new vector of the same dimension. Predictions about missing relation types are made by comparing the vector representation of the path with the vector representation of the relation using the sigmoid function. We represent each binary relation using a ddimensional real valued vector. We model composition using recurrent neural networks (Werbos, 1990). We learn a separate composition matrix for every relation that is predicted. Let vr(S) E Rd be the vector representation of relation S and vp(7r) E Rd be the vector representation of path 7r. vp(7r) denotes the relation vector if path 7r is of length one. To predict relation S = CountryOfHeadquarters, the vector representation of the path 7r = IsBasedIn → StateLocatedIn containing two relations IsBasedIn and StateLocatedIn is computed by (Figure 2), vp(7r) = f(Wδ[vr(IsBasedIn); vr(StateLocatedIn)]) CountryOfHeadquarters ~ Compositon Compositon IsBasedIn StateLocatedIn CountryLocatedIn Micros</context>
<context position="13474" citStr="Werbos, 1990" startWordPosition="2130" endWordPosition="2131"> b] represents the concatenation of two vectors a E Rd, b E Rd along with a bias feature to get a new vector [a; b] E R2d+1. The vector representation of the path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn in Figure 2 is computed similarly by, vp(H) = f(Wδ[vp(π); vr( CountryLocatedIn)]) where vp(π) is the vector representation of path IsBasedIn —* StateLocatedIn. While computing the vector representation of a path we always traverse left to right, composing the relation vector in the right with the accumulated path vector in the left1. This makes our model a recurrent neural network (Werbos, 1990). Finally, we make a prediction regarding CountryOfHeadquarters(Microsoft, USA) using the path H = IsBasedIn —* StateLocatedIn —* CountryLocatedIn by comparing the vector representation of the path (vp(H)) with the vector representation of the relation CountryOfHeadquarters (vr(CountryOfHeadquarters)) using the sigmoid function. 3.1 Model Training We train the model with the existing facts in a KB using them as positive examples and negative examples are obtained by treating the unobserved instances as negative examples (Mintz et al., 2009; Lao et al., 2011; Riedel et al., 2013; Bordes et al.,</context>
</contexts>
<marker>Werbos, 1990</marker>
<rawString>Paul Werbos. 1990. Backpropagation through time: what it does and how to do it. In IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Ron Weiss</author>
<author>Hector Yee</author>
</authors>
<title>Nonlinear latent factorization by embedding multiple user interests.</title>
<date>2013</date>
<booktitle>In ACM International Conference on Recommender Systems.</booktitle>
<contexts>
<context position="14653" citStr="Weston et al., 2013" startWordPosition="2325" endWordPosition="2328">011; Riedel et al., 2013; Bordes et al., 2013). Unlike in previous work that use RNNs(Socher et al., 2011; Iyyer et al., 2014; Irsoy and Cardie, 2014), a challenge with using them for our task is that among the set of paths connecting an entity pair, we do not observe which of the path(s) is predictive of a relation. We select the path that is closest to the relation type to be predicted in the vector space. This not only allows for faster training (compared to marginalization) but also gives improved performance. This technique has been successfully used in models other than RNNs previously (Weston et al., 2013; Neelakantan et al., 2014). 1we did not get significant improvements when we tried more sophisticated ordering schemes for computing the path representations. Algorithm 1 Training Algorithm of RNN model for relation δ 1: Input: Aδ = A+δ U A−δ , Φδ, number of iterations T, mini-batch size B 2: Initialize vr, Wδ randomly 3: for t = 1,2,...,T do 4: Vvr = 0, VWδ = 0 and b = 0 5: for λ = (γ, δ) E Aδ do 6: µλ = arg maxπ∈Φδ(γ) vp(π).vr(δ) 7: Accumulate gradients to Vvr, VWδ 8: using path µλ. 9: b = b + 1 10: ifb=Bthen 11: Gradient Update for vr, Wδ 12: Vvr = 0, VWδ = 0 and b = 0 13: end if 14: end f</context>
</contexts>
<marker>Weston, Weiss, Yee, 2013</marker>
<rawString>Jason Weston, Ron Weiss, and Hector Yee. 2013. Nonlinear latent factorization by embedding multiple user interests. In ACM International Conference on Recommender Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
</authors>
<title>Embedding entities and relations for learning and inference in knowledge bases.</title>
<date>2014</date>
<booktitle>In CoRR.</booktitle>
<contexts>
<context position="23046" citStr="Yang et al. (2014)" startWordPosition="3801" endWordPosition="3804"> (2013) which replaces relation types from ClueWeb triples with their cluster membership in the KB graph before the path finding step. After this step, their method proceeds in exactly the same manner as Lao et al. (2012) training a logistic regression classifier by creating a feature for every path type. We use pre-trained relation vectors from Riedel et al. (2013) and use k-means clustering to cluster the relation types to 25 clusters as done in Gardner et al. (2013). Composition-Add uses a simple element-wise addition followed by sigmoid non-linearity as the composition function similar to Yang et al. (2014). RNN-random is the supervised RNN model described in section 3 with the relation vectors initialized randomly. RNN is the supervised RNN model described in section 3 with the relation vectors initialized using the method in Riedel et al. (2013). PRA Classifier-b is our simple extension to the method in Lao et al. (2012) which additionally uses bigrams in the path as features. We add a special start and stop symbol to the path before computing the bigram features. Cluster PRA Classifier-b is our simple extension to the method in Gardner et al. (2013) which additionally uses bigram features com</context>
<context position="31618" citStr="Yang et al. (2014)" startWordPosition="5119" endWordPosition="5122"> (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever e</context>
</contexts>
<marker>Yang, Yih, He, Gao, Deng, 2014</marker>
<rawString>Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. Embedding entities and relations for learning and inference in knowledge bases. In CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised resolution of objects and relations on the web.</title>
<date>2007</date>
<journal>In North American Chapter of the Association for Computational Linguistics.</journal>
<contexts>
<context position="31007" citStr="Yates and Etzioni (2007)" startWordPosition="5013" endWordPosition="5016">less than the performance of RNN + PRA Classifier-b. We suspect the RNN model does not capture some of the important local structure as well as the classifier using bigram features. To overcome this drawback, in future work, we plan to explore compositional models that have a longer memory (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Mikolov et al., 2014). We also plan to include vector representations for the entities and develop models that address the issue of polysemy in verb phrases (Cheng et al., 2014). 6 Related Work KB Completion includes methods such as Lin and Pantel (2001), Yates and Etzioni (2007) and Berant et al. (2011) that learn inference rules of length one. Schoenmackers et al. (2010) learn general inference rules by considering the set of all paths in the KB and selecting paths that satisfy a certain precision threshold. Their method does not scale well to modern KBs and also depends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et</context>
</contexts>
<marker>Yates, Etzioni, 2007</marker>
<rawString>Alexander Yates and Oren Etzioni. 2007. Unsupervised resolution of objects and relations on the web. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="31929" citStr="Yessenalina and Cardie, 2011" startWordPosition="5163" endWordPosition="5167">epends on carefully tuned thresholds. Lao et al. (2011) train a simple logistic regression classifier with NELL KB paths as features to perform KB completion while Gardner et al. (2013) and Gardner et al. (2014) extend it by using pre-trained relation vectors to overcome feature sparsity. Recently, Yang et al. (2014) learn inference rules using simple element-wise addition or multiplication as the composition function. Compositional Vector Space Models have been developed to represent phrases and sentences in natural language as vectors (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Yessenalina and Cardie, 2011). Neural networks have been successfully used to learn vector representations of phrases using the vector representations of the words in that phrase. Recurrent neural networks have been used for many tasks such as language modeling (Mikolov et al., 2010), machine translation (Sutskever et al., 2014) and parsing (Vinyals et al., 2014). Recursive neural networks, a more general version of the recurrent neural networks have been used for many tasks like parsing (Socher et al., 2011), sentiment classification (Socher et al., 2012; Socher et al., 2013c; Irsoy and Cardie, 2014), question answering </context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>