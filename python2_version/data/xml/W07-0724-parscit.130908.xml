<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033067">
<title confidence="0.937375">
NRC’s PORTAGE system for WMT 2007
</title>
<author confidence="0.999717">
Nicola UefFng, Michel Simard, Samuel Larkin Howard Johnson
</author>
<affiliation confidence="0.965325666666667">
Interactive Language Technologies Group Interactive Information Group
National Research Council Canada National Research Council Canada
Gatineau, Quebec, Canada Ottawa, Ontario, Canada
</affiliation>
<email confidence="0.982153">
firstname.lastname@nrc.gc.ca Howard.Johnson@nrc.gc.ca
</email>
<sectionHeader confidence="0.97887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956454545455">
We present the PORTAGE statistical
machine translation system which par-
ticipated in the shared task of the ACL
2007 Second Workshop on Statistical
Machine Translation. The focus of this
description is on improvements which
were incorporated into the system over
the last year. These include adapted lan-
guage models, phrase table pruning, an
IBM1-based decoder feature, and rescor-
ing with posterior probabilities.
</bodyText>
<sectionHeader confidence="0.995512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998355095238095">
The statistical machine translation (SMT) sys-
tem PORTAGE was developed at the National
Research Council Canada and has recently been
made available to Canadian universities and
research institutions. It is a state-of-the-art
phrase-based SMT system. We will shortly de-
scribe its basics in this paper and then high-
light the new methods which we incorporated
since our participation in the WMT 2006 shared
task. These include new scoring methods for
phrase pairs, pruning of phrase tables based
on significance, a higher-order language model,
adapted language models, and several new de-
coder and rescoring models. PORTAGE was
also used in a joint system developed in coop-
eration with Systran. The interested reader is
referred to (Simard et al., 2007).
Throughout this paper, let si := sl ... sj de-
note a source sentence of length J, ti := ti ... ti
a target sentence of length I, and s� and t phrases
in source and target language, respectively.
</bodyText>
<sectionHeader confidence="0.969097" genericHeader="introduction">
2 Baseline
</sectionHeader>
<bodyText confidence="0.996020181818182">
As baseline for our experiments, we used a ver-
sion of PORTAGE corresponding to its state at
the time of the WMT 2006 shared task. We pro-
vide a basic description of this system here; for
more details see (Johnson et al., 2006).
PORTAGE implements a two-stage transla-
tion process: First, the decoder generates N-
best lists, using a basic set of models which are
then rescored with additional models in a sec-
ond step. In the baseline system, the decoder
uses the following models (or feature functions):
</bodyText>
<listItem confidence="0.813153545454545">
• one or several phrase table(s), which model
the translation direction p(s  |t). They are
generated from the training corpus via the
“diag-and&amp;quot; method (Koehn et al., 2003)
and smoothed using Kneser-Ney smooth-
ing (Foster et al., 2006),
• one or several n-gram language model(s)
trained with the SRILM toolkit (Stolcke,
2002); in the baseline experiments reported
here, we used a trigram model,
• a distortion model which assigns a penalty
based on the number of source words which
are skipped when generating a new target
phrase,
• a word penalty.
These different models are combined log-
linearly. Their weights are optimized
w.r.t. BLEU score using the algorithm de-
scribed in (Och, 2003). This is done on the
provided development corpus. The search
algorithm implemented in the decoder is a
dynamic-programming beam-search algorithm.
</listItem>
<page confidence="0.979198">
185
</page>
<bodyText confidence="0.9584804">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 185–188,
Prague, June 2007. c�2007 Association for Computational Linguistics
After the decoding step, rescoring with addi-
tional models is performed. The baseline system
generates a 1,000-best list of alternative trans-
lations for each source sentence. These lists
are rescored with the different models described
above, a character penalty, and three different
features based on IBM Models 1 and 2 (Brown
et al., 1993) calculated in both translation di-
rections. The weights of these additional models
and of the decoder models are again optimized
to maximize BLEU score.
Note that we did not use the decision-tree-
based distortion models described in (Johnson
et al., 2006) here because they did not improve
translation quality.
In the following subsections, we will describe
the new models added to the system for our
WMT 2007 submissions.
</bodyText>
<sectionHeader confidence="0.969079" genericHeader="method">
3 Improvements in PORTAGE
</sectionHeader>
<subsectionHeader confidence="0.991373">
3.1 Phrase translation models
</subsectionHeader>
<bodyText confidence="0.999831903225807">
Whereas the phrase tables used in the baseline
system contain only one score for each phrase
pair, namely conditional probabilities calculated
using Kneser-Ney smoothing, our current sys-
tem combines seven different phrase scores.
First, we used several types of phrase table
smoothing in the WMT 2007 system because
this proved helpful on other translation tasks:
relative frequency estimates, Kneser-Ney- and
Zens-Ney-smoothed probabilities (Foster et al.,
2006). Furthermore, we added normalized joint
probability estimates to the phrase translation
model. The other three scores will be explained
at the end of this subsection.
We pruned the generated phrase tables fol-
lowing the method introduced in (Johnson et
al., 2007). This approach considers all phrase
pairs (s, t) in the phrase table. The count C(9, �t)
of all sentence pairs containing (s, � is deter-
mined, as well as the count of all source/target
sentences containing s/�t. Using these counts,
Fisher’s exact test is carried out to calculate
the significance of the phrase pair. The phrase
tables are then pruned based on the p-value.
Phrase pairs with low significance, i.e. which are
only weakly supported by the training data, are
pruned. This reduces the size of the phrase ta-
bles to 8-16% on the different language pairs.
See (Johnson et al., 2007) for details.
Three additional phrase scores were derived
from information on which this pruning is based:
</bodyText>
<listItem confidence="0.992384166666667">
• the significance level (or p-value),
• the number C(9, � of sentence pairs con-
taining the phrase pair, normalized by the
number of source sentences containing s,
• C(9, �t), normalized by the number of target
sentences containing t.
</listItem>
<bodyText confidence="0.998156833333333">
For our submissions, we used the last three
phrase scores only when translating the Eu-
roParl data. Initial experiments showed that
they do not improve translation quality on the
News Commentary data. Apart from this, the
systems for both domains are identical.
</bodyText>
<subsectionHeader confidence="0.994582">
3.2 Adapted language models
</subsectionHeader>
<bodyText confidence="0.999987692307693">
Concerning the language models, we made two
changes to our system since WMT 2006. First,
we replaced the trigram language model by a 4-
gram model trained on the WMT 2007 data. We
also investigated the use of a 5-gram, but that
did not improve translation quality. Second,
we included adapted language models which
are specific to the development and test cor-
pora. For each development or test corpus, we
built this language model using information re-
trieval&apos; to find relevant sentences in the train-
ing data. To this end, we merged the train-
ing corpora for EuroParl and News Commen-
tary. The source sentences from the develop-
ment or test corpus served as individual queries
to find relevant training sentence pairs. For
each source sentence, we retrieved 10 sentence
pairs from the training data and used their tar-
get sides as language model training data. On
this small corpus, we trained a trigram lan-
guage model, again using the SRILM toolkit.
The feature function weights in the decoder and
the rescoring model were optimized using the
adapted language model for the development
corpus. When translating the test corpus, we
kept these weights, but replaced the adapted
</bodyText>
<footnote confidence="0.901973">
&apos;We used the lemur toolkit for querying, see
http://www.lemurproject.org/
</footnote>
<page confidence="0.99625">
186
</page>
<bodyText confidence="0.9986785">
language model by that specific to the test cor-
pus.
</bodyText>
<subsectionHeader confidence="0.995392">
3.3 New decoder and rescoring features
</subsectionHeader>
<bodyText confidence="0.999985666666667">
We integrated several new decoder and rescoring
features into PORTAGE. During decoding, the
system now makes use of a feature based on IBM
Model 1. This feature calculates the probability
of the (partial) translation over the source sen-
tence, using an IBM1 translation model in the
direction p(ti  |si ).
In the rescoring process, we additionally in-
cluded several types of posterior probabilities.
One is the posterior probability of the sentence
length over the N-best list for this source sen-
tence. The others are determined on the level
of words, phrases, and n-grams, and then com-
bined into a value for the whole sentence. All
posterior probabilities are calculated over the N-
best list, using the sentence probabilities which
the baseline system assigns to the translation
hypotheses. For details on the posterior prob-
abilities, see (Ueffing and Ney, 2007; Zens and
Ney, 2006). This year, we increased the length
of the N-best lists from 1,000 to 5,000.
</bodyText>
<subsectionHeader confidence="0.996978">
3.4 Post-processing
</subsectionHeader>
<bodyText confidence="0.999989444444444">
For truecasing the translation output, we used
the model described in (Agbago et al., 2005).
This model uses a combination of statisti-
cal components, including an n-gram language
model, a case mapping model, and a special-
ized language model for unknown words. The
language model is a 5-gram model trained on
the WMT 2007 data. The detokenizer which we
used is the one provided for WMT 2007.
</bodyText>
<sectionHeader confidence="0.997374" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999335026315789">
We submitted results for six of the translation
directions of the shared task: French H English,
German H English, and Spanish H English.
Table 1 shows the improvements result-
ing from incorporating new techniques into
PORTAGE on the Spanish -* English EuroParl
task. The baseline system is the one described
in section 2. Trained on the 2007 training cor-
pora, this yields a BLEU score of 30.48. Adding
the new phrase scores introduced in section 3.1
yields a slight improvement in translation qual-
ity. This improvement by itself is not signifi-
cant, but we observed it consistently across all
evaluation metrics and across the different devel-
opment and test corpora. Increasing the order
of the language model and adding an adapted
language model specific to the translation input
(see section 3.2) improves the BLEU score by
0.6 points. This is the biggest gain we observe
from introducing a new method. The incorpora-
tion of the IBM1-based decoder feature causes
a slight drop in translation quality. This sur-
prised us because we found this feature to be
very helpful on the NIST Chinese -* English
translation task. Adding the posterior proba-
bilities presented in section 3.3 in rescoring and
increasing the length of the N-best lists yielded
a small, but consistent gain in translation qual-
ity. The overall improvement compared to last
year’s system is around 1 BLEU point. The gain
achieved from introducing the new methods by
themselves are relatively small, but they add up.
Table 2 shows results on all six language pairs
we translated for the shared task. The trans-
lation quality achieved on the 2007 test set is
similar to that on the 2006 test set. The system
clearly performs better on the EuroParl domain
than on News Commentary.
</bodyText>
<tableCaption confidence="0.757189333333333">
Table 2: Translation quality in terms of
BLEU[%] and NIST score on all tasks. True-
cased and detokenized translation output.
</tableCaption>
<table confidence="0.999853785714286">
task test2006 test2007
BLEU NIST BLEU NIST
Eu D-*E 25.27 6.82 26.02 6.91
E-*D 19.36 5.86 18.94 5.71
S-*E 31.54 7.55 32.09 7.67
E-*S 30.94 7.39 30.92 7.41
F-*E 30.90 7.51 31.90 7.68
E-*F 30.08 7.26 30.06 7.26
NC D-*E 20.23 6.19 23.17 7.10
E-*D 13.84 5.38 16.30 5.95
S-*E 31.07 7.68 31.08 8.11
E-*S 30.79 7.73 32.56 8.25
F-*E 24.97 6.78 26.84 7.47
E-*F 24.91 6.79 26.60 7.24
</table>
<page confidence="0.935423">
187
</page>
<tableCaption confidence="0.910029333333333">
Table 1: Effect of integrating new models and methods into the PORTAGE system. Translation
quality in terms of BLEU and NIST score, WER and PER on the EuroParl Spanish–English 2006
test set. True-cased and detokenized translation output. Best results printed in boldface.
</tableCaption>
<table confidence="0.999654">
system BLEU[%] NIST WER[%] PER[%]
baseline 30.48 7.44 58.62 42.74
+ new phrase table features 30.66 7.48 58.25 42.46
+ 4-gram LM + adapted LM 31.26 7.53 57.93 42.26
+ IBM1-based decoder feature 31.18 7.51 58.13 42.53
+ refined rescoring 31.54 7.55 57.81 42.24
</table>
<sectionHeader confidence="0.996152" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999582">
We presented the PORTAGE system with which
we translated six language pairs in the WMT
2007 shared task. Starting from the state of
the system during the WMT 2006 evaluation,
we analyzed the contribution of new methods
which were incorporated over the last year in
detail. Our experiments showed that most of
these changes result in (small) improvements in
translation quality. In total, we gain about 1
BLEU point compared to last year’s system.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991315">
Our thanks go to the PORTAGE team at NRC
for their contributions and valuable feedback.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994735333333334">
A. Agbago, R. Kuhn, and G. Foster. 2005. True-
casing for the Portage system. In Recent Ad-
vances in Natural Language Processing, pages 21–
24, Borovets, Bulgaria, September.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311,
June.
G. Foster, R. Kuhn, and J. H. Johnson. 2006.
Phrasetable smoothing for statistical machine
translation. In Proc. of the Conf. on Empir-
ical Methods for Natural Language Processing
(EMNLP), pages 53–61, Sydney, Australia, July.
J. H. Johnson, F. Sadat, G. Foster, R. Kuhn,
M. Simard, E. Joanis, and S. Larkin. 2006.
Portage: with smoothed phrase tables and seg-
ment choice models. In Proc. HLT/NAACL
Workshop on Statistical Machine Translation
(WMT), pages 134–137, New York, NY, June.
H. Johnson, J. Martin, G. Foster, and R. Kuhn.
2007. Improving translation quality by discard-
ing most of the phrasetable. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing and Conf. on Computational Natural
Language Learning (EMNLP-CoNLL), to appear,
Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Human
Language Technology Conf. (HLT-NAACL), pages
127–133, Edmonton, Canada, May/June.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
M. Simard, J. Senellart, P. Isabelle, R. Kuhn,
J. Stephan, and N. Ueffing. 2007. Knowledge-
based translation with statistical phrase-based
post-editing. In Proc. ACL Second Workshop on
Statistical Machine Translation (WMT), to ap-
pear, Prague, Czech Republic, June.
A. Stolcke. 2002. SRILM – an extensible language
modeling toolkit. In Proc. Int. Conf. on Spoken
Language Processing (ICSLP), volume 2, pages
901–904, Denver, CO.
N. Ueffing and H. Ney. 2007. Word-level confi-
dence estimation for machine translation. Com-
putational Linguistics, 33(1):9–40, March.
R. Zens and H. Ney. 2006. N-gram posterior
probabilities for statistical machine translation.
In Proc. HLT/NAACL Workshop on Statistical
Machine Translation (WMT), pages 72–77, New
York, NY, June.
</reference>
<page confidence="0.997528">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.450500">
<title confidence="0.501948">NRC’s PORTAGE system for WMT 2007</title>
<author confidence="0.997499">Nicola UefFng</author>
<author confidence="0.997499">Michel Simard</author>
<author confidence="0.997499">Samuel Larkin Howard Johnson</author>
<affiliation confidence="0.9990545">Interactive Language Technologies Group Interactive Information Group National Research Council Canada National Research Council Canada</affiliation>
<address confidence="0.994045">Gatineau, Quebec, Canada Ottawa, Ontario, Canada</address>
<email confidence="0.927389">firstname.lastname@nrc.gc.caHoward.Johnson@nrc.gc.ca</email>
<abstract confidence="0.997628833333334">We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation. The focus of this description is on improvements which were incorporated into the system over the last year. These include adapted language models, phrase table pruning, an IBM1-based decoder feature, and rescoring with posterior probabilities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Agbago</author>
<author>R Kuhn</author>
<author>G Foster</author>
</authors>
<title>Truecasing for the Portage system.</title>
<date>2005</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<pages>21--24</pages>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="8392" citStr="Agbago et al., 2005" startWordPosition="1336" endWordPosition="1339">ngth over the N-best list for this source sentence. The others are determined on the level of words, phrases, and n-grams, and then combined into a value for the whole sentence. All posterior probabilities are calculated over the Nbest list, using the sentence probabilities which the baseline system assigns to the translation hypotheses. For details on the posterior probabilities, see (Ueffing and Ney, 2007; Zens and Ney, 2006). This year, we increased the length of the N-best lists from 1,000 to 5,000. 3.4 Post-processing For truecasing the translation output, we used the model described in (Agbago et al., 2005). This model uses a combination of statistical components, including an n-gram language model, a case mapping model, and a specialized language model for unknown words. The language model is a 5-gram model trained on the WMT 2007 data. The detokenizer which we used is the one provided for WMT 2007. 4 Experimental results We submitted results for six of the translation directions of the shared task: French H English, German H English, and Spanish H English. Table 1 shows the improvements resulting from incorporating new techniques into PORTAGE on the Spanish -* English EuroParl task. The baseli</context>
</contexts>
<marker>Agbago, Kuhn, Foster, 2005</marker>
<rawString>A. Agbago, R. Kuhn, and G. Foster. 2005. Truecasing for the Portage system. In Recent Advances in Natural Language Processing, pages 21– 24, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3556" citStr="Brown et al., 1993" startWordPosition="552" endWordPosition="555">ided development corpus. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. 185 Proceedings of the Second Workshop on Statistical Machine Translation, pages 185–188, Prague, June 2007. c�2007 Association for Computational Linguistics After the decoding step, rescoring with additional models is performed. The baseline system generates a 1,000-best list of alternative translations for each source sentence. These lists are rescored with the different models described above, a character penalty, and three different features based on IBM Models 1 and 2 (Brown et al., 1993) calculated in both translation directions. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. Note that we did not use the decision-treebased distortion models described in (Johnson et al., 2006) here because they did not improve translation quality. In the following subsections, we will describe the new models added to the system for our WMT 2007 submissions. 3 Improvements in PORTAGE 3.1 Phrase translation models Whereas the phrase tables used in the baseline system contain only one score for each phrase pair, namely conditional prob</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
<author>J H Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>53--61</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2458" citStr="Foster et al., 2006" startWordPosition="383" endWordPosition="386">006 shared task. We provide a basic description of this system here; for more details see (Johnson et al., 2006). PORTAGE implements a two-stage translation process: First, the decoder generates Nbest lists, using a basic set of models which are then rescored with additional models in a second step. In the baseline system, the decoder uses the following models (or feature functions): • one or several phrase table(s), which model the translation direction p(s |t). They are generated from the training corpus via the “diag-and&amp;quot; method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. </context>
<context position="4497" citStr="Foster et al., 2006" startWordPosition="694" endWordPosition="697">owing subsections, we will describe the new models added to the system for our WMT 2007 submissions. 3 Improvements in PORTAGE 3.1 Phrase translation models Whereas the phrase tables used in the baseline system contain only one score for each phrase pair, namely conditional probabilities calculated using Kneser-Ney smoothing, our current system combines seven different phrase scores. First, we used several types of phrase table smoothing in the WMT 2007 system because this proved helpful on other translation tasks: relative frequency estimates, Kneser-Ney- and Zens-Ney-smoothed probabilities (Foster et al., 2006). Furthermore, we added normalized joint probability estimates to the phrase translation model. The other three scores will be explained at the end of this subsection. We pruned the generated phrase tables following the method introduced in (Johnson et al., 2007). This approach considers all phrase pairs (s, t) in the phrase table. The count C(9, �t) of all sentence pairs containing (s, � is determined, as well as the count of all source/target sentences containing s/�t. Using these counts, Fisher’s exact test is carried out to calculate the significance of the phrase pair. The phrase tables a</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>G. Foster, R. Kuhn, and J. H. Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), pages 53–61, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Johnson</author>
<author>F Sadat</author>
<author>G Foster</author>
<author>R Kuhn</author>
<author>M Simard</author>
<author>E Joanis</author>
<author>S Larkin</author>
</authors>
<title>Portage: with smoothed phrase tables and segment choice models.</title>
<date>2006</date>
<booktitle>In Proc. HLT/NAACL Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>134--137</pages>
<location>New York, NY,</location>
<contexts>
<context position="1950" citStr="Johnson et al., 2006" startWordPosition="300" endWordPosition="303">dels, and several new decoder and rescoring models. PORTAGE was also used in a joint system developed in cooperation with Systran. The interested reader is referred to (Simard et al., 2007). Throughout this paper, let si := sl ... sj denote a source sentence of length J, ti := ti ... ti a target sentence of length I, and s� and t phrases in source and target language, respectively. 2 Baseline As baseline for our experiments, we used a version of PORTAGE corresponding to its state at the time of the WMT 2006 shared task. We provide a basic description of this system here; for more details see (Johnson et al., 2006). PORTAGE implements a two-stage translation process: First, the decoder generates Nbest lists, using a basic set of models which are then rescored with additional models in a second step. In the baseline system, the decoder uses the following models (or feature functions): • one or several phrase table(s), which model the translation direction p(s |t). They are generated from the training corpus via the “diag-and&amp;quot; method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); </context>
<context position="3810" citStr="Johnson et al., 2006" startWordPosition="593" endWordPosition="596"> Computational Linguistics After the decoding step, rescoring with additional models is performed. The baseline system generates a 1,000-best list of alternative translations for each source sentence. These lists are rescored with the different models described above, a character penalty, and three different features based on IBM Models 1 and 2 (Brown et al., 1993) calculated in both translation directions. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. Note that we did not use the decision-treebased distortion models described in (Johnson et al., 2006) here because they did not improve translation quality. In the following subsections, we will describe the new models added to the system for our WMT 2007 submissions. 3 Improvements in PORTAGE 3.1 Phrase translation models Whereas the phrase tables used in the baseline system contain only one score for each phrase pair, namely conditional probabilities calculated using Kneser-Ney smoothing, our current system combines seven different phrase scores. First, we used several types of phrase table smoothing in the WMT 2007 system because this proved helpful on other translation tasks: relative fre</context>
</contexts>
<marker>Johnson, Sadat, Foster, Kuhn, Simard, Joanis, Larkin, 2006</marker>
<rawString>J. H. Johnson, F. Sadat, G. Foster, R. Kuhn, M. Simard, E. Joanis, and S. Larkin. 2006. Portage: with smoothed phrase tables and segment choice models. In Proc. HLT/NAACL Workshop on Statistical Machine Translation (WMT), pages 134–137, New York, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Johnson</author>
<author>J Martin</author>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing and Conf. on Computational Natural Language Learning (EMNLP-CoNLL), to appear,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4760" citStr="Johnson et al., 2007" startWordPosition="735" endWordPosition="738"> conditional probabilities calculated using Kneser-Ney smoothing, our current system combines seven different phrase scores. First, we used several types of phrase table smoothing in the WMT 2007 system because this proved helpful on other translation tasks: relative frequency estimates, Kneser-Ney- and Zens-Ney-smoothed probabilities (Foster et al., 2006). Furthermore, we added normalized joint probability estimates to the phrase translation model. The other three scores will be explained at the end of this subsection. We pruned the generated phrase tables following the method introduced in (Johnson et al., 2007). This approach considers all phrase pairs (s, t) in the phrase table. The count C(9, �t) of all sentence pairs containing (s, � is determined, as well as the count of all source/target sentences containing s/�t. Using these counts, Fisher’s exact test is carried out to calculate the significance of the phrase pair. The phrase tables are then pruned based on the p-value. Phrase pairs with low significance, i.e. which are only weakly supported by the training data, are pruned. This reduces the size of the phrase tables to 8-16% on the different language pairs. See (Johnson et al., 2007) for det</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proc. of the Conf. on Empirical Methods for Natural Language Processing and Conf. on Computational Natural Language Learning (EMNLP-CoNLL), to appear, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Human Language Technology Conf. (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada, May/June.</location>
<contexts>
<context position="2396" citStr="Koehn et al., 2003" startWordPosition="373" endWordPosition="376">f PORTAGE corresponding to its state at the time of the WMT 2006 shared task. We provide a basic description of this system here; for more details see (Johnson et al., 2006). PORTAGE implements a two-stage translation process: First, the decoder generates Nbest lists, using a basic set of models which are then rescored with additional models in a second step. In the baseline system, the decoder uses the following models (or feature functions): • one or several phrase table(s), which model the translation direction p(s |t). They are generated from the training corpus via the “diag-and&amp;quot; method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented i</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of the Human Language Technology Conf. (HLT-NAACL), pages 127–133, Edmonton, Canada, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="2911" citStr="Och, 2003" startWordPosition="460" endWordPosition="461">|t). They are generated from the training corpus via the “diag-and&amp;quot; method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. 185 Proceedings of the Second Workshop on Statistical Machine Translation, pages 185–188, Prague, June 2007. c�2007 Association for Computational Linguistics After the decoding step, rescoring with additional models is performed. The baseline system generates a 1,000-best list of alternative translations for each source sentence. These lists are rescored with the different models described above, a character penalty, and three different features bas</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>J Senellart</author>
<author>P Isabelle</author>
<author>R Kuhn</author>
<author>J Stephan</author>
<author>N Ueffing</author>
</authors>
<title>Knowledgebased translation with statistical phrase-based post-editing.</title>
<date>2007</date>
<booktitle>In Proc. ACL Second Workshop on Statistical Machine Translation (WMT), to appear,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1518" citStr="Simard et al., 2007" startWordPosition="216" endWordPosition="219">een made available to Canadian universities and research institutions. It is a state-of-the-art phrase-based SMT system. We will shortly describe its basics in this paper and then highlight the new methods which we incorporated since our participation in the WMT 2006 shared task. These include new scoring methods for phrase pairs, pruning of phrase tables based on significance, a higher-order language model, adapted language models, and several new decoder and rescoring models. PORTAGE was also used in a joint system developed in cooperation with Systran. The interested reader is referred to (Simard et al., 2007). Throughout this paper, let si := sl ... sj denote a source sentence of length J, ti := ti ... ti a target sentence of length I, and s� and t phrases in source and target language, respectively. 2 Baseline As baseline for our experiments, we used a version of PORTAGE corresponding to its state at the time of the WMT 2006 shared task. We provide a basic description of this system here; for more details see (Johnson et al., 2006). PORTAGE implements a two-stage translation process: First, the decoder generates Nbest lists, using a basic set of models which are then rescored with additional mode</context>
</contexts>
<marker>Simard, Senellart, Isabelle, Kuhn, Stephan, Ueffing, 2007</marker>
<rawString>M. Simard, J. Senellart, P. Isabelle, R. Kuhn, J. Stephan, and N. Ueffing. 2007. Knowledgebased translation with statistical phrase-based post-editing. In Proc. ACL Second Workshop on Statistical Machine Translation (WMT), to appear, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Int. Conf. on Spoken Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="2548" citStr="Stolcke, 2002" startWordPosition="399" endWordPosition="400">on et al., 2006). PORTAGE implements a two-stage translation process: First, the decoder generates Nbest lists, using a basic set of models which are then rescored with additional models in a second step. In the baseline system, the decoder uses the following models (or feature functions): • one or several phrase table(s), which model the translation direction p(s |t). They are generated from the training corpus via the “diag-and&amp;quot; method (Koehn et al., 2003) and smoothed using Kneser-Ney smoothing (Foster et al., 2006), • one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the baseline experiments reported here, we used a trigram model, • a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, • a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on the provided development corpus. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. 185 Proceedings of the Second Workshop on Statistical Machine Translation, pages 185–188, </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. Int. Conf. on Spoken Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Word-level confidence estimation for machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="8182" citStr="Ueffing and Ney, 2007" startWordPosition="1301" endWordPosition="1304">ence, using an IBM1 translation model in the direction p(ti |si ). In the rescoring process, we additionally included several types of posterior probabilities. One is the posterior probability of the sentence length over the N-best list for this source sentence. The others are determined on the level of words, phrases, and n-grams, and then combined into a value for the whole sentence. All posterior probabilities are calculated over the Nbest list, using the sentence probabilities which the baseline system assigns to the translation hypotheses. For details on the posterior probabilities, see (Ueffing and Ney, 2007; Zens and Ney, 2006). This year, we increased the length of the N-best lists from 1,000 to 5,000. 3.4 Post-processing For truecasing the translation output, we used the model described in (Agbago et al., 2005). This model uses a combination of statistical components, including an n-gram language model, a case mapping model, and a specialized language model for unknown words. The language model is a 5-gram model trained on the WMT 2007 data. The detokenizer which we used is the one provided for WMT 2007. 4 Experimental results We submitted results for six of the translation directions of the s</context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>N. Ueffing and H. Ney. 2007. Word-level confidence estimation for machine translation. Computational Linguistics, 33(1):9–40, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>N-gram posterior probabilities for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. HLT/NAACL Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>72--77</pages>
<location>New York, NY,</location>
<contexts>
<context position="8203" citStr="Zens and Ney, 2006" startWordPosition="1305" endWordPosition="1308">nslation model in the direction p(ti |si ). In the rescoring process, we additionally included several types of posterior probabilities. One is the posterior probability of the sentence length over the N-best list for this source sentence. The others are determined on the level of words, phrases, and n-grams, and then combined into a value for the whole sentence. All posterior probabilities are calculated over the Nbest list, using the sentence probabilities which the baseline system assigns to the translation hypotheses. For details on the posterior probabilities, see (Ueffing and Ney, 2007; Zens and Ney, 2006). This year, we increased the length of the N-best lists from 1,000 to 5,000. 3.4 Post-processing For truecasing the translation output, we used the model described in (Agbago et al., 2005). This model uses a combination of statistical components, including an n-gram language model, a case mapping model, and a specialized language model for unknown words. The language model is a 5-gram model trained on the WMT 2007 data. The detokenizer which we used is the one provided for WMT 2007. 4 Experimental results We submitted results for six of the translation directions of the shared task: French H </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. N-gram posterior probabilities for statistical machine translation. In Proc. HLT/NAACL Workshop on Statistical Machine Translation (WMT), pages 72–77, New York, NY, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>