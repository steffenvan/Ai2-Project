<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001198">
<title confidence="0.992695">
Searching for alignments in SMT. A novel approach based on an Estimation
of Distribution Algorithm *
</title>
<author confidence="0.966909">
Luis Rodriguez, Ismael Garcia-Varea, Jos´e A. G´amez
</author>
<affiliation confidence="0.904541">
Departamento de Sistemas Inform´aticos
</affiliation>
<address confidence="0.525555">
Universidad de Castilla-La Mancha
</address>
<email confidence="0.987874">
luisr@dsi.uclm.es, ivarea@dsi.uclm.es, jgamez@dsi.uclm.es
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966136363637">
In statistical machine translation, an align-
ment defines a mapping between the
words in the source and in the target sen-
tence. Alignments are used, on the one
hand, to train the statistical models and, on
the other, during the decoding process to
link the words in the source sentence to the
words in the partial hypotheses generated.
In both cases, the quality of the alignments
is crucial for the success of the translation
process. In this paper, we propose an al-
gorithm based on an Estimation of Dis-
tribution Algorithm for computing align-
ments between two sentences in a paral-
lel corpus. This algorithm has been tested
on different tasks involving different pair
of languages. In the different experiments
presented here for the two word-alignment
shared tasks proposed in the HLT-NAACL
2003 and in the ACL 2005, the EDA-
based algorithm outperforms the best par-
ticipant systems.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.968825219512195">
Nowadays, statistical approach to machine trans-
lation constitutes one of the most promising ap-
proaches in this field. The rationale behind this ap-
proximation is to learn a statistical model from a par-
allel corpus. A parallel corpus can be defined as a set
*This work has been supported by the Spanish Projects
JCCM (PBI-05-022) and HERMES 05/06 (Vic. Inv. UCLM)
of sentence pairs, each pair containing a sentence in
a source language and a translation of this sentence
in a target language. Word alignments are neces-
sary to link the words in the source and in the tar-
get sentence. Statistical models for machine trans-
lation heavily depend on the concept of alignment,
specifically, the well known IBM word based mod-
els (Brown et al., 1993). As a result of this, differ-
ent task on aligments in statistical machine transla-
tion have been proposed in the last few years (HLT-
NAACL 2003 (Mihalcea and Pedersen, 2003) and
ACL 2005 (Joel Martin, 2005)).
In this paper, we propose a novel approach to deal
with alignments. Specifically, we address the prob-
lem of searching for the best word alignment be-
tween a source and a target sentence. As there is
no efficient exact method to compute the optimal
alignment (known as Viterbi alignment) in most of
the cases (specifically in the IBM models 3,4 and 5),
in this work we propose the use of a recently ap-
peared meta-heuristic family of algorithms, Estima-
tion of Distribution Algorithms (EDAs). Clearly, by
using a heuristic-based method we cannot guarantee
the achievement of the optimal alignment. Nonethe-
less, we expect that the global search carried out
by our algorithm will produce high quality results
in most cases, since previous experiments with this
technique (Larra˜naga and Lozano, 2001) in different
optimization task have demonstrated. In addition to
this, the results presented in section 5 support the
approximation presented here.
This paper is structured as follows. Firstly, Sta-
tistical word alignments are described in section 2.
Estimation of Distribution Algorithms (EDAs) are
</bodyText>
<page confidence="0.993783">
47
</page>
<note confidence="0.913647">
Proceedings of the Workshop on Statistical Machine Translation, pages 47–54,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.402165">
rewritten as follows:
</bodyText>
<equation confidence="0.997586555555556">
Pr(f, a|e) = J Pr(fj, aj|fj−1
j=1 1 , aj−1
1 ,eI1)
Pr(aj|fj−1
1 , aj−1
1 , eI1)
J
=
j=1
</equation>
<bodyText confidence="0.9998685">
introduced in section 3. An implementation of the
search for alignments using an EDA is described in
section 4. In section 5, we discuss the experimental
issues and show the different results obtained. Fi-
nally, some conclussions and future work are dis-
cussed in section 6.
</bodyText>
<sectionHeader confidence="0.980285" genericHeader="introduction">
2 Word Alignments In Statistical Machine
translation
</sectionHeader>
<bodyText confidence="0.999957571428571">
In statistical machine translation, a word alignment
between two sentences (a source sentence f and a
target sentence e) defines a mapping between the
words f1...fJ in the source sentence and the words
e1..eI in the target sentence. The search for the op-
timal alignment between the source sentence f and
the target sentence e can be stated as:
</bodyText>
<equation confidence="0.9969175">
a� = argmax Pr(a|f, e) = argmax Pr(f, a|e) (1)
a∈A a∈A
</equation>
<bodyText confidence="0.991948592592592">
being A the set of all the possible alignments be-
tween f and e.
The transformation made in Eq. (1) allows us to
address the alignment problem by using the statitisti-
cal approach to machine translation described as fol-
lows. This approach can be stated as: a source lan-
guage string f = fJ1 = f1 ... fJ is to be translated
into a target language string e = eI1 =
e1 ... eI.
Every target string is regarded as a possible transla-
tion for the source language string with maximum a-
posteriori probability Pr(e|f). According to Bayes’
decision rule, we have to choose the target string
that maximizes the product of both the target lan-
guage model Pr(e) and the string translation model
Pr(f|e). Alignment models to structure the trans-
lation model are introduced in (Brown et al., 1993).
These alignment models are similar to the concept
of Hidden Markov models (HMM) in speech recog-
nition. The alignment mapping is j —* i = aj from
source position j to target position i = aj. In sta-
tistical alignment models, Pr(f, a|e), the alignment
a is usually introduced as a hidden variable. Never-
theless, in the problem described in this article, the
source and the target sentences are given, and we are
focusing on the optimization of the aligment a.
The translation probability Pr(f, a|e) can be
</bodyText>
<equation confidence="0.995922">
&apos;Pr(fj|fj−1
1 , aj1, eI1) (2)
</equation>
<bodyText confidence="0.999857714285714">
The probability Pr(f, a|e) can be estimated by
using the word-based IBM statistical alignment
models (Brown et al., 1993). These models, how-
ever, constrain the set of possible alignments so that
each word in the source sentence can be aligned at
most to one word in the target sentence. Of course,
“real” alignments, in most of the cases, do not fol-
low this limitation. Hence, the alignments obtained
from the IBM models have to be extended in some
way to achieve more realistic alignments. This is
usually performed by computing the alignments in
both directions (i.e, first from f to e and then from
e to f) and then combining them in a suitable way
(this process is known as symmetrization).
</bodyText>
<sectionHeader confidence="0.909824" genericHeader="method">
3 Estimation of Distribution Algorithms
</sectionHeader>
<bodyText confidence="0.994111956521739">
Estimation of Distribution Algorithms (EDAs)
(Larra˜naga and Lozano, 2001) are metaheuristics
which has gained interest during the last five years
due to their high performance when solving com-
binatorial optimization problems. EDAs, as well
as genetics algorithms (Michalewicz, 1996), are
population-based evolutionary algorithms but, in-
stead of using genetic operators are based on the es-
timation/learning and posterior sampling of a prob-
ability distribution, which relates the variables or
genes forming and individual or chromosome. In
this way the dependence/independence relations be-
tween these variables can be explicitly modelled in
the EDAs framework. The operation mode of a
canonical EDA is shown in Figure 1.
As we can see, the algorithm maintains a popu-
lation of m individuals during the search. An in-
dividual is a candidate or potential solution to the
problem being optimized, e.g., in the problem con-
sidered here an individual would be a possible align-
ment. Usually, in combinatorial optimization prob-
lems an individual is represented as a vector of inte-
gers a = (a1,... , aJ), where each position aj can
</bodyText>
<page confidence="0.990417">
48
</page>
<listItem confidence="0.840876">
1. D0 +— Generate the initial population (m individuals)
2. Evaluate the population D0
3. k = 1
4. Repeat
</listItem>
<figure confidence="0.661196">
(a) Dtra +— Select s &lt; m individuals from Dk_1
(b) Estimate/learn a new model M from Dtra
(c) Da.,,,y +— Sample m individuals from M
(d) Evaluate Da.,,,y
(e) Dk +— Select m individuals from Dk_1 U Da.,,,y
(f) k = k + 1
Until stop condition
</figure>
<figureCaption confidence="0.99851">
Figure 1: A canonical EDA
</figureCaption>
<bodyText confidence="0.998956640625">
take a set of finite values Qa, = 10, ... , I}. The first
step in an evolutionary algorithm is to generate the
initial population D0. Although D0 is usually gener-
ated randomly (to ensure diversity), prior knowledge
can be of utility in this step.
Once we have a population our next step is to
evaluate it, that is, we have to measure the goodness
or fitness of each individual with respect to the prob-
lem we are solving. Thus, we use a fitness function
f(a) = Pr(f, a|e) (see Eq. (3)) to score individu-
als. Evolutionary algorithms in general and EDAs in
particular seek to improve the quality of the individ-
uals in the population during the search. In genetic
algorithms the main idea is to build a new popula-
tion from the current one by copying some individu-
als and constructing new ones from those contained
in the current population. Of course, as we aim to
improve the quality of the population with respect to
fitness, the best/fittest individuals have more chance
to be copied or selected for recombination.
In EDAs, the transition between populations is
quite different. The basic idea is to summarize
the properties of the individuals in the population
by learning a probability distribution that describes
them as much as possible. Since the quality of the
population should be improved in each step, only
the s fittest individuals are selected to be included in
the dataset used to learn the probability distribution
Pr(a1, ... , aJ), in this way we try to discover the
common regularities among good individuals. The
next step is to obtain a set of new individuals by
sampling the learnt distribution. These individuals
are scored by using the fitness function and added to
the ones forming the current population. Finally, the
new population is formed by selecting n individuals
from the 2n contained in the current one. A common
practice is to use some kind of fitness-based elitism
during this selection, in order to guarantee that the
best(s) individual(s) is/are retained.
The main problem in the previous description is
related to the estimation/learning of the probability
distribution, since estimating the joint distribution is
intractable in most cases. In the practice, what is
learnt is a probabilistic model that consists in a fac-
torization of the joint distribution. Different levels
of complexity can be considered in that factoriza-
tion, from univariate distributions to n-variate ones
or Bayesian networks (see (Larra˜naga and Lozano,
2001, Chapter 3) for a review). In this paper, as
this is the first approximation to the alignment prob-
lem with EDAs and, because of some questions that
will be discussed later, we use the simplest EDA
model: the Univariate Marginal Distribution Algo-
rithm or UMDA (Muhlenbein, 1997). In UMDA
it is assumed that all the variables are marginally
independent, thus, the n-dimensional probability
distribution, Pr(a1, ... , aJ), is factorized as the
product of J marginal/unidimensional distributions:
HJj=1 Pr(aj). Among the advantages of UMDA
we can cite the following: no structural learning is
needed; parameter learning is fast; small dataset can
be used because only marginal probabilities have to
be estimated; and, the sampling process is easy be-
cause each variable is independently sampled.
</bodyText>
<sectionHeader confidence="0.995425" genericHeader="method">
4 Design of an EDA to search for
alignments
</sectionHeader>
<bodyText confidence="0.999465">
In this section, an EDA algorithm to align a source
and a target sentences is described.
</bodyText>
<subsectionHeader confidence="0.966497">
4.1 Representation
</subsectionHeader>
<bodyText confidence="0.9998319">
One of the most important issues in the definition
of a search algorithm is to properly represent the
space of solutions to the problem. In the problem
considered here, we are searching for an “optimal”
alignment between a source sentence f and a target
sentence e. Therefore, the space of solutions can be
stated as the set of possible alignments between both
sentences. Owing to the constraints imposed by the
IBM models (a word in f can be aligned at most to
one word in e), the most natural way to represent a
</bodyText>
<page confidence="0.997359">
49
</page>
<bodyText confidence="0.999831">
solution to this problem consists in storing each pos-
sible alignment in a vector a = a1...aJ, being J the
length of f. Each position of this vector can take the
value of “0” to represent a NULL alignment (that is,
a word in the source sentence that is aligned to no
words in the target sentence) or an index represent-
ing any position in the target sentence. An example
of alignment is shown in Figure 4.1.
</bodyText>
<figureCaption confidence="0.756808">
Figure 2: Example of alignment and its representa-
tion as a vector
</figureCaption>
<subsectionHeader confidence="0.969007">
4.2 Evaluation function
</subsectionHeader>
<bodyText confidence="0.999944714285714">
During the search process, each individual (search
hypothesis) is scored using the fitness function de-
scribed as follows. Let a = a1 · · · aJ be the align-
ment represented by an individual. This alignment a
is evaluated by computing the probability p(f, a|e).
This probability is computed by using the IBM
model 4 as:
</bodyText>
<equation confidence="0.9975718">
p(f, a|e) = � p(T, 7r|e)
(τ,π)E(f,a)
I n(0i|ei) x I �φi t(Tik|ei) x
i=1 i=1 k=1
I
d=1(7ri1 − cρi|£c(eρi),Jc(Ti1)) x
i=1,φi&gt;0
I �φi d&gt;1(7rik − 7ri(k−1)|Jc(Tik)) x
i=1 k=2
k=1
</equation>
<bodyText confidence="0.987243">
where the factors separated by x symbols denote
fertility, translation, head permutation, non-head
permutation, null-fertility, and null-translation prob-
abilities1.
This model was trained using the GIZA++ toolkit
(Och and Ney, 2003) on the material available for the
different alignment tasks described in section 5.1
</bodyText>
<subsectionHeader confidence="0.997326">
4.3 Search
</subsectionHeader>
<bodyText confidence="0.995207790697675">
In this section, some specific details about the search
are given. As was mentioned in section 3, the algo-
rithm starts by generating an initial set of hypothe-
ses (initial population). In this case, a set of ran-
domly generated alignments between the source and
the target sentences are generated. Afterwards, all
the individuals in this population (a fragment of a
real population is shown in figure 3) are scored using
the function defined in Eq.(4.2). At this point, the
actual search starts by applying the scheme shown
in section 3, thereby leading to a gradual improve-
ment in the hypotheses handled by the algorithm in
each step of the search.
This process finishes when some finalization cri-
terium (or criteria) is reached. In our implementa-
tion, the algorithm finishes when it passes a certain
number of generations without improving the qual-
ity of the hypotheses (individuals). Afterwards, the
best individual in the current population is returned
as the final solution.
Regarding the EDA model, as commented before,
our approach rely on the UMDA model due mainly
to the size of the search space defined by the task.
The algorithm has to deal with individuals of length
J, where each position can take (I + 1) possible
values. Thus, in the case of UMDA, the number of
free parameters to be learnt for each position is I
(e.g., in the English-French task avg(J) = 15 and
avg(I) = 17.3). If more complex models were con-
sidered, the size of the probability tables would have
grown exponentially. As an example, in a bivariate
model, each variable (position) is conditioned on an-
other variable and thus the probability tables P(.|.)
to be learnt have I(I + 1) free parameters. In or-
der to properly estimate the probabilty distributions,
the size of the populations has to be increased con-
siderably. As a result, the computational resources
1 The symbols in this formula are: J (the length of e), I (the
length of f), ei (the i-th word in el), e0 (the NULL word), Oi
(the fertility of ei), rik (the k-th word produced by ei in a), Erik
(the position of rik in f), pi (the position of the first fertile word
to the left of ei in a), cp, (the ceiling of the average of all 7rp,k
for pi, or 0 if pi is undefined).
</bodyText>
<figure confidence="0.9910294">
f : Por favor ,
desearia reservar una habitacion .
null
e :
Please , I would like to book a room
( 0 1 2 4 6 7 8 9 )
.
φ0
J − 00 po−2φ0p10x rj t(T0k |e0) (3)
00
</figure>
<page confidence="0.935798">
50
</page>
<table confidence="0.956370571428572">
1 1 5 3 2 0 6 0 (-60.7500)
1 6 5 2 3 0 0 5 (-89.7449)
1 2 2 6 4 0 5 0 (-90.2221)
1 2 3 5 0 3 6 2 (-99.2313)
0 6 0 2 4 6 3 5 (-99.7786)
2 0 0 2 2 0 3 4 (-100.587)
1 0 1 6 3 6 0 5 (-101.335)
</table>
<figureCaption confidence="0.985145">
Figure 3: Part of one population generated during
</figureCaption>
<bodyText confidence="0.97670524137931">
the search for the alignments between the English
sentence and then he tells us the correct result !
and the Romanian sentence si ne spune noua rezul-
tatul corect !. These sentences are part of the HLT-
NAACL 2005 shared task. Some individuals and
their scores (fitness) are shown.
required by the algorithm rise dramatically.
Finally, as was described in section 3, some pa-
rameters have to be fixed in the design of an EDA.
On the one hand, the size of each population must
be defined. In this case, this size is proportional to
the length of the sentences to be aligned. Specifi-
cally, the size of the population adopted is equal to
the length of source sentence f multiplied by a factor
of ten.
On the other hand, as we mentioned in section 3
the probability distribution over the individuals is
not estimated from the whole population. In the
present task about 20% of the best individuals in
each population are used for this purpose.
As mentioned above, the fitness function used in
the algorithm just allows for unidirectional align-
ments. Therefore, the search was conducted in
both directions (i.e, from f to a and from a to
f) combining the final results to achieve bidirec-
tional alignments. To this end, diffferent approaches
(symmetrization methods) were tested. The results
shown in section 5.2 were obtained by applying the
refined method proposed in (Och and Ney, 2000).
</bodyText>
<sectionHeader confidence="0.992649" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9992915">
Different experiments have been carried out in or-
der to assess the correctness of the search algorithm.
Next, the experimental metodology employed and
the results obtained are described.
</bodyText>
<subsectionHeader confidence="0.990945">
5.1 Corpora and evaluation
</subsectionHeader>
<bodyText confidence="0.999985351351351">
Three different corpora and four different test sets
have been used. All of them are taken from the
two shared tasks in word alignments developed in
HLT/NAACL 2003 (Mihalcea and Pedersen, 2003)
and ACL 2005 (Joel Martin, 2005). These two tasks
involved four different pair of languages, English-
French, Romanian-English, English-Inuktitut and
English-Hindi. English-French and Romanian-
English pairs have been considered in these exper-
iments (owing to the lack of timeto properly pre-
process the Hindi and the Inuktitut). Next, a brief
description of the corpora used is given.
Regarding the Romanian-English task, the test
data used to evaluate the alignments consisted in
248 sentences for the 2003 evaluation task and 200
for the 2005 evaluation task. In addition to this, a
training corpus, consisting of about 1 million Ro-
manian words and about the same number of En-
glish word has been used. The IBM word-based
alignment models were training on the whole cor-
pus (training + test). On the other hand, a subset
of the Canadian Hansards corpus has been used in
the English-French task. The test corpus consists of
447 English-French sentences. The training corpus
contains about 20 million English words, and about
the same number of French words. In Table 1, the
features of the different corpora used are shown.
To evaluate the quality of the final alignments ob-
tained, different measures have been taken into ac-
count: Precision, Recall, F-measure, and Alignment
Error Rate. Given an alignment A and a reference
alignment G (both A and G can be split into two
subsets AS,AP and GS, GP, respectively represent-
ing Sure and Probable alignments) Precision (PT),
Recall (RT), F-measure (FT) and Alignment Error
Rate (AER) are computed as (where T is the align-
ment type, and can be set to either S or P):
</bodyText>
<equation confidence="0.9983401">
PT = |AT n GT |
|AT |
RT = |AT n GT |
|GT|
|2PTRT |
|PT+RT|
AER =
1 −  |As n GS |+ |AP n GP|
|AP |+ |GS|
FT =
</equation>
<page confidence="0.998903">
51
</page>
<tableCaption confidence="0.996545">
Table 1: Features of the corpora used in the different alignment task
</tableCaption>
<table confidence="0.9994394">
En-Fr Ro-En 03 Ro-En 05
Training size 1M 97K 97K
Vocabulary 68K / 86K 48K / 27K 48K / 27K
Running words 20M / 23M 1.9M / 2M 1.9M / 2M
Test size 447 248 200
</table>
<bodyText confidence="0.9997816">
It is important to emphasize that EDAs are non-
deterministics algorithms. Because of this, the re-
sults presented in section 5.2 are actually the mean
of the results obtained in ten different executions of
the search algorithm.
</bodyText>
<sectionHeader confidence="0.589974" genericHeader="evaluation">
5.2 Results
</sectionHeader>
<bodyText confidence="0.999963388888889">
In Tables 2, 3 and 4 the results obtained from the
different tasks are presented. The results achieved
by the technique proposed in this paper are com-
pared with the best results presented in the shared
tasks described in (Mihalcea and Pedersen, 2003)
(Joel Martin, 2005). The results obtained by the
GIZA++ hill-climbing algorithm are also presented.
In these tables, the mean and the variance of the re-
sults obtained in ten executions of the search algo-
rithm are shown. According to the small variances
observed in the results we can conclude that the non-
deterministic nature of this approach it is not statis-
tically significant.
According to these results, the proposed EDA-
based search is very competitive with respect to the
best result presented in the two shared task.
In addition to these results, additional experi-
ments were carried out in to evaluate the actual be-
havior of the search algorithm. These experiments
were focused on measuring the quality of the algo-
rithm, distinguishing between the errors produced
by the search process itself and the errors produced
by the model that leads the search (i.e, the errors in-
troduced by the fitness function). To this end, the
next approach was adopted. Firstly, the (bidirec-
tional) reference alignments used in the computation
of the Alignment Error Rate were split into two sets
of unidirectional alignments. Owing to the fact that
there is no exact method to perform this decomposi-
tion, we employed the method described in the fol-
lowing way. For each reference alignment, all the
possible decompositions into unidirectional align-
ments were perfomed, scoring each of them with
the evaluation function F(a) = p(f, ale) defined in
section (3), and being selected the best one, aref.
Afterwards, this alignment was compared with the
solution provided by the EDA, aeda . This com-
parison was made for each sentence in the test set,
being measuried the AER for both alignments as
well as the value of the fitness function. At this
point, we can say that a model-error is produced if
F(aeda) &gt; F(aref). In addition, we can say that a
search-error is produced if F(aeda) &lt; F(aref). In
table 5, a summary for both kinds of errors for the
English-Romanian 2005 task is shown. In this table
we can also see that these results correlate with the
AER figures.
These experiments show that most of the errors
were not due to the search process itself but to an-
other different factors. From this, we can conclude
that, on the one hand, the model used to lead the
search should be improved and, on the other, dif-
ferent techniques for symmetrization should be ex-
plored.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999935733333333">
In this paper, a new approach, based on the use of an
Estimation of Distribution Algorithm has been pre-
sented. The results obtained with this technique are
very promising even with the simple scheme here
considered.
According to the results presented in the previ-
ous section, the non-deterministic nature of the algo-
rithm has not a real influence in the performance of
this approach. Therefore, the main theoretical draw-
back of evolutionary algorithms have been proven
not to be an important issue for the task we have ad-
dressed here.
Finally, we are now focusing on the influence of
these improved alignments in the statistical models
for machine translation and on the degree of accu-
</bodyText>
<page confidence="0.999375">
52
</page>
<tableCaption confidence="0.99815">
Table 2: Alignment quality (%) for the English-French task with NULL alignments
</tableCaption>
<table confidence="0.9995962">
System P3 R3 F3 Pp Rp Fp AER
EDA 73.82 82.76 78.04 83.91 29.50 43.36 13.61 ±0.03
GIZA++ 73.61 82.56 77.92 79.94 32.96 46.67 15.89
Ralign.EF1 72.54 80.61 76.36 77.56 36.79 49.91 18.50
XRCE.Nolem.EF.3 55.43 93.81 69.68 72.01 36.00 48.00 21.27
</table>
<tableCaption confidence="0.994668">
Table 3: Alignment quality (%) for the Romanian-English 2003 task with NULL aligments
</tableCaption>
<table confidence="0.9957042">
System P3 R3 F3 Pp Rp Fp AER
EDA 94.22 49.67 65.05 76.66 60.97 67.92 32.08 ±0.05
GIZA++ 95.20 48.54 64.30 79.89 57.82 67.09 32.91
XRCE.Trilex.RE.3 80.97 53.64 64.53 63.64 61.58 62.59 37.41
XRCE.Nolem-56k.RE.2 82.65 54.12 65.41 61.59 61.50 61.54 38.46
</table>
<tableCaption confidence="0.992449">
Table 4: Alignment quality (%) for the Romanian-English 2005 task
</tableCaption>
<table confidence="0.922635333333333">
System P3 R3 F3 Pp Rp Fp AER
EDA 95.37 54.90 69.68 80.61 67.83 73.67 26.33 ±0.044
GIZA++ 95.68 53.29 68.45 81.46 65.83 72.81 27.19
ISI.Run5.vocab.grow 87.90 63.08 73.45 87.90 63.08 73.45 26.55
ISI.Run4.simple.intersect 94.29 57.42 71.38 94.29 57.42 71,38 28.62
ISI.Run2.simple.union 70.46 71.31 70.88 70.46 71.31 70.88 29.12
</table>
<tableCaption confidence="0.82872375">
Table 5: Comparison between reference aligments (decomposed into two unidirectional alignments) and
the alignments provided by the EDA. Search errors and model errors for EDA and GIZA++ algorithms are
presented. In addition, the AER for the unidirectional EDA and reference alignments is also shown. These
result are obtained on the Romanian-English 05 task
</tableCaption>
<table confidence="0.988217285714286">
Romanian-English English-Romanian
EDA search errors (%) 35 (17.5 %) 18 (9 %)
EDA model errors (%) 165 (82.5 %) 182 (91 %)
GIZA++ search errors (%) 87 (43 %) 81 (40 %)
GIZA++ model errors (%) 113 (57 %) 119 (60 %)
AER-EDA 29.67 % 30.66 %
AER-reference 12.77 % 11.03 %
</table>
<page confidence="0.998596">
53
</page>
<bodyText confidence="0.9999286">
racy that could be achieved by means of these alig-
ments. In addition to this, the integration of the
aligment algorithm into the training process of the
statistical translation models is currently being per-
formed.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99993328125">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statisti-
cal machine translation: Parameter estimation. Comp.
Linguistics, 19(2):263–311.
Ted Pedersen Joel Martin, Rada Mihalcea. 2005. Word
alignment for languages with scarce resources. In
Rada Mihalcea and Ted Pedersen, editors, Proceed-
ings of the ACL Workshop on Building and Exploiting
Parallel Texts: Data Driven Machine Translation and
Beyond, pages 1–10, Michigan, USA, June 31. Asso-
ciation for Computational Linguistics.
P. Larra˜naga and J.A. Lozano. 2001. Estimation of
Distribution Algorithms. A New Tool for Evolutionary
Computation. Kluwer Academic Publishers.
Z. Michalewicz. 1996. Genetic Algorithms + Data
Structures = Evolution Programs. Springer-Verlag.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 1–10, Edmonton,
Alberta, Canada, May 31. Association for Computa-
tional Linguistics.
Heinz Muhlenbein. 1997. The equation for response
to selection and its use for prediction. Evolutionary
Computation, 5(3):303–346.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In ACL00, pages 440–447,
Hongkong, China, October.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
</reference>
<page confidence="0.999026">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888963">
<title confidence="0.9977395">Searching for alignments in SMT. A novel approach based on an Estimation Distribution Algorithm</title>
<author confidence="0.987838">Luis Rodriguez</author>
<author confidence="0.987838">Ismael Garcia-Varea</author>
<author confidence="0.987838">A Jos´e</author>
<affiliation confidence="0.991175">Departamento de Sistemas Universidad de Castilla-La</affiliation>
<email confidence="0.932448">luisr@dsi.uclm.es,ivarea@dsi.uclm.es,jgamez@dsi.uclm.es</email>
<abstract confidence="0.999216347826087">In statistical machine translation, an alignment defines a mapping between the words in the source and in the target sentence. Alignments are used, on the one hand, to train the statistical models and, on the other, during the decoding process to link the words in the source sentence to the words in the partial hypotheses generated. In both cases, the quality of the alignments is crucial for the success of the translation process. In this paper, we propose an algorithm based on an Estimation of Distribution Algorithm for computing alignments between two sentences in a parallel corpus. This algorithm has been tested on different tasks involving different pair of languages. In the different experiments presented here for the two word-alignment shared tasks proposed in the HLT-NAACL 2003 and in the ACL 2005, the EDAbased algorithm outperforms the best participant systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Comp. Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1930" citStr="Brown et al., 1993" startWordPosition="308" endWordPosition="311">e rationale behind this approximation is to learn a statistical model from a parallel corpus. A parallel corpus can be defined as a set *This work has been supported by the Spanish Projects JCCM (PBI-05-022) and HERMES 05/06 (Vic. Inv. UCLM) of sentence pairs, each pair containing a sentence in a source language and a translation of this sentence in a target language. Word alignments are necessary to link the words in the source and in the target sentence. Statistical models for machine translation heavily depend on the concept of alignment, specifically, the well known IBM word based models (Brown et al., 1993). As a result of this, different task on aligments in statistical machine translation have been proposed in the last few years (HLTNAACL 2003 (Mihalcea and Pedersen, 2003) and ACL 2005 (Joel Martin, 2005)). In this paper, we propose a novel approach to deal with alignments. Specifically, we address the problem of searching for the best word alignment between a source and a target sentence. As there is no efficient exact method to compute the optimal alignment (known as Viterbi alignment) in most of the cases (specifically in the IBM models 3,4 and 5), in this work we propose the use of a recen</context>
<context position="4997" citStr="Brown et al., 1993" startWordPosition="827" endWordPosition="830">atitistical approach to machine translation described as follows. This approach can be stated as: a source language string f = fJ1 = f1 ... fJ is to be translated into a target language string e = eI1 = e1 ... eI. Every target string is regarded as a possible translation for the source language string with maximum aposteriori probability Pr(e|f). According to Bayes’ decision rule, we have to choose the target string that maximizes the product of both the target language model Pr(e) and the string translation model Pr(f|e). Alignment models to structure the translation model are introduced in (Brown et al., 1993). These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j —* i = aj from source position j to target position i = aj. In statistical alignment models, Pr(f, a|e), the alignment a is usually introduced as a hidden variable. Nevertheless, in the problem described in this article, the source and the target sentences are given, and we are focusing on the optimization of the aligment a. The translation probability Pr(f, a|e) can be &apos;Pr(fj|fj−1 1 , aj1, eI1) (2) The probability Pr(f, a|e) can be estimated by using the word-bas</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comp. Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen Joel Martin</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word alignment for languages with scarce resources.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL Workshop on Building and Exploiting Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>1--10</pages>
<editor>In Rada Mihalcea and Ted Pedersen, editors,</editor>
<location>Michigan, USA,</location>
<marker>Martin, Mihalcea, 2005</marker>
<rawString>Ted Pedersen Joel Martin, Rada Mihalcea. 2005. Word alignment for languages with scarce resources. In Rada Mihalcea and Ted Pedersen, editors, Proceedings of the ACL Workshop on Building and Exploiting Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–10, Michigan, USA, June 31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Larra˜naga</author>
<author>J A Lozano</author>
</authors>
<title>Estimation of Distribution Algorithms. A New Tool for Evolutionary Computation.</title>
<date>2001</date>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Larra˜naga, Lozano, 2001</marker>
<rawString>P. Larra˜naga and J.A. Lozano. 2001. Estimation of Distribution Algorithms. A New Tool for Evolutionary Computation. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Michalewicz</author>
</authors>
<title>Genetic Algorithms + Data Structures = Evolution Programs.</title>
<date>1996</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="6550" citStr="Michalewicz, 1996" startWordPosition="1088" endWordPosition="1089"> from the IBM models have to be extended in some way to achieve more realistic alignments. This is usually performed by computing the alignments in both directions (i.e, first from f to e and then from e to f) and then combining them in a suitable way (this process is known as symmetrization). 3 Estimation of Distribution Algorithms Estimation of Distribution Algorithms (EDAs) (Larra˜naga and Lozano, 2001) are metaheuristics which has gained interest during the last five years due to their high performance when solving combinatorial optimization problems. EDAs, as well as genetics algorithms (Michalewicz, 1996), are population-based evolutionary algorithms but, instead of using genetic operators are based on the estimation/learning and posterior sampling of a probability distribution, which relates the variables or genes forming and individual or chromosome. In this way the dependence/independence relations between these variables can be explicitly modelled in the EDAs framework. The operation mode of a canonical EDA is shown in Figure 1. As we can see, the algorithm maintains a population of m individuals during the search. An individual is a candidate or potential solution to the problem being opt</context>
</contexts>
<marker>Michalewicz, 1996</marker>
<rawString>Z. Michalewicz. 1996. Genetic Algorithms + Data Structures = Evolution Programs. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An evaluation exercise for word alignment.</title>
<date>2003</date>
<booktitle>HLT-NAACL 2003 Workshop: Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>1--10</pages>
<editor>In Rada Mihalcea and Ted Pedersen, editors,</editor>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="2101" citStr="Mihalcea and Pedersen, 2003" startWordPosition="338" endWordPosition="341">ted by the Spanish Projects JCCM (PBI-05-022) and HERMES 05/06 (Vic. Inv. UCLM) of sentence pairs, each pair containing a sentence in a source language and a translation of this sentence in a target language. Word alignments are necessary to link the words in the source and in the target sentence. Statistical models for machine translation heavily depend on the concept of alignment, specifically, the well known IBM word based models (Brown et al., 1993). As a result of this, different task on aligments in statistical machine translation have been proposed in the last few years (HLTNAACL 2003 (Mihalcea and Pedersen, 2003) and ACL 2005 (Joel Martin, 2005)). In this paper, we propose a novel approach to deal with alignments. Specifically, we address the problem of searching for the best word alignment between a source and a target sentence. As there is no efficient exact method to compute the optimal alignment (known as Viterbi alignment) in most of the cases (specifically in the IBM models 3,4 and 5), in this work we propose the use of a recently appeared meta-heuristic family of algorithms, Estimation of Distribution Algorithms (EDAs). Clearly, by using a heuristic-based method we cannot guarantee the achievem</context>
<context position="17432" citStr="Mihalcea and Pedersen, 2003" startWordPosition="2992" endWordPosition="2995">l alignments. To this end, diffferent approaches (symmetrization methods) were tested. The results shown in section 5.2 were obtained by applying the refined method proposed in (Och and Ney, 2000). 5 Experimental Results Different experiments have been carried out in order to assess the correctness of the search algorithm. Next, the experimental metodology employed and the results obtained are described. 5.1 Corpora and evaluation Three different corpora and four different test sets have been used. All of them are taken from the two shared tasks in word alignments developed in HLT/NAACL 2003 (Mihalcea and Pedersen, 2003) and ACL 2005 (Joel Martin, 2005). These two tasks involved four different pair of languages, EnglishFrench, Romanian-English, English-Inuktitut and English-Hindi. English-French and RomanianEnglish pairs have been considered in these experiments (owing to the lack of timeto properly preprocess the Hindi and the Inuktitut). Next, a brief description of the corpora used is given. Regarding the Romanian-English task, the test data used to evaluate the alignments consisted in 248 sentences for the 2003 evaluation task and 200 for the 2005 evaluation task. In addition to this, a training corpus, c</context>
<context position="19865" citStr="Mihalcea and Pedersen, 2003" startWordPosition="3424" endWordPosition="3427">o-En 05 Training size 1M 97K 97K Vocabulary 68K / 86K 48K / 27K 48K / 27K Running words 20M / 23M 1.9M / 2M 1.9M / 2M Test size 447 248 200 It is important to emphasize that EDAs are nondeterministics algorithms. Because of this, the results presented in section 5.2 are actually the mean of the results obtained in ten different executions of the search algorithm. 5.2 Results In Tables 2, 3 and 4 the results obtained from the different tasks are presented. The results achieved by the technique proposed in this paper are compared with the best results presented in the shared tasks described in (Mihalcea and Pedersen, 2003) (Joel Martin, 2005). The results obtained by the GIZA++ hill-climbing algorithm are also presented. In these tables, the mean and the variance of the results obtained in ten executions of the search algorithm are shown. According to the small variances observed in the results we can conclude that the nondeterministic nature of this approach it is not statistically significant. According to these results, the proposed EDAbased search is very competitive with respect to the best result presented in the two shared task. In addition to these results, additional experiments were carried out in to </context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Rada Mihalcea and Ted Pedersen, editors, HLT-NAACL 2003 Workshop: Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–10, Edmonton, Alberta, Canada, May 31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinz Muhlenbein</author>
</authors>
<title>The equation for response to selection and its use for prediction.</title>
<date>1997</date>
<journal>Evolutionary Computation,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="10506" citStr="Muhlenbein, 1997" startWordPosition="1757" endWordPosition="1758">oint distribution is intractable in most cases. In the practice, what is learnt is a probabilistic model that consists in a factorization of the joint distribution. Different levels of complexity can be considered in that factorization, from univariate distributions to n-variate ones or Bayesian networks (see (Larra˜naga and Lozano, 2001, Chapter 3) for a review). In this paper, as this is the first approximation to the alignment problem with EDAs and, because of some questions that will be discussed later, we use the simplest EDA model: the Univariate Marginal Distribution Algorithm or UMDA (Muhlenbein, 1997). In UMDA it is assumed that all the variables are marginally independent, thus, the n-dimensional probability distribution, Pr(a1, ... , aJ), is factorized as the product of J marginal/unidimensional distributions: HJj=1 Pr(aj). Among the advantages of UMDA we can cite the following: no structural learning is needed; parameter learning is fast; small dataset can be used because only marginal probabilities have to be estimated; and, the sampling process is easy because each variable is independently sampled. 4 Design of an EDA to search for alignments In this section, an EDA algorithm to align</context>
</contexts>
<marker>Muhlenbein, 1997</marker>
<rawString>Heinz Muhlenbein. 1997. The equation for response to selection and its use for prediction. Evolutionary Computation, 5(3):303–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL00,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="17000" citStr="Och and Ney, 2000" startWordPosition="2925" endWordPosition="2928">tribution over the individuals is not estimated from the whole population. In the present task about 20% of the best individuals in each population are used for this purpose. As mentioned above, the fitness function used in the algorithm just allows for unidirectional alignments. Therefore, the search was conducted in both directions (i.e, from f to a and from a to f) combining the final results to achieve bidirectional alignments. To this end, diffferent approaches (symmetrization methods) were tested. The results shown in section 5.2 were obtained by applying the refined method proposed in (Och and Ney, 2000). 5 Experimental Results Different experiments have been carried out in order to assess the correctness of the search algorithm. Next, the experimental metodology employed and the results obtained are described. 5.1 Corpora and evaluation Three different corpora and four different test sets have been used. All of them are taken from the two shared tasks in word alignments developed in HLT/NAACL 2003 (Mihalcea and Pedersen, 2003) and ACL 2005 (Joel Martin, 2005). These two tasks involved four different pair of languages, EnglishFrench, Romanian-English, English-Inuktitut and English-Hindi. Engl</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In ACL00, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12908" citStr="Och and Ney, 2003" startWordPosition="2164" endWordPosition="2167">ibed as follows. Let a = a1 · · · aJ be the alignment represented by an individual. This alignment a is evaluated by computing the probability p(f, a|e). This probability is computed by using the IBM model 4 as: p(f, a|e) = � p(T, 7r|e) (τ,π)E(f,a) I n(0i|ei) x I �φi t(Tik|ei) x i=1 i=1 k=1 I d=1(7ri1 − cρi|£c(eρi),Jc(Ti1)) x i=1,φi&gt;0 I �φi d&gt;1(7rik − 7ri(k−1)|Jc(Tik)) x i=1 k=2 k=1 where the factors separated by x symbols denote fertility, translation, head permutation, non-head permutation, null-fertility, and null-translation probabilities1. This model was trained using the GIZA++ toolkit (Och and Ney, 2003) on the material available for the different alignment tasks described in section 5.1 4.3 Search In this section, some specific details about the search are given. As was mentioned in section 3, the algorithm starts by generating an initial set of hypotheses (initial population). In this case, a set of randomly generated alignments between the source and the target sentences are generated. Afterwards, all the individuals in this population (a fragment of a real population is shown in figure 3) are scored using the function defined in Eq.(4.2). At this point, the actual search starts by applyin</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>