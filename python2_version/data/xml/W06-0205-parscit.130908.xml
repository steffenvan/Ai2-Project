<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999235">
Automatic Knowledge Representation using a Graph-based Algorithm for
Language-Independent Lexical Chaining
</title>
<author confidence="0.688844">
Ga¨el Dias
</author>
<affiliation confidence="0.7081085">
HULTIG
University of Beira Interior
</affiliation>
<address confidence="0.749848">
Covilh˜a, Portugal
</address>
<email confidence="0.992485">
ddg@di.ubi.pt
</email>
<author confidence="0.489777">
Cl´audia Santos
</author>
<affiliation confidence="0.679585">
HULTIG
University of Beira Interior
</affiliation>
<address confidence="0.747838">
Covilh˜a, Portugal
</address>
<email confidence="0.991508">
claudia@dmnet.ubi.pt
</email>
<author confidence="0.796476">
Guillaume Cleuziou
</author>
<affiliation confidence="0.8207005">
LIFO
University of Orl´eans
</affiliation>
<address confidence="0.806336">
Orl´eans, France
</address>
<email confidence="0.845518">
cleuziou@univ-orleans.fr
</email>
<sectionHeader confidence="0.989783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997434941176471">
Lexical Chains are powerful representa-
tions of documents. In particular, they
have successfully been used in the field
of Automatic Text Summarization. How-
ever, until now, Lexical Chaining algo-
rithms have only been proposed for Eng-
lish. In this paper, we propose a greedy
Language-Independent algorithm that au-
tomatically extracts Lexical Chains from
texts. For that purpose, we build a hier-
archical lexico-semantic knowledge base
from a collection of texts by using the
Pole-Based Overlapping Clustering Algo-
rithm. As a consequence, our method-
ology can be applied to any language
and proposes a solution to language-
dependent Lexical Chainers.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999737382978723">
Lexical Chains are powerful representations of doc-
uments compared to broadly used bag-of-words rep-
resentations. In particular, they have successfully
been used in the field of Automatic Text Summa-
rization (Barzilay and Elhadad, 1997). However, un-
til now, Lexical Chaining algorithms have only been
proposed for English as they rely on linguistic re-
sources such as Thesauri (Morris and Hirst, 1991) or
Ontologies (Barzilay and Elhadad, 1997; Hirst and
St-Onge, 1997; Silber and McCoy, 2002; Galley and
McKeown, 2003).
Morris and Hirst (1991) were the first to propose
the concept of Lexical Chains to explore the dis-
course structure of a text. However, at the time of
writing their paper, no machine-readable thesaurus
was available so they manually generated Lexical
Chains using Roget’s Thesaurus (Roget, 1852).
A first computational model of Lexical Chains
is introduced by Hirst and St-Onge (1997). Their
biggest contribution to the study of Lexical Chains
is the mapping of WordNet (Miller, 1995) relations
and paths (transitive relationships) to (Morris and
Hirst, 1991) word relationship types. However, their
greedy algorithm does not use a part-of-speech tag-
ger. Instead, the algorithm only selects those words
that contain noun entries in WordNet to compute
Lexical Chains. But, as Barzilay and Elhadad (1997)
point at, the use of a part-of-speech tagger could
eliminate wrong inclusions of words such as read,
which has both noun and verb entries in WordNet.
So, Barzilay and Elhadad (1997) propose the first
dynamic method to compute Lexical Chains. They
argue that the most appropriate sense of a word can
only be chosen after examining all possible Lexi-
cal Chain combinations that can be generated from
a text. Because all possible senses of the word are
not taken into account, except at the time of inser-
tion, potentially pertinent context information that
is likely to appear after the word is lost. However,
this method of retaining all possible interpretations
until the end of the process, causes the exponential
growth of the time and space complexity.
As a consequence, Silber and McCoy (2002) pro-
pose a linear time version of (Barzilay and Elhadad,
1997) lexical chaining algorithm. In particular, (Sil-
ber and McCoy, 2002)’s implementation creates a
structure, called meta-chains, that implicitly stores
</bodyText>
<page confidence="0.980876">
36
</page>
<note confidence="0.69467">
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 36–47,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.988760352941176">
all chain interpretations without actually creating
them, thus keeping both the space and time usage
of the program linear.
Finally, Galley and McKeown (2003) propose a
chaining method that disambiguates nouns prior to
the processing of Lexical Chains. Their evaluation
shows that their algorithm is more accurate than
(Barzilay and Elhadad, 1997) and (Silber and Mc-
Coy, 2002) ones.
One common point of all these works is that Lex-
ical Chains are built using WordNet as the standard
linguistic resource. Unfortunately, systems based on
static linguistic knowledge bases are limited. First,
such resources are difficult to find. Second, they
are largely obsolete by the time they are available.
Third, linguistic resources capture a particular form
of lexical knowledge which is often very different
from the sort needed to specifically relate words or
sentences. In particular, WordNet is missing a lot
of explicit links between intuitively related words.
Fellbaum (1998) refers to such obvious omissions
in WordNet as the “tennis problem” where nouns
such as nets, rackets and umpires are all present,
but WordNet provides no links between these related
tennis concepts.
In order to solve these problems, we propose to
automatically construct from a collection of docu-
ments a lexico-semantic knowledge base with the
purpose to identify cohesive lexical relationships be-
tween words based on corpus evidence. This hi-
erarchical lexico-semantic knowledge base is built
by using the Pole-Based Overlapping Clustering Al-
gorithm (Cleuziou et al., 2004) that clusters words
with similar meanings and allows words with mul-
tiple meanings to belong to different clusters. The
second step of the process aims at automatically
extracting Lexical Chains from texts based on our
knowledge base. For that purpose, we propose a
new greedy algorithm which can be seen as an ex-
tension of (Hirst and St-Onge, 1997) and (Barzilay
and Elhadad, 1997) algorithms which allows polyse-
mous words to belong to different chains thus break-
ing the “one-word/one-concept per document” par-
adigm (Gale et al., 1992)1. In particular, it imple-
1This characteristic can be interesting for multi-topic docu-
ments like web news stories. Indeed, in this case, there may be
different topics in the same document as different news stories
may appear. In some way, it follows the idea of (Krovetz, 1998).
ments (Lin, 1998) information-theoretic definition
of similarity as the relatedness criterion for the at-
tribution of words to Lexical Chains2.
</bodyText>
<sectionHeader confidence="0.406754" genericHeader="method">
2 Building a Similarity Matrix
</sectionHeader>
<bodyText confidence="0.999508454545454">
In order to build the lexico-semantic knowledge
base, the Pole-Based Overlapping Clustering Algo-
rithm needs as input a similarity matrix that gathers
the similarities between all the words in the corpus.
For that purpose, we propose a contextual analysis
of each nominal unit (nouns and compound nouns)
in the corpus. In particular, each nominal unit is as-
sociated to a word context vector and the similar-
ity between nominal units is calculated by the in-
formative similarity measure proposed by (Dias and
Alves, 2005).
</bodyText>
<subsectionHeader confidence="0.982988">
2.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999990956521739">
The context corpus is first pre-processed in order
to extract nominal units from it. The TnT tagger
(Brants, 2000) is first applied to our context cor-
pus to morpho-syntactically mark all the words in
it. Once all words have been morpho-syntactically
tagged, we apply the statistically-based multiword
unit extractor SENTA (Dias et al., 1999) that ex-
tracts multiword units based on any input text3. For
example, multiword units are compound nouns (free
kick), compound determinants (an amount of), ver-
bal locutions (to put forward), adjectival locutions
(dark blue) or institutionalized phrases (con carne).
Finally, we use a set of well-known heuristics
(Daille, 1995) to retrieve compound nouns using the
idea that groups of words that correspond to a pri-
ori defined syntactical patterns such as Adj+Noun,
Noun+Noun, Noun+Prep+Noun can be identified
as compound nouns. Indeed, nouns usually con-
vey most of the information in a written text. They
are the main contributors to the “aboutness” of a
text. For example, free kick, city hall, operating sys-
tem are compound nouns which sense is not com-
positional i.e. the sense of the multiword unit can
</bodyText>
<footnote confidence="0.99579075">
2Of course, other similarity measures (Resnik, 1995; Jiang
and Conrath, 1997; Leacock and Chodorow, 1998) could be
implemented and should be evaluated in further work. How-
ever, we used (Lin, 1998) similarity measure as it has shown
improved results for Lexical Chains construction.
3By choosing both the TnT tagger and the multiword unit
extractor SENTA, we guarantee that our architecture remains as
language-independent as possible.
</footnote>
<page confidence="0.999707">
37
</page>
<bodyText confidence="0.99855625">
not be expressed by the sum of its constituents
senses. So, identifying lexico-semantic connections
between nouns is an adequate means of determining
cohesive ties between textual units4.
</bodyText>
<subsectionHeader confidence="0.998449">
2.2 Word Context Vectors
</subsectionHeader>
<bodyText confidence="0.999735705882353">
The similarity matrix is a matrix where each cell cor-
responds to a similarity value between two nominal
units5. In this paper, we propose a contextual analy-
sis of nominal units based on similarity between
word context vectors.
Word context vectors are an automated method
for representing information based on the local con-
text of words in texts. So, for each nominal unit in
the corpus, we associate an N-dimension vector con-
sisting of its N most related words6.
In order to find the most relevant co-occurrent
nominal units, we implement the Symmetric Con-
ditional Probability (Silva et al., 1999) which is
defined in Equation 1 where p(w1, w2), p(w1)
and p(w2) are respectively the probability of co-
occurrence of the nominal units w1 and w2 and the
marginal probabilities of w1 and w2.
</bodyText>
<equation confidence="0.999761">
SCP(w1, w2) = p(w1, w2)2 (1)
p(w1) � p(w2)
</equation>
<bodyText confidence="0.9999355">
In particular, the window context for the calcula-
tion of co-occurrence probabilities is settled to F=20
words. In fact, we count, in all the texts of the
corpus, the number of occurrences of w1 and w2
appearing together in a window context of F − 2
words. So, p(w1, w2) represents the density func-
tion computed as follows: the number of times w1
and w2 co-occur divided by the number of words in
the corpus7. In the present work, the values of the
SCP(., .) are not used as a factor of importance be-
tween words in the word context vector i.e. no dif-
ferentiation is made in terms of relevance between
the words within the word context vector. This issue
will be tackled in future work8.
</bodyText>
<footnote confidence="0.9352341">
4However, we acknowledge that verbs and adjectives should
also be tackled in future work.
5Many works have been proposed on word similarity (Lin,
1998).
6In our experiments, N=10.
7We note that multiword units are counted as single words
as when they are identified (e.g. President of the United States),
they are re-written in the corpus by linking all single words with
an underscore (e.g. President of the United States)
8We may point at the fact that satisfying results were
</footnote>
<subsectionHeader confidence="0.999218">
2.3 Similarity between Context Vectors
</subsectionHeader>
<bodyText confidence="0.9999845">
The closeness of vectors in the space is equivalent to
the closeness of the subject content. Thus, nominal
units that are used in a similar local context will have
vectors that are relatively close to each other. How-
ever, in order to define similarities between vectors,
we must transform each word context vector into
a high dimensional vector consisting of real-valued
components. As a consequence, each co-occurring
word of the word context vector is associated to a
weight which evaluates its importance in the corpus.
</bodyText>
<subsectionHeader confidence="0.992894">
2.3.1 Weighting score
</subsectionHeader>
<bodyText confidence="0.999121285714285">
The weighting score of any word in a document
can be directly derived from an adaptation of the
score proposed in (Dias and Alves, 2005). In par-
ticular, we consider the combination of two main
heuristics: the well-known tf.idf measure proposed
by (Salton et al., 1975) and a new density measure
(Dias and Alves, 2005).
tf.idf: Given a word w and a document d, the
tf.idf(w, d) is defined in Equation 2 where tf(w, d)
is the number of occurrences of w in d, |d |corre-
sponds to the number of words in d, N is the num-
ber of documents in the corpus and df(w) stands for
the number of documents in the corpus in which the
word w occurs.
</bodyText>
<equation confidence="0.99764">
tf.idf (w, d) = tf (|S|, d) x log2 ✒N
df (w)✓ (2)
</equation>
<bodyText confidence="0.9960972">
density: The basic idea of the word density mea-
sure is to evaluate the dispersion of a word within
a document. So, very disperse words will not be
as relevant as dense words. This density measure
dens(.,.) is defined in Equation 3.
</bodyText>
<equation confidence="0.999097">
1
ln(dist(o(w,k), o(w,k+1)) + e) (3)
</equation>
<bodyText confidence="0.985027888888889">
For any given word w, its density dens(w, d)
is calculated from all the distances between all
its occurrences in document d, tf(w, d). So,
dist(o(w,k),o(w,k+1)) calculates the distance that
separates two consecutive occurrences of w in terms
of words within the document. In particular, e is the
obtained by the Symmetric Conditional Probability measure
compared to the Pointwise Mutual Information for instance
(Cleuziou et al., 2003)
</bodyText>
<equation confidence="0.994677333333333">
tf(w,d)−1
dens(w, d) = ❳
k=1
</equation>
<page confidence="0.972394">
38
</page>
<bodyText confidence="0.997819727272727">
base of the natural logarithm so that ln(e) = 1. This
argument is included into Equation 3 as it will give
a density value of 1 for any word that only occurs
once in the document. In fact, we give this word a
high density value.
final weight: The weighting score weight(w) of
any word w in the corpus can be directly derived
from the previous two heuristics. This score is de-
fined in Equation 4 where tf and dens are respec-
tively the average of tf(., .) and dens(.,.) over all
the documents in which the word w occurs i.e. Nw.
</bodyText>
<equation confidence="0.999721">
weight(w) = tf.idf(w) x dens(w) (4)
</equation>
<bodyText confidence="0.933149">
where tf = Y-d tf(w,d) and dens(w) = Y-d dens(w,d)
</bodyText>
<subsectionHeader confidence="0.468634">
Nw Nw
2.3.2 Informative Similarity Measure
</subsectionHeader>
<bodyText confidence="0.999810294117647">
The next step aims at determining the similarity
between all nominal units. Theoretically, a similar-
ity measure can be defined as follows. Suppose that
Xi = (Xi1, Xie, Xi3, , Xip) is a row vector of ob-
servations on p variables associated with a label i.
The similarity between two words i and j is defined
as Sij = f(Xi, Xj) where f is some function of the
observed values. In the context of our work, Xi and
Xj are 10-dimension word context vectors.
In order to avoid the lexical repetition problem of
similarity measures, (Dias and Alves, 2005) have
proposed an informative similarity measure called
infoSimBA, which basic idea is to integrate into
the Cosine measure, the word co-occurrence fac-
tor inferred from a collection of documents with
the Symmetric Conditional Probability (Silva et al.,
1999). See Equation 5.
</bodyText>
<equation confidence="0.899728333333333">
Aij
InfoSimBA(Xi, Xj) = (5)
Bi x Bj + Aij
</equation>
<bodyText confidence="0.743699">
where
</bodyText>
<equation confidence="0.99266">
Xik x Xjl x SCP(wik, wjl)
Xik x Xil x SCP (wik, wil)
</equation>
<bodyText confidence="0.999004666666667">
and any Xzv corresponds to the word weighting fac-
tor weight(wzv), SCP(wik, wjl) is the Symmetric
Conditional Probability value between wik, the word
that indexes the word context vector i at position k
and wjl, the word that indexes the word context vec-
tor j at position l.
In particular, this similarity measure has proved to
lead to better results compared to the classical simi-
larity measure (Cosine) and shares the same idea as
the Latent Semantic Analysis (LSA) but in a differ-
ent manner. Let’s consider the following two sen-
tences.
</bodyText>
<listItem confidence="0.9991665">
(1) Ronaldo defeated the goalkeeper once more.
(2) Real_Madrid_striker scored again.
</listItem>
<bodyText confidence="0.999952">
It is clear that both sentences (1) and (2) are simi-
lar although they do not share any word in common.
Such a situation would result in a null Cosine value
so evidencing no relationship between (1) and (2).
To solve this problem, the InfoSimBA( ., .) func-
tion would calculate for each word in sentence (2),
the product of its weight with each weight of all the
words in sentence (1), and would then multiply this
product by the degree of cohesiveness existing be-
tween those two words calculated by the Symmet-
ric Conditional Probability measure. For example,
Real Madrid striker would give rise to the sum of
6 products i.e. Real Madrid striker with Ronaldo,
Real Madrid striker with defeated and so on and
so forth. As a consequence, sentence (1) and (2)
would show a high similarity as Real Madrid striker
is highly related to Ronaldo.
Once the similarity matrix is built based on the
infoSimBA between all word context vectors of
all nominal units in the corpus, we give it as in-
put to the Pole-Based Overlapping Clustering Algo-
rithm (Cleuziou et al., 2004) to build a hierarchy of
concepts i.e. our lexico-semantic knowledge base.
</bodyText>
<sectionHeader confidence="0.847041" genericHeader="method">
3 Hierarchy of Concepts
</sectionHeader>
<bodyText confidence="0.997160833333333">
Clustering is the task that structures units in such
a way it reflects the semantic relations existing be-
tween them. In our framework nominal units are first
grouped into overlapping clusters (or soft-clusters)
such that final clusters correspond to conceptual
classes (called “concepts” in the following). Then,
concepts are hierarchically structured in order to
capture semantic links between them.
Many clustering methods have been proposed in
the data analysis research fields. Few of them
propose overlapping clusters as output, in spite of
the interest it represents for domains of application
</bodyText>
<equation confidence="0.987253363636364">
❳p
l=1
Aij =
❳p
k=1
�i✈
Vi, Bi =
❳p
l=1
❳p
k=1
</equation>
<page confidence="0.987626">
39
</page>
<bodyText confidence="0.999961565217391">
such as Natural Language Processing or Bioinfor-
matics. PoBOC (Pole-Based Overlapping Cluster-
ing) (Cleuziou et al., 2004) and CBC (Clustering By
Committees) (Pantel and Lin, 2002) are two clus-
tering algorithms suitable for the word clustering
task. They both proceed by first constructing tight
clusters9 and then assigning residual objects to their
most similar tight clusters.
A recent comparative study (Cicurel et al., 2006)
shows that CBC and PoBOC both lead to relevant
results for the task of word clustering. Neverthe-
less CBC requires parameters hard to tune whereas
PoBOC is free of any parametrization. The last ar-
gument encouraged us to use the PoBOC algorithm.
Unlike most of commonly used clustering algo-
rithms, the Pole-Based Overlapping Clustering Al-
gorithm shows the following advantages among oth-
ers : (1) it requires no parameters i.e. input is re-
stricted to a single similarity matrix, (2) the num-
ber of final clusters is automatically found and (3) it
provides overlapping clusters allowing to take into
account the different possible meanings of lexical
units.
</bodyText>
<subsectionHeader confidence="0.999808">
3.1 A Graph-based Approach
</subsectionHeader>
<bodyText confidence="0.995934857142857">
The Pole-Based Overlapping Clustering Algorithm
is based on a graph-theoretical framework. Graph
formalism is often used in the context of cluster-
ing (graph-clustering). It first consists in defining
a graph structure which illustrates the data (vertices)
with links (edges) between them and then in propos-
ing a graph-partitioning process.
Numerous graph structures have been proposed
(Estivill-Castro et al., 2001). They all consider the
data set as set of vertices but differ on the way to de-
cide that two vertices are connected. Some method-
ologies are listed below where V is the set of ver-
tices, E the set of edges, G(V, E) a graph and d a
distance measure:
</bodyText>
<listItem confidence="0.995254666666667">
• Nearest Neighbor Graph (NNG) : each vertex
is connected to its nearest neighbor,
• Minimum Spanning Tree (MST) : `d(xi, xj) E
V x V a path exists between xi and xj in G with
Y_(x xj)EE d(xi, xj) minimized,
9The tight clusters are called “committees” in CBC and
“poles” in PoBOC.
• Relative Neighborhood Graph (RNG) : xi and
xj are connected iff `dxk E V \ {xi, xj},
d(xi, xj) &lt; max{d(xi, xk), d(xj, xk)}
• Gabriel Graph (GG) : xi and xj are connected
iff the circle with diameter xixj is empty,
• Delaunay Triangulation (DT) : xi and xj are
connected iff the associated Voronoi cells are
adjacent.
</listItem>
<bodyText confidence="0.88976405">
In particular, an inclusion order exists on these
graphs. One can show that NNG C MST C RNG C
GG C DT.
The choice of the suitable graph structure depends
on the expressiveness we want an edge to capture
and the partitioning process we plan to perform. The
Pole-Based Overlapping Clustering Algorithm aims
at retrieving dense subsets in a graph where two
similar data are connected and two dissimilar ones
are disconnected. Noticing that previous structures
do not match with this definition of a proximity-
graph10, a new variant is proposed with the Pole-
Based Overlapping Clustering Algorithm in defini-
tion 3.1.
Definition 3.1 Given a similarity measure s on a
data set X, the graph (denoted G3(V, E)) is defined
by the set of vertices V = X and the set of edges E
such that (xi, xj) E E 4* xi E N(xj) ∧ xj E N(xi).
In particular, N(xi) corresponds to the local neigh-
borhood of xi built as in equation 6.
</bodyText>
<equation confidence="0.983587">
N(xi) = {xj E X�s(xi, xj) &gt; s(xi, X)} (6)
</equation>
<bodyText confidence="0.993390333333333">
where the notation s(xi, I) denotes the average sim-
ilarity of xi with the set of objects I i.e.
This definition of neighborhood is a way to avoid
requiring to a parameter that would be too dependent
of the similarity used. Furthermore, the use of lo-
cal neighborhoods avoids the use of arbitrary thresh-
olds which mask the variations of densities. Indeed,
clusters are extracted from a similarity graph which
differs from traditional proximity graphs (Jarom-
czyk and Toussaint, 1992) in the definition of local
10Indeed, for instance, all of these graphs connect an outlier
with at least one other vertex. This is not the case with PoBOC.
</bodyText>
<equation confidence="0.9482734">
(7)
�I�
❳
xkEI
s(xi, xk)
</equation>
<page confidence="0.959857">
40
</page>
<bodyText confidence="0.999819625">
neighborhoods which condition edges in the graph.
Neighborhood is different for each object and is
computed on the basis of similarities with all other
objects. Finally, an edge connects two vertices if
they are both contained in the neighborhood of the
other one. Figure 1 illustrates the neighborhood con-
straint above. In this case, as xi and xj are not both
in the intersection, they would not be connected.
</bodyText>
<figureCaption confidence="0.652175">
Figure 1: To be connected, both xi and xj must be
in the intersection.
</figureCaption>
<bodyText confidence="0.999357833333333">
As a consequence, initialized with P = {x}, the
clique then grows until no vertex can be added.
The second heuristic guides the selection of the
starting vertices in a simple manner. Given a set
of Poles P1, ... , Pm already extracted, we select the
vertex x as in Equation 8.
</bodyText>
<equation confidence="0.950179">
s(x, P1 ∪ ··· ∪ P,) = min s(xi, P1 ∪ ··· ∪ P,) (8)
xi
</equation>
<bodyText confidence="0.9923905">
A new Pole is then built from x if and only if x
satisfies the following conditions:
</bodyText>
<listItem confidence="0.983127333333333">
• ∀k ∈ {1,..., m} , x ∈�Pk,
• s(x, P1 ∪ ··· ∪ Pm) &lt; s(X, X) = |X|2 ❳ ❳ s(xi, xj)
xi xj
</listItem>
<bodyText confidence="0.988930285714286">
Poles are thus extracted while P1 U • • • U Pm =�
X and the next starting vertex x is far enough from
the previous Poles. In particular, as Poles represent
the seeds of the further final clusters, this heuristic
gives no restriction on the number of clusters. The
first Pole is obtained from the starting point x* that
checks Equation 9.
</bodyText>
<equation confidence="0.9036655">
x =arg min s(xk, X) (9)
xkEX
</equation>
<subsectionHeader confidence="0.999798">
3.2 Discovery of Poles
</subsectionHeader>
<bodyText confidence="0.9999795625">
The graph representation helps to discover a set
of fully-connected subgraphs (cliques) highly sep-
arated, denoted as Poles. Because G3(V, E) is built
such that two vertices xi and xj are connected if and
only if they are similar11, a clique has the required
properties to be a good cluster. Indeed, such a clus-
ter guarantees that all its constituents are similar.
The search of maximal cliques in a graph is an
NP-complete problem. As a consequence, heuristics
are used in order to (1) build a great clique around a
starting vertex (Bomze et al., 1999) and (2) choose
the starting vertices in such a way cliques are as dis-
tant as possible.
Given a starting vertex x, the first heuristic con-
sists in adding iteratively the vertex xi which satis-
fies the following conditions:
</bodyText>
<subsectionHeader confidence="0.999597">
3.3 Multi-Assignment
</subsectionHeader>
<bodyText confidence="0.972166428571429">
Once the Poles are built, the Pole-Based Overlap-
ping Clustering algorithm uses them as clusters rep-
resentatives. Membership functions m(.,.) are de-
fined in order to assign each object to its nearest
Poles as shown in Equation 10.
∀xi ∈ X, Pj ∈ {P1, ... , Pm} : m(xi, Pj) = s(xi, Pj) (10)
For each object xi to assign, the set of poles is
ordered (P1(xi), ... , Pm(xi)) such that P1(xi) de-
notes the nearest pole12 for xi, P2(xi) the second
nearest pole for xi and so on. We first assign xi to its
closest Pole (P1(xi)). Then, for each pole Pk(xi)(in
the order previously defined) we decide to assign xi
to Pk(xi) if it satisfies to the following two condi-
tions :
</bodyText>
<listItem confidence="0.866949625">
• ∀k&apos; &lt; k, xi is assigned to Pk, (xi),
• xi is connected to each vertex in P (with P the
clique/Pole in construction),
• if k &lt; m,
s(xi, Pk(xi)) ≥ 2
s(xi, Pk−1(xi)) + s(xi, Pk}1(xi))
• among the connected vertices, xi is the nearest
one in average (s(xi, P)).
</listItem>
<bodyText confidence="0.8670744">
11In the sense that xi (resp. xj) is more similar to xj (resp.
xi) than to other data on average.
This methodology results into a coverage of the
starting data set with overlapping clusters (extended
Poles).
</bodyText>
<equation confidence="0.395814">
12P1(xi) =arg max,j s(xi, Pj)
</equation>
<page confidence="0.997332">
41
</page>
<subsectionHeader confidence="0.932722">
3.4 Hierarchical Organization
</subsectionHeader>
<bodyText confidence="0.9992946">
A final step consists in organizing the obtained clus-
ters into a hierarchical tree. This structure is use-
ful to catch the topology of a set of a priori discon-
nected groups. The Pole-Based Overlapping Clus-
tering algorithm integrates this stage and proceeds
by successive merging of the two nearest clusters
like for usual agglomerative approaches (Sneath and
Sokal, 1973). In this process, the similarity be-
tween two clusters is obtained by the average-link
(or complete-link) method:
</bodyText>
<equation confidence="0.9729018">
❳
s(Ip, Iq) = |Ip|.|Iq|
❳
1
x��I�
</equation>
<bodyText confidence="0.999637333333333">
To deal with overlapping clusters we considere in
Equation 11 the similarity between an object and it-
self to be equal to 1 : s(xi7 xi) = 1.
</bodyText>
<sectionHeader confidence="0.988431" genericHeader="method">
4 Lexical Chaining Algorithm
</sectionHeader>
<bodyText confidence="0.999781826086957">
Once the lexico-semantic knowledge base has been
built, it is possible to use it for Lexical Chaining.
In this section, we propose a new greedy algorithm
which can be seen as an extension of (Hirst and St-
Onge, 1997) and (Barzilay and Elhadad, 1997) al-
gorithms as it allows polysemous words to belong
to different chains thus breaking the “one-word/one-
concept per document” paradigm (Gale et al., 1992).
Indeed, multi-topic documents like web news sto-
ries may introduce different topics in the same doc-
ument/url and do not respect the “one sense per dis-
course” paradigm. As we want to deal with real-
world applications, this characteristic may show in-
teresting results for the specific task of Text Summa-
rization for Web documents. Indeed, comparatively
to the experiments made by (Gale et al., 1992) that
deal with “well written discourse”, web documents
show unusual discourse structures. In some way,
our algorithm follows the idea of (Krovetz, 1998).
Finally, it implements (Lin, 1998)’s information-
theoretic definition of similarity as the relatedness
criterion for the attribution of words to Lexical
Chains.
</bodyText>
<subsectionHeader confidence="0.973199">
4.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.996185857142857">
Our chaining algorithm is based on both approaches
of (Barzilay and Elhadad, 1997) and (Hirst and St-
Onge, 1997). So, our chaining model is developed
according to all possible alternatives of word senses.
In fact, all senses of a word are defined by the clus-
ters the word appears in13. We present our algorithm
below.
</bodyText>
<figure confidence="0.973415">
Begin with no chain.
For all distinct nominal units in text order do
For all its senses do
a) - among present chains find the sense
which satisfies the relatedness
criterion and link the new word to
this chain.
- Remove unappropriate senses of the
new word and the chain members.
b)if no sense is close enough, start a new chain.
End For
End For
End
</figure>
<subsectionHeader confidence="0.998393">
4.2 Assignment of a word to a Lexical Chain
</subsectionHeader>
<bodyText confidence="0.999983833333333">
In order to assign a word to a given Lexical Chain,
we need to evaluate the degree of relatedness of the
given word to the words in the chain. This is done
by evaluating the relatedness between all the clusters
present in the Lexical Chain and all the clusters in
which the word appears.
</bodyText>
<subsectionHeader confidence="0.646158">
4.2.1 Scoring Function
</subsectionHeader>
<bodyText confidence="0.99961325">
In order to determine if two clusters are semanti-
cally related, we use our lexico-semantic knowledge
base and apply (Lin, 1998)’s measure of semantic
similarity defined in Equation 12.
</bodyText>
<equation confidence="0.998109">
2 x log P(C0)
simLin(C1, C2) = log P(C1) + log P(C2) (12)
</equation>
<bodyText confidence="0.990809">
The computation of Equation 12 is illustrated be-
low using the fragment of WordNet in Figure 2.
</bodyText>
<figureCaption confidence="0.994412">
Figure 2: Fragment of WordNet (Lin, 1998).
</figureCaption>
<bodyText confidence="0.7870485">
13From now on, for presentation purposes, we will take as
synonymous the words clusters and senses
</bodyText>
<equation confidence="0.860313">
s(xi, xj) (11)
x��I9
</equation>
<page confidence="0.988931">
42
</page>
<bodyText confidence="0.999934333333333">
In this case, it would be easy to compute the sim-
ilarity between the concepts of hill and coast where
the number attached to each node C is P(C). It is
shown in Equation 13.
assigned to the Lexical Chain. This situation is de-
fined in Equation 15 where c is a constant to be tuned
and n is the number of words in the taxonomy. So,
if Equation 15 is satisfied, the word w with cluster
Ck is agglomerated to the Lexical Chain.
</bodyText>
<equation confidence="0.965963285714286">
simLin(hill, coast) = 2 log P(geological − formation) = 0.59 (13)
log P(hill) + log P(coast)
n
❳
i=0
❳n
7=i+1
</equation>
<bodyText confidence="0.9983455">
However, in our taxonomy, as in any knowl-
edge base computed by hierarchical clustering algo-
rithms, only leaves contain words. So, upper clusters
(i.e. nodes) in the taxonomy gather all distinct words
that appear in the clusters they subsume. We present
this situation in Figure 3.
</bodyText>
<figureCaption confidence="0.997618">
Figure 3: Fragment of our taxonomy.
</figureCaption>
<bodyText confidence="0.9342545">
In particular, clusters C305 and C306 of our
hierarchical tree, for the domain of Economy,
are represented by the following sets of words
C305 ={life, effort, stability, steps, negotiations}
and C306 ={steps, restructure, corporations, abuse,
interests, ministers} and the number attached to each
node C is P(C) calculated as in Equation 1414.
# of words in the cluster
</bodyText>
<equation confidence="0.943926">
P(Ci) = # of distinct words in all clusters (14)
</equation>
<subsubsectionHeader confidence="0.624368">
4.2.2 Relatedness criterion
</subsubsectionHeader>
<bodyText confidence="0.9356278">
The relatedness criterion is the threshold that
needs to be respected in order to assign a word to
a Lexical Chain. In fact, it works like a threshold.
In this case, it is based on the average semantic sim-
ilarity between all the clusters present in the taxon-
omy. So, if all semantic similarities between a candi-
date word cluster Ck and all the clusters in the chain
bl, Cl respect the relatedness criterion, the word is
14The value 2843 in Figure 3 is the total number of distinct
words in our concept hierarchy.
</bodyText>
<equation confidence="0.976728">
simLin(Ci, C;)
n2
2 − n
</equation>
<bodyText confidence="0.9992085">
In the following section, we present an example
of our algorithm.
</bodyText>
<subsectionHeader confidence="0.923236">
4.2.3 Example of the Lexical Chain algorithm
</subsectionHeader>
<bodyText confidence="0.999584083333333">
The example below illustrates our Lexical Chain
algorithm. Let’s consider that a node is created
for the first nominal unit encountered in the text
i.e. crisis with its sense (C31). The next ap-
pearing candidate word is recession which has two
senses (C29 and C34). Considering a relatedness cri-
terion equal to 0.81 and the following similarities,
simLin(C31, C29) = 0.87, simLin(C31, C34) = 0.82 , the
choice of the sense for recession splits the Lexical
Chain into two different interpretations as shown
in Figure 4, as both similarities overtake the given
threshold 0.81.
</bodyText>
<figureCaption confidence="0.981352">
Figure 4: Interpretations 1 and 2.
</figureCaption>
<bodyText confidence="0.999958222222222">
The next candidate word trouble has also two
senses (C29 and C32). As all the words in a Lexi-
cal Chain influence each other in the selection of the
respective senses of the new word considered, we
have the following situation in Figure 5.
So, three cases can happen: (1) all similarities
overtake the threshold and we must consider both
representations, (2) only the similarities related to
one representation overtake the threshold and we
</bodyText>
<equation confidence="0.89939">
`dl, simLin(Ck, Cl) &gt; c x
(15)
</equation>
<page confidence="0.998914">
43
</page>
<figureCaption confidence="0.999953">
Figure 5: Selection of senses.
</figureCaption>
<bodyText confidence="0.974210476190476">
only consider this representation or (3) none of the
similarities overtake the threshold and we create a
new Lexical Chain. So, we proceed with our algo-
rithm for both interpretations.
Interpretation 1 shows the following similari-
ties simLin(C31, C29) = 0.87, simLin(C31, C32) =
0.75, simLin(C29, C29) = 1.0, simLin(C29, C32) =
0.78 and interpretation 2 the following ones,
simLin(C31, C29) = 0.87, simLin(C31, C32) = 0.75,
simLin(C34, C29) = 0.54, simLin(C34, C32) = 0.55 .
By computing the average similarities for in-
terpretations 1 and 2, we reach the following re-
sults: average(Interpretation1) = 0.85 &gt; 0.81 and
average(Interpretation2) = 0.68 ?� 0.81 .
As a consequence, the word trouble is inserted in
the Lexical Chain with the appropriate sense (C29)
as it maximizes the overall similarity of the chain
and the chain members senses are updated. In this
example, the interpretation with (C32) is discarded
as is the cluster (C34) for recession. This processing
is described in Figure 6.
</bodyText>
<figureCaption confidence="0.997618">
Figure 6: Selection of appropriate senses.
</figureCaption>
<subsectionHeader confidence="0.933139">
4.2.4 Score of a chain
</subsectionHeader>
<bodyText confidence="0.99995425">
Once all chains have been computed, only the
high-scoring ones must be picked up as represent-
ing the important concepts of the original docu-
ment. Therefore, one must first identify the strongest
chains. Like in (Barzilay and Elhadad, 1997), we
define a chain score which is defined in Equation 16
where |chain |is the number of words in the chain.
As all chains will be scored, the ones with higher
scores will be extracted. Of course, a threshold will
have to be defined by the user. In the next section,
we will show some qualitative and quantitative re-
sults of our architecture.
</bodyText>
<sectionHeader confidence="0.997094" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999946933333334">
The evaluation of Lexical Chains is generally diffi-
cult. Even if they can be effectively used in many
practical applications, Lexical Chains are seldom
desirable outputs in a real-world application, and
it is unclear how to assess their quality indepen-
dently of the underlying application in which they
are used (Budanitsky and Hirst, 2006). For example,
in Summarization, it is hard to determine whether a
good or bad performance comes from the efficiency
of the lexical chaining algorithm or from the appro-
priateness of using Lexical Chains in that kind of
application. It is also true that some work has been
done in this direction (Budanitsky and Hirst, 2006)
by collecting Human Lexical Chains to compare
against automatically built Lexical Chains. How-
ever, this type of evaluation is logistically impos-
sible to perform as we aim at developing a system
that does not depend on any language or topic. So,
in this section, we will only present some results
generated by our architecture (like (Barzilay and El-
hadad, 1997; Teich and Fankhauser, 2004) do), al-
though we acknowledge that other comparative eval-
uations (with WordNet, with Human Lexical Chains
or within independent applications like Text Sum-
marization) must be done in order to draw definitive
conclusions.
We have generated four taxonomies from four dif-
ferent domains (Sport, Economy, Politics and War)
from a set of documents of the DUC 200415. More-
over, we have extracted Lexical Chains for all four
</bodyText>
<equation confidence="0.829406">
15http://duc.nist.gov/duc2004/
score(chain) =
(jchainj − 1)jchainj
2
jchainj−1 jchainj simLin(Ci, C;)
❳ ❳
i=0 7=i+1
(16)
</equation>
<page confidence="0.988734">
44
</page>
<bodyText confidence="0.999261">
domains to show the ability of our system to switch
from domain to domain without any problem.
</bodyText>
<subsectionHeader confidence="0.998556">
5.1 Quantitative Function
</subsectionHeader>
<bodyText confidence="0.976239888888889">
Four texts from each domain of the DUC 2004 cor-
pus have been used to extract Lexical Chains based
on the four knowledge bases built from all texts of
DUC 2004 for each one of the four following do-
mains: Sport, Economy, Politics and War. However,
in this section, we will only present the results from
the Sport Domain as results show similar behaviors
for the other domains. In particular, we present in
Table 1 the characteristics of each document.
</bodyText>
<table confidence="0.9998364">
# Words #Distinct Words #Distinct Nouns
Doc 1 8133 1956 672
Doc 2 3823 1630 708
Doc 3 4594 953 324
Doc 4 4530 1265 431
</table>
<tableCaption confidence="0.9999">
Table 1: Characteristics of Documents for Sport
</tableCaption>
<bodyText confidence="0.999120375">
The first interesting conclusion shown in Table 2
is that the number of Lexical Chains does not de-
pend on the document size but rather on the nominal
units distribution. Indeed, for example, the number
of words in Document 1 is twice as big as in Doc-
ument 2. Although, we have more Lexical Chains
in Document 2 than in Document 1, as Document 2
has more distinct nominal units.
</bodyText>
<table confidence="0.9892638">
c=5 c=6 c=7 c=8
Doc 1 27 43 73 73
Doc 2 31 52 81 83
Doc 3 28 40 51 51
Doc 4 29 53 83 87
</table>
<tableCaption confidence="0.998662">
Table 2: # Lexical Chains per Document
</tableCaption>
<bodyText confidence="0.9984295">
The second interesting conclusion is that our algo-
rithm does not gather words that belong to only one
cluster and take advantage of the automatically built
lexico-semantic knowledge base. This is illustrated
in Table 3. However, it is obvious that by increasing
the constant c the words in a chain tend to belong to
only one cluster as it is the case for most of the best
Lexical Chains with c = 8.
</bodyText>
<subsectionHeader confidence="0.996067">
5.2 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.993127">
In this section, as it is done in (Barzilay and Elhadad,
1997; Teich and Fankhauser, 2004), we present the
</bodyText>
<table confidence="0.9986506">
c=5 c=6 c=7 c=8
Doc 1 19 13 7 7
Doc 2 13 6 3 3
Doc 3 3 4 4 4
Doc 4 6 4 3 3
</table>
<tableCaption confidence="0.999384">
Table 3: # Clusters per Lexical Chain
</tableCaption>
<bodyText confidence="0.999877">
five highest-scoring chains for the best threshold that
we experimentally evaluated to be c = 7 for each
domain (See Tables 4, 5, 6, 7). It is clear that the
obtained Lexical Chains show a desirable degree of
representativeness of the text in analysis.
</bodyText>
<subsectionHeader confidence="0.422348">
Domain=Sport, Document=3, c=7
</subsectionHeader>
<bodyText confidence="0.7213668">
- #0, 1 cluster and score=1.0: {United States, couple, competition}
- #6, 3 clusters and score=1.0: {boats, Sunday night, sailor, Sword, Orion,
veteran, cutter, Winston Churchill, Solo Globe, Challenger, navy, Race, sup-
position, instructions, responsibility, skipper, east, Melbourne, deck, kilo-
meter, masts, bodies, races, GMT, Admiral’s, Cups, Britain, Star, Class,
Atlanta, Seattle, arms, fatality, sea, waves, dark, yacht’s, Dad, Guy’s, son,
Mark, beer, talk, life, Richard, Winning, affair, canopy, death}
- #9, 1 cluster and score=1.0: {record, days, hours, minutes, rescue}
- #16, 3 clusters and score=1.0: {Snow, shape, north, easters, thunder,
storm, change, knots, west, level, maxi’s, search, Authority, seas, helicopter,
night vision, equipment, feet, rescues, Campbell, suffering, hypothermia,
safety, foot, sailors, colleagues, Hospital, deaths, bodies, fatality}
- #19, 2 clusters and score=1.0: {challenge, crew, Monday, VC, Offshore,
Stand, Newcastle, mid morning, Eden, Rescuers, aircraft, unsure, where-
abouts, killing, contact}
</bodyText>
<tableCaption confidence="0.962469">
Table 4: 5 best Lexical Chains for Sport
</tableCaption>
<subsectionHeader confidence="0.161687">
Domain=Economy, Document=5, c=7
</subsectionHeader>
<bodyText confidence="0.9732174">
- #88, 4 clusters and score=1.0: {sign, chance, Rio, Janeiro, Grande, Sul,
uphill, promise, hospitals, powerhouse, success, inhabitants, victory, pad,
presidency, contingent, exit, legislature}
- #50, 1 cluster and score=1.0: {transactions, taxes, Stabilization, spate,
fuel, income, fortunes, means}
- #77, 1 cluster and score=1.0: {proposal, factory, owners, Fund, Rubin’s}
- #126, 1 cluster and score=1.0: {disaster, control, investment, review}
-
#12, 2 clusters and score=0.99: {issue, order, University, population, ques-
tion, timing, currencies}
</bodyText>
<tableCaption confidence="0.869479">
Table 5: 5 best Lexical Chains for Economy
</tableCaption>
<bodyText confidence="0.999916222222222">
For instance, the Lexical Chain #16 in the domain
of Sport clearly exemplifies the tragedy of climbers
that were killed in a sudden change of weather in
the mountains and who could not be rescued by the
authorities.
However, some Lexical Chains are less expres-
sive. For instance, it is not clear what the Lexical
Chain #40 expresses in the domain of Politics. In-
deed, none of the words present in the chain seem
</bodyText>
<page confidence="0.997979">
45
</page>
<note confidence="0.246702">
Domain=Politics, Document=3, c=7
</note>
<bodyText confidence="0.987319666666666">
- #5, 1 cluster and score=1.0: {report, leaders, lives, information}
- #33, 1 cluster and score=1.0: {past, attention, defenders, investigations}
- #28, 2 clusters and score=0.95: {investigators, hospital, ward, wounds,
neck, description, fashion, suspects, raids, assault, rifles, door, further de-
tails, surgery, service, detective, Igor, Kozhevnikov, Ministry}
-
#40, 2 clusters and score=0.92: {security, times, weeks, fire}
- #24, 3 clusters and score=0.85: {enemies, Choice, stairwell, assailants,
woman, attackers, entrance, car, guns, Friends, relatives, Mrs. Staravoitova,
founder, movement, well thought, Sergei, Kozyrev, Association, Societies,
supporter, Stalin’s, council, criminals, Yegor, Gaidar, minister, ally, sugges-
tions, measures, smile, commitment}
</bodyText>
<tableCaption confidence="0.962745">
Table 6: 5 best Lexical Chains for Politics
</tableCaption>
<author confidence="0.21592">
Domain=War, Document=1, c=7
</author>
<bodyText confidence="0.816002714285714">
- #25, 2 clusters and score=1.0: {lightning, advance, Africa’s, nation,
outskirts, capital Kinshasa, troops, Angola, Zimbabwe, Namibia, chunk,
routes, Katanga, Eastern, Kasai, provinces, copper}
- #53, 1 cluster and score=1.0: {Back, years, Ngeyo, farm, farmers, organi-
zation, breadbasket, quarter, century, businessman, hotels, tourist, memory,
rivalry, rebellions}
- #56, 1 cluster and score=1.0: {political, freedoms, Hutus, Mai-Mai, war-
riors, Hunde, Nande, militiamen, Rwanda, ideology, weapons, persecu-
tion, landowners, ranchers, anarchy, Safari, Ngezayo, farmer, hotel, owner,
camps}
- #24, 2 clusters and score=0.87: {fighting, people, leaders, diplomats,
cause, president, Washington, U.S, units, weeks}
- #51, 2 clusters and score=0.82: {West, buildings, sight, point, tourists,
mountain, gorillas, shops, guest, disputes}
</bodyText>
<tableCaption confidence="0.929693">
Table 7: 5 best Lexical Chains for War
</tableCaption>
<bodyText confidence="0.999897166666667">
to express any idea about Politics. Moreover, due
to the small number of inter-related nominal units
within the Lexical Chain, this one can not be under-
stood as it is without context. In fact, it was related
to problems of car firing that have been occurring in
the past few weeks and provoked security problems
in the town.
Although some Lexical Chains are understand-
able as they are, most of them must be replaced in
their context to fully understand their representative-
ness of the topics or subtopics of the text being an-
alyzed. As a consequence, we deeply believe that
Lexical Chains must be evaluated in the context of
Natural Language Processing applications (such as
Text Summarization (Doran et al., 2004)), as com-
paring Lexical Chains as they are is a very difficult
task to tackle which may even lead to inconclusive
results.
</bodyText>
<sectionHeader confidence="0.998967" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999955862068966">
In this paper, we implemented a greedy Language-
Independent algorithm for building Lexical Chains.
For that purpose, we first constructed a lexico-
semantic knowledge base by applying the Pole-
Based Overlapping Clustering algorithm (Cleuziou
et al., 2004) to word-context vectors obtained by the
application of the SCP(.,.) measure (Silva et al.,
1999) and the InfoSimBA(., .) (Dias and Alves,
2005) similarity measure. In a second step, we im-
plemented (Lin, 1998)’s similarity measure and used
it to define the relatedness criterion in order to as-
sign a given word to a given chain in the lexical
chaining process. Finally, our experimental eval-
uation shows that relevant Lexical Chains can be
constructed with our lexical chaining algorithm, al-
though we acknowledge that more comparative eval-
uations must be done in order to draw definitive con-
clusions. In particular, in future work, we want to
compare our methodology using WordNet as the ba-
sic knowledge base, implement different similarity
measures (Resnik, 1995; Jiang and Conrath, 1997;
Leacock and Chodorow, 1998), experiment differ-
ent Lexical Chains algorithms (Hirst and St-Onge,
1997; Barzilay and Elhadad, 1997; Galley and McK-
eown, 2003), scale our greedy algorithm for real-
world applications following (Silber and McCoy,
2002) ideas and finally evaluate our system in inde-
pendent Natural Language Processing applications
such as Text Summarization (Doran et al., 2004).
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860222222222">
R. Barzilay and M. Elhadad. 1997. Using Lexical Chains
for Text Summarization. Proceedings of the Intelli-
gent Scalable Text Summarization Workshop (ISTS-
97), ACL, Madrid, Spain, pages 10-18.
I. Bomze, M. Budinich, P. Pardalos, and M. Pelillo. 1999.
The Maximum Clique Problem. Handbook of Com-
binatorial Optimization, volume 4. Kluwer Academic
publishers, Boston, MA.
T. Brants. 2000. TnT - a Statistical Part-of-Speech Tag-
ger. In Proceedings of the 6th Applied NLP Confer-
ence, ANLP-2000. Seattle, WA.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness. In
Computational Linguistics, 32(1). pages: 13-47.
L. Cicurel, S. Bloehdorn and P. Cimiano. 2006. Cluster-
ing of Polysemic Words. In Advances in Data Analysis
- 30th Annual Conference of the German Classifica-
tion Society (GfKl). Berlin, Germany, March 8-10.
</reference>
<page confidence="0.989594">
46
</page>
<reference confidence="0.999886142857143">
G. Cleuziou, L. Martin, and C. Vrain. 2004. PoBOC:
an Overlapping Clustering Algorithm. Application to
Rule-Based Classication and Textual Data. In Pro-
ceedings of the 16th European Conference on Artifi-
cial Intelligence, pages 440-444, Spain, August 22-27.
G. Cleuziou, V. Clavier, L. Martin. 2003. Une M´ethode
de Regroupement de Mots Fond´ee sur la Recherche de
Cliques dans un Graphe de Cooccurrences. In Pro-
ceedings of Rencontres Terminologie et Intelligence
Artificielle, France. pages 179-182.
B. Daille. 1995. Study and Implementation of Combined
Techniques for Automatic Extraction of Terminology.
In The balancing act combining symbolic and statisti-
cal approaches to language. MIT Press.
G. Dias and E. Alves. 2005. Unsupervised Topic Seg-
mentation Based on Word Co-occurrence and Multi-
Word Units for Text Summarization. In Proceedings
of the ELECTRA Workshop associated to 28th ACM
SIGIR Conference, Salvador, Brazil, pages 41-48.
G. Dias, S. Guillor´e and J.G.P. Lopes. 1999. Language
Independent Automatic Acquisition of Rigid Multi-
word Units from Unrestricted Text Corpora. In Pro-
ceedings of 6th Annual Conference on Natural Lan-
guage Processing, Carg`ese, France, pages 333-339.
W. Doran, N. Stokes, J. Carthy and J. Dunnion. 2004.
Assessing the Impact of Lexical Chain Scoring Meth-
ods and Sentence Extraction Schemes on Summariza-
tion. In Proc. of the 5th Conference on Intelligent Text
Processing and Computational Linguistics.
V. Estivill-Castro, I. Lee, and A. T. Murray. 2001. Crite-
ria on Proximity Graphs for Boundary Extraction and
Spatial Clustering. In Proceedings of the 5th Pacific-
Asia Conference on Knowledge Discovery and Data
Mining, Springer-Verlag. pages 348-357.
C.D. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press, New York.
W. Gale, K. Church, and D. Yarowsky. 1992. One Sense
per Discourse. In Proceedings of the DARPA Speech
and Natural Language Workshop.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of 18th International Joint Conference on Ar-
tificial Intelligence (IJCAI’03), Acapulco, Mexico.
G. Hirst and D. St-Onge. 1997. Lexical Chains as Repre-
sentation of Context for the Detection and Correction
of Malapropisms. In WordNet: An electronic lexical
database and some of its applications. MIT Press.
J.W. Jaromczyk and G.T. Toussaint. 1992. Relative
Neighborhood Graphs and Their Relatives. P-IEEE,
80, pages 1502-1517.
J.J. Jiang and D.W. Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, Taiwan.
R. Krovetz. 1998. More than One Sense per Discourse.
NEC Princeton NJ Labs., Research Memorandum.
C. Leacock and M. Chodorow. 1998. Combining Local
Context and WordNet Similarity for Word Sense Iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database. MIT Press. pages 265-283.
D. Lin. 1998. An Information-theoretic Definition of
Similarity. In 15th International Conference on Ma-
chine Learning. Morgan Kaufmann, San Francisco.
G. Miller. 1995. WordNet: An Lexical Database for Eng-
lish. Communications of the Association for Comput-
ing Machinery (CACM), 38(11), pages 39-41.
J. Morris and G. Hirst. 1991. Lexical Cohesion Com-
puted by Thesaural Relations as an Indicator of the
Structure of Text. Computational Linguistics, 17(1).
P. Pantel and D. Lin. 2002. Discovering Word
Senses from Text. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. pages 613-619.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal. pages 448-453.
P.M. Roget. 1852. Roget’s Thesaurus of English Words
and Phrases. Harlow, Essex, England: Longman.
G. Salton, C.S. Yang and C.T. Yu. 1975. A Theory
of Term Importance in Automatic Text Analysis. In
American Society of Information Science, 26(1).
G. Silber and K. McCoy. 2002. Efficiently Computed
Lexical Chains as an Intermediate Representation for
Automatic Text Summarization. Computational Lin-
guistics, 28(4), pages 487-496.
J. Silva, G. Dias, S. Guillor´e and J.G.P. Lopes. 1999. Us-
ing LocalMaxs Algorithm for the Extraction of Con-
tiguous and Non-contiguous Multiword Lexical Units.
In Proceedings of 9th Portuguese Conference in Arti-
ficial Intelligence. Springer-Verlag.
P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxon-
omy - The Principles and Practice of Numerical Clas-
sification. San Francisco, Freeman and Co.
E. Teich and P. Fankhauser. 2004. Exploring Lexical Pat-
terns in Text: Lexical Cohesion Analysis with Word-
Net. In Proceedings of the 2nd International Wordnet
Conference, Brno, Czech Republic. pages 326-331.
</reference>
<page confidence="0.999492">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.215742">
<title confidence="0.992972">Automatic Knowledge Representation using a Graph-based Algorithm Language-Independent Lexical Chaining</title>
<author confidence="0.999229">Ga¨el_Dias</author>
<affiliation confidence="0.9938215">HULTIG University of Beira</affiliation>
<email confidence="0.947778">ddg@di.ubi.pt</email>
<affiliation confidence="0.863407">Cl´audia University of Beira</affiliation>
<email confidence="0.969295">claudia@dmnet.ubi.pt</email>
<author confidence="0.906815">Guillaume</author>
<affiliation confidence="0.999834">University of</affiliation>
<address confidence="0.524127">Orl´eans,</address>
<email confidence="0.997147">cleuziou@univ-orleans.fr</email>
<abstract confidence="0.983156611111111">Lexical Chains are powerful representations of documents. In particular, they have successfully been used in the field of Automatic Text Summarization. However, until now, Lexical Chaining algorithms have only been proposed for English. In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts. For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm. As a consequence, our methodology can be applied to any language and proposes a solution to languagedependent Lexical Chainers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>Proceedings of the Intelligent Scalable Text Summarization Workshop (ISTS97), ACL,</booktitle>
<pages>10--18</pages>
<location>Madrid,</location>
<contexts>
<context position="1269" citStr="Barzilay and Elhadad, 1997" startWordPosition="172" endWordPosition="175">greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts. For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm. As a consequence, our methodology can be applied to any language and proposes a solution to languagedependent Lexical Chainers. 1 Introduction Lexical Chains are powerful representations of documents compared to broadly used bag-of-words representations. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). However, until now, Lexical Chaining algorithms have only been proposed for English as they rely on linguistic resources such as Thesauri (Morris and Hirst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003). Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. However, at the time of writing their paper, no machine-readable thesaurus was available so they manually generated Lexical Chains using Roget’s Thesaurus (Roget, 1852). A first computationa</context>
<context position="2536" citStr="Barzilay and Elhadad (1997)" startWordPosition="372" endWordPosition="375">by Hirst and St-Onge (1997). Their biggest contribution to the study of Lexical Chains is the mapping of WordNet (Miller, 1995) relations and paths (transitive relationships) to (Morris and Hirst, 1991) word relationship types. However, their greedy algorithm does not use a part-of-speech tagger. Instead, the algorithm only selects those words that contain noun entries in WordNet to compute Lexical Chains. But, as Barzilay and Elhadad (1997) point at, the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet. So, Barzilay and Elhadad (1997) propose the first dynamic method to compute Lexical Chains. They argue that the most appropriate sense of a word can only be chosen after examining all possible Lexical Chain combinations that can be generated from a text. Because all possible senses of the word are not taken into account, except at the time of insertion, potentially pertinent context information that is likely to appear after the word is lost. However, this method of retaining all possible interpretations until the end of the process, causes the exponential growth of the time and space complexity. As a consequence, Silber an</context>
<context position="3868" citStr="Barzilay and Elhadad, 1997" startWordPosition="577" endWordPosition="580">In particular, (Silber and McCoy, 2002)’s implementation creates a structure, called meta-chains, that implicitly stores 36 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 36–47, Sydney, July 2006. c�2006 Association for Computational Linguistics all chain interpretations without actually creating them, thus keeping both the space and time usage of the program linear. Finally, Galley and McKeown (2003) propose a chaining method that disambiguates nouns prior to the processing of Lexical Chains. Their evaluation shows that their algorithm is more accurate than (Barzilay and Elhadad, 1997) and (Silber and McCoy, 2002) ones. One common point of all these works is that Lexical Chains are built using WordNet as the standard linguistic resource. Unfortunately, systems based on static linguistic knowledge bases are limited. First, such resources are difficult to find. Second, they are largely obsolete by the time they are available. Third, linguistic resources capture a particular form of lexical knowledge which is often very different from the sort needed to specifically relate words or sentences. In particular, WordNet is missing a lot of explicit links between intuitively related</context>
<context position="5448" citStr="Barzilay and Elhadad, 1997" startWordPosition="826" endWordPosition="829">e base with the purpose to identify cohesive lexical relationships between words based on corpus evidence. This hierarchical lexico-semantic knowledge base is built by using the Pole-Based Overlapping Clustering Algorithm (Cleuziou et al., 2004) that clusters words with similar meanings and allows words with multiple meanings to belong to different clusters. The second step of the process aims at automatically extracting Lexical Chains from texts based on our knowledge base. For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the “one-word/one-concept per document” paradigm (Gale et al., 1992)1. In particular, it imple1This characteristic can be interesting for multi-topic documents like web news stories. Indeed, in this case, there may be different topics in the same document as different news stories may appear. In some way, it follows the idea of (Krovetz, 1998). ments (Lin, 1998) information-theoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical Chains2. 2 Building a Similarity </context>
<context position="24682" citStr="Barzilay and Elhadad, 1997" startWordPosition="4163" endWordPosition="4166">ke for usual agglomerative approaches (Sneath and Sokal, 1973). In this process, the similarity between two clusters is obtained by the average-link (or complete-link) method: ❳ s(Ip, Iq) = |Ip|.|Iq| ❳ 1 x��I� To deal with overlapping clusters we considere in Equation 11 the similarity between an object and itself to be equal to 1 : s(xi7 xi) = 1. 4 Lexical Chaining Algorithm Once the lexico-semantic knowledge base has been built, it is possible to use it for Lexical Chaining. In this section, we propose a new greedy algorithm which can be seen as an extension of (Hirst and StOnge, 1997) and (Barzilay and Elhadad, 1997) algorithms as it allows polysemous words to belong to different chains thus breaking the “one-word/oneconcept per document” paradigm (Gale et al., 1992). Indeed, multi-topic documents like web news stories may introduce different topics in the same document/url and do not respect the “one sense per discourse” paradigm. As we want to deal with realworld applications, this characteristic may show interesting results for the specific task of Text Summarization for Web documents. Indeed, comparatively to the experiments made by (Gale et al., 1992) that deal with “well written discourse”, web docu</context>
<context position="31446" citStr="Barzilay and Elhadad, 1997" startWordPosition="5318" endWordPosition="5321">ce, the word trouble is inserted in the Lexical Chain with the appropriate sense (C29) as it maximizes the overall similarity of the chain and the chain members senses are updated. In this example, the interpretation with (C32) is discarded as is the cluster (C34) for recession. This processing is described in Figure 6. Figure 6: Selection of appropriate senses. 4.2.4 Score of a chain Once all chains have been computed, only the high-scoring ones must be picked up as representing the important concepts of the original document. Therefore, one must first identify the strongest chains. Like in (Barzilay and Elhadad, 1997), we define a chain score which is defined in Equation 16 where |chain |is the number of words in the chain. As all chains will be scored, the ones with higher scores will be extracted. Of course, a threshold will have to be defined by the user. In the next section, we will show some qualitative and quantitative results of our architecture. 5 Evaluation The evaluation of Lexical Chains is generally difficult. Even if they can be effectively used in many practical applications, Lexical Chains are seldom desirable outputs in a real-world application, and it is unclear how to assess their quality</context>
<context position="32825" citStr="Barzilay and Elhadad, 1997" startWordPosition="5551" endWordPosition="5555">whether a good or bad performance comes from the efficiency of the lexical chaining algorithm or from the appropriateness of using Lexical Chains in that kind of application. It is also true that some work has been done in this direction (Budanitsky and Hirst, 2006) by collecting Human Lexical Chains to compare against automatically built Lexical Chains. However, this type of evaluation is logistically impossible to perform as we aim at developing a system that does not depend on any language or topic. So, in this section, we will only present some results generated by our architecture (like (Barzilay and Elhadad, 1997; Teich and Fankhauser, 2004) do), although we acknowledge that other comparative evaluations (with WordNet, with Human Lexical Chains or within independent applications like Text Summarization) must be done in order to draw definitive conclusions. We have generated four taxonomies from four different domains (Sport, Economy, Politics and War) from a set of documents of the DUC 200415. Moreover, we have extracted Lexical Chains for all four 15http://duc.nist.gov/duc2004/ score(chain) = (jchainj − 1)jchainj 2 jchainj−1 jchainj simLin(Ci, C;) ❳ ❳ i=0 7=i+1 (16) 44 domains to show the ability of </context>
<context position="35124" citStr="Barzilay and Elhadad, 1997" startWordPosition="5966" endWordPosition="5969">inal units. c=5 c=6 c=7 c=8 Doc 1 27 43 73 73 Doc 2 31 52 81 83 Doc 3 28 40 51 51 Doc 4 29 53 83 87 Table 2: # Lexical Chains per Document The second interesting conclusion is that our algorithm does not gather words that belong to only one cluster and take advantage of the automatically built lexico-semantic knowledge base. This is illustrated in Table 3. However, it is obvious that by increasing the constant c the words in a chain tend to belong to only one cluster as it is the case for most of the best Lexical Chains with c = 8. 5.2 Qualitative Evaluation In this section, as it is done in (Barzilay and Elhadad, 1997; Teich and Fankhauser, 2004), we present the c=5 c=6 c=7 c=8 Doc 1 19 13 7 7 Doc 2 13 6 3 3 Doc 3 3 4 4 4 Doc 4 6 4 3 3 Table 3: # Clusters per Lexical Chain five highest-scoring chains for the best threshold that we experimentally evaluated to be c = 7 for each domain (See Tables 4, 5, 6, 7). It is clear that the obtained Lexical Chains show a desirable degree of representativeness of the text in analysis. Domain=Sport, Document=3, c=7 - #0, 1 cluster and score=1.0: {United States, couple, competition} - #6, 3 clusters and score=1.0: {boats, Sunday night, sailor, Sword, Orion, veteran, cutte</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>R. Barzilay and M. Elhadad. 1997. Using Lexical Chains for Text Summarization. Proceedings of the Intelligent Scalable Text Summarization Workshop (ISTS97), ACL, Madrid, Spain, pages 10-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bomze</author>
<author>M Budinich</author>
<author>P Pardalos</author>
<author>M Pelillo</author>
</authors>
<title>The Maximum Clique Problem.</title>
<date>1999</date>
<journal>Handbook of Combinatorial Optimization,</journal>
<volume>4</volume>
<publisher>Kluwer Academic publishers,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="22311" citStr="Bomze et al., 1999" startWordPosition="3738" endWordPosition="3741">ks Equation 9. x =arg min s(xk, X) (9) xkEX 3.2 Discovery of Poles The graph representation helps to discover a set of fully-connected subgraphs (cliques) highly separated, denoted as Poles. Because G3(V, E) is built such that two vertices xi and xj are connected if and only if they are similar11, a clique has the required properties to be a good cluster. Indeed, such a cluster guarantees that all its constituents are similar. The search of maximal cliques in a graph is an NP-complete problem. As a consequence, heuristics are used in order to (1) build a great clique around a starting vertex (Bomze et al., 1999) and (2) choose the starting vertices in such a way cliques are as distant as possible. Given a starting vertex x, the first heuristic consists in adding iteratively the vertex xi which satisfies the following conditions: 3.3 Multi-Assignment Once the Poles are built, the Pole-Based Overlapping Clustering algorithm uses them as clusters representatives. Membership functions m(.,.) are defined in order to assign each object to its nearest Poles as shown in Equation 10. ∀xi ∈ X, Pj ∈ {P1, ... , Pm} : m(xi, Pj) = s(xi, Pj) (10) For each object xi to assign, the set of poles is ordered (P1(xi), ..</context>
</contexts>
<marker>Bomze, Budinich, Pardalos, Pelillo, 1999</marker>
<rawString>I. Bomze, M. Budinich, P. Pardalos, and M. Pelillo. 1999. The Maximum Clique Problem. Handbook of Combinatorial Optimization, volume 4. Kluwer Academic publishers, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT - a Statistical Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied NLP Conference, ANLP-2000.</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="6709" citStr="Brants, 2000" startWordPosition="1029" endWordPosition="1030">wledge base, the Pole-Based Overlapping Clustering Algorithm needs as input a similarity matrix that gathers the similarities between all the words in the corpus. For that purpose, we propose a contextual analysis of each nominal unit (nouns and compound nouns) in the corpus. In particular, each nominal unit is associated to a word context vector and the similarity between nominal units is calculated by the informative similarity measure proposed by (Dias and Alves, 2005). 2.1 Data Preparation The context corpus is first pre-processed in order to extract nominal units from it. The TnT tagger (Brants, 2000) is first applied to our context corpus to morpho-syntactically mark all the words in it. Once all words have been morpho-syntactically tagged, we apply the statistically-based multiword unit extractor SENTA (Dias et al., 1999) that extracts multiword units based on any input text3. For example, multiword units are compound nouns (free kick), compound determinants (an amount of), verbal locutions (to put forward), adjectival locutions (dark blue) or institutionalized phrases (con carne). Finally, we use a set of well-known heuristics (Daille, 1995) to retrieve compound nouns using the idea tha</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT - a Statistical Part-of-Speech Tagger. In Proceedings of the 6th Applied NLP Conference, ANLP-2000. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>32</volume>
<issue>1</issue>
<pages>13--47</pages>
<contexts>
<context position="32142" citStr="Budanitsky and Hirst, 2006" startWordPosition="5437" endWordPosition="5440">n |is the number of words in the chain. As all chains will be scored, the ones with higher scores will be extracted. Of course, a threshold will have to be defined by the user. In the next section, we will show some qualitative and quantitative results of our architecture. 5 Evaluation The evaluation of Lexical Chains is generally difficult. Even if they can be effectively used in many practical applications, Lexical Chains are seldom desirable outputs in a real-world application, and it is unclear how to assess their quality independently of the underlying application in which they are used (Budanitsky and Hirst, 2006). For example, in Summarization, it is hard to determine whether a good or bad performance comes from the efficiency of the lexical chaining algorithm or from the appropriateness of using Lexical Chains in that kind of application. It is also true that some work has been done in this direction (Budanitsky and Hirst, 2006) by collecting Human Lexical Chains to compare against automatically built Lexical Chains. However, this type of evaluation is logistically impossible to perform as we aim at developing a system that does not depend on any language or topic. So, in this section, we will only p</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased Measures of Lexical Semantic Relatedness. In Computational Linguistics, 32(1). pages: 13-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cicurel</author>
<author>S Bloehdorn</author>
<author>P Cimiano</author>
</authors>
<title>Clustering of Polysemic Words.</title>
<date>2006</date>
<booktitle>In Advances in Data Analysis - 30th Annual Conference of the German Classification Society (GfKl).</booktitle>
<location>Berlin, Germany,</location>
<contexts>
<context position="16831" citStr="Cicurel et al., 2006" startWordPosition="2750" endWordPosition="2753">analysis research fields. Few of them propose overlapping clusters as output, in spite of the interest it represents for domains of application ❳p l=1 Aij = ❳p k=1 �i✈ Vi, Bi = ❳p l=1 ❳p k=1 39 such as Natural Language Processing or Bioinformatics. PoBOC (Pole-Based Overlapping Clustering) (Cleuziou et al., 2004) and CBC (Clustering By Committees) (Pantel and Lin, 2002) are two clustering algorithms suitable for the word clustering task. They both proceed by first constructing tight clusters9 and then assigning residual objects to their most similar tight clusters. A recent comparative study (Cicurel et al., 2006) shows that CBC and PoBOC both lead to relevant results for the task of word clustering. Nevertheless CBC requires parameters hard to tune whereas PoBOC is free of any parametrization. The last argument encouraged us to use the PoBOC algorithm. Unlike most of commonly used clustering algorithms, the Pole-Based Overlapping Clustering Algorithm shows the following advantages among others : (1) it requires no parameters i.e. input is restricted to a single similarity matrix, (2) the number of final clusters is automatically found and (3) it provides overlapping clusters allowing to take into acco</context>
</contexts>
<marker>Cicurel, Bloehdorn, Cimiano, 2006</marker>
<rawString>L. Cicurel, S. Bloehdorn and P. Cimiano. 2006. Clustering of Polysemic Words. In Advances in Data Analysis - 30th Annual Conference of the German Classification Society (GfKl). Berlin, Germany, March 8-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cleuziou</author>
<author>L Martin</author>
<author>C Vrain</author>
</authors>
<title>PoBOC: an Overlapping Clustering Algorithm. Application to Rule-Based Classication and Textual Data.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th European Conference on Artificial Intelligence,</booktitle>
<pages>440--444</pages>
<contexts>
<context position="5066" citStr="Cleuziou et al., 2004" startWordPosition="762" endWordPosition="765">ween intuitively related words. Fellbaum (1998) refers to such obvious omissions in WordNet as the “tennis problem” where nouns such as nets, rackets and umpires are all present, but WordNet provides no links between these related tennis concepts. In order to solve these problems, we propose to automatically construct from a collection of documents a lexico-semantic knowledge base with the purpose to identify cohesive lexical relationships between words based on corpus evidence. This hierarchical lexico-semantic knowledge base is built by using the Pole-Based Overlapping Clustering Algorithm (Cleuziou et al., 2004) that clusters words with similar meanings and allows words with multiple meanings to belong to different clusters. The second step of the process aims at automatically extracting Lexical Chains from texts based on our knowledge base. For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the “one-word/one-concept per document” paradigm (Gale et al., 1992)1. In particular, it imple1This characteristic can be interestin</context>
<context position="15655" citStr="Cleuziou et al., 2004" startWordPosition="2566" endWordPosition="2569"> cohesiveness existing between those two words calculated by the Symmetric Conditional Probability measure. For example, Real Madrid striker would give rise to the sum of 6 products i.e. Real Madrid striker with Ronaldo, Real Madrid striker with defeated and so on and so forth. As a consequence, sentence (1) and (2) would show a high similarity as Real Madrid striker is highly related to Ronaldo. Once the similarity matrix is built based on the infoSimBA between all word context vectors of all nominal units in the corpus, we give it as input to the Pole-Based Overlapping Clustering Algorithm (Cleuziou et al., 2004) to build a hierarchy of concepts i.e. our lexico-semantic knowledge base. 3 Hierarchy of Concepts Clustering is the task that structures units in such a way it reflects the semantic relations existing between them. In our framework nominal units are first grouped into overlapping clusters (or soft-clusters) such that final clusters correspond to conceptual classes (called “concepts” in the following). Then, concepts are hierarchically structured in order to capture semantic links between them. Many clustering methods have been proposed in the data analysis research fields. Few of them propose</context>
<context position="40559" citStr="Cleuziou et al., 2004" startWordPosition="6775" endWordPosition="6778">ubtopics of the text being analyzed. As a consequence, we deeply believe that Lexical Chains must be evaluated in the context of Natural Language Processing applications (such as Text Summarization (Doran et al., 2004)), as comparing Lexical Chains as they are is a very difficult task to tackle which may even lead to inconclusive results. 6 Conclusions and Future Work In this paper, we implemented a greedy LanguageIndependent algorithm for building Lexical Chains. For that purpose, we first constructed a lexicosemantic knowledge base by applying the PoleBased Overlapping Clustering algorithm (Cleuziou et al., 2004) to word-context vectors obtained by the application of the SCP(.,.) measure (Silva et al., 1999) and the InfoSimBA(., .) (Dias and Alves, 2005) similarity measure. In a second step, we implemented (Lin, 1998)’s similarity measure and used it to define the relatedness criterion in order to assign a given word to a given chain in the lexical chaining process. Finally, our experimental evaluation shows that relevant Lexical Chains can be constructed with our lexical chaining algorithm, although we acknowledge that more comparative evaluations must be done in order to draw definitive conclusions.</context>
</contexts>
<marker>Cleuziou, Martin, Vrain, 2004</marker>
<rawString>G. Cleuziou, L. Martin, and C. Vrain. 2004. PoBOC: an Overlapping Clustering Algorithm. Application to Rule-Based Classication and Textual Data. In Proceedings of the 16th European Conference on Artificial Intelligence, pages 440-444, Spain, August 22-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cleuziou</author>
<author>V Clavier</author>
<author>L Martin</author>
</authors>
<title>Une M´ethode de Regroupement de Mots Fond´ee sur la Recherche de Cliques dans un Graphe de Cooccurrences.</title>
<date>2003</date>
<booktitle>In Proceedings of Rencontres Terminologie et Intelligence Artificielle,</booktitle>
<pages>179--182</pages>
<contexts>
<context position="12360" citStr="Cleuziou et al., 2003" startWordPosition="1980" endWordPosition="1983">of a word within a document. So, very disperse words will not be as relevant as dense words. This density measure dens(.,.) is defined in Equation 3. 1 ln(dist(o(w,k), o(w,k+1)) + e) (3) For any given word w, its density dens(w, d) is calculated from all the distances between all its occurrences in document d, tf(w, d). So, dist(o(w,k),o(w,k+1)) calculates the distance that separates two consecutive occurrences of w in terms of words within the document. In particular, e is the obtained by the Symmetric Conditional Probability measure compared to the Pointwise Mutual Information for instance (Cleuziou et al., 2003) tf(w,d)−1 dens(w, d) = ❳ k=1 38 base of the natural logarithm so that ln(e) = 1. This argument is included into Equation 3 as it will give a density value of 1 for any word that only occurs once in the document. In fact, we give this word a high density value. final weight: The weighting score weight(w) of any word w in the corpus can be directly derived from the previous two heuristics. This score is defined in Equation 4 where tf and dens are respectively the average of tf(., .) and dens(.,.) over all the documents in which the word w occurs i.e. Nw. weight(w) = tf.idf(w) x dens(w) (4) wher</context>
</contexts>
<marker>Cleuziou, Clavier, Martin, 2003</marker>
<rawString>G. Cleuziou, V. Clavier, L. Martin. 2003. Une M´ethode de Regroupement de Mots Fond´ee sur la Recherche de Cliques dans un Graphe de Cooccurrences. In Proceedings of Rencontres Terminologie et Intelligence Artificielle, France. pages 179-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>Study and Implementation of Combined Techniques for Automatic Extraction of Terminology. In The balancing act combining symbolic and statistical approaches to language.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7263" citStr="Daille, 1995" startWordPosition="1113" endWordPosition="1114">xtract nominal units from it. The TnT tagger (Brants, 2000) is first applied to our context corpus to morpho-syntactically mark all the words in it. Once all words have been morpho-syntactically tagged, we apply the statistically-based multiword unit extractor SENTA (Dias et al., 1999) that extracts multiword units based on any input text3. For example, multiword units are compound nouns (free kick), compound determinants (an amount of), verbal locutions (to put forward), adjectival locutions (dark blue) or institutionalized phrases (con carne). Finally, we use a set of well-known heuristics (Daille, 1995) to retrieve compound nouns using the idea that groups of words that correspond to a priori defined syntactical patterns such as Adj+Noun, Noun+Noun, Noun+Prep+Noun can be identified as compound nouns. Indeed, nouns usually convey most of the information in a written text. They are the main contributors to the “aboutness” of a text. For example, free kick, city hall, operating system are compound nouns which sense is not compositional i.e. the sense of the multiword unit can 2Of course, other similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodorow, 1998) could be imple</context>
</contexts>
<marker>Daille, 1995</marker>
<rawString>B. Daille. 1995. Study and Implementation of Combined Techniques for Automatic Extraction of Terminology. In The balancing act combining symbolic and statistical approaches to language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dias</author>
<author>E Alves</author>
</authors>
<title>Unsupervised Topic Segmentation Based on Word Co-occurrence and MultiWord Units for Text Summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the ELECTRA Workshop associated to 28th ACM SIGIR Conference,</booktitle>
<pages>41--48</pages>
<location>Salvador, Brazil,</location>
<contexts>
<context position="6572" citStr="Dias and Alves, 2005" startWordPosition="1005" endWordPosition="1008">e relatedness criterion for the attribution of words to Lexical Chains2. 2 Building a Similarity Matrix In order to build the lexico-semantic knowledge base, the Pole-Based Overlapping Clustering Algorithm needs as input a similarity matrix that gathers the similarities between all the words in the corpus. For that purpose, we propose a contextual analysis of each nominal unit (nouns and compound nouns) in the corpus. In particular, each nominal unit is associated to a word context vector and the similarity between nominal units is calculated by the informative similarity measure proposed by (Dias and Alves, 2005). 2.1 Data Preparation The context corpus is first pre-processed in order to extract nominal units from it. The TnT tagger (Brants, 2000) is first applied to our context corpus to morpho-syntactically mark all the words in it. Once all words have been morpho-syntactically tagged, we apply the statistically-based multiword unit extractor SENTA (Dias et al., 1999) that extracts multiword units based on any input text3. For example, multiword units are compound nouns (free kick), compound determinants (an amount of), verbal locutions (to put forward), adjectival locutions (dark blue) or instituti</context>
<context position="11110" citStr="Dias and Alves, 2005" startWordPosition="1756" endWordPosition="1759">loseness of the subject content. Thus, nominal units that are used in a similar local context will have vectors that are relatively close to each other. However, in order to define similarities between vectors, we must transform each word context vector into a high dimensional vector consisting of real-valued components. As a consequence, each co-occurring word of the word context vector is associated to a weight which evaluates its importance in the corpus. 2.3.1 Weighting score The weighting score of any word in a document can be directly derived from an adaptation of the score proposed in (Dias and Alves, 2005). In particular, we consider the combination of two main heuristics: the well-known tf.idf measure proposed by (Salton et al., 1975) and a new density measure (Dias and Alves, 2005). tf.idf: Given a word w and a document d, the tf.idf(w, d) is defined in Equation 2 where tf(w, d) is the number of occurrences of w in d, |d |corresponds to the number of words in d, N is the number of documents in the corpus and df(w) stands for the number of documents in the corpus in which the word w occurs. tf.idf (w, d) = tf (|S|, d) x log2 ✒N df (w)✓ (2) density: The basic idea of the word density measure is</context>
<context position="13596" citStr="Dias and Alves, 2005" startWordPosition="2210" endWordPosition="2213">,d) and dens(w) = Y-d dens(w,d) Nw Nw 2.3.2 Informative Similarity Measure The next step aims at determining the similarity between all nominal units. Theoretically, a similarity measure can be defined as follows. Suppose that Xi = (Xi1, Xie, Xi3, , Xip) is a row vector of observations on p variables associated with a label i. The similarity between two words i and j is defined as Sij = f(Xi, Xj) where f is some function of the observed values. In the context of our work, Xi and Xj are 10-dimension word context vectors. In order to avoid the lexical repetition problem of similarity measures, (Dias and Alves, 2005) have proposed an informative similarity measure called infoSimBA, which basic idea is to integrate into the Cosine measure, the word co-occurrence factor inferred from a collection of documents with the Symmetric Conditional Probability (Silva et al., 1999). See Equation 5. Aij InfoSimBA(Xi, Xj) = (5) Bi x Bj + Aij where Xik x Xjl x SCP(wik, wjl) Xik x Xil x SCP (wik, wil) and any Xzv corresponds to the word weighting factor weight(wzv), SCP(wik, wjl) is the Symmetric Conditional Probability value between wik, the word that indexes the word context vector i at position k and wjl, the word tha</context>
<context position="40703" citStr="Dias and Alves, 2005" startWordPosition="6798" endWordPosition="6801"> Processing applications (such as Text Summarization (Doran et al., 2004)), as comparing Lexical Chains as they are is a very difficult task to tackle which may even lead to inconclusive results. 6 Conclusions and Future Work In this paper, we implemented a greedy LanguageIndependent algorithm for building Lexical Chains. For that purpose, we first constructed a lexicosemantic knowledge base by applying the PoleBased Overlapping Clustering algorithm (Cleuziou et al., 2004) to word-context vectors obtained by the application of the SCP(.,.) measure (Silva et al., 1999) and the InfoSimBA(., .) (Dias and Alves, 2005) similarity measure. In a second step, we implemented (Lin, 1998)’s similarity measure and used it to define the relatedness criterion in order to assign a given word to a given chain in the lexical chaining process. Finally, our experimental evaluation shows that relevant Lexical Chains can be constructed with our lexical chaining algorithm, although we acknowledge that more comparative evaluations must be done in order to draw definitive conclusions. In particular, in future work, we want to compare our methodology using WordNet as the basic knowledge base, implement different similarity mea</context>
</contexts>
<marker>Dias, Alves, 2005</marker>
<rawString>G. Dias and E. Alves. 2005. Unsupervised Topic Segmentation Based on Word Co-occurrence and MultiWord Units for Text Summarization. In Proceedings of the ELECTRA Workshop associated to 28th ACM SIGIR Conference, Salvador, Brazil, pages 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dias</author>
<author>S Guillor´e</author>
<author>J G P Lopes</author>
</authors>
<title>Language Independent Automatic Acquisition of Rigid Multiword Units from Unrestricted Text Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of 6th Annual Conference on Natural Language Processing,</booktitle>
<pages>333--339</pages>
<location>Carg`ese, France,</location>
<marker>Dias, Guillor´e, Lopes, 1999</marker>
<rawString>G. Dias, S. Guillor´e and J.G.P. Lopes. 1999. Language Independent Automatic Acquisition of Rigid Multiword Units from Unrestricted Text Corpora. In Proceedings of 6th Annual Conference on Natural Language Processing, Carg`ese, France, pages 333-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Doran</author>
<author>N Stokes</author>
<author>J Carthy</author>
<author>J Dunnion</author>
</authors>
<title>Assessing the Impact of Lexical Chain Scoring Methods and Sentence Extraction Schemes on Summarization.</title>
<date>2004</date>
<booktitle>In Proc. of the 5th Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="40155" citStr="Doran et al., 2004" startWordPosition="6711" endWordPosition="6714">units within the Lexical Chain, this one can not be understood as it is without context. In fact, it was related to problems of car firing that have been occurring in the past few weeks and provoked security problems in the town. Although some Lexical Chains are understandable as they are, most of them must be replaced in their context to fully understand their representativeness of the topics or subtopics of the text being analyzed. As a consequence, we deeply believe that Lexical Chains must be evaluated in the context of Natural Language Processing applications (such as Text Summarization (Doran et al., 2004)), as comparing Lexical Chains as they are is a very difficult task to tackle which may even lead to inconclusive results. 6 Conclusions and Future Work In this paper, we implemented a greedy LanguageIndependent algorithm for building Lexical Chains. For that purpose, we first constructed a lexicosemantic knowledge base by applying the PoleBased Overlapping Clustering algorithm (Cleuziou et al., 2004) to word-context vectors obtained by the application of the SCP(.,.) measure (Silva et al., 1999) and the InfoSimBA(., .) (Dias and Alves, 2005) similarity measure. In a second step, we implemente</context>
</contexts>
<marker>Doran, Stokes, Carthy, Dunnion, 2004</marker>
<rawString>W. Doran, N. Stokes, J. Carthy and J. Dunnion. 2004. Assessing the Impact of Lexical Chain Scoring Methods and Sentence Extraction Schemes on Summarization. In Proc. of the 5th Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Estivill-Castro</author>
<author>I Lee</author>
<author>A T Murray</author>
</authors>
<title>Criteria on Proximity Graphs for Boundary Extraction and Spatial Clustering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 5th PacificAsia Conference on Knowledge Discovery and Data Mining, Springer-Verlag.</booktitle>
<pages>348--357</pages>
<contexts>
<context position="17927" citStr="Estivill-Castro et al., 2001" startWordPosition="2920" endWordPosition="2923">y matrix, (2) the number of final clusters is automatically found and (3) it provides overlapping clusters allowing to take into account the different possible meanings of lexical units. 3.1 A Graph-based Approach The Pole-Based Overlapping Clustering Algorithm is based on a graph-theoretical framework. Graph formalism is often used in the context of clustering (graph-clustering). It first consists in defining a graph structure which illustrates the data (vertices) with links (edges) between them and then in proposing a graph-partitioning process. Numerous graph structures have been proposed (Estivill-Castro et al., 2001). They all consider the data set as set of vertices but differ on the way to decide that two vertices are connected. Some methodologies are listed below where V is the set of vertices, E the set of edges, G(V, E) a graph and d a distance measure: • Nearest Neighbor Graph (NNG) : each vertex is connected to its nearest neighbor, • Minimum Spanning Tree (MST) : `d(xi, xj) E V x V a path exists between xi and xj in G with Y_(x xj)EE d(xi, xj) minimized, 9The tight clusters are called “committees” in CBC and “poles” in PoBOC. • Relative Neighborhood Graph (RNG) : xi and xj are connected iff `dxk E</context>
</contexts>
<marker>Estivill-Castro, Lee, Murray, 2001</marker>
<rawString>V. Estivill-Castro, I. Lee, and A. T. Murray. 2001. Criteria on Proximity Graphs for Boundary Extraction and Spatial Clustering. In Proceedings of the 5th PacificAsia Conference on Knowledge Discovery and Data Mining, Springer-Verlag. pages 348-357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>New York.</location>
<contexts>
<context position="4491" citStr="Fellbaum (1998)" startWordPosition="676" endWordPosition="677">lber and McCoy, 2002) ones. One common point of all these works is that Lexical Chains are built using WordNet as the standard linguistic resource. Unfortunately, systems based on static linguistic knowledge bases are limited. First, such resources are difficult to find. Second, they are largely obsolete by the time they are available. Third, linguistic resources capture a particular form of lexical knowledge which is often very different from the sort needed to specifically relate words or sentences. In particular, WordNet is missing a lot of explicit links between intuitively related words. Fellbaum (1998) refers to such obvious omissions in WordNet as the “tennis problem” where nouns such as nets, rackets and umpires are all present, but WordNet provides no links between these related tennis concepts. In order to solve these problems, we propose to automatically construct from a collection of documents a lexico-semantic knowledge base with the purpose to identify cohesive lexical relationships between words based on corpus evidence. This hierarchical lexico-semantic knowledge base is built by using the Pole-Based Overlapping Clustering Algorithm (Cleuziou et al., 2004) that clusters words with</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C.D. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One Sense per Discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="5602" citStr="Gale et al., 1992" startWordPosition="850" endWordPosition="853">t by using the Pole-Based Overlapping Clustering Algorithm (Cleuziou et al., 2004) that clusters words with similar meanings and allows words with multiple meanings to belong to different clusters. The second step of the process aims at automatically extracting Lexical Chains from texts based on our knowledge base. For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the “one-word/one-concept per document” paradigm (Gale et al., 1992)1. In particular, it imple1This characteristic can be interesting for multi-topic documents like web news stories. Indeed, in this case, there may be different topics in the same document as different news stories may appear. In some way, it follows the idea of (Krovetz, 1998). ments (Lin, 1998) information-theoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical Chains2. 2 Building a Similarity Matrix In order to build the lexico-semantic knowledge base, the Pole-Based Overlapping Clustering Algorithm needs as input a similarity matrix that gathe</context>
<context position="24835" citStr="Gale et al., 1992" startWordPosition="4187" endWordPosition="4190">link) method: ❳ s(Ip, Iq) = |Ip|.|Iq| ❳ 1 x��I� To deal with overlapping clusters we considere in Equation 11 the similarity between an object and itself to be equal to 1 : s(xi7 xi) = 1. 4 Lexical Chaining Algorithm Once the lexico-semantic knowledge base has been built, it is possible to use it for Lexical Chaining. In this section, we propose a new greedy algorithm which can be seen as an extension of (Hirst and StOnge, 1997) and (Barzilay and Elhadad, 1997) algorithms as it allows polysemous words to belong to different chains thus breaking the “one-word/oneconcept per document” paradigm (Gale et al., 1992). Indeed, multi-topic documents like web news stories may introduce different topics in the same document/url and do not respect the “one sense per discourse” paradigm. As we want to deal with realworld applications, this characteristic may show interesting results for the specific task of Text Summarization for Web documents. Indeed, comparatively to the experiments made by (Gale et al., 1992) that deal with “well written discourse”, web documents show unusual discourse structures. In some way, our algorithm follows the idea of (Krovetz, 1998). Finally, it implements (Lin, 1998)’s information</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. One Sense per Discourse. In Proceedings of the DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Improving Word Sense Disambiguation in Lexical Chaining.</title>
<date>2003</date>
<booktitle>In Proceedings of 18th International Joint Conference on Artificial Intelligence (IJCAI’03),</booktitle>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="1551" citStr="Galley and McKeown, 2003" startWordPosition="217" endWordPosition="220"> can be applied to any language and proposes a solution to languagedependent Lexical Chainers. 1 Introduction Lexical Chains are powerful representations of documents compared to broadly used bag-of-words representations. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). However, until now, Lexical Chaining algorithms have only been proposed for English as they rely on linguistic resources such as Thesauri (Morris and Hirst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003). Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. However, at the time of writing their paper, no machine-readable thesaurus was available so they manually generated Lexical Chains using Roget’s Thesaurus (Roget, 1852). A first computational model of Lexical Chains is introduced by Hirst and St-Onge (1997). Their biggest contribution to the study of Lexical Chains is the mapping of WordNet (Miller, 1995) relations and paths (transitive relationships) to (Morris and Hirst, 1991) word relationship types. However, their</context>
<context position="3679" citStr="Galley and McKeown (2003)" startWordPosition="549" endWordPosition="552">e exponential growth of the time and space complexity. As a consequence, Silber and McCoy (2002) propose a linear time version of (Barzilay and Elhadad, 1997) lexical chaining algorithm. In particular, (Silber and McCoy, 2002)’s implementation creates a structure, called meta-chains, that implicitly stores 36 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 36–47, Sydney, July 2006. c�2006 Association for Computational Linguistics all chain interpretations without actually creating them, thus keeping both the space and time usage of the program linear. Finally, Galley and McKeown (2003) propose a chaining method that disambiguates nouns prior to the processing of Lexical Chains. Their evaluation shows that their algorithm is more accurate than (Barzilay and Elhadad, 1997) and (Silber and McCoy, 2002) ones. One common point of all these works is that Lexical Chains are built using WordNet as the standard linguistic resource. Unfortunately, systems based on static linguistic knowledge bases are limited. First, such resources are difficult to find. Second, they are largely obsolete by the time they are available. Third, linguistic resources capture a particular form of lexical </context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>M. Galley and K. McKeown. 2003. Improving Word Sense Disambiguation in Lexical Chaining. In Proceedings of 18th International Joint Conference on Artificial Intelligence (IJCAI’03), Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical Chains as Representation of Context for the Detection and Correction of Malapropisms. In WordNet: An electronic lexical database and some of its applications.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1500" citStr="Hirst and St-Onge, 1997" startWordPosition="209" endWordPosition="212">ring Algorithm. As a consequence, our methodology can be applied to any language and proposes a solution to languagedependent Lexical Chainers. 1 Introduction Lexical Chains are powerful representations of documents compared to broadly used bag-of-words representations. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). However, until now, Lexical Chaining algorithms have only been proposed for English as they rely on linguistic resources such as Thesauri (Morris and Hirst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003). Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. However, at the time of writing their paper, no machine-readable thesaurus was available so they manually generated Lexical Chains using Roget’s Thesaurus (Roget, 1852). A first computational model of Lexical Chains is introduced by Hirst and St-Onge (1997). Their biggest contribution to the study of Lexical Chains is the mapping of WordNet (Miller, 1995) relations and paths (transitive relationships) to (Morris and H</context>
<context position="5415" citStr="Hirst and St-Onge, 1997" startWordPosition="821" endWordPosition="824">nts a lexico-semantic knowledge base with the purpose to identify cohesive lexical relationships between words based on corpus evidence. This hierarchical lexico-semantic knowledge base is built by using the Pole-Based Overlapping Clustering Algorithm (Cleuziou et al., 2004) that clusters words with similar meanings and allows words with multiple meanings to belong to different clusters. The second step of the process aims at automatically extracting Lexical Chains from texts based on our knowledge base. For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the “one-word/one-concept per document” paradigm (Gale et al., 1992)1. In particular, it imple1This characteristic can be interesting for multi-topic documents like web news stories. Indeed, in this case, there may be different topics in the same document as different news stories may appear. In some way, it follows the idea of (Krovetz, 1998). ments (Lin, 1998) information-theoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical </context>
</contexts>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>G. Hirst and D. St-Onge. 1997. Lexical Chains as Representation of Context for the Detection and Correction of Malapropisms. In WordNet: An electronic lexical database and some of its applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Jaromczyk</author>
<author>G T Toussaint</author>
</authors>
<title>Relative Neighborhood Graphs and Their Relatives.</title>
<date>1992</date>
<journal>P-IEEE,</journal>
<volume>80</volume>
<pages>1502--1517</pages>
<contexts>
<context position="20200" citStr="Jaromczyk and Toussaint, 1992" startWordPosition="3338" endWordPosition="3342">(xj) ∧ xj E N(xi). In particular, N(xi) corresponds to the local neighborhood of xi built as in equation 6. N(xi) = {xj E X�s(xi, xj) &gt; s(xi, X)} (6) where the notation s(xi, I) denotes the average similarity of xi with the set of objects I i.e. This definition of neighborhood is a way to avoid requiring to a parameter that would be too dependent of the similarity used. Furthermore, the use of local neighborhoods avoids the use of arbitrary thresholds which mask the variations of densities. Indeed, clusters are extracted from a similarity graph which differs from traditional proximity graphs (Jaromczyk and Toussaint, 1992) in the definition of local 10Indeed, for instance, all of these graphs connect an outlier with at least one other vertex. This is not the case with PoBOC. (7) �I� ❳ xkEI s(xi, xk) 40 neighborhoods which condition edges in the graph. Neighborhood is different for each object and is computed on the basis of similarities with all other objects. Finally, an edge connects two vertices if they are both contained in the neighborhood of the other one. Figure 1 illustrates the neighborhood constraint above. In this case, as xi and xj are not both in the intersection, they would not be connected. Figur</context>
</contexts>
<marker>Jaromczyk, Toussaint, 1992</marker>
<rawString>J.W. Jaromczyk and G.T. Toussaint. 1992. Relative Neighborhood Graphs and Their Relatives. P-IEEE, 80, pages 1502-1517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="7819" citStr="Jiang and Conrath, 1997" startWordPosition="1203" endWordPosition="1206">rne). Finally, we use a set of well-known heuristics (Daille, 1995) to retrieve compound nouns using the idea that groups of words that correspond to a priori defined syntactical patterns such as Adj+Noun, Noun+Noun, Noun+Prep+Noun can be identified as compound nouns. Indeed, nouns usually convey most of the information in a written text. They are the main contributors to the “aboutness” of a text. For example, free kick, city hall, operating system are compound nouns which sense is not compositional i.e. the sense of the multiword unit can 2Of course, other similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodorow, 1998) could be implemented and should be evaluated in further work. However, we used (Lin, 1998) similarity measure as it has shown improved results for Lexical Chains construction. 3By choosing both the TnT tagger and the multiword unit extractor SENTA, we guarantee that our architecture remains as language-independent as possible. 37 not be expressed by the sum of its constituents senses. So, identifying lexico-semantic connections between nouns is an adequate means of determining cohesive ties between textual units4. 2.2 Word Context Vectors The similarity matrix is </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>More than One Sense per Discourse.</title>
<date>1998</date>
<journal>NEC Princeton NJ Labs., Research Memorandum.</journal>
<contexts>
<context position="5879" citStr="Krovetz, 1998" startWordPosition="898" endWordPosition="899"> texts based on our knowledge base. For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the “one-word/one-concept per document” paradigm (Gale et al., 1992)1. In particular, it imple1This characteristic can be interesting for multi-topic documents like web news stories. Indeed, in this case, there may be different topics in the same document as different news stories may appear. In some way, it follows the idea of (Krovetz, 1998). ments (Lin, 1998) information-theoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical Chains2. 2 Building a Similarity Matrix In order to build the lexico-semantic knowledge base, the Pole-Based Overlapping Clustering Algorithm needs as input a similarity matrix that gathers the similarities between all the words in the corpus. For that purpose, we propose a contextual analysis of each nominal unit (nouns and compound nouns) in the corpus. In particular, each nominal unit is associated to a word context vector and the similarity between nominal</context>
<context position="25385" citStr="Krovetz, 1998" startWordPosition="4278" endWordPosition="4279"> “one-word/oneconcept per document” paradigm (Gale et al., 1992). Indeed, multi-topic documents like web news stories may introduce different topics in the same document/url and do not respect the “one sense per discourse” paradigm. As we want to deal with realworld applications, this characteristic may show interesting results for the specific task of Text Summarization for Web documents. Indeed, comparatively to the experiments made by (Gale et al., 1992) that deal with “well written discourse”, web documents show unusual discourse structures. In some way, our algorithm follows the idea of (Krovetz, 1998). Finally, it implements (Lin, 1998)’s informationtheoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical Chains. 4.1 Algorithm Our chaining algorithm is based on both approaches of (Barzilay and Elhadad, 1997) and (Hirst and StOnge, 1997). So, our chaining model is developed according to all possible alternatives of word senses. In fact, all senses of a word are defined by the clusters the word appears in13. We present our algorithm below. Begin with no chain. For all distinct nominal units in text order do For all its senses do a) - among prese</context>
</contexts>
<marker>Krovetz, 1998</marker>
<rawString>R. Krovetz. 1998. More than One Sense per Discourse. NEC Princeton NJ Labs., Research Memorandum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining Local Context and WordNet Similarity for Word Sense Identification.</title>
<date>1998</date>
<pages>265--283</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7848" citStr="Leacock and Chodorow, 1998" startWordPosition="1207" endWordPosition="1210">et of well-known heuristics (Daille, 1995) to retrieve compound nouns using the idea that groups of words that correspond to a priori defined syntactical patterns such as Adj+Noun, Noun+Noun, Noun+Prep+Noun can be identified as compound nouns. Indeed, nouns usually convey most of the information in a written text. They are the main contributors to the “aboutness” of a text. For example, free kick, city hall, operating system are compound nouns which sense is not compositional i.e. the sense of the multiword unit can 2Of course, other similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodorow, 1998) could be implemented and should be evaluated in further work. However, we used (Lin, 1998) similarity measure as it has shown improved results for Lexical Chains construction. 3By choosing both the TnT tagger and the multiword unit extractor SENTA, we guarantee that our architecture remains as language-independent as possible. 37 not be expressed by the sum of its constituents senses. So, identifying lexico-semantic connections between nouns is an adequate means of determining cohesive ties between textual units4. 2.2 Word Context Vectors The similarity matrix is a matrix where each cell corr</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining Local Context and WordNet Similarity for Word Sense Identification. In C. Fellbaum, editor, WordNet: An electronic lexical database. MIT Press. pages 265-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An Information-theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>In 15th International Conference on Machine Learning.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="5898" citStr="Lin, 1998" startWordPosition="901" endWordPosition="902">wledge base. For that purpose, we propose a new greedy algorithm which can be seen as an extension of (Hirst and St-Onge, 1997) and (Barzilay and Elhadad, 1997) algorithms which allows polysemous words to belong to different chains thus breaking the “one-word/one-concept per document” paradigm (Gale et al., 1992)1. In particular, it imple1This characteristic can be interesting for multi-topic documents like web news stories. Indeed, in this case, there may be different topics in the same document as different news stories may appear. In some way, it follows the idea of (Krovetz, 1998). ments (Lin, 1998) information-theoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical Chains2. 2 Building a Similarity Matrix In order to build the lexico-semantic knowledge base, the Pole-Based Overlapping Clustering Algorithm needs as input a similarity matrix that gathers the similarities between all the words in the corpus. For that purpose, we propose a contextual analysis of each nominal unit (nouns and compound nouns) in the corpus. In particular, each nominal unit is associated to a word context vector and the similarity between nominal units is calculate</context>
<context position="7939" citStr="Lin, 1998" startWordPosition="1225" endWordPosition="1226">at correspond to a priori defined syntactical patterns such as Adj+Noun, Noun+Noun, Noun+Prep+Noun can be identified as compound nouns. Indeed, nouns usually convey most of the information in a written text. They are the main contributors to the “aboutness” of a text. For example, free kick, city hall, operating system are compound nouns which sense is not compositional i.e. the sense of the multiword unit can 2Of course, other similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodorow, 1998) could be implemented and should be evaluated in further work. However, we used (Lin, 1998) similarity measure as it has shown improved results for Lexical Chains construction. 3By choosing both the TnT tagger and the multiword unit extractor SENTA, we guarantee that our architecture remains as language-independent as possible. 37 not be expressed by the sum of its constituents senses. So, identifying lexico-semantic connections between nouns is an adequate means of determining cohesive ties between textual units4. 2.2 Word Context Vectors The similarity matrix is a matrix where each cell corresponds to a similarity value between two nominal units5. In this paper, we propose a conte</context>
<context position="10062" citStr="Lin, 1998" startWordPosition="1585" endWordPosition="1586">ndow context of F − 2 words. So, p(w1, w2) represents the density function computed as follows: the number of times w1 and w2 co-occur divided by the number of words in the corpus7. In the present work, the values of the SCP(., .) are not used as a factor of importance between words in the word context vector i.e. no differentiation is made in terms of relevance between the words within the word context vector. This issue will be tackled in future work8. 4However, we acknowledge that verbs and adjectives should also be tackled in future work. 5Many works have been proposed on word similarity (Lin, 1998). 6In our experiments, N=10. 7We note that multiword units are counted as single words as when they are identified (e.g. President of the United States), they are re-written in the corpus by linking all single words with an underscore (e.g. President of the United States) 8We may point at the fact that satisfying results were 2.3 Similarity between Context Vectors The closeness of vectors in the space is equivalent to the closeness of the subject content. Thus, nominal units that are used in a similar local context will have vectors that are relatively close to each other. However, in order to</context>
<context position="25421" citStr="Lin, 1998" startWordPosition="4283" endWordPosition="4284">digm (Gale et al., 1992). Indeed, multi-topic documents like web news stories may introduce different topics in the same document/url and do not respect the “one sense per discourse” paradigm. As we want to deal with realworld applications, this characteristic may show interesting results for the specific task of Text Summarization for Web documents. Indeed, comparatively to the experiments made by (Gale et al., 1992) that deal with “well written discourse”, web documents show unusual discourse structures. In some way, our algorithm follows the idea of (Krovetz, 1998). Finally, it implements (Lin, 1998)’s informationtheoretic definition of similarity as the relatedness criterion for the attribution of words to Lexical Chains. 4.1 Algorithm Our chaining algorithm is based on both approaches of (Barzilay and Elhadad, 1997) and (Hirst and StOnge, 1997). So, our chaining model is developed according to all possible alternatives of word senses. In fact, all senses of a word are defined by the clusters the word appears in13. We present our algorithm below. Begin with no chain. For all distinct nominal units in text order do For all its senses do a) - among present chains find the sense which satis</context>
<context position="26710" citStr="Lin, 1998" startWordPosition="4510" endWordPosition="4511">appropriate senses of the new word and the chain members. b)if no sense is close enough, start a new chain. End For End For End 4.2 Assignment of a word to a Lexical Chain In order to assign a word to a given Lexical Chain, we need to evaluate the degree of relatedness of the given word to the words in the chain. This is done by evaluating the relatedness between all the clusters present in the Lexical Chain and all the clusters in which the word appears. 4.2.1 Scoring Function In order to determine if two clusters are semantically related, we use our lexico-semantic knowledge base and apply (Lin, 1998)’s measure of semantic similarity defined in Equation 12. 2 x log P(C0) simLin(C1, C2) = log P(C1) + log P(C2) (12) The computation of Equation 12 is illustrated below using the fragment of WordNet in Figure 2. Figure 2: Fragment of WordNet (Lin, 1998). 13From now on, for presentation purposes, we will take as synonymous the words clusters and senses s(xi, xj) (11) x��I9 42 In this case, it would be easy to compute the similarity between the concepts of hill and coast where the number attached to each node C is P(C). It is shown in Equation 13. assigned to the Lexical Chain. This situation is </context>
<context position="40768" citStr="Lin, 1998" startWordPosition="6811" endWordPosition="6812">as comparing Lexical Chains as they are is a very difficult task to tackle which may even lead to inconclusive results. 6 Conclusions and Future Work In this paper, we implemented a greedy LanguageIndependent algorithm for building Lexical Chains. For that purpose, we first constructed a lexicosemantic knowledge base by applying the PoleBased Overlapping Clustering algorithm (Cleuziou et al., 2004) to word-context vectors obtained by the application of the SCP(.,.) measure (Silva et al., 1999) and the InfoSimBA(., .) (Dias and Alves, 2005) similarity measure. In a second step, we implemented (Lin, 1998)’s similarity measure and used it to define the relatedness criterion in order to assign a given word to a given chain in the lexical chaining process. Finally, our experimental evaluation shows that relevant Lexical Chains can be constructed with our lexical chaining algorithm, although we acknowledge that more comparative evaluations must be done in order to draw definitive conclusions. In particular, in future work, we want to compare our methodology using WordNet as the basic knowledge base, implement different similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodoro</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An Information-theoretic Definition of Similarity. In 15th International Conference on Machine Learning. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: An Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the Association for Computing Machinery (CACM),</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="2036" citStr="Miller, 1995" startWordPosition="296" endWordPosition="297">rst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003). Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. However, at the time of writing their paper, no machine-readable thesaurus was available so they manually generated Lexical Chains using Roget’s Thesaurus (Roget, 1852). A first computational model of Lexical Chains is introduced by Hirst and St-Onge (1997). Their biggest contribution to the study of Lexical Chains is the mapping of WordNet (Miller, 1995) relations and paths (transitive relationships) to (Morris and Hirst, 1991) word relationship types. However, their greedy algorithm does not use a part-of-speech tagger. Instead, the algorithm only selects those words that contain noun entries in WordNet to compute Lexical Chains. But, as Barzilay and Elhadad (1997) point at, the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet. So, Barzilay and Elhadad (1997) propose the first dynamic method to compute Lexical Chains. They argue that the most appropriate se</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. Miller. 1995. WordNet: An Lexical Database for English. Communications of the Association for Computing Machinery (CACM), 38(11), pages 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="1433" citStr="Morris and Hirst, 1991" startWordPosition="199" endWordPosition="202">om a collection of texts by using the Pole-Based Overlapping Clustering Algorithm. As a consequence, our methodology can be applied to any language and proposes a solution to languagedependent Lexical Chainers. 1 Introduction Lexical Chains are powerful representations of documents compared to broadly used bag-of-words representations. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). However, until now, Lexical Chaining algorithms have only been proposed for English as they rely on linguistic resources such as Thesauri (Morris and Hirst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003). Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. However, at the time of writing their paper, no machine-readable thesaurus was available so they manually generated Lexical Chains using Roget’s Thesaurus (Roget, 1852). A first computational model of Lexical Chains is introduced by Hirst and St-Onge (1997). Their biggest contribution to the study of Lexical Chains is the mapping of WordNet (Miller, 19</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>J. Morris and G. Hirst. 1991. Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text. Computational Linguistics, 17(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<pages>613--619</pages>
<contexts>
<context position="16582" citStr="Pantel and Lin, 2002" startWordPosition="2712" endWordPosition="2715">rs) such that final clusters correspond to conceptual classes (called “concepts” in the following). Then, concepts are hierarchically structured in order to capture semantic links between them. Many clustering methods have been proposed in the data analysis research fields. Few of them propose overlapping clusters as output, in spite of the interest it represents for domains of application ❳p l=1 Aij = ❳p k=1 �i✈ Vi, Bi = ❳p l=1 ❳p k=1 39 such as Natural Language Processing or Bioinformatics. PoBOC (Pole-Based Overlapping Clustering) (Cleuziou et al., 2004) and CBC (Clustering By Committees) (Pantel and Lin, 2002) are two clustering algorithms suitable for the word clustering task. They both proceed by first constructing tight clusters9 and then assigning residual objects to their most similar tight clusters. A recent comparative study (Cicurel et al., 2006) shows that CBC and PoBOC both lead to relevant results for the task of word clustering. Nevertheless CBC requires parameters hard to tune whereas PoBOC is free of any parametrization. The last argument encouraged us to use the PoBOC algorithm. Unlike most of commonly used clustering algorithms, the Pole-Based Overlapping Clustering Algorithm shows </context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering Word Senses from Text. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 613-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<location>Montreal.</location>
<contexts>
<context position="7794" citStr="Resnik, 1995" startWordPosition="1201" endWordPosition="1202">hrases (con carne). Finally, we use a set of well-known heuristics (Daille, 1995) to retrieve compound nouns using the idea that groups of words that correspond to a priori defined syntactical patterns such as Adj+Noun, Noun+Noun, Noun+Prep+Noun can be identified as compound nouns. Indeed, nouns usually convey most of the information in a written text. They are the main contributors to the “aboutness” of a text. For example, free kick, city hall, operating system are compound nouns which sense is not compositional i.e. the sense of the multiword unit can 2Of course, other similarity measures (Resnik, 1995; Jiang and Conrath, 1997; Leacock and Chodorow, 1998) could be implemented and should be evaluated in further work. However, we used (Lin, 1998) similarity measure as it has shown improved results for Lexical Chains construction. 3By choosing both the TnT tagger and the multiword unit extractor SENTA, we guarantee that our architecture remains as language-independent as possible. 37 not be expressed by the sum of its constituents senses. So, identifying lexico-semantic connections between nouns is an adequate means of determining cohesive ties between textual units4. 2.2 Word Context Vectors </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using Information Content to Evaluate Semantic Similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montreal. pages 448-453.</rawString>
</citation>
<citation valid="false">
<title>Roget’s Thesaurus of English Words and Phrases.</title>
<publisher>Longman.</publisher>
<location>Harlow, Essex, England:</location>
<marker></marker>
<rawString>P.M. Roget. 1852. Roget’s Thesaurus of English Words and Phrases. Harlow, Essex, England: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
<author>C T Yu</author>
</authors>
<title>A Theory of Term Importance in Automatic Text Analysis.</title>
<date>1975</date>
<journal>In American Society of Information Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="11242" citStr="Salton et al., 1975" startWordPosition="1777" endWordPosition="1780">lose to each other. However, in order to define similarities between vectors, we must transform each word context vector into a high dimensional vector consisting of real-valued components. As a consequence, each co-occurring word of the word context vector is associated to a weight which evaluates its importance in the corpus. 2.3.1 Weighting score The weighting score of any word in a document can be directly derived from an adaptation of the score proposed in (Dias and Alves, 2005). In particular, we consider the combination of two main heuristics: the well-known tf.idf measure proposed by (Salton et al., 1975) and a new density measure (Dias and Alves, 2005). tf.idf: Given a word w and a document d, the tf.idf(w, d) is defined in Equation 2 where tf(w, d) is the number of occurrences of w in d, |d |corresponds to the number of words in d, N is the number of documents in the corpus and df(w) stands for the number of documents in the corpus in which the word w occurs. tf.idf (w, d) = tf (|S|, d) x log2 ✒N df (w)✓ (2) density: The basic idea of the word density measure is to evaluate the dispersion of a word within a document. So, very disperse words will not be as relevant as dense words. This densit</context>
</contexts>
<marker>Salton, Yang, Yu, 1975</marker>
<rawString>G. Salton, C.S. Yang and C.T. Yu. 1975. A Theory of Term Importance in Automatic Text Analysis. In American Society of Information Science, 26(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Silber</author>
<author>K McCoy</author>
</authors>
<title>Efficiently Computed Lexical Chains as an Intermediate Representation for Automatic Text Summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>487--496</pages>
<contexts>
<context position="1524" citStr="Silber and McCoy, 2002" startWordPosition="213" endWordPosition="216">equence, our methodology can be applied to any language and proposes a solution to languagedependent Lexical Chainers. 1 Introduction Lexical Chains are powerful representations of documents compared to broadly used bag-of-words representations. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). However, until now, Lexical Chaining algorithms have only been proposed for English as they rely on linguistic resources such as Thesauri (Morris and Hirst, 1991) or Ontologies (Barzilay and Elhadad, 1997; Hirst and St-Onge, 1997; Silber and McCoy, 2002; Galley and McKeown, 2003). Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. However, at the time of writing their paper, no machine-readable thesaurus was available so they manually generated Lexical Chains using Roget’s Thesaurus (Roget, 1852). A first computational model of Lexical Chains is introduced by Hirst and St-Onge (1997). Their biggest contribution to the study of Lexical Chains is the mapping of WordNet (Miller, 1995) relations and paths (transitive relationships) to (Morris and Hirst, 1991) word relatio</context>
<context position="3150" citStr="Silber and McCoy (2002)" startWordPosition="474" endWordPosition="477">ad (1997) propose the first dynamic method to compute Lexical Chains. They argue that the most appropriate sense of a word can only be chosen after examining all possible Lexical Chain combinations that can be generated from a text. Because all possible senses of the word are not taken into account, except at the time of insertion, potentially pertinent context information that is likely to appear after the word is lost. However, this method of retaining all possible interpretations until the end of the process, causes the exponential growth of the time and space complexity. As a consequence, Silber and McCoy (2002) propose a linear time version of (Barzilay and Elhadad, 1997) lexical chaining algorithm. In particular, (Silber and McCoy, 2002)’s implementation creates a structure, called meta-chains, that implicitly stores 36 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 36–47, Sydney, July 2006. c�2006 Association for Computational Linguistics all chain interpretations without actually creating them, thus keeping both the space and time usage of the program linear. Finally, Galley and McKeown (2003) propose a chaining method that disambiguates nouns prior to the proces</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>G. Silber and K. McCoy. 2002. Efficiently Computed Lexical Chains as an Intermediate Representation for Automatic Text Summarization. Computational Linguistics, 28(4), pages 487-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Silva</author>
<author>G Dias</author>
<author>S Guillor´e</author>
<author>J G P Lopes</author>
</authors>
<title>Using LocalMaxs Algorithm for the Extraction of Contiguous and Non-contiguous Multiword Lexical Units.</title>
<date>1999</date>
<booktitle>In Proceedings of 9th Portuguese Conference in Artificial Intelligence.</booktitle>
<publisher>Springer-Verlag.</publisher>
<marker>Silva, Dias, Guillor´e, Lopes, 1999</marker>
<rawString>J. Silva, G. Dias, S. Guillor´e and J.G.P. Lopes. 1999. Using LocalMaxs Algorithm for the Extraction of Contiguous and Non-contiguous Multiword Lexical Units. In Proceedings of 9th Portuguese Conference in Artificial Intelligence. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P H A Sneath</author>
<author>R R Sokal</author>
</authors>
<title>Numerical Taxonomy - The Principles and Practice of Numerical Classification.</title>
<date>1973</date>
<location>San</location>
<contexts>
<context position="24117" citStr="Sneath and Sokal, 1973" startWordPosition="4062" endWordPosition="4065">t xi (resp. xj) is more similar to xj (resp. xi) than to other data on average. This methodology results into a coverage of the starting data set with overlapping clusters (extended Poles). 12P1(xi) =arg max,j s(xi, Pj) 41 3.4 Hierarchical Organization A final step consists in organizing the obtained clusters into a hierarchical tree. This structure is useful to catch the topology of a set of a priori disconnected groups. The Pole-Based Overlapping Clustering algorithm integrates this stage and proceeds by successive merging of the two nearest clusters like for usual agglomerative approaches (Sneath and Sokal, 1973). In this process, the similarity between two clusters is obtained by the average-link (or complete-link) method: ❳ s(Ip, Iq) = |Ip|.|Iq| ❳ 1 x��I� To deal with overlapping clusters we considere in Equation 11 the similarity between an object and itself to be equal to 1 : s(xi7 xi) = 1. 4 Lexical Chaining Algorithm Once the lexico-semantic knowledge base has been built, it is possible to use it for Lexical Chaining. In this section, we propose a new greedy algorithm which can be seen as an extension of (Hirst and StOnge, 1997) and (Barzilay and Elhadad, 1997) algorithms as it allows polysemous</context>
</contexts>
<marker>Sneath, Sokal, 1973</marker>
<rawString>P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxonomy - The Principles and Practice of Numerical Classification. San Francisco, Freeman and Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Teich</author>
<author>P Fankhauser</author>
</authors>
<title>Exploring Lexical Patterns in Text: Lexical Cohesion Analysis with WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2nd International Wordnet Conference,</booktitle>
<pages>326--331</pages>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="32854" citStr="Teich and Fankhauser, 2004" startWordPosition="5556" endWordPosition="5559">mance comes from the efficiency of the lexical chaining algorithm or from the appropriateness of using Lexical Chains in that kind of application. It is also true that some work has been done in this direction (Budanitsky and Hirst, 2006) by collecting Human Lexical Chains to compare against automatically built Lexical Chains. However, this type of evaluation is logistically impossible to perform as we aim at developing a system that does not depend on any language or topic. So, in this section, we will only present some results generated by our architecture (like (Barzilay and Elhadad, 1997; Teich and Fankhauser, 2004) do), although we acknowledge that other comparative evaluations (with WordNet, with Human Lexical Chains or within independent applications like Text Summarization) must be done in order to draw definitive conclusions. We have generated four taxonomies from four different domains (Sport, Economy, Politics and War) from a set of documents of the DUC 200415. Moreover, we have extracted Lexical Chains for all four 15http://duc.nist.gov/duc2004/ score(chain) = (jchainj − 1)jchainj 2 jchainj−1 jchainj simLin(Ci, C;) ❳ ❳ i=0 7=i+1 (16) 44 domains to show the ability of our system to switch from dom</context>
<context position="35153" citStr="Teich and Fankhauser, 2004" startWordPosition="5970" endWordPosition="5973">Doc 1 27 43 73 73 Doc 2 31 52 81 83 Doc 3 28 40 51 51 Doc 4 29 53 83 87 Table 2: # Lexical Chains per Document The second interesting conclusion is that our algorithm does not gather words that belong to only one cluster and take advantage of the automatically built lexico-semantic knowledge base. This is illustrated in Table 3. However, it is obvious that by increasing the constant c the words in a chain tend to belong to only one cluster as it is the case for most of the best Lexical Chains with c = 8. 5.2 Qualitative Evaluation In this section, as it is done in (Barzilay and Elhadad, 1997; Teich and Fankhauser, 2004), we present the c=5 c=6 c=7 c=8 Doc 1 19 13 7 7 Doc 2 13 6 3 3 Doc 3 3 4 4 4 Doc 4 6 4 3 3 Table 3: # Clusters per Lexical Chain five highest-scoring chains for the best threshold that we experimentally evaluated to be c = 7 for each domain (See Tables 4, 5, 6, 7). It is clear that the obtained Lexical Chains show a desirable degree of representativeness of the text in analysis. Domain=Sport, Document=3, c=7 - #0, 1 cluster and score=1.0: {United States, couple, competition} - #6, 3 clusters and score=1.0: {boats, Sunday night, sailor, Sword, Orion, veteran, cutter, Winston Churchill, Solo Gl</context>
</contexts>
<marker>Teich, Fankhauser, 2004</marker>
<rawString>E. Teich and P. Fankhauser. 2004. Exploring Lexical Patterns in Text: Lexical Cohesion Analysis with WordNet. In Proceedings of the 2nd International Wordnet Conference, Brno, Czech Republic. pages 326-331.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>