<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003144">
<title confidence="0.997498">
Integrating Ngram Model and Case-based Learning
For Chinese Word Segmentation
</title>
<author confidence="0.895401">
Chunyu Kit Zhiming Xu Jonathan J. Webster
</author>
<affiliation confidence="0.913951">
Department of Chinese, Translation and Linguistics
City University of Hong Kong
</affiliation>
<address confidence="0.37262">
Tat Chee Ave., Kowloon, Hong Kong
</address>
<email confidence="0.992899">
{ctckit, ctxuzm, ctjjw}@cityu.edu.hk
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999825133333333">
This paper presents our recent work
for participation in the First Interna-
tional Chinese Word Segmentation Bake-
off (ICWSB-1). It is based on a general-
purpose ngram model for word segmen-
tation and a case-based learning approach
to disambiguation. This system excels
in identifying in-vocabulary (IV) words,
achieving a recall of around 96-98%.
Here we present our strategies for lan-
guage model training and disambiguation
rule learning, analyze the system’s perfor-
mance, and discuss areas for further im-
provement, e.g., out-of-vocabulary (OOV)
word discovery.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932788461539">
After about two decades of studies of Chinese word
segmentation, ICWSB-1 (henceforth, the bakeoff)
is the first effort to put different approaches and
systems to the test and comparison on common
datasets. We participated in the bakeoff with a
segmentation system that is designed to integrate a
general-purpose ngram model for probabilistic seg-
mentation and a case- or example-based learning
approach (Kit et al., 2002) for disambiguation.
The ngram model, with words extracted from
training corpora, is trained with the EM algorithm
(Dempster et al., 1977) using unsegmented train-
ing corpora. Originally it was developed to en-
hance word segmentation accuracy so as to facili-
tate Chinese-English word alignment for our ongo-
ing EBMT project, where only unsegmented texts
are available for training. It is expected to be ro-
bust enough to handle novel texts, independent of
any segmented texts for training. To simplify the
EM training, we used the uni-gram model for the
bakeoff and relied on the Viterbi algorithm (Viterbi,
1967) for the most probable segmentation, instead of
attempting to exhaust all possible segmentations of
each sentence for a complicated full version of EM
training.
The case-based learning works in a straightfor-
ward way. It first extracts case-based knowledge,
as a set of context-dependent transformation rules,
from the segmented training corpus, and then ap-
plies them to ambiguous strings in a test corpus in
terms of the similarity of their contexts. The simi-
larity is empirically computed in terms of the length
of relevant common affixes of context strings.
The effectiveness of this integrated approach is
verified by its outstanding performance on IV word
identification. Its IV recall rate, ranging from 96%
to 98%, stands at the top or the next to the top in all
closed tests in which we have participated. Unfortu-
nately, its overall performance is not sustainable at
the same level, due to the lack of a module for OOV
word detection.
This paper is intended to present the implementa-
tion of the system and analyze its performance and
problems, aiming at exploration of directions for fur-
ther improvement. The remaining sections are or-
ganized as follows. Section 2 presents the ngram
model and its training with the EM algorithm, and
Section 3 presents the case-based learning for dis-
ambiguation. The overall architecture of our system
is given in Section 4, and its performance and prob-
lems are analyzed in Section 5. Section 6 concludes
the paper and previews future work.
</bodyText>
<sectionHeader confidence="0.686891" genericHeader="method">
2 Ngram model and training
</sectionHeader>
<bodyText confidence="0.967758166666667">
An ngram model can be utilized to find the most
probable segmentation of a sentence. Given a Chi-
nese sentence s = c1c2 · · · cm (also denoted as cn1),
its probabilistic segmentation into a word sequence
w1w2 · · · wk (also denoted as wk1) with the aid of an
ngram model can be formulated as
</bodyText>
<equation confidence="0.997068666666667">
seg(s) = arg max k p(wi|wi−1
s = w1w2···wk ri i−n+1) (1)
i
</equation>
<bodyText confidence="0.8755292">
where o denotes string concatenation, wi−1
i−n+1 the
context (or history) of wi, and n is the order of the
ngram model in use. We have opted for uni-gram for
the sake of simplicity. Accordingly, p(wi|wi−1
</bodyText>
<equation confidence="0.688902">
i−n+1)
</equation>
<bodyText confidence="0.9987065">
in (1) becomes p(wi), which is commonly estimated
as follows, given a corpus C for training.
</bodyText>
<equation confidence="0.9886925">
p(wi) = . f(wi)/ � f(w) (2)
wC
</equation>
<bodyText confidence="0.9997152">
In order to estimate a reliable p(wi), the ngram
model needs to be trained with the EM algorithm
using the available training corpus. Each EM itera-
tion aims at approaching to a more reliable f(w) for
estimating p(w), as follows:
</bodyText>
<equation confidence="0.9986625">
�fk+1(w) = E pk(s) fk(w E s) (3)
sC s&apos;S(s)
</equation>
<bodyText confidence="0.9986676">
where k denotes the current iteration, S(s) the set of
all possible segmentations for s, and f k(w E s) the
occurrences of w in a particular segmentation s.
However, assuming that every sentence always
has a segmentation, the following equation holds:
</bodyText>
<equation confidence="0.989218">
E pk(s) = 1 (4)
s&apos;S(s)
</equation>
<bodyText confidence="0.9997325">
Accordingly, we can adjust (3) as (5) with a normal-
ization factor a = Es&apos;S(s) pk(s), to avoid favor-
ing words in shorter sentences too much. In general,
shorter sentences have higher probabilities.
</bodyText>
<equation confidence="0.9838215">
�fk+1(w) = E pk(s) fk(w E s) (5)
sC s&apos;S(s) a
</equation>
<bodyText confidence="0.999984">
Following the conventional idea to speed up the
EM training, we turned to the Viterbi algorithm. The
underlying philosophy is to distribute more prob-
ability to more probable events. The Viterbi seg-
mentation, by utilizing dynamic programming tech-
niques to go through the word trellis of a sentence
efficiently, finds the most probable segmentation un-
der the current parameter estimation of the language
model, fulfilling (1)). Accordingly, (6) becomes
</bodyText>
<equation confidence="0.9088998">
�fk+1(w) = pk(seg(s)) fk(w E seg(s)) (6)
sC
and (5) becomes
�fk+1(w) = fk(w E seg(s)) (7)
sC
</equation>
<bodyText confidence="0.999919">
where the normalization factor is skipped, for
only the Viterbi segmentation is used for EM re-
estimation. Equation (7) makes the EM training
with the Viterbi algorithm very simple for the uni-
gram model: iterate word segmentation, as (1), and
word count updating, via (7), sentence by sentence
through the training corpus until there is a conver-
gence.
Since the EM algorithm converges to a local max-
ima only, it is critical to start the training with an
initial f0(w) for each word not too far away from its
“true” value. Our strategy for initializing f 0(w) is
to assume all possible words in the training corpus
as equiprobable and count each of them as 1; and
then p0(w) is derived using (2). This strategy is sup-
posed to have a weaker bias to favor longer words
than maximal matching segmentation.
For the bakeoff, the ngram model is trained with
the unsegmented training corpora together with the
test sets. It is a kind of unsupervised training.
Adding the test set to the training data is reasonable,
to allow the model to have necessary adaptation to-
wards the test sets. Experiments show that the train-
ing converges very fast, and the segmentation per-
formance improves significantly from iteration to it-
eration. For the bakeoff experiments, we carried out
the training in 6 iterations, because more iterations
than this have not been observed to bring any signif-
icant improvement on segmentation accuracy to the
training sets.
</bodyText>
<sectionHeader confidence="0.970795" genericHeader="method">
3 Case-based learning for disambiguation
</sectionHeader>
<bodyText confidence="0.999968461538462">
No matter how well the language model is trained,
probabilistic segmentation cannot avoid mistakes on
ambiguous strings, although it resolves most ambi-
guities by virtue of probability. For the remaining
unresolved ambiguities, however, we have to resort
to other strategies and/or resources. Our recent study
(Kit et al., 2002) shows that case-based learning is
an effective approach to disambiguation.
The basic idea behind the case-based learning is
to utilize existing resolutions for known ambiguous
strings to do disambiguation if similar ambiguities
occur again. This learning strategy can be imple-
mented in two straightforward steps:
</bodyText>
<listItem confidence="0.970453166666667">
1. Collection of correct answers from the train-
ing corpus for ambiguous strings together with
their contexts, resulting in a set of context-
dependent transformation rules;
2. Application of appropriate rules to ambiguous
strings.
</listItem>
<bodyText confidence="0.999912">
A transformation rule of this type is actually an ex-
ample of segmentation, indicating how an ambigu-
ous string is segmented within a particular context.
It has the following general form:
</bodyText>
<subsectionHeader confidence="0.350025">
Cla Cr : a  w1 w2 ··· wk
</subsectionHeader>
<bodyText confidence="0.999955384615385">
where a is the ambiguous string, Cl and Cr its left
and right contexts, respectively, and w1 w2 · · · wk
the correct segmentation of a given the contexts.
In our implementation, we set the context length on
each side to two words.
For a particular ambiguity, the example with the
most similar context in the example (or, rule) base
is applied. The similarity is measured by the sum
of the length of the common suffix and prefix of,
respectively, the left and right contexts. The details
of computing this similarity can be found in (Kit et
al., 2002) . If no rule is applicable, its probabilistic
segmentation is retained.
For the bakeoff, we have based our approach to
ambiguity detection and disambiguation rule extrac-
tion on the assumption that only ambiguous strings
cause mistakes: we detect the discrepancies of our
probabilistic segmentation and the standard segmen-
tation of the training corpus, and turn them into
transformation rules. An advantage of this approach
is that the rules so derived carry out not only disam-
biguation but also error correction. This links our
disambiguation strategy to the application of Brill’s
(1993) transformation-based error-driven learning to
Chinese word segmentation (Palmer, 1997; Hocken-
maier and Brew, 1998).
</bodyText>
<sectionHeader confidence="0.910875" genericHeader="method">
4 System architecture
</sectionHeader>
<bodyText confidence="0.956144">
The overall architecture of our word segmentation
system is presented in Figure 1.
</bodyText>
<figureCaption confidence="0.990979">
Figure 1: Overall architecture of the system
</figureCaption>
<sectionHeader confidence="0.701271" genericHeader="evaluation">
5 Performance and analysis
</sectionHeader>
<bodyText confidence="0.9996015">
The performance of our system in the bakeoff is pre-
sented in Table 1 in terms of precision (P), recall
(R) and F score in percentages, where “c” denotes
closed tests. Its IV word identification performance
is remarkable.
However, its overall performance is not in bal-
ance with this, due to the lack of a module for OOV
word discovery. It only gets a small number of OOV
words correct by chance. The higher OOV propor-
tion in the test set, the worse is its F score. The rel-
atively high Roos, for PKc track is, mostly, the result
of number recognition with regular expressions.
</bodyText>
<table confidence="0.98011425">
Test P R F OOV Roov Riv
SAc 95.2 93.1 94.2 02.2 04.3 97.2
CTBc 80.0 67.4 73.2 18.1 07.6 95.9
PKc 92.3 86.7 89.4 06.9 15.9 98.0
</table>
<tableCaption confidence="0.999949">
Table 1: System performance, in percentages (%)
</tableCaption>
<subsectionHeader confidence="0.991697">
5.1 Error analysis
</subsectionHeader>
<bodyText confidence="0.999959666666666">
Most errors on IV words are due to the side-effect
of the context-dependent transformation rules. The
rules resolve most remaining ambiguities and cor-
rect many errors, but at the same time they also cor-
rupt some proper segmentations. This side-effect is
most likely to occur when there is inadequate con-
text information to decide which rules to apply.
There are two strategies to remedy, or at least al-
leviate, this side-effect: (1) retrain probabilistic seg-
mentation – a conservative strategy; or, (2) incorpo-
rate Brill’s error-driven learning with several rounds
of transformation rule extraction and application, al-
lowing mistakes caused by some rules in previous
rounds to be corrected by other rules in later rounds.
However, even worse than the above side-effect is
a bug in our disambiguation module: it always ap-
plies the first available rule, leading to many unex-
pected errors, each of which may result in more than
one erroneous word. For instance, among 430 er-
rors made by the system in the SA closed test, some
70 are due to this bug. A number of representative
examples of these errors are presented in Table 2,
together with some false errors resulting from the
inconsistency in the standard segmentation.
</bodyText>
<table confidence="0.999405857142857">
Errors Standard False errors Standard
2í (8) 2 í ùŸ×D ù Ÿ ×D
R u (7) Ru tjs— tj s—
. ? (7) .? ÙÍ’Ä ÙÍ ’Ä
_ A (5) _A 4_,ñ 4_ ,ñ
R  (4) R 1w±Ñ 1w± Ñ
:Y;? (4) :Y; ? .TM¤Þ . TM ¤ Þ
</table>
<tableCaption confidence="0.998315">
Table 2: Errors and false errors
</tableCaption>
<sectionHeader confidence="0.968882" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999956285714286">
We have presented our recent work for partici-
pation in ICWSB-1 based on a general-purpose
ngram model for probabilistic word segmentation
and a case-based learning strategy for disambigua-
tion. The ngram model is trained using available
unsegmented texts with the EM algorithm with the
aid of Viterbi segmentation. The learning strategy
acquires a set of context-dependent transformation
rules to correct mistakes in the probabilistic segmen-
tation of ambiguous substrings. This integrated ap-
proach demonstrates an impressive effectiveness by
its outstanding performance on IV word identifica-
tion. With elimination of the bug and false errors, its
performance could be significantly better.
</bodyText>
<subsectionHeader confidence="0.617731">
6.1 Future work
</subsectionHeader>
<bodyText confidence="0.99992225">
The above problem analysis points to two main di-
rections for improvement in our future work: (1)
OOV word detection; (2) a better strategy for learn-
ing and applying transformation rules to reduce the
side-effect. In addition, we are also interested in
studying the effectiveness of higher-order ngram
models and variants of EM training for Chinese
word segmentation.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999957428571429">
The work is part of the CERG project “EBMT for
HK Legal Texts” funded by HK UGC under the
grant #9040482, with Jonathan J. Webster as the
principal investigator and Chunyu Kit, Caesar S.
Lun, Haihua Pan, King Kuai Sin and Vincent Wong
as investigators. The authors wish to thank all team
members for their contribution to this paper.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999036818181818">
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
A. P. Dempster, N. M. Laird, and D. B.Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 34:1–38.
J. Hockenmaier and C. Brew. 1998. Error-driven learn-
ing of Chinese word segmentation. In PACLIC-12,
pages 218–229, Singapore. Chinese and Oriental Lan-
guages Processing Society.
C. Kit, H. Pan, and H. Chen. 2002. Learning case-based
knowledge for disambiguating Chinese word segmen-
tation: A preliminary study. In COLING2002 work-
shop: SIGHAN-1, pages 33–39, Taipei.
D. Palmer. 1997. A trainable rule-based algorithm
for word segmentation. In ACL-97, pages 321–328,
Madrid.
A. J. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, IT-13:260–
267.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455853">
<title confidence="0.999639">Integrating Ngram Model and Case-based For Chinese Word Segmentation</title>
<author confidence="0.999896">Chunyu Kit Zhiming Xu Jonathan J</author>
<affiliation confidence="0.9424055">Department of Chinese, Translation and City University of Hong</affiliation>
<address confidence="0.527027">Tat Chee Ave., Kowloon, Hong</address>
<email confidence="0.962261">ctxuzm,</email>
<abstract confidence="0.993003875">This paper presents our recent work for participation in the First International Chinese Word Segmentation Bakeoff (ICWSB-1). It is based on a generalpurpose ngram model for word segmentation and a case-based learning approach to disambiguation. This system excels in identifying in-vocabulary (IV) words, achieving a recall of around 96-98%. Here we present our strategies for language model training and disambiguation rule learning, analyze the system’s performance, and discuss areas for further improvement, e.g., out-of-vocabulary (OOV) word discovery.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Brill, 1993</marker>
<rawString>E. Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>34--1</pages>
<contexts>
<context position="1412" citStr="Dempster et al., 1977" startWordPosition="206" endWordPosition="209">t, e.g., out-of-vocabulary (OOV) word discovery. 1 Introduction After about two decades of studies of Chinese word segmentation, ICWSB-1 (henceforth, the bakeoff) is the first effort to put different approaches and systems to the test and comparison on common datasets. We participated in the bakeoff with a segmentation system that is designed to integrate a general-purpose ngram model for probabilistic segmentation and a case- or example-based learning approach (Kit et al., 2002) for disambiguation. The ngram model, with words extracted from training corpora, is trained with the EM algorithm (Dempster et al., 1977) using unsegmented training corpora. Originally it was developed to enhance word segmentation accuracy so as to facilitate Chinese-English word alignment for our ongoing EBMT project, where only unsegmented texts are available for training. It is expected to be robust enough to handle novel texts, independent of any segmented texts for training. To simplify the EM training, we used the uni-gram model for the bakeoff and relied on the Viterbi algorithm (Viterbi, 1967) for the most probable segmentation, instead of attempting to exhaust all possible segmentations of each sentence for a complicat</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B.Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 34:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>C Brew</author>
</authors>
<title>Error-driven learning of Chinese word segmentation.</title>
<date>1998</date>
<booktitle>In PACLIC-12,</booktitle>
<pages>218--229</pages>
<publisher>Society.</publisher>
<contexts>
<context position="9242" citStr="Hockenmaier and Brew, 1998" startWordPosition="1514" endWordPosition="1518">the bakeoff, we have based our approach to ambiguity detection and disambiguation rule extraction on the assumption that only ambiguous strings cause mistakes: we detect the discrepancies of our probabilistic segmentation and the standard segmentation of the training corpus, and turn them into transformation rules. An advantage of this approach is that the rules so derived carry out not only disambiguation but also error correction. This links our disambiguation strategy to the application of Brill’s (1993) transformation-based error-driven learning to Chinese word segmentation (Palmer, 1997; Hockenmaier and Brew, 1998). 4 System architecture The overall architecture of our word segmentation system is presented in Figure 1. Figure 1: Overall architecture of the system 5 Performance and analysis The performance of our system in the bakeoff is presented in Table 1 in terms of precision (P), recall (R) and F score in percentages, where “c” denotes closed tests. Its IV word identification performance is remarkable. However, its overall performance is not in balance with this, due to the lack of a module for OOV word discovery. It only gets a small number of OOV words correct by chance. The higher OOV proportion </context>
</contexts>
<marker>Hockenmaier, Brew, 1998</marker>
<rawString>J. Hockenmaier and C. Brew. 1998. Error-driven learning of Chinese word segmentation. In PACLIC-12, pages 218–229, Singapore. Chinese and Oriental Languages Processing Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kit</author>
<author>H Pan</author>
<author>H Chen</author>
</authors>
<title>Learning case-based knowledge for disambiguating Chinese word segmentation: A preliminary study.</title>
<date>2002</date>
<booktitle>In COLING2002 workshop: SIGHAN-1,</booktitle>
<pages>33--39</pages>
<location>Taipei.</location>
<contexts>
<context position="1274" citStr="Kit et al., 2002" startWordPosition="185" endWordPosition="188"> language model training and disambiguation rule learning, analyze the system’s performance, and discuss areas for further improvement, e.g., out-of-vocabulary (OOV) word discovery. 1 Introduction After about two decades of studies of Chinese word segmentation, ICWSB-1 (henceforth, the bakeoff) is the first effort to put different approaches and systems to the test and comparison on common datasets. We participated in the bakeoff with a segmentation system that is designed to integrate a general-purpose ngram model for probabilistic segmentation and a case- or example-based learning approach (Kit et al., 2002) for disambiguation. The ngram model, with words extracted from training corpora, is trained with the EM algorithm (Dempster et al., 1977) using unsegmented training corpora. Originally it was developed to enhance word segmentation accuracy so as to facilitate Chinese-English word alignment for our ongoing EBMT project, where only unsegmented texts are available for training. It is expected to be robust enough to handle novel texts, independent of any segmented texts for training. To simplify the EM training, we used the uni-gram model for the bakeoff and relied on the Viterbi algorithm (Viter</context>
<context position="7232" citStr="Kit et al., 2002" startWordPosition="1191" endWordPosition="1194">ntly from iteration to iteration. For the bakeoff experiments, we carried out the training in 6 iterations, because more iterations than this have not been observed to bring any significant improvement on segmentation accuracy to the training sets. 3 Case-based learning for disambiguation No matter how well the language model is trained, probabilistic segmentation cannot avoid mistakes on ambiguous strings, although it resolves most ambiguities by virtue of probability. For the remaining unresolved ambiguities, however, we have to resort to other strategies and/or resources. Our recent study (Kit et al., 2002) shows that case-based learning is an effective approach to disambiguation. The basic idea behind the case-based learning is to utilize existing resolutions for known ambiguous strings to do disambiguation if similar ambiguities occur again. This learning strategy can be implemented in two straightforward steps: 1. Collection of correct answers from the training corpus for ambiguous strings together with their contexts, resulting in a set of contextdependent transformation rules; 2. Application of appropriate rules to ambiguous strings. A transformation rule of this type is actually an example</context>
<context position="8538" citStr="Kit et al., 2002" startWordPosition="1410" endWordPosition="1413">text. It has the following general form: Cla Cr : a  w1 w2 ··· wk where a is the ambiguous string, Cl and Cr its left and right contexts, respectively, and w1 w2 · · · wk the correct segmentation of a given the contexts. In our implementation, we set the context length on each side to two words. For a particular ambiguity, the example with the most similar context in the example (or, rule) base is applied. The similarity is measured by the sum of the length of the common suffix and prefix of, respectively, the left and right contexts. The details of computing this similarity can be found in (Kit et al., 2002) . If no rule is applicable, its probabilistic segmentation is retained. For the bakeoff, we have based our approach to ambiguity detection and disambiguation rule extraction on the assumption that only ambiguous strings cause mistakes: we detect the discrepancies of our probabilistic segmentation and the standard segmentation of the training corpus, and turn them into transformation rules. An advantage of this approach is that the rules so derived carry out not only disambiguation but also error correction. This links our disambiguation strategy to the application of Brill’s (1993) transforma</context>
</contexts>
<marker>Kit, Pan, Chen, 2002</marker>
<rawString>C. Kit, H. Pan, and H. Chen. 2002. Learning case-based knowledge for disambiguating Chinese word segmentation: A preliminary study. In COLING2002 workshop: SIGHAN-1, pages 33–39, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation. In</title>
<date>1997</date>
<booktitle>ACL-97,</booktitle>
<pages>321--328</pages>
<location>Madrid.</location>
<contexts>
<context position="9213" citStr="Palmer, 1997" startWordPosition="1512" endWordPosition="1513">retained. For the bakeoff, we have based our approach to ambiguity detection and disambiguation rule extraction on the assumption that only ambiguous strings cause mistakes: we detect the discrepancies of our probabilistic segmentation and the standard segmentation of the training corpus, and turn them into transformation rules. An advantage of this approach is that the rules so derived carry out not only disambiguation but also error correction. This links our disambiguation strategy to the application of Brill’s (1993) transformation-based error-driven learning to Chinese word segmentation (Palmer, 1997; Hockenmaier and Brew, 1998). 4 System architecture The overall architecture of our word segmentation system is presented in Figure 1. Figure 1: Overall architecture of the system 5 Performance and analysis The performance of our system in the bakeoff is presented in Table 1 in terms of precision (P), recall (R) and F score in percentages, where “c” denotes closed tests. Its IV word identification performance is remarkable. However, its overall performance is not in balance with this, due to the lack of a module for OOV word discovery. It only gets a small number of OOV words correct by chanc</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>D. Palmer. 1997. A trainable rule-based algorithm for word segmentation. In ACL-97, pages 321–328, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<pages>267</pages>
<contexts>
<context position="1883" citStr="Viterbi, 1967" startWordPosition="285" endWordPosition="286">2002) for disambiguation. The ngram model, with words extracted from training corpora, is trained with the EM algorithm (Dempster et al., 1977) using unsegmented training corpora. Originally it was developed to enhance word segmentation accuracy so as to facilitate Chinese-English word alignment for our ongoing EBMT project, where only unsegmented texts are available for training. It is expected to be robust enough to handle novel texts, independent of any segmented texts for training. To simplify the EM training, we used the uni-gram model for the bakeoff and relied on the Viterbi algorithm (Viterbi, 1967) for the most probable segmentation, instead of attempting to exhaust all possible segmentations of each sentence for a complicated full version of EM training. The case-based learning works in a straightforward way. It first extracts case-based knowledge, as a set of context-dependent transformation rules, from the segmented training corpus, and then applies them to ambiguous strings in a test corpus in terms of the similarity of their contexts. The similarity is empirically computed in terms of the length of relevant common affixes of context strings. The effectiveness of this integrated app</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, IT-13:260– 267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>