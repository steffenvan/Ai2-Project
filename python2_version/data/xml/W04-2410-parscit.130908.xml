<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000118">
<title confidence="0.973316">
Thesauruses for Prepositional Phrase Attachment
</title>
<author confidence="0.998507">
Mark McLauchlan
</author>
<affiliation confidence="0.9955555">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.980387">
Brighton, BN1 9RH
</address>
<email confidence="0.99798">
mrm21@sussex.ac.uk
</email>
<sectionHeader confidence="0.982942" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879">
Probabilistic models have been effective in re-
solving prepositional phrase attachment am-
biguity, but sparse data remains a significant
problem. We propose a solution based on
similarity-based smoothing, where the proba-
bility of new PPs is estimated with informa-
tion from similar examples generated using a
thesaurus. Three thesauruses are compared on
this task: two existing generic thesauruses and
a new specialist PP thesaurus tailored for this
problem. We also compare three smoothing
techniques for prepositional phrases. We find
that the similarity scores provided by the the-
saurus tend to weight distant neighbours too
highly, and describe a better score based on the
rank of a word in the list of similar words. Our
smoothing methods are applied to an existing
PP attachment model and we obtain significant
improvements over the baseline.
</bodyText>
<sectionHeader confidence="0.995164" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986355140187">
Prepositional phrases are an interesting example of syn-
tactic ambiguity and a challenge for automatic parsers.
The ambiguity arises whenever a prepositional phrase
can modify a preceding verb or noun, as in the canoni-
cal example I saw the man with the telescope. In syn-
tactic terms, the prepositional phrase attaches either to
the noun phrase or the verb phrase. Many kinds of syn-
tactic ambiguity can be resolved using structural infor-
mation alone (Briscoe and Carroll, 1995; Lin, 1998a;
Klein and Manning, 2003), but in this case both candidate
structures are perfectly grammatical and roughly equally
likely. Therefore ambiguous prepositional phrases re-
quire some kind of additional context to disambiguate
correctly. In some cases a small amount of lexical knowl-
edge is sufficient: for example of almost always modifies
the noun. Other cases, such as the telescope example, are
potentially much harder since discourse or world knowl-
edge might be required.
Fortunately it is possible to do well at this task just
by considering the lexical preferences of the words mak-
ing up the PP. Lexical preferences describe the tendency
for certain words to occur together or only in specific
constructions. For example, saw and telescope are more
likely to occur together than man and telescope, so we
can infer that the correct attachment is likely to be ver-
bal. The most useful lexical preferences are captured by
the quadruple (v, n1, p, n2) where v is the verb, n1 is the
head of the direct object, p is the preposition and n2 is the
head of the prepositional phrase. A benchmark dataset
of 27,937 such quadruples was extracted from the Wall
Street Journal corpus by Ratnaparkhi et al. (1994) and
has been the basis of many subsequent studies comparing
machine learning algorithms and lexical resources. This
paper examines the effect of particular smoothing algo-
rithms on the performance of an existing statistical PP
model.
A major problem faced by any statistical attachment al-
gorithm is sparse data, which occurs when plausible PPs
are not well-represented in the training data. For exam-
ple, if the observed frequency of a PP in the training is
zero then the maximum likelihood estimate is also zero.
Since the training corpus only represents a fraction of all
possible PPs, this is probably an underestimate of the true
probability. An appealing course of action when faced
with an unknown PP is to consider similar known exam-
ples instead. For example, we may not have any data for
eatpizza with fork, but if we have seen eatpasta with fork
or even drink beer with straw then it seems reasonable to
base our decision on these instead.
Similarity is a rather nebulous concept but for our pur-
poses we can define it to be distributional similarity,
where two words are considered similar if they occur in
similar contexts. For example, pizza and pasta are sim-
ilar since they both often occur as the direct object of
eat. A thesaurus collects together lists of such similar
words. The first step in constructing a thesaurus is to
collect co-occurrence statistics from some large corpus
of text. Each word is assigned a probability distribution
describing the probability of it occurring with all other
words, and by comparing distributions we can arrive at a
similarity score. The corpus, co-occurrence relationships
and distributional similarity metric all affect the nature of
the final thesaurus.
There has been a considerable amount of research
comparing corpora, co-occurrence relations and similar-
ity measures for general-purpose thesauruses, and these
thesauruses are often compared against wide-coverage
and general purpose semantic resources such as Word-
Net. In this paper we examine whether it is useful to tai-
lor the thesaurus to the task. General purpose thesauruses
list words that tend to occur together in free text; we
want to find words that behave in similar ways specifi-
cally within prepositional phrases. To this end we create
a PP thesaurus using existing similarity metrics but using
a corpus consisting of automatically extracted preposi-
tional phrases.
A thesaurus alone is not sufficient to solve the PP at-
tachment problem; we also need a model of the lexi-
cal preferences of prepositional phrases. Here we use
the back-off model described in (Collins and Brooks,
1995) but with maximum likelihood estimates smoothed
using similar PPs discovered using a thesaurus. Such
similarity-based smoothing methods have been success-
fully used in other NLP applications but our use of them
here is novel. A key difference is that smoothing is not
done over individual words but over entire prepositional
phrases. Similar PPs are generated by replacing each
component word with a distributionally similar word, and
we define a similarity functions for comparing PPs. We
find that using a score based on the rank of a word in the
similarity list is more accurate than the actual similarity
scores provided by the thesaurus, which tend to weight
less similar words too highly.
In Section 2 we cover related work in PP attachment
and smoothing techniques, with a brief comparison be-
tween similarity-based smoothing and the more common
(for PP attachment) class-based smoothing. Section 3 de-
scribes Collins’ PP attachment model and our thesaurus-
based smoothing extensions. Section 4 discusses the the-
sauruses used in our experiment and describes how the
specialist thesaurus is constructed. Experimental results
are given in Section 5 and we show statistically signifi-
cant improvements over the baseline model using generic
thesauruses. Contrary to our hypothesis the specialist
thesaurus does not lead to significant improvements and
we discuss possible reasons why it underperforms on this
task.
</bodyText>
<sectionHeader confidence="0.990872" genericHeader="method">
2 Previous work
</sectionHeader>
<subsectionHeader confidence="0.968215">
2.1 PP attachment
</subsectionHeader>
<bodyText confidence="0.999958913043478">
Early work on PP attachment disambiguation used
strictly syntactic or high-level pragmatic rules to decide
on an attachment (Frazier, 1979; Altman and Steedman,
1988). However, work by Whittemore et al. (1990) and
Hindle and Rooth (1993) showed that simple lexical pref-
erences alone can deliver reasonable accuracy. Hindle
and Rooth’s approach was to use mostly unambiguous
(v, n1, p) triples extracted from automatically parsed text
to train a maximum likelihood classifier. This achieved
around 80% accuracy on ambiguous samples.
This marked a flowering in the field of PP attachment,
with a succession of papers bringing the whole armoury
of machine learning techniques to bear on the problem.
Ratnaparkhi et al. (1994) trained a maximum entropy
model on (v, n1, p, n2) quadruples extracted from the
Wall Street Journal corpus and achieved 81.6% accuracy.
The Collins and Brooks (1995) model scores 84.5% accu-
racy on this task, and is one of the most accurate models
that do not use additional supervision. The current state
of the art is 88% reported by Stetina and Nagao (1997)
using the WSJ text in conjunction with WordNet. The
next section discusses other specific approaches that in-
corporate smoothing techniques.
</bodyText>
<subsectionHeader confidence="0.998411">
2.2 Similarity-based smoothing
</subsectionHeader>
<bodyText confidence="0.9999755">
Smoothing for statistical models involves adjusting prob-
ability estimates away from the maximum likelihood es-
timates to avoid the low probabilities caused by sparse
data. Typically this involves mixing in probability distri-
butions that have less context and are less likely to suffer
from sparse data problems. For example, if the probabil-
ity of an attachment given a PP p(alv, n1, p, n2) is unde-
fined because that quadruple was not seen in the training
data, then a less specific distribution such as p(alv, n1, p)
can be used instead. A wide range of different techniques
have been proposed (Chen and Goodman, 1996) includ-
ing the backing-off technique used by Collins’ model (see
Section 3).
An alternative but complementary approach is to mix
in probabilities from distributions over “similar” con-
texts. This is the idea behind both similarity-based and
class-based smoothing. Class-based methods cluster sim-
ilar words into classes which are then used in place of ac-
tual words. For example the class-based language model
of (Brown et al., 1992) is defined as:
</bodyText>
<equation confidence="0.999966">
p(w2|w1) = p(w2|c2)p(c2|c1) (1)
</equation>
<bodyText confidence="0.999974652173913">
This helps solve the sparse data problem since the
number of classes is usually much smaller than the num-
ber of words.
Class-based methods have been applied to the PP at-
tachment task in several guises, using both automatic
clustering and hand-crafted classes such as WordNet. Li
and Abe (1998) use both WordNet and an automatic clus-
tering algorithm to achieve 85.2% accuracy on the WSJ
dataset. The maximum entropy approach of Ratnaparkhi
et al. (1994) uses the mutual information clustering algo-
rithm described in (Brown et al., 1992). Although class-
based smoothing is shown to improve the model in both
cases, some researchers have suggested that clustering
words is counterproductive since the information lost by
conflating words into broader classes outweighs the ben-
efits derived from reducing data sparseness. This remains
to be proven conclusively (Dagan et al., 1999).
In contrast, similarity-based techniques do not discard
any data. Instead the smoothed probability of a word is
defined as the total probability of all similar words S(w)
as drawn from a thesaurus, weighted by their similarity
α(w, w0). For example, the similarity-based language
model of (Dagan et al., 1999) is defined as:
</bodyText>
<equation confidence="0.9988825">
�p(w2|w1) = α(w1, w01)p(w2|w01) (2)
w&apos;1∈S(w1)
</equation>
<bodyText confidence="0.999973931034483">
where Ew&apos;1 ∈S(w1) α(w1, wi) = 1. The similarity func-
tion reflects how often the two words appear in the
same context. For example, Lin’s similarity metric (Lin,
1998b) used in this paper is based on an information-
theoretic comparison between a pair of co-occurrence
probability distributions.
This language model was incorporated into a speech
recognition system with some success (Dagan et al.,
1999). Similarity-based methods have also been suc-
cessfully applied word sense disambiguation (Dagan et
al., 1997) and extraction of grammatical relations (Gr-
ishman and Sterling, 1994). Similarity-based smooth-
ing techniques of the kind described here have not yet
been applied to probabilistic PP attachment models. The
memory-based learning approach of (Zavrel et al., 1997)
is the closest point of contact and shares many of the same
ideas, although the details are quite different. Memory-
based learning consults similar previously-seen examples
to make a decision, but the similarity judgements are usu-
ally based on a strict feature matching measure rather
than on co-occurrence statistics. Under this scheme pizza
and pasta are as different as pizza and Paris. To overcome
this Zavrel et al. also experiment with features based on a
reduced-dimensionality vector of co-occurrence statistics
and note a small (0.2%) increase in performance, leading
to a final accuracy of 84.4%.
Our use of specialist thesauruses for this task is also
novel, although in they have been used in the some-
what related field of selectional preference acquisition by
</bodyText>
<figure confidence="0.4628632">
p(a|v, n1, p, n2) =
1. f(a,v,n1,p,n2)
f(v,n1,p,n2)
2. f(a,v,n1,p)+f(a,v,p,n2)+f(a,n1,p,n2)
f(v,n1,p)+f(v,p,n2)+f(n1,p,n2)
3. f(a,v,p)+f(a,n1,p)+f(a,p,n2)
f(v,p)+f(n1,p)+f(p,n2)
4. f(a,p)
f(p)
5. Default: noun attachment
</figure>
<figureCaption confidence="0.722164666666667">
Figure 1: Collins and Brooks (1995) backing off algo-
rithm. A less specific context is used when the denomi-
nator is zero or p(a|v, n1, p, n2) = 0.5.
</figureCaption>
<bodyText confidence="0.999939">
Takenobu et. al. (1995). Different thesauruses were cre-
ated for different grammatical roles such as subject and
object, and used to build a set of word clusters. Clus-
ters based on specialist thesauruses were found to predict
fillers for these roles more accurately than generic clus-
ters.
</bodyText>
<sectionHeader confidence="0.982869" genericHeader="method">
3 Smoothing
</sectionHeader>
<bodyText confidence="0.999927">
Our baseline model is Collins and Brooks (1995) model,
which implements the popular and effective backing-
off smoothing technique. The idea is to initially use
p(a|v, n1, p, n2), but if there isn’t enough data to support
a maximum likelihood estimate of this distribution, or
p(a|v, n1, p, n2) = 0.5, then the algorithm backs off and
uses a distribution with less conditioning context. The
backing off steps are shown in Figure 1.
If we use the similarity-based language model shown
in (2) as a guide, then we can create a smoothed version
of Collins’ model using the weighted probability of all
similar PPs (for brevity we use c in to indicate the context,
in this case an entire PP quadruple):
</bodyText>
<equation confidence="0.9974665">
p(a|c) = � α(c, c0)p(a|c0) (3)
c&apos;∈S(c)
</equation>
<bodyText confidence="0.999798846153846">
In contrast to the language model shown in (2), the set
of similar contexts S(c) and similarity function α(c, c0)
must be defined for multiple words (we abuse our no-
tation slightly by using the same α and S for both
PPs and words, but the meaning should be clear from
the context). Thesauruses only supply neighbours and
similarity scores for single words, but we can gener-
ate distributionally similar PPs by replacing each word
in the phrase independently with a similar one provided
by the thesaurus. For example, if eat has two neigh-
bours: S(eat) = {drink, enjoy}, and pizza has just one:
S(pizza) = {pasta}, then the following examples will
be generated for eat pizza with fork:
</bodyText>
<construct confidence="0.9982308">
eat pasta with fork
drinkpizza with fork
drinkpasta with fork
enjoy pizza with fork
enjoy pasta with fork
</construct>
<bodyText confidence="0.999918538461538">
Clearly this strategy of generates some nonsensical or
at least unhelpful examples. This is not necessarily a se-
rious problem since such instances should occur at best
infrequently in the training data. Unfortunately our base-
line model will back off and attempt to provide a rea-
sonable probability for them all, for example by using
p(aJwith) in place of p(aJenjoy, pasta, with, fork).
This introduces unwanted noise into the smoothed prob-
ability estimate.
Our solution is to apply smoothing to the counts used
by the probability model. The smoothed frequency of
a prepositional phrase fs(a, c) is the weighted average
frequency of the set of similar PPs S(c):
</bodyText>
<equation confidence="0.995254">
Efs(a,c) = α(c, c0)f(a, c0) (4)
c&apos;∈S(c)
</equation>
<bodyText confidence="0.99908">
These smoothed frequencies are used to calculate the
conditional probabilities for the model. For example, the
probability distribution in step one is defined as:
</bodyText>
<equation confidence="0.889839">
p(aJv, n1,p, n2) =
fs(a, v, n1, p, n2)
fs(v, n1,p, n2)
</equation>
<bodyText confidence="0.999854342105263">
Distributionally similar triples are generated for step
two using the same word replacement strategy and
smoothed frequency estimates for triples are calculated
in the same way as quadruples. We back off to a smaller
amount of context if the smoothed denominator is less
than 1. This is done for empirical reasons, since de-
cisions based on very low frequency counts are unreli-
able. The distributions used in steps three and four are
not smoothed. Attempting to disambiguate a PP based
on just two words is risky enough; introducing similar
PPs found by replacing these two words with synonyms
introduces too much noise.
Quadruples and triples are more reliable since the con-
text rules out those unhelpful PPs. For example, our
model automatically deals with polysemous words with-
out the need for explicit word sense disambiguation. Al-
though thesauruses do conflate multiple senses in their
neighbour lists, implausible senses result in infrequent
PPs. The similarity set for the PP open plant in Ko-
rea might contain open tree in Korea but the latter’s fre-
quency is likely to be zero. Generating triples is riskier
since there is less context to rule out unlikely PPs: the
triple tree in Korea is more plausible and possibly mis-
leading. But our model does have a natural preference
for the most frequent sense in the thesaurus training cor-
pus, which is a useful heuristic for word sense disam-
biguation (Pedersen and Bruce, 1997). For example, if
the thesaurus is trained on business text then factory will
be ranked higher than tree when the thesaurus trained on
a business corpus (this issue is discussed further in Sec-
tion 5.2).
Finally, to complete our PP attachment scheme we
need to define a similarity function between PPs, ex-
pressed fully as α ((v, n1, p, n2), (v0, ni, p0, n02)). The
raw materials we have to work with are the similarity
scores for matching pairs of verbs and nouns as given by
the thesaurus. We do not smooth preposition counts. In
this paper we compare three similarity measures:
</bodyText>
<listItem confidence="0.838077833333333">
• average: The average similarity score of all word
pairs in the PP using the similarity measure pro-
vided by the thesaurus. For example, α(c, c0)
when c = (eat, pizza, with, fork) and c0 =
(enjoy, pasta, with, fork) is defined as:
13α(eat,enjoy)+α(pizza,pasta)+α(fork, fork)
</listItem>
<bodyText confidence="0.9927505">
The similarity score of identical words is assumed
to be 1.
</bodyText>
<listItem confidence="0.9020155">
• rank: The rank score of the nth neighbour w0 of a
word w is defined as:
</listItem>
<equation confidence="0.867918">
rs(w, w0) = 0n
</equation>
<bodyText confidence="0.999883333333333">
where 0 &lt; 0 &lt; 1. The rank similarity scores
for the pizza example above when 0 = 0.1 are
rs(eat, enjoy) = 0.2 and rs(pizza, pasta) = 0.1.
The combined score for a PP is found by summing
the rank score for each word pair and subtracting this
total from one:
</bodyText>
<equation confidence="0.9975555">
Eα(c, c0) = 1 − rs(w, w0)
w∈v,n1,n2
</equation>
<bodyText confidence="0.998546428571429">
We impose a floor of zero on this score. Con-
tinuing with the pizza example, the rank simi-
larity score between (eat, pizza, with, fork) and
(enjoy, pasta, with, fork) is α(c, c0) = 1 − 0.2 −
0.1 = 0.7. Note that the similarity score provided by
the thesaurus is used to determine the ranking but it
otherwise not used.
</bodyText>
<listItem confidence="0.995277125">
• single best: Instead of smoothing using several sim-
ilar contexts, we can set α(c, c0) = 1 for the clos-
est context for which f(c0) &gt; 0 and ignore all oth-
ers, thereby just replacing an unknown feature with a
similar known one. This simplified form of smooth-
ing may be appropriate for non-statistical models
or situations where relative frequency estimates are
hard to incorporate.
</listItem>
<sectionHeader confidence="0.993196" genericHeader="method">
4 Thesauruses
</sectionHeader>
<bodyText confidence="0.999908476190476">
As noted above, a thesaurus is a resource that groups to-
gether words that are distributionally similar. Although
we refer to such resources using the singular, a thesaurus
has several parts for different word categories such as
nouns, verbs and adjectives.
We compare three thesauruses on this task. The first
two are large-scale generic thesauruses, both constructed
using the similarity metric described in (Lin, 1998b), but
based on different corpora. The first, which we call Lin,
is derived from 300 million words of newswire text and
is available on the Internet1. The second, which we call
WASPS, forms part of the WASPS lexicographical work-
bench developed at Brighton University 2 and is derived
from the 100 million word BNC. The co-occurrence re-
lations for both are a variety of grammatical relations
such as direct object, subject and modifier. WASPS also
includes prepositional phrase relations but without at-
tempting to disambiguate them. All possible attachments
are included under the assumption that correct attach-
ments will tend to have higher frequency (Adam Kilgar-
riff, p.c.).
These thesauruses are designed to find words that are
similar in a very general sense, and are often compared
against hand-crafted semantic resources such as Word-
Net. However for the PP attachment task semantic sim-
ilarity may be less important. We are more interested in
how words behave in particular syntactic roles. For exam-
ple, eat and bake are rather loosely related semantically
but will be close neighbours in PP terms if they both of-
ten occur with prepositional phrase contexts such as pizza
with anchovies.
The third thesaurus is designed to supply such spe-
cialised, task-specific neighbours. It consists of three
sub-thesauruses, one for the each of the v, n1 and n2
words in the PP (a preposition thesaurus was also con-
structed with plausible-looking neighbours but was found
not to be useful in practice). The co-occurrence relations
used in each case consist of all possible subsets of the
three remaining words together with the attachment deci-
sion. For example, given eat pizza with fork the following
co-occurrences will be included in the thesaurus training
corpus:
</bodyText>
<equation confidence="0.998306285714286">
eat – n1-pizza,p-with,n2-fork,N
eat – n1-pizza,p-with,N
eat – n1-pizza,n2-fork,N
eat – p-with,n2-fork,N
eat – n1-pizza,N
eat – p-with,N
eat – n2-fork,N
</equation>
<footnote confidence="0.9161885">
1http://www.cs.ualberta.ca/ lindek/downloads.htm
2http://wasps.itri.brighton.ac.uk
</footnote>
<bodyText confidence="0.999907882352941">
The training corpus is created from 3.3 million prepo-
sitional phrases extracted from the British National Cor-
pus. These PPs are identified semi-automatically using a
version of the weighted GR extraction scheme described
in (Carroll and Briscoe, 2001). The raw text is parsed
and any PPs that occur in a large percentage of the highly
ranked candidate parses are considered reliable and added
to the thesaurus training corpus. Mostly these are unam-
biguous (v, p, n1) or (n1, p, n2) triples from phrases such
as we met in January. The dataset is rather noisy due
to tagging and parsing errors, so we discarded any co-
occurrence relations occurring fewer than 100 times.
We use the similarity metric described in Weeds
(2003). This is a parameterised measure that can be ad-
justed to suit different tasks, but to ensure compatibility
with the two generic thesauruses we chose parameter set-
tings that mimic Lin’s measure.
</bodyText>
<sectionHeader confidence="0.998729" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999862923076923">
For our experiments we use the Wall Street Journal
dataset created by Ratnaparkhi et al. (1994). This is
divided into a training set of 20,801 words, a develop-
ment set of 4,039 words and a test set of 3,097 words.
Each word was reduced to its morphological root using
the morphological analyser described in (Minnen et al.,
2000). Strings of four digits beginning with a 1 or 2
are replaced with YEAR and all other digit strings in-
cluding those including commas and full stops were re-
placed with NUM. Our implementation of Collins’ algo-
rithm only achieves 84.3% on the test data, with the short-
fall of 0.2% primarily due to the different morphological
analysers used3
</bodyText>
<subsectionHeader confidence="0.992201">
5.1 Smoothing
</subsectionHeader>
<bodyText confidence="0.999339875">
Firstly we compare the different PP similarity functions.
Figure 2 shows the accuracy of each as a function of
k, the number of examples in S(c) . The WASPS the-
saurus was used in all cases. The best smoothed model is
rank with 85.1% accuracy when 0 = 0.05 and k = 15.
The accuracy of rank with the smallest 0 value drops off
rapidly when k &gt; 10, showing that neighbours beyond
this point are providing unreliable evidence and should
be discounted more aggressively. More interestingly, this
problem also affects average, suggesting that the similar-
ity scores provided by the thesaurus are also misleadingly
high for less similar words. The same effect was also ob-
served when we used the harmonic mean of all similarity
scores, so it is unlikely that the problem is an artifact of
the averaging operation.
On the other hand, if 0 is set quite low (for example
</bodyText>
<footnote confidence="0.480282333333333">
3This result is interesting since this analyser is more accurate
than the one used by Collins. We chose to use this analyser
because it matches the word forms in the thesauruses better.
</footnote>
<figure confidence="0.9740745">
0 5 10 15 20 25
num. neighbours (k)
</figure>
<figureCaption confidence="0.994461">
Figure 2: Accuracy for different smoothing functions on
</figureCaption>
<bodyText confidence="0.982113076923077">
the development set plotted against k, the number of sim-
ilar words used for smoothing
0 = 0.01) then accuracy levels off very quickly as less
similar neighbours are assigned zero frequency. The mid-
dle value of 0 = 0.05 appears to offer a good trade-off.
Regardless of the similarity function we can see that rel-
atively small values for k are sufficient, which is good
news for efficiency reasons (each attachment decision is
an O(k) operation).
Figure 3 shows the combined coverage of the triple
and quadruple features in Collins’ model, which are the
only smoothed features in our model. For example, al-
most 75% of attachment decisions are resolved by 3- or
4-tuples using the average function and setting k = 25.
Again, average is comparable to rank with 0 = 0.01.
Table 1 compares the accuracy of the smoothed and un-
smoothed models at each backing off stage. Smoothing
has a negative effect on accuracy, but this is made for by
an increase in accuracy.
The reduction in the error rate with the single best pol-
icy on the development set is somewhat less than with the
smoothed frequency models, and the results more error-
prone and sensitive to the choice of k. These models
are more likely to be unlucky with a choice of feature
than with the smoothed frequencies. As noted above, this
technique may still be useful for algorithms which cannot
</bodyText>
<table confidence="0.998562666666667">
Stage Smoothed Unsm.
Acc. Cov. Acc. Cov.
1 90.9 12.4 91.2 8.5
2 87.3 49.7 87.5 33.5
3 80.8 34.2 82.1 54.2
4 73.4 3.6 73.9 3.7
</table>
<tableCaption confidence="0.819208666666667">
Table 1: Accuracy and coverage of the first two backing
off stages on the development data. The smoothed model
uses WASPS with 0 = 0.5 and k = 5.
</tableCaption>
<figure confidence="0.9686685">
0 5 10 15 20 25
num. neighbours (k)
</figure>
<figureCaption confidence="0.9911175">
Figure 3: Coverage for different smoothing functions
against the number of neighbours used for smoothing
</figureCaption>
<bodyText confidence="0.868228">
easily incorporate smoothed frequency estimates.
</bodyText>
<subsectionHeader confidence="0.986668">
5.2 Thesauruses
</subsectionHeader>
<bodyText confidence="0.999940130434783">
A thesaurus providing better neighbours should do better
on this task. Figure 4 shows the accuracy of the three the-
sauruses using rank smoothing and 0 = 0.05 on the de-
velopment data. Final results using k = 5 and 0 = 0.05
on the data is shown in Table 2, together with the size of
the noun sections of each thesaurus (the direct object the-
saurus in the case of specialist) and coverage of 3- and
4-tuples.
Clearly both generic thesauruses consistently outper-
form the specialist thesaurus. The latter tends to pro-
duce neighbours with have less obvious semantic simi-
larity, for example providing pour as the first neighbour
offetch. We hypothesised that using syntactic rather than
semantic neighbours could be desirable, but in this case it
often generates contexts that are unlikely to occur: pour
price ofprofit as a neighbour offetch price ofprofit, for
example. Although this may be a flaw in the approach,
we may simply be using too few contexts to create a re-
liable thesaurus. Previous research has found that using
more data leads to better quality thesauruses (Curran and
Moens, 2002). We are also conflating attachment prefer-
ences, since a word must appear with similar contexts in
both noun and verb modifying PPs to achieve a high sim-
</bodyText>
<table confidence="0.9975748">
Thesaurus Acc. Size (N) Cov.
None 84.30 - 30.5
Lin 85.02 13,850 72.1
WASPS 85.05 17,843 60.1
Specialist 84.50 5,669 61.0
</table>
<tableCaption confidence="0.895182">
Table 2: Accuracy on the test data using 0 = 0.05 and
k = 5; the size of the noun section of each thesaurus, and
coverage of smoothed 4- and 3-tuples
</tableCaption>
<figure confidence="0.992645666666667">
85.2
85
84.8
84.6
�
u
�
Accuracy
84.4
84.2
84
rank 0.01
rank 0.05
rank 0.1
average
single
83.8
83.6
83.4
75
rank 0.01
rank 0.05
rank 0.1
average
single
60
55
Coverage
�
e
50
45
40
35
30
70
65
0 5 10 15 20 25
num. neighbours (k)
</figure>
<figureCaption confidence="0.9296095">
Figure 4: Accuracy of the three different thesauruses on
the development set using rank smoothing with β = 0.05
</figureCaption>
<table confidence="0.9933932">
Method Accuracy WN?
Zavrel et. al. (1997) 84.1 No
WASPS 85.1 No
Li &amp; Abe (1998) 85.2 Yes
Stetina &amp; Nagao (1997) 88.1 Yes
</table>
<tableCaption confidence="0.814442">
Table 3: Accuracy of various attachment models using
WordNet or automatic clustering algorithms
</tableCaption>
<bodyText confidence="0.976250571428572">
ilarity score. There may be merit in creating separate the-
sauruses for noun-attachment and verb-attachment, since
there may be words that are strongly similar in only one
of these cases.
Interestingly, although Lin is smaller than WASPS it
has better coverage. This is most likely due to the differ-
ent corpora used to construct each thesaurus. Lin is built
using newswire text which is closer in genre to the Wall
Street Journal. For example, the first neighbour for fetch
in WASPS is grab, but none of the top 25 neighbours of
this word in Lin have this sporting sense. Both WASPS
and specialist are derived from the BNC and have similar
coverage, although the quality of specialist neighbours is
not as good.
The WASPS and Lin models produce statistically
significant (P &lt; 0.05) improvements over the vanilla
Collins model using a paired t-test with 10-fold cross-
validation on the entire dataset4. The specialist model is
not significantly better. Table 3 compares our results with
other comparable PP attachment models.
On the face of it, these are not resounding improve-
ments over the baseline, but this is a very hard task.
Ratnaparkhi (1994) established a human upper bound of
88.2% but subsequent research has put this as low as
78.3% (Mitchell, 2003). At least two thirds of the re-
4The Collins model achieves 84.50±1.0% accuracy and the
smoothed model 84.90±1.0% accuracy by this measure.
maining errors are therefore likely to be very difficult.
An inspection of the data shows that many of the re-
maining errors are due to poor neighbouring PPs be-
ing used for smoothing. For example, the PP in entrust
company with cash modifies the verb, but no matching
quadruples are present in the training data. The only
matching (n1, p, n2) triple using WASPS is (industry, for,
income), which appears twice in the training data modi-
fying the noun. The model therefore guesses incorrectly
even though the thesaurus is providing what appear to be
semantically appropriate neighbours. Another example is
attend meeting with representative, where the (v, p, n2)
triple (talk, with, official) convinces the model to incor-
rectly guess verb attachment.
Part of the problem is that words in the PP are replaced
independently and without consideration to the remaining
context. However we had hoped the specialist thesaurus
might alleviate this problem by providing neighbours that
are more appropriate for this specific task. Finding good
neighbours for verbs is clearly more difficult than for
nouns since subcategorisation and selectional preferences
also play a role.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999655">
Our results show that the similarity-based smoothing of
frequency estimates significantly improves an already re-
spectable probabilistic PP attachment model. However
our hypothesis that a task-specific thesaurus would out-
perform a generic thesaurus was not borne out by our
experiments. The neighbours provided by the specialist
thesaurus are not as informative as those supplied by the
generic thesauruses. Of course, this negative result is nat-
urally good news for developers of generic thesauruses.
We described ways of finding and scoring distribution-
ally similar PPs. A significant number of errors in the
final model can be traced to the way individual words in
the PP are replaced without regard to the wider context,
producing neighbouring PPs that have conflicting attach-
ment preferences. The specialist thesaurus was not able
to overcome this problem. A second finding is that dis-
tributional similarity scores provided by all thesauruses
weight dissimilar neighbours too highly, and more ag-
gressive weighting schemes are better for smoothing.
Our aim is to apply similarity-based smoothing with
both generic and specialist thesauruses to other areas in
lexicalised parse selection, particularly other overtly lex-
ical problems such as noun-noun modifiers and conjunc-
tion scope. Lexical information has a lot of promise for
parse selection in theory, but there are practical problems
such as sparse data and genre effects (Gildea, 2001). Ap-
propriately trained thesauruses and similarity-based tech-
niques should help to alleviate both problems.
</bodyText>
<figure confidence="0.960232615384615">
85.2
84.8
84.6
84.4
84.2
85
84
WASPS
Lin
Specialist
Accuracy
�
u
</figure>
<sectionHeader confidence="0.987455" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999625">
Many thanks to Julie Weeds and Adam Kilgarriff for pro-
viding the specialist and WASPS thesauruses, and for
useful discussions. Thanks also to the anonymous re-
viewers for many helpful comments.
</bodyText>
<sectionHeader confidence="0.996041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99909487804878">
Gerry Altman and Mark Steedman. 1988. Interaction
with context during human sentence processing. Cog-
nition, 30:191–238.
Edward Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic lr parser of part-of-speech
and punctuation labels. In Proceedings of the IWPT
’95, pages 48–58.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):468–479.
John Carroll and Ted Briscoe. 2001. High precision ex-
traction of grammatical relations. In Proceedings of
the IWPT ’01.
Stanley Chen and Joshua Goodman. 1996. An empirical
study of smoothing techniques for language modelling.
In Proceedings ofACL ’96, pages 310–318.
Michael Collins and James Brooks. 1995. Prepositional
phrase attachment through a backed-offmodel. In Pro-
ceedings of EMNLP ’95, pages 27–38.
James Curran and Mark Moens. 2002. Scaling context
space. In Proceedings of the ACL ’02, pages 222–229.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disambigua-
tion. In Proceedings ofACL ’97, pages 56–63.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence prob-
abilities. Machine Learning, 34(1-3):43–69.
L. Frazier. 1979. On comprehending sentences: Syn-
actic parsing strategies. Ph.D. thesis, University of
Connecticut.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of EMNLP ’01, Pittsburgh,
PA.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of COLING ’94, pages 742–747.
Don Hindle and Mats Rooth. 1993. Structural ambigu-
ity and lexical relations. Computational Linguistics,
19:103–120.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalised parsing. In Proceedings ofACL ’03.
Hang Li and Naoki Abe. 1998. Word clustering and
disambiguation based on co-occurrence data. In Pro-
ceedings of COLING ’98, pages 749–755.
Dekang Lin. 1998a. Dependency-based evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems.
Dekang Lin. 1998b. An information-theoretic measure
of similarity. In Proceedings ofICML ’98, pages 296–
304.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings ofINLG 2000, pages 201–208.
Brian Mitchell. 2003. Prepositional phrase attachment
using machine learning algorithms. Ph.D. thesis, Uni-
versity of Sheffield.
Ted Pedersen and Rebecca Bruce. 1997. Distinguish-
ing word senses in untagged text. In Proceedings of
EMNLP ’97.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of the ARPA Work-
shop on Human Language Technology, pages 250–
255.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP
attachment ambiguity resolution with a semantic dic-
tionary. In Proceedings of WVLC ’97, pages 66–80.
Tokunaga Takenobu, Iwayama Makoto, and Tanaka
Hozumi. 1995. Automatic thesaurus construction
based on grammatical relations. In Proceedings of IJ-
CAI ’95, pages 1308–1313.
Julie Weeds. 2003. A general framework for distribu-
tional similarity. In Proceedings of the EMNLP ’03.
G. Whittemore, K. Ferrara, and H. Brunner. 1990. Em-
pirical study of predictive powers of simple attachment
schemes for post-modifier prepositional phrases. In
Proceedings ofACL ’90, pages 23–30.
Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1997. Resolving PP attachment ambiguities with
memory-based learning. In Proceedings of CoNLL
’97, pages 136–144.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928260">
<title confidence="0.99936">Thesauruses for Prepositional Phrase Attachment</title>
<author confidence="0.99999">Mark McLauchlan</author>
<affiliation confidence="0.9999005">Department of Informatics University of Sussex</affiliation>
<address confidence="0.984078">Brighton, BN1 9RH</address>
<email confidence="0.991013">mrm21@sussex.ac.uk</email>
<abstract confidence="0.99760845">Probabilistic models have been effective in resolving prepositional phrase attachment ambiguity, but sparse data remains a significant problem. We propose a solution based on similarity-based smoothing, where the probability of new PPs is estimated with information from similar examples generated using a thesaurus. Three thesauruses are compared on this task: two existing generic thesauruses and a new specialist PP thesaurus tailored for this problem. We also compare three smoothing techniques for prepositional phrases. We find that the similarity scores provided by the thesaurus tend to weight distant neighbours too highly, and describe a better score based on the rank of a word in the list of similar words. Our smoothing methods are applied to an existing PP attachment model and we obtain significant improvements over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerry Altman</author>
<author>Mark Steedman</author>
</authors>
<title>Interaction with context during human sentence processing.</title>
<date>1988</date>
<journal>Cognition,</journal>
<pages>30--191</pages>
<contexts>
<context position="6919" citStr="Altman and Steedman, 1988" startWordPosition="1101" endWordPosition="1104"> Section 4 discusses the thesauruses used in our experiment and describes how the specialist thesaurus is constructed. Experimental results are given in Section 5 and we show statistically significant improvements over the baseline model using generic thesauruses. Contrary to our hypothesis the specialist thesaurus does not lead to significant improvements and we discuss possible reasons why it underperforms on this task. 2 Previous work 2.1 PP attachment Early work on PP attachment disambiguation used strictly syntactic or high-level pragmatic rules to decide on an attachment (Frazier, 1979; Altman and Steedman, 1988). However, work by Whittemore et al. (1990) and Hindle and Rooth (1993) showed that simple lexical preferences alone can deliver reasonable accuracy. Hindle and Rooth’s approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train a maximum likelihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maximum entropy model on (v, n1, </context>
</contexts>
<marker>Altman, Steedman, 1988</marker>
<rawString>Gerry Altman and Mark Steedman. 1988. Interaction with context during human sentence processing. Cognition, 30:191–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Developing and evaluating a probabilistic lr parser of part-of-speech and punctuation labels.</title>
<date>1995</date>
<booktitle>In Proceedings of the IWPT ’95,</booktitle>
<pages>48--58</pages>
<contexts>
<context position="1492" citStr="Briscoe and Carroll, 1995" startWordPosition="226" endWordPosition="229">r words. Our smoothing methods are applied to an existing PP attachment model and we obtain significant improvements over the baseline. 1 Introduction Prepositional phrases are an interesting example of syntactic ambiguity and a challenge for automatic parsers. The ambiguity arises whenever a prepositional phrase can modify a preceding verb or noun, as in the canonical example I saw the man with the telescope. In syntactic terms, the prepositional phrase attaches either to the noun phrase or the verb phrase. Many kinds of syntactic ambiguity can be resolved using structural information alone (Briscoe and Carroll, 1995; Lin, 1998a; Klein and Manning, 2003), but in this case both candidate structures are perfectly grammatical and roughly equally likely. Therefore ambiguous prepositional phrases require some kind of additional context to disambiguate correctly. In some cases a small amount of lexical knowledge is sufficient: for example of almost always modifies the noun. Other cases, such as the telescope example, are potentially much harder since discourse or world knowledge might be required. Fortunately it is possible to do well at this task just by considering the lexical preferences of the words making </context>
</contexts>
<marker>Briscoe, Carroll, 1995</marker>
<rawString>Edward Briscoe and John Carroll. 1995. Developing and evaluating a probabilistic lr parser of part-of-speech and punctuation labels. In Proceedings of the IWPT ’95, pages 48–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, Jenifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):468–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
</authors>
<title>High precision extraction of grammatical relations.</title>
<date>2001</date>
<booktitle>In Proceedings of the IWPT ’01.</booktitle>
<contexts>
<context position="21106" citStr="Carroll and Briscoe, 2001" startWordPosition="3422" endWordPosition="3425">attachment decision. For example, given eat pizza with fork the following co-occurrences will be included in the thesaurus training corpus: eat – n1-pizza,p-with,n2-fork,N eat – n1-pizza,p-with,N eat – n1-pizza,n2-fork,N eat – p-with,n2-fork,N eat – n1-pizza,N eat – p-with,N eat – n2-fork,N 1http://www.cs.ualberta.ca/ lindek/downloads.htm 2http://wasps.itri.brighton.ac.uk The training corpus is created from 3.3 million prepositional phrases extracted from the British National Corpus. These PPs are identified semi-automatically using a version of the weighted GR extraction scheme described in (Carroll and Briscoe, 2001). The raw text is parsed and any PPs that occur in a large percentage of the highly ranked candidate parses are considered reliable and added to the thesaurus training corpus. Mostly these are unambiguous (v, p, n1) or (n1, p, n2) triples from phrases such as we met in January. The dataset is rather noisy due to tagging and parsing errors, so we discarded any cooccurrence relations occurring fewer than 100 times. We use the similarity metric described in Weeds (2003). This is a parameterised measure that can be adjusted to suit different tasks, but to ensure compatibility with the two generic </context>
</contexts>
<marker>Carroll, Briscoe, 2001</marker>
<rawString>John Carroll and Ted Briscoe. 2001. High precision extraction of grammatical relations. In Proceedings of the IWPT ’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modelling.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL ’96,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="8624" citStr="Chen and Goodman, 1996" startWordPosition="1377" endWordPosition="1380">thing Smoothing for statistical models involves adjusting probability estimates away from the maximum likelihood estimates to avoid the low probabilities caused by sparse data. Typically this involves mixing in probability distributions that have less context and are less likely to suffer from sparse data problems. For example, if the probability of an attachment given a PP p(alv, n1, p, n2) is undefined because that quadruple was not seen in the training data, then a less specific distribution such as p(alv, n1, p) can be used instead. A wide range of different techniques have been proposed (Chen and Goodman, 1996) including the backing-off technique used by Collins’ model (see Section 3). An alternative but complementary approach is to mix in probabilities from distributions over “similar” contexts. This is the idea behind both similarity-based and class-based smoothing. Class-based methods cluster similar words into classes which are then used in place of actual words. For example the class-based language model of (Brown et al., 1992) is defined as: p(w2|w1) = p(w2|c2)p(c2|c1) (1) This helps solve the sparse data problem since the number of classes is usually much smaller than the number of words. Cla</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modelling. In Proceedings ofACL ’96, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-offmodel.</title>
<date>1995</date>
<booktitle>In Proceedings of EMNLP ’95,</booktitle>
<pages>27--38</pages>
<contexts>
<context position="5306" citStr="Collins and Brooks, 1995" startWordPosition="852" endWordPosition="855"> In this paper we examine whether it is useful to tailor the thesaurus to the task. General purpose thesauruses list words that tend to occur together in free text; we want to find words that behave in similar ways specifically within prepositional phrases. To this end we create a PP thesaurus using existing similarity metrics but using a corpus consisting of automatically extracted prepositional phrases. A thesaurus alone is not sufficient to solve the PP attachment problem; we also need a model of the lexical preferences of prepositional phrases. Here we use the back-off model described in (Collins and Brooks, 1995) but with maximum likelihood estimates smoothed using similar PPs discovered using a thesaurus. Such similarity-based smoothing methods have been successfully used in other NLP applications but our use of them here is novel. A key difference is that smoothing is not done over individual words but over entire prepositional phrases. Similar PPs are generated by replacing each component word with a distributionally similar word, and we define a similarity functions for comparing PPs. We find that using a score based on the rank of a word in the similarity list is more accurate than the actual sim</context>
<context position="7641" citStr="Collins and Brooks (1995)" startWordPosition="1215" endWordPosition="1218">l preferences alone can deliver reasonable accuracy. Hindle and Rooth’s approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train a maximum likelihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieved 81.6% accuracy. The Collins and Brooks (1995) model scores 84.5% accuracy on this task, and is one of the most accurate models that do not use additional supervision. The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. The next section discusses other specific approaches that incorporate smoothing techniques. 2.2 Similarity-based smoothing Smoothing for statistical models involves adjusting probability estimates away from the maximum likelihood estimates to avoid the low probabilities caused by sparse data. Typically this involves mixing in probability distributions tha</context>
<context position="12150" citStr="Collins and Brooks (1995)" startWordPosition="1916" endWordPosition="1919">his Zavrel et al. also experiment with features based on a reduced-dimensionality vector of co-occurrence statistics and note a small (0.2%) increase in performance, leading to a final accuracy of 84.4%. Our use of specialist thesauruses for this task is also novel, although in they have been used in the somewhat related field of selectional preference acquisition by p(a|v, n1, p, n2) = 1. f(a,v,n1,p,n2) f(v,n1,p,n2) 2. f(a,v,n1,p)+f(a,v,p,n2)+f(a,n1,p,n2) f(v,n1,p)+f(v,p,n2)+f(n1,p,n2) 3. f(a,v,p)+f(a,n1,p)+f(a,p,n2) f(v,p)+f(n1,p)+f(p,n2) 4. f(a,p) f(p) 5. Default: noun attachment Figure 1: Collins and Brooks (1995) backing off algorithm. A less specific context is used when the denominator is zero or p(a|v, n1, p, n2) = 0.5. Takenobu et. al. (1995). Different thesauruses were created for different grammatical roles such as subject and object, and used to build a set of word clusters. Clusters based on specialist thesauruses were found to predict fillers for these roles more accurately than generic clusters. 3 Smoothing Our baseline model is Collins and Brooks (1995) model, which implements the popular and effective backingoff smoothing technique. The idea is to initially use p(a|v, n1, p, n2), but if th</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional phrase attachment through a backed-offmodel. In Proceedings of EMNLP ’95, pages 27–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Mark Moens</author>
</authors>
<title>Scaling context space.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL ’02,</booktitle>
<pages>222--229</pages>
<contexts>
<context position="26520" citStr="Curran and Moens, 2002" startWordPosition="4371" endWordPosition="4374">pecialist thesaurus. The latter tends to produce neighbours with have less obvious semantic similarity, for example providing pour as the first neighbour offetch. We hypothesised that using syntactic rather than semantic neighbours could be desirable, but in this case it often generates contexts that are unlikely to occur: pour price ofprofit as a neighbour offetch price ofprofit, for example. Although this may be a flaw in the approach, we may simply be using too few contexts to create a reliable thesaurus. Previous research has found that using more data leads to better quality thesauruses (Curran and Moens, 2002). We are also conflating attachment preferences, since a word must appear with similar contexts in both noun and verb modifying PPs to achieve a high simThesaurus Acc. Size (N) Cov. None 84.30 - 30.5 Lin 85.02 13,850 72.1 WASPS 85.05 17,843 60.1 Specialist 84.50 5,669 61.0 Table 2: Accuracy on the test data using 0 = 0.05 and k = 5; the size of the noun section of each thesaurus, and coverage of smoothed 4- and 3-tuples 85.2 85 84.8 84.6 � u � Accuracy 84.4 84.2 84 rank 0.01 rank 0.05 rank 0.1 average single 83.8 83.6 83.4 75 rank 0.01 rank 0.05 rank 0.1 average single 60 55 Coverage � e 50 45</context>
</contexts>
<marker>Curran, Moens, 2002</marker>
<rawString>James Curran and Mark Moens. 2002. Scaling context space. In Proceedings of the ACL ’02, pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based methods for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL ’97,</booktitle>
<pages>56--63</pages>
<contexts>
<context position="10861" citStr="Dagan et al., 1997" startWordPosition="1728" endWordPosition="1731">ty-based language model of (Dagan et al., 1999) is defined as: �p(w2|w1) = α(w1, w01)p(w2|w01) (2) w&apos;1∈S(w1) where Ew&apos;1 ∈S(w1) α(w1, wi) = 1. The similarity function reflects how often the two words appear in the same context. For example, Lin’s similarity metric (Lin, 1998b) used in this paper is based on an informationtheoretic comparison between a pair of co-occurrence probability distributions. This language model was incorporated into a speech recognition system with some success (Dagan et al., 1999). Similarity-based methods have also been successfully applied word sense disambiguation (Dagan et al., 1997) and extraction of grammatical relations (Grishman and Sterling, 1994). Similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic PP attachment models. The memory-based learning approach of (Zavrel et al., 1997) is the closest point of contact and shares many of the same ideas, although the details are quite different. Memorybased learning consults similar previously-seen examples to make a decision, but the similarity judgements are usually based on a strict feature matching measure rather than on co-occurrence statistics. Under this scheme pi</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1997. Similarity-based methods for word sense disambiguation. In Proceedings ofACL ’97, pages 56–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="9978" citStr="Dagan et al., 1999" startWordPosition="1591" endWordPosition="1594">s such as WordNet. Li and Abe (1998) use both WordNet and an automatic clustering algorithm to achieve 85.2% accuracy on the WSJ dataset. The maximum entropy approach of Ratnaparkhi et al. (1994) uses the mutual information clustering algorithm described in (Brown et al., 1992). Although classbased smoothing is shown to improve the model in both cases, some researchers have suggested that clustering words is counterproductive since the information lost by conflating words into broader classes outweighs the benefits derived from reducing data sparseness. This remains to be proven conclusively (Dagan et al., 1999). In contrast, similarity-based techniques do not discard any data. Instead the smoothed probability of a word is defined as the total probability of all similar words S(w) as drawn from a thesaurus, weighted by their similarity α(w, w0). For example, the similarity-based language model of (Dagan et al., 1999) is defined as: �p(w2|w1) = α(w1, w01)p(w2|w01) (2) w&apos;1∈S(w1) where Ew&apos;1 ∈S(w1) α(w1, wi) = 1. The similarity function reflects how often the two words appear in the same context. For example, Lin’s similarity metric (Lin, 1998b) used in this paper is based on an informationtheoretic comp</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<title>On comprehending sentences: Synactic parsing strategies.</title>
<date>1979</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Connecticut.</institution>
<contexts>
<context position="6891" citStr="Frazier, 1979" startWordPosition="1099" endWordPosition="1100">ing extensions. Section 4 discusses the thesauruses used in our experiment and describes how the specialist thesaurus is constructed. Experimental results are given in Section 5 and we show statistically significant improvements over the baseline model using generic thesauruses. Contrary to our hypothesis the specialist thesaurus does not lead to significant improvements and we discuss possible reasons why it underperforms on this task. 2 Previous work 2.1 PP attachment Early work on PP attachment disambiguation used strictly syntactic or high-level pragmatic rules to decide on an attachment (Frazier, 1979; Altman and Steedman, 1988). However, work by Whittemore et al. (1990) and Hindle and Rooth (1993) showed that simple lexical preferences alone can deliver reasonable accuracy. Hindle and Rooth’s approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train a maximum likelihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maxim</context>
</contexts>
<marker>Frazier, 1979</marker>
<rawString>L. Frazier. 1979. On comprehending sentences: Synactic parsing strategies. Ph.D. thesis, University of Connecticut.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP ’01,</booktitle>
<location>Pittsburgh, PA.</location>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of EMNLP ’01, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>John Sterling</author>
</authors>
<title>Generalizing automatically generated selectional patterns.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING ’94,</booktitle>
<pages>742--747</pages>
<contexts>
<context position="10931" citStr="Grishman and Sterling, 1994" startWordPosition="1737" endWordPosition="1741">: �p(w2|w1) = α(w1, w01)p(w2|w01) (2) w&apos;1∈S(w1) where Ew&apos;1 ∈S(w1) α(w1, wi) = 1. The similarity function reflects how often the two words appear in the same context. For example, Lin’s similarity metric (Lin, 1998b) used in this paper is based on an informationtheoretic comparison between a pair of co-occurrence probability distributions. This language model was incorporated into a speech recognition system with some success (Dagan et al., 1999). Similarity-based methods have also been successfully applied word sense disambiguation (Dagan et al., 1997) and extraction of grammatical relations (Grishman and Sterling, 1994). Similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic PP attachment models. The memory-based learning approach of (Zavrel et al., 1997) is the closest point of contact and shares many of the same ideas, although the details are quite different. Memorybased learning consults similar previously-seen examples to make a decision, but the similarity judgements are usually based on a strict feature matching measure rather than on co-occurrence statistics. Under this scheme pizza and pasta are as different as pizza and Paris. To overcome this Za</context>
</contexts>
<marker>Grishman, Sterling, 1994</marker>
<rawString>Ralph Grishman and John Sterling. 1994. Generalizing automatically generated selectional patterns. In Proceedings of COLING ’94, pages 742–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<booktitle>Computational Linguistics,</booktitle>
<pages>19--103</pages>
<contexts>
<context position="6990" citStr="Hindle and Rooth (1993)" startWordPosition="1113" endWordPosition="1116">how the specialist thesaurus is constructed. Experimental results are given in Section 5 and we show statistically significant improvements over the baseline model using generic thesauruses. Contrary to our hypothesis the specialist thesaurus does not lead to significant improvements and we discuss possible reasons why it underperforms on this task. 2 Previous work 2.1 PP attachment Early work on PP attachment disambiguation used strictly syntactic or high-level pragmatic rules to decide on an attachment (Frazier, 1979; Altman and Steedman, 1988). However, work by Whittemore et al. (1990) and Hindle and Rooth (1993) showed that simple lexical preferences alone can deliver reasonable accuracy. Hindle and Rooth’s approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train a maximum likelihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and ach</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Don Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19:103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalised parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL ’03.</booktitle>
<contexts>
<context position="1530" citStr="Klein and Manning, 2003" startWordPosition="232" endWordPosition="235">ied to an existing PP attachment model and we obtain significant improvements over the baseline. 1 Introduction Prepositional phrases are an interesting example of syntactic ambiguity and a challenge for automatic parsers. The ambiguity arises whenever a prepositional phrase can modify a preceding verb or noun, as in the canonical example I saw the man with the telescope. In syntactic terms, the prepositional phrase attaches either to the noun phrase or the verb phrase. Many kinds of syntactic ambiguity can be resolved using structural information alone (Briscoe and Carroll, 1995; Lin, 1998a; Klein and Manning, 2003), but in this case both candidate structures are perfectly grammatical and roughly equally likely. Therefore ambiguous prepositional phrases require some kind of additional context to disambiguate correctly. In some cases a small amount of lexical knowledge is sufficient: for example of almost always modifies the noun. Other cases, such as the telescope example, are potentially much harder since discourse or world knowledge might be required. Fortunately it is possible to do well at this task just by considering the lexical preferences of the words making up the PP. Lexical preferences describ</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalised parsing. In Proceedings ofACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Word clustering and disambiguation based on co-occurrence data.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING ’98,</booktitle>
<pages>749--755</pages>
<contexts>
<context position="9395" citStr="Li and Abe (1998)" startWordPosition="1501" endWordPosition="1504">tributions over “similar” contexts. This is the idea behind both similarity-based and class-based smoothing. Class-based methods cluster similar words into classes which are then used in place of actual words. For example the class-based language model of (Brown et al., 1992) is defined as: p(w2|w1) = p(w2|c2)p(c2|c1) (1) This helps solve the sparse data problem since the number of classes is usually much smaller than the number of words. Class-based methods have been applied to the PP attachment task in several guises, using both automatic clustering and hand-crafted classes such as WordNet. Li and Abe (1998) use both WordNet and an automatic clustering algorithm to achieve 85.2% accuracy on the WSJ dataset. The maximum entropy approach of Ratnaparkhi et al. (1994) uses the mutual information clustering algorithm described in (Brown et al., 1992). Although classbased smoothing is shown to improve the model in both cases, some researchers have suggested that clustering words is counterproductive since the information lost by conflating words into broader classes outweighs the benefits derived from reducing data sparseness. This remains to be proven conclusively (Dagan et al., 1999). In contrast, si</context>
<context position="27363" citStr="Li &amp; Abe (1998)" startWordPosition="4537" endWordPosition="4540">85.05 17,843 60.1 Specialist 84.50 5,669 61.0 Table 2: Accuracy on the test data using 0 = 0.05 and k = 5; the size of the noun section of each thesaurus, and coverage of smoothed 4- and 3-tuples 85.2 85 84.8 84.6 � u � Accuracy 84.4 84.2 84 rank 0.01 rank 0.05 rank 0.1 average single 83.8 83.6 83.4 75 rank 0.01 rank 0.05 rank 0.1 average single 60 55 Coverage � e 50 45 40 35 30 70 65 0 5 10 15 20 25 num. neighbours (k) Figure 4: Accuracy of the three different thesauruses on the development set using rank smoothing with β = 0.05 Method Accuracy WN? Zavrel et. al. (1997) 84.1 No WASPS 85.1 No Li &amp; Abe (1998) 85.2 Yes Stetina &amp; Nagao (1997) 88.1 Yes Table 3: Accuracy of various attachment models using WordNet or automatic clustering algorithms ilarity score. There may be merit in creating separate thesauruses for noun-attachment and verb-attachment, since there may be words that are strongly similar in only one of these cases. Interestingly, although Lin is smaller than WASPS it has better coverage. This is most likely due to the different corpora used to construct each thesaurus. Lin is built using newswire text which is closer in genre to the Wall Street Journal. For example, the first neighbour</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Word clustering and disambiguation based on co-occurrence data. In Proceedings of COLING ’98, pages 749–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="1503" citStr="Lin, 1998" startWordPosition="230" endWordPosition="231">ods are applied to an existing PP attachment model and we obtain significant improvements over the baseline. 1 Introduction Prepositional phrases are an interesting example of syntactic ambiguity and a challenge for automatic parsers. The ambiguity arises whenever a prepositional phrase can modify a preceding verb or noun, as in the canonical example I saw the man with the telescope. In syntactic terms, the prepositional phrase attaches either to the noun phrase or the verb phrase. Many kinds of syntactic ambiguity can be resolved using structural information alone (Briscoe and Carroll, 1995; Lin, 1998a; Klein and Manning, 2003), but in this case both candidate structures are perfectly grammatical and roughly equally likely. Therefore ambiguous prepositional phrases require some kind of additional context to disambiguate correctly. In some cases a small amount of lexical knowledge is sufficient: for example of almost always modifies the noun. Other cases, such as the telescope example, are potentially much harder since discourse or world knowledge might be required. Fortunately it is possible to do well at this task just by considering the lexical preferences of the words making up the PP. </context>
<context position="10516" citStr="Lin, 1998" startWordPosition="1680" endWordPosition="1681">sparseness. This remains to be proven conclusively (Dagan et al., 1999). In contrast, similarity-based techniques do not discard any data. Instead the smoothed probability of a word is defined as the total probability of all similar words S(w) as drawn from a thesaurus, weighted by their similarity α(w, w0). For example, the similarity-based language model of (Dagan et al., 1999) is defined as: �p(w2|w1) = α(w1, w01)p(w2|w01) (2) w&apos;1∈S(w1) where Ew&apos;1 ∈S(w1) α(w1, wi) = 1. The similarity function reflects how often the two words appear in the same context. For example, Lin’s similarity metric (Lin, 1998b) used in this paper is based on an informationtheoretic comparison between a pair of co-occurrence probability distributions. This language model was incorporated into a speech recognition system with some success (Dagan et al., 1999). Similarity-based methods have also been successfully applied word sense disambiguation (Dagan et al., 1997) and extraction of grammatical relations (Grishman and Sterling, 1994). Similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic PP attachment models. The memory-based learning approach of (Zavrel et al.,</context>
<context position="18860" citStr="Lin, 1998" startWordPosition="3077" endWordPosition="3078"> a similar known one. This simplified form of smoothing may be appropriate for non-statistical models or situations where relative frequency estimates are hard to incorporate. 4 Thesauruses As noted above, a thesaurus is a resource that groups together words that are distributionally similar. Although we refer to such resources using the singular, a thesaurus has several parts for different word categories such as nouns, verbs and adjectives. We compare three thesauruses on this task. The first two are large-scale generic thesauruses, both constructed using the similarity metric described in (Lin, 1998b), but based on different corpora. The first, which we call Lin, is derived from 300 million words of newswire text and is available on the Internet1. The second, which we call WASPS, forms part of the WASPS lexicographical workbench developed at Brighton University 2 and is derived from the 100 million word BNC. The co-occurrence relations for both are a variety of grammatical relations such as direct object, subject and modifier. WASPS also includes prepositional phrase relations but without attempting to disambiguate them. All possible attachments are included under the assumption that cor</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998a. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic measure of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings ofICML ’98,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="1503" citStr="Lin, 1998" startWordPosition="230" endWordPosition="231">ods are applied to an existing PP attachment model and we obtain significant improvements over the baseline. 1 Introduction Prepositional phrases are an interesting example of syntactic ambiguity and a challenge for automatic parsers. The ambiguity arises whenever a prepositional phrase can modify a preceding verb or noun, as in the canonical example I saw the man with the telescope. In syntactic terms, the prepositional phrase attaches either to the noun phrase or the verb phrase. Many kinds of syntactic ambiguity can be resolved using structural information alone (Briscoe and Carroll, 1995; Lin, 1998a; Klein and Manning, 2003), but in this case both candidate structures are perfectly grammatical and roughly equally likely. Therefore ambiguous prepositional phrases require some kind of additional context to disambiguate correctly. In some cases a small amount of lexical knowledge is sufficient: for example of almost always modifies the noun. Other cases, such as the telescope example, are potentially much harder since discourse or world knowledge might be required. Fortunately it is possible to do well at this task just by considering the lexical preferences of the words making up the PP. </context>
<context position="10516" citStr="Lin, 1998" startWordPosition="1680" endWordPosition="1681">sparseness. This remains to be proven conclusively (Dagan et al., 1999). In contrast, similarity-based techniques do not discard any data. Instead the smoothed probability of a word is defined as the total probability of all similar words S(w) as drawn from a thesaurus, weighted by their similarity α(w, w0). For example, the similarity-based language model of (Dagan et al., 1999) is defined as: �p(w2|w1) = α(w1, w01)p(w2|w01) (2) w&apos;1∈S(w1) where Ew&apos;1 ∈S(w1) α(w1, wi) = 1. The similarity function reflects how often the two words appear in the same context. For example, Lin’s similarity metric (Lin, 1998b) used in this paper is based on an informationtheoretic comparison between a pair of co-occurrence probability distributions. This language model was incorporated into a speech recognition system with some success (Dagan et al., 1999). Similarity-based methods have also been successfully applied word sense disambiguation (Dagan et al., 1997) and extraction of grammatical relations (Grishman and Sterling, 1994). Similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic PP attachment models. The memory-based learning approach of (Zavrel et al.,</context>
<context position="18860" citStr="Lin, 1998" startWordPosition="3077" endWordPosition="3078"> a similar known one. This simplified form of smoothing may be appropriate for non-statistical models or situations where relative frequency estimates are hard to incorporate. 4 Thesauruses As noted above, a thesaurus is a resource that groups together words that are distributionally similar. Although we refer to such resources using the singular, a thesaurus has several parts for different word categories such as nouns, verbs and adjectives. We compare three thesauruses on this task. The first two are large-scale generic thesauruses, both constructed using the similarity metric described in (Lin, 1998b), but based on different corpora. The first, which we call Lin, is derived from 300 million words of newswire text and is available on the Internet1. The second, which we call WASPS, forms part of the WASPS lexicographical workbench developed at Brighton University 2 and is derived from the 100 million word BNC. The co-occurrence relations for both are a variety of grammatical relations such as direct object, subject and modifier. WASPS also includes prepositional phrase relations but without attempting to disambiguate them. All possible attachments are included under the assumption that cor</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998b. An information-theoretic measure of similarity. In Proceedings ofICML ’98, pages 296– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Robust, applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings ofINLG</booktitle>
<pages>201--208</pages>
<contexts>
<context position="22115" citStr="Minnen et al., 2000" startWordPosition="3597" endWordPosition="3600">rring fewer than 100 times. We use the similarity metric described in Weeds (2003). This is a parameterised measure that can be adjusted to suit different tasks, but to ensure compatibility with the two generic thesauruses we chose parameter settings that mimic Lin’s measure. 5 Experiments For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al. (1994). This is divided into a training set of 20,801 words, a development set of 4,039 words and a test set of 3,097 words. Each word was reduced to its morphological root using the morphological analyser described in (Minnen et al., 2000). Strings of four digits beginning with a 1 or 2 are replaced with YEAR and all other digit strings including those including commas and full stops were replaced with NUM. Our implementation of Collins’ algorithm only achieves 84.3% on the test data, with the shortfall of 0.2% primarily due to the different morphological analysers used3 5.1 Smoothing Firstly we compare the different PP similarity functions. Figure 2 shows the accuracy of each as a function of k, the number of examples in S(c) . The WASPS thesaurus was used in all cases. The best smoothed model is rank with 85.1% accuracy when </context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust, applied morphological generation. In Proceedings ofINLG 2000, pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Mitchell</author>
</authors>
<title>Prepositional phrase attachment using machine learning algorithms.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sheffield.</institution>
<contexts>
<context position="28759" citStr="Mitchell, 2003" startWordPosition="4771" endWordPosition="4772">rage, although the quality of specialist neighbours is not as good. The WASPS and Lin models produce statistically significant (P &lt; 0.05) improvements over the vanilla Collins model using a paired t-test with 10-fold crossvalidation on the entire dataset4. The specialist model is not significantly better. Table 3 compares our results with other comparable PP attachment models. On the face of it, these are not resounding improvements over the baseline, but this is a very hard task. Ratnaparkhi (1994) established a human upper bound of 88.2% but subsequent research has put this as low as 78.3% (Mitchell, 2003). At least two thirds of the re4The Collins model achieves 84.50±1.0% accuracy and the smoothed model 84.90±1.0% accuracy by this measure. maining errors are therefore likely to be very difficult. An inspection of the data shows that many of the remaining errors are due to poor neighbouring PPs being used for smoothing. For example, the PP in entrust company with cash modifies the verb, but no matching quadruples are present in the training data. The only matching (n1, p, n2) triple using WASPS is (industry, for, income), which appears twice in the training data modifying the noun. The model t</context>
</contexts>
<marker>Mitchell, 2003</marker>
<rawString>Brian Mitchell. 2003. Prepositional phrase attachment using machine learning algorithms. Ph.D. thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Rebecca Bruce</author>
</authors>
<title>Distinguishing word senses in untagged text.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP ’97.</booktitle>
<contexts>
<context position="16432" citStr="Pedersen and Bruce, 1997" startWordPosition="2642" endWordPosition="2645">it word sense disambiguation. Although thesauruses do conflate multiple senses in their neighbour lists, implausible senses result in infrequent PPs. The similarity set for the PP open plant in Korea might contain open tree in Korea but the latter’s frequency is likely to be zero. Generating triples is riskier since there is less context to rule out unlikely PPs: the triple tree in Korea is more plausible and possibly misleading. But our model does have a natural preference for the most frequent sense in the thesaurus training corpus, which is a useful heuristic for word sense disambiguation (Pedersen and Bruce, 1997). For example, if the thesaurus is trained on business text then factory will be ranked higher than tree when the thesaurus trained on a business corpus (this issue is discussed further in Section 5.2). Finally, to complete our PP attachment scheme we need to define a similarity function between PPs, expressed fully as α ((v, n1, p, n2), (v0, ni, p0, n02)). The raw materials we have to work with are the similarity scores for matching pairs of verbs and nouns as given by the thesaurus. We do not smooth preposition counts. In this paper we compare three similarity measures: • average: The averag</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>Ted Pedersen and Rebecca Bruce. 1997. Distinguishing word senses in untagged text. In Proceedings of EMNLP ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeff Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>250--255</pages>
<contexts>
<context position="2705" citStr="Ratnaparkhi et al. (1994)" startWordPosition="429" endWordPosition="432">ords making up the PP. Lexical preferences describe the tendency for certain words to occur together or only in specific constructions. For example, saw and telescope are more likely to occur together than man and telescope, so we can infer that the correct attachment is likely to be verbal. The most useful lexical preferences are captured by the quadruple (v, n1, p, n2) where v is the verb, n1 is the head of the direct object, p is the preposition and n2 is the head of the prepositional phrase. A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by Ratnaparkhi et al. (1994) and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources. This paper examines the effect of particular smoothing algorithms on the performance of an existing statistical PP model. A major problem faced by any statistical attachment algorithm is sparse data, which occurs when plausible PPs are not well-represented in the training data. For example, if the observed frequency of a PP in the training is zero then the maximum likelihood estimate is also zero. Since the training corpus only represents a fraction of all possible PPs, this is proba</context>
<context position="7475" citStr="Ratnaparkhi et al. (1994)" startWordPosition="1188" endWordPosition="1191">to decide on an attachment (Frazier, 1979; Altman and Steedman, 1988). However, work by Whittemore et al. (1990) and Hindle and Rooth (1993) showed that simple lexical preferences alone can deliver reasonable accuracy. Hindle and Rooth’s approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train a maximum likelihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieved 81.6% accuracy. The Collins and Brooks (1995) model scores 84.5% accuracy on this task, and is one of the most accurate models that do not use additional supervision. The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. The next section discusses other specific approaches that incorporate smoothing techniques. 2.2 Similarity-based smoothing Smoothing for statistical models involves adjusting probability esti</context>
<context position="9554" citStr="Ratnaparkhi et al. (1994)" startWordPosition="1527" endWordPosition="1530">into classes which are then used in place of actual words. For example the class-based language model of (Brown et al., 1992) is defined as: p(w2|w1) = p(w2|c2)p(c2|c1) (1) This helps solve the sparse data problem since the number of classes is usually much smaller than the number of words. Class-based methods have been applied to the PP attachment task in several guises, using both automatic clustering and hand-crafted classes such as WordNet. Li and Abe (1998) use both WordNet and an automatic clustering algorithm to achieve 85.2% accuracy on the WSJ dataset. The maximum entropy approach of Ratnaparkhi et al. (1994) uses the mutual information clustering algorithm described in (Brown et al., 1992). Although classbased smoothing is shown to improve the model in both cases, some researchers have suggested that clustering words is counterproductive since the information lost by conflating words into broader classes outweighs the benefits derived from reducing data sparseness. This remains to be proven conclusively (Dagan et al., 1999). In contrast, similarity-based techniques do not discard any data. Instead the smoothed probability of a word is defined as the total probability of all similar words S(w) as </context>
<context position="21881" citStr="Ratnaparkhi et al. (1994)" startWordPosition="3555" endWordPosition="3558">hesaurus training corpus. Mostly these are unambiguous (v, p, n1) or (n1, p, n2) triples from phrases such as we met in January. The dataset is rather noisy due to tagging and parsing errors, so we discarded any cooccurrence relations occurring fewer than 100 times. We use the similarity metric described in Weeds (2003). This is a parameterised measure that can be adjusted to suit different tasks, but to ensure compatibility with the two generic thesauruses we chose parameter settings that mimic Lin’s measure. 5 Experiments For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al. (1994). This is divided into a training set of 20,801 words, a development set of 4,039 words and a test set of 3,097 words. Each word was reduced to its morphological root using the morphological analyser described in (Minnen et al., 2000). Strings of four digits beginning with a 1 or 2 are replaced with YEAR and all other digit strings including those including commas and full stops were replaced with NUM. Our implementation of Collins’ algorithm only achieves 84.3% on the test data, with the shortfall of 0.2% primarily due to the different morphological analysers used3 5.1 Smoothing Firstly we co</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In Proceedings of the ARPA Workshop on Human Language Technology, pages 250– 255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>Corpus based PP attachment ambiguity resolution with a semantic dictionary.</title>
<date>1997</date>
<booktitle>In Proceedings of WVLC ’97,</booktitle>
<pages>66--80</pages>
<contexts>
<context position="7835" citStr="Stetina and Nagao (1997)" startWordPosition="1251" endWordPosition="1254">ihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieved 81.6% accuracy. The Collins and Brooks (1995) model scores 84.5% accuracy on this task, and is one of the most accurate models that do not use additional supervision. The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. The next section discusses other specific approaches that incorporate smoothing techniques. 2.2 Similarity-based smoothing Smoothing for statistical models involves adjusting probability estimates away from the maximum likelihood estimates to avoid the low probabilities caused by sparse data. Typically this involves mixing in probability distributions that have less context and are less likely to suffer from sparse data problems. For example, if the probability of an attachment given a PP p(alv, n1, p, n2) is undefined because that quadruple was</context>
<context position="27395" citStr="Stetina &amp; Nagao (1997)" startWordPosition="4543" endWordPosition="4546">ist 84.50 5,669 61.0 Table 2: Accuracy on the test data using 0 = 0.05 and k = 5; the size of the noun section of each thesaurus, and coverage of smoothed 4- and 3-tuples 85.2 85 84.8 84.6 � u � Accuracy 84.4 84.2 84 rank 0.01 rank 0.05 rank 0.1 average single 83.8 83.6 83.4 75 rank 0.01 rank 0.05 rank 0.1 average single 60 55 Coverage � e 50 45 40 35 30 70 65 0 5 10 15 20 25 num. neighbours (k) Figure 4: Accuracy of the three different thesauruses on the development set using rank smoothing with β = 0.05 Method Accuracy WN? Zavrel et. al. (1997) 84.1 No WASPS 85.1 No Li &amp; Abe (1998) 85.2 Yes Stetina &amp; Nagao (1997) 88.1 Yes Table 3: Accuracy of various attachment models using WordNet or automatic clustering algorithms ilarity score. There may be merit in creating separate thesauruses for noun-attachment and verb-attachment, since there may be words that are strongly similar in only one of these cases. Interestingly, although Lin is smaller than WASPS it has better coverage. This is most likely due to the different corpora used to construct each thesaurus. Lin is built using newswire text which is closer in genre to the Wall Street Journal. For example, the first neighbour for fetch in WASPS is grab, but</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attachment ambiguity resolution with a semantic dictionary. In Proceedings of WVLC ’97, pages 66–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tokunaga Takenobu</author>
<author>Iwayama Makoto</author>
<author>Tanaka Hozumi</author>
</authors>
<title>Automatic thesaurus construction based on grammatical relations.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI ’95,</booktitle>
<pages>1308--1313</pages>
<marker>Takenobu, Makoto, Hozumi, 1995</marker>
<rawString>Tokunaga Takenobu, Iwayama Makoto, and Tanaka Hozumi. 1995. Automatic thesaurus construction based on grammatical relations. In Proceedings of IJCAI ’95, pages 1308–1313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP ’03.</booktitle>
<contexts>
<context position="21577" citStr="Weeds (2003)" startWordPosition="3507" endWordPosition="3508">us. These PPs are identified semi-automatically using a version of the weighted GR extraction scheme described in (Carroll and Briscoe, 2001). The raw text is parsed and any PPs that occur in a large percentage of the highly ranked candidate parses are considered reliable and added to the thesaurus training corpus. Mostly these are unambiguous (v, p, n1) or (n1, p, n2) triples from phrases such as we met in January. The dataset is rather noisy due to tagging and parsing errors, so we discarded any cooccurrence relations occurring fewer than 100 times. We use the similarity metric described in Weeds (2003). This is a parameterised measure that can be adjusted to suit different tasks, but to ensure compatibility with the two generic thesauruses we chose parameter settings that mimic Lin’s measure. 5 Experiments For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al. (1994). This is divided into a training set of 20,801 words, a development set of 4,039 words and a test set of 3,097 words. Each word was reduced to its morphological root using the morphological analyser described in (Minnen et al., 2000). Strings of four digits beginning with a 1 or 2 are replaced </context>
</contexts>
<marker>Weeds, 2003</marker>
<rawString>Julie Weeds. 2003. A general framework for distributional similarity. In Proceedings of the EMNLP ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Whittemore</author>
<author>K Ferrara</author>
<author>H Brunner</author>
</authors>
<title>Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.</title>
<date>1990</date>
<booktitle>In Proceedings ofACL ’90,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="6962" citStr="Whittemore et al. (1990)" startWordPosition="1108" endWordPosition="1111">our experiment and describes how the specialist thesaurus is constructed. Experimental results are given in Section 5 and we show statistically significant improvements over the baseline model using generic thesauruses. Contrary to our hypothesis the specialist thesaurus does not lead to significant improvements and we discuss possible reasons why it underperforms on this task. 2 Previous work 2.1 PP attachment Early work on PP attachment disambiguation used strictly syntactic or high-level pragmatic rules to decide on an attachment (Frazier, 1979; Altman and Steedman, 1988). However, work by Whittemore et al. (1990) and Hindle and Rooth (1993) showed that simple lexical preferences alone can deliver reasonable accuracy. Hindle and Rooth’s approach was to use mostly unambiguous (v, n1, p) triples extracted from automatically parsed text to train a maximum likelihood classifier. This achieved around 80% accuracy on ambiguous samples. This marked a flowering in the field of PP attachment, with a succession of papers bringing the whole armoury of machine learning techniques to bear on the problem. Ratnaparkhi et al. (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall S</context>
</contexts>
<marker>Whittemore, Ferrara, Brunner, 1990</marker>
<rawString>G. Whittemore, K. Ferrara, and H. Brunner. 1990. Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases. In Proceedings ofACL ’90, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
<author>Jorn Veenstra</author>
</authors>
<title>Resolving PP attachment ambiguities with memory-based learning.</title>
<date>1997</date>
<booktitle>In Proceedings of CoNLL ’97,</booktitle>
<pages>136--144</pages>
<contexts>
<context position="11122" citStr="Zavrel et al., 1997" startWordPosition="1766" endWordPosition="1769">ric (Lin, 1998b) used in this paper is based on an informationtheoretic comparison between a pair of co-occurrence probability distributions. This language model was incorporated into a speech recognition system with some success (Dagan et al., 1999). Similarity-based methods have also been successfully applied word sense disambiguation (Dagan et al., 1997) and extraction of grammatical relations (Grishman and Sterling, 1994). Similarity-based smoothing techniques of the kind described here have not yet been applied to probabilistic PP attachment models. The memory-based learning approach of (Zavrel et al., 1997) is the closest point of contact and shares many of the same ideas, although the details are quite different. Memorybased learning consults similar previously-seen examples to make a decision, but the similarity judgements are usually based on a strict feature matching measure rather than on co-occurrence statistics. Under this scheme pizza and pasta are as different as pizza and Paris. To overcome this Zavrel et al. also experiment with features based on a reduced-dimensionality vector of co-occurrence statistics and note a small (0.2%) increase in performance, leading to a final accuracy of </context>
</contexts>
<marker>Zavrel, Daelemans, Veenstra, 1997</marker>
<rawString>Jakub Zavrel, Walter Daelemans, and Jorn Veenstra. 1997. Resolving PP attachment ambiguities with memory-based learning. In Proceedings of CoNLL ’97, pages 136–144.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>