<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036017">
<title confidence="0.983262">
Towards Agile and Test-Driven Development in NLP Applications
</title>
<author confidence="0.974154">
Jana Z. Sukkarieh Jyoti Kamal
</author>
<affiliation confidence="0.738795666666667">
Educational Testing Service Educational Testing Service
Rosedale Road Rosedale Road
Princeton, NJ 08541, USA Princeton, NJ 08541, USA
</affiliation>
<email confidence="0.927075">
Jsukkarieh@ets.org Jkamal@ets.org
</email>
<note confidence="0.555838846153846">
Example Item (Full Credit 2) Concepts or main/key points:
Figures are given C1: The polygon/it is a quadri-
lateral with two sets of par-
Prompt: allel sides OR the opposite
sides are of equal length OR
The figures show three poly- opposite angles are equal
gons. Is the polygon in Figure 1 C2: The polygon/it has four/4
an octagon, hexagon, or paral-
lelogram? Explain your answer. sides
Scoring rules:
2 points for C1 (only if C2 is not present)
1 point for C1 and C2
Otherwise 0
</note>
<tableCaption confidence="0.824747">
Table 1. Example item for c-rater scoring
</tableCaption>
<sectionHeader confidence="0.966545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996286">
c-rater® is the Educational Testing Service technol-
ogy for automatic content scoring for short free-text
responses. In this paper, we contend that an Agile
and test-driven development environment optimizes
the development of an NLP-based technology.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940333333333">
c-rater (Leacock and Chodorow, 2003) is the Edu-
cational Testing Service technology for the auto-
matic content scoring of short free-text responses
for items whose rubrics are concept-based. This
means that a set of concepts or main points are pre-
specified in the rubric (see the example in Table 1).
We view c-rater’s task as a textual entailment
problem that involves the detection of whether a
student’s answer entails a particular concept (with
the additional challenge that the students’ data con-
tains misspellings and grammatical errors). Our
solution depends on a combination of rule-based
and statistically-based NLP modules (Sukkarieh
and Blackmore, 2009). In addition to databases, a
JBOSS server (www.jboss.org), and two user inter-
faces, c-rater consists of 10 modules–eight of
which are Natural Language Processing (NLP)
modules. Figure 1 depicts the system’s architec-
ture. The c-rater engine is where all the linguistic
processing and concept detection takes place. Sec-
tion 2 lists some of the major problems we face
while developing such a complex NLP-based ap-
plication and how our adoption of Agile and test-
driven development is helping us.
</bodyText>
<figureCaption confidence="0.998352">
Figure 1. c-rater’s System Architecture
</figureCaption>
<sectionHeader confidence="0.77103" genericHeader="introduction">
2 Major Concerns and Solutions
</sectionHeader>
<subsectionHeader confidence="0.962666">
2.1 Communication
</subsectionHeader>
<bodyText confidence="0.99915">
In the past, the implementation of each module
was done in isolation and communication among
team members was lacking. When a team member
</bodyText>
<page confidence="0.974405">
42
</page>
<note confidence="0.6248975">
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 42–44,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99991975">
encountered a problem, it was only then that s/he
would be aware of some logic or data structure
changes by another member. This is not necessar-
ily an NLP-specific problem, however due to the
particularly frequent modifications in NLP-based
applications (see Section 2.2), communication is
more challenging and updates are even more cru-
cial. The adoption of Scrum within Agile
(Augustine, 2005) has improved communication
tremendously. Although both the task backlog and
the choice of tasks within each sprint is done by
the product owner, throughout the sprint the plan-
ning, requirement analysis, design, coding, and
testing is performed by all of the team members.
This has been effecting in decreasing the number
of logic design errors.
</bodyText>
<subsectionHeader confidence="0.995497">
2.2 Planning and Frequent Modification
</subsectionHeader>
<bodyText confidence="0.999960823529412">
Very frequent modifications and re-prioritizing are,
to a great extent, due to the nature of NL input and
constant re-specification, extension, and customi-
zation of NLP modules. This could also be due to
changes in business requirements, e.g. to tailor the
needs of the application to a particular client’s
needs. Further, this could be a response to emerg-
ing research, following a sudden intuition or per-
forming a heuristic approach. Agile takes care of
all these issues. It allows the development to adapt
to changes more quickly and retract/replace the last
feature-based enhancement(s) when the need
arises. It allows for incorporating research time and
experimental studies into the task backlog; hence
the various sprints. The nature of the Agile envi-
ronment allows us also to add tasks driven by the
business needs and consider them highest in value.
</bodyText>
<subsectionHeader confidence="0.958428">
2.3 Metrics for Functionality and Progress
</subsectionHeader>
<bodyText confidence="0.999969888888889">
Metrics for functionality includes measuring pro-
gress, comparing one version to another and moni-
toring the effect of frequent modifications. This
particularly proves challenging due to the nature of
c-rater’s tasks and the NLP modules. In most soft-
ware, the business value is a working product. In c-
rater, it is not only about producing a score but
producing one for the “right” reasons and not due
to errors in the linguistic features obtained.
Until recently, comparing versions meant compar-
ing holistic scores without a sense of the effect of
particular changes. Evaluating the effect of a
change often meant hand-checking hundreds and
hundreds of cases. To improve monitoring, we
have designed an engine test suite (each is a pair
&lt;model-sentence, answer&gt; where model-sentence
is a variant of a concept) and introduced automated
testing. The suite is categorized according to the
linguistic phenomenon of interest (e.g., passive,
ergative, negation, appositive, parser output, co-
reference output). Some categories follow the phe-
nomena in Vanderwende and Dolan (2006). Some
RTE data was transformed for engine tests. This
produced a finer-grained view of the NLP modules
performance, decreased the amount of hand-
checking, and increased our confidence about the
“correctness” of our scores.
</bodyText>
<subsectionHeader confidence="0.99376">
2.4 Maintenance and Debugging
</subsectionHeader>
<bodyText confidence="0.999974107142857">
Until very recently maintaining and debugging
the system was very challenging. We faced many
issues including the unsystematic scattering of
common data structures, making it hard to manage
dependencies; long functions making it difficult to
track bugs; and late integration or lack of regular
updates causing, at times, the system to crash or
not compile. Although this may not be deemed
NLP-specific, the need to modify NLP modules
more frequently than anticipated has made this par-
ticularly challenging. To face this challenge, we
introduced unit tests (UT) and continuous integra-
tion. We usually select some representative or
“typical” NL input for certain phenomena, create
an expected output, create a failed UT, and make it
pass. An additional challenge is that since stu-
dents’ responses are noisy, sometimes choosing
“typical” text is hard. Ideally, unit tests are sup-
posed to be written before or at the same time as
the code; we were able to do that for approxi-
mately 40% of the code. The rest of the unit testing
was being written after the code was written. For
legacy code, we have covered around 10-20% of
the code.
In conclusion, we strongly believe like Degerstedt
and Jönsson (2006), Agile and Test-Driven Devel-
opment form a most-suitable environment for
building NLP-based applications.
</bodyText>
<sectionHeader confidence="0.998667" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.941991">
Special thanks to Kenneth Willian, and Rene Law-
less.
</bodyText>
<page confidence="0.999721">
43
</page>
<sectionHeader confidence="0.990338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999487909090909">
Augustine, S. Managing Agile Projects. 2005. Published
by Prentice Hall Professional Technical Reference.
ISBN 0131240714, 9780131240711. 229 pages.
Degerstedt, L. and Jönsson, A. 2006. LINTest, A devel-
opment tool for testing dialogue systems. In: Pro-
ceedings of the 9th International Conference on
Spoken Language Processing (Interspeech/ICSLP),
Pittsburgh, USA, pp. 489-492.
Leacock, C. and Chodorow, M. 2003. C-rater: Auto-
mated Scoring of Short-Answer Question. Journal of
Computers and Humanities. pp. 389-405.
Sukkarieh, J. Z., &amp; Blackmore, J. To appear. c-rater:
Automatic Content Scoring for Short Constructed
Responses. To appear in the Proceedings of the 22nd
International Conference for the Florida Artificial In-
telligence Research Society, Florida, USA, May
2009.
Vanderwende, L. and Dolan, W. B. 2006. What Syntax
Can Contribute in the Entailment Task. J. Quinonero-
Candela et al. (eds.). Machine Learning Challenges,
Lecture notes in computer science, pp. 205-216.
Springer Berlin/Heidelberg.
</reference>
<page confidence="0.999291">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336666">
<title confidence="0.999784">Towards Agile and Test-Driven Development in NLP Applications</title>
<author confidence="0.989832">Z Sukkarieh Kamal</author>
<affiliation confidence="0.929209">Educational Testing Service Educational Testing Service</affiliation>
<address confidence="0.833229">Rosedale Road Rosedale Road Princeton, NJ 08541, USA Princeton, NJ 08541, USA</address>
<email confidence="0.788591">Jsukkarieh@ets.orgJkamal@ets.org</email>
<abstract confidence="0.933019555555556">Example Item (Full Credit 2) Concepts or main/key points: Figures are given C1: The polygon/it is a quadriwith two sets of par- Prompt: allel sides OR the opposite sides are of equal length OR The figures show three opposite angles are equal gons. Is the polygon in Figure C2: The polygon/it has four/4 an octagon, hexagon, or parallelogram? Explain your answer. sides Scoring rules: 2 points for C1 (only if C2 is not present) 1 point for C1 and C2 Otherwise 0 1. item for c-rater scoring Abstract is the Educational Testing Service technology for automatic content scoring for short free-text responses. In this paper, we contend that an Agile and test-driven development environment optimizes the development of an NLP-based technology.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Augustine</author>
</authors>
<title>Managing Agile Projects.</title>
<date>2005</date>
<booktitle>Professional Technical Reference. ISBN 0131240714,</booktitle>
<pages>9780131240711--229</pages>
<publisher>Prentice Hall</publisher>
<note>Published by</note>
<contexts>
<context position="3049" citStr="Augustine, 2005" startWordPosition="470" endWordPosition="471">42 Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 42–44, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics encountered a problem, it was only then that s/he would be aware of some logic or data structure changes by another member. This is not necessarily an NLP-specific problem, however due to the particularly frequent modifications in NLP-based applications (see Section 2.2), communication is more challenging and updates are even more crucial. The adoption of Scrum within Agile (Augustine, 2005) has improved communication tremendously. Although both the task backlog and the choice of tasks within each sprint is done by the product owner, throughout the sprint the planning, requirement analysis, design, coding, and testing is performed by all of the team members. This has been effecting in decreasing the number of logic design errors. 2.2 Planning and Frequent Modification Very frequent modifications and re-prioritizing are, to a great extent, due to the nature of NL input and constant re-specification, extension, and customization of NLP modules. This could also be due to changes in </context>
</contexts>
<marker>Augustine, 2005</marker>
<rawString>Augustine, S. Managing Agile Projects. 2005. Published by Prentice Hall Professional Technical Reference. ISBN 0131240714, 9780131240711. 229 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Degerstedt</author>
<author>A Jönsson</author>
</authors>
<title>LINTest, A development tool for testing dialogue systems. In:</title>
<date>2006</date>
<booktitle>Proceedings of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP),</booktitle>
<pages>489--492</pages>
<location>Pittsburgh, USA,</location>
<marker>Degerstedt, Jönsson, 2006</marker>
<rawString>Degerstedt, L. and Jönsson, A. 2006. LINTest, A development tool for testing dialogue systems. In: Proceedings of the 9th International Conference on Spoken Language Processing (Interspeech/ICSLP), Pittsburgh, USA, pp. 489-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>C-rater: Automated Scoring of Short-Answer Question.</title>
<date>2003</date>
<journal>Journal of Computers and Humanities.</journal>
<pages>389--405</pages>
<contexts>
<context position="1088" citStr="Leacock and Chodorow, 2003" startWordPosition="166" endWordPosition="169">e figures show three poly- opposite angles are equal gons. Is the polygon in Figure 1 C2: The polygon/it has four/4 an octagon, hexagon, or parallelogram? Explain your answer. sides Scoring rules: 2 points for C1 (only if C2 is not present) 1 point for C1 and C2 Otherwise 0 Table 1. Example item for c-rater scoring Abstract c-rater® is the Educational Testing Service technology for automatic content scoring for short free-text responses. In this paper, we contend that an Agile and test-driven development environment optimizes the development of an NLP-based technology. 1 Introduction c-rater (Leacock and Chodorow, 2003) is the Educational Testing Service technology for the automatic content scoring of short free-text responses for items whose rubrics are concept-based. This means that a set of concepts or main points are prespecified in the rubric (see the example in Table 1). We view c-rater’s task as a textual entailment problem that involves the detection of whether a student’s answer entails a particular concept (with the additional challenge that the students’ data contains misspellings and grammatical errors). Our solution depends on a combination of rule-based and statistically-based NLP modules (Sukk</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Leacock, C. and Chodorow, M. 2003. C-rater: Automated Scoring of Short-Answer Question. Journal of Computers and Humanities. pp. 389-405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Z Sukkarieh</author>
<author>J Blackmore</author>
</authors>
<title>To appear. c-rater: Automatic Content Scoring for Short Constructed Responses.</title>
<date>2009</date>
<booktitle>the Proceedings of the 22nd International Conference for the Florida Artificial Intelligence Research Society,</booktitle>
<location>Florida, USA,</location>
<note>To appear in</note>
<contexts>
<context position="1714" citStr="Sukkarieh and Blackmore, 2009" startWordPosition="263" endWordPosition="266">003) is the Educational Testing Service technology for the automatic content scoring of short free-text responses for items whose rubrics are concept-based. This means that a set of concepts or main points are prespecified in the rubric (see the example in Table 1). We view c-rater’s task as a textual entailment problem that involves the detection of whether a student’s answer entails a particular concept (with the additional challenge that the students’ data contains misspellings and grammatical errors). Our solution depends on a combination of rule-based and statistically-based NLP modules (Sukkarieh and Blackmore, 2009). In addition to databases, a JBOSS server (www.jboss.org), and two user interfaces, c-rater consists of 10 modules–eight of which are Natural Language Processing (NLP) modules. Figure 1 depicts the system’s architecture. The c-rater engine is where all the linguistic processing and concept detection takes place. Section 2 lists some of the major problems we face while developing such a complex NLP-based application and how our adoption of Agile and testdriven development is helping us. Figure 1. c-rater’s System Architecture 2 Major Concerns and Solutions 2.1 Communication In the past, the im</context>
</contexts>
<marker>Sukkarieh, Blackmore, 2009</marker>
<rawString>Sukkarieh, J. Z., &amp; Blackmore, J. To appear. c-rater: Automatic Content Scoring for Short Constructed Responses. To appear in the Proceedings of the 22nd International Conference for the Florida Artificial Intelligence Research Society, Florida, USA, May 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>W B Dolan</author>
</authors>
<title>What Syntax Can Contribute in the Entailment Task.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges, Lecture notes in computer science,</booktitle>
<pages>205--216</pages>
<editor>J. QuinoneroCandela et al. (eds.).</editor>
<publisher>Springer Berlin/Heidelberg.</publisher>
<contexts>
<context position="5405" citStr="Vanderwende and Dolan (2006)" startWordPosition="838" endWordPosition="841">ed. Until recently, comparing versions meant comparing holistic scores without a sense of the effect of particular changes. Evaluating the effect of a change often meant hand-checking hundreds and hundreds of cases. To improve monitoring, we have designed an engine test suite (each is a pair &lt;model-sentence, answer&gt; where model-sentence is a variant of a concept) and introduced automated testing. The suite is categorized according to the linguistic phenomenon of interest (e.g., passive, ergative, negation, appositive, parser output, coreference output). Some categories follow the phenomena in Vanderwende and Dolan (2006). Some RTE data was transformed for engine tests. This produced a finer-grained view of the NLP modules performance, decreased the amount of handchecking, and increased our confidence about the “correctness” of our scores. 2.4 Maintenance and Debugging Until very recently maintaining and debugging the system was very challenging. We faced many issues including the unsystematic scattering of common data structures, making it hard to manage dependencies; long functions making it difficult to track bugs; and late integration or lack of regular updates causing, at times, the system to crash or not</context>
</contexts>
<marker>Vanderwende, Dolan, 2006</marker>
<rawString>Vanderwende, L. and Dolan, W. B. 2006. What Syntax Can Contribute in the Entailment Task. J. QuinoneroCandela et al. (eds.). Machine Learning Challenges, Lecture notes in computer science, pp. 205-216. Springer Berlin/Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>