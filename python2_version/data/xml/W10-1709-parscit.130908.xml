<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.042587">
<title confidence="0.984364">
Improved Features and Grammar Selection for Syntax-Based MT
</title>
<author confidence="0.998829">
Greg Hanneman and Jonathan Clark and Alon Lavie
</author>
<affiliation confidence="0.904212">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.992869">
{ghannema, jhclark, alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895333333333">
We present the Carnegie Mellon Univer-
sity Stat-XFER group submission to the
WMT 2010 shared translation task. Up-
dates to our syntax-based SMT system
mainly fell in the areas of new feature for-
mulations in the translation model and im-
proved filtering of SCFG rules. Compared
to our WMT 2009 submission, we report
a gain of 1.73 BLEU by using the new
features and decoding environment, and a
gain of up to 0.52 BLEU from improved
grammar selection.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999765208333333">
From its earlier focus on linguistically rich ma-
chine translation for resource-poor languages, the
statistical transfer MT group at Carnegie Mellon
University has expanded in recent years to the in-
creasingly successful domain of syntax-based sta-
tistical MT in large-data scenarios. Our submis-
sion to the 2010 Workshop on Machine Transla-
tion is a syntax-based SMT system with a syn-
chonous context-free grammar (SCFG), where the
SCFG rules are derived from full constituency
parse trees on both the source and target sides of
parallel training sentences. We participated in the
French-to-English shared translation task.
This year, we focused our efforts on making
more and better use of syntactic grammar. Much
of the work went into formulating a more expan-
sive feature set in the translation model and a new
method of assigning scores to phrase pairs and
grammar rules. Following a change of decoder
that allowed us to experiment with systems using
much larger syntactic grammars than previously,
we also adapted a technique to more intelligently
pre-filter grammar rules to those most likely to be
useful.
</bodyText>
<sectionHeader confidence="0.993167" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.999963764705883">
We built our system on a partial selection of
the provided French–English training data, us-
ing the Europarl, News Commentary, and UN
sets, but ignoring the Giga-FrEn data. After
tokenization and some pruning of our training
data, this left us with a corpus of approximately
8.6 million sentence pairs. We word-aligned the
corpus with MGIZA++ (Gao and Vogel, 2008),
a multi-threaded implementation of the standard
word alignment tool GIZA++ (Och and Ney,
2003). Word alignments were symmetrized with
the “grow-diag-final-and” heuristic. We automati-
cally parsed the French side of the corpus with the
Berkeley parser (Petrov and Klein, 2007), while
we used the fast vanilla PCFG model of the Stan-
ford parser (Klein and Manning, 2003) for the
English side. These steps resulted in a parallel
parsed corpus from which to extract phrase pairs
and grammar rules.
Phrase extraction involves three distinct steps.
In the first, we perform standard (non-syntactic)
phrase extraction according to the heuristics of
phrase-based SMT (Koehn et al., 2003). In the
second, we obtain syntactic phrase pairs using
the tree-to-tree matching method of Lavie et al.
(2008). Briefly, this method aligns nodes in par-
allel parse trees by projecting up from the word
alignments. A source-tree node s will be aligned
to a target-tree node t if the word alignments in the
yield of s all land within the yield of t, and vice
versa. This node alignment is similar in spirit to
the subtree alignment method of Zhechev and Way
(2008), except our method is based on the spe-
cific Viterbi word alignment links found for each
</bodyText>
<page confidence="0.98599">
82
</page>
<note confidence="0.453012">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82–87,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999763236842106">
sentence rather than on the general word trans-
lation probabilities computed for the corpus as a
whole. This enables us to use efficient dynamic
programming to infer node alignments, rather than
resorting to a greedy search or the enumeration of
all possible alignments. Finally, in the third step,
we use the node alignments from syntactic phrase
pair extraction to extract grammar rules. Each
aligned node in a tree pair specifies a decompo-
sition point for breaking the parallel trees into a
series of SCFG rules. Like Galley et al. (2006),
we allow “composed” (non-minimal) rules when
they build entirely on lexical items. However, to
control the size of the grammar, we do not produce
composed rules that build on other non-terminals,
nor do we produce multiple possible rules when
we encounter unaligned words. Another differ-
ence is that we discard internal structure of com-
posed lexical rules so that we produce SCFG rules
rather than synchronous tree substitution grammar
rules.
The extracted phrase pairs and grammar rules
are collected together and scored according to a
variety of features (Section 3). Instead of decod-
ing with the very large complete set of extracted
grammar rules, we select only a small number of
rules meeting certain criteria (Section 4).
In contrast to previous years, when we used the
Stat-XFER decoder, this year we switched to the
the Joshua decoder (Li et al., 2009) to take advan-
tage of its more efficient architecture and imple-
mentation of modern decoding techniques, such as
cube pruning and multi-threading. We also man-
aged system-building workflows with LoonyBin
(Clark and Lavie, 2010), a toolkit for managing
multi-step experiments across different servers or
computing clusters. Section 5 details our experi-
mental results.
</bodyText>
<sectionHeader confidence="0.999071" genericHeader="method">
3 Translation Model Construction
</sectionHeader>
<bodyText confidence="0.9995455">
One major improvement in our system this year
is the feature scores we applied to our grammar
and phrase pairs. Inspired largely by the Syntax-
Augmented MT system (Zollmann and Venu-
gopal, 2006), our translation model contains 22
features in addition to the language model. In con-
trast to earlier formulations of our features (Han-
neman and Lavie, 2009), our maximum-likelihood
features are now based on a strict separation be-
tween counts drawn from non-syntactic phrase ex-
traction heuristics and our syntactic rule extractor;
no feature is estimated from counts in both spaces.
We define an aggregate rule instance as a 5-
tuple r = (L, S, T, Cphr, Csyn) that contains a
left-hand-side label L, a sequence of terminals
and non-terminals for the source (S) and target
(T) right-hand sides, and aggregated counts from
phrase-based SMT extraction heuristics Cphr and
the syntactic rule extractor Csyn.
In preparation for feature scoring, we:
</bodyText>
<listItem confidence="0.975471666666667">
1. Run phrase instance extraction using stan-
dard phrase-based SMT heuristics to obtain
tuples (PHR, S, T, Cphr, 0) where S and T
never contain non-terminals
2. Run syntactic rule instance extraction as de-
scribed in Section 2 above to obtain tuples
(L, S, T, 0, Csyn)
3. Share non-syntactic counts such that, for
any two tuples r1 = (PHR, S, T, Cphr, 0)
and r2 = (L2, S, T, 0, Csyn) with equiv-
alent S and T values, we produce r2 =
(L2, S, T, Cphr, Csyn)
</listItem>
<bodyText confidence="0.999623428571429">
Note that there is no longer any need to retain
PHR rules (PHR, S, T) that have syntactic equiv-
alents (L =� PHR, S, T) since they have the same
features In addition, we assume there will be no
tuples where S and T contain non-terminals while
Cphr = 0 and Csyn &gt; 0. That is, the syntactic
phrases are a subset of non-syntactic phrases.
</bodyText>
<subsectionHeader confidence="0.995678">
3.1 Maximum-Likelihood Features
</subsectionHeader>
<bodyText confidence="0.9964746">
Our most traditional features are Pphr(T  |S) and
Pphr(S  |T), estimated using only counts Cphr.
These features apply only to rules not con-
taining any non-terminals. They are equiva-
lent to the phrase P(T  |S) and P(S  |T) fea-
tures from the Moses decoder, even when L =�
PHR. In contrast, we used Psyn∪phr(L, S  |T) and
Psyn∪phr(L,T  |S) last year, which applied to all
rules. The new features are no longer subject to
increased sparsity as the number of non-terminals
in the grammar increases.
We also have grammar rule probabili-
ties Psyn(T  |S), Psyn(S  |T), Psyn(L  |S),
Psyn(L |T), and Psyn(L |S, T) estimated using
Csyn; these apply only to rules where S and T
contain non-terminals. By no longer including
counts from phrase-based SMT extraction heuris-
tics in these features, we encourage rules where
L =� PHR since the smaller counts from the rule
learner would have otherwise been overshadowed
</bodyText>
<page confidence="0.993292">
83
</page>
<bodyText confidence="0.9999506">
by the much larger counts from the phrase-based
SMT heuristics.
Finally, we estimate “not labelable” (NL) fea-
tures Psyn(NL  |S) and Psyn(NL  |T). With R de-
noting the set of all extracted rules,
</bodyText>
<equation confidence="0.9620965">
Csyn
Psyn(NL  |S) = (1)
(2)
Er′∈R s.t. T′=T C′syn
</equation>
<bodyText confidence="0.999941857142857">
We use additive smoothing (with n = 1 for our ex-
periments) to avoid a probability of 0 when there
is no syntactic label for an (S, T) pair. These fea-
tures can encourage syntactic rules when syntax
is likely given a particular string since probability
mass is often distributed among several different
syntactic labels.
</bodyText>
<subsectionHeader confidence="0.987482">
3.2 Instance Features
</subsectionHeader>
<bodyText confidence="0.998053545454546">
We add several features that use sufficient statis-
tics local to each rule. First, we add three binary
low-count features that take on the value 1 when
the frequency of the rule is exactly 1, 2, or 3. There
are also two indicator features related to syntax:
one each that fires when L = PHR and when
L =� PHR. Other indicator features analyze the
abstractness of grammar rules: AS = 1 when the
source side contains only non-terminals, AT = 1
when the target side contains only non-terminals,
TGTINSERTION = 1 when AS = 1, AT = 0,
SRCDELETION = 1 when AS = 0, AT = 1, and
INTERLEAVED = 1 when AS = 0, AT = 0.
Bidirectional lexical probabilities for each rule
are calculated from a unigram lexicon MLE-
estimated over aligned word pairs in the training
corpus, as is the default in Moses.
Finally, we include a glue rule indicator feature
that fires whenever a glue rule is applied during
decoding. In the Joshua decoder, these monotonic
rules stitch syntactic parse fragments together at
no model cost.
</bodyText>
<sectionHeader confidence="0.99828" genericHeader="method">
4 Grammar Selection
</sectionHeader>
<bodyText confidence="0.999550526315789">
With extracted grammars typically reaching tens
of millions of unique rules — not to mention
phrase pairs — our systems clearly face an en-
gineering challenge when attempting to include
the full grammar at decoding time. Iglesias et al.
(2009) classified SCFG rules according to the pat-
tern of terminals and non-terminals on the rules’
right-hand sides, and found that certain patterns
could be entirely left out of the grammar without
loss of MT quality. In particular, large classes of
monotonic rules could be removed without a loss
in automatic metric scores, while small classes of
reordering rules contributed much more to the suc-
cess of the system. Inspired by that approach, we
passed our full set of extracted grammar rule in-
stances through a filter after scoring. Using the
rule notation from Section 3, the filter retained
only those rules that matched one of the follow-
ing patterns:
</bodyText>
<equation confidence="0.9985395">
S = X1 w, T = w X1
S = w X1, T = X1 w
S = X1 X2, T = X2 X1
S = X1 X2, T = X1 X2
</equation>
<bodyText confidence="0.999815">
where X represents any non-terminal and w rep-
resents any span of one or more terminals. The
choice of the specific reordering patterns above
captures our intuition that binary swaps are a fun-
damental ordering divergence between languages,
while the inclusion of the abstract monotonic pat-
tern (X1 X2, X1 X2) ensures that the decoder is
not disproportionately biased towards applying re-
ordering rules without supporting lexical evidence
merely because in-order rules are left out.
Orthogonally to the pattern-based pruning, we
also selected grammars by sorting grammar rules
in decreasing order of frequency count and using
the top n in the decoder. We experimented with
n = 0, 100, 1000, and 10,000. In all cases of
grammar selection, we disallowed rules that in-
serted unaligned target-side terminals unless the
inserted terminals were among the top 100 most
frequent unigrams in the target-side vocabulary.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999828">
5.1 Comparison with WMT 2009 Results
</subsectionHeader>
<bodyText confidence="0.999973">
We performed our initial development work on
an updated version of our previous WMT sub-
mission (Hanneman et al., 2009) so that the ef-
fects of our changes could be directly compared.
Our 2009 system was trained from the full Eu-
roparl and News Commentary data available that
year, plus the pre-release version of the Giga-FrEn
data, for a total of 9.4 million sentence pairs. We
used the news-dev2009a set for minimum error-
rate training and tested system performance on
news-dev2009b. To maintain continuity with our
previously reported scores, we report new scores
here using the same training, tuning, and test-
ing sets, using the uncased versions of IBM-style
</bodyText>
<equation confidence="0.97006475">
r′∈R s.t. S′=S C′syn
C
Psyn(NL  |T) =
syn
</equation>
<page confidence="0.995666">
84
</page>
<table confidence="0.99040125">
System Configuration METEOR BLEU
1. WMT ’09 submission 0.5263 0.2073
2. Joshua decoder 0.5231 0.2158
3. New TM features 0.5348 0.2246
</table>
<tableCaption confidence="0.989621333333333">
Table 1: Dev test results (on news-dev2009b) from
our WMT 2009 system when updating decoding
environment and feature formulations.
</tableCaption>
<figure confidence="0.993317142857143">
System Configuration METEOR BLEU
1. n = 100 0.5314 0.2200
2. n = 100, filtered 0.5341 0.2242
3. n = 1000 0.5324 0.2206
4. n = 1000, filtered 0.5330 0.2233
5. n = 10,000 0.5332 0.2198
6. n = 10,000, filtered 0.5350 0.2250
</figure>
<tableCaption confidence="0.717395">
Table 2: Dev test results (on news-dev2009b) from
our WMT 2009 system with and without pattern-
based grammar selection.
</tableCaption>
<bodyText confidence="0.99814775">
BLEU 1.04 (Papineni et al., 2002) and METEOR
0.6 (Lavie and Agarwal, 2007).
Table 1 shows the effect of our new scoring and
decoding environment. Line 2 uses the same ex-
tracted phrase pairs and grammar rules as line 1,
but the system is tuned and tested with the Joshua
decoder instead of Stat-XFER. For line 3, we re-
scored the extracted phrase pairs from lines 1 and
2 using the updated features discussed in Sec-
tion 3.1 The difference in automatic metric scores
shows a significant benefit from both the new de-
coder and the updated feature formulations: 0.8
BLEU points from the change in decoder, and 0.9
BLEU points from the expanded set of 22 transla-
tion model features.
Our next test was to examine the usefulness of
the pattern-based grammar selection described in
Section 4. For various numbers of rules n, Ta-
ble 2 shows the scores obtained with and without
filtering the grammar before the n most frequent
rules are skimmed off for use. We observe a small
but consistent gain in scores from the grammar se-
lection process, up to half a BLEU point in the
largest-grammar systems (lines 5 and 6).
</bodyText>
<footnote confidence="0.905195142857143">
1In line 2, we did not control for difference in formulation
of the translation length feature: Stat-XFER uses a length
ratio, while Joshua uses a target word count. Line 3 does
not include 26 manually selected grammar rules present in
lines 1 and 2; this is because our new feature scoring requires
information from the grammar rules that was not present in
our 2009 extracted resources.
</footnote>
<subsectionHeader confidence="0.922265">
Source Target
</subsectionHeader>
<bodyText confidence="0.922665777777778">
un rˆole AP1 ADJP1 roles
l’ instabilit´e AP1 ADJP1 instability
l’ argent PP1 NP1 money
une pression AP1 ADJP1 pressure
la gouvernance AP1 ADJP1 governance
la concurrence AP1 ADJP1 competition
des preuves AP1 ADJP1 evidence
les outils AP1 ADJP1 tools
des changements AP1 ADJP1 changes
</bodyText>
<tableCaption confidence="0.9324975">
Table 3: Rules fitting the pattern (5 = w X1, T =
X1 w) that applied on the news-test2010 test set.
</tableCaption>
<subsectionHeader confidence="0.990563">
5.2 WMT 2010 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999981485714286">
We built the WMT 2010 version of our system
from the training data described in Section 2. (The
system falls under the strictly constrained track:
we used neither the Giga-FrEn data for training
nor the LDC Gigaword corpora for language mod-
eling.) We used the provided news-test2008 set
for system tuning, while news-test2009 served
as our 2010 dev test set. Based on the results
in Table 2, our official submission to this year’s
shared task was constructed as in line 6, with
10,000 syntactic grammar rules chosen after a
pattern-based grammar selection step. On the
news-test2010 test set, this system scored 0.2327
on case-insensitive IBM-style BLEU 1.04, 0.5614
on METEOR 0.6, and 0.5519 on METEOR 1.0
(Lavie and Denkowski, 2009).
The actual application of grammar rules in the
system is quite surprising. Despite having a gram-
mar of 10,000 rules at its disposal, the decoder
chose to only apply a total of 20 unique rules
in 392 application instances in the 2489-sentence
news-test2010 set. On a per-sentence basis, this
is actually fewer rule applications than our sys-
tem performed last year with a 26-rule handpicked
grammar! The most frequently applied rules are
fully abstract, monotonic structure-building rules,
such as for stitching together compound noun
phrases with adverbial phrases or prepositional
phrases. Nine of the 20 rules, listed in Table 3,
demonstrate the effect of our pattern-based gram-
mar selection. These partially lexicalized rules fit
the pattern (5 = w X1, T = X1 w) and han-
dle cases of lexicalized binary reordering between
French and English. Though the overall impact of
these rules on automatic metric scores is presum-
</bodyText>
<page confidence="0.999187">
85
</page>
<bodyText confidence="0.999968">
ably quite small, we believe that the key to effec-
tive syntactic grammars in our MT approach lies
in retaining precise rules of this type for common
linguistically motivated reordering patterns.
The above pattern of rule applications is also
observed in our dev test set, news-test2009, where
16 distinct rules apply a total of 352 times. Seven
of the fully abstract rules and three of the lexical-
ized rules that applied on news-test2009 also ap-
plied on news-test2010, while a further two ab-
stract and four lexicalized rules applied on news-
test2009 alone. We thus have a general trend of a
set of general rules applying with higher frequency
across test sets, while the set of lexicalized rules
used varies according to the particular set.
Since, overall, we still do not see as much gram-
mar application in our systems as we would like,
we plan to concentrate future work on further im-
proving this aspect. This includes a more detailed
study of grammar filtering or refinement to select
the most useful rules. We would also like to ex-
plore the effect of the features of Section 3 individ-
ually, on different language pairs, and using differ-
ent grammar types.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998356">
This research was supported in part by NSF grant
IIS-0534217 (LETRAS) and the DARPA GALE
program. We thank Yahoo! for the use of the M45
research computing cluster, where we ran many
steps of our experimental pipeline.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998336108108108">
Jonathan Clark and Alon Lavie. 2010. LoonyBin:
Keeping language technologists sane through
automated management of experimental (hy-
per)workflows. In Proceedings of the Seventh
International Language Resources and Evaluation
(LREC ’10), Valletta, Malta, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961–968, Sydney, Australia,
July.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49–57, Columbus, OH,
June.
Greg Hanneman and Alon Lavie. 2009. Decoding
with syntactic and non-syntactic phrases in a syntax-
based machine translation system. In Proceedings of
the Third Workshop on Syntax and Structure in Sta-
tistical Translations, pages 1–9, Boulder, CO, June.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An improved
statistical transfer systems for French–English ma-
chine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
140–144, Athens, Greece, March.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the ACL, pages 380–388, Athens, Greece, March–
April.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3–10. MIT Press,
Cambridge, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 48–54, Ed-
monton, Alberta, May–June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228–231, Prague, Czech
Republic, June.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2–3):105–115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87–95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In Proceed-
ings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 135–139, Athens, Greece,
March.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
</reference>
<page confidence="0.969454">
86
</page>
<reference confidence="0.9968864375">
the 40th Annual Meeting ofthe Associationfor Com-
putational Linguistics, pages 311–318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings ofNAACL
HLT 2007, pages 404–411, Rochester, NY, April.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics, pages 1105–1112, Manchester,
England, August.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138–141, New York, NY,
June.
</reference>
<page confidence="0.999463">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527350">
<title confidence="0.987272">Improved Features and Grammar Selection for Syntax-Based MT</title>
<author confidence="0.757897">Hanneman Clark</author>
<affiliation confidence="0.806307">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.999718">Pittsburgh, PA 15213</address>
<email confidence="0.966325">jhclark,</email>
<abstract confidence="0.992855923076923">We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2010 shared translation task. Updates to our syntax-based SMT system mainly fell in the areas of new feature formulations in the translation model and improved filtering of SCFG rules. Compared to our WMT 2009 submission, we report a gain of 1.73 BLEU by using the new features and decoding environment, and a gain of up to 0.52 BLEU from improved grammar selection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Clark</author>
<author>Alon Lavie</author>
</authors>
<title>LoonyBin: Keeping language technologists sane through automated management of experimental (hyper)workflows.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Language Resources and Evaluation (LREC ’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="5236" citStr="Clark and Lavie, 2010" startWordPosition="837" endWordPosition="840"> pairs and grammar rules are collected together and scored according to a variety of features (Section 3). Instead of decoding with the very large complete set of extracted grammar rules, we select only a small number of rules meeting certain criteria (Section 4). In contrast to previous years, when we used the Stat-XFER decoder, this year we switched to the the Joshua decoder (Li et al., 2009) to take advantage of its more efficient architecture and implementation of modern decoding techniques, such as cube pruning and multi-threading. We also managed system-building workflows with LoonyBin (Clark and Lavie, 2010), a toolkit for managing multi-step experiments across different servers or computing clusters. Section 5 details our experimental results. 3 Translation Model Construction One major improvement in our system this year is the feature scores we applied to our grammar and phrase pairs. Inspired largely by the SyntaxAugmented MT system (Zollmann and Venugopal, 2006), our translation model contains 22 features in addition to the language model. In contrast to earlier formulations of our features (Hanneman and Lavie, 2009), our maximum-likelihood features are now based on a strict separation betwee</context>
</contexts>
<marker>Clark, Lavie, 2010</marker>
<rawString>Jonathan Clark and Alon Lavie. 2010. LoonyBin: Keeping language technologists sane through automated management of experimental (hyper)workflows. In Proceedings of the Seventh International Language Resources and Evaluation (LREC ’10), Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4149" citStr="Galley et al. (2006)" startWordPosition="662" endWordPosition="665">sala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics sentence rather than on the general word translation probabilities computed for the corpus as a whole. This enables us to use efficient dynamic programming to infer node alignments, rather than resorting to a greedy search or the enumeration of all possible alignments. Finally, in the third step, we use the node alignments from syntactic phrase pair extraction to extract grammar rules. Each aligned node in a tree pair specifies a decomposition point for breaking the parallel trees into a series of SCFG rules. Like Galley et al. (2006), we allow “composed” (non-minimal) rules when they build entirely on lexical items. However, to control the size of the grammar, we do not produce composed rules that build on other non-terminals, nor do we produce multiple possible rules when we encounter unaligned words. Another difference is that we discard internal structure of composed lexical rules so that we produce SCFG rules rather than synchronous tree substitution grammar rules. The extracted phrase pairs and grammar rules are collected together and scored according to a variety of features (Section 3). Instead of decoding with the</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="2190" citStr="Gao and Vogel, 2008" startWordPosition="344" endWordPosition="347">. Following a change of decoder that allowed us to experiment with systems using much larger syntactic grammars than previously, we also adapted a technique to more intelligently pre-filter grammar rules to those most likely to be useful. 2 System Overview We built our system on a partial selection of the provided French–English training data, using the Europarl, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase </context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Decoding with syntactic and non-syntactic phrases in a syntaxbased machine translation system.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translations,</booktitle>
<pages>1--9</pages>
<location>Boulder, CO,</location>
<contexts>
<context position="5759" citStr="Hanneman and Lavie, 2009" startWordPosition="918" endWordPosition="922">g and multi-threading. We also managed system-building workflows with LoonyBin (Clark and Lavie, 2010), a toolkit for managing multi-step experiments across different servers or computing clusters. Section 5 details our experimental results. 3 Translation Model Construction One major improvement in our system this year is the feature scores we applied to our grammar and phrase pairs. Inspired largely by the SyntaxAugmented MT system (Zollmann and Venugopal, 2006), our translation model contains 22 features in addition to the language model. In contrast to earlier formulations of our features (Hanneman and Lavie, 2009), our maximum-likelihood features are now based on a strict separation between counts drawn from non-syntactic phrase extraction heuristics and our syntactic rule extractor; no feature is estimated from counts in both spaces. We define an aggregate rule instance as a 5- tuple r = (L, S, T, Cphr, Csyn) that contains a left-hand-side label L, a sequence of terminals and non-terminals for the source (S) and target (T) right-hand sides, and aggregated counts from phrase-based SMT extraction heuristics Cphr and the syntactic rule extractor Csyn. In preparation for feature scoring, we: 1. Run phrase</context>
</contexts>
<marker>Hanneman, Lavie, 2009</marker>
<rawString>Greg Hanneman and Alon Lavie. 2009. Decoding with syntactic and non-syntactic phrases in a syntaxbased machine translation system. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translations, pages 1–9, Boulder, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Vamshi Ambati</author>
<author>Jonathan H Clark</author>
<author>Alok Parlikar</author>
<author>Alon Lavie</author>
</authors>
<title>An improved statistical transfer systems for French–English machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>140--144</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="11722" citStr="Hanneman et al., 2009" startWordPosition="1961" endWordPosition="1964">s are left out. Orthogonally to the pattern-based pruning, we also selected grammars by sorting grammar rules in decreasing order of frequency count and using the top n in the decoder. We experimented with n = 0, 100, 1000, and 10,000. In all cases of grammar selection, we disallowed rules that inserted unaligned target-side terminals unless the inserted terminals were among the top 100 most frequent unigrams in the target-side vocabulary. 5 Results and Analysis 5.1 Comparison with WMT 2009 Results We performed our initial development work on an updated version of our previous WMT submission (Hanneman et al., 2009) so that the effects of our changes could be directly compared. Our 2009 system was trained from the full Europarl and News Commentary data available that year, plus the pre-release version of the Giga-FrEn data, for a total of 9.4 million sentence pairs. We used the news-dev2009a set for minimum errorrate training and tested system performance on news-dev2009b. To maintain continuity with our previously reported scores, we report new scores here using the same training, tuning, and testing sets, using the uncased versions of IBM-style r′∈R s.t. S′=S C′syn C Psyn(NL |T) = syn 84 System Configu</context>
</contexts>
<marker>Hanneman, Ambati, Clark, Parlikar, Lavie, 2009</marker>
<rawString>Greg Hanneman, Vamshi Ambati, Jonathan H. Clark, Alok Parlikar, and Alon Lavie. 2009. An improved statistical transfer systems for French–English machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 140–144, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adri`a de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>380--388</pages>
<location>Athens, Greece, March–</location>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, Athens, Greece, March– April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2558" citStr="Klein and Manning, 2003" startWordPosition="401" endWordPosition="404">, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word al</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15, pages 3–10. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Alberta, May–June.</location>
<contexts>
<context position="2869" citStr="Koehn et al., 2003" startWordPosition="448" endWordPosition="451">ment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word alignments in the yield of s all land within the yield of t, and vice versa. This node alignment is similar in spirit to the subtree alignment method of Zhechev and Way (2008), except our method is based on the specific Viterbi word alignment links found for each 82 Proceedings of the Joint 5th Workshop on Stati</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 48–54, Edmonton, Alberta, May–June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>228--231</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="12987" citStr="Lavie and Agarwal, 2007" startWordPosition="2175" endWordPosition="2178">0.5263 0.2073 2. Joshua decoder 0.5231 0.2158 3. New TM features 0.5348 0.2246 Table 1: Dev test results (on news-dev2009b) from our WMT 2009 system when updating decoding environment and feature formulations. System Configuration METEOR BLEU 1. n = 100 0.5314 0.2200 2. n = 100, filtered 0.5341 0.2242 3. n = 1000 0.5324 0.2206 4. n = 1000, filtered 0.5330 0.2233 5. n = 10,000 0.5332 0.2198 6. n = 10,000, filtered 0.5350 0.2250 Table 2: Dev test results (on news-dev2009b) from our WMT 2009 system with and without patternbased grammar selection. BLEU 1.04 (Papineni et al., 2002) and METEOR 0.6 (Lavie and Agarwal, 2007). Table 1 shows the effect of our new scoring and decoding environment. Line 2 uses the same extracted phrase pairs and grammar rules as line 1, but the system is tuned and tested with the Joshua decoder instead of Stat-XFER. For line 3, we rescored the extracted phrase pairs from lines 1 and 2 using the updated features discussed in Section 3.1 The difference in automatic metric scores shows a significant benefit from both the new decoder and the updated feature formulations: 0.8 BLEU points from the change in decoder, and 0.9 BLEU points from the expanded set of 22 translation model features</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="15571" citStr="Lavie and Denkowski, 2009" startWordPosition="2619" endWordPosition="2622"> under the strictly constrained track: we used neither the Giga-FrEn data for training nor the LDC Gigaword corpora for language modeling.) We used the provided news-test2008 set for system tuning, while news-test2009 served as our 2010 dev test set. Based on the results in Table 2, our official submission to this year’s shared task was constructed as in line 6, with 10,000 syntactic grammar rules chosen after a pattern-based grammar selection step. On the news-test2010 test set, this system scored 0.2327 on case-insensitive IBM-style BLEU 1.04, 0.5614 on METEOR 0.6, and 0.5519 on METEOR 1.0 (Lavie and Denkowski, 2009). The actual application of grammar rules in the system is quite surprising. Despite having a grammar of 10,000 rules at its disposal, the decoder chose to only apply a total of 20 unique rules in 392 application instances in the 2489-sentence news-test2010 set. On a per-sentence basis, this is actually fewer rule applications than our system performed last year with a 26-rule handpicked grammar! The most frequently applied rules are fully abstract, monotonic structure-building rules, such as for stitching together compound noun phrases with adverbial phrases or prepositional phrases. Nine of </context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23(2–3):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Alok Parlikar</author>
<author>Vamshi Ambati</author>
</authors>
<title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>87--95</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="2980" citStr="Lavie et al. (2008)" startWordPosition="466" endWordPosition="469">. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word alignments in the yield of s all land within the yield of t, and vice versa. This node alignment is similar in spirit to the subtree alignment method of Zhechev and Way (2008), except our method is based on the specific Viterbi word alignment links found for each 82 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82–87, Uppsala, Sweden, 15-16 July 2010. c�2010 Association f</context>
</contexts>
<marker>Lavie, Parlikar, Ambati, 2008</marker>
<rawString>Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008. Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora. In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation, pages 87–95, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="5011" citStr="Li et al., 2009" startWordPosition="803" endWordPosition="806">e encounter unaligned words. Another difference is that we discard internal structure of composed lexical rules so that we produce SCFG rules rather than synchronous tree substitution grammar rules. The extracted phrase pairs and grammar rules are collected together and scored according to a variety of features (Section 3). Instead of decoding with the very large complete set of extracted grammar rules, we select only a small number of rules meeting certain criteria (Section 4). In contrast to previous years, when we used the Stat-XFER decoder, this year we switched to the the Joshua decoder (Li et al., 2009) to take advantage of its more efficient architecture and implementation of modern decoding techniques, such as cube pruning and multi-threading. We also managed system-building workflows with LoonyBin (Clark and Lavie, 2010), a toolkit for managing multi-step experiments across different servers or computing clusters. Section 5 details our experimental results. 3 Translation Model Construction One major improvement in our system this year is the feature scores we applied to our grammar and phrase pairs. Inspired largely by the SyntaxAugmented MT system (Zollmann and Venugopal, 2006), our tran</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N.G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2286" citStr="Och and Ney, 2003" startWordPosition="358" endWordPosition="361">ctic grammars than previously, we also adapted a technique to more intelligently pre-filter grammar rules to those most likely to be useful. 2 System Overview We built our system on a partial selection of the provided French–English training data, using the Europarl, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evalution of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting ofthe Associationfor Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="12946" citStr="Papineni et al., 2002" startWordPosition="2168" endWordPosition="2171">tion METEOR BLEU 1. WMT ’09 submission 0.5263 0.2073 2. Joshua decoder 0.5231 0.2158 3. New TM features 0.5348 0.2246 Table 1: Dev test results (on news-dev2009b) from our WMT 2009 system when updating decoding environment and feature formulations. System Configuration METEOR BLEU 1. n = 100 0.5314 0.2200 2. n = 100, filtered 0.5341 0.2242 3. n = 1000 0.5324 0.2206 4. n = 1000, filtered 0.5330 0.2233 5. n = 10,000 0.5332 0.2198 6. n = 10,000, filtered 0.5350 0.2250 Table 2: Dev test results (on news-dev2009b) from our WMT 2009 system with and without patternbased grammar selection. BLEU 1.04 (Papineni et al., 2002) and METEOR 0.6 (Lavie and Agarwal, 2007). Table 1 shows the effect of our new scoring and decoding environment. Line 2 uses the same extracted phrase pairs and grammar rules as line 1, but the system is tuned and tested with the Joshua decoder instead of Stat-XFER. For line 3, we rescored the extracted phrase pairs from lines 1 and 2 using the updated features discussed in Section 3.1 The difference in automatic metric scores shows a significant benefit from both the new decoder and the updated feature formulations: 0.8 BLEU points from the change in decoder, and 0.9 BLEU points from the expa</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evalution of machine translation. In Proceedings of the 40th Annual Meeting ofthe Associationfor Computational Linguistics, pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofNAACL HLT 2007,</booktitle>
<pages>404--411</pages>
<location>Rochester, NY,</location>
<contexts>
<context position="2466" citStr="Petrov and Klein, 2007" startWordPosition="384" endWordPosition="387">tem on a partial selection of the provided French–English training data, using the Europarl, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings ofNAACL HLT 2007, pages 404–411, Rochester, NY, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ventsislav Zhechev</author>
<author>Andy Way</author>
</authors>
<title>Automatic generation of parallel treebanks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>1105--1112</pages>
<location>Manchester, England,</location>
<contexts>
<context position="3331" citStr="Zhechev and Way (2008)" startWordPosition="531" endWordPosition="534">s three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word alignments in the yield of s all land within the yield of t, and vice versa. This node alignment is similar in spirit to the subtree alignment method of Zhechev and Way (2008), except our method is based on the specific Viterbi word alignment links found for each 82 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82–87, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics sentence rather than on the general word translation probabilities computed for the corpus as a whole. This enables us to use efficient dynamic programming to infer node alignments, rather than resorting to a greedy search or the enumeration of all possible alignments. Finally, in the third step, we use the node alignmen</context>
</contexts>
<marker>Zhechev, Way, 2008</marker>
<rawString>Ventsislav Zhechev and Andy Way. 2008. Automatic generation of parallel treebanks. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 1105–1112, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<location>New York, NY,</location>
<contexts>
<context position="5601" citStr="Zollmann and Venugopal, 2006" startWordPosition="892" endWordPosition="896">he the Joshua decoder (Li et al., 2009) to take advantage of its more efficient architecture and implementation of modern decoding techniques, such as cube pruning and multi-threading. We also managed system-building workflows with LoonyBin (Clark and Lavie, 2010), a toolkit for managing multi-step experiments across different servers or computing clusters. Section 5 details our experimental results. 3 Translation Model Construction One major improvement in our system this year is the feature scores we applied to our grammar and phrase pairs. Inspired largely by the SyntaxAugmented MT system (Zollmann and Venugopal, 2006), our translation model contains 22 features in addition to the language model. In contrast to earlier formulations of our features (Hanneman and Lavie, 2009), our maximum-likelihood features are now based on a strict separation between counts drawn from non-syntactic phrase extraction heuristics and our syntactic rule extractor; no feature is estimated from counts in both spaces. We define an aggregate rule instance as a 5- tuple r = (L, S, T, Cphr, Csyn) that contains a left-hand-side label L, a sequence of terminals and non-terminals for the source (S) and target (T) right-hand sides, and a</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, pages 138–141, New York, NY, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>