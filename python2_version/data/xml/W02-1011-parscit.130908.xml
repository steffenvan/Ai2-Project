<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004860">
<note confidence="0.443868">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.
Association for Computational Linguistics.
</note>
<title confidence="0.978429">
Thumbs up? Sentiment Classification using Machine Learning
Techniques
</title>
<author confidence="0.99932">
Bo Pang and Lillian Lee
</author>
<affiliation confidence="0.997821">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.528131">
Ithaca, NY 14853 USA
</address>
<email confidence="0.999026">
{pabo,llee}@cs.cornell.edu
</email>
<author confidence="0.98381">
Shivakumar Vaithyanathan
</author>
<affiliation confidence="0.998261">
IBM Almaden Research Center
</affiliation>
<address confidence="0.9928075">
650 Harry Rd.
San Jose, CA 95120 USA
</address>
<email confidence="0.999381">
shiv@almaden.ibm.com
</email>
<sectionHeader confidence="0.996668" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999626">
We consider the problem of classifying doc-
uments not by topic, but by overall senti-
ment, e.g., determining whether a review
is positive or negative. Using movie re-
views as data, we find that standard ma-
chine learning techniques definitively out-
perform human-produced baselines. How-
ever, the three machine learning methods
we employed (Naive Bayes, maximum en-
tropy classification, and support vector ma-
chines) do not perform as well on sentiment
classification as on traditional topic-based
categorization. We conclude by examining
factors that make the sentiment classifica-
tion problem more challenging.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888212765958">
Today, very large amounts of information are avail-
able in on-line documents. As part of the effort to
better organize this information for users, researchers
have been actively investigating the problem of au-
tomatic text categorization.
The bulk of such work has focused on topical cat-
egorization, attempting to sort documents accord-
ing to their subject matter (e.g., sports vs. poli-
tics). However, recent years have seen rapid growth
in on-line discussion groups and review sites (e.g.,
the New York Times’ Books web page) where a cru-
cial characteristic of the posted articles is their senti-
ment, or overall opinion towards the subject matter
— for example, whether a product review is pos-
itive or negative. Labeling these articles with their
sentiment would provide succinct summaries to read-
ers; indeed, these labels are part of the appeal and
value-add of such sites as www.rottentomatoes.com,
which both labels movie reviews that do not con-
tain explicit rating indicators and normalizes the
different rating schemes that individual reviewers
use. Sentiment classification would also be helpful in
business intelligence applications (e.g. MindfulEye’s
Lexant system&apos;) and recommender systems (e.g.,
Terveen et al. (1997), Tatemura (2000)), where user
input and feedback could be quickly summarized; in-
deed, in general, free-form survey responses given in
natural language format could be processed using
sentiment categorization. Moreover, there are also
potential applications to message filtering; for exam-
ple, one might be able to use sentiment information
to recognize and discard “flames”(Spertus, 1997).
In this paper, we examine the effectiveness of ap-
plying machine learning techniques to the sentiment
classification problem. A challenging aspect of this
problem that seems to distinguish it from traditional
topic-based classification is that while topics are of-
ten identifiable by keywords alone, sentiment can be
expressed in a more subtle manner. For example, the
sentence “How could anyone sit through this movie?”
contains no single word that is obviously negative.
(See Section 7 for more examples). Thus, sentiment
seems to require more understanding than the usual
topic-based classification. So, apart from presenting
our results obtained via machine learning techniques,
we also analyze the problem to gain a better under-
standing of how difficult it is.
</bodyText>
<sectionHeader confidence="0.997593" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.993120117647059">
This section briefly surveys previous work on non-
topic-based text categorization.
One area of research concentrates on classifying
documents according to their source or source style,
with statistically-detected stylistic variation (Biber,
1988) serving as an important cue. Examples in-
clude author, publisher (e.g., the New York Times vs.
The Daily News), native-language background, and
“brow” (e.g., high-brow vs. “popular”, or low-brow)
(Mosteller and Wallace, 1984; Argamon-Engelson et
lhttp://www.mindfuleye.com/about/lexant.htm
al., 1998; Tomokiyo and Jones, 2001; Kessler et al.,
1997).
Another, more related area of research is that of
determining the genre of texts; subjective genres,
such as “editorial”, are often one of the possible
categories (Karlgren and Cutting, 1994; Kessler et
al., 1997; Finn et al., 2002). Other work explicitly
attempts to find features indicating that subjective
language is being used (Hatzivassiloglou and Wiebe,
2000; Wiebe et al., 2001). But, while techniques for
genre categorization and subjectivity detection can
help us recognize documents that express an opin-
ion, they do not address our specific classification
task of determining what that opinion actually is.
Most previous research on sentiment-based classi-
fication has been at least partially knowledge-based.
Some of this work focuses on classifying the semantic
orientation of individual words or phrases, using lin-
guistic heuristics or a pre-selected set of seed words
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2002). Past work on sentiment-based cat-
egorization of entire documents has often involved
either the use of models inspired by cognitive lin-
guistics (Hearst, 1992; Sack, 1994) or the manual or
semi-manual construction of discriminant-word lex-
icons (Huettner and Subasic, 2000; Das and Chen,
2001; Tong, 2001). Interestingly, our baseline exper-
iments, described in Section 4, show that humans
may not always have the best intuition for choosing
discriminating words.
Turney’s (2002) work on classification of reviews
is perhaps the closest to ours.2 He applied a spe-
cific unsupervised learning technique based on the
mutual information between document phrases and
the words “excellent” and “poor”, where the mu-
tual information is computed using statistics gath-
ered by a search engine. In contrast, we utilize sev-
eral completely prior-knowledge-free supervised ma-
chine learning methods, with the goal of understand-
ing the inherent difficulty of the task.
</bodyText>
<sectionHeader confidence="0.980974" genericHeader="method">
3 The Movie-Review Domain
</sectionHeader>
<bodyText confidence="0.999790162162162">
For our experiments, we chose to work with movie
reviews. This domain is experimentally convenient
because there are large on-line collections of such re-
views, and because reviewers often summarize their
overall sentiment with a machine-extractable rat-
ing indicator, such as a number of stars; hence, we
did not need to hand-label the data for supervised
learning or evaluation purposes. We also note that
Turney (2002) found movie reviews to be the most
2Indeed, although our choice of title was completely
independent of his, our selections were eerily similar.
difficult of several domains for sentiment classifica-
tion, reporting an accuracy of 65.83% on a 120-
document set (random-choice performance: 50%).
But we stress that the machine learning methods and
features we use are not specific to movie reviews, and
should be easily applicable to other domains as long
as sufficient training data exists.
Our data source was the Internet Movie Database
(IMDb) archive of the rec.arts.movies.reviews
newsgroup.3 We selected only reviews where the au-
thor rating was expressed either with stars or some
numerical value (other conventions varied too widely
to allow for automatic processing). Ratings were
automatically extracted and converted into one of
three categories: positive, negative, or neutral. For
the work described in this paper, we concentrated
only on discriminating between positive and nega-
tive sentiment. To avoid domination of the corpus
by a small number of prolific reviewers, we imposed
a limit of fewer than 20 reviews per author per sen-
timent category, yielding a corpus of 752 negative
and 1301 positive reviews, with a total of 144 re-
viewers represented. This dataset will be available
on-line at http://www.cs.cornell.edu/people/pabo/-
movie-review-data/ (the URL contains hyphens only
around the word “review”).
</bodyText>
<sectionHeader confidence="0.953941" genericHeader="method">
4 A Closer Look At the Problem
</sectionHeader>
<bodyText confidence="0.999637833333333">
Intuitions seem to differ as to the difficulty of the sen-
timent detection problem. An expert on using ma-
chine learning for text categorization predicted rela-
tively low performance for automatic methods. On
the other hand, it seems that distinguishing positive
from negative reviews is relatively easy for humans,
especially in comparison to the standard text catego-
rization problem, where topics can be closely related.
One might also suspect that there are certain words
people tend to use to express strong sentiments, so
that it might suffice to simply produce a list of such
words by introspection and rely on them alone to
classify the texts.
To test this latter hypothesis, we asked two gradu-
ate students in computer science to (independently)
choose good indicator words for positive and nega-
tive sentiments in movie reviews. Their selections,
shown in Figure 1, seem intuitively plausible. We
then converted their responses into simple decision
procedures that essentially count the number of the
proposed positive and negative words in a given doc-
ument. We applied these procedures to uniformly-
distributed data, so that the random-choice baseline
result would be 50%. As shown in Figure 1, the
</bodyText>
<footnote confidence="0.86821">
3http://reviews.imdb.com/Reviews/
</footnote>
<figureCaption confidence="0.672305636363636">
Proposed word lists Accuracy Ties
Human 1 positive: dazzling, brilliant, phenomenal, excellent, fantastic 58% 75%
negative: suck, terrible, awful, unwatchable, hideous
Human 2 positive: gripping, mesmerizing, riveting, spectacular, cool, 64% 39%
awesome, thrilling, badass, excellent, moving, exciting
negative: bad, cliched, sucks, boring, stupid, slow
Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.
Proposed word lists Accuracy Ties
Human 3 + stats positive: love, wonderful, best, great, superb, still, beautiful 69% 16%
negative: bad, worst, stupid, waste, boring, ?, !
Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).
</figureCaption>
<bodyText confidence="0.999992888888889">
accuracy — percentage of documents classified cor-
rectly — for the human-based classifiers were 58%
and 64%, respectively.4 Note that the tie rates —
percentage of documents where the two sentiments
were rated equally likely — are quite highs (we chose
a tie breaking policy that maximized the accuracy of
the baselines).
While the tie rates suggest that the brevity of
the human-produced lists is a factor in the relatively
poor performance results, it is not the case that size
alone necessarily limits accuracy. Based on a very
preliminary examination of frequency counts in the
entire corpus (including test data) plus introspection,
we created a list of seven positive and seven negative
words (including punctuation), shown in Figure 2.
As that figure indicates, using these words raised the
accuracy to 69%. Also, although this third list is of
comparable length to the other two, it has a much
lower tie rate of 16%. We further observe that some
of the items in this third list, such as “?” or “still”,
would probably not have been proposed as possible
candidates merely through introspection, although
upon reflection one sees their merit (the question
mark tends to occur in sentences like “What was the
director thinking?”; “still” appears in sentences like
“Still, though, it was worth seeing”).
We conclude from these preliminary experiments
that it is worthwhile to explore corpus-based tech-
niques, rather than relying on prior intuitions, to se-
lect good indicator features and to perform sentiment
classification in general. These experiments also pro-
vide us with baselines for experimental comparison;
in particular, the third baseline of 69% might actu-
ally be considered somewhat difficult to beat, since
it was achieved by examination of the test data (al-
though our examination was rather cursory; we do
</bodyText>
<footnote confidence="0.998907666666667">
4Later experiments using these words as features for
machine learning methods did not yield better results.
5This is largely due to 0-0 ties.
</footnote>
<bodyText confidence="0.9816845">
not claim that our list was the optimal set of four-
teen words).
</bodyText>
<sectionHeader confidence="0.99198" genericHeader="method">
5 Machine Learning Methods
</sectionHeader>
<bodyText confidence="0.996601285714286">
Our aim in this work was to examine whether it suf-
fices to treat sentiment classification simply as a spe-
cial case of topic-based categorization (with the two
“topics” being positive sentiment and negative sen-
timent), or whether special sentiment-categorization
methods need to be developed. We experimented
with three standard algorithms: Naive Bayes clas-
sification, maximum entropy classification, and sup-
port vector machines. The philosophies behind these
three algorithms are quite different, but each has
been shown to be effective in previous text catego-
rization studies.
To implement these machine learning algorithms
on our document data, we used the following stan-
dard bag-of-features framework. Let {f1, ... , fmj be
a predefined set of m features that can appear in
a document; examples include the word “still” or
the bigram “really stinks”. Let ni(d) be the num-
ber of times fi occurs in document d. Then, each
document d is represented by the document vector
d&apos;:= (n1(d), n2(d), ... , nm(d)).
</bodyText>
<subsectionHeader confidence="0.988945">
5.1 Naive Bayes
</subsectionHeader>
<bodyText confidence="0.99986425">
One approach to text classification is to assign to a
given document d the class c* = arg maxc P(c  |d).
We derive the Naive Bayes (NB) classifier by first
observing that by Bayes’ rule,
</bodyText>
<equation confidence="0.9997145">
P(c)P(d  |c)
P(c  |d) =
</equation>
<bodyText confidence="0.999976">
where P(d) plays no role in selecting c*. To estimate
the term P(d  |c), Naive Bayes decomposes it by as-
suming the fi’s are conditionally independent given
</bodyText>
<equation confidence="0.9967326">
P(d) ,
d’s class:
P(c) �IIm i=1 P(fi  |c)ni(d)�
PNB(c  |d) :=
P(d)
</equation>
<bodyText confidence="0.997516461538462">
Our training method consists of relative-frequency
estimation of P(c) and P(fi  |c), using add-one
smoothing.
Despite its simplicity and the fact that its con-
ditional independence assumption clearly does not
hold in real-world situations, Naive Bayes-based text
categorization still tends to perform surprisingly well
(Lewis, 1998); indeed, Domingos and Pazzani (1997)
show that Naive Bayes is optimal for certain problem
classes with highly dependent features. On the other
hand, more sophisticated algorithms might (and of-
ten do) yield better results; we examine two such
algorithms next.
</bodyText>
<subsectionHeader confidence="0.999092">
5.2 Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.999389">
Maximum entropy classification (MaxEnt, or ME,
for short) is an alternative technique which has
proven effective in a number of natural lan-
guage processing applications (Berger et al., 1996).
Nigam et al. (1999) show that it sometimes, but not
always, outperforms Naive Bayes at standard text
classification. Its estimate of P(c  |d) takes the fol-
lowing exponential form:
</bodyText>
<equation confidence="0.878713666666667">
PME(c  |d) := Z( d) d) (Ei,cFi,c(d,c)
J ,
i
</equation>
<bodyText confidence="0.999376777777778">
where Z(d) is a normalization function. Fi,c is a fea-
ture/class function for feature fi and class c, defined
as follows:6
class c. The parameter values are set so as to max-
imize the entropy of the induced distribution (hence
the classifier’s name) subject to the constraint that
the expected values of the feature/class functions
with respect to the model are equal to their expected
values with respect to the training data: the under-
lying philosophy is that we should choose the model
making the fewest assumptions about the data while
still remaining consistent with it, which makes intu-
itive sense. We use ten iterations of the improved
iterative scaling algorithm (Della Pietra et al., 1997)
for parameter training (this was a sufficient num-
ber of iterations for convergence of training-data ac-
curacy), together with a Gaussian prior to prevent
overfitting (Chen and Rosenfeld, 2000).
</bodyText>
<subsectionHeader confidence="0.99857">
5.3 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999901333333334">
Support vector machines (SVMs) have been shown to
be highly effective at traditional text categorization,
generally outperforming Naive Bayes (Joachims,
1998). They are large-margin, rather than proba-
bilistic, classifiers, in contrast to Naive Bayes and
MaxEnt. In the two-category case, the basic idea be-
hind the training procedure is to find a hyperplane,
represented by vector w, that not only separates
the document vectors in one class from those in the
other, but for which the separation, or margin, is as
large as possible. This search corresponds to a con-
strained optimization problem; letting cj E 11, −11
(corresponding to positive and negative) be the cor-
rect class of document dj, the solution can be written
as
</bodyText>
<equation confidence="0.99672">
Ew := jcj dj, j &gt; 0,
j
.
�
,ic(d,�) :=J1, ni(d) &gt; 0 and c&apos; = c
l0 otherwise
</equation>
<bodyText confidence="0.9816545">
where the j’s are obtained by solving a dual opti-

mization problem. Those dj such that j is greater
For instance, a particular feature/class function
might fire if and only if the bigram “still hate” ap-
pears and the document’s sentiment is hypothesized
to be negative.7 Importantly, unlike Naive Bayes,
MaxEnt makes no assumptions about the relation-
ships between features, and so might potentially per-
form better when conditional independence assump-
tions are not met.
The i,c’s are feature-weight parameters; inspec-
tion of the definition of PME shows that a large i,c
means that fi is considered a strong indicator for
</bodyText>
<footnote confidence="0.906488333333333">
6We use a restricted definition of feature/class func-
tions so that MaxEnt relies on the same sort of feature
information as Naive Bayes.
7The dependence on class is necessary for parameter
induction. See Nigam et al. (1999) for additional moti-
vation.
</footnote>
<bodyText confidence="0.999027">
than zero are called support vectors, since they are
the only document vectors contributing to w. Clas-
sification of test instances consists simply of deter-
mining which side of w’s hyperplane they fall on.
We used Joachim’s (1999) SVMlight package8 for
training and testing, with all parameters set to their
default values, after first length-normalizing the doc-
ument vectors, as is standard (neglecting to normal-
ize generally hurt performance slightly).
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.989976">
6.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.996899333333333">
We used documents from the movie-review corpus
described in Section 3. To create a data set with uni-
form class distribution (studying the effect of skewed
</bodyText>
<footnote confidence="0.985076">
8http://svmlight.joachims.org
</footnote>
<table confidence="0.9976051">
Features # of frequency or NB ME SVM
features presence?
unigrams 16165 freq. 78.7 N/A 72.8
unigrams ” pres. 81.0 80.4 82.9
unigrams+bigrams 32330 pres. 80.6 80.8 82.7
bigrams 16165 pres. 77.3 77.4 77.1
unigrams+POS 16695 pres. 81.5 80.4 81.9
adjectives 2633 pres. 77.0 77.7 75.1
top 2633 unigrams 2633 pres. 80.3 81.0 81.4
unigrams+position 22430 pres. 81.0 80.1 81.6
</table>
<figureCaption confidence="0.9963185">
Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given
setting (row). Recall that our baseline results ranged from 50% to 69%.
</figureCaption>
<bodyText confidence="0.999703">
class distributions was out of the scope of this study),
we randomly selected 700 positive-sentiment and 700
negative-sentiment documents. We then divided this
data into three equal-sized folds, maintaining bal-
anced class distributions in each fold. (We did not
use a larger number of folds due to the slowness of
the MaxEnt training procedure.) All results reported
below, as well as the baseline results from Section 4,
are the average three-fold cross-validation results on
this data (of course, the baseline algorithms had no
parameters to tune).
To prepare the documents, we automatically re-
moved the rating indicators and extracted the tex-
tual information from the original HTML docu-
ment format, treating punctuation as separate lex-
ical items. No stemming or stoplists were used.
One unconventional step we took was to attempt
to model the potentially important contextual effect
of negation: clearly “good” and “not very good” in-
dicate opposite sentiment orientations. Adapting a
technique of Das and Chen (2001), we added the tag
NOT to every word between a negation word (“not”,
“isn’t”, “didn’t”, etc.) and the first punctuation
mark following the negation word. (Preliminary ex-
periments indicate that removing the negation tag
had a negligible, but on average slightly harmful, ef-
fect on performance.)
For this study, we focused on features based on
unigrams (with negation tagging) and bigrams. Be-
cause training MaxEnt is expensive in the number of
features, we limited consideration to (1) the 16165
unigrams appearing at least four times in our 1400-
document corpus (lower count cutoffs did not yield
significantly different results), and (2) the 16165 bi-
grams occurring most often in the same data (the
selected bigrams all occurred at least seven times).
Note that we did not add negation tags to the bi-
grams, since we consider bigrams (and n-grams in
general) to be an orthogonal way to incorporate con-
text.
</bodyText>
<subsectionHeader confidence="0.901011">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.987052609375001">
Initial unigram results The classification accu-
racies resulting from using only unigrams as fea-
tures are shown in line (1) of Figure 3. As a whole,
the machine learning algorithms clearly surpass the
random-choice baseline of 50%. They also hand-
ily beat our two human-selected-unigram baselines
of 58% and 64%, and, furthermore, perform well in
comparison to the 69% baseline achieved via limited
access to the test-data statistics, although the im-
provement in the case of SVMs is not so large.
On the other hand, in topic-based classification,
all three classifiers have been reported to use bag-
of-unigram features to achieve accuracies of 90%
and above for particular categories (Joachims, 1998;
Nigam et al., 1999)9 — and such results are for set-
tings with more than two classes. This provides
suggestive evidence that sentiment categorization is
more difficult than topic classification, which cor-
responds to the intuitions of the text categoriza-
tion expert mentioned above.10 Nonetheless, we still
wanted to investigate ways to improve our senti-
ment categorization results; these experiments are
reported below.
Feature frequency vs. presence Recall that we
represent each document d by a feature-count vector
(n1(d), ... , n,.(d)). However, the definition of the
9Joachims (1998) used stemming and stoplists; in
some of their experiments, Nigam et al. (1999), like us,
did not.
10We could not perform the natural experiment of at-
tempting topic-based categorization on our data because
the only obvious topics would be the film being reviewed;
unfortunately, in our data, the maximum number of re-
views per movie is 27, too small for meaningful results.
MaxEnt feature/class functions Fi,c only reflects the
presence or absence of a feature, rather than directly
incorporating feature frequency. In order to investi-
gate whether reliance on frequency information could
account for the higher accuracies of Naive Bayes and
SVMs, we binarized the document vectors, setting
ni(d) to 1 if and only feature fi appears in d, and
reran Naive Bayes and SVMlight on these new vec-
tors.11
As can be seen from line (2) of Figure 3,
better performance (much better performance for
SVMs) is achieved by accounting only for fea-
ture presence, not feature frequency. Interestingly,
this is in direct opposition to the observations of
McCallum and Nigam (1998) with respect to Naive
Bayes topic classification. We speculate that this in-
dicates a difference between sentiment and topic cat-
egorization — perhaps due to topic being conveyed
mostly by particular content words that tend to be
repeated — but this remains to be verified. In any
event, as a result of this finding, we did not incor-
porate frequency information into Naive Bayes and
SVMs in any of the following experiments.
Bigrams In addition to looking specifically for
negation words in the context of a word, we also
studied the use of bigrams to capture more context
in general. Note that bigrams and unigrams are
surely not conditionally independent, meaning that
the feature set they comprise violates Naive Bayes’
conditional-independence assumptions; on the other
hand, recall that this does not imply that Naive
Bayes will necessarily do poorly (Domingos and Paz-
zani, 1997).
Line (3) of the results table shows that bigram
information does not improve performance beyond
that of unigram presence, although adding in the bi-
grams does not seriously impact the results, even for
Naive Bayes. This would not rule out the possibility
that bigram presence is as equally useful a feature
as unigram presence; in fact, Pedersen (2001) found
that bigrams alone can be effective features for word
sense disambiguation. However, comparing line (4)
to line (2) shows that relying just on bigrams causes
accuracy to decline by as much as 5.8 percentage
points. Hence, if context is in fact important, as our
intuitions suggest, bigrams are not effective at cap-
turing it in our setting.
11Alternatively, we could have tried integrating fre-
quency information into MaxEnt. However, feature/class
functions are traditionally defined as binary (Berger et
al., 1996); hence, explicitly incorporating frequencies
would require different functions for each count (or count
bin), making training impractical. But cf. (Nigam et al.,
1999).
Parts of speech We also experimented with ap-
pending POS tags to every word via Oliver Mason’s
Qtag program.12 This serves as a crude form of word
sense disambiguation (Wilks and Stevenson, 1998):
for example, it would distinguish the different usages
of “love” in “I love this movie” (indicating sentiment
orientation) versus “This is a love story” (neutral
with respect to sentiment). However, the effect of
this information seems to be a wash: as depicted in
line (5) of Figure 3, the accuracy improves slightly
for Naive Bayes but declines for SVMs, and the per-
formance of MaxEnt is unchanged.
Since adjectives have been a focus of previous work
in sentiment detection (Hatzivassiloglou and Wiebe,
2000; Turney, 2002)13, we looked at the performance
of using adjectives alone. Intuitively, we might ex-
pect that adjectives carry a great deal of informa-
tion regarding a document’s sentiment; indeed, the
human-produced lists from Section 4 contain almost
no other parts of speech. Yet, the results, shown in
line (6) of Figure 3, are relatively poor: the 2633
adjectives provide less useful information than uni-
gram presence. Indeed, line (7) shows that simply
using the 2633 most frequent unigrams is a better
choice, yielding performance comparable to that of
using (the presence of) all 16165 (line (2)). This may
imply that applying explicit feature-selection algo-
rithms on unigrams could improve performance.
Position An additional intuition we had was that
the position of a word in the text might make a dif-
ference: movie reviews, in particular, might begin
with an overall sentiment statement, proceed with
a plot discussion, and conclude by summarizing the
author’s views. As a rough approximation to deter-
mining this kind of structure, we tagged each word
according to whether it appeared in the first quar-
ter, last quarter, or middle half of the document14.
The results (line (8)) didn’t differ greatly from using
unigrams alone, but more refined notions of position
might be more successful.
</bodyText>
<sectionHeader confidence="0.997557" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9995874">
The results produced via machine learning tech-
niques are quite good in comparison to the human-
generated baselines discussed in Section 4. In terms
of relative performance, Naive Bayes tends to do the
worst and SVMs tend to do the best, although the
</bodyText>
<footnote confidence="0.945811833333333">
12http://www.english.bham.ac.uk/staff/oliver/soft-
ware/tagger/index.htm
13Turney’s (2002) unsupervised algorithm uses bi-
grams containing an adjective or an adverb.
14We tried a few other settings, e.g., first third vs. last
third vs middle third, and found them to be less effective.
</footnote>
<bodyText confidence="0.994349101449275">
differences aren’t very large.
On the other hand, we were not able to achieve ac-
curacies on the sentiment classification problem com-
parable to those reported for standard topic-based
categorization, despite the several different types of
features we tried. Unigram presence information
turned out to be the most effective; in fact, none of
the alternative features we employed provided consis-
tently better performance once unigram presence was
incorporated. Interestingly, though, the superiority
of presence information in comparison to frequency
information in our setting contradicts previous obser-
vations made in topic-classification work (McCallum
and Nigam, 1998).
What accounts for these two differences — dif-
ficulty and types of information proving useful —
between topic and sentiment classification, and how
might we improve the latter? To answer these ques-
tions, we examined the data further. (All examples
below are drawn from the full 2053-document cor-
pus.)
As it turns out, a common phenomenon in the doc-
uments was a kind of “thwarted expectations” narra-
tive, where the author sets up a deliberate contrast
to earlier discussion: for example, “This film should
be brilliant. It sounds like a great plot, the actors are
first grade, and the supporting cast is good as well, and
Stallone is attempting to deliver a good performance.
However, it can’t hold up” or “I hate the Spice Girls.
...[3 things the author hates about them]... Why I saw
this movie is a really, really, really long story, but I
did, and one would think I’d despise every minute of
it. But... Okay, I’m really ashamed of it, but I enjoyed
it. I mean, I admit it’s a really awful movie ...the ninth
floor of hell...The plot is such a mess that it’s terrible.
But I loved it.” 15
In these examples, a human would easily detect
the true sentiment of the review, but bag-of-features
classifiers would presumably find these instances dif-
ficult, since there are many words indicative of the
opposite sentiment to that of the entire review. Fun-
damentally, it seems that some form of discourse
analysis is necessary (using more sophisticated tech-
15This phenomenon is related to another common
theme, that of “a good actor trapped in a bad movie”:
“AN AMERICAN WEREWOLF IN PARIS is a failed at-
tempt... Julie Delpy is far too good for this movie. She im-
bues Serafine with spirit, spunk, and humanity. This isn’t
necessarily a good thing, since it prevents us from relax-
ing and enjoying AN AMERICAN WEREWOLF IN PARIS
as a completely mindless, campy entertainment experience.
Delpy’s injection of class into an otherwise classless produc-
tion raises the specter of what this film could have been
with a better script and a better cast ... She was radiant,
charismatic, and effective ....”
niques than our positional feature mentioned above),
or at least some way of determining the focus of each
sentence, so that one can decide when the author is
talking about the film itself. (Turney (2002) makes
a similar point, noting that for reviews, “the whole
is not necessarily the sum of the parts”.) Further-
more, it seems likely that this thwarted-expectations
rhetorical device will appear in many types of texts
(e.g., editorials) devoted to expressing an overall
opinion about some topic. Hence, we believe that an
important next step is the identification of features
indicating whether sentences are on-topic (which is
a kind of co-reference problem); we look forward to
addressing this challenge in future work.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999632714285714">
We thank Joshua Goodman, Thorsten Joachims, Jon
Kleinberg, Vikas Krishna, John Lafferty, Jussi Myl-
lymaki, Phoebe Sengers, Richard Tong, Peter Tur-
ney, and the anonymous reviewers for many valuable
comments and helpful suggestions, and Hubie Chen
and Tony Faradjian for participating in our baseline
experiments. Portions of this work were done while
the first author was visiting IBM Almaden. This pa-
per is based upon work supported in part by the Na-
tional Science Foundation under ITR/IM grant IIS-
0081334. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.997458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999411308333334">
Shlomo Argamon-Engelson, Moshe Koppel, and
Galit Avneri. 1998. Style-based text categoriza-
tion: What newspaper am I reading? In Proc. of
the AAAI Workshop on Text Categorization, pages
1–4.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39–71.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for ME models. IEEE
Trans. Speech and Audio Processing, 8(1):37–50.
Sanjiv Das and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Proc. of the 8th Asia Pacific
Finance Association Annual Conference (APFA
2001).
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 19(4):380–393.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple Bayesian classifier un-
der zero-one loss. Machine Learning, 29(2-3):103–
130.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2002. Genre classification and domain transfer
for information filtering. In Proc. of the Eu-
ropean Colloquium on Information Retrieval Re-
search, pages 353–362, Glasgow.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the 35th ACL/8th EACL, pages
174–181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In Proc. of COLING.
Marti Hearst. 1992. Direction-based text interpre-
tation as an information access refinement. In
Paul Jacobs, editor, Text-Based Intelligent Sys-
tems. Lawrence Erlbaum Associates.
Alison Huettner and Pero Subasic. 2000. Fuzzy
typing for document management. In ACL
2000 Companion Volume: Tutorial Abstracts and
Demonstration Notes, pages 26–27.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Proc. of the European Confer-
ence on Machine Learning (ECML), pages 137–
142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Sch¨olkopf and
Alexander Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 44–56.
MIT Press.
Jussi Karlgren and Douglass Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proc. of COLING.
Brett Kessler, Geoffrey Nunberg, and Hinrich
Sch¨utze. 1997. Automatic detection of text genre.
In Proc. of the 35th ACL/8th EACL, pages 32–38.
David D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information retrieval.
In Proc. of the European Conference on Machine
Learning (ECML), pages 4–15. Invited talk.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proc. of the AAAI-98 Workshop on
Learning for Text Categorization, pages 41–48.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Kamal Nigam, John Lafferty, and Andrew McCal-
lum. 1999. Using maximum entropy for text clas-
sification. In Proc. of the IJCAI-99 Workshop on
Machine Learning for Information Filtering, pages
61–67.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proc. of the
Second NAACL, pages 79–86.
Warren Sack. 1994. On the computation of point of
view. In Proc. of the Twelfth AAAI, page 1488.
Student abstract.
Ellen Spertus. 1997. Smokey: Automatic recog-
nition of hostile messages. In Proc. of Innova-
tive Applications of Artificial Intelligence (IAAI),
pages 1058–1065.
Junichi Tatemura. 2000. Virtual reviewers for col-
laborative exploration of movie reviews. In Proc.
of the 5th International Conference on Intelligent
User Interfaces, pages 272–275.
Loren Terveen, Will Hill, Brian Amento, David Mc-
Donald, and Josh Creter. 1997. PHOAKS: A sys-
tem for sharing recommendations. Communica-
tions of the ACM, 40(3):59–62.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You’re not from round here, are you? Naive Bayes
detection of non-native utterance text. In Proc. of
the Second NAACL, pages 239–246.
Richard M. Tong. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. Workshop note, SIGIR 2001 Workshop on
Operational Text Classification.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from
a hundred-billion-word corpus. Technical Report
EGB-1094, National Research Council Canada.
Peter Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proc. of the ACL.
Janyce M. Wiebe, Theresa Wilson, and Matthew
Bell. 2001. Identifying collocations for recognizing
opinions. In Proc. of the ACL/EACL Workshop
on Collocation.
Yorick Wilks and Mark Stevenson. 1998. The gram-
mar of sense: Using part-of-speech tags as a first
step in semantic disambiguation. Journal of Nat-
ural Language Engineering, 4(2):135–144.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238309">
<note confidence="0.871432666666667">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86. Association for Computational Linguistics.</note>
<title confidence="0.9611365">Thumbs up? Sentiment Classification using Machine Learning Techniques</title>
<author confidence="0.971752">Pang</author>
<affiliation confidence="0.959192">Department of Computer Cornell</affiliation>
<address confidence="0.998521">Ithaca, NY 14853</address>
<author confidence="0.488241">Shivakumar</author>
<affiliation confidence="0.997276">IBM Almaden Research</affiliation>
<address confidence="0.9992835">650 Harry San Jose, CA 95120</address>
<email confidence="0.999588">shiv@almaden.ibm.com</email>
<abstract confidence="0.9989869375">We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon-Engelson</author>
<author>Moshe Koppel</author>
<author>Galit Avneri</author>
</authors>
<title>Style-based text categorization: What newspaper am I reading?</title>
<date>1998</date>
<booktitle>In Proc. of the AAAI Workshop on Text Categorization,</booktitle>
<pages>1--4</pages>
<marker>Argamon-Engelson, Koppel, Avneri, 1998</marker>
<rawString>Shlomo Argamon-Engelson, Moshe Koppel, and Galit Avneri. 1998. Style-based text categorization: What newspaper am I reading? In Proc. of the AAAI Workshop on Text Categorization, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="14150" citStr="Berger et al., 1996" startWordPosition="2162" endWordPosition="2165">ssumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features. On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next. 5.2 Maximum Entropy Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification. Its estimate of P(c |d) takes the following exponential form: PME(c |d) := Z( d) d) (Ei,cFi,c(d,c) J , i where Z(d) is a normalization function. Fi,c is a feature/class function for feature fi and class c, defined as follows:6 class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier’s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their </context>
<context position="24241" citStr="Berger et al., 1996" startWordPosition="3786" endWordPosition="3789">y that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation. However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points. Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at capturing it in our setting. 11Alternatively, we could have tried integrating frequency information into MaxEnt. However, feature/class functions are traditionally defined as binary (Berger et al., 1996); hence, explicitly incorporating frequencies would require different functions for each count (or count bin), making training impractical. But cf. (Nigam et al., 1999). Parts of speech We also experimented with appending POS tags to every word via Oliver Mason’s Qtag program.12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment). However, the effect of this information seem</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Variation across Speech and Writing.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3740" citStr="Biber, 1988" startWordPosition="548" endWordPosition="549">vie?” contains no single word that is obviously negative. (See Section 7 for more examples). Thus, sentiment seems to require more understanding than the usual topic-based classification. So, apart from presenting our results obtained via machine learning techniques, we also analyze the problem to gain a better understanding of how difficult it is. 2 Previous Work This section briefly surveys previous work on nontopic-based text categorization. One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work expli</context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Douglas Biber. 1988. Variation across Speech and Writing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Trans. Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="15263" citStr="Chen and Rosenfeld, 2000" startWordPosition="2346" endWordPosition="2349">constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000). 5.3 Support Vector Machines Support vector machines (SVMs) have been shown to be highly effective at traditional text categorization, generally outperforming Naive Bayes (Joachims, 1998). They are large-margin, rather than probabilistic, classifiers, in contrast to Naive Bayes and MaxEnt. In the two-category case, the basic idea behind the training procedure is to find a hyperplane, represented by vector w, that not only separates the document vectors in one class from those in the other, but for which the separation, or margin, is as large as possible. This search corresponds to a constrai</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for ME models. IEEE Trans. Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv Das</author>
<author>Mike Chen</author>
</authors>
<title>Yahoo! for Amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proc. of the 8th Asia Pacific Finance Association Annual Conference (APFA</booktitle>
<contexts>
<context position="5324" citStr="Das and Chen, 2001" startWordPosition="777" endWordPosition="780">. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsupervised learning technique based on the mutual information between document phrases and the words “excellent” and “poor”, where the mutual information is computed using statistics gathered by a search engine. In contrast, we utilize several completely prior-knowledge-free supervised machine learning methods, with the </context>
<context position="19220" citStr="Das and Chen (2001)" startWordPosition="2978" endWordPosition="2981">sults from Section 4, are the average three-fold cross-validation results on this data (of course, the baseline algorithms had no parameters to tune). To prepare the documents, we automatically removed the rating indicators and extracted the textual information from the original HTML document format, treating punctuation as separate lexical items. No stemming or stoplists were used. One unconventional step we took was to attempt to model the potentially important contextual effect of negation: clearly “good” and “not very good” indicate opposite sentiment orientations. Adapting a technique of Das and Chen (2001), we added the tag NOT to every word between a negation word (“not”, “isn’t”, “didn’t”, etc.) and the first punctuation mark following the negation word. (Preliminary experiments indicate that removing the negation tag had a negligible, but on average slightly harmful, effect on performance.) For this study, we focused on features based on unigrams (with negation tagging) and bigrams. Because training MaxEnt is expensive in the number of features, we limited consideration to (1) the 16165 unigrams appearing at least four times in our 1400- document corpus (lower count cutoffs did not yield sig</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>Sanjiv Das and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proc. of the 8th Asia Pacific Finance Association Annual Conference (APFA 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="15071" citStr="Pietra et al., 1997" startWordPosition="2317" endWordPosition="2320"> feature fi and class c, defined as follows:6 class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier’s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000). 5.3 Support Vector Machines Support vector machines (SVMs) have been shown to be highly effective at traditional text categorization, generally outperforming Naive Bayes (Joachims, 1998). They are large-margin, rather than probabilistic, classifiers, in contrast to Naive Bayes and MaxEnt. In the two-category case, the basic idea behind the training procedure is to find a hyperplane, represented by vecto</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Michael J Pazzani</author>
</authors>
<title>On the optimality of the simple Bayesian classifier under zero-one loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="13717" citStr="Domingos and Pazzani (1997)" startWordPosition="2095" endWordPosition="2098">t by Bayes’ rule, P(c)P(d |c) P(c |d) = where P(d) plays no role in selecting c*. To estimate the term P(d |c), Naive Bayes decomposes it by assuming the fi’s are conditionally independent given P(d) , d’s class: P(c) �IIm i=1 P(fi |c)ni(d)� PNB(c |d) := P(d) Our training method consists of relative-frequency estimation of P(c) and P(fi |c), using add-one smoothing. Despite its simplicity and the fact that its conditional independence assumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features. On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next. 5.2 Maximum Entropy Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification. Its estimate of P(c |d) takes the following expo</context>
<context position="23367" citStr="Domingos and Pazzani, 1997" startWordPosition="3646" endWordPosition="3650"> to be verified. In any event, as a result of this finding, we did not incorporate frequency information into Naive Bayes and SVMs in any of the following experiments. Bigrams In addition to looking specifically for negation words in the context of a word, we also studied the use of bigrams to capture more context in general. Note that bigrams and unigrams are surely not conditionally independent, meaning that the feature set they comprise violates Naive Bayes’ conditional-independence assumptions; on the other hand, recall that this does not imply that Naive Bayes will necessarily do poorly (Domingos and Pazzani, 1997). Line (3) of the results table shows that bigram information does not improve performance beyond that of unigram presence, although adding in the bigrams does not seriously impact the results, even for Naive Bayes. This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation. However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points. Hence, if context is in f</context>
</contexts>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>Pedro Domingos and Michael J. Pazzani. 1997. On the optimality of the simple Bayesian classifier under zero-one loss. Machine Learning, 29(2-3):103– 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
<author>Barry Smyth</author>
</authors>
<title>Genre classification and domain transfer for information filtering.</title>
<date>2002</date>
<booktitle>In Proc. of the European Colloquium on Information Retrieval Research,</booktitle>
<pages>353--362</pages>
<location>Glasgow.</location>
<contexts>
<context position="4322" citStr="Finn et al., 2002" startWordPosition="630" endWordPosition="633">ed stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguist</context>
</contexts>
<marker>Finn, Kushmerick, Smyth, 2002</marker>
<rawString>Aidan Finn, Nicholas Kushmerick, and Barry Smyth. 2002. Genre classification and domain transfer for information filtering. In Proc. of the European Colloquium on Information Retrieval Research, pages 353–362, Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th ACL/8th EACL,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="5007" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="729" endWordPosition="732">ndicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsuper</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of the 35th ACL/8th EACL, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Janyce Wiebe</author>
</authors>
<title>Effects of adjective orientation and gradability on sentence subjectivity.</title>
<date>2000</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="4455" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="648" endWordPosition="651">k Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentim</context>
<context position="25116" citStr="Hatzivassiloglou and Wiebe, 2000" startWordPosition="3925" endWordPosition="3928">d via Oliver Mason’s Qtag program.12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment). However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged. Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone. Intuitively, we might expect that adjectives carry a great deal of information regarding a document’s sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech. Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than unigram presence. Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2</context>
</contexts>
<marker>Hatzivassiloglou, Wiebe, 2000</marker>
<rawString>Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Direction-based text interpretation as an information access refinement.</title>
<date>1992</date>
<editor>In Paul Jacobs, editor, Text-Based</editor>
<contexts>
<context position="5191" citStr="Hearst, 1992" startWordPosition="759" endWordPosition="760">ents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsupervised learning technique based on the mutual information between document phrases and the words “excellent” and “poor”, where the mutual information is computed using statistics gather</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Direction-based text interpretation as an information access refinement. In Paul Jacobs, editor, Text-Based Intelligent Systems. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Huettner</author>
<author>Pero Subasic</author>
</authors>
<title>Fuzzy typing for document management.</title>
<date>2000</date>
<booktitle>In ACL 2000 Companion Volume: Tutorial Abstracts and Demonstration Notes,</booktitle>
<pages>26--27</pages>
<contexts>
<context position="5304" citStr="Huettner and Subasic, 2000" startWordPosition="773" endWordPosition="776">hat that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsupervised learning technique based on the mutual information between document phrases and the words “excellent” and “poor”, where the mutual information is computed using statistics gathered by a search engine. In contrast, we utilize several completely prior-knowledge-free supervised machine learnin</context>
</contexts>
<marker>Huettner, Subasic, 2000</marker>
<rawString>Alison Huettner and Pero Subasic. 2000. Fuzzy typing for document management. In ACL 2000 Companion Volume: Tutorial Abstracts and Demonstration Notes, pages 26–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proc. of the European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<contexts>
<context position="15451" citStr="Joachims, 1998" startWordPosition="2373" endWordPosition="2374">e should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000). 5.3 Support Vector Machines Support vector machines (SVMs) have been shown to be highly effective at traditional text categorization, generally outperforming Naive Bayes (Joachims, 1998). They are large-margin, rather than probabilistic, classifiers, in contrast to Naive Bayes and MaxEnt. In the two-category case, the basic idea behind the training procedure is to find a hyperplane, represented by vector w, that not only separates the document vectors in one class from those in the other, but for which the separation, or margin, is as large as possible. This search corresponds to a constrained optimization problem; letting cj E 11, −11 (corresponding to positive and negative) be the correct class of document dj, the solution can be written as Ew := jcj dj, j &gt; 0, j . � ,</context>
<context position="20835" citStr="Joachims, 1998" startWordPosition="3243" endWordPosition="3244">igrams as features are shown in line (1) of Figure 3. As a whole, the machine learning algorithms clearly surpass the random-choice baseline of 50%. They also handily beat our two human-selected-unigram baselines of 58% and 64%, and, furthermore, perform well in comparison to the 69% baseline achieved via limited access to the test-data statistics, although the improvement in the case of SVMs is not so large. On the other hand, in topic-based classification, all three classifiers have been reported to use bagof-unigram features to achieve accuracies of 90% and above for particular categories (Joachims, 1998; Nigam et al., 1999)9 — and such results are for settings with more than two classes. This provides suggestive evidence that sentiment categorization is more difficult than topic classification, which corresponds to the intuitions of the text categorization expert mentioned above.10 Nonetheless, we still wanted to investigate ways to improve our sentiment categorization results; these experiments are reported below. Feature frequency vs. presence Recall that we represent each document d by a feature-count vector (n1(d), ... , n,.(d)). However, the definition of the 9Joachims (1998) used stemm</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proc. of the European Conference on Machine Learning (ECML), pages 137– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Bernhard Sch¨olkopf and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>44--56</pages>
<publisher>MIT Press.</publisher>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Bernhard Sch¨olkopf and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 44–56. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jussi Karlgren</author>
<author>Douglass Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="4280" citStr="Karlgren and Cutting, 1994" startWordPosition="622" endWordPosition="625"> source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of i</context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>Jussi Karlgren and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett Kessler</author>
<author>Geoffrey Nunberg</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th ACL/8th EACL,</booktitle>
<pages>32--38</pages>
<marker>Kessler, Nunberg, Sch¨utze, 1997</marker>
<rawString>Brett Kessler, Geoffrey Nunberg, and Hinrich Sch¨utze. 1997. Automatic detection of text genre. In Proc. of the 35th ACL/8th EACL, pages 32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of the European Conference on Machine Learning (ECML),</booktitle>
<pages>4--15</pages>
<note>Invited talk.</note>
<contexts>
<context position="13680" citStr="Lewis, 1998" startWordPosition="2092" endWordPosition="2093">by first observing that by Bayes’ rule, P(c)P(d |c) P(c |d) = where P(d) plays no role in selecting c*. To estimate the term P(d |c), Naive Bayes decomposes it by assuming the fi’s are conditionally independent given P(d) , d’s class: P(c) �IIm i=1 P(fi |c)ni(d)� PNB(c |d) := P(d) Our training method consists of relative-frequency estimation of P(c) and P(fi |c), using add-one smoothing. Despite its simplicity and the fact that its conditional independence assumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features. On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next. 5.2 Maximum Entropy Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification. Its estimat</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Proc. of the European Conference on Machine Learning (ECML), pages 4–15. Invited talk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Proc. of the AAAI-98 Workshop on Learning for Text Categorization,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="22484" citStr="McCallum and Nigam (1998)" startWordPosition="3503" endWordPosition="3506"> or absence of a feature, rather than directly incorporating feature frequency. In order to investigate whether reliance on frequency information could account for the higher accuracies of Naive Bayes and SVMs, we binarized the document vectors, setting ni(d) to 1 if and only feature fi appears in d, and reran Naive Bayes and SVMlight on these new vectors.11 As can be seen from line (2) of Figure 3, better performance (much better performance for SVMs) is achieved by accounting only for feature presence, not feature frequency. Interestingly, this is in direct opposition to the observations of McCallum and Nigam (1998) with respect to Naive Bayes topic classification. We speculate that this indicates a difference between sentiment and topic categorization — perhaps due to topic being conveyed mostly by particular content words that tend to be repeated — but this remains to be verified. In any event, as a result of this finding, we did not incorporate frequency information into Naive Bayes and SVMs in any of the following experiments. Bigrams In addition to looking specifically for negation words in the context of a word, we also studied the use of bigrams to capture more context in general. Note that bigram</context>
<context position="27630" citStr="McCallum and Nigam, 1998" startWordPosition="4311" endWordPosition="4314">hand, we were not able to achieve accuracies on the sentiment classification problem comparable to those reported for standard topic-based categorization, despite the several different types of features we tried. Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classification work (McCallum and Nigam, 1998). What accounts for these two differences — difficulty and types of information proving useful — between topic and sentiment classification, and how might we improve the latter? To answer these questions, we examined the data further. (All examples below are drawn from the full 2053-document corpus.) As it turns out, a common phenomenon in the documents was a kind of “thwarted expectations” narrative, where the author sets up a deliberate contrast to earlier discussion: for example, “This film should be brilliant. It sounds like a great plot, the actors are first grade, and the supporting cast</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Proc. of the AAAI-98 Workshop on Learning for Text Categorization, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Applied Bayesian and Classical Inference: The Case of the Federalist Papers.</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3964" citStr="Mosteller and Wallace, 1984" startWordPosition="579" endWordPosition="582">ting our results obtained via machine learning techniques, we also analyze the problem to gain a better understanding of how difficult it is. 2 Previous Work This section briefly surveys previous work on nontopic-based text categorization. One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us </context>
</contexts>
<marker>Mosteller, Wallace, 1984</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1984. Applied Bayesian and Classical Inference: The Case of the Federalist Papers. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
</authors>
<title>Using maximum entropy for text classification.</title>
<date>1999</date>
<booktitle>In Proc. of the IJCAI-99 Workshop on Machine Learning for Information Filtering,</booktitle>
<pages>61--67</pages>
<contexts>
<context position="14171" citStr="Nigam et al. (1999)" startWordPosition="2166" endWordPosition="2169"> not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features. On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next. 5.2 Maximum Entropy Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural language processing applications (Berger et al., 1996). Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification. Its estimate of P(c |d) takes the following exponential form: PME(c |d) := Z( d) d) (Ei,cFi,c(d,c) J , i where Z(d) is a normalization function. Fi,c is a feature/class function for feature fi and class c, defined as follows:6 class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier’s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with </context>
<context position="16946" citStr="Nigam et al. (1999)" startWordPosition="2627" endWordPosition="2630">s sentiment is hypothesized to be negative.7 Importantly, unlike Naive Bayes, MaxEnt makes no assumptions about the relationships between features, and so might potentially perform better when conditional independence assumptions are not met. The i,c’s are feature-weight parameters; inspection of the definition of PME shows that a large i,c means that fi is considered a strong indicator for 6We use a restricted definition of feature/class functions so that MaxEnt relies on the same sort of feature information as Naive Bayes. 7The dependence on class is necessary for parameter induction. See Nigam et al. (1999) for additional motivation. than zero are called support vectors, since they are the only document vectors contributing to w. Classification of test instances consists simply of determining which side of w’s hyperplane they fall on. We used Joachim’s (1999) SVMlight package8 for training and testing, with all parameters set to their default values, after first length-normalizing the document vectors, as is standard (neglecting to normalize generally hurt performance slightly). 6 Evaluation 6.1 Experimental Set-up We used documents from the movie-review corpus described in Section 3. To creat</context>
<context position="20856" citStr="Nigam et al., 1999" startWordPosition="3245" endWordPosition="3248">es are shown in line (1) of Figure 3. As a whole, the machine learning algorithms clearly surpass the random-choice baseline of 50%. They also handily beat our two human-selected-unigram baselines of 58% and 64%, and, furthermore, perform well in comparison to the 69% baseline achieved via limited access to the test-data statistics, although the improvement in the case of SVMs is not so large. On the other hand, in topic-based classification, all three classifiers have been reported to use bagof-unigram features to achieve accuracies of 90% and above for particular categories (Joachims, 1998; Nigam et al., 1999)9 — and such results are for settings with more than two classes. This provides suggestive evidence that sentiment categorization is more difficult than topic classification, which corresponds to the intuitions of the text categorization expert mentioned above.10 Nonetheless, we still wanted to investigate ways to improve our sentiment categorization results; these experiments are reported below. Feature frequency vs. presence Recall that we represent each document d by a feature-count vector (n1(d), ... , n,.(d)). However, the definition of the 9Joachims (1998) used stemming and stoplists; in</context>
<context position="24409" citStr="Nigam et al., 1999" startWordPosition="3809" endWordPosition="3812">ambiguation. However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points. Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at capturing it in our setting. 11Alternatively, we could have tried integrating frequency information into MaxEnt. However, feature/class functions are traditionally defined as binary (Berger et al., 1996); hence, explicitly incorporating frequencies would require different functions for each count (or count bin), making training impractical. But cf. (Nigam et al., 1999). Parts of speech We also experimented with appending POS tags to every word via Oliver Mason’s Qtag program.12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment). However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged. S</context>
</contexts>
<marker>Nigam, Lafferty, McCallum, 1999</marker>
<rawString>Kamal Nigam, John Lafferty, and Andrew McCallum. 1999. Using maximum entropy for text classification. In Proc. of the IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A decision tree of bigrams is an accurate predictor of word sense.</title>
<date>2001</date>
<booktitle>In Proc. of the Second NAACL,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="23720" citStr="Pedersen (2001)" startWordPosition="3707" endWordPosition="3708">rely not conditionally independent, meaning that the feature set they comprise violates Naive Bayes’ conditional-independence assumptions; on the other hand, recall that this does not imply that Naive Bayes will necessarily do poorly (Domingos and Pazzani, 1997). Line (3) of the results table shows that bigram information does not improve performance beyond that of unigram presence, although adding in the bigrams does not seriously impact the results, even for Naive Bayes. This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation. However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points. Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at capturing it in our setting. 11Alternatively, we could have tried integrating frequency information into MaxEnt. However, feature/class functions are traditionally defined as binary (Berger et al., 1996); hence, explicitly incorporating frequencies would require different functions</context>
</contexts>
<marker>Pedersen, 2001</marker>
<rawString>Ted Pedersen. 2001. A decision tree of bigrams is an accurate predictor of word sense. In Proc. of the Second NAACL, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warren Sack</author>
</authors>
<title>On the computation of point of view.</title>
<date>1994</date>
<booktitle>In Proc. of the Twelfth AAAI,</booktitle>
<pages>1488</pages>
<note>Student abstract.</note>
<contexts>
<context position="5204" citStr="Sack, 1994" startWordPosition="761" endWordPosition="762">ess an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsupervised learning technique based on the mutual information between document phrases and the words “excellent” and “poor”, where the mutual information is computed using statistics gathered by a searc</context>
</contexts>
<marker>Sack, 1994</marker>
<rawString>Warren Sack. 1994. On the computation of point of view. In Proc. of the Twelfth AAAI, page 1488. Student abstract.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Spertus</author>
</authors>
<title>Smokey: Automatic recognition of hostile messages.</title>
<date>1997</date>
<booktitle>In Proc. of Innovative Applications of Artificial Intelligence (IAAI),</booktitle>
<pages>1058--1065</pages>
<contexts>
<context position="2713" citStr="Spertus, 1997" startWordPosition="396" endWordPosition="397">fferent rating schemes that individual reviewers use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye’s Lexant system&apos;) and recommender systems (e.g., Terveen et al. (1997), Tatemura (2000)), where user input and feedback could be quickly summarized; indeed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization. Moreover, there are also potential applications to message filtering; for example, one might be able to use sentiment information to recognize and discard “flames”(Spertus, 1997). In this paper, we examine the effectiveness of applying machine learning techniques to the sentiment classification problem. A challenging aspect of this problem that seems to distinguish it from traditional topic-based classification is that while topics are often identifiable by keywords alone, sentiment can be expressed in a more subtle manner. For example, the sentence “How could anyone sit through this movie?” contains no single word that is obviously negative. (See Section 7 for more examples). Thus, sentiment seems to require more understanding than the usual topic-based classificatio</context>
</contexts>
<marker>Spertus, 1997</marker>
<rawString>Ellen Spertus. 1997. Smokey: Automatic recognition of hostile messages. In Proc. of Innovative Applications of Artificial Intelligence (IAAI), pages 1058–1065.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Tatemura</author>
</authors>
<title>Virtual reviewers for collaborative exploration of movie reviews.</title>
<date>2000</date>
<booktitle>In Proc. of the 5th International Conference on Intelligent User Interfaces,</booktitle>
<pages>272--275</pages>
<contexts>
<context position="2342" citStr="Tatemura (2000)" startWordPosition="343" endWordPosition="344">he subject matter — for example, whether a product review is positive or negative. Labeling these articles with their sentiment would provide succinct summaries to readers; indeed, these labels are part of the appeal and value-add of such sites as www.rottentomatoes.com, which both labels movie reviews that do not contain explicit rating indicators and normalizes the different rating schemes that individual reviewers use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye’s Lexant system&apos;) and recommender systems (e.g., Terveen et al. (1997), Tatemura (2000)), where user input and feedback could be quickly summarized; indeed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization. Moreover, there are also potential applications to message filtering; for example, one might be able to use sentiment information to recognize and discard “flames”(Spertus, 1997). In this paper, we examine the effectiveness of applying machine learning techniques to the sentiment classification problem. A challenging aspect of this problem that seems to distinguish it from traditional topic-based classif</context>
</contexts>
<marker>Tatemura, 2000</marker>
<rawString>Junichi Tatemura. 2000. Virtual reviewers for collaborative exploration of movie reviews. In Proc. of the 5th International Conference on Intelligent User Interfaces, pages 272–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loren Terveen</author>
<author>Will Hill</author>
<author>Brian Amento</author>
<author>David McDonald</author>
<author>Josh Creter</author>
</authors>
<title>PHOAKS: A system for sharing recommendations.</title>
<date>1997</date>
<journal>Communications of the ACM,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="2325" citStr="Terveen et al. (1997)" startWordPosition="339" endWordPosition="342">erall opinion towards the subject matter — for example, whether a product review is positive or negative. Labeling these articles with their sentiment would provide succinct summaries to readers; indeed, these labels are part of the appeal and value-add of such sites as www.rottentomatoes.com, which both labels movie reviews that do not contain explicit rating indicators and normalizes the different rating schemes that individual reviewers use. Sentiment classification would also be helpful in business intelligence applications (e.g. MindfulEye’s Lexant system&apos;) and recommender systems (e.g., Terveen et al. (1997), Tatemura (2000)), where user input and feedback could be quickly summarized; indeed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization. Moreover, there are also potential applications to message filtering; for example, one might be able to use sentiment information to recognize and discard “flames”(Spertus, 1997). In this paper, we examine the effectiveness of applying machine learning techniques to the sentiment classification problem. A challenging aspect of this problem that seems to distinguish it from traditional to</context>
</contexts>
<marker>Terveen, Hill, Amento, McDonald, Creter, 1997</marker>
<rawString>Loren Terveen, Will Hill, Brian Amento, David McDonald, and Josh Creter. 1997. PHOAKS: A system for sharing recommendations. Communications of the ACM, 40(3):59–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Mayfield Tomokiyo</author>
<author>Rosie Jones</author>
</authors>
<title>You’re not from round here, are you? Naive Bayes detection of non-native utterance text.</title>
<date>2001</date>
<booktitle>In Proc. of the Second NAACL,</booktitle>
<pages>239--246</pages>
<contexts>
<context position="4065" citStr="Tomokiyo and Jones, 2001" startWordPosition="588" endWordPosition="591">understanding of how difficult it is. 2 Previous Work This section briefly surveys previous work on nontopic-based text categorization. One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue. Examples include author, publisher (e.g., the New York Times vs. The Daily News), native-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of </context>
</contexts>
<marker>Tomokiyo, Jones, 2001</marker>
<rawString>Laura Mayfield Tomokiyo and Rosie Jones. 2001. You’re not from round here, are you? Naive Bayes detection of non-native utterance text. In Proc. of the Second NAACL, pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard M Tong</author>
</authors>
<title>An operational system for detecting and tracking opinions in on-line discussion.</title>
<date>2001</date>
<booktitle>Workshop note, SIGIR 2001 Workshop on Operational Text Classification.</booktitle>
<contexts>
<context position="5337" citStr="Tong, 2001" startWordPosition="781" endWordPosition="782">arch on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsupervised learning technique based on the mutual information between document phrases and the words “excellent” and “poor”, where the mutual information is computed using statistics gathered by a search engine. In contrast, we utilize several completely prior-knowledge-free supervised machine learning methods, with the goal of under</context>
</contexts>
<marker>Tong, 2001</marker>
<rawString>Richard M. Tong. 2001. An operational system for detecting and tracking opinions in on-line discussion. Workshop note, SIGIR 2001 Workshop on Operational Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Unsupervised learning of semantic orientation from a hundred-billion-word corpus.</title>
<date>2002</date>
<tech>Technical Report EGB-1094,</tech>
<institution>National Research Council Canada.</institution>
<contexts>
<context position="5034" citStr="Turney and Littman, 2002" startWordPosition="733" endWordPosition="736">s being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001). Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words. Turney’s (2002) work on classification of reviews is perhaps the closest to ours.2 He applied a specific unsupervised learning technique ba</context>
</contexts>
<marker>Turney, Littman, 2002</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2002. Unsupervised learning of semantic orientation from a hundred-billion-word corpus. Technical Report EGB-1094, National Research Council Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="6428" citStr="Turney (2002)" startWordPosition="949" endWordPosition="950">n contrast, we utilize several completely prior-knowledge-free supervised machine learning methods, with the goal of understanding the inherent difficulty of the task. 3 The Movie-Review Domain For our experiments, we chose to work with movie reviews. This domain is experimentally convenient because there are large on-line collections of such reviews, and because reviewers often summarize their overall sentiment with a machine-extractable rating indicator, such as a number of stars; hence, we did not need to hand-label the data for supervised learning or evaluation purposes. We also note that Turney (2002) found movie reviews to be the most 2Indeed, although our choice of title was completely independent of his, our selections were eerily similar. difficult of several domains for sentiment classification, reporting an accuracy of 65.83% on a 120- document set (random-choice performance: 50%). But we stress that the machine learning methods and features we use are not specific to movie reviews, and should be easily applicable to other domains as long as sufficient training data exists. Our data source was the Internet Movie Database (IMDb) archive of the rec.arts.movies.reviews newsgroup.3 We se</context>
<context position="25131" citStr="Turney, 2002" startWordPosition="3929" endWordPosition="3930">12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment). However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged. Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone. Intuitively, we might expect that adjectives carry a great deal of information regarding a document’s sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech. Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than unigram presence. Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2)). This may im</context>
<context position="29924" citStr="Turney (2002)" startWordPosition="4708" endWordPosition="4709">t, spunk, and humanity. This isn’t necessarily a good thing, since it prevents us from relaxing and enjoying AN AMERICAN WEREWOLF IN PARIS as a completely mindless, campy entertainment experience. Delpy’s injection of class into an otherwise classless production raises the specter of what this film could have been with a better script and a better cast ... She was radiant, charismatic, and effective ....” niques than our positional feature mentioned above), or at least some way of determining the focus of each sentence, so that one can decide when the author is talking about the film itself. (Turney (2002) makes a similar point, noting that for reviews, “the whole is not necessarily the sum of the parts”.) Furthermore, it seems likely that this thwarted-expectations rhetorical device will appear in many types of texts (e.g., editorials) devoted to expressing an overall opinion about some topic. Hence, we believe that an important next step is the identification of features indicating whether sentences are on-topic (which is a kind of co-reference problem); we look forward to addressing this challenge in future work. Acknowledgments We thank Joshua Goodman, Thorsten Joachims, Jon Kleinberg, Vika</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Theresa Wilson</author>
<author>Matthew Bell</author>
</authors>
<title>Identifying collocations for recognizing opinions.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL/EACL Workshop on Collocation.</booktitle>
<contexts>
<context position="4476" citStr="Wiebe et al., 2001" startWordPosition="652" endWordPosition="655">e-language background, and “brow” (e.g., high-brow vs. “popular”, or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et lhttp://www.mindfuleye.com/about/lexant.htm al., 1998; Tomokiyo and Jones, 2001; Kessler et al., 1997). Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002). Other work explicitly attempts to find features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our specific classification task of determining what that opinion actually is. Most previous research on sentiment-based classification has been at least partially knowledge-based. Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002). Past work on sentiment-based categorizat</context>
</contexts>
<marker>Wiebe, Wilson, Bell, 2001</marker>
<rawString>Janyce M. Wiebe, Theresa Wilson, and Matthew Bell. 2001. Identifying collocations for recognizing opinions. In Proc. of the ACL/EACL Workshop on Collocation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Mark Stevenson</author>
</authors>
<title>The grammar of sense: Using part-of-speech tags as a first step in semantic disambiguation.</title>
<date>1998</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="24605" citStr="Wilks and Stevenson, 1998" startWordPosition="3842" endWordPosition="3845">t, as our intuitions suggest, bigrams are not effective at capturing it in our setting. 11Alternatively, we could have tried integrating frequency information into MaxEnt. However, feature/class functions are traditionally defined as binary (Berger et al., 1996); hence, explicitly incorporating frequencies would require different functions for each count (or count bin), making training impractical. But cf. (Nigam et al., 1999). Parts of speech We also experimented with appending POS tags to every word via Oliver Mason’s Qtag program.12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of “love” in “I love this movie” (indicating sentiment orientation) versus “This is a love story” (neutral with respect to sentiment). However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged. Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone. Intuitively, w</context>
</contexts>
<marker>Wilks, Stevenson, 1998</marker>
<rawString>Yorick Wilks and Mark Stevenson. 1998. The grammar of sense: Using part-of-speech tags as a first step in semantic disambiguation. Journal of Natural Language Engineering, 4(2):135–144.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>