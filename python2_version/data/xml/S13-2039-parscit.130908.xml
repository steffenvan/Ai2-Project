<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.261565">
<title confidence="0.91694">
unimelb: Topic Modelling-based Word Sense Induction for Web Snippet
Clustering
</title>
<author confidence="0.998529">
Jey Han Lau, Paul Cook and Timothy Baldwin
</author>
<affiliation confidence="0.9990845">
Department of Computing and Information Systems
The University of Melbourne
</affiliation>
<email confidence="0.963941">
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
</email>
<sectionHeader confidence="0.995384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999166066666667">
This paper describes our system for Task 11
of SemEval-2013. In the task, participants
are provided with a set of ambiguous search
queries and the snippets returned by a search
engine, and are asked to associate senses with
the snippets. The snippets are then clus-
tered using the sense assignments and sys-
tems are evaluated based on the quality of the
snippet clusters. Our system adopts a pre-
existing Word Sense Induction (WSI) method-
ology based on Hierarchical Dirichlet Process
(HDP), a non-parametric topic model. Our
system is trained over extracts from the full
text of English Wikipedia, and is shown to per-
form well in the shared task.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953944444445">
The basic premise behind research on word sense
disambiguation (WSD) is that there exists a static,
discrete set of word senses that can be used to la-
bel distinct usages of a given word (Agirre and Ed-
monds, 2006; Navigli, 2009). There are various pit-
falls underlying this premise, including: (1) what
sense inventory is appropriate for a particular task
(given that sense inventories can vary considerably
in their granularity and partitioning of word usages)?
(2) given that word senses tend to take the form of
prototypes, is discrete labelling a felicitous represen-
tation of word usages, especially for non-standard
word usages? (3) how should novel word usages be
captured under this model? and (4) given the rapid
pace of language evolution on real-time social me-
dia such as Twitter and Facebook, is it reasonable
to assume a static sense inventory? Given this back-
drop, there has been a recent growth of interest in the
task of word sense induction (WSI), where the word
sense representation for a given word is automati-
cally inferred from a given data source, and word
usages are labelled (often probabilistically) accord-
ing to that data source. While WSI has considerable
appeal as a task, intrinsic cross-comparison of WSI
systems is fraught with many of the same issues as
WSD (Agirre and Soroa, 2007; Manandhar et al.,
2010), leading to a move towards task-based WSI
evaluation, such as in Task 11 of SemEval-2013, ti-
tled “Evaluating Word Sense Induction &amp; Disam-
biguation within an End-User Application”.
This paper presents the UNIMELB system entry to
SemEval-2013 Task 11. Our method is based heav-
ily on the WSI methodology proposed by Lau et
al. (2012) for novel word sense detection. Largely
the same methodology was also applied to SemEval-
2013 Task 13 on WSI (Lau et al., to appear).
</bodyText>
<sectionHeader confidence="0.973849" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999958583333333">
Our system is based on the WSI methodology pro-
posed by Lau et al. (2012) for the task of novel word
sense detection. The core machinery of our sys-
tem is driven by a Latent Dirichlet Allocation (LDA)
topic model (Blei et al., 2003). In LDA, the model
learns latent topics for a collection of documents,
and associates these latent topics with every docu-
ment in the collection. A topic is represented by
a multinomial distribution of words, and the asso-
ciation of topics with documents is represented by a
multinomial distribution of topics, with one distribu-
tion per document. The generative process of LDA
</bodyText>
<page confidence="0.979082">
217
</page>
<bodyText confidence="0.947951333333333">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 217–221, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
for drawing word w in document d is as follows:
</bodyText>
<listItem confidence="0.9661885">
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
</listItem>
<bodyText confidence="0.9581245">
The probability of selecting word w given a doc-
ument d is thus given by:
</bodyText>
<equation confidence="0.993085">
T
P(wld) = E P(wlt = z)P(t = z1d).
z=1
</equation>
<bodyText confidence="0.999960269230769">
where t is the topic variable, and T is the number of
topics.
The number of topics, T, is a parameter in LDA,
and the model tends to be highly sensitive to this set-
ting. To remove the need for parameter tuning over
development data, we make use of a non-parametric
variant of LDA, in the form of a Hierarchical Dirich-
let Process (HDP: Teh et al. (2006)). HDP learns the
number of topics based on data, and the concentra-
tion parameters -y and α0 control the variability of
topics in the documents (for details of HDP please
refer to the original paper, Teh et al. (2006)).
To apply HDP in the context of WSI, the latent
topics are interpreted as the word senses, and the
documents are usages that contain the target word of
interest (or search query in the case of Task 11). That
is, given a search query (e.g. Prince of Persia), a
“document” in our application is a sentence/snippet
containing the target word. In addition to the bag of
words surrounding the target word, we also include
positional context word information, as used in the
original methodology of Lau et al. (2012). That is,
we introduce an additional word feature for each of
the three words to the left and right of the target
word. An example of the topic model features for
a context sentence is given in Table 1.
</bodyText>
<subsectionHeader confidence="0.999523">
2.1 Background Corpus and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999176088888889">
As part of the task setup, we were provided with
snippets for each search query, constituting the doc-
uments for the topic model for that query (each
search query is topic-modelled separately). Our sys-
tem uses only the text of the snippets as features, and
ignores the URL information. The text of the snip-
pets is tokenised and lemmatised using OpenNLP
and Morpha (Minnen et al., 2001).
As there are only 64 snippets for each query in
the test dataset, which is very small by topic mod-
elling standards, we turn to English Wikipedia to
expand the data, by extracting all context sentences
that contain the search query in the full collection
of Wikipedia articles.1 Each extracted usage is a
three-sentence context containing the search query:
the original sentence that contains the actual usage
and its preceding and succeeding sentences. The
extraction of usages from Wikipedia significantly
increases the amount of information for the topic
model to learn the senses for the search queries. To
give an estimate: for very ambiguous queries such
as queen we extracted almost 150,000 usages from
Wikipedia; for most queries, however, this number
tends to be a few thousand usages.
To summarise, for each search query we apply the
HDP model to the combined collection of the 64
snippets and the extracted usages from Wikipedia.
The topic model learns the senses/topics for all
documents in the collection, but we only use the
sense/topic distribution for the 64 snippets as they
are the documents that are evaluated in the shared
task.
Our English Wikipedia collection is tokenised and
lemmatised using OpenNLP and Morpha (Minnen et
al., 2001). The search queries provided in the task,
however, are not lemmatised. Two approaches are
used to extract the usages of search queries from
Wikipedia:
HDP-CLUSTERS-LEMMA Search queries are lem-
matised using Morpha (Minnen et al., 2001),
and both the original and lemmatised forms are
used for extraction;2
HDP-CLUSTERS-NOLEMMA Search queries are
not lemmatised and only their original forms
are used for extraction.
</bodyText>
<footnote confidence="0.9992882">
1The Wikipedia dump was retrieved on November 28th
2009.
2Morpha requires the part-of-speech (POS) of a given word,
which is determined by the majority POS aggregated over all of
that word’s occurrences in Wikipedia.
</footnote>
<page confidence="0.967984">
218
</page>
<table confidence="0.99108275">
Search query dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
</table>
<tableCaption confidence="0.998643">
Table 1: An example of topic model features.
</tableCaption>
<table confidence="0.999752533333333">
System F1 ARI RI JI Avg. No. of Avg. Cluster
Clusters Size
HDP-CLUSTERS-LEMMA 0.6830 0.2131 0.6522 0.3302 6.6300 11.0756
HDP-CLUSTERS-NOLEMMA 0.6803 0.2149 0.6486 0.3375 6.5400 11.6803
TASK11.DULUTH.SYS1.PK2 0.5683 0.0574 0.5218 0.3179 2.5300 26.4533
TASK11.DULUTH.SYS7.PK2 0.5878 0.0678 0.5204 0.3103 3.0100 25.1596
TASK11.DULUTH.SYS9.PK2 0.5702 0.0259 0.5463 0.2224 3.3200 19.8400
TASK11-SATTY-APPROACH1 0.6709 0.0719 0.5955 0.1505 9.9000 6.4631
TASK11-UKP-WSI-WACKY-LLR 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434
TASK11-UKP-WSI-WP-LLR2 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702
TASK11-UKP-WSI-WP-PMI 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098
RAKESH 0.3949 0.0811 0.5876 0.3052 9.0700 2.9441
SINGLETON 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000
ALLINONE 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000
GOLD 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630
</table>
<tableCaption confidence="0.982845">
Table 2: Cluster quality results for all systems. The best result for each column is presented in boldface. SINGLETON
and ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task.
</tableCaption>
<sectionHeader confidence="0.994471" genericHeader="evaluation">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999740764705883">
Following Lau et al. (2012), we use the default pa-
rameters (-y = 0.1 and α0 = 1.0) for HDP.3 For each
search query, we apply HDP to induce the senses,
and a distribution of senses is produced for each
“document” in the model. As the snippets in the test
dataset correspond to the documents in the model
and evaluation is based on “hard” clusters of snip-
pets, we assign a sense to each snippet based on the
sense (= topic) which has the highest probability for
that snippet.
The task requires participants to produce a ranked
list of snippets for each induced sense, based on the
relative fit between the snippet and the sense. We in-
duce the ranking based on the sense probabilities as-
signed to the senses, such that snippets that have the
highest probability of the induced sense are ranked
highest, and snippets with lower sense probabilities
</bodyText>
<footnote confidence="0.753691">
3Our implementation can be accessed via https://
github.com/jhlau/hdp-wsi.
</footnote>
<bodyText confidence="0.890127">
are ranked lower.
Two classes of evaluation are used in the shared
task:
</bodyText>
<listItem confidence="0.9993542">
1. cluster quality measures: Jaccard Index (JI),
RandIndex (RI), Adjusted RandIndex (ARI)
and F1;
2. diversification of search results: Subtopic Re-
call@K and Subtopic Precision@r.
</listItem>
<bodyText confidence="0.996441833333333">
Details of the evaluation measures are described in
Navigli and Vannella (2013).
The idea behind the second form of evaluation
(i.e. diversification of search results) is that search
engine results should cluster the results based on
senses (of the query term in the documents) given an
ambiguous query. For example, if a user searches for
apple, the search engine may return results related to
both the computer brand sense and the fruit sense of
apple. Given this assumption, the best WSI/WSD
system is the one that can correctly identify the di-
versity of senses in the snippets.
</bodyText>
<page confidence="0.998994">
219
</page>
<figureCaption confidence="0.999679">
Figure 1: Subtopic Recall@K for all participating systems.
</figureCaption>
<bodyText confidence="0.999968904761905">
Cluster quality, subtopic recall@K and subtopic
precision@r results for all systems entered in the
task are presented in Table 2, Figure 1 and Figure 2,
respectively.
In terms of cluster quality, our systems
(HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA) consistently outperform the other teams
for all measures except for the Jaccard Index (where
we rank second and third, by a narrow margin). The
average number of induced clusters and the average
cluster size of our systems are similar to those
of the gold standard system (GOLD), indicating
that our systems are learning an appropriate sense
granularity.
In terms of diversification of search results, our
systems perform markedly better than most teams,
other than RAKESH which trails closely behind our
systems (despite a relatively low ranking in terms of
the cluster quality evaluation). Overall, the results
are encouraging and our system performs very well
over the task.
</bodyText>
<sectionHeader confidence="0.99313" genericHeader="conclusions">
4 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.9999655">
Our system adopts the WSI system proposed in Lau
et al. (2012) with no parameters tuned for this task,
and performs very well over it. Parameter tuning and
exploiting URL information in the snippets could
potentially boost the system performance further.
Other background corpora (such as news articles)
could also be used to increase the size of the training
data. We leave these ideas for future work.
Inspecting the difference between the HDP-
CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA approaches, only 6 out of the 100
lemmas have a lemmatised form which differs from
the original query composition: pods (pod), ten
commandments (ten commandment), guild wars
(guild war), stand by me (stand by i), sisters of
mercy (sister of mercy) and lord of the flies (lord of
the fly). In most cases, including the lemmatised
query results in the extraction of additional useful
usages, e.g. using only the original form lord of
the flies would extract no usages from Wikipedia
(because this corpus has itself been lemmatised).
In other cases, however, including the lemmatised
forms results in many common noun usages, e.g.
the number of usages of the lemmatised pod is
significantly greater than that of the original form
pods (which corresponds to proper noun usages in
the lemmatised corpus), resulting in senses being
induced only for common noun usages of pods. The
</bodyText>
<page confidence="0.995279">
220
</page>
<figureCaption confidence="0.999354">
Figure 2: Subtopic Precision@r for all participating systems.
</figureCaption>
<bodyText confidence="0.999899555555556">
advantages and disadvantages of both approaches
are reflected in the results: performance is mixed
and no one method clearly outperforms the other.
To conclude, we apply a topic model-based WSI
methodology to the task of web result clustering, us-
ing English Wikipedia as an external resource for ex-
tracting additional usages. Our system is completely
unsupervised and requires no annotated resources,
and appears to perform very well on the task.
</bodyText>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99790712195122">
Eneko Agirre and Philip Edmonds. 2006. Word
Sense Disambiguation: Algorithms and Applications.
Springer, Dordrecht, Netherlands.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proc. of the 4th International Work-
shop on Semantic Evaluations, pages 7–12, Prague,
Czech Republic.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591–
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction. In Proc. of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction &amp; disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63–68, Uppsala, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207–223.
Roberto Navigli and Daniele Vannella. 2013. SemEval-
2013 task 11: Evaluating word sense induction &amp; dis-
ambiguation within an end-user application. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with
the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566–1581.
</reference>
<page confidence="0.998325">
221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.885022">
<title confidence="0.9986535">unimelb: Topic Modelling-based Word Sense Induction for Web Snippet Clustering</title>
<author confidence="0.999259">Han Lau</author>
<author confidence="0.999259">Paul Cook</author>
<affiliation confidence="0.9941905">Department of Computing and Information The University of</affiliation>
<email confidence="0.912836">tb@ldwin.net</email>
<abstract confidence="0.9988315625">This paper describes our system for Task 11 of SemEval-2013. In the task, participants are provided with a set of ambiguous search queries and the snippets returned by a search engine, and are asked to associate senses with the snippets. The snippets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters. Our system adopts a preexisting Word Sense Induction (WSI) methodology based on Hierarchical Dirichlet Process (HDP), a non-parametric topic model. Our system is trained over extracts from the full text of English Wikipedia, and is shown to perform well in the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Philip Edmonds</author>
</authors>
<title>Word Sense Disambiguation: Algorithms and Applications.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="1142" citStr="Agirre and Edmonds, 2006" startWordPosition="177" endWordPosition="181">pets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters. Our system adopts a preexisting Word Sense Induction (WSI) methodology based on Hierarchical Dirichlet Process (HDP), a non-parametric topic model. Our system is trained over extracts from the full text of English Wikipedia, and is shown to perform well in the shared task. 1 Introduction The basic premise behind research on word sense disambiguation (WSD) is that there exists a static, discrete set of word senses that can be used to label distinct usages of a given word (Agirre and Edmonds, 2006; Navigli, 2009). There are various pitfalls underlying this premise, including: (1) what sense inventory is appropriate for a particular task (given that sense inventories can vary considerably in their granularity and partitioning of word usages)? (2) given that word senses tend to take the form of prototypes, is discrete labelling a felicitous representation of word usages, especially for non-standard word usages? (3) how should novel word usages be captured under this model? and (4) given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it rea</context>
</contexts>
<marker>Agirre, Edmonds, 2006</marker>
<rawString>Eneko Agirre and Philip Edmonds. 2006. Word Sense Disambiguation: Algorithms and Applications. Springer, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>SemEval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proc. of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2241" citStr="Agirre and Soroa, 2007" startWordPosition="358" endWordPosition="361">model? and (4) given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction &amp; Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of </context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proc. of the 4th International Workshop on Semantic Evaluations, pages 7–12, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2982" citStr="Blei et al., 2003" startWordPosition="488" endWordPosition="491">“Evaluating Word Sense Induction &amp; Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detection. The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and the association of topics with documents is represented by a multinomial distribution of topics, with one distribution per document. The generative process of LDA 217 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 217–221, Atlanta, Georgia, June 14-15, 2013. c�2013 Association</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proc. of the 13th Conference of the EACL (EACL 2012),</booktitle>
<pages>591--601</pages>
<location>Avignon, France.</location>
<contexts>
<context position="2597" citStr="Lau et al. (2012)" startWordPosition="417" endWordPosition="420">iven data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction &amp; Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detection. The core machinery of our system is driven by a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). In LDA, the model learns latent topics for a collection of documents, and associates these latent topics with every document in the collection. A topic is represented by a multinomial distribution of words, and th</context>
<context position="4941" citStr="Lau et al. (2012)" startWordPosition="830" endWordPosition="833">lity of topics in the documents (for details of HDP please refer to the original paper, Teh et al. (2006)). To apply HDP in the context of WSI, the latent topics are interpreted as the word senses, and the documents are usages that contain the target word of interest (or search query in the case of Task 11). That is, given a search query (e.g. Prince of Persia), a “document” in our application is a sentence/snippet containing the target word. In addition to the bag of words surrounding the target word, we also include positional context word information, as used in the original methodology of Lau et al. (2012). That is, we introduce an additional word feature for each of the three words to the left and right of the target word. An example of the topic model features for a context sentence is given in Table 1. 2.1 Background Corpus and Preprocessing As part of the task setup, we were provided with snippets for each search query, constituting the documents for the topic model for that query (each search query is topic-modelled separately). Our system uses only the text of the snippets as features, and ignores the URL information. The text of the snippets is tokenised and lemmatised using OpenNLP and </context>
<context position="8867" citStr="Lau et al. (2012)" startWordPosition="1440" endWordPosition="1443">02 0.3394 3.6400 32.3434 TASK11-UKP-WSI-WP-LLR2 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702 TASK11-UKP-WSI-WP-PMI 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098 RAKESH 0.3949 0.0811 0.5876 0.3052 9.0700 2.9441 SINGLETON 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000 ALLINONE 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000 GOLD 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630 Table 2: Cluster quality results for all systems. The best result for each column is presented in boldface. SINGLETON and ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task. 3 Experiments and Results Following Lau et al. (2012), we use the default parameters (-y = 0.1 and α0 = 1.0) for HDP.3 For each search query, we apply HDP to induce the senses, and a distribution of senses is produced for each “document” in the model. As the snippets in the test dataset correspond to the documents in the model and evaluation is based on “hard” clusters of snippets, we assign a sense to each snippet based on the sense (= topic) which has the highest probability for that snippet. The task requires participants to produce a ranked list of snippets for each induced sense, based on the relative fit between the snippet and the sense. </context>
<context position="11678" citStr="Lau et al. (2012)" startWordPosition="1898" endWordPosition="1901"> number of induced clusters and the average cluster size of our systems are similar to those of the gold standard system (GOLD), indicating that our systems are learning an appropriate sense granularity. In terms of diversification of search results, our systems perform markedly better than most teams, other than RAKESH which trails closely behind our systems (despite a relatively low ranking in terms of the cluster quality evaluation). Overall, the results are encouraging and our system performs very well over the task. 4 Discussion and Conclusion Our system adopts the WSI system proposed in Lau et al. (2012) with no parameters tuned for this task, and performs very well over it. Parameter tuning and exploiting URL information in the snippets could potentially boost the system performance further. Other background corpora (such as news articles) could also be used to increase the size of the training data. We leave these ideas for future work. Inspecting the difference between the HDPCLUSTERS-LEMMA and HDP-CLUSTERSNOLEMMA approaches, only 6 out of the 100 lemmas have a lemmatised form which differs from the original query composition: pods (pod), ten commandments (ten commandment), guild wars (gui</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction for novel sense detection. In Proc. of the 13th Conference of the EACL (EACL 2012), pages 591– 601, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>to appear. unimelb: Topic modelling-based word sense induction.</title>
<date>2013</date>
<booktitle>In Proc. of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>Lau, Cook, Baldwin, 2013</marker>
<rawString>Jey Han Lau, Paul Cook, and Timothy Baldwin. to appear. unimelb: Topic modelling-based word sense induction. In Proc. of the 7th International Workshop on Semantic Evaluation (SemEval 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer Pradhan</author>
</authors>
<title>SemEval-2010 Task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<location>Uppsala,</location>
<contexts>
<context position="2266" citStr="Manandhar et al., 2010" startWordPosition="362" endWordPosition="365"> rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory? Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation for a given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source. While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (Agirre and Soroa, 2007; Manandhar et al., 2010), leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled “Evaluating Word Sense Induction &amp; Disambiguation within an End-User Application”. This paper presents the UNIMELB system entry to SemEval-2013 Task 11. Our method is based heavily on the WSI methodology proposed by Lau et al. (2012) for novel word sense detection. Largely the same methodology was also applied to SemEval2013 Task 13 on WSI (Lau et al., to appear). 2 System Description Our system is based on the WSI methodology proposed by Lau et al. (2012) for the task of novel word sense detectio</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach, and Sameer Pradhan. 2010. SemEval-2010 Task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="5569" citStr="Minnen et al., 2001" startWordPosition="941" endWordPosition="944">s, we introduce an additional word feature for each of the three words to the left and right of the target word. An example of the topic model features for a context sentence is given in Table 1. 2.1 Background Corpus and Preprocessing As part of the task setup, we were provided with snippets for each search query, constituting the documents for the topic model for that query (each search query is topic-modelled separately). Our system uses only the text of the snippets as features, and ignores the URL information. The text of the snippets is tokenised and lemmatised using OpenNLP and Morpha (Minnen et al., 2001). As there are only 64 snippets for each query in the test dataset, which is very small by topic modelling standards, we turn to English Wikipedia to expand the data, by extracting all context sentences that contain the search query in the full collection of Wikipedia articles.1 Each extracted usage is a three-sentence context containing the search query: the original sentence that contains the actual usage and its preceding and succeeding sentences. The extraction of usages from Wikipedia significantly increases the amount of information for the topic model to learn the senses for the search </context>
<context position="6826" citStr="Minnen et al., 2001" startWordPosition="1145" endWordPosition="1148">y ambiguous queries such as queen we extracted almost 150,000 usages from Wikipedia; for most queries, however, this number tends to be a few thousand usages. To summarise, for each search query we apply the HDP model to the combined collection of the 64 snippets and the extracted usages from Wikipedia. The topic model learns the senses/topics for all documents in the collection, but we only use the sense/topic distribution for the 64 snippets as they are the documents that are evaluated in the shared task. Our English Wikipedia collection is tokenised and lemmatised using OpenNLP and Morpha (Minnen et al., 2001). The search queries provided in the task, however, are not lemmatised. Two approaches are used to extract the usages of search queries from Wikipedia: HDP-CLUSTERS-LEMMA Search queries are lemmatised using Morpha (Minnen et al., 2001), and both the original and lemmatised forms are used for extraction;2 HDP-CLUSTERS-NOLEMMA Search queries are not lemmatised and only their original forms are used for extraction. 1The Wikipedia dump was retrieved on November 28th 2009. 2Morpha requires the part-of-speech (POS) of a given word, which is determined by the majority POS aggregated over all of that </context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>SemEval2013 task 11: Evaluating word sense induction &amp; disambiguation within an end-user application.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013),</booktitle>
<location>Atlanta, USA.</location>
<contexts>
<context position="10091" citStr="Navigli and Vannella (2013)" startWordPosition="1644" endWordPosition="1647">e. We induce the ranking based on the sense probabilities assigned to the senses, such that snippets that have the highest probability of the induced sense are ranked highest, and snippets with lower sense probabilities 3Our implementation can be accessed via https:// github.com/jhlau/hdp-wsi. are ranked lower. Two classes of evaluation are used in the shared task: 1. cluster quality measures: Jaccard Index (JI), RandIndex (RI), Adjusted RandIndex (ARI) and F1; 2. diversification of search results: Subtopic Recall@K and Subtopic Precision@r. Details of the evaluation measures are described in Navigli and Vannella (2013). The idea behind the second form of evaluation (i.e. diversification of search results) is that search engine results should cluster the results based on senses (of the query term in the documents) given an ambiguous query. For example, if a user searches for apple, the search engine may return results related to both the computer brand sense and the fruit sense of apple. Given this assumption, the best WSI/WSD system is the one that can correctly identify the diversity of senses in the snippets. 219 Figure 1: Subtopic Recall@K for all participating systems. Cluster quality, subtopic recall@K</context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Roberto Navigli and Daniele Vannella. 2013. SemEval2013 task 11: Evaluating word sense induction &amp; disambiguation within an end-user application. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013), Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1158" citStr="Navigli, 2009" startWordPosition="182" endWordPosition="183">ing the sense assignments and systems are evaluated based on the quality of the snippet clusters. Our system adopts a preexisting Word Sense Induction (WSI) methodology based on Hierarchical Dirichlet Process (HDP), a non-parametric topic model. Our system is trained over extracts from the full text of English Wikipedia, and is shown to perform well in the shared task. 1 Introduction The basic premise behind research on word sense disambiguation (WSD) is that there exists a static, discrete set of word senses that can be used to label distinct usages of a given word (Agirre and Edmonds, 2006; Navigli, 2009). There are various pitfalls underlying this premise, including: (1) what sense inventory is appropriate for a particular task (given that sense inventories can vary considerably in their granularity and partitioning of word usages)? (2) given that word senses tend to take the form of prototypes, is discrete labelling a felicitous representation of word usages, especially for non-standard word usages? (3) how should novel word usages be captured under this model? and (4) given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assum</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="4212" citStr="Teh et al. (2006)" startWordPosition="702" endWordPosition="705">ional Linguistics for drawing word w in document d is as follows: 1. draw latent topic z from document d; 2. draw word w from the chosen latent topic z. The probability of selecting word w given a document d is thus given by: T P(wld) = E P(wlt = z)P(t = z1d). z=1 where t is the topic variable, and T is the number of topics. The number of topics, T, is a parameter in LDA, and the model tends to be highly sensitive to this setting. To remove the need for parameter tuning over development data, we make use of a non-parametric variant of LDA, in the form of a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)). HDP learns the number of topics based on data, and the concentration parameters -y and α0 control the variability of topics in the documents (for details of HDP please refer to the original paper, Teh et al. (2006)). To apply HDP in the context of WSI, the latent topics are interpreted as the word senses, and the documents are usages that contain the target word of interest (or search query in the case of Task 11). That is, given a search query (e.g. Prince of Persia), a “document” in our application is a sentence/snippet containing the target word. In addition to the bag of words surroundi</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>