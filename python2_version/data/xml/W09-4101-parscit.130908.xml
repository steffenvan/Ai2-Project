<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.9968265">
Exploiting the Russian National Corpus in the Development of a
Russian Resource Grammar
</title>
<author confidence="0.806422">
Tania Avgustinova
</author>
<affiliation confidence="0.468213">
DFKI GmbH &amp; Saarland University
</affiliation>
<address confidence="0.525857">
P.O. Box 151150
66041 Saarbrücken, Germany
</address>
<email confidence="0.987189">
avgustinova@dfki.de
</email>
<sectionHeader confidence="0.997594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992513789473684">
In this paper we present the on-going grammar
engineering project in our group for developing in parallel
resource precision grammars for Slavic languages. The
project utilizes DELPH-IN software (LKB/[incr tsdb()]) as
the grammar development platform, and has strong affinity
to the LinGO Grammar Matrix project. It is innovative in
that we focus on a closed set of related but extremely diverse
languages. The goal is to encode mutually interoperable
analyses of a wide variety of linguistic phenomena, taking
into account eminent typological commonalities and
systematic differences. As one major objective of the
project, we aim to develop a core Slavic grammar whose
components can be commonly shared among the set of
languages, and facilitate new grammar development. As a
showcase, we discuss a small HPSG grammar for Russian.
The interesting bit of this grammar is that the development
is assisted by interfacing with existing corpora and
processing tools for the language, which saves significant
amount of engineering effort.
</bodyText>
<sectionHeader confidence="0.996239" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.826979">
Parallel grammar engineering, corpora, Slavic languages
</keyword>
<sectionHeader confidence="0.995838" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999917357142857">
Our long-term goal is to develop grammatical resources
for Slavic languages and to make them freely available for
the purposes of research, teaching and natural language
applications. As one major objective of the project, we
aim to develop and implement a core Slavic grammar
whose components can be commonly shared among the
set of languages, and facilitate new grammar
development. A decision on the proper set up along with a
commitment to a reliable infrastructure right from the
beginning are essential for such an endeavor because the
implementation of linguistically-informed grammars for
natural languages draws on a combination of engineering
skills, sound grammatical theory, and software
development tools.
</bodyText>
<subsectionHeader confidence="0.77199">
1.1 DELPH-IN initiative
</subsectionHeader>
<bodyText confidence="0.999443333333333">
Current international collaborative efforts on deep
linguistic processing with Head-driven Phrase Structure
Grammar [1-3] exploit the notion of shared grammar for
</bodyText>
<note confidence="0.96249875">
Yi Zhang
DFKI GmbH &amp; Saarland University
P.O. Box 151150
66041 Saarbrücken, Germany
</note>
<email confidence="0.937837">
yzhang@coli.uni-sb.de
</email>
<bodyText confidence="0.999978322580645">
the rapid development of grammars for new languages and
for the systematic adaptation of grammars to variants of
the same language. This international partnership, which
became popular under the name DELPH-IN1, is based on a
shared commitment to re-usable, multi-purpose
resources and active exchange. Its leading idea is to
combine linguistic and statistical processing methods for
getting at the meaning of texts and utterances. Based on
contributions from several member institutions and joint
development over many years, an open-source repository of
software and linguistic resources has been created that
already enjoys wide usage in education, research, and
application building.
In accord with the DELPH-IN community we view rule-
based precision grammars as linguistically-motivated
resources designed to model human languages as
accurately as possible. Unlike statistical grammars, these
systems are hand-built by grammar engineers, taking into
account the engineer&apos;s theory and analysis for how to best
represent various syntactic and semantic phenomena in the
language of interest. A side effect of this, however, is that
such grammars tend to be substantially different from each
other, with no best practices or common representations.2
As implementations evolved for several languages within
the same common formalism, it became clear that
homogeneity among existing grammars could be increased
and development cost for new grammars greatly reduced
by compiling an inventory of cross-linguistically valid (or
at least useful) types and constructions. To speed up and
simplify the grammar development as well as provide a
common framework, making the resulting grammars more
</bodyText>
<footnote confidence="0.452998">
1 Deep Linguistic Processing with HPSG Initiative (DELPH-
IN), URL: http://www.delph-in.net/
</footnote>
<bodyText confidence="0.950802571428571">
2 Exceptions do exist, of course: ParGram (Parallel Grammar)
project is one example of multiple grammars developed using
a common standard. It aims at producing wide coverage
grammars for a wide variety of languages. These are written
collaboratively within the linguistic framework of Lexical
Functional Grammar (LFG) and with a commonly-agreed-
upon set of grammatical features.
</bodyText>
<footnote confidence="0.531817">
URL: http://www2.parc.com/isl/groups/nltt/pargram/
</footnote>
<page confidence="0.9071">
1
</page>
<bodyText confidence="0.979903333333333">
Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11
comparable the LinGO3 Grammar Matrix4 has been set up
as a multi-lingual grammar engineering project [4] which
provides a web-based tool designed to support the
creation of linguistically-motivated grammatical resources
in the framework of HPSG [5].
The Grammar Matrix is written in the TDL (type
description language) formalism, which is interpreted by
the LKB5 grammar development environment [6]. It is
compatible with the broader range of DELPH-IN tools,
e.g., for machine translation [7], treebanking [8] and parse
selection [9].
</bodyText>
<subsectionHeader confidence="0.988768">
1.2 LinGO Grammar Matrix
</subsectionHeader>
<bodyText confidence="0.999975857142857">
Generally speaking, the Grammar Matrix is an attempt to
distill the wisdom of already existing broad coverage
grammars and document it in a form that can be used as
the basis for new grammars. The main goals are to
develop in detail semantic representations and the syntax-
semantics interface, consistent with other work in HPSG;
to represent generalizations across linguistic objects and
across languages; and to allow for very quick start-up as
the Matrix is applied to new languages.
The fact that different parts of a single grammar can be
abstracted into separate, independent modules, either for
processing or grammar development, is approached in
[10] from the perspective of reuse of grammar code. A
web-based configuration system elicits typological
information from the user-linguist through a questionnaire
[10, 11] and then outputs a grammar consisting of the Ma-
trix core plus selected types, rules and constraints from the
libraries according to the specifications in the
questionnaire, and lexical entries for the language in
question. In other words, users specify phenomena
relevant to their particular language, with their selections
being compiled from libraries of available analyses into a
starter grammar which can be immediately loaded into the
LKB grammar development environment [6], as well as
the PET parser [12], in order to parse sentences using the
rules and constraints defined therein. The regression
testing facilities of [incr tsdb()] allow for rapid
experimentation with alternative analyses as new
</bodyText>
<footnote confidence="0.691370333333333">
3 The Linguistic Grammars Online (LinGO) team is committed
to the development of linguistically precise grammars based
on the HPSG framework, and general-purpose tools for use in
grammar engineering, profiling, parsing and generation. URL:
http://lingo.stanford.edu/
4 URL: http://www.delph-in.net/matrix/
</footnote>
<bodyText confidence="0.976405642857143">
5 LKB (Linguistic Knowledge Builder) system is a grammar and
lexicon development environment for use with unification-
based linguistic formalisms. While not restricted to HPSG, the
LKB implements the DELPH-IN reference formalism of typed
feature structures (jointly with other DELPH-IN software
using the same formalism).
phenomena are brought into the grammar [13]. The
original Grammar Matrix consisted of types defining the
basic feature geometry, e.g. [14], types for lexical and
syntactic rules encoding the ways that heads combine with
arguments and adjuncts, and configuration files for the
LKB grammar development environment [6] and the PET
system [12]. Subsequent releases have refined the original
types and developed a lexical hierarchy, including linking
types for relating syntactic to semantic arguments, and the
constraints required to compositionally build up semantic
representations in the format of Minimal Recursion
Semantics [15-17]. The constraints in this ‘core’ Matrix
are intended to be language-independent and
monotonically extensible in any given grammar. In its
recent development, the Grammar Matrix project aims at
employing typologically motivated, customizable
extensions to a language-independent core grammar.
The implemented prototype consists of a small set of
modules targeting basic word order (addressing the
relative order of subjects, verbs, and verbal complements),
sentential negation, main-clause yes-no questions, and a
small range of lexical entries. In particular:
</bodyText>
<listItem confidence="0.973092">
• The Matrix core grammar provides definitions of
basic head-complement and head-subject schemata
which are consistent with the implementation of
compositional semantics [16], as well as definitions
of head-initial and head-final phrase types. The word
order module creates subtypes joining the head-
complement and head-subject schemata with the
types specifying head/dependent order, creates
instances of those types as required by the LKB
parser, and constrains the rules to eliminate spurious
ambiguity in the case of free word order.
• For yes-no questions, four alternatives have been
implemented: inversion of the subject and a main or
auxiliary verb relative to declarative word order and
sentence initial or final question particles.
• The sentential negation module handles two general
negation strategies: via verbal inflection or via a
negative adverb. Neither, either or both of these
strategies may be selected.
• In a strongly lexicalist theory like HPSG, words tend
to carry quite a bit of information, which is reflected
in the lexicon structure. This information is encoded
in lexical types; lexical entries merely specify the
type they instantiate, their orthographic form, and
their semantic predicate. Many of the constraints
required (e.g., for the linking of syntactic to semantic
arguments) are already provided by the core Matrix.
</listItem>
<bodyText confidence="0.89178125">
However, there also is cross-linguistic variation. The
forms are assumed to be fully inflected (modulo
negation), support morphological processes awaiting
future work. This information and the knowledge
</bodyText>
<page confidence="0.988506">
2
</page>
<bodyText confidence="0.999850454545455">
base are used to produce a set of lexical types
inheriting from the types defined in the core Matrix
and specifying appropriate language-specific
constraints, and a set of lexical entries.
In a lexicalist constraint-based framework, the grammars
are expressed as a collection of typed feature structures
which are arranged into a hierarchy such that information
shared across multiple lexical entries or construction types
is represented only on a single supertype. As a result, a
cross-linguistic type hierarchy comes with a collection of
phenomenon-specific libraries.
</bodyText>
<sectionHeader confidence="0.701365" genericHeader="method">
2. Typologically motivated modularity
</sectionHeader>
<bodyText confidence="0.999923032608696">
Aiming at typologically motivated modularity [10]
describe a method for extending a language-independent
core grammar with modules handling cross-linguistically
variable but still recurring patterns. This method allows
for extremely rapid prototyping of HPSG-conform
grammars in such a way that the prototypes themselves
can serve as the basis for sustained development, being
able to scale up to broad-coverage resource grammars.
The authors envision four potential uses for such grammar
prototyping: (i) in pedagogical contexts, where it would
allow grammar engineering students to more quickly work
on cutting-edge problems; (ii) in language documentation,
where a documentary linguist in the field might be
collaborating remotely with a grammar engineer to
propose and test hypotheses; (iii) in leveraging the results
from economically powerful languages to reduce the cost
of creating resources for minority languages; and (iv) in
supporting typological or comparative studies of linguistic
phenomena or interactions between phenomena across
languages.
The modular approach of [10] has been designed to
handle two kinds of typological variation. On the one
hand, there are systems (formal or functional) which must
be represented in every language. For example, every
language has some set of permissible word orders (formal)
and a means of expressing sentential negation
(functional). On the other hand, there are linguistic
phenomena which appear in only some languages, and are
not typically conceptualized as alternative realizations of
some universal function, phenomena such as noun
incorporation, numeral classifiers, and auxiliary verbs. It
is indeed expected that the constraint definitions which are
supplied to grammar developers can be extended to
capture generalizations holding only for subsets of
languages.
The strategy of [10] is actually consistent with data driven
investigation of linguistic universals and constraints on
cross-linguistic variation. Therefore, we refer to this
approach of grammar development (with the help from
Grammar Matrix) as the &amp;quot;bottom-up&amp;quot; approach, because it
is driven purely by the specific phenomena in the target
language. It is certainly efficient: Specifying the choice
file and building a small working grammar can be done
within an hour (excluding the time spent on deciding
specific choices for the given language). For instance [18]
reports a relatively short development time required to
create a precision, hand-built grammar for the Australian
language Wambaya as a qualitative evaluation of the
Grammar Matrix as a cross-linguistic resource.&apos; The
major drawback of this approach, however, is that, for the
set of customized grammars for a group of languages, it
soon becomes difficult (if not impossible) to harmonize
the treatment of related phenomena across languages.
With grammars being created individually, the treatment
of shared phenomena would work to the degree that
satisfies but does not guarantee cross-linguistic
compatibility. As the number and breadth of implemented
grammars grows, linguistic predictions are expected to
emerge and become part of improved modules,
particularly with respect to interactions among the distinct
phenomena covered. Our focus in creating a Slavic
Grammar Matrix is therefore somewhat different: since we
are dealing with representatives of a language family, this
effectively enables a “top-down” perspective in the
multilingual grammar design.
It is an appealing goal indeed to develop a theoretical
account of the way that language variation may be
described in HPSG. Crucial in this respect is the fact that
the HPSG framework allows a clean way of encoding at
least some aspects of language variability within the type
system. Another more ambitious line of research is
initiated for investigating whether the description of
differences among any set of two or more languages can
be reduced to a minimal set of types. Progress in this
research will lead to shared portions of grammar since the
similarity of phenomena among the different languages
will then be reflected in identical HPSG descriptions
within the type systems. The goal is a grammar matrix
designed for maximum reusability, lifting out the elements
that can and should be common across HPSG grammars.
Therefore, it is important to determine which analyses or
building blocks of analyses appear to be cross-
linguistically applicable. As the grammar matrix won’t be
a complete grammar fragment by itself, it will be used in
combination with mini-grammars for various languages.
For a language family, and closely related languages in
general, it is certainly justified to introduce intermediate
&apos; Despite large typological differences between Wambaya and
the languages on which the development of the resource was
based, the Grammar Matrix is found to provide a significant
jump-start as the creation of grammar itself is reported to have
taken less than 5.5 person-weeks of effort.
</bodyText>
<page confidence="0.994955">
3
</page>
<bodyText confidence="0.9982715">
parameterizations of the cross-linguistic core of the
grammar matrix.
</bodyText>
<subsectionHeader confidence="0.9934">
2.1 SlaviGraM: Slavic Grammar Matrix
</subsectionHeader>
<bodyText confidence="0.99674425">
The common properties of Slavic languages have been
observed both in literature and related research at various
intermediate levels of linguistic abstraction. Intermediate
levels of typological variation are essential to our project
because we work with a closed set of well-studied, well-
documented and generally resource-rich languages
belonging to the same language family. In this context, the
interesting question arises whether minimal differences
are also detectable as parameters of systematic variation.
Our concept of Slavic core grammar (Figure 1) will shape
up and crystallize through rigorous testing in parallel
grammar engineering for a closed set of richly
documented and well studied genetically related
languages for which a variety of linguistic resources is
already available. We use Grammar Matrix to quickly
build small grammars for individual languages, utilizing
the online Matrix configuration system&apos; to specify choice
files for representatives of each Slavic subgroup, namely
for Russian (East Slavic), Bulgarian (South Slavic) and
Polish (West Slavic), as an initial step.
Apart from the shared core in the Grammar Matrix,
however, the customization script treats the individual
languages as separate instances, which means that the fact
that we have to do with a group of closely related
languages cannot be taken into account in the original
setting. Therefore, shared analyses from individual
languages are put into the Slavic Core in the form of
generalized Slavic hierarchy and libraries. When new
language is added, the Slavic core helps to more
efficiently build the new grammar, and potentially
receives cross-Slavic validation.
&apos; The system consists of the following three parts:
</bodyText>
<listItem confidence="0.9331065">
• Customization Page. In order for the system to create a
starter grammar, the required information must be elicited
from the user-linguist. The medium for this elicitation
is a web interface.
• Choices File. The options selected by the user are saved
in a plain text file, called the choices file. Before a grammar
is built, the choices file is verified to be internally
consistent and contain all the information it needs.
• Customization Script. Matrix grammars are written in a
type description language (TDL). The customization script
is a Python script that reads in the choices file, and uses
the information it contains to select or construct relevant
sections of TDL code. The output is a collection of files
containing the language-specific TDL code. This is then
bundled with the core Matrix files to provide a small but
functioning grammar fragment.
</listItem>
<bodyText confidence="0.999896083333333">
Our approach to Slavic grammatical resources is unique in
the sense that grammar engineering for each individual
language takes place in a common Slavic setting. This in
particular means that if for example two possibilities are
conceivable of how to model a particular phenomenon
observed in a certain Slavic language, then we strongly
prefer the option that would potentially be consistent with
what is found in the other grammars. As a result the
Matrix-driven starter grammars for Russian and Bulgarian,
the two typological extremes within the Slavic language
family, eventually incorporate novel theoretical decisions
even for seemingly trivial tasks.
</bodyText>
<figureCaption confidence="0.9909015">
Figure 1: Matrix-driven starter grammars in Slavic core
grammar setting
</figureCaption>
<bodyText confidence="0.99952425">
The Grammar Matrix in combination with the Slavic
Core Grammar allows new grammars to directly leverage
the expertise in grammar engineering gained in extensive
work on previous grammars of the same language family.
Both the general LinGO grammar Matrix and the Slavic
Core Grammar are not static objects, but are designed to
evolve and be refined as more languages are covered. The
advantage of separating the Slavic Core Grammar from
the general grammar Matrix is that the closed set of
languages under consideration allow our Slavic Core to
evolve more liberally than the grammar Matrix, without
concerns over unstudied languages.
</bodyText>
<sectionHeader confidence="0.808521" genericHeader="method">
3. Focus on Russian Resource Grammar
</sectionHeader>
<bodyText confidence="0.999976555555556">
As a showcase, let us consider the he Russian HPSG
grammar, which is currently under active construction in
our group. In fact, the Russian Resource Grammar has a
central position in the SlaviGraM project and is
anticipated as a major outcome in terms of end product
and a large-scale experimental set up for hypothesis
testing. The interesting aspect of the initial Russian
grammar is that its development is assisted by interfacing
with existing corpora and processing tools for the
</bodyText>
<page confidence="0.987936">
4
</page>
<bodyText confidence="0.9976145">
language, which saves significant amount of engineering
effort.
</bodyText>
<subsectionHeader confidence="0.998435">
3.1 Morphological pre-processing
</subsectionHeader>
<bodyText confidence="0.99995375">
The morphological information associated with word
forms in the disambiguated part of the Russian National
Corpus, i.e. where the full analysis is displayed, is
structured into four fields:
</bodyText>
<listItem confidence="0.792317333333333">
(i) lexeme and its part of speech;
(ii) (word-classifying invariable features (for
example, gender for nouns and transitivity for
verbs);
(iii) word-form specific inflectional features (for
example, case for nouns and number for verbs);
</listItem>
<bodyText confidence="0.947807684210527">
(iv) non-standard forms, orthographic variations, etc.
In the rest of the corpus only the lexeme and the
part of speech are displayed.
Morphological analysis is the basic enabling technology
for many kinds of text processing. Integrating a
morphological analyzer is a crucial prerequisite for all
grammar development activities involving Slavic
languages. For research purposes, such systems are by and
large freely available nowadays, and the LKB grammar
engineering environment provides the required interface
for integrating a morphological pre-processor.
For the pre-processing module we have considered two
morphological analyzers for Russian: Mystem [19] and
Dialing [20]. Both systems of are based on finite state
transducers and have been used in the Russian National
Corpus. Unlike Mystem, the system Dialing covers both
inflectional and derivational morphology and is based on
a large dictionary which also contains information on
inflections, prefixes and affixes and stress patterns.
</bodyText>
<figureCaption confidence="0.990627">
Figure 2: Morphological input via inflectional rules
</figureCaption>
<table confidence="0.355273571428571">
npocpeccop(npoc�eccop=S,nnyxc,OA=wm,eA)
4NTaeT(4NTOTb=V,HeCOB=HenpoW,eA,H3bAB,3-A)
HN!y{ HN!a=S,JKeH,HeOA=BNH,eA)
S&amp;quot;wm #� $%&amp;$-$%&apos;-()&amp;*+,
S&amp;quot;BHH #� $%&amp;$--..-()&amp;*+,
S&amp;quot;eA #~ $%&amp;$-/0-()&amp;*+,
V&amp;quot;Henpow #� 1+)2-$%$3-/4-()&amp;*+,
</table>
<bodyText confidence="0.999712148148148">
Mystem, however, is the morphological component used
by the popular Russian search engine Yandex. The
underlying algorithm for analysis and synthesis achieves
quite precise inflectional morphology of a wide lexical
coverage without implying any particular morphological
model of the dictionary.
The fact that Mystem is available for Polish too is an
additional criterion in favor of adopting it in our project.
Thus, during the preparatory phase, we have chosen the
system Mystem, and it is already integrated as a
morphological pre-processor in the LKB environment, as
illustrated in Figure 2.
The Russian National Corpus is without a doubt an
important source of structured grammatical knowledge to
be integrated in our Russian grammar. A snapshot of the
main search interface to the RNC is given in Figure 3,
while Figure 6 illustrates the access to the syntactically
annotated and disambiguated sub-corpus of the RNC.
Furthermore, Figure 4 gives us the inventory of
morphologically relevant “grammatical features” to select
from in the main corpus. Note, however, that the inventory
of grammatical features accessed from the syntactic search
page is somewhat different, as shown by Figure 5.
Unlike the morphologically annotated portion of the RNC,
the deeply annotated sub-corpus only contains fully
disambiguated annotations (i.e. both morphological and
syntactic ambiguity is resolved).
</bodyText>
<subsectionHeader confidence="0.999442">
3.2 Syntactic dependencies
</subsectionHeader>
<bodyText confidence="0.9991916">
In the deeply annotated sub-corpus of the RNC, every
sentence is marked up with a dependency syntactic
structure – cf. Figure 7, with nodes corresponding to the
words of the sentence, and labeled edges encoding the
syntactic relations.
</bodyText>
<figure confidence="0.978992764705882">
678&apos;* 1+)/(%$=9:,;9 +$.%5($0J9&amp;4&lt;-=97&gt;
6/+0&apos;+$4&gt;
64%?+$ &lt;%)&apos;J9npoc�eccop9 &lt;)%&apos;J9;9 4%=9@9&gt;
6-$-*A/(/ /4+&apos;=9npoc�eccop9&gt;
6)&amp;*+ (5=9$%&amp;$-$%&apos;-()&amp;*+9 &lt;%)&apos;J9npoc�eccop9B&gt;
6)&amp;*+ (5=9$%&amp;$-/0-()&amp;*+9 &lt;%)&apos;=9npoc�eccoo9B&gt;
6B-$-*A/(/&gt;
6B4%?+$&gt;
64%?+$ &lt;%)&apos;J94HTaeT9 &lt;)%&apos;=9:;9 4%=9:C9&gt;
6-$-*A/(/ /4+&apos;=94NTarb9&gt;
6)&amp;*+ (5=91+)2-$%$3-/4-()&amp;*+9 &lt;%)&apos;=9�NTaeT9B&gt;
6)&amp;*+ (5=91+)2-($5(.-()&amp;*+9 &lt;%)&apos;J94HTaeT9B&gt;
6)&amp;*+ (5=91+)2-334)&amp;*+9 &lt;%)&apos;J94HTaeT9B&gt;
6B-$-*A/(/&gt;
6B4%?+$&gt;
64%?+$ &lt;%)&apos;J9 HH!y9 &lt;)%&apos;=9:D9 4%J9EE9&gt;
5
</figure>
<figureCaption confidence="0.962932">
Figure 3: RNC search
</figureCaption>
<figure confidence="0.996506214285714">
Main corpus L Syntactic corp.. L Spoken corpus
customize subcorpus pyccxasa oepcvR
Search by exact form • 6 B
Word or phrase
search clearl
Lexico-grammatical search ?
Word &amp;quot;. ramie features , select Semantic features , select
Adak. feattres 7 5111,0 &apos;17 sem &apos;17 sem2 7 semf E semf2
Distance: from it I to 11
Word Gramm. features fjqg Semantic features , select
Add it features select I FC&apos; sem [7 sem2 7 semi I- semf2
search I clear I
Russian National Corpus Search provided by Rnctex.Server
(.2., 2003-2008
</figure>
<figureCaption confidence="0.999364">
Figure 4: Morphological information in RNC
</figureCaption>
<table confidence="0.996247829268293">
Part of speech Case Mood / Verb form Degree / Adj. form
F noun IF nominative 7 indicative 7 comparative
7 adjective 7 vocative&apos; 7 imperative 7 comparative 2*
F numeral 7 genitive 7 imperative 2 7 superlative
F numeral adjective E genitive 2 I— infinitive 7 full fonn
7 verb 7 dative 7 participle F short form
E adverb 7 accusative F genuid
F predicative I— accusative &apos;?*
7 parenthesis F instrumental
F pronoun 7 locative
7 adjective pronoun 7 locative 2
E predicative pronoun 7 adnumerative
F adverbial pronoun
7 preposition
7 conjunction
F particle
I- interjection
Tense Transitivity
F present F transitive*
7 future F intransitive*
7 past
Number Person Other features
F singular 7 first 7 dictionary form
7 plural 7 second 7 numeral recording
7 third 7 anomalous form*
7 distorted form*
r non-dictionary fonn**
F initials*
7 abbreviation*
7 indeclinable*
Antroponymic Gender Voice
F family name E masculine F active
F first name 7 feminine 7 passive
7 patronymic 7 neuter 7 middle
F common*
Animacy Aspect
7 animate 7 perfective
7 inanimate 7 imperfective
C,I.0 I ■• Itcel I
. - only in the cogne with tesolved homonymy
** - only to the wants with unresolved homonymy
</table>
<figureCaption confidence="0.930806">
Figure 5: Grammatical features used in the syntactic sub-corpus
</figureCaption>
<figure confidence="0.999511459016394">
Part of speech
7 nominal
7 adjective
7 numeral
7 verb
7 adverb
7 preposition
7 conjunction
7 particle
7 interjection
7 compound word
word-sentence
7 foreign word,
non-lexical formula
Case
nominative
I- genitive
partitive
I- dative
I- accusative
I- instrumental
F prepositive
I- locative
I- vocative
Grade
✓ comparative
I- comparative 2
F superlative
Aspect
F perfective
I- imperfective
Tense
r present
I- non-past
▪ past
Person
I- first
I- second
F third
Animacy Form
F animate 7 short form
7 inanimate
Gender
7 masculine
7 feminine
7 neuter
Representation
• finite verb
infinitive
E participle
I- gerund
Voice
E passive
Number
7 singular
7 plural
Mood
indicative
✓ imperative
Other
I- part of a compound word
</figure>
<page confidence="0.663429">
6
</page>
<figureCaption confidence="0.999542">
Figure 6: RNC access to the syntactic sub-corpus
</figureCaption>
<bodyText confidence="0.9997116">
The syntactic formalism originates in the Meaning-Text
Theory [21], but the inventory of syntactic relations has
been extended for the purposes of corpus annotation,
incorporating a number of specific linguistic decisions
[22, 23].
</bodyText>
<figureCaption confidence="0.999688">
Figure 7: A sample structure
</figureCaption>
<bodyText confidence="0.999933378378379">
We, therefore, observe the following straightforward
convention when the components of a RNC dependency
relation (cf. the inventory in
Figure 8) are to be mapped on HPSG categories: in a
given syntactic dependency relation, the “governor X”
corresponds in HPSG to the lexical head of the head
daughter, while the “dependent Y” corresponds to the
lexical head of the non-head daughter.
As actantial surface syntactic relations connect a predicate
word [X] with its syntactic argument [Y], they would by
and large map to headed phrases saturating valence
requirements. For instance, the first syntactic argument
[Y] stands in a predicative, dative-subjective, agentive or
quasi-agentive relation to its head [X]. In HPSG, this
corresponds to the first position on the head’s ARG-ST
(argument structure) list, e.g. to the “a-subject”. Only in
the prototypical predicative relation, however, this also is
the single element on the SUBJ (subject) valence list. A
non-first syntactic argument [Y] stands in a completive
relation to its head [X]. As a rule, the direct object of a
transitive verb stands in the first-completive relation to its
head while non-transitive single-argument verbs like
“sleep”, for instance, take no completive relations
whatsoever. Eventually, there could be several completive
relations, depending on the actual valence requirements of
the head. In HPSG this corresponds to the second, third,
etc. positions on the head’s ARG-ST (argument structure)
list. A second large group of surface syntactic relations
contains attributive dependencies. These relations connect
a word [X] with its dependent word [Y] which functions
as a modifier, i.e. is not subcategorized, and by and large
would map in an HPSG setup to head–adjunct phrases
As for coordinative constructions, these are conceived in
dependency syntax as directed asymmetric relations and in
this respect do not stand out from the rest. In an HPSG
setup, however, this group of relations would correspond
to various types of (non-headed) coordinate phrases. The
</bodyText>
<page confidence="0.998846">
7
</page>
<bodyText confidence="0.999717405405406">
so-called syncategorematic dependencies connect two
tightly bound elements [X] and [Y] that are often
conceived as intrinsic parts of a larger unit, e.g. of a
compound. In an HPSG setup, this group of relations
would only partly correspond to headed phrases with
functional categories, e.g. auxiliary verbs.
In this valuable resource, even more structured
grammatical knowledge is accessible, e.g. with regard to
multi-word expressions (MWE), syntactic ellipsis and
gapping. The RNC website contains structured lists of
orthographically multi-componential lexical units enriched
with frequency information from the disambiguated sub-
corpus. Based on the collocation analysis and
lexicographic resources, two general MWE types are
distinguished.
Inasmuch as the components of a MWE can be neither
changed nor separated, it is considered equivalent to a
single word and represented as a separate node in the
syntactic structure. To this first type belong fixed
expressions functioning as: (i) prepositions, e.g. no
oTaomemno (in relation to); (ii) conjunctions, e.g. xo.tu
cxopo (as soon); (iii) particles, e.g. passe qTo (unless),
,qTo nn ecTb (no matter), He To gTo6bi (not that), aeT-neT
Aa n (once in while); (iv) adverbs, e.g. noxa xrro (as yet),
xax 6W To nn 6vmo (anyway), gyTV ]m He (almost),
cxperia cep;we (reluctantly), ns pyx Bon nnoxo
(thoroughly bad), Mao 6um (thus), To n Reno (time and
again), B o6HHmxy (embracing each other), ncrioxoa
BexoB (since the beginning of time).
On the other hand, there are syntactically transparent
expressions whose components show certain degree of
inflectional variation or allow other words to intervene in
between. For such a MWE no standard syntactic structure
is built, but (some of) its components are combined in an
auxiliary dependency relation. It is assumed to hold (from
X to Y) in the following examples: cam[Y] ce6a[X]
(oneself); H30[X] X4A B[Y] Aeab (from day to day);
</bodyText>
<figureCaption confidence="0.991657">
Figure 8: Syntactic relations in RNC
</figureCaption>
<bodyText confidence="0.999804739130435">
Tax[Y] Ha3riBaeMbH3[X] (so called); Bce[Y] paHHO[X] (all
the same); 3HaTb[Y] He 3xaio[X] (me having no idea
whatsoever); Aypax[Y]-TO ox Aypax[X] (him being
admittedly a fool).
In elliptical constructions the missing words are
reconstructed in the syntactic annotation as “phantom”
units which participate in the respective syntactic
dependencies without introducing any changes in the
original text. Similar approach is adopted in case of
gapping, i.e. in constructions with missing verb of
“vague” semantic content. An additional empty node is
included the dependency structure, with its lemma set to
“non-specific verb” assigning it the most plausible
characteristics and, based on them, an indication of a
lexeme that would represents a “natural hypothesis” for
the missing verb.
Having adopted linguistically informed strategies in the
modular grammar design, we deliberately concentrate on
making the most of the freely available structured
grammatical knowledge in the Russian National Corpus.
Interfacing with existing corpora and processing tools is
therefore fundamental to the Russian Resource Grammar
development.
</bodyText>
<sectionHeader confidence="0.971138" genericHeader="method">
4. Proof-of-concept Implementation
</sectionHeader>
<bodyText confidence="0.999900642857143">
To wrap up, here are some basic figures on the current
state of the Russian grammar: ~1000 lines of code
(excluding Matrix files and lexicon); ~350 newly
introduced types (excluding Matrix types). The invested
grammar engineering effort can be estimated as
approximately 100 person hours of collaborative
grammar development, plus some help from student
assistants. Already at this initial stage, the Russian
grammar covers the basic word order and agreement
phenomena, as well as linking of syntactic to semantic
arguments, case assignment by verbs to dependents, ‘pro-
drop’ and argument optionality (2, 3, 9), passive (5) and
various impersonal constructions (8, 11, 13, 14), among
other mosphosyntactic phenomena.
</bodyText>
<figure confidence="0.965811190476191">
(1) Hpocpeccop HxraeT xHHry
professor[nom.mask.sg] read[pres.act.3sg]
book[acc.fem.sg]
&apos;The professor reads the book.&apos;
(2) Hpocpeccop HxraeT
professor[nom.mask.sg] read[pres.act.3sg]
&apos;The professor reads.&apos;
(3) 1IHTaeT xHFHy
read[pres.act.3sg] book[acc.fem.sg]
&apos;(pro-drop) reads the book.&apos;
(4) CTyAeHT peniaeT 3agaxry
student[nom.mask.sg] solve[pres.act.3sg]
task[acc.fem.sg]
&apos;The student solves the task.&apos;
(5) agaHa pemeHa
task[nom.fem.sg] solve[pcp.pass.sg.fem]
&apos;The task is solved.&apos;
(6) Hpocpeccop ;Ian 3agwry cTyAeHTaM
professor[nom.mask.sg] give[past.sg.masc]
task[acc.fem.sg] student[dat.pl]
&apos;The professor gave the task to the students.&apos;
(7) !onpoc Tpe6yeT oco6oro BHHMaHHH
question[nom.masc.sg] require[pres.3sg]
special[gen.neut.sg] attention[gen.neut.sg]
&apos;The question requires special attention.&apos;
(8) &amp;quot;bicTpo cBeTaeT.
quickly dawn[pres.3sg]
&apos;It&apos;s dawning quickly.&apos;
(9) HHmeM HOByro cTaTbio
write[pres.1pl] new[acc.fem.sg] article[acc.fem.sg]
&apos;We write a new article.&apos;
(10) Hpe3HAeHT CxopO JTHIHHTCa AosepHA
presient[nom.masc.sg] soon be-deprived[non-past.3sg]
trust[gen.neut.sg]
&apos;The president will soon lose credibility.&apos;
(11) HeTpa TomxFrr
Peter[acc.mask.sg] feel-sick[pres.3sg]
&apos;Peter feels sick.&apos;
(12) CTapbiH ripocpeccop ropX4TCA CTyAeHTaMH.
old[nom.mask.sg] professor[nom.mask.sg] be-
proud[pres.3sg] student[ins.pl]
&apos;The old professor is proud of the students.&apos;
</figure>
<listItem confidence="0.9258815">
(13) &amp;quot;LICTpO TeMHeno.
quickly get-dark[past.sg.neut]
&apos;It was getting dark quickly.&apos;
(14) #Tuy He3Aopowrrca.
</listItem>
<bodyText confidence="0.9996474">
father[dat.masc,sg] feel-unwell[pres.3sg]
&apos;Father does not feel well.
The linguistic analyses encoded in the grammar serve to
map the surface strings to semantic representations in
Minimal Recursion Semantics (MRS) format [15]. For
instance, the MRS in Figure 9 is assigned to the example
in (6). It includes the basic propositional structure: a
situation of ‘giving’ in which the first argument, or agent,
is ‘professor’, the second (recipient) is ‘student’, and the
third (patient), is ‘task’. The relations are given English
predicate names for the convenience of the grammar
developer. A simple tree display in Figure 10 offers an
abbreviated view over the HPSG derivation while hiding
the detailed typed feature structures beneath away from
the user.
</bodyText>
<page confidence="0.993786">
9
</page>
<figureCaption confidence="0.9996605">
Figure 9: MRS representation.
Figure 10: Tree representation
</figureCaption>
<sectionHeader confidence="0.98795" genericHeader="method">
5. Outlook
</sectionHeader>
<bodyText confidence="0.999971692307692">
In the elaboration of the individual grammars and
especially for discovering structured linguistic knowledge
to be reflected in the respective modules we shall
systematically exploit the open access to rich linguistically
interpreted corpora available for Slavic languages.
All individual grammars will be designed to support the
innovative implementation of a Slavic core module that
consolidates strategies for constructing a cross-linguistic
resource based on concepts of shared and non-shared
morphosyntactic phenomena.
An important desideratum for the individual resource
grammars is to eventually couple them with treebanks
which either pre-exist or will be constructed in parallel.
</bodyText>
<sectionHeader confidence="0.995835" genericHeader="conclusions">
6. Acknowledgements
</sectionHeader>
<bodyText confidence="0.99997">
In all these areas, we anticipate international cooperation
with distinguished research groups from the Russian
Academy of Sciences (Leonid Iomdin), Bulgarian
Academy of Sciences (Kiril Simov and Petya Osenova),
Polish Academy of Sciences (Adam Przepiórkowski), and
others. We envisage for this international exchange to
eventually result in an international infrastructural project
on Slavic corpora and grammars (SlaviCoGram), and are
grateful to all these colleagues for preliminary discussions
and constructive cooperation.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99998134375">
[1] Uszkoreit, H., D. Flickinger, and S. Oepen, Proposal
of Themes and Modalities for International
Collaboration on Deep Linguistic Processing with
HPSG. 2001, DFKI LT Lab and Saarland University,
CSLI Stanford and YY Technologies.
[2] Uszkoreit, H. DELPHIN: Deep Linguistic Processing
with HPSG -- an International Collaboration. 2002
http://hans.uszkoreit.net/delphinhome.html.
[3] Uszkoreit, H. New Chances for Deep Linguistic
Processing. The 19th International Conference on
Computational Linguistics COLING&apos;02. 2002. Taipei,
Taiwan.
[4] Bender, E.M., D. Flickinger, and S. Oepen. The
Grammar Matrix: An Open-Source-Kit for the Rapid
Development of Cross-Linguistically Consistent
Broad-Coverage Precision Grammars. Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational
Linguistics. 2002. Taipei, Taiwan.
[5] Pollard, C. and I. Sag, Head-Driven Phrase Structure
Grammar. 1994, Chicago: University of Chicago
Press.
[6] Copestake, A., Implementing Typed Feature Structure
Grammars. CSLI Publications. 2002.
[7] Lønning, J.T. and S. Oepen. Re-usable tools for
precision machine translation. COLING|ACL 2006
Interactive Presentation Sessions. 2006. Sydney,
Australia.
[8] Oepen, S., et al., LinGO Redwoods. A rich and
dynamic treebank for HPSG. Journal of Research on
Language and Computation, 2004. 2(4 ): p. 575-596.
[9] Toutanova, K., et al., Stochastic HPSG parse selection
using the Redwoods corpus. Journal of Research on
Language and Computation, 2005. 3(1 ): p. 83-105.
[10] Bender, E.M. and D. Flickinger. Rapid Prototyping of
Scalable Grammars: Towards Modularity Extensions
to a Language-Independent Core. 2nd International
Joint Conference on Natural Language Processing.
2005. Jeju, Korea.
[11] Drellishak, S. and E.M. Bender. A Coordination
Module for a Crosslinguistic Grammar Resource. 12th
International Conference on Head-Driven Phrase
Structure Grammar. 2005. Stanford: CSLI.
[12] Callmeier, U., PET - a platform for experimentation
with efficient HPSG processing techniques. Natural
Language Engineering, 2000. 6 p. 99-107
[13] Oepen, S., et al. The LinGO Redwoods treebank.
Motivation and preliminary applications. The 19th
International Conference on Computational
Linguistics. 2002. Taipei, Taiwan.
[14] Copestake, A., A. Lascarides, and D. Flickinger. An
algebra for semantic construction in constraint-based
grammars. The 39th Meeting of the Association for
Computational Linguistics. 2001. Toulouse, France.
[15] Copestake, A., et al., Minimal Recursion Semantics:
An Introduction. Journal of Research on Language
and Computation, 2005. 3(4): p. 281-332.
[16] Flickinger, D. and E.M. Bender. Compositional
Semantics in a Multilingual Grammar Resource.
ESSLLI Workshop on Ideas and Strategies for
Multilingual Grammar Development. 2003.
[17] Flickinger, D., E.M. Bender, and S. Oepen, MRS in
the LinGO Grammar Matrix: A Practical User&apos;s Guide.
2003.
</reference>
<page confidence="0.949072">
10
</page>
<reference confidence="0.999853913043478">
[18] Bender, E.M. Evaluating a Crosslinguistic Grammar
Resource: A Case Study of Wambaya. ACL08 : HLT,.
2008. Columbus, Ohio.
[19] Segalovich, I. A fast morphological algorithm with
unknown word guessing induced by a dictionary for a
web search engine. MLMTA-2003. 2003. Las Vegas.
[20] Sokirko, A., Semantic Dictionaries in Automatic Text
Processing (based on materials of the system
DIALING) (in Russian). 2007.
[21] Mel&apos;cuk, I.A., The Russian Language in the Meaning-
Text Perspective. Wiener slawistischer Almanach.
Vol. Sonderband 39. 1995, Moskau - Wien:
Gesellschaft zur Förderung slawistischer Studien.
[22] Apresjan, J., et al. A Syntactically and Semantically
Tagged Corpus of Russian: State of the Art and
Prospects. The fifth international conference on
Language Resources and Evaluation, LREC 2006.
2006. Genoa, Italy.
[23] Boguslavsky, I., et al. Development of a dependency
treebank for Russian and its possible applications in
NLP. The third International Conference on Language
Resources and Evaluation (LREC-2002). 2002. Las
Palmas
</reference>
<page confidence="0.999479">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286058">
<title confidence="0.9937625">Exploiting the Russian National Corpus in the Development of a Russian Resource Grammar</title>
<author confidence="0.801219">Tania</author>
<affiliation confidence="0.556762">DFKI GmbH &amp; Saarland</affiliation>
<address confidence="0.80345">P.O. Box 66041 Saarbrücken,</address>
<email confidence="0.99843">avgustinova@dfki.de</email>
<abstract confidence="0.99990975">In this paper we present the on-going grammar engineering project in our group for developing in parallel resource precision grammars for Slavic languages. The project utilizes DELPH-IN software (LKB/[incr tsdb()]) as the grammar development platform, and has strong affinity to the LinGO Grammar Matrix project. It is innovative in that we focus on a closed set of related but extremely diverse languages. The goal is to encode mutually interoperable analyses of a wide variety of linguistic phenomena, taking into account eminent typological commonalities and systematic differences. As one major objective of the project, we aim to develop a core Slavic grammar whose components can be commonly shared among the set of languages, and facilitate new grammar development. As a showcase, we discuss a small HPSG grammar for Russian. The interesting bit of this grammar is that the development is assisted by interfacing with existing corpora and processing tools for the language, which saves significant amount of engineering effort.</abstract>
<keyword confidence="0.992259">Keywords</keyword>
<intro confidence="0.666377">Parallel grammar engineering, corpora, Slavic languages</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>H Uszkoreit</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<booktitle>Proposal of Themes and Modalities for International Collaboration on Deep Linguistic Processing with HPSG. 2001, DFKI LT Lab and</booktitle>
<institution>Saarland University, CSLI Stanford and YY Technologies.</institution>
<contexts>
<context position="2194" citStr="[1, 2, 3]" startWordPosition="319" endWordPosition="319">re Slavic grammar whose components can be commonly shared among the set of languages, and facilitate new grammar development. A decision on the proper set up along with a commitment to a reliable infrastructure right from the beginning are essential for such an endeavor because the implementation of linguistically-informed grammars for natural languages draws on a combination of engineering skills, sound grammatical theory, and software development tools. 1.1 DELPH-IN initiative Current international collaborative efforts on deep linguistic processing with Head-driven Phrase Structure Grammar [1, 2, 3] exploit the notion of shared grammar for Yi Zhang DFKI GmbH &amp; Saarland University P.O. Box 151150 66041 Saarbrücken, Germany yzhang@coli.uni-sb.de the rapid development of grammars for new languages and for the systematic adaptation of grammars to variants of the same language. This international partnership, which became popular under the name DELPH-IN1, is based on a shared commitment to re-usable, multi-purpose resources and active exchange. Its leading idea is to combine linguistic and statistical processing methods for getting at the meaning of texts and utterances. Based on contribution</context>
</contexts>
<marker>[1]</marker>
<rawString>Uszkoreit, H., D. Flickinger, and S. Oepen, Proposal of Themes and Modalities for International Collaboration on Deep Linguistic Processing with HPSG. 2001, DFKI LT Lab and Saarland University, CSLI Stanford and YY Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Uszkoreit</author>
</authors>
<title>DELPHIN: Deep Linguistic Processing with HPSG -- an International Collaboration.</title>
<date>2002</date>
<note>http://hans.uszkoreit.net/delphinhome.html.</note>
<contexts>
<context position="2194" citStr="[1, 2, 3]" startWordPosition="319" endWordPosition="319">re Slavic grammar whose components can be commonly shared among the set of languages, and facilitate new grammar development. A decision on the proper set up along with a commitment to a reliable infrastructure right from the beginning are essential for such an endeavor because the implementation of linguistically-informed grammars for natural languages draws on a combination of engineering skills, sound grammatical theory, and software development tools. 1.1 DELPH-IN initiative Current international collaborative efforts on deep linguistic processing with Head-driven Phrase Structure Grammar [1, 2, 3] exploit the notion of shared grammar for Yi Zhang DFKI GmbH &amp; Saarland University P.O. Box 151150 66041 Saarbrücken, Germany yzhang@coli.uni-sb.de the rapid development of grammars for new languages and for the systematic adaptation of grammars to variants of the same language. This international partnership, which became popular under the name DELPH-IN1, is based on a shared commitment to re-usable, multi-purpose resources and active exchange. Its leading idea is to combine linguistic and statistical processing methods for getting at the meaning of texts and utterances. Based on contribution</context>
</contexts>
<marker>[2]</marker>
<rawString>Uszkoreit, H. DELPHIN: Deep Linguistic Processing with HPSG -- an International Collaboration. 2002 http://hans.uszkoreit.net/delphinhome.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Uszkoreit</author>
</authors>
<title>New Chances for Deep Linguistic Processing.</title>
<date>2002</date>
<booktitle>The 19th International Conference on Computational Linguistics COLING&apos;02.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="2194" citStr="[1, 2, 3]" startWordPosition="319" endWordPosition="319">re Slavic grammar whose components can be commonly shared among the set of languages, and facilitate new grammar development. A decision on the proper set up along with a commitment to a reliable infrastructure right from the beginning are essential for such an endeavor because the implementation of linguistically-informed grammars for natural languages draws on a combination of engineering skills, sound grammatical theory, and software development tools. 1.1 DELPH-IN initiative Current international collaborative efforts on deep linguistic processing with Head-driven Phrase Structure Grammar [1, 2, 3] exploit the notion of shared grammar for Yi Zhang DFKI GmbH &amp; Saarland University P.O. Box 151150 66041 Saarbrücken, Germany yzhang@coli.uni-sb.de the rapid development of grammars for new languages and for the systematic adaptation of grammars to variants of the same language. This international partnership, which became popular under the name DELPH-IN1, is based on a shared commitment to re-usable, multi-purpose resources and active exchange. Its leading idea is to combine linguistic and statistical processing methods for getting at the meaning of texts and utterances. Based on contribution</context>
</contexts>
<marker>[3]</marker>
<rawString>Uszkoreit, H. New Chances for Deep Linguistic Processing. The 19th International Conference on Computational Linguistics COLING&apos;02. 2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<title>The Grammar Matrix: An Open-Source-Kit for the Rapid Development of Cross-Linguistically Consistent Broad-Coverage Precision Grammars.</title>
<date>2002</date>
<booktitle>Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4766" citStr="[4]" startWordPosition="687" endWordPosition="687">Parallel Grammar) project is one example of multiple grammars developed using a common standard. It aims at producing wide coverage grammars for a wide variety of languages. These are written collaboratively within the linguistic framework of Lexical Functional Grammar (LFG) and with a commonly-agreedupon set of grammatical features. URL: http://www2.parc.com/isl/groups/nltt/pargram/ 1 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11 comparable the LinGO3 Grammar Matrix4 has been set up as a multi-lingual grammar engineering project [4] which provides a web-based tool designed to support the creation of linguistically-motivated grammatical resources in the framework of HPSG [5]. The Grammar Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB5 grammar development environment [6]. It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation [7], treebanking [8] and parse selection [9]. 1.2 LinGO Grammar Matrix Generally speaking, the Grammar Matrix is an attempt to distill the wisdom of already existing broad coverage grammars and document it in a form</context>
</contexts>
<marker>[4]</marker>
<rawString>Bender, E.M., D. Flickinger, and S. Oepen. The Grammar Matrix: An Open-Source-Kit for the Rapid Development of Cross-Linguistically Consistent Broad-Coverage Precision Grammars. Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics. 2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="4910" citStr="[5]" startWordPosition="707" endWordPosition="707"> a wide variety of languages. These are written collaboratively within the linguistic framework of Lexical Functional Grammar (LFG) and with a commonly-agreedupon set of grammatical features. URL: http://www2.parc.com/isl/groups/nltt/pargram/ 1 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11 comparable the LinGO3 Grammar Matrix4 has been set up as a multi-lingual grammar engineering project [4] which provides a web-based tool designed to support the creation of linguistically-motivated grammatical resources in the framework of HPSG [5]. The Grammar Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB5 grammar development environment [6]. It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation [7], treebanking [8] and parse selection [9]. 1.2 LinGO Grammar Matrix Generally speaking, the Grammar Matrix is an attempt to distill the wisdom of already existing broad coverage grammars and document it in a form that can be used as the basis for new grammars. The main goals are to develop in detail semantic representations and the syntaxsemantics interf</context>
</contexts>
<marker>[5]</marker>
<rawString>Pollard, C. and I. Sag, Head-Driven Phrase Structure Grammar. 1994, Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="5060" citStr="[6]" startWordPosition="729" endWordPosition="729">y-agreedupon set of grammatical features. URL: http://www2.parc.com/isl/groups/nltt/pargram/ 1 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11 comparable the LinGO3 Grammar Matrix4 has been set up as a multi-lingual grammar engineering project [4] which provides a web-based tool designed to support the creation of linguistically-motivated grammatical resources in the framework of HPSG [5]. The Grammar Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB5 grammar development environment [6]. It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation [7], treebanking [8] and parse selection [9]. 1.2 LinGO Grammar Matrix Generally speaking, the Grammar Matrix is an attempt to distill the wisdom of already existing broad coverage grammars and document it in a form that can be used as the basis for new grammars. The main goals are to develop in detail semantic representations and the syntaxsemantics interface, consistent with other work in HPSG; to represent generalizations across linguistic objects and across languages; and to allow for very quick star</context>
<context position="6527" citStr="[6]" startWordPosition="954" endWordPosition="954">e. A web-based configuration system elicits typological information from the user-linguist through a questionnaire [10, 11] and then outputs a grammar consisting of the Matrix core plus selected types, rules and constraints from the libraries according to the specifications in the questionnaire, and lexical entries for the language in question. In other words, users specify phenomena relevant to their particular language, with their selections being compiled from libraries of available analyses into a starter grammar which can be immediately loaded into the LKB grammar development environment [6], as well as the PET parser [12], in order to parse sentences using the rules and constraints defined therein. The regression testing facilities of [incr tsdb()] allow for rapid experimentation with alternative analyses as new 3 The Linguistic Grammars Online (LinGO) team is committed to the development of linguistically precise grammars based on the HPSG framework, and general-purpose tools for use in grammar engineering, profiling, parsing and generation. URL: http://lingo.stanford.edu/ 4 URL: http://www.delph-in.net/matrix/ 5 LKB (Linguistic Knowledge Builder) system is a grammar and lexico</context>
</contexts>
<marker>[6]</marker>
<rawString>Copestake, A., Implementing Typed Feature Structure Grammars. CSLI Publications. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Lønning</author>
<author>S Oepen</author>
</authors>
<title>Re-usable tools for precision machine translation. COLING|ACL</title>
<date>2006</date>
<location>Sydney, Australia.</location>
<contexts>
<context position="5154" citStr="[7]" startWordPosition="744" endWordPosition="744"> Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11 comparable the LinGO3 Grammar Matrix4 has been set up as a multi-lingual grammar engineering project [4] which provides a web-based tool designed to support the creation of linguistically-motivated grammatical resources in the framework of HPSG [5]. The Grammar Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB5 grammar development environment [6]. It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation [7], treebanking [8] and parse selection [9]. 1.2 LinGO Grammar Matrix Generally speaking, the Grammar Matrix is an attempt to distill the wisdom of already existing broad coverage grammars and document it in a form that can be used as the basis for new grammars. The main goals are to develop in detail semantic representations and the syntaxsemantics interface, consistent with other work in HPSG; to represent generalizations across linguistic objects and across languages; and to allow for very quick start-up as the Matrix is applied to new languages. The fact that different parts of a single gram</context>
</contexts>
<marker>[7]</marker>
<rawString>Lønning, J.T. and S. Oepen. Re-usable tools for precision machine translation. COLING|ACL 2006 Interactive Presentation Sessions. 2006. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
</authors>
<title>Redwoods. A rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Journal of Research on Language and Computation,</journal>
<pages>2--4</pages>
<contexts>
<context position="5171" citStr="[8]" startWordPosition="746" endWordPosition="746">ion of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11 comparable the LinGO3 Grammar Matrix4 has been set up as a multi-lingual grammar engineering project [4] which provides a web-based tool designed to support the creation of linguistically-motivated grammatical resources in the framework of HPSG [5]. The Grammar Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB5 grammar development environment [6]. It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation [7], treebanking [8] and parse selection [9]. 1.2 LinGO Grammar Matrix Generally speaking, the Grammar Matrix is an attempt to distill the wisdom of already existing broad coverage grammars and document it in a form that can be used as the basis for new grammars. The main goals are to develop in detail semantic representations and the syntaxsemantics interface, consistent with other work in HPSG; to represent generalizations across linguistic objects and across languages; and to allow for very quick start-up as the Matrix is applied to new languages. The fact that different parts of a single grammar can be abstra</context>
</contexts>
<marker>[8]</marker>
<rawString>Oepen, S., et al., LinGO Redwoods. A rich and dynamic treebank for HPSG. Journal of Research on Language and Computation, 2004. 2(4 ): p. 575-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
</authors>
<title>Stochastic HPSG parse selection using the Redwoods corpus.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation,</journal>
<pages>3--1</pages>
<contexts>
<context position="5195" citStr="[9]" startWordPosition="750" endWordPosition="750">s and Technology to New Domains 2009 - Borovets, Bulgaria, pages 1–11 comparable the LinGO3 Grammar Matrix4 has been set up as a multi-lingual grammar engineering project [4] which provides a web-based tool designed to support the creation of linguistically-motivated grammatical resources in the framework of HPSG [5]. The Grammar Matrix is written in the TDL (type description language) formalism, which is interpreted by the LKB5 grammar development environment [6]. It is compatible with the broader range of DELPH-IN tools, e.g., for machine translation [7], treebanking [8] and parse selection [9]. 1.2 LinGO Grammar Matrix Generally speaking, the Grammar Matrix is an attempt to distill the wisdom of already existing broad coverage grammars and document it in a form that can be used as the basis for new grammars. The main goals are to develop in detail semantic representations and the syntaxsemantics interface, consistent with other work in HPSG; to represent generalizations across linguistic objects and across languages; and to allow for very quick start-up as the Matrix is applied to new languages. The fact that different parts of a single grammar can be abstracted into separate, inde</context>
</contexts>
<marker>[9]</marker>
<rawString>Toutanova, K., et al., Stochastic HPSG parse selection using the Redwoods corpus. Journal of Research on Language and Computation, 2005. 3(1 ): p. 83-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Bender</author>
<author>D Flickinger</author>
</authors>
<title>Rapid Prototyping of Scalable Grammars: Towards Modularity Extensions to a Language-Independent Core.</title>
<date>2005</date>
<booktitle>2nd International Joint Conference on Natural Language Processing.</booktitle>
<location>Jeju,</location>
<contexts>
<context position="5879" citStr="[10]" startWordPosition="859" endWordPosition="859">to distill the wisdom of already existing broad coverage grammars and document it in a form that can be used as the basis for new grammars. The main goals are to develop in detail semantic representations and the syntaxsemantics interface, consistent with other work in HPSG; to represent generalizations across linguistic objects and across languages; and to allow for very quick start-up as the Matrix is applied to new languages. The fact that different parts of a single grammar can be abstracted into separate, independent modules, either for processing or grammar development, is approached in [10] from the perspective of reuse of grammar code. A web-based configuration system elicits typological information from the user-linguist through a questionnaire [10, 11] and then outputs a grammar consisting of the Matrix core plus selected types, rules and constraints from the libraries according to the specifications in the questionnaire, and lexical entries for the language in question. In other words, users specify phenomena relevant to their particular language, with their selections being compiled from libraries of available analyses into a starter grammar which can be immediately loaded </context>
<context position="10795" citStr="[10]" startWordPosition="1565" endWordPosition="1565">ting from the types defined in the core Matrix and specifying appropriate language-specific constraints, and a set of lexical entries. In a lexicalist constraint-based framework, the grammars are expressed as a collection of typed feature structures which are arranged into a hierarchy such that information shared across multiple lexical entries or construction types is represented only on a single supertype. As a result, a cross-linguistic type hierarchy comes with a collection of phenomenon-specific libraries. 2. Typologically motivated modularity Aiming at typologically motivated modularity [10] describe a method for extending a language-independent core grammar with modules handling cross-linguistically variable but still recurring patterns. This method allows for extremely rapid prototyping of HPSG-conform grammars in such a way that the prototypes themselves can serve as the basis for sustained development, being able to scale up to broad-coverage resource grammars. The authors envision four potential uses for such grammar prototyping: (i) in pedagogical contexts, where it would allow grammar engineering students to more quickly work on cutting-edge problems; (ii) in language docu</context>
<context position="12594" citStr="[10]" startWordPosition="1824" endWordPosition="1824">language. For example, every language has some set of permissible word orders (formal) and a means of expressing sentential negation (functional). On the other hand, there are linguistic phenomena which appear in only some languages, and are not typically conceptualized as alternative realizations of some universal function, phenomena such as noun incorporation, numeral classifiers, and auxiliary verbs. It is indeed expected that the constraint definitions which are supplied to grammar developers can be extended to capture generalizations holding only for subsets of languages. The strategy of [10] is actually consistent with data driven investigation of linguistic universals and constraints on cross-linguistic variation. Therefore, we refer to this approach of grammar development (with the help from Grammar Matrix) as the &amp;quot;bottom-up&amp;quot; approach, because it is driven purely by the specific phenomena in the target language. It is certainly efficient: Specifying the choice file and building a small working grammar can be done within an hour (excluding the time spent on deciding specific choices for the given language). For instance [18] reports a relatively short development time required t</context>
</contexts>
<marker>[10]</marker>
<rawString>Bender, E.M. and D. Flickinger. Rapid Prototyping of Scalable Grammars: Towards Modularity Extensions to a Language-Independent Core. 2nd International Joint Conference on Natural Language Processing. 2005. Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Drellishak</author>
<author>E M Bender</author>
</authors>
<title>A Coordination Module for a Crosslinguistic Grammar Resource.</title>
<date>2005</date>
<booktitle>12th International Conference on Head-Driven Phrase Structure Grammar.</booktitle>
<publisher>CSLI.</publisher>
<location>Stanford:</location>
<contexts>
<context position="6047" citStr="[10, 11]" startWordPosition="881" endWordPosition="882">velop in detail semantic representations and the syntaxsemantics interface, consistent with other work in HPSG; to represent generalizations across linguistic objects and across languages; and to allow for very quick start-up as the Matrix is applied to new languages. The fact that different parts of a single grammar can be abstracted into separate, independent modules, either for processing or grammar development, is approached in [10] from the perspective of reuse of grammar code. A web-based configuration system elicits typological information from the user-linguist through a questionnaire [10, 11] and then outputs a grammar consisting of the Matrix core plus selected types, rules and constraints from the libraries according to the specifications in the questionnaire, and lexical entries for the language in question. In other words, users specify phenomena relevant to their particular language, with their selections being compiled from libraries of available analyses into a starter grammar which can be immediately loaded into the LKB grammar development environment [6], as well as the PET parser [12], in order to parse sentences using the rules and constraints defined therein. The regre</context>
</contexts>
<marker>[11]</marker>
<rawString>Drellishak, S. and E.M. Bender. A Coordination Module for a Crosslinguistic Grammar Resource. 12th International Conference on Head-Driven Phrase Structure Grammar. 2005. Stanford: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Callmeier</author>
</authors>
<title>PET - a platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering,</title>
<date>2000</date>
<volume>6</volume>
<pages>99--107</pages>
<contexts>
<context position="6559" citStr="[12]" startWordPosition="961" endWordPosition="961">stem elicits typological information from the user-linguist through a questionnaire [10, 11] and then outputs a grammar consisting of the Matrix core plus selected types, rules and constraints from the libraries according to the specifications in the questionnaire, and lexical entries for the language in question. In other words, users specify phenomena relevant to their particular language, with their selections being compiled from libraries of available analyses into a starter grammar which can be immediately loaded into the LKB grammar development environment [6], as well as the PET parser [12], in order to parse sentences using the rules and constraints defined therein. The regression testing facilities of [incr tsdb()] allow for rapid experimentation with alternative analyses as new 3 The Linguistic Grammars Online (LinGO) team is committed to the development of linguistically precise grammars based on the HPSG framework, and general-purpose tools for use in grammar engineering, profiling, parsing and generation. URL: http://lingo.stanford.edu/ 4 URL: http://www.delph-in.net/matrix/ 5 LKB (Linguistic Knowledge Builder) system is a grammar and lexicon development environment for us</context>
</contexts>
<marker>[12]</marker>
<rawString>Callmeier, U., PET - a platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 2000. 6 p. 99-107</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
</authors>
<title>The LinGO Redwoods treebank. Motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>The 19th International Conference on Computational Linguistics.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7424" citStr="[13]" startWordPosition="1079" endWordPosition="1079">development of linguistically precise grammars based on the HPSG framework, and general-purpose tools for use in grammar engineering, profiling, parsing and generation. URL: http://lingo.stanford.edu/ 4 URL: http://www.delph-in.net/matrix/ 5 LKB (Linguistic Knowledge Builder) system is a grammar and lexicon development environment for use with unificationbased linguistic formalisms. While not restricted to HPSG, the LKB implements the DELPH-IN reference formalism of typed feature structures (jointly with other DELPH-IN software using the same formalism). phenomena are brought into the grammar [13]. The original Grammar Matrix consisted of types defining the basic feature geometry, e.g. [14], types for lexical and syntactic rules encoding the ways that heads combine with arguments and adjuncts, and configuration files for the LKB grammar development environment [6] and the PET system [12]. Subsequent releases have refined the original types and developed a lexical hierarchy, including linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Minimal Recursion Semantics [15, 16, 17]. The </context>
</contexts>
<marker>[13]</marker>
<rawString>Oepen, S., et al. The LinGO Redwoods treebank. Motivation and preliminary applications. The 19th International Conference on Computational Linguistics. 2002. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>A Lascarides</author>
<author>D Flickinger</author>
</authors>
<title>An algebra for semantic construction in constraint-based grammars.</title>
<date>2001</date>
<booktitle>The 39th Meeting of the Association for Computational Linguistics.</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="7519" citStr="[14]" startWordPosition="1093" endWordPosition="1093"> tools for use in grammar engineering, profiling, parsing and generation. URL: http://lingo.stanford.edu/ 4 URL: http://www.delph-in.net/matrix/ 5 LKB (Linguistic Knowledge Builder) system is a grammar and lexicon development environment for use with unificationbased linguistic formalisms. While not restricted to HPSG, the LKB implements the DELPH-IN reference formalism of typed feature structures (jointly with other DELPH-IN software using the same formalism). phenomena are brought into the grammar [13]. The original Grammar Matrix consisted of types defining the basic feature geometry, e.g. [14], types for lexical and syntactic rules encoding the ways that heads combine with arguments and adjuncts, and configuration files for the LKB grammar development environment [6] and the PET system [12]. Subsequent releases have refined the original types and developed a lexical hierarchy, including linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Minimal Recursion Semantics [15, 16, 17]. The constraints in this ‘core’ Matrix are intended to be language-independent and monotonically ext</context>
</contexts>
<marker>[14]</marker>
<rawString>Copestake, A., A. Lascarides, and D. Flickinger. An algebra for semantic construction in constraint-based grammars. The 39th Meeting of the Association for Computational Linguistics. 2001. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Minimal Recursion Semantics: An Introduction.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>281--332</pages>
<contexts>
<context position="8018" citStr="[15, 16, 17]" startWordPosition="1163" endWordPosition="1163">o the grammar [13]. The original Grammar Matrix consisted of types defining the basic feature geometry, e.g. [14], types for lexical and syntactic rules encoding the ways that heads combine with arguments and adjuncts, and configuration files for the LKB grammar development environment [6] and the PET system [12]. Subsequent releases have refined the original types and developed a lexical hierarchy, including linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Minimal Recursion Semantics [15, 16, 17]. The constraints in this ‘core’ Matrix are intended to be language-independent and monotonically extensible in any given grammar. In its recent development, the Grammar Matrix project aims at employing typologically motivated, customizable extensions to a language-independent core grammar. The implemented prototype consists of a small set of modules targeting basic word order (addressing the relative order of subjects, verbs, and verbal complements), sentential negation, main-clause yes-no questions, and a small range of lexical entries. In particular: • The Matrix core grammar provides defin</context>
<context position="34643" citStr="[15]" startWordPosition="5087" endWordPosition="5087">l soon lose credibility.&apos; (11) HeTpa TomxFrr Peter[acc.mask.sg] feel-sick[pres.3sg] &apos;Peter feels sick.&apos; (12) CTapbiH ripocpeccop ropX4TCA CTyAeHTaMH. old[nom.mask.sg] professor[nom.mask.sg] beproud[pres.3sg] student[ins.pl] &apos;The old professor is proud of the students.&apos; (13) &amp;quot;LICTpO TeMHeno. quickly get-dark[past.sg.neut] &apos;It was getting dark quickly.&apos; (14) #Tuy He3Aopowrrca. father[dat.masc,sg] feel-unwell[pres.3sg] &apos;Father does not feel well. The linguistic analyses encoded in the grammar serve to map the surface strings to semantic representations in Minimal Recursion Semantics (MRS) format [15]. For instance, the MRS in Figure 9 is assigned to the example in (6). It includes the basic propositional structure: a situation of ‘giving’ in which the first argument, or agent, is ‘professor’, the second (recipient) is ‘student’, and the third (patient), is ‘task’. The relations are given English predicate names for the convenience of the grammar developer. A simple tree display in Figure 10 offers an abbreviated view over the HPSG derivation while hiding the detailed typed feature structures beneath away from the user. 9 Figure 9: MRS representation. Figure 10: Tree representation 5. Outl</context>
</contexts>
<marker>[15]</marker>
<rawString>Copestake, A., et al., Minimal Recursion Semantics: An Introduction. Journal of Research on Language and Computation, 2005. 3(4): p. 281-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>E M Bender</author>
</authors>
<title>Compositional Semantics in a Multilingual Grammar Resource.</title>
<date>2003</date>
<booktitle>ESSLLI Workshop on Ideas and Strategies for Multilingual Grammar Development.</booktitle>
<contexts>
<context position="8018" citStr="[15, 16, 17]" startWordPosition="1163" endWordPosition="1163">o the grammar [13]. The original Grammar Matrix consisted of types defining the basic feature geometry, e.g. [14], types for lexical and syntactic rules encoding the ways that heads combine with arguments and adjuncts, and configuration files for the LKB grammar development environment [6] and the PET system [12]. Subsequent releases have refined the original types and developed a lexical hierarchy, including linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Minimal Recursion Semantics [15, 16, 17]. The constraints in this ‘core’ Matrix are intended to be language-independent and monotonically extensible in any given grammar. In its recent development, the Grammar Matrix project aims at employing typologically motivated, customizable extensions to a language-independent core grammar. The implemented prototype consists of a small set of modules targeting basic word order (addressing the relative order of subjects, verbs, and verbal complements), sentential negation, main-clause yes-no questions, and a small range of lexical entries. In particular: • The Matrix core grammar provides defin</context>
</contexts>
<marker>[16]</marker>
<rawString>Flickinger, D. and E.M. Bender. Compositional Semantics in a Multilingual Grammar Resource. ESSLLI Workshop on Ideas and Strategies for Multilingual Grammar Development. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>E M Bender</author>
<author>S Oepen</author>
</authors>
<title>MRS in the LinGO Grammar Matrix: A Practical User&apos;s Guide.</title>
<date>2003</date>
<contexts>
<context position="8018" citStr="[15, 16, 17]" startWordPosition="1163" endWordPosition="1163">o the grammar [13]. The original Grammar Matrix consisted of types defining the basic feature geometry, e.g. [14], types for lexical and syntactic rules encoding the ways that heads combine with arguments and adjuncts, and configuration files for the LKB grammar development environment [6] and the PET system [12]. Subsequent releases have refined the original types and developed a lexical hierarchy, including linking types for relating syntactic to semantic arguments, and the constraints required to compositionally build up semantic representations in the format of Minimal Recursion Semantics [15, 16, 17]. The constraints in this ‘core’ Matrix are intended to be language-independent and monotonically extensible in any given grammar. In its recent development, the Grammar Matrix project aims at employing typologically motivated, customizable extensions to a language-independent core grammar. The implemented prototype consists of a small set of modules targeting basic word order (addressing the relative order of subjects, verbs, and verbal complements), sentential negation, main-clause yes-no questions, and a small range of lexical entries. In particular: • The Matrix core grammar provides defin</context>
</contexts>
<marker>[17]</marker>
<rawString>Flickinger, D., E.M. Bender, and S. Oepen, MRS in the LinGO Grammar Matrix: A Practical User&apos;s Guide. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Bender</author>
</authors>
<title>Evaluating a Crosslinguistic Grammar Resource: A Case Study of Wambaya.</title>
<date>2008</date>
<booktitle>ACL08 : HLT,.</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="13139" citStr="[18]" startWordPosition="1906" endWordPosition="1906">holding only for subsets of languages. The strategy of [10] is actually consistent with data driven investigation of linguistic universals and constraints on cross-linguistic variation. Therefore, we refer to this approach of grammar development (with the help from Grammar Matrix) as the &amp;quot;bottom-up&amp;quot; approach, because it is driven purely by the specific phenomena in the target language. It is certainly efficient: Specifying the choice file and building a small working grammar can be done within an hour (excluding the time spent on deciding specific choices for the given language). For instance [18] reports a relatively short development time required to create a precision, hand-built grammar for the Australian language Wambaya as a qualitative evaluation of the Grammar Matrix as a cross-linguistic resource.&apos; The major drawback of this approach, however, is that, for the set of customized grammars for a group of languages, it soon becomes difficult (if not impossible) to harmonize the treatment of related phenomena across languages. With grammars being created individually, the treatment of shared phenomena would work to the degree that satisfies but does not guarantee cross-linguistic c</context>
</contexts>
<marker>[18]</marker>
<rawString>Bender, E.M. Evaluating a Crosslinguistic Grammar Resource: A Case Study of Wambaya. ACL08 : HLT,. 2008. Columbus, Ohio.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Segalovich</author>
</authors>
<title>A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine.</title>
<booktitle>MLMTA-2003. 2003. Las Vegas.</booktitle>
<contexts>
<context position="21465" citStr="[19]" startWordPosition="3173" endWordPosition="3173">n the rest of the corpus only the lexeme and the part of speech are displayed. Morphological analysis is the basic enabling technology for many kinds of text processing. Integrating a morphological analyzer is a crucial prerequisite for all grammar development activities involving Slavic languages. For research purposes, such systems are by and large freely available nowadays, and the LKB grammar engineering environment provides the required interface for integrating a morphological pre-processor. For the pre-processing module we have considered two morphological analyzers for Russian: Mystem [19] and Dialing [20]. Both systems of are based on finite state transducers and have been used in the Russian National Corpus. Unlike Mystem, the system Dialing covers both inflectional and derivational morphology and is based on a large dictionary which also contains information on inflections, prefixes and affixes and stress patterns. Figure 2: Morphological input via inflectional rules npocpeccop(npoc�eccop=S,nnyxc,OA=wm,eA) 4NTaeT(4NTOTb=V,HeCOB=HenpoW,eA,H3bAB,3-A) HN!y{ HN!a=S,JKeH,HeOA=BNH,eA) S&amp;quot;wm #� $%&amp;$-$%&apos;-()&amp;*+, S&amp;quot;BHH #� $%&amp;$--..-()&amp;*+, S&amp;quot;eA #~ $%&amp;$-/0-()&amp;*+, V&amp;quot;Henpow #� 1+)2-$%$3-/4-</context>
</contexts>
<marker>[19]</marker>
<rawString>Segalovich, I. A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine. MLMTA-2003. 2003. Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sokirko</author>
</authors>
<title>Semantic Dictionaries in Automatic Text Processing (based on materials of the system DIALING) (in Russian).</title>
<date>2007</date>
<contexts>
<context position="21482" citStr="[20]" startWordPosition="3176" endWordPosition="3176"> corpus only the lexeme and the part of speech are displayed. Morphological analysis is the basic enabling technology for many kinds of text processing. Integrating a morphological analyzer is a crucial prerequisite for all grammar development activities involving Slavic languages. For research purposes, such systems are by and large freely available nowadays, and the LKB grammar engineering environment provides the required interface for integrating a morphological pre-processor. For the pre-processing module we have considered two morphological analyzers for Russian: Mystem [19] and Dialing [20]. Both systems of are based on finite state transducers and have been used in the Russian National Corpus. Unlike Mystem, the system Dialing covers both inflectional and derivational morphology and is based on a large dictionary which also contains information on inflections, prefixes and affixes and stress patterns. Figure 2: Morphological input via inflectional rules npocpeccop(npoc�eccop=S,nnyxc,OA=wm,eA) 4NTaeT(4NTOTb=V,HeCOB=HenpoW,eA,H3bAB,3-A) HN!y{ HN!a=S,JKeH,HeOA=BNH,eA) S&amp;quot;wm #� $%&amp;$-$%&apos;-()&amp;*+, S&amp;quot;BHH #� $%&amp;$--..-()&amp;*+, S&amp;quot;eA #~ $%&amp;$-/0-()&amp;*+, V&amp;quot;Henpow #� 1+)2-$%$3-/4-()&amp;*+, Mystem, ho</context>
</contexts>
<marker>[20]</marker>
<rawString>Sokirko, A., Semantic Dictionaries in Automatic Text Processing (based on materials of the system DIALING) (in Russian). 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;cuk</author>
</authors>
<title>The Russian Language in the MeaningText Perspective. Wiener slawistischer Almanach. Vol.</title>
<date>1995</date>
<journal>Sonderband</journal>
<volume>39</volume>
<contexts>
<context position="26852" citStr="[21]" startWordPosition="3986" endWordPosition="3986"> dative I- accusative I- instrumental F prepositive I- locative I- vocative Grade ✓ comparative I- comparative 2 F superlative Aspect F perfective I- imperfective Tense r present I- non-past ▪ past Person I- first I- second F third Animacy Form F animate 7 short form 7 inanimate Gender 7 masculine 7 feminine 7 neuter Representation • finite verb infinitive E participle I- gerund Voice E passive Number 7 singular 7 plural Mood indicative ✓ imperative Other I- part of a compound word 6 Figure 6: RNC access to the syntactic sub-corpus The syntactic formalism originates in the Meaning-Text Theory [21], but the inventory of syntactic relations has been extended for the purposes of corpus annotation, incorporating a number of specific linguistic decisions [22, 23]. Figure 7: A sample structure We, therefore, observe the following straightforward convention when the components of a RNC dependency relation (cf. the inventory in Figure 8) are to be mapped on HPSG categories: in a given syntactic dependency relation, the “governor X” corresponds in HPSG to the lexical head of the head daughter, while the “dependent Y” corresponds to the lexical head of the non-head daughter. As actantial surface</context>
</contexts>
<marker>[21]</marker>
<rawString>Mel&apos;cuk, I.A., The Russian Language in the MeaningText Perspective. Wiener slawistischer Almanach. Vol. Sonderband 39. 1995, Moskau - Wien: Gesellschaft zur Förderung slawistischer Studien.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Apresjan</author>
</authors>
<title>A Syntactically and Semantically Tagged Corpus of Russian: State of the Art and Prospects.</title>
<date>2006</date>
<booktitle>The fifth international conference on Language Resources and Evaluation, LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="27016" citStr="[22, 23]" startWordPosition="4009" endWordPosition="4010">ve Tense r present I- non-past ▪ past Person I- first I- second F third Animacy Form F animate 7 short form 7 inanimate Gender 7 masculine 7 feminine 7 neuter Representation • finite verb infinitive E participle I- gerund Voice E passive Number 7 singular 7 plural Mood indicative ✓ imperative Other I- part of a compound word 6 Figure 6: RNC access to the syntactic sub-corpus The syntactic formalism originates in the Meaning-Text Theory [21], but the inventory of syntactic relations has been extended for the purposes of corpus annotation, incorporating a number of specific linguistic decisions [22, 23]. Figure 7: A sample structure We, therefore, observe the following straightforward convention when the components of a RNC dependency relation (cf. the inventory in Figure 8) are to be mapped on HPSG categories: in a given syntactic dependency relation, the “governor X” corresponds in HPSG to the lexical head of the head daughter, while the “dependent Y” corresponds to the lexical head of the non-head daughter. As actantial surface syntactic relations connect a predicate word [X] with its syntactic argument [Y], they would by and large map to headed phrases saturating valence requirements. Fo</context>
</contexts>
<marker>[22]</marker>
<rawString>Apresjan, J., et al. A Syntactically and Semantically Tagged Corpus of Russian: State of the Art and Prospects. The fifth international conference on Language Resources and Evaluation, LREC 2006. 2006. Genoa, Italy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Boguslavsky</author>
</authors>
<title>Development of a dependency treebank for Russian and its possible applications in NLP.</title>
<booktitle>The third International Conference on Language Resources and Evaluation (LREC-2002). 2002. Las</booktitle>
<location>Palmas</location>
<contexts>
<context position="27016" citStr="[22, 23]" startWordPosition="4009" endWordPosition="4010">ve Tense r present I- non-past ▪ past Person I- first I- second F third Animacy Form F animate 7 short form 7 inanimate Gender 7 masculine 7 feminine 7 neuter Representation • finite verb infinitive E participle I- gerund Voice E passive Number 7 singular 7 plural Mood indicative ✓ imperative Other I- part of a compound word 6 Figure 6: RNC access to the syntactic sub-corpus The syntactic formalism originates in the Meaning-Text Theory [21], but the inventory of syntactic relations has been extended for the purposes of corpus annotation, incorporating a number of specific linguistic decisions [22, 23]. Figure 7: A sample structure We, therefore, observe the following straightforward convention when the components of a RNC dependency relation (cf. the inventory in Figure 8) are to be mapped on HPSG categories: in a given syntactic dependency relation, the “governor X” corresponds in HPSG to the lexical head of the head daughter, while the “dependent Y” corresponds to the lexical head of the non-head daughter. As actantial surface syntactic relations connect a predicate word [X] with its syntactic argument [Y], they would by and large map to headed phrases saturating valence requirements. Fo</context>
</contexts>
<marker>[23]</marker>
<rawString>Boguslavsky, I., et al. Development of a dependency treebank for Russian and its possible applications in NLP. The third International Conference on Language Resources and Evaluation (LREC-2002). 2002. Las Palmas</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>