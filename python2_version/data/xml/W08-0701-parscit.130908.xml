<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.267400">
<title confidence="0.9884475">
Invited Talk:
Phonological Models in Automatic Speech Recognition
</title>
<author confidence="0.988041">
Karen Livescu
</author>
<affiliation confidence="0.923324">
Toyota Technological Institute at Chicago
</affiliation>
<address confidence="0.599446">
1427 E. 60th St., Chicago, IL 60637
</address>
<email confidence="0.771423">
klivescu@tti-c.org
</email>
<sectionHeader confidence="0.968782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861382352941">
The performance of automatic speech recognition systems varies widely across different contexts. Very
good performance can be achieved on single-speaker, large-vocabulary dictation in a clean acoustic environ-
ment, as well as on very small vocabulary tasks with much fewer constraints on the speakers and acoustic
conditions. In other domains, speech recognition is still far from usable for real-world applications. One
domain that is still elusive is that of spontaneous conversational speech. This type of speech poses a number
of challenges, such as the presence of disfluencies, a mix of speech and non-speech sounds such as laughter,
and extreme variation in pronunciation. In this talk, I will focus on the challenge of pronunciation variation.
A number of analyses suggest that this variability is responsible for a large part of the drop in recognition
performance between read (dictated) speech and conversational speech.
I will describe efforts in the speech recognition community to characterize and model pronunciation
variation, both for conversational speech and in general. The work can be roughly divided into several types
of approaches, including: augmentation of a phonetic pronunciation lexicon with phonological rules; the use
of large (syllable- or word-sized) units instead of the more traditional phonetic ones; and the use of smaller
units, such as distinctive or articulatory features. Of these, the first is the most thoroughly studied and
also the most disappointing: Despite successes in a few domains, it has been difficult to obtain significant
recognition improvements by including in the lexicon those phonetic pronunciations that appear to exist in
the data. In part as a reaction to this, many have advocated the use of a “null pronunciation model,” i.e. a
very limited lexicon including only canonical pronunciations. The assumption in this approach is that the
observation model—the distribution of the acoustics given phonetic units—will better model the “noise”
introduced by pronunciation variability.
I will advocate an alternative view: that the phone unit may not be the most appropriate for modeling the
lexicon. When considering a variety of pronunciation phenomena, it becomes apparent that phonetic tran-
scription often obscures some of the fundamental processes that are at play. I will describe approaches using
both larger and “smaller” units. Larger units are typically syllables or words, and allow greater freedom to
model the component states of each unit. In the class of “smaller” unit models, ideas from articulatory and
autosegmental phonology motivate multi-tier models in which different features (or groups of features) have
semi-independent behavior. I will present a particular model in which articulatory features are represented
as variables in a dynamic Bayesian network.
Non-phonetic pronunciation models can involve significantly different model structures than those typi-
cally used in speech recognition, and as a result they may also entail modifications to other components such
as the observation model and training algorithms. At this point it is not clear what the “winning” approach
will be. The success of a given approach may depend on the domain or on the amount and type of training
data available. I will describe some of the current challenges and ongoing work, with a particular focus on
the role of phonological theories in statistical models of pronunciation (and vice versa?).
</bodyText>
<page confidence="0.626919">
1
</page>
<reference confidence="0.848617">
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 1,
Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.532885">
<title confidence="0.9830125">Invited Talk: Phonological Models in Automatic Speech Recognition</title>
<author confidence="0.990756">Karen</author>
<affiliation confidence="0.884583">Toyota Technological Institute at</affiliation>
<address confidence="0.99072">1427 E. 60th St., Chicago, IL</address>
<email confidence="0.986881">klivescu@tti-c.org</email>
<abstract confidence="0.997893514285714">The performance of automatic speech recognition systems varies widely across different contexts. Very good performance can be achieved on single-speaker, large-vocabulary dictation in a clean acoustic environment, as well as on very small vocabulary tasks with much fewer constraints on the speakers and acoustic conditions. In other domains, speech recognition is still far from usable for real-world applications. One domain that is still elusive is that of spontaneous conversational speech. This type of speech poses a number of challenges, such as the presence of disfluencies, a mix of speech and non-speech sounds such as laughter, and extreme variation in pronunciation. In this talk, I will focus on the challenge of pronunciation variation. A number of analyses suggest that this variability is responsible for a large part of the drop in recognition performance between read (dictated) speech and conversational speech. I will describe efforts in the speech recognition community to characterize and model pronunciation variation, both for conversational speech and in general. The work can be roughly divided into several types of approaches, including: augmentation of a phonetic pronunciation lexicon with phonological rules; the use large (syllableor word-sized) units instead of the more traditional phonetic ones; and the use of units, such as distinctive or articulatory features. Of these, the first is the most thoroughly studied and also the most disappointing: Despite successes in a few domains, it has been difficult to obtain significant recognition improvements by including in the lexicon those phonetic pronunciations that appear to exist in the data. In part as a reaction to this, many have advocated the use of a “null pronunciation model,” i.e. a very limited lexicon including only canonical pronunciations. The assumption in this approach is that the observation model—the distribution of the acoustics given phonetic units—will better model the “noise” introduced by pronunciation variability. I will advocate an alternative view: that the phone unit may not be the most appropriate for modeling the lexicon. When considering a variety of pronunciation phenomena, it becomes apparent that phonetic transcription often obscures some of the fundamental processes that are at play. I will describe approaches using both larger and “smaller” units. Larger units are typically syllables or words, and allow greater freedom to model the component states of each unit. In the class of “smaller” unit models, ideas from articulatory and autosegmental phonology motivate multi-tier models in which different features (or groups of features) have semi-independent behavior. I will present a particular model in which articulatory features are represented as variables in a dynamic Bayesian network. Non-phonetic pronunciation models can involve significantly different model structures than those typically used in speech recognition, and as a result they may also entail modifications to other components such as the observation model and training algorithms. At this point it is not clear what the “winning” approach will be. The success of a given approach may depend on the domain or on the amount and type of training data available. I will describe some of the current challenges and ongoing work, with a particular focus on the role of phonological theories in statistical models of pronunciation (and vice versa?).</abstract>
<note confidence="0.809225">1 of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and page 1, Ohio, USA June 2008. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>1</pages>
<marker></marker>
<rawString>Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 1,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>