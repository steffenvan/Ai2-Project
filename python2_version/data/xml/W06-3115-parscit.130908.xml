<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039905">
<title confidence="0.946174">
NTT System Description for the WMT2006 Shared Task
</title>
<author confidence="0.813491">
Taro Watanabe Hajime Tsukada Hideki Isozaki
</author>
<affiliation confidence="0.701619">
NTT Communication Science Laboratories
</affiliation>
<address confidence="0.89958">
2-4 Hikaridai, Seika-cho, Soraku-gun,
Kyoto, Japan 619-0237
</address>
<email confidence="0.997858">
{taro,tsukada,isozaki}@kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865">
We present two translation systems ex-
perimented for the shared-task of “Work-
shop on Statistical Machine Translation,”
a phrase-based model and a hierarchical
phrase-based model. The former uses a
phrasal unit for translation, whereas the
latter is conceptualized as a synchronous-
CFG in which phrases are hierarchically
combined using non-terminals. Experi-
ments showed that the hierarchical phrase-
based model performed very comparable
to the phrase-based model. We also report
a phrase/rule extraction technique differ-
entiating tokenization of corpora.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999452">
We contrasted two translation methods for the
Workshop on Statistical Machine Translation
(WMT2006) shared-task. One is a phrase-based
translation in which a phrasal unit is employed
for translation (Koehn et al., 2003). The other is
a hierarchical phrase-based translation in which
translation is realized as a set of paired production
rules (Chiang, 2005). Section 2 discusses those two
models and details extraction algorithms, decoding
algorithms and feature functions.
We also explored three types of corpus pre-
processing in Section 3. As expected, different
tokenization would lead to different word align-
ments which, in turn, resulted in the divergence
of the extracted phrase/rule size. In our method,
phrase/rule translation pairs extracted from three
distinctly word-aligned corpora are aggregated into
one large phrase/rule translation table. The experi-
ments and the final translation results are presented
in Section 4.
</bodyText>
<sectionHeader confidence="0.99093" genericHeader="method">
2 Translation Models
</sectionHeader>
<bodyText confidence="0.9959424">
We used a log-linear approach (Och and Ney,
2002) in which a foreign language sentence f1J =
f1, f2, ...fJ is translated into another language, i.e.
English, eI1 = e1, e2,..., eI by seeking a maximum
likelihood solution of
</bodyText>
<equation confidence="0.996899285714286">
ˆeI 1= argmax Pr(eI1 |f1J) (1)
eI 1
= argmax (EM )
eI exp m=1 λmhm(eI 1, f 1 J)
1 (2)
M
Ee′i′ exp (Em=1 λmhm(e′i , f1J))
</equation>
<bodyText confidence="0.9980226875">
In this framework, the posterior probability
Pr(eI1 |f1J) is directly maximized using a log-linear
combination of feature functions hm(eI1, f1J), such
as a ngram language model or a translation model.
When decoding, the denominator is dropped since it
depends only on f1J. Feature function scaling factors
λm are optimized based on a maximum likelihood
approach (Och and Ney, 2002) or on a direct error
minimization approach (Och, 2003). This modeling
allows the integration of various feature functions
depending on the scenario of how a translation is
constituted.
In a phrase-based statistical translation (Koehn
et al., 2003), a bilingual text is decomposed as K
phrase translation pairs (¯e1, ¯f¯a1), (¯e2, ¯f¯a2), ...: The in-
put foreign sentence is segmented into phrases
</bodyText>
<page confidence="0.986684">
fK
1,
122
</page>
<subsectionHeader confidence="0.78439">
Proceedings of the Workshop on Statistical Machine Translation, pages 122–125,
New York City, June 2006. c�2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.998797875">
mapped into corresponding English¯eK1 , then, re-
ordered to form the output English sentence accord-
ing to a phrase alignment index mapping ¯a.
In a hierarchical phrase-based translation (Chi-
ang, 2005), translation is modeled after a weighted
synchronous-CFG consisting of production rules
whose right-hand side is paired (Aho and Ullman,
1969):
</bodyText>
<equation confidence="0.943751">
X —� (γ,α,—)
</equation>
<bodyText confidence="0.993538333333333">
where X is a non-terminal, γ and α are strings of ter-
minals and non-terminals. — is a one-to-one corre-
spondence for the non-terminals appeared in γ and
α. Starting from an initial non-terminal, each rule
rewrites non-terminals in γ and α that are associated
with —.
</bodyText>
<subsectionHeader confidence="0.952864">
2.1 Phrase/Rule Extraction
</subsectionHeader>
<bodyText confidence="0.997638588235294">
The phrase extraction algorithm is based on those
presented by Koehn et al. (2003). First, many-
to-many word alignments are induced by running
a one-to-many word alignment model, such as
GIZA++ (Och and Ney, 2003), in both directions
and by combining the results based on a heuristic
(Och and Ney, 2004). Second, phrase translation
pairs are extracted from the word aligned corpus
(Koehn et al., 2003). The method exhaustively ex-
tracts phrase pairs ( f j+m
j , ei+n
i ) from a sentence pair
(f1J , eI1) that do not violate the word alignment con-
straints a.
In the hierarchical phrase-based model, produc-
tion rules are accumulated by computing “holes” for
extracted contiguous phrases (Chiang, 2005):
</bodyText>
<listItem confidence="0.999425333333333">
1. A phrase pair (f¯, ¯e) constitutes a rule:
)X � (f¯ , e¯
2. A rule X —� (γ, α) and a phrase pair (f¯, ¯e) s.t.
</listItem>
<equation confidence="0.961423333333333">
f¯γ′′ and α = α′¯eα′′ constitutes a rule:
(γ′ Xk γ′′, α′ Xk α′′)
X �
</equation>
<subsectionHeader confidence="0.995466">
2.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999996380952381">
The decoder for the phrase-based model is a left-to-
right generation decoder with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words. The decoding process is very
similar to those described in (Koehn et al., 2003):
It starts from an initial empty hypothesis. From an
existing hypothesis, new hypothesis is generated by
consuming a phrase translation pair that covers un-
translated foreign word positions. The score for the
newly generated hypothesis is updated by combin-
ing the scores of feature functions described in Sec-
tion 2.3. The English side of the phrase is simply
concatenated to form a new prefix of English sen-
tence.
In the hierarchical phrase-based model, decoding
is realized as an Earley-style top-down parser on the
foreign language side with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words (Watanabe et al., 2006). The ma-
jor difference to the phrase-based model’s decoder is
the handling of non-terminals, or holes, in each rule.
</bodyText>
<subsectionHeader confidence="0.992578">
2.3 Feature Functions
</subsectionHeader>
<bodyText confidence="0.999047666666667">
Our phrase-based model uses a standard pharaoh
feature functions listed as follows (Koehn et al.,
2003):
</bodyText>
<listItem confidence="0.997729666666667">
• Relative-count based phrase translation proba-
bilities in both directions.
• Lexically weighted feature functions in both di-
rections.
• The supplied trigram language model.
• Distortion model that counts the number of
words skipped.
• The number of words in English-side and the
number of phrases that constitute translation.
</listItem>
<bodyText confidence="0.9920405">
For details, please refer to Koehn et al. (2003).
In addition, we added three feature functions to
restrict reorderings and to represent globalized in-
sertion/deletion of words:
</bodyText>
<listItem confidence="0.961293">
• Lexicalized reordering feature function scores
whether a phrase translation pair is monotoni-
cally translated or not (Och et al., 2004):
</listItem>
<equation confidence="0.996852">
K
hlex(¯aK1 |f¯1K, ¯eK1) = log H pr(δk|¯f¯ak,¯ek) (3)
k=1
</equation>
<bodyText confidence="0.992738">
where δk = 1 iff ¯ak _ ¯ak_1 = 1 otherwise δk = 0.
</bodyText>
<listItem confidence="0.9949115">
• Deletion feature function penalizes words that
do not constitute a translation according to a
</listItem>
<equation confidence="0.934419">
γ = γ′
</equation>
<page confidence="0.997718">
123
</page>
<tableCaption confidence="0.999553">
Table 1: Number of word alignment by different preprocessings.
</tableCaption>
<table confidence="0.999013333333333">
de-en es-en fr-en en-de en-es en-fr
lower 17,660,187 17,221,890 16,176,075 17,596,764 17,237,723 16,220,520
stem 17,110,890 16,601,306 15,635,900 17,052,808 16,597,274 15,658,940
prefix4 16,975,398 16,540,767 15,610,319 16,936,710 16,530,810 15,613,755
intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242
union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570
</table>
<tableCaption confidence="0.978138">
Table 2: Number of phrases extracted from differently preprocessed corpora.
</tableCaption>
<bodyText confidence="0.343973">
de-en es-en fr-en en-de en-es en-fr
lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497
stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045
prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947
merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840
lexicon model t(f|e) (Bender et al., 2004):
</bodyText>
<equation confidence="0.979478333333333">
ZJ [max t(fj|ei) &lt; Tdel] (4)
0≤i≤I
j=1
</equation>
<bodyText confidence="0.999971777777778">
The deletion model simply counts the number
of words whose lexicon model probability is
lower than a threshold Tdel. Likewise, we also
added an insertion model hins(eI1, f1J) that pe-
nalizes the spuriously inserted English words
using a lexicon model t(e |f).
For the hierarchical phrase-based model, we em-
ployed the same feature set except for the distortion
model and the lexicalized reordering model.
</bodyText>
<sectionHeader confidence="0.9847335" genericHeader="method">
3 Phrase Extraction from Different Word
Alignment
</sectionHeader>
<bodyText confidence="0.999761764705883">
We prepared three kinds of corpora differentiated
by tokenization methods. First, the simplest pre-
processing is lower-casing (lower). Second, corpora
were transformed by a Porter’s algorithm based mul-
tilingual stemmer (stem) 1. Third, mixed-cased cor-
pora were truncated to the prefix of four letters of
each word (prefix4). For each differently tokenized
corpus, we computed word alignments by a HMM
translation model (Och and Ney, 2003) and by a
word alignment refinement heuristic of “grow-diag-
final” (Koehn et al., 2003). Different preprocessing
yields quite divergent alignment points as illustrated
in Table 1. The table also shows the numbers for
the intersection and union of three alignment anno-
tations.
The (hierarchical) phrase translation pairs are ex-
tracted from three distinctly word aligned corpora.
</bodyText>
<footnote confidence="0.950795">
1We used the Snowball stemmer from http://snowball.
tartarus.org
</footnote>
<bodyText confidence="0.999895708333333">
In this process, each word is recovered into its lower-
cased form. The associated counts are aggregated
to constitute relative count-based feature functions.
Table 2 summarizes the size of phrase tables in-
duced from the corpora. The number of rules ex-
tracted for the hierarchical phrase-based model was
roughly twice as large as those for the phrase-based
model. Fewer word alignments resulted in larger
phrase translation table size as observed in the “pre-
fix4” corpus. The size is further increased by our
aggregation step (merged).
Different induction/refinement algorithms or pre-
processings of a corpus bias word alignment. We
found that some word alignments were consistent
even with different preprocessings, though we could
not justify whether such alignments would match
against human intuition. If we could trust such
consistently aligned words, reliable (hierarchical)
phrase translation pairs would be extracted, which,
in turn, would result in better estimates for relative
count-based feature functions. At the same time, dif-
ferently biased word alignment annotations suggest
alternative phrase translation pairs that is useful for
increasing the coverage of translations.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999692142857143">
Table 3 shows the open test translation results on
2005 and 2006 test set (the development-test set and
the final test set) 2. We used the merged (hierar-
chical) phrase tables for decoding. Feature function
scaling factors were optimized on BLEU score us-
ing the supplied development set that is identical to
the 2005’s development set. We observed that our
</bodyText>
<footnote confidence="0.9812505">
2We did not differetiated in-domain or out-of-domain for
2006 test set.
</footnote>
<equation confidence="0.878942">
hdel(eI1,f1J) =
</equation>
<page confidence="0.998525">
124
</page>
<tableCaption confidence="0.999969">
Table 3: Open test on the 2005/2006 test sets (BLEU [%]).
</tableCaption>
<table confidence="0.997512333333333">
de-en es-en fr-en en-de en-es en-fr
test2005 Phrase 25.72 30.97 30.97 18.08 30.48 32.14
Rule 25.14 30.11 30.31 17.96 27.96 31.04
2005’s best 24.77 30.95 30.27
test2006 Phrase 23.16 29.90 27.89 15.79 29.54 29.19
Rule 22.74 28.80 27.28 15.99 26.56 27.86
</table>
<bodyText confidence="0.998595476190476">
results are very comparable to the last year’s best re-
sults in test2005. Also found that our hierarchical
phrase-based translation (Rule) performed slightly
inferior to the phrase-based translation (Phrase) in
both test sets. The hierarchically combined phrases
seem to be too flexible to represent the relationship
of similar language pairs. Note that our hierarchical
phrase-based model performed better in the English-
to-German translation task. Those language pair re-
quires rather distorted reordering, which could be
represented by hierarchically combined phrases.
We also conducted additional studies on how
differently aligned corpora might affect the trans-
lation quality on Spanish-to-English task for the
2005 test set. Using our phrase-based model,
the BLEU scores for lower/stem/prefix4 were
30.90/30.89/30.76, respectively. The differences of
translation qualities were statistically significant at
the 95% confidence level. Our phrase translation
pairs aggregated from all the differently prepro-
cessed corpora improved the translation quality.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999678125">
We presented two translation models, a phrase-
based model and a hierarchical phrase-based model.
The former performed as well as the last year’s best
system, whereas the latter performed comparable to
our phrase-based model. We are going to experi-
ment new feature functions to restrict the too flexible
reordering represented by our hierarchical phrase-
based model.
We also investigated different word alignment an-
notations, first using lower-cased corpus, second
performing stemming, and third retaining only 4-
letter prefix. Differently preprocessed corpora re-
sulted in quite divergent word alignment. Large
phrase/rule translation tables were accumulated
from three distinctly aligned corpora, which in turn,
increased the translation quality.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999689157894737">
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37–56.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system”. In Proc. of IWSLT 2004, pages 79–84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263–270, Ann Arbor, Michigan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
ofNAACL 2003, pages 48–54, Edmonton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ofACL 2002, pages
295–302.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417–449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex a
nd Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings, pages
161–168, Boston, Massachusetts, USA, May 2 - May
7.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ofACL 2003,
pages 160–167.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING-ACL
2006 (to appear), Sydney, Australia, July.
</reference>
<page confidence="0.998496">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881243">
<title confidence="0.996643">NTT System Description for the WMT2006 Shared Task</title>
<author confidence="0.994442">Taro Watanabe Hajime Tsukada Hideki</author>
<affiliation confidence="0.999398">NTT Communication Science</affiliation>
<address confidence="0.950966">2-4 Hikaridai, Seika-cho, Kyoto, Japan</address>
<abstract confidence="0.998820733333333">We present two translation systems experimented for the shared-task of “Workshop on Statistical Machine Translation,” a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronous- CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report extraction technique entiating tokenization of corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="3389" citStr="Aho and Ullman, 1969" startWordPosition="504" endWordPosition="507">s K phrase translation pairs (¯e1, ¯f¯a1), (¯e2, ¯f¯a2), ...: The input foreign sentence is segmented into phrases fK 1, 122 Proceedings of the Workshop on Statistical Machine Translation, pages 122–125, New York City, June 2006. c�2006 Association for Computational Linguistics mapped into corresponding English¯eK1 , then, reordered to form the output English sentence according to a phrase alignment index mapping ¯a. In a hierarchical phrase-based translation (Chiang, 2005), translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969): X —� (γ,α,—) where X is a non-terminal, γ and α are strings of terminals and non-terminals. — is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with —. 2.1 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax directed translations and the pushdown assembler. J. Comput. Syst. Sci., 3(1):37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Richard Zens</author>
<author>Evgeny Matusov</author>
<author>Hermann Ney</author>
</authors>
<title>Alignment templates: the RWTH SMT system”.</title>
<date>2004</date>
<booktitle>In Proc. of IWSLT</booktitle>
<pages>79--84</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="7523" citStr="Bender et al., 2004" startWordPosition="1146" endWordPosition="1149">6,530,810 15,613,755 intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242 union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570 Table 2: Number of phrases extracted from differently preprocessed corpora. de-en es-en fr-en en-de en-es en-fr lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497 stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045 prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947 merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840 lexicon model t(f|e) (Bender et al., 2004): ZJ [max t(fj|ei) &lt; Tdel] (4) 0≤i≤I j=1 The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold Tdel. Likewise, we also added an insertion model hins(eI1, f1J) that penalizes the spuriously inserted English words using a lexicon model t(e |f). For the hierarchical phrase-based model, we employed the same feature set except for the distortion model and the lexicalized reordering model. 3 Phrase Extraction from Different Word Alignment We prepared three kinds of corpora differentiated by tokenization methods. First, the simplest preprocessi</context>
</contexts>
<marker>Bender, Zens, Matusov, Ney, 2004</marker>
<rawString>Oliver Bender, Richard Zens, Evgeny Matusov, and Hermann Ney. 2004. Alignment templates: the RWTH SMT system”. In Proc. of IWSLT 2004, pages 79–84, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1165" citStr="Chiang, 2005" startWordPosition="158" endWordPosition="159">hically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora. 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005). Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated into one large phrase/rule translation table. The experiments and the final translation results are presented in Section 4. 2 Translation Models We u</context>
<context position="3246" citStr="Chiang, 2005" startWordPosition="485" endWordPosition="487">o of how a translation is constituted. In a phrase-based statistical translation (Koehn et al., 2003), a bilingual text is decomposed as K phrase translation pairs (¯e1, ¯f¯a1), (¯e2, ¯f¯a2), ...: The input foreign sentence is segmented into phrases fK 1, 122 Proceedings of the Workshop on Statistical Machine Translation, pages 122–125, New York City, June 2006. c�2006 Association for Computational Linguistics mapped into corresponding English¯eK1 , then, reordered to form the output English sentence according to a phrase alignment index mapping ¯a. In a hierarchical phrase-based translation (Chiang, 2005), translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969): X —� (γ,α,—) where X is a non-terminal, γ and α are strings of terminals and non-terminals. — is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with —. 2.1 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-t</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL 2005, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. ofNAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1027" citStr="Koehn et al., 2003" startWordPosition="135" endWordPosition="138">sed model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronousCFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora. 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005). Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated into one la</context>
<context position="2734" citStr="Koehn et al., 2003" startWordPosition="406" endWordPosition="409">amework, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions hm(eI1, f1J), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J. Feature function scaling factors λm are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. In a phrase-based statistical translation (Koehn et al., 2003), a bilingual text is decomposed as K phrase translation pairs (¯e1, ¯f¯a1), (¯e2, ¯f¯a2), ...: The input foreign sentence is segmented into phrases fK 1, 122 Proceedings of the Workshop on Statistical Machine Translation, pages 122–125, New York City, June 2006. c�2006 Association for Computational Linguistics mapped into corresponding English¯eK1 , then, reordered to form the output English sentence according to a phrase alignment index mapping ¯a. In a hierarchical phrase-based translation (Chiang, 2005), translation is modeled after a weighted synchronous-CFG consisting of production rules</context>
<context position="4097" citStr="Koehn et al., 2003" startWordPosition="623" endWordPosition="626">als. — is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with —. 2.1 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively extracts phrase pairs ( f j+m j , ei+n i ) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): 1. A phrase pair (f¯, ¯e) constitutes a rule: )X � (f¯ , e¯ 2. A rule X —� (γ, α) and a phrase pair (f¯, ¯e) s.t. f¯γ′′ and α = α′¯eα′′ constitutes a rule: (γ′ Xk γ′′, α′ Xk α′′) X � 2.2 Decoding The decoder for the phrase-based model is a left-toright generation decoder with a beam search strategy s</context>
<context position="5748" citStr="Koehn et al., 2003" startWordPosition="904" endWordPosition="907">unctions described in Section 2.3. The English side of the phrase is simply concatenated to form a new prefix of English sentence. In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.3 Feature Functions Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003): • Relative-count based phrase translation probabilities in both directions. • Lexically weighted feature functions in both directions. • The supplied trigram language model. • Distortion model that counts the number of words skipped. • The number of words in English-side and the number of phrases that constitute translation. For details, please refer to Koehn et al. (2003). In addition, we added three feature functions to restrict reorderings and to represent globalized insertion/deletion of words: • Lexicalized reordering feature function scores whether a phrase translation pair is monotoni</context>
<context position="8540" citStr="Koehn et al., 2003" startWordPosition="1305" endWordPosition="1308">n model and the lexicalized reordering model. 3 Phrase Extraction from Different Word Alignment We prepared three kinds of corpora differentiated by tokenization methods. First, the simplest preprocessing is lower-casing (lower). Second, corpora were transformed by a Porter’s algorithm based multilingual stemmer (stem) 1. Third, mixed-cased corpora were truncated to the prefix of four letters of each word (prefix4). For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of “grow-diagfinal” (Koehn et al., 2003). Different preprocessing yields quite divergent alignment points as illustrated in Table 1. The table also shows the numbers for the intersection and union of three alignment annotations. The (hierarchical) phrase translation pairs are extracted from three distinctly word aligned corpora. 1We used the Snowball stemmer from http://snowball. tartarus.org In this process, each word is recovered into its lowercased form. The associated counts are aggregated to constitute relative count-based feature functions. Table 2 summarizes the size of phrase tables induced from the corpora. The number of ru</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. ofNAACL 2003, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>295--302</pages>
<contexts>
<context position="1810" citStr="Och and Ney, 2002" startWordPosition="251" endWordPosition="254"> two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated into one large phrase/rule translation table. The experiments and the final translation results are presented in Section 4. 2 Translation Models We used a log-linear approach (Och and Ney, 2002) in which a foreign language sentence f1J = f1, f2, ...fJ is translated into another language, i.e. English, eI1 = e1, e2,..., eI by seeking a maximum likelihood solution of ˆeI 1= argmax Pr(eI1 |f1J) (1) eI 1 = argmax (EM ) eI exp m=1 λmhm(eI 1, f 1 J) 1 (2) M Ee′i′ exp (Em=1 λmhm(e′i , f1J)) In this framework, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions hm(eI1, f1J), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J. Feature function scaling factors</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ofACL 2002, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3909" citStr="Och and Ney, 2003" startWordPosition="592" endWordPosition="595">chronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969): X —� (γ,α,—) where X is a non-terminal, γ and α are strings of terminals and non-terminals. — is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with —. 2.1 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively extracts phrase pairs ( f j+m j , ei+n i ) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): 1. A phrase pair (f¯, ¯e) constitutes a rule: )X � (f¯ , e¯ 2. A rule X —� (γ, α) and a phrase pair (f¯, ¯e) s.t.</context>
<context position="8454" citStr="Och and Ney, 2003" startWordPosition="1291" endWordPosition="1294">rchical phrase-based model, we employed the same feature set except for the distortion model and the lexicalized reordering model. 3 Phrase Extraction from Different Word Alignment We prepared three kinds of corpora differentiated by tokenization methods. First, the simplest preprocessing is lower-casing (lower). Second, corpora were transformed by a Porter’s algorithm based multilingual stemmer (stem) 1. Third, mixed-cased corpora were truncated to the prefix of four letters of each word (prefix4). For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of “grow-diagfinal” (Koehn et al., 2003). Different preprocessing yields quite divergent alignment points as illustrated in Table 1. The table also shows the numbers for the intersection and union of three alignment annotations. The (hierarchical) phrase translation pairs are extracted from three distinctly word aligned corpora. 1We used the Snowball stemmer from http://snowball. tartarus.org In this process, each word is recovered into its lowercased form. The associated counts are aggregated to constitute relative count-based feature functions. T</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Comput. Linguist.,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3999" citStr="Och and Ney, 2004" startWordPosition="608" endWordPosition="611">n, 1969): X —� (γ,α,—) where X is a non-terminal, γ and α are strings of terminals and non-terminals. — is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with —. 2.1 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively extracts phrase pairs ( f j+m j , ei+n i ) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): 1. A phrase pair (f¯, ¯e) constitutes a rule: )X � (f¯ , e¯ 2. A rule X —� (γ, α) and a phrase pair (f¯, ¯e) s.t. f¯γ′′ and α = α′¯eα′′ constitutes a rule: (γ′ Xk γ′′, α′ Xk α′′) X � 2.2 Decoding The dec</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Comput. Linguist., 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex a nd Kumar,</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>161--168</pages>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex a nd Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In HLT-NAACL 2004: Main Proceedings, pages 161–168, Boston, Massachusetts, USA, May 2 - May 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2541" citStr="Och, 2003" startWordPosition="380" endWordPosition="381">,..., eI by seeking a maximum likelihood solution of ˆeI 1= argmax Pr(eI1 |f1J) (1) eI 1 = argmax (EM ) eI exp m=1 λmhm(eI 1, f 1 J) 1 (2) M Ee′i′ exp (Em=1 λmhm(e′i , f1J)) In this framework, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions hm(eI1, f1J), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J. Feature function scaling factors λm are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. In a phrase-based statistical translation (Koehn et al., 2003), a bilingual text is decomposed as K phrase translation pairs (¯e1, ¯f¯a1), (¯e2, ¯f¯a2), ...: The input foreign sentence is segmented into phrases fK 1, 122 Proceedings of the Workshop on Statistical Machine Translation, pages 122–125, New York City, June 2006. c�2006 Association for Computational Linguistics mapped into corresponding English¯eK1 , then, reordered to form the output English sentence ac</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ofACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<location>Sydney, Australia,</location>
<note>(to appear),</note>
<contexts>
<context position="5506" citStr="Watanabe et al., 2006" startWordPosition="866" endWordPosition="869">thesis. From an existing hypothesis, new hypothesis is generated by consuming a phrase translation pair that covers untranslated foreign word positions. The score for the newly generated hypothesis is updated by combining the scores of feature functions described in Section 2.3. The English side of the phrase is simply concatenated to form a new prefix of English sentence. In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.3 Feature Functions Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003): • Relative-count based phrase translation probabilities in both directions. • Lexically weighted feature functions in both directions. • The supplied trigram language model. • Distortion model that counts the number of words skipped. • The number of words in English-side and the number of phrases that constitute translation. For details, please refer to </context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proc. of COLING-ACL 2006 (to appear), Sydney, Australia, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>