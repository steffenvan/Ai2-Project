<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.9958325">
Learning Hybrid Representations to Retrieve
Semantically Equivalent Questions
</title>
<address confidence="0.294615">
Cicero dos Santos1, Luciano Barbosa1, Dasha Bogdanova2, Bianca Zadrozny1
1IBM Research, 138/146 Av. Pasteur, Rio de Janeiro, Brazil
</address>
<email confidence="0.815945">
{cicerons,lucianoa,biancaz}@br.ibm.com
</email>
<address confidence="0.453474">
2ADAPT centre, School of Computing, Dublin City University, Dublin, Ireland
</address>
<email confidence="0.990292">
dbogdanova@computing.dcu.ie
</email>
<sectionHeader confidence="0.99729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993535">
Retrieving similar questions in online
Q&amp;A community sites is a difficult task
because different users may formulate the
same question in a variety of ways, us-
ing different vocabulary and structure.
In this work, we propose a new neural
network architecture to perform the task
of semantically equivalent question re-
trieval. The proposed architecture, which
we call BOW-CNN, combines a bag-of-
words (BOW) representation with a dis-
tributed vector representation created by a
convolutional neural network (CNN). We
perform experiments using data collected
from two Stack Exchange communities.
Our experimental results evidence that: (1)
BOW-CNN is more effective than BOW
based information retrieval methods such
as TFIDF; (2) BOW-CNN is more robust
than the pure CNN for long texts.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999520344827586">
Most Question-answering (Q&amp;A) community
sites advise users before posting a new question
to search for similar questions. This is not always
an easy task because different users may formulate
the same question in a variety of ways.
We define two questions as semantically equiv-
alent if they can be adequately answered by the
exact same answer. Here is an example of a pair
of such questions from Ask Ubuntu community,
which is part of the Stack Exchange Q&amp;A com-
munity site: (q1)“I have downloaded ISO files re-
cently. How do I burn it to a CD or DVD or mount
it?” and (q2)“I need to copy the iso file for Ubuntu
12.04 to a CD-R in Win8. How do I do so?”.
Retrieving semantically equivalent questions is a
challenging task due to two main factors: (1) the
same question can be rephrased in many different
ways; and (2) two questions may be different but
may refer implicitly to a common problem with
the same answer. Therefore, traditional similarity
measures based on word overlap such as shingling
and Jaccard coefficient (Broder, 1997) and its vari-
ations (Wu et al., 2011) are not able to capture
many cases of semantic equivalence. To capture
the semantic relationship between pair of ques-
tions, different strategies have been used such as
machine translation (Jeon et al., 2005; Xue et al.,
2008), knowledge graphs (Zhou et al., 2013) and
topic modelling (Cai et al., 2011; Ji et al., 2012).
Recent papers (Kim, 2014; Hu et al., 2014; Yih
et al., 2014; dos Santos and Gatti, 2014; Shen et
al., 2014) have shown the effectiveness of convo-
lutional neural networks (CNN) for sentence-level
analysis of short texts in a variety of different nat-
ural language processing and information retrieval
tasks. This motivated us to investigate CNNs for
the task of semantically equivalent question re-
trieval. However, given the fact that the size of a
question in an online community may vary from a
single sentence to a detailed problem description
with several sentences, it was not clear that the
CNN representation would be the most adequate.
In this paper, we propose a hybrid neural net-
work architecture, which we call BOW-CNN. It
combines a traditional bag-of-words (BOW) rep-
resentation with a distributed vector representa-
tion created by a CNN, to retrieve semantically
equivalent questions. Using a ranking loss func-
tion in the training, BOW-CNN learns to represent
questions while learning to rank them according to
their semantic similarity. We evaluate BOW-CNN
over two different Q&amp;A communities in the Stack
Exchange site, comparing it against CNN and 6
well-established information retrieval algorithms
based on BOW. The results show that our proposed
solution outperforms BOW-based information re-
trieval methods such as the term frequency - in-
verse document frequency (TFIDF) in all evalu-
</bodyText>
<page confidence="0.923496">
694
</page>
<figureCaption confidence="0.679044">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 694–699,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Figure 1: Representing and scoring questions with
weighted bag-of-words.
</figureCaption>
<bodyText confidence="0.999184666666667">
ated scenarios. Moreover, we were able to show
that for short texts (title of the questions), an ap-
proach using only CNN obtains the best results,
whereas for long texts (title and body of the ques-
tions), our hybrid approach (BOW-CNN) is more
effective.
</bodyText>
<sectionHeader confidence="0.992872" genericHeader="introduction">
2 BOW-CNN
</sectionHeader>
<subsectionHeader confidence="0.997023">
2.1 Feed Forward Processing
</subsectionHeader>
<bodyText confidence="0.9887455">
The goal of the feed forward processing is to cal-
culate the similarity between a pair of questions
(q1, q2). To perform this task, each question
q follows two parallel paths (BOW and CNN),
each one producing a distinct vector representa-
tions of q. The BOW path produces a weighted
bag-of-words representation of the question, rbow
q ,
where the weight of each word in the vocabu-
lary V is learned by the neural network. The
CNN path, uses a convolutional approach to con-
struct a distributed vector representations, rconv
q ,
of the question. After producing the BOW and
CNN representations for the two input questions,
the BOW-CNN computes two partial similarity
scores sbow(q1, q2), for the CNN representations,
and sconv(q1, q2), for the BOW representations.
Finally, it combines the two partial scores to create
the final score s(q1, q2).
</bodyText>
<subsectionHeader confidence="0.998913">
2.2 BOW Path
</subsectionHeader>
<bodyText confidence="0.999693666666667">
The generation of the bag-of-words representation
for a given question q is quite straightforward. As
detailed in Figure 1, we first create a sparse vec-
tor qbow E R|V  |that contains the frequency in q of
each word of the vocabulary. Next, we compute
the weighted bag-of-words representation by per-
</bodyText>
<figureCaption confidence="0.9828515">
Figure 2: Representing and scoring questions with
a convolutional approach.
</figureCaption>
<bodyText confidence="0.959448">
forming the element-wise vector multiplication:
</bodyText>
<equation confidence="0.5876895">
rbow = qbow ∗ t (1)
q
</equation>
<bodyText confidence="0.999989833333333">
where the vector t E R|V |, contains a weight for
each word in the vocabulary V . The vector t is a
parameter to be learned by the network. This is
closely related to the TFIDF text representation.
In fact, if we fix t to the vector of IDFs, this corre-
sponds to the exact TFIDF representation.
</bodyText>
<subsectionHeader confidence="0.997818">
2.3 CNN Path
</subsectionHeader>
<bodyText confidence="0.997870615384616">
As detailed in Figure 2, the first layer of the
CNN path transforms words into representations
that capture syntactic and semantic information
about the words. Given a question consisting of
N words q = {w1, ..., wN}, every word wn is
converted into a real-valued vector rwn. There-
fore, for each question, the input to the next NN
layer is a sequence of real-valued vectors qemb =
{rw1, ..., rwN }. Word representations are encoded
by column vectors in an embedding matrix W0 E
Rd×|V |, where V is a fixed-sized vocabulary.
The next step in the CNN path consists in cre-
ating distributed vector representations rconv
</bodyText>
<equation confidence="0.21874275">
q1 and
rconv
q2 from the word embedding sequencies qemb
1
</equation>
<bodyText confidence="0.929912875">
and qemb
2 .We perform this by using a convolu-
tional layer in the same way as used in (dos Santos
and Gatti, 2014) to create sentence-level represen-
tations.
More specifically, given a question q1, the con-
volutional layer applies a matrix-vector operation
to each window of size k of successive windows
</bodyText>
<page confidence="0.962996">
695
</page>
<bodyText confidence="0.576806">
in qemb
</bodyText>
<equation confidence="0.95545375">
1 = {rw1, ..., rwN }. Let us define the vector
zn E Rdk as the concatenation of a sequence of k
word embeddings, centralized in the n-th word:
zn = (rwn−(k−1)/2, ..., rwn+(k−1)/2)T
</equation>
<bodyText confidence="0.9972505">
The convolutional layer computes the j-th ele-
ment of the vector rconv
</bodyText>
<equation confidence="0.943056">
q1 E Rclu as follows:
[rco q nv]j = f(1&lt;n&lt;N max [W1 zn + b1 ] I (2)
</equation>
<bodyText confidence="0.999947">
where W1 E Rcluxdk is the weight matrix of the
convolutional layer and f is the hyperbolic tangent
function. Matrices W0 and W1, and the vector b1
are parameters to be learned. The word embedding
size d, the number of convolutional units clu, and
the size of the word context window k are hyper-
parameters to be chosen by the user.
</bodyText>
<subsectionHeader confidence="0.998078">
2.4 Question Pair Scoring
</subsectionHeader>
<bodyText confidence="0.999970346153846">
After the bag-of-words and convolutional-based
representations are generated for the input pair (q1,
q2), the partial scores are computed as the cosine
similarity between the respective vectors:
larger range. This helps to penalize more on the
prediction errors. Following (Yih et al., 2011), in
our experiments we set γ to 10.
Sampling informative negative examples can
have a significant impact in the effectiveness of the
learned model. In our experiments, before train-
ing, we create 20 pairs of negative examples for
each positive pair (q1,q2)+. To create a negative
example we (1) randomly sample a question qx
that is not semantically equivalent to q1 or q2; (2)
then create negative pairs (q1,qx)− and (q2,qx)−.
During training, at each iteration we only use the
negative example x that produces the smallest dif-
ferent sθ(q1, q2)+ − sθ(q1, qx)−. Using this strat-
egy, we select more representative negative exam-
ples.
We use stochastic gradient descent (SGD) to
minimize the loss function with respect to θ.
The backpropagation algorithm is used to com-
pute the gradients of the network. In our exper-
iments, BOW-CNN architecture is implemented
using Theano (Bergstra et al., 2010).
</bodyText>
<sectionHeader confidence="0.997922" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<equation confidence="0.978964">
rbow q1 w
I I rbo
�
rconv rconv
q1 q2
�rconv
q1 ��rconv
q2 �
</equation>
<bodyText confidence="0.999798">
The final score for the input questions (q1, q2) is
given by the following linear combination
s(q1, q2) = β1 * sbow(q1, q2) + β2 * sconv(q1, q2)
where β1 and β2 are parameters to be learned.
</bodyText>
<subsectionHeader confidence="0.983882">
2.5 Training Procedure
</subsectionHeader>
<bodyText confidence="0.994778210526316">
Our network is trained by minimizing a ranking
loss function over the training set D. The input in
each round is two pairs of questions (q1, q2)+ and
(q1, qx)− where the questions in the first pair are
semantically equivalent (positive example), and
the ones in the second pair are not (negative ex-
ample). Let A be the difference of their similarity
scores, A = sθ(q1, q2) − sθ(q1, qx), generated by
the network with parameter set θ. As in (Yih et al.,
2011), we use a logistic loss over A
L(A, θ) = log(1 + exp(−γA))
where γ is a scaling factor that magnifies A from
[-2,2] (in the case of using cosine similarity) to a
A well-structured source of semantically equiv-
alent questions is the Stack Exchange site. It
is composed by multiple Q&amp;A communities,
whereby users can ask and answer questions, and
vote up and down both questions and answers.
Questions are composed by a title and a body.
Moderators can mark questions as duplicates, and
eventually a question can have multiple duplicates.
For this evaluation, we chose two highly-
accessed Q&amp;A communities: Ask Ubuntu and En-
glish. They differ in terms of content and size.
Whereas Ask Ubuntu has 29510 duplicated ques-
tions, English has 6621. We performed exper-
iments using only the title of the questions as
well as title + body, which we call all for the
rest of this section. The average size of a title
is very small (about 10 words), which is at least
10 times smaller than the average size of all for
both datasets. The data was tokenized using the
tokenizer available with the Stanford POS Tag-
ger (Toutanova et al., 2003), and all links were re-
placed by a unique string. For Ask Ubuntu, we
did not consider the content inside the tag code,
which contains some specific Linux commands or
programming code.
</bodyText>
<equation confidence="0.89808075">
For each community, we created training, vali-
sbow(q1, q2) =
sconv(q1,q2) =
rbow q1 .rbow q2 3.1 Data
</equation>
<page confidence="0.990571">
696
</page>
<table confidence="0.99743">
Community Training Validation Test
Ask Ubuntu 9802 1991 3800
English 2235 428 816
</table>
<tableCaption confidence="0.9217095">
Table 1: Partition of training, validation and test
sets for the experiments.
</tableCaption>
<bodyText confidence="0.994320833333333">
dation and test sets. In Table 1, we inform the size
of each set. The number of instances in the train-
ing set corresponds to the number of positive pairs
of semantically equivalent questions. The number
of instances in the validation and the test sets cor-
respond to the number of questions which are used
as queries. All questions in the validation and test
set contain at least one duplicated question in the
set of all questions. In our experiments, given a
query question q, all questions in the Q&amp;A com-
munity are evaluated when searching for a dupli-
cate of q.
</bodyText>
<subsectionHeader confidence="0.998792">
3.2 Baselines and Neural Network Setup
</subsectionHeader>
<bodyText confidence="0.995331846153846">
In order to verify the impact of jointly using
BOW and CNN representations, we perform ex-
periments with two NN architectures: the BOW-
CNN and the CNN alone, which consists in us-
ing only the CNN path of BOW-CNN and, con-
sequently, computing the score for a pair of ques-
tions using s(q1, q2) = sconv(q1, q2).
Additionally, we compare BOW-CNN with six
well-established IR algorithms available on the
Lucene package (Hatcher et al., 2004). Here we
provide a brief overview of them. For further de-
tails, we refer the reader to the citation associated
with the algorithm.
</bodyText>
<listItem confidence="0.9451619375">
• TFIDF (Manning et al., 2008) uses the tradi-
tional Vector Space Model to represent docu-
ments as vectors in a high-dimensional space.
Each position in the vector represents a word
and the weight of words are calculated using
TFIDF.
• BM25 (Robertson and Walker, 1994) is a
probabilistic weighting method that takes
into consideration term frequency, inverse
document frequency and document length.
Its has two free parameters: k1 to tune term-
frequency saturation; and b to calibrate the
document-length normalization.
• IB (Clinchant and Gaussier, 2010) uses
information-based models to capture the im-
portance of a term by measuring how much
</listItem>
<table confidence="0.9976272">
Param. Name BOW-CNN CNN
Word Emb. Size 200 200
Context Winow Size 3 3
Convol. Units 400 1000
Learning Rate 0.01 0.05
</table>
<tableCaption confidence="0.991217">
Table 2: Neural Network Hyper-Parameters
</tableCaption>
<bodyText confidence="0.9364575">
its behavior in a document deviates from its
behavior in the whole collection.
</bodyText>
<listItem confidence="0.958647818181818">
• DFR (Amati and Van Rijsbergen, 2002) is
based on divergence from randomness frame-
work. The relevance of a term is measured by
the divergence between its actual distribution
and the distribution from a random process.
• LMDirichlet and LMJelinekMercer apply
probabilistic language model approaches for
retrieval (Zhai and Lafferty, 2004). They dif-
fer in the smoothing method: LMDirichlet
uses Dirichlet priors and LMJelinekMercer
uses the Jelinek-Mercer method.
</listItem>
<bodyText confidence="0.999992764705882">
The word embeddings used in our experiments
are initialized by means of unsupervised pre-
training. We perform pre-training using the skip-
gram NN architecture (Mikolov et al., 2013) avail-
able in the word2vec tool. We use the En-
glish Wikipedia to train word embeddings for
experiments with the English dataset. For the
AskUbuntu dataset, we use all available Ask-
Ubuntu community data to train word embed-
dings.
The hyper-parameters of the neural networks
and the baselines are tuned using the development
sets. In Table 2, we show the selected hyper-
parameter values. In our experiments, we initialize
each element [t]i of the bag-of-word weight vector
t with the IDF of i−th word wi computed over the
respective set of questions Q as follows
</bodyText>
<equation confidence="0.981301">
[t]i = IDF(wi, Q) = log |Q|
|q E Q : wi E q|
</equation>
<sectionHeader confidence="0.998931" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.996181625">
Comparison with Baselines. In Tables 3 and
4, we present the question retrieval performance
(Accuracy@k) of different algorithms over the
AskUbuntu and English datasets for the title and
all settings, respectively. For both datasets, BOW-
CNN outperforms the six IR algorithms for both
title and all settings. For the AskUbuntu all,
BOW-CNN is four absolute points larger than the
</bodyText>
<page confidence="0.99475">
697
</page>
<table confidence="0.9998688">
AskUbuntu English
Algorithm @1 @5 @10 @1 @5 @10
TFIDF 8.3 17.5 22.5 10.0 18.1 21.6
BM25 7.3 17.1 21.8 10.0 18.9 23.2
IB 8.1 18.1 22.6 10.1 18.4 22.7
DFR 7.7 17.8 22.4 10.5 19.0 23.0
LMD 5.6 14.1 19.0 10.9 20.1 24.2
LMJ 8.3 17.5 22.5 10.3 18.5 22.1
CNN 11.5 24.8 31.4 11.6 23.0 26.9
BOW-CNN 10.9 22.6 28.7 11.3 21.4 26.0
</table>
<tableCaption confidence="0.9935275">
Table 3: Question title retrieval performance (Ac-
curacy@k) for different algorithms.
</tableCaption>
<table confidence="0.9999199">
AskUbuntu English
Algorithm @1 @5 @10 @1 @5 @10
TFIDF 16.9 31.3 38.3 25.9 42.0 48.1
BM25 18.2 33.1 39.8 29.4 45.7 52.5
IB 14.9 28.2 34.8 25.4 42.3 48.0
DFR 18.0 32.6 39.2 28.6 45.4 52.5
LMD 13.7 26.8 34.4 23.0 40.2 46.0
LMJ 18.3 33.4 40.7 28.5 45.7 52.3
CNN 20.0 33.8 40.1 17.2 29.6 33.8
BOW-CNN 22.3 39.7 46.4 30.8 47.7 54.9
</table>
<tableCaption confidence="0.971233">
Table 4: Question title + body (all) retrieval per-
formance for different algorithms.
</tableCaption>
<bodyText confidence="0.998112238095238">
best IR baseline (LMJ) in terms of Accuracy@1,
which represents an improvement of 21.9%. Since
the BOW representation we use is closely related
to TFIDF, an important comparison is the perfor-
mance of BOW-CNN vs. TFIDF. In Tables 3 and
4, we can see that BOW-CNN consistently outper-
forms the TFIDF model in the two datasets for
both cases title and all. These findings suggest
that BOW-CNN is indeed combining the strong
semantic representation power conveyed by the
convolutional-based representation to, jointly with
the BOW representation, construct a more effec-
tive model.
Another interesting finding is that CNN out-
performs BOW-CNN for short texts (Table 3)
and, conversely, BOW-CNN outperforms CNN for
long texts (Table 4). This demonstrates that, when
dealing with large input texts, BOW-CNN is an
effective approach to combine the strengths of
convolutional-based representation and BOW.
Impact of Initialization of BOW Weights. In
the BOW-CNN experiments whose results are pre-
sented in tables 3 and 4 we initialize the elements
of the BOW weight vector t with the IDF of each
word in V computed over the question set Q. In
this section we show some experimental results
that indicate the contribution of this initialization.
In Table 5, we present the performance of
BOW-CNN for the English dataset when differ-
ent configurations of the BOW weight vector t are
used. The first column of Table 5 indicates the
type of initialization, where ones means that t is
initialized with the value 1 (one) in all positions.
The second column informs whether t is allowed
to be updated (Yes) by the network or not (No).
The numbers suggest that letting BOW weights
free to be updated by the network produces better
results than fixing them to IDF values. In addition,
using IDF to initialize the BOW weight vector is
better than using the same weight (ones) to initial-
ize it. This is expected, since we are injecting a
prior knowledge known to be helpful in IR tasks.
</bodyText>
<table confidence="0.9994072">
Title All
t initial t updated @1 @10 @1 @10
IDF Yes 11.3 26.0 30.8 54.9
IDF No 10.6 25.3 29.7 54.9
Ones Yes 10.7 24.2 26.3 51.2
</table>
<tableCaption confidence="0.955881">
Table 5: BOW-CNN performance using different
methods to initialize the BOW weight vector t.
</tableCaption>
<sectionHeader confidence="0.999256" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999059">
In this paper, we propose a hybrid neural network
architecture, BOW-CNN, that combines bag-of-
words with distributed vector representations cre-
ated by a CNN, to retrieve semantically equivalent
questions. Our experimental evaluation showed
that: our approach outperforms traditional bow ap-
proaches; for short texts, a pure CNN obtains the
best results, whereas for long texts, BOW-CNN is
more effective; and initializing the BOW weight
vector with IDF values is beneficial.
</bodyText>
<sectionHeader confidence="0.998142" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998638666666667">
Dasha Bogdanova’s contributions were made
during an internship at IBM Research. Her
work was partially supported by Science Foun-
dation Ireland through the CNGL Programme
(Grant 12/CE/I2267) in the ADAPT Centre
(www.adaptcentre.ie) at DCU.
</bodyText>
<sectionHeader confidence="0.99927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995377142857143">
Gianni Amati and Cornelis Joost Van Rijsbergen.
2002. Probabilistic models of information retrieval
based on measuring the divergence from random-
ness. ACM Transactions on Information Systems
(TOIS), 20(4):357–389.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
</reference>
<page confidence="0.991355">
698
</page>
<reference confidence="0.999570212962963">
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and GPU
math expression compiler. In Proceedings of the
Python for Scientific Computing Conference.
A. Broder. 1997. On the resemblance and containment
of documents. In Proceedings of the Compression
and Complexity of Sequences 1997, SEQUENCES
’97, pages 21–, Washington, DC, USA. IEEE Com-
puter Society.
Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.
2011. Learning the latent topics for question re-
trieval in community qa. In IJCNLP, volume 11,
pages 273–281.
St´ephane Clinchant and Eric Gaussier. 2010.
Information-based models for ad hoc ir. In Proceed-
ings of the 33rd international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 234–241. ACM.
C´ıcero Nogueira dos Santos and Ma´ıra Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics (COLING), Dublin, Ireland.
Erik Hatcher, Otis Gospodnetic, and Michael McCan-
dless. 2004. Lucene in action.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Advances in Neural Information Processing Systems
27: Annual Conference on Neural Information Pro-
cessing Systems 2014, December 8-13 2014, Mon-
treal, Quebec, Canada, pages 2042–2050.
Jiwoon Jeon, W Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowl-
edge management, pages 84–90.
Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012.
Question-answer topic model for question retrieval
in community question answering. In Proceedings
of the 21st ACM international conference on Infor-
mation and knowledge management, pages 2471–
2474.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods for Natural Lan-
guage Processing, pages 1746–1751.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In In Proceedings of Work-
shop at ICLR.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Pro-
ceedings of the 17th annual international confer-
ence on Research and development in information
retrieval, pages 232–241.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr´egoire Mesnil. 2014. A latent semantic
model with convolutional-pooling structure for in-
formation retrieval. In Proceedings of the 23rd ACM
International Conference on Conference on Infor-
mation and Knowledge Management, pages 101–
110.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 173–180.
Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Ef-
ficient near-duplicate detection for q&amp;a forum. In
Proceedings of 5th International Joint Conference
on Natural Language Processing, pages 1001–1009,
Chiang Mai, Thailand, November. Asian Federation
of Natural Language Processing.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, SIGIR ’08, pages
475–482.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discrimina-
tive projections for text similarity measures. In
Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, CoNLL’11,
pages 247–256.
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation ques-
tion answering. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 643–648.
Association for Computational Linguistics.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS), 22(2):179–214.
Guangyou Zhou, Yang Liu, Fang Liu, Daojian Zeng,
and Jun Zhao. 2013. Improving question retrieval in
community question answering using world knowl-
edge. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2239–2245.
</reference>
<page confidence="0.998835">
699
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530444">
<title confidence="0.997206">Learning Hybrid Representations to Semantically Equivalent Questions</title>
<author confidence="0.997123">dos Luciano Dasha Bianca</author>
<affiliation confidence="0.7242165">Research, 138/146 Av. Pasteur, Rio de Janeiro, centre, School of Computing, Dublin City University, Dublin,</affiliation>
<email confidence="0.993277">dbogdanova@computing.dcu.ie</email>
<abstract confidence="0.999716428571429">Retrieving similar questions in online Q&amp;A community sites is a difficult task because different users may formulate the same question in a variety of ways, using different vocabulary and structure. In this work, we propose a new neural network architecture to perform the task of semantically equivalent question retrieval. The proposed architecture, which we call BOW-CNN, combines a bag-ofwords (BOW) representation with a distributed vector representation created by a convolutional neural network (CNN). We perform experiments using data collected from two Stack Exchange communities. Our experimental results evidence that: (1) BOW-CNN is more effective than BOW based information retrieval methods such as TFIDF; (2) BOW-CNN is more robust than the pure CNN for long texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gianni Amati</author>
<author>Cornelis Joost Van Rijsbergen</author>
</authors>
<title>Probabilistic models of information retrieval based on measuring the divergence from randomness.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Amati, Van Rijsbergen, 2002</marker>
<rawString>Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS), 20(4):357–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference.</booktitle>
<contexts>
<context position="9048" citStr="Bergstra et al., 2010" startWordPosition="1485" endWordPosition="1488">negative example we (1) randomly sample a question qx that is not semantically equivalent to q1 or q2; (2) then create negative pairs (q1,qx)− and (q2,qx)−. During training, at each iteration we only use the negative example x that produces the smallest different sθ(q1, q2)+ − sθ(q1, qx)−. Using this strategy, we select more representative negative examples. We use stochastic gradient descent (SGD) to minimize the loss function with respect to θ. The backpropagation algorithm is used to compute the gradients of the network. In our experiments, BOW-CNN architecture is implemented using Theano (Bergstra et al., 2010). 3 Experimental Setup rbow q1 w I I rbo � rconv rconv q1 q2 �rconv q1 ��rconv q2 � The final score for the input questions (q1, q2) is given by the following linear combination s(q1, q2) = β1 * sbow(q1, q2) + β2 * sconv(q1, q2) where β1 and β2 are parameters to be learned. 2.5 Training Procedure Our network is trained by minimizing a ranking loss function over the training set D. The input in each round is two pairs of questions (q1, q2)+ and (q1, qx)− where the questions in the first pair are semantically equivalent (positive example), and the ones in the second pair are not (negative exampl</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>In Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES ’97,</booktitle>
<pages>21</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="2194" citStr="Broder, 1997" startWordPosition="339" endWordPosition="340"> is part of the Stack Exchange Q&amp;A community site: (q1)“I have downloaded ISO files recently. How do I burn it to a CD or DVD or mount it?” and (q2)“I need to copy the iso file for Ubuntu 12.04 to a CD-R in Win8. How do I do so?”. Retrieving semantically equivalent questions is a challenging task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of differ</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>A. Broder. 1997. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences 1997, SEQUENCES ’97, pages 21–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Cai</author>
<author>Guangyou Zhou</author>
<author>Kang Liu</author>
<author>Jun Zhao</author>
</authors>
<title>Learning the latent topics for question retrieval in community qa.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<volume>11</volume>
<pages>273--281</pages>
<contexts>
<context position="2533" citStr="Cai et al., 2011" startWordPosition="395" endWordPosition="398">question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not </context>
</contexts>
<marker>Cai, Zhou, Liu, Zhao, 2011</marker>
<rawString>Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao. 2011. Learning the latent topics for question retrieval in community qa. In IJCNLP, volume 11, pages 273–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Clinchant</author>
<author>Eric Gaussier</author>
</authors>
<title>Information-based models for ad hoc ir.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>234--241</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13096" citStr="Clinchant and Gaussier, 2010" startWordPosition="2196" endWordPosition="2199">details, we refer the reader to the citation associated with the algorithm. • TFIDF (Manning et al., 2008) uses the traditional Vector Space Model to represent documents as vectors in a high-dimensional space. Each position in the vector represents a word and the weight of words are calculated using TFIDF. • BM25 (Robertson and Walker, 1994) is a probabilistic weighting method that takes into consideration term frequency, inverse document frequency and document length. Its has two free parameters: k1 to tune termfrequency saturation; and b to calibrate the document-length normalization. • IB (Clinchant and Gaussier, 2010) uses information-based models to capture the importance of a term by measuring how much Param. Name BOW-CNN CNN Word Emb. Size 200 200 Context Winow Size 3 3 Convol. Units 400 1000 Learning Rate 0.01 0.05 Table 2: Neural Network Hyper-Parameters its behavior in a document deviates from its behavior in the whole collection. • DFR (Amati and Van Rijsbergen, 2002) is based on divergence from randomness framework. The relevance of a term is measured by the divergence between its actual distribution and the distribution from a random process. • LMDirichlet and LMJelinekMercer apply probabilistic l</context>
</contexts>
<marker>Clinchant, Gaussier, 2010</marker>
<rawString>St´ephane Clinchant and Eric Gaussier. 2010. Information-based models for ad hoc ir. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 234–241. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero Nogueira dos Santos</author>
<author>Ma´ıra Gatti</author>
</authors>
<title>Deep convolutional neural networks for sentiment analysis of short texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2640" citStr="Santos and Gatti, 2014" startWordPosition="416" endWordPosition="419">r implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not clear that the CNN representation would be the most adequate. In this paper, we propose a hybrid neural net</context>
<context position="6988" citStr="Santos and Gatti, 2014" startWordPosition="1135" endWordPosition="1138"> a question consisting of N words q = {w1, ..., wN}, every word wn is converted into a real-valued vector rwn. Therefore, for each question, the input to the next NN layer is a sequence of real-valued vectors qemb = {rw1, ..., rwN }. Word representations are encoded by column vectors in an embedding matrix W0 E Rd×|V |, where V is a fixed-sized vocabulary. The next step in the CNN path consists in creating distributed vector representations rconv q1 and rconv q2 from the word embedding sequencies qemb 1 and qemb 2 .We perform this by using a convolutional layer in the same way as used in (dos Santos and Gatti, 2014) to create sentence-level representations. More specifically, given a question q1, the convolutional layer applies a matrix-vector operation to each window of size k of successive windows 695 in qemb 1 = {rw1, ..., rwN }. Let us define the vector zn E Rdk as the concatenation of a sequence of k word embeddings, centralized in the n-th word: zn = (rwn−(k−1)/2, ..., rwn+(k−1)/2)T The convolutional layer computes the j-th element of the vector rconv q1 E Rclu as follows: [rco q nv]j = f(1&lt;n&lt;N max [W1 zn + b1 ] I (2) where W1 E Rcluxdk is the weight matrix of the convolutional layer and f is the h</context>
</contexts>
<marker>Santos, Gatti, 2014</marker>
<rawString>C´ıcero Nogueira dos Santos and Ma´ıra Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Hatcher</author>
<author>Otis Gospodnetic</author>
<author>Michael McCandless</author>
</authors>
<date>2004</date>
<note>Lucene in action.</note>
<contexts>
<context position="12411" citStr="Hatcher et al., 2004" startWordPosition="2086" endWordPosition="2089">of all questions. In our experiments, given a query question q, all questions in the Q&amp;A community are evaluated when searching for a duplicate of q. 3.2 Baselines and Neural Network Setup In order to verify the impact of jointly using BOW and CNN representations, we perform experiments with two NN architectures: the BOWCNN and the CNN alone, which consists in using only the CNN path of BOW-CNN and, consequently, computing the score for a pair of questions using s(q1, q2) = sconv(q1, q2). Additionally, we compare BOW-CNN with six well-established IR algorithms available on the Lucene package (Hatcher et al., 2004). Here we provide a brief overview of them. For further details, we refer the reader to the citation associated with the algorithm. • TFIDF (Manning et al., 2008) uses the traditional Vector Space Model to represent documents as vectors in a high-dimensional space. Each position in the vector represents a word and the weight of words are calculated using TFIDF. • BM25 (Robertson and Walker, 1994) is a probabilistic weighting method that takes into consideration term frequency, inverse document frequency and document length. Its has two free parameters: k1 to tune termfrequency saturation; and </context>
</contexts>
<marker>Hatcher, Gospodnetic, McCandless, 2004</marker>
<rawString>Erik Hatcher, Otis Gospodnetic, and Michael McCandless. 2004. Lucene in action.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</booktitle>
<pages>2042--2050</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="2594" citStr="Hu et al., 2014" startWordPosition="407" endWordPosition="410">questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not clear that the CNN representation would be the most adequate.</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2042–2050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="2438" citStr="Jeon et al., 2005" startWordPosition="378" endWordPosition="381">g semantically equivalent questions is a challenging task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may va</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 84–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongcheng Ji</author>
<author>Fei Xu</author>
<author>Bin Wang</author>
<author>Ben He</author>
</authors>
<title>Question-answer topic model for question retrieval in community question answering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>2471--2474</pages>
<contexts>
<context position="2551" citStr="Ji et al., 2012" startWordPosition="399" endWordPosition="402">phrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not clear that the CNN</context>
</contexts>
<marker>Ji, Xu, Wang, He, 2012</marker>
<rawString>Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012. Question-answer topic model for question retrieval in community question answering. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 2471– 2474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing,</booktitle>
<pages>1746--1751</pages>
<contexts>
<context position="2577" citStr="Kim, 2014" startWordPosition="405" endWordPosition="406">nd (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not clear that the CNN representation would be t</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing, pages 1746–1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1. Cambridge university press Cambridge.</title>
<date>2008</date>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge university press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="14064" citStr="Mikolov et al., 2013" startWordPosition="2347" endWordPosition="2350">Rijsbergen, 2002) is based on divergence from randomness framework. The relevance of a term is measured by the divergence between its actual distribution and the distribution from a random process. • LMDirichlet and LMJelinekMercer apply probabilistic language model approaches for retrieval (Zhai and Lafferty, 2004). They differ in the smoothing method: LMDirichlet uses Dirichlet priors and LMJelinekMercer uses the Jelinek-Mercer method. The word embeddings used in our experiments are initialized by means of unsupervised pretraining. We perform pre-training using the skipgram NN architecture (Mikolov et al., 2013) available in the word2vec tool. We use the English Wikipedia to train word embeddings for experiments with the English dataset. For the AskUbuntu dataset, we use all available AskUbuntu community data to train word embeddings. The hyper-parameters of the neural networks and the baselines are tuned using the development sets. In Table 2, we show the selected hyperparameter values. In our experiments, we initialize each element [t]i of the bag-of-word weight vector t with the IDF of i−th word wi computed over the respective set of questions Q as follows [t]i = IDF(wi, Q) = log |Q| |q E Q : wi E</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
</authors>
<title>Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th annual international conference on Research and development in information retrieval,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="12810" citStr="Robertson and Walker, 1994" startWordPosition="2155" endWordPosition="2158">-CNN and, consequently, computing the score for a pair of questions using s(q1, q2) = sconv(q1, q2). Additionally, we compare BOW-CNN with six well-established IR algorithms available on the Lucene package (Hatcher et al., 2004). Here we provide a brief overview of them. For further details, we refer the reader to the citation associated with the algorithm. • TFIDF (Manning et al., 2008) uses the traditional Vector Space Model to represent documents as vectors in a high-dimensional space. Each position in the vector represents a word and the weight of words are calculated using TFIDF. • BM25 (Robertson and Walker, 1994) is a probabilistic weighting method that takes into consideration term frequency, inverse document frequency and document length. Its has two free parameters: k1 to tune termfrequency saturation; and b to calibrate the document-length normalization. • IB (Clinchant and Gaussier, 2010) uses information-based models to capture the importance of a term by measuring how much Param. Name BOW-CNN CNN Word Emb. Size 200 200 Context Winow Size 3 3 Convol. Units 400 1000 Learning Rate 0.01 0.05 Table 2: Neural Network Hyper-Parameters its behavior in a document deviates from its behavior in the whole </context>
</contexts>
<marker>Robertson, Walker, 1994</marker>
<rawString>Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of the 17th annual international conference on Research and development in information retrieval, pages 232–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelong Shen</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Gr´egoire Mesnil</author>
</authors>
<title>A latent semantic model with convolutional-pooling structure for information retrieval.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="2660" citStr="Shen et al., 2014" startWordPosition="420" endWordPosition="423"> problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not clear that the CNN representation would be the most adequate. In this paper, we propose a hybrid neural network architecture, w</context>
</contexts>
<marker>Shen, He, Gao, Deng, Mesnil, 2014</marker>
<rawString>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr´egoire Mesnil. 2014. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101– 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="10927" citStr="Toutanova et al., 2003" startWordPosition="1827" endWordPosition="1830">estion can have multiple duplicates. For this evaluation, we chose two highlyaccessed Q&amp;A communities: Ask Ubuntu and English. They differ in terms of content and size. Whereas Ask Ubuntu has 29510 duplicated questions, English has 6621. We performed experiments using only the title of the questions as well as title + body, which we call all for the rest of this section. The average size of a title is very small (about 10 words), which is at least 10 times smaller than the average size of all for both datasets. The data was tokenized using the tokenizer available with the Stanford POS Tagger (Toutanova et al., 2003), and all links were replaced by a unique string. For Ask Ubuntu, we did not consider the content inside the tag code, which contains some specific Linux commands or programming code. For each community, we created training, valisbow(q1, q2) = sconv(q1,q2) = rbow q1 .rbow q2 3.1 Data 696 Community Training Validation Test Ask Ubuntu 9802 1991 3800 English 2235 428 816 Table 1: Partition of training, validation and test sets for the experiments. dation and test sets. In Table 1, we inform the size of each set. The number of instances in the training set corresponds to the number of positive pai</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
</authors>
<title>Efficient near-duplicate detection for q&amp;a forum.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1001--1009</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="2231" citStr="Wu et al., 2011" startWordPosition="345" endWordPosition="348"> community site: (q1)“I have downloaded ISO files recently. How do I burn it to a CD or DVD or mount it?” and (q2)“I need to copy the iso file for Ubuntu 12.04 to a CD-R in Win8. How do I do so?”. Retrieving semantically equivalent questions is a challenging task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and i</context>
</contexts>
<marker>Wu, Zhang, Huang, 2011</marker>
<rawString>Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Efficient near-duplicate detection for q&amp;a forum. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1001–1009, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Retrieval models for question and answer archives.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="2457" citStr="Xue et al., 2008" startWordPosition="382" endWordPosition="385">valent questions is a challenging task due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single se</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008. Retrieval models for question and answer archives. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’08, pages 475–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL’11,</booktitle>
<pages>247--256</pages>
<contexts>
<context position="8155" citStr="Yih et al., 2011" startWordPosition="1338" endWordPosition="1341">t matrix of the convolutional layer and f is the hyperbolic tangent function. Matrices W0 and W1, and the vector b1 are parameters to be learned. The word embedding size d, the number of convolutional units clu, and the size of the word context window k are hyperparameters to be chosen by the user. 2.4 Question Pair Scoring After the bag-of-words and convolutional-based representations are generated for the input pair (q1, q2), the partial scores are computed as the cosine similarity between the respective vectors: larger range. This helps to penalize more on the prediction errors. Following (Yih et al., 2011), in our experiments we set γ to 10. Sampling informative negative examples can have a significant impact in the effectiveness of the learned model. In our experiments, before training, we create 20 pairs of negative examples for each positive pair (q1,q2)+. To create a negative example we (1) randomly sample a question qx that is not semantically equivalent to q1 or q2; (2) then create negative pairs (q1,qx)− and (q2,qx)−. During training, at each iteration we only use the negative example x that produces the smallest different sθ(q1, q2)+ − sθ(q1, qx)−. Using this strategy, we select more re</context>
<context position="9804" citStr="Yih et al., 2011" startWordPosition="1628" endWordPosition="1631">en by the following linear combination s(q1, q2) = β1 * sbow(q1, q2) + β2 * sconv(q1, q2) where β1 and β2 are parameters to be learned. 2.5 Training Procedure Our network is trained by minimizing a ranking loss function over the training set D. The input in each round is two pairs of questions (q1, q2)+ and (q1, qx)− where the questions in the first pair are semantically equivalent (positive example), and the ones in the second pair are not (negative example). Let A be the difference of their similarity scores, A = sθ(q1, q2) − sθ(q1, qx), generated by the network with parameter set θ. As in (Yih et al., 2011), we use a logistic loss over A L(A, θ) = log(1 + exp(−γA)) where γ is a scaling factor that magnifies A from [-2,2] (in the case of using cosine similarity) to a A well-structured source of semantically equivalent questions is the Stack Exchange site. It is composed by multiple Q&amp;A communities, whereby users can ask and answer questions, and vote up and down both questions and answers. Questions are composed by a title and a body. Moderators can mark questions as duplicates, and eventually a question can have multiple duplicates. For this evaluation, we chose two highlyaccessed Q&amp;A communitie</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL’11, pages 247–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>643--648</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2612" citStr="Yih et al., 2014" startWordPosition="411" endWordPosition="414">different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem description with several sentences, it was not clear that the CNN representation would be the most adequate. In this paper, we</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 643–648. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="13760" citStr="Zhai and Lafferty, 2004" startWordPosition="2302" endWordPosition="2305">e the importance of a term by measuring how much Param. Name BOW-CNN CNN Word Emb. Size 200 200 Context Winow Size 3 3 Convol. Units 400 1000 Learning Rate 0.01 0.05 Table 2: Neural Network Hyper-Parameters its behavior in a document deviates from its behavior in the whole collection. • DFR (Amati and Van Rijsbergen, 2002) is based on divergence from randomness framework. The relevance of a term is measured by the divergence between its actual distribution and the distribution from a random process. • LMDirichlet and LMJelinekMercer apply probabilistic language model approaches for retrieval (Zhai and Lafferty, 2004). They differ in the smoothing method: LMDirichlet uses Dirichlet priors and LMJelinekMercer uses the Jelinek-Mercer method. The word embeddings used in our experiments are initialized by means of unsupervised pretraining. We perform pre-training using the skipgram NN architecture (Mikolov et al., 2013) available in the word2vec tool. We use the English Wikipedia to train word embeddings for experiments with the English dataset. For the AskUbuntu dataset, we use all available AskUbuntu community data to train word embeddings. The hyper-parameters of the neural networks and the baselines are tu</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS), 22(2):179–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Yang Liu</author>
<author>Fang Liu</author>
<author>Daojian Zeng</author>
<author>Jun Zhao</author>
</authors>
<title>Improving question retrieval in community question answering using world knowledge.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<pages>2239--2245</pages>
<contexts>
<context position="2495" citStr="Zhou et al., 2013" startWordPosition="388" endWordPosition="391">k due to two main factors: (1) the same question can be rephrased in many different ways; and (2) two questions may be different but may refer implicitly to a common problem with the same answer. Therefore, traditional similarity measures based on word overlap such as shingling and Jaccard coefficient (Broder, 1997) and its variations (Wu et al., 2011) are not able to capture many cases of semantic equivalence. To capture the semantic relationship between pair of questions, different strategies have been used such as machine translation (Jeon et al., 2005; Xue et al., 2008), knowledge graphs (Zhou et al., 2013) and topic modelling (Cai et al., 2011; Ji et al., 2012). Recent papers (Kim, 2014; Hu et al., 2014; Yih et al., 2014; dos Santos and Gatti, 2014; Shen et al., 2014) have shown the effectiveness of convolutional neural networks (CNN) for sentence-level analysis of short texts in a variety of different natural language processing and information retrieval tasks. This motivated us to investigate CNNs for the task of semantically equivalent question retrieval. However, given the fact that the size of a question in an online community may vary from a single sentence to a detailed problem descripti</context>
</contexts>
<marker>Zhou, Liu, Liu, Zeng, Zhao, 2013</marker>
<rawString>Guangyou Zhou, Yang Liu, Fang Liu, Daojian Zeng, and Jun Zhao. 2013. Improving question retrieval in community question answering using world knowledge. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2239–2245.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>