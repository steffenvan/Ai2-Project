<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9622235">
Optimizing Synonym Extraction Using Monolingual and Bilingual
Resources
</title>
<author confidence="0.948652">
Hua WU, Ming ZHOU
</author>
<affiliation confidence="0.953941">
Microsoft Research Asia
</affiliation>
<address confidence="0.94605">
5F Sigma Center, No.49 Zhichun Road, Haidian District
Beijing, 100080, China
</address>
<email confidence="0.9924">
wu_hua_@msn.com, mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.997365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99949">
Automatically acquiring synonymous words
(synonyms) from corpora is a challenging task.
For this task, methods that use only one kind
of resources are inadequate because of low
precision or low recall. To improve the per-
formance of synonym extraction, we propose
a method to extract synonyms with multiple
resources including a monolingual dictionary,
a bilingual corpus, and a large monolingual
corpus. This approach uses an ensemble to
combine the synonyms extracted by individ-
ual extractors which use the three resources.
Experimental results prove that the three re-
sources are complementary to each other on
synonym extraction, and that the ensemble
method we used is very effective to improve
both precisions and recalls of extracted
synonyms.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998895890411">
This paper addresses the problem of extracting
synonymous English words (synonyms) from
multiple resources: a monolingual dictionary, a
parallel bilingual corpus, and a monolingual cor-
pus. The extracted synonyms can be used in a
number of NLP applications. In information re-
trieval and question answering, the synonymous
words are employed to bridge the expressions
gaps between the query space and the document
space (Mandala et al., 1999; Radev et al., 2001;
Kiyota et al., 2002). In automatic text summari-
zation, synonymous words are employed to iden-
tify repetitive information in order to avoid re-
dundant contents in a summary (Barzilay and
Elhadad, 1997). In language generation, syno-
nyms are employed to create more varied texts
(Langkilde and Knight, 1998).
Up to our knowledge, there are few studies in-
vestigating the combination of different resources
for synonym extraction. However, many studies
investigate synonym extraction from only one
resource. The most frequently used resource for
synonym extraction is large monolingual corpora
(Hindle, 1990; Crouch and Yang, 1992; Grefen-
statte, 1994; Park and Choi, 1997; Gasperin et al.,
2001 and Lin, 1998). The methods used the con-
texts around the investigated words to discover
synonyms. The problem of the methods is that the
precision of the extracted synonymous words is
low because it extracts many word pairs such as
“cat” and “dog”, which are similar but not syn-
onymous.
Other resources are also used for synonym ex-
traction. Barzilay and Mckeown (2001), and Shi-
mohata and Sumita (2002) used bilingual corpora
to extract synonyms. However, these methods can
only extract synonyms which occur in the bilingual
corpus. Thus, the extracted synonyms are limited.
Besides, Blondel and Sennelart (2002) used mono-
lingual dictionaries to extract synonyms. Although
the precision of this method is high, the coverage is
low because the result of this method heavily de-
pends on the definitions of words.
In order to improve the performance of syno-
nym extraction, Curran (2002) used an ensemble
method to combine the results of different methods
using a monolingual corpus. Although Curran
(2002) showed that the ensemble extractors out-
performed the individual extractors, it still cannot
overcome the deficiency of the methods using the
monolingual corpus.
To overcome the deficiencies of the methods
using only one resource, our approach combines
both monolingual and bilingual resources to auto-
matically extract synonymous words. By combin-
ing the synonyms extracted by the individual ex-
tractors using the three resources, our approach can
combine the merits of the individual extractors to
improve the performance of synonym extraction.
In fact, our approach can be considered as an
ensemble of different resources for synonym
extraction. Experimental results prove that the
three resources are complementary to each other
on synonym extraction, and that the ensemble
method we used is very effective to improve both
precisions and recalls of extracted synonyms.
The remainder of this paper is organized as
follows. The next section presents our approach
for synonym extraction. Section 3 describes an
implementation of the three individual extractors.
Section 4 presents the evaluation results. Section 5
discusses our method. In the last section, we draw
the conclusions of this work.
</bodyText>
<sectionHeader confidence="0.992778" genericHeader="method">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.999870875">
Instead of using only one kind of resource to
extract synonyms, we combine both monolingual
and bilingual resources for synonym extraction.
The resources include a monolingual dictionary,
an English-Chinese bilingual corpus, and a large
corpus of monolingual documents. Before com-
bining them, we first propose three methods to
extract synonyms from the three resources. Espe-
cially, a novel method is proposed to increase the
coverage of the extracted synonyms using the
bilingual corpus. Next, we develop an ensemble
method to combine the individual extractors. The
advantage of our approach is that it can combine
the merits of the individual extractors to improve
the precision and recalls of the extracted syno-
nyms.
</bodyText>
<subsectionHeader confidence="0.9964725">
2.1 Synonym Extraction with a Monolin-
gual Dictionary
</subsectionHeader>
<bodyText confidence="0.99996565">
This section proposes a method to extract syno-
nyms from a monolingual dictionary. In a mono-
lingual dictionary, each entry is defined by other
words and may also be used in the definitions for
other words. For a word in the dictionary, the
words used to define it are called hubs and the
words whose definitions include this word are
called authorities as in (Blondel and Sennelart,
2002). We use the hubs and authorities of a word
to represent its meaning. The assumption behind
this method is that two words are similar if they
have common hubs and authorities. In this paper,
we only use content words as members of hubs and
authorities.
We take these hubs and authorities as features of
a word. The vector constructed with them is re-
ferred to as the feature vector of a word. The simi-
larity between two words is calculated through
their feature vectors with the cosine measure as
shown in Equation (1).
</bodyText>
<equation confidence="0.797004625">
v2 2
j
(1)
where
Fi =&lt; (wi1, vi1 ), (wi2 , vi2 ), ... (wim ,vim) &gt;
Fi is the feature vector of wi;
vij = 1 if word wij is a hub or an authority of the
word wi; else, vij = 0 ;
</equation>
<subsectionHeader confidence="0.9969305">
2.2 Synonym Extraction with a Bilingual
Corpus
</subsectionHeader>
<bodyText confidence="0.9998384">
This section proposes a novel method to extract
synonyms from a bilingual corpus. It uses the
translations of a word to express its meaning. The
assumption of this method is that two words are
synonymous if their translations are similar.
Given an English word, we get their translations
with an English-Chinese bilingual dictionary. Each
translation is assigned a translation probability,
which is trained with a bilingual English-Chinese
corpus based on the result of word alignment. The
aligner use the model described in (Wang et al.,
2001). In order to deal with the problem of data
sparseness, we conduct a simple smoothing by
adding 0.5 to the counts of each translation pair as
in (2).
</bodyText>
<equation confidence="0.952975714285714">
count c e
( , )0.5
+
e) 0.5* |
+ trans c
_
j
</equation>
<bodyText confidence="0.937474166666667">
where
count(c, e) represents the co-occurring fre-
quency of the Chinese word c and the English
word e in the sentence pairs.
count(e) represents the frequency of the English
word e occurring in the bilingual corpus.
</bodyText>
<figure confidence="0.660697075471698">
1
sim
)
cos(
(w1
F1, F2
, w2)
*
(v1
v2
i
)
j
j
I
w1i =w2
�v
2*
1 i
i
(
 |e
c
)
p
(
count
|
(2)
F F
1 , 2
cos(
,
pij is the translation probability of the word wi
is translated into cij
For example, the feature vectors of two words
“abandon” and “forsake” are:
forsake: &lt; (ᬒᓗ, 0.1333), (ᡯᓗ, 0.1333), (଒ᓗ
(, )
w w
1 2
3
attj = (r, w)
p(attj) = count(* , r, w)
N
*
(p1
p2
i
)
j
(3)
j
</figure>
<bodyText confidence="0.966294428571429">
 |trans _ c  |represents the number of Chinese
translations for a given English word e.
The translations and the translation probabili-
ties of a word are used to construct its feature
vector. The similarity of two words is estimated
through their feature vectors with the cosine
measure as shown in (3).
</bodyText>
<equation confidence="0.7170758">
)
=
2
, w2)
sim
(w1
�=
c c
1 i 2
=
2
p2
1
j
j
</equation>
<bodyText confidence="0.983907285714286">
where
Fi =&lt; (ci1 , pi1 ), (ci2, pi2 ), ... (cim, pim) &gt;
Fi is the feature vector of wi;
cij is the jth Chinese translation of the word wi;
0.0667) (䘫ᓗ, 0.0667), (⾏ᓔ, 0.0667), ...&gt;
abandon: &lt;(ᬒᓗ, 0.3018), (ᡯᓗ, 0.1126), (䘫ᓗ,
0.0405), (㟡ᓗ, 0.0225), (ᡯᓔ, 0.0135),...&gt;
</bodyText>
<listItem confidence="0.4978935">
2.3 Synonym Extraction with a Monolin-
gual Corpus
</listItem>
<bodyText confidence="0.999900090909091">
The context method described in Section 1 is also
used for synonym extraction from large mono-
lingual corpora of documents. This method relies
on the assumption that synonymous words tend to
have similar contexts. In this paper, we use the
words which have dependency relationships with
the investigated word as contexts. The contexts are
obtained by parsing the monolingual documents.
The parsing results are represented by dependency
triples which are denoted as &lt;w1, Relation Type,
w2&gt;. For example, the sentence “I declined the
invitation” is transformed into three triples after
parsing: &lt;decline, SUBJ, I&gt;, &lt;decline, OBJ, invi-
tation&gt; and &lt;invitation, DET, the&gt;. If we name
&lt;Relation Type, w2&gt; as an attribute of the word w1,
the verb “decline” in the above sentence has two
attributes &lt;OBJ, invitation&gt; and &lt;SUBJ, I&gt; . Thus,
the contexts of a word can be expressed using its
attributes. In this case, two words are synonymous
if they have similar attributes.
We use a weighted version of the Dice measure
to calculate the similarity of two words.
</bodyText>
<equation confidence="0.99545">
Y_ (W (w,att ) +W (w,att ))
1 k 2 k
tt
a
k
W (w1,atti)+ E W(
w1 ) attj A w
Î ( 2 )
</equation>
<bodyText confidence="0.979340666666667">
where
atti , attj , attk stands for attributes of words.
W (wi , attj) indicates the association strength
between the attribute attj with the word wi.
A(wi) denotes the attribute set of the word wi.
The measure used to measure the association
strength between a word and its attributes is
weighted mutual information (WMI) (Fung and
Mckeown, 1997) as described in (5).
</bodyText>
<figure confidence="0.818337666666667">
W(wi,attj) =WMI(wi
p
)
( )
i
p w *p
(attj
where
p(wi) count(wi ,
,
count(*, r, w) : frequency of the triples having
dependency relation r with the word w.
count( wi,*,*) : frequency of the triples including
word wi.
N: number of triples in the corpus.
</figure>
<bodyText confidence="0.9997562">
We use it instead of point-wise mutual information
in Lin (1998) because the latter tends to overesti-
mate the association between two parts with low
frequencies. Weighted mutual information melio-
rates this effect by adding p(wi , attj) .
</bodyText>
<subsectionHeader confidence="0.982488">
2.4 Combining the Three Extractors
</subsectionHeader>
<bodyText confidence="0.992016625">
In terms of combining the outputs of the different
methods, the ensemble method is a good candidate.
Originally, the ensemble method is a machine
learning technique of combining the outputs of
several classifiers to improve the classification
performance (Dietterich, 2000). It has been suc-
cessfully used in many NLP tasks. For example,
(Curran, 2002) proved that the ensembles of indi-
</bodyText>
<figure confidence="0.98219958974359">
sim
=
atti
ÎA (4)
2
*
i
p
1
J
i
�
(
A
Î
,
att
)
j
w2
*
=
,
)
p
(wi
log
attj
,
)
attj
,
)
(wi
(5)
att j
*,
*)
N
</figure>
<bodyText confidence="0.998975111111111">
vidual extractors using different contexts in the
monolingual corpus improve the performance of
synonym extraction.
In fact, we can consider the extractors in the
previous sections as binary classifiers. Thus, we
use the ensemble method to combine the output of
the individual extractors described in the previous
sections for synonym extraction. The method is
described in Equation (6).
</bodyText>
<equation confidence="0.981895666666667">
3
sim(w1 , w2) = E (ai  simi (w1 , w2)) (6)
i=1
</equation>
<bodyText confidence="0.927947578947369">
where
simi (w1, w2) (i =1, 2, 3) stands for the different
similarity measure using different resources
described in the previous sections.
i
individual extractors.
The reasons that we use the weighted ensemble
method are as follows: (1) If the majority of three
extractors select the same word as a synonym of a
investigated word, it tend to be a real synonym.
This method can ensure it has a high similarity
score. Thus, it will improve the precision of the
extracted synonyms. (2) With this method, it can
improve the coverage of the extracted synonyms.
This is because if the similarity score of a candi-
date with the investigated word is higher than a
threshold, our method can select the candidate as a
synonym even though it is only suggested by one
extractor.
</bodyText>
<sectionHeader confidence="0.9810555" genericHeader="method">
3 Implementation of Individual
Extractors
</sectionHeader>
<bodyText confidence="0.955831214285714">
For the extractor employing a monolingual dic-
tionary, we use the same online dictionary as in
(Blondel and Sennelart, 2002), which is named the
Online Plain Text Dictionary. The dictionary
consists of 27 HTML files, which is available
from the web site http://www.gutenberg.net/. With
the method described in Section 2.1, the result for
the extracted synonyms is shown in Table 1 when
the similarity threshold is set to 0.04. An example
is shown as follows:
acclimatize:
(acclimate, 0.1481; habituate, 0.0976)
The numbers in the example are the similarity
scores of two words.
</bodyText>
<tableCaption confidence="0.874929">
Table 1. Synonyms Extracted from the Monolingual
Dictionary
</tableCaption>
<table confidence="0.9882125">
Category # Entries # Average
Synonyms
Noun 16963 4.7
Verb 5084 7.1
</table>
<bodyText confidence="0.998876454545455">
For synonym extraction from the bilingual
corpus, we use an English-Chinese lexicon, which
includes 219,404 English words with each source
word having 3 translations on average. The word
translation probabilities are estimated from a bi-
lingual corpus that obtains 170,025 pairs of Chi-
nese-English sentences, including about 2.1 million
English words and about 2.5 million Chinese words.
With the method described in Section 2.2, we
extracted synonyms as shown in Table 2 when the
similarity threshold is set to 0.04.
</bodyText>
<tableCaption confidence="0.8692985">
Table 2. Synonyms Extracted from the Bilingual
corpus
</tableCaption>
<table confidence="0.98532475">
Category #Entries #Average
Synonyms
Noun 26253 10.2
Verb 7364 14.8
</table>
<bodyText confidence="0.999147857142857">
For synonym extraction from a monolingual
corpus, we use the Wall Street Journal from 1987 to
1992, the size of which is about 500M bytes. In
order to get contexts of words, we parse the corpus
with an English parser –NLPWIN 1 . From the
parsing results, we extracted the following four
types of dependency triples.
</bodyText>
<figure confidence="0.5216075">
(a) &lt;verb, OBJ, noun&gt;
(b) &lt;verb, SUBJ, noun&gt;
(c) &lt;noun, ATTRIB, adjective&gt;
(d) &lt;verb, MODS, adjunct&gt;
</figure>
<bodyText confidence="0.999649571428571">
The statistics are shown in Table 3. Token
means the total number of triples in the triple set
and type means a unique instance of triple in the
corpus. These triples are used as contexts of words
to calculate the similarity between words as de-
scribed in Section 2.3. The result is shown in Table
4 when the similarity threshold is set to 0.1.
</bodyText>
<equation confidence="0.987963333333333">
ai i = �
( 1, 2, 3, and ai = is the weight for the
1)
</equation>
<tableCaption confidence="0.5062008">
1 The NLPWIN parser is developed at Microsoft Re-
search. Its output can be a phrase structure parse tree or a
logical form which is represented with dependency
triples.
Table 3. Statistics for Triples
</tableCaption>
<table confidence="0.999917666666667">
# Token # Type
OBJ 7,041,382 1,487,543
SUBJ 7,180,572 2,116,761
ATTRIB 4,976,822 1,393,188
MODS 3,557,737 94,004
Total 22,756,512 5,937,496
</table>
<tableCaption confidence="0.999078">
Table 4. Synonyms Extracted from the Monolingual
</tableCaption>
<table confidence="0.9613734">
Corpus
Category Entries Average
Synonyms
Noun 16963 4.6
Verb 5084 7.1
</table>
<sectionHeader confidence="0.999117" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999248">
4.1 The Gold Standard
</subsectionHeader>
<bodyText confidence="0.999076769230769">
The simplest evaluation measure is direct com-
parison of the extracted synonyms with the manu-
ally created thesaurus. However, the thesaurus
coverage is a problem. In this paper, we combined
two thesauri as a gold stardard: WordNet 1.6
http://www.cogsci.princeton.edu/~wn/) and Roget
(Roget’s II: The New Thesaurus, 1995.
http://www.bartleby.com/thesauri/).
In WordNet, one synset consists of several
synonyms which represent a single sense. There-
fore, a polysemous word occurs in more than one
synsets. For example, the polysemous word
“abandon” occur in five different synsets:
</bodyText>
<equation confidence="0.960902">
(abandon, forsake, desolate, desert, lurch)
(vacate, empty, abandon)
(abandon, give up, give)
(abandon, give up)
(abandon)
</equation>
<bodyText confidence="0.929694529411765">
For a given word, we combine its synonyms from
all synsets including the word. Thus, we get the
synonyms of the word “ abandon” as follows:
abandon4 forsake, desolate, desert, lurch, vacate,
empty, give up, give
For synonyms in Roget, we also combine the
synonyms in different synsets into one set as we
do for WordNet. Thus, we get the synonyms of the
word “abandon”as follows:
abandon4break off, desist, discontinue, give up, leave
off, quit, relinquish, remit, stop, desert, forsake, leave,
throw over, abdicate, cede, demit, forswear, hand over,
quitclaim, render, renounce, resign, surrender, waive,
yield, give over, forgo, lay down
Combining the results of WordNet and Roget,
we can get the synonyms of the word “abandon” as
follows.
</bodyText>
<construct confidence="0.877103">
abandon4 desolate, lurch, vacate, empty, give, abdicate,
break off, cede, demit, desert, desist, discontinue, forgo,
forsake, forswear, give up, give over, hand over, lay
down, lay off, leave off, leave, quit, quitclaim, relinquish,
remit, stop, swear off, throw over, render, renounce,
resign, surrender, waive, yield
</construct>
<subsectionHeader confidence="0.996408">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999950363636363">
The evaluation metrics are precision, recall, and
f-measure. If we use S to indicate the synonyms
that our method extracts for a word and SG to
denote the synonyms of the word in WordNet and
Roget, the methods to calculate the precision, recall,
and f-measure of our methods are shown in Equa-
tion (7), (8), and (9). To investigate the results of
more than one word, we calculate the average
precision, recall and f-measure, which sum the
individual values divided by the number of the
investigated words.
</bodyText>
<equation confidence="0.8645874">
 |S SG |
precision = (7)
 |S |
 |S S |
recall =  G
|S |
G
f - measure = 2 precision  recall

precision recall
</equation>
<bodyText confidence="0.737448">
+
</bodyText>
<subsectionHeader confidence="0.999953">
4.3 Test Set
</subsectionHeader>
<bodyText confidence="0.9998855">
In order to evaluate our methods, we build up a test
set which includes three parts:
</bodyText>
<listItem confidence="0.999074166666667">
(a) high-frequency words: occurring more than
100 times;
(b) middle-frequency words: occurring more than
10 times and not greater than 100 times;
(c) low-frequency words: occurring no greater
than 10 times.
</listItem>
<tableCaption confidence="0.995076">
Table 5. Statistics for Nouns and Verbs
</tableCaption>
<table confidence="0.789142571428571">
High Fre- Middle Low
quency Frequency Frequency
Noun 600 2000 1000
Verb 340 1300 800
The frequency counts are estimated from Wall
Street Journal (1987-1992), from which we ran-
domly extracted 3600 nouns and 2440 verbs. These
</table>
<tableCaption confidence="0.877415">
Table 6. Evaluation Results for Nouns
</tableCaption>
<table confidence="0.9999185">
High-Frequency Nouns Middle-Frequency Nouns Low-Frequency Nouns
Pre Rec F Pre Rec F Pre Rec F
1 0.174 0.140 0.155 0.212 0.137 0.167 0.198 0.119 0.149
2 0.225 0.209 0.217 0.242 0.212 0.226 0.207 0.212 0.209
3 0.118 0.109 0.114 0.117 0.104 0.109 0.099 0.096 0.098
1+2+3 0.240 0.201 0.219 0.271 0.220 0.243 0.222 0.232 0.227
</table>
<tableCaption confidence="0.998382">
Table 7. Evaluation Results for Verbs
</tableCaption>
<table confidence="0.999359833333333">
High-Frequency Verbs Middle-Frequency Verbs Low-Frequency Verbs
Pre Rec F Pre Rec F Pre Rec F
1 0.228 0.243 0.235 0.272 0.233 0.251 0.209 0.216 0.212
2 0.226 0.312 0.262 0.224 0.292 0.253 0.184 0.275 0.220
3 0.143 0.166 0.154 0.162 0.127 0.142 0.128 0.135 0.132
1+2+3 0.295 0.323 0.308 0.311 0.304 0.307 0.238 0.302 0.266
</table>
<figureCaption confidence="0.597981">
Note: 1, 2, and 3 represent the extractor using the monolingual dictionary, the bilingual corpus, and the monolingual
corpus respectively. The symbols “Pre”, “Rec”, and “F” represent precision, recall, and f-measure scores.
</figureCaption>
<figure confidence="0.996728178571429">
Precision
0.45
0.35
0.25
0.15
0.05
0.2
0.1
0.4
0.3
0
0 0.1 0.2 0.3 0.4 0.5
Recall
2 1 3 1+2+3
Precision
0.45
0.35
0.25
0.15
0.05
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Recall
2 1 3 1+2+3
</figure>
<figureCaption confidence="0.999857">
Figure 1. Recall-Precision curves for nouns Figure 2. Recall-Precision curves for verbs
</figureCaption>
<bodyText confidence="0.999447">
words have synonyms both in our results extracted
from the three resources and in the thesauri
WordNet and Roget. The statistics of the test set
are shown in Table 5.
</bodyText>
<subsectionHeader confidence="0.998818">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999973641025641">
In this section, we compare the extracted syno-
nyms of the nouns and verbs in the test set with
those in WordNet and Roget. For each method, we
select those as synonyms whose similarity scores
with the investigated word are larger than a given
threshold. A development set is used to determine
the thresholds of each method. The thresholds for
getting highest f-measure scores on the develop-
ment set are selected. In our experiments, we get
0.04, 0.04, 0.1 and 0.04 for Method 1, Method 2,
Method 3 and the combined approach, respectively.
The evaluation results for the individual extractors
and the ensemble extractor are shown in Table 6
and Table 7. We set a1=0.4, a2=0.4 and a3=0.2 in
Equation (6) for the ensemble to combine the re-
sults from the three resources. The weights are also
obtained with the development set.
In order to examine the performance of each
method in more details, we also get the precisions
and recalls under different thresholds. Figure 1 and
Figure 2 shows the precision values under different
recall values (different thresholds) for all nouns and
verbs, respectively.
Among all of the methods, the method com-
bining all of the three resources gets the best results
in terms of both precision and recall. The effect is
similar to the ensemble methods for synonym
extraction in (Curran 2002). However, our method
uses an ensemble of different resources instead of
one single resource. During the experiments, we
also find the ensemble combining all of the three
extractors outperforms the ensembles only com-
bining any two of the three extractors. This indi-
cates that the extractors using the three different
resources are complementary to each other. For
example, the extractor using the monolingual
dictionary gets a high precision and the extractor
using the bilingual corpus gets a high recall. Al-
though the extractor using the monolingual corpus
achieved much lower precision and recall on
synonym extraction, it is still useful to be included
in the ensemble. This shows that the monolingual
corpus is complementary to the other two re-
sources on synonym extraction. The success of
our method also indicates that our ensemble
method by weighting all extractors is effective for
synonym extraction.
Among the methods only using one kind of
resource, Method 2, which uses the bilingual
corpus, has the highest f-measure scores on both
nouns and verbs. From the results in Figure 1 and
Figure 2, we can see that the coverage of syno-
nyms extracted by Method 2 is the highest. Al-
though it has lower precisions than Method 1
under low recalls, its precisions are higher than
those of Method 1 under higher recalls. This
shows that Method 2 can get a good compromise
between precision and recall. We also note that the
maximum recall of Method 2 is much larger than
that of Method 1. This is because (1) in Method 1,
the words used in the definitions are highly limited.
Thus, the coverage of the synonyms is limited; (2)
the advantage of Method 2 is that the coverage of
extracted synonyms is high because it can extract
the synonyms not occurring in the corpus. It is
different from the method in (Barzilay and
Mckeown, 2001; Shimohata and Sumita, 2002),
which can only extract the synonyms in the bi-
lingual corpus.
The performance of Method 3 is the worst. It is
caused by two factors: (1) the context model of
Method 3 introduces much noise because of the
errors of the parser; (2) this method is unable to
distinguish synonyms, antonyms, and similar
words because they tend to have similar contexts.
From the contexts it uses, method 3 is suitable to
extract related words which have the similar us-
ages from the view of syntax.
</bodyText>
<sectionHeader confidence="0.998416" genericHeader="evaluation">
5 Discussions
</sectionHeader>
<bodyText confidence="0.999977652173913">
This paper uses three different methods and re-
sources for synonym extraction. By using the cor-
pus-based method, we can get some synonyms or
near synonyms which can not be found in the
hand-built thesauri. For Example: “handspring 4
handstand”, “audiology 4 otology”, “roisterer
4 carouser” and “parmesan 4 gouda”. These
kinds of synonyms are difficult for hand-built
thesauri to cover because they occur too infrequent
to be caught by humans. In addition, this cor-
pus-based method can get synonyms in specific
domains while the general thesauri don’t provide
such fine-grained knowledge.
Comparing the results with the human-built
thesauri is not the best way to evaluate synonym
extraction because the coverage of the human-built
thesaurus is also limited. However, manually
evaluating the results is time consuming. And it
also cannot get the precise evaluation of the ex-
tracted synonyms. Although the human-built
thesauri cannot help to precisely evaluate the re-
sults, they can still be used to detect the effective-
ness of extraction methods.
</bodyText>
<sectionHeader confidence="0.908595" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.99997753125">
This paper proposes a new method to extract
synonyms from three resources: a monolingual
dictionary, a bilingual corpus, and a large mono-
lingual corpus. This method uses a weighted en-
semble to combine all of the results of the indi-
vidual extractors using one of the three resources
respectively. Experimental results prove that the
three resources are complementary to each other on
synonym extraction, and that the ensemble method
we used is very effective to improve both preci-
sions and recalls when the results are compared
with the manually-built thesauri WordNet and
Roget.
Further, we also propose a new method to ex-
tract synonyms from a bilingual corpus. This
method uses the translations of a word to represent
its meaning. The translation probabilities are
trained with the bilingual corpus. The advantage of
this method is that it can improve the coverage of
the extracted synonyms. Experiments indicate that
this method outperforms the other methods using a
monolingual corpus or a monolingual dictionary.
The contribution of this work lies in three as-
pects: (1) develop a method to combine the results
of individual extractors using the three resources
on synonym extraction; (2) investigate the per-
formance of the three extraction methods using
different resources, exposing the merits and de-
merits of each method; (3) propose a new method
to extract synonyms from a bilingual corpus,
which greatly improves the coverage of the ex-
tracted synonyms.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847426470589">
Barzilay R. and Elhadad M. 1997. Using lexical chains
for text summarization. In proceedings of the ACL
Workshop on Intelligent Scalable Text Summariza-
tion, pp10-17.
Barzilay R. and McKeown K. 2001. Extracting Para-
phrases from a Parallel Corpus. In Proc. of
ACL/EACL.
Blondel V. D. and Sennelart P. 2002. Automatic ex-
traction of synonyms in a dictionary. In Proc. of the
SIAM Workshop on Text Mining.
Crouch C. J. and Yang B. 1992. Experiments in auto-
matic statistical thesaurus construction. In Proc. of
the 15th Annual International ACM SIGIR confer-
ence on Research and Development in Information
Retrieval, pp77-88.
Curran J. 2002 Ensemble Methods for Automatic The-
saurus Extraction. In Proc. of the Conference on
Empirical Methods in Natural Language Processing.
pp. 222-229.
Dietterich T. 2000. Ensemble Methods in Machine
Learning. In Proc. of the First International Work-
shop on Multiple Classier Systems. pp 1-15.
Fung P., Mckeown K. 1997. A Technical Word- and
Term- Translation Aid Using Noisy Parallel Corpora
across Language Groups. In: Machine Translation,
Vol.1-2 (special issue), pp53-87.
Gasperin C., Gamallo P., Agustini A., Lopes G., Lima
V. 2001 Using Syntactic Contexts for Measuring
Word Similarity. Workshop on Knowledge Acquisi-
tion &amp; Categorization, ESSLLI.
Grefenstette G. 1994 Explorations in Automatic The-
saurus Discovery. Kluwer Academic Press.
Hindle D. 1990. Noun Classification from Predi-
cate-Argument Structure. In Proc. of the 28th Annual
Meeting of the Association for Computational Lin-
guistics.
Kiyota Y., Kurohashi S., Kido F. 2002. &amp;quot;Dialog Navi-
gator&amp;quot;: A Question Answering System Based on
Large Text Knowledge Base. In Proc. of the 19th
International Conference on Computational Linguis-
tics.
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Proc. of
the COLING-ACL.
Lin D. 1998 Automatic Retrieval and Clustering of
Similar Words. In Proc. of the 36th Annual Meeting of
the Association for Computational Linguistics.
Mandala R., Tokunaga T. Tanaka H. 1999. Combining
Multiple Evidence from Different Type of Thesaurus
for Query Expansion. In Proc. of the 22nd annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval.
Park Y.C. and Choi K. S. 1997. Automatic Thesaurus
Construction Using Baysian Networks. Information
Processing &amp; Management. Vol. 32.
Radev D., Qi H., Zheng Z., Goldensohn S., Zhang Z.,
Fan W., Prager J. 2001. Mining the Web for Answers
to Natural Language Questions. In the Tenth Interna-
tional ACM Conference on Information and Knowl-
edge Management.
Shimohata M. and Sumita E. 2002. Automatic Para-
phrasing Based on Parallel Corpus for Normalization.
In Proc. of the Third International Conference on
Language Resources and Evaluation.
Wang W., Huang J., Zhou M., Huang C.N. 2001. Find-
ing Target Language Correspondence for Lexicalized
EBMT System. In Proc. of the 6th Natural Language
Processing Pacific Rim Symposium.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994085">Optimizing Synonym Extraction Using Monolingual and Bilingual Resources</title>
<author confidence="0.99311">Hua WU</author>
<author confidence="0.99311">Ming</author>
<affiliation confidence="0.999688">Microsoft Research</affiliation>
<address confidence="0.9906645">5F Sigma Center, No.49 Zhichun Road, Haidian Beijing, 100080,</address>
<email confidence="0.999066">wu_hua_@msn.com,mingzhou@microsoft.com</email>
<abstract confidence="0.969252294695481">Automatically acquiring synonymous words (synonyms) from corpora is a challenging task. For this task, methods that use only one kind of resources are inadequate because of low precision or low recall. To improve the performance of synonym extraction, we propose a method to extract synonyms with multiple resources including a monolingual dictionary, a bilingual corpus, and a large monolingual corpus. This approach uses an ensemble to combine the synonyms extracted by individual extractors which use the three resources. Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. This paper addresses the problem of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002) showed that the ensemble extractors outperformed the individual extractors, it still cannot overcome the deficiency of the methods using the monolingual corpus. To overcome the deficiencies of the methods using only one resource, our approach combines both monolingual and bilingual resources to automatically extract synonymous words. By combining the synonyms extracted by the individual extractors using the three resources, our approach can combine the merits of the individual extractors to improve the performance of synonym extraction. In fact, our approach can be considered as an ensemble of different resources for synonym extraction. Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. The remainder of this paper is organized as follows. The next section presents our approach for synonym extraction. Section 3 describes an implementation of the three individual extractors. Section 4 presents the evaluation results. Section 5 discusses our method. In the last section, we draw the conclusions of this work. Approach Instead of using only one kind of resource to extract synonyms, we combine both monolingual and bilingual resources for synonym extraction. The resources include a monolingual dictionary, an English-Chinese bilingual corpus, and a large corpus of monolingual documents. Before combining them, we first propose three methods to extract synonyms from the three resources. Especially, a novel method is proposed to increase the coverage of the extracted synonyms using the bilingual corpus. Next, we develop an ensemble method to combine the individual extractors. The advantage of our approach is that it can combine the merits of the individual extractors to improve the precision and recalls of the extracted synonyms. 2.1 Synonym Extraction with a Monolingual Dictionary This section proposes a method to extract synonyms from a monolingual dictionary. In a monolingual dictionary, each entry is defined by other words and may also be used in the definitions for other words. For a word in the dictionary, the words used to define it are called hubs and the words whose definitions include this word are called authorities as in (Blondel and Sennelart, 2002). We use the hubs and authorities of a word to represent its meaning. The assumption behind this method is that two words are similar if they have common hubs and authorities. In this paper, we only use content words as members of hubs and authorities. We take these hubs and authorities as features of a word. The vector constructed with them is referred to as the feature vector of a word. The similarity between two words is calculated through their feature vectors with the cosine measure as shown in Equation (1). 2 j (1) where ... the feature vector of word a hub or an authority of the else, 2.2 Synonym Extraction with a Bilingual Corpus This section proposes a novel method to extract synonyms from a bilingual corpus. It uses the translations of a word to express its meaning. The assumption of this method is that two words are synonymous if their translations are similar. Given an English word, we get their translations with an English-Chinese bilingual dictionary. Each translation is assigned a translation probability, which is trained with a bilingual English-Chinese corpus based on the result of word alignment. The aligner use the model described in (Wang et al., 2001). In order to deal with the problem of data sparseness, we conduct a simple smoothing by adding 0.5 to the counts of each translation pair as in (2). count c e ( , )0.5 + 0.5* | c _ j where the co-occurring freof the Chinese word the English the sentence pairs. the frequency of the English in the bilingual corpus. 1 sim ) cos( * i ) j I ( c ) p ( count | (2) F F cos( , the translation probability of the word translated into For example, the feature vectors of two words “abandon” and “forsake” are: &lt; 0.1333), 0.1333), (, ) w w 1 2 3 = , N * i ) j (3) j the number of Chinese for a given English word The translations and the translation probabilities of a word are used to construct its feature vector. The similarity of two words is estimated through their feature vectors with the cosine measure as shown in (3). ) = 2 sim c c = 2 1 j j where =&lt; , ), ... the feature vector of the Chinese translation of the word 0.0667), 0.0667), ...&gt; 0.3018), 0.1126), 0.0225), 0.0135),...&gt; 2.3 Synonym Extraction with a Monolingual Corpus The context method described in Section 1 is also used for synonym extraction from large monolingual corpora of documents. This method relies on the assumption that synonymous words tend to have similar contexts. In this paper, we use the words which have dependency relationships with the investigated word as contexts. The contexts are obtained by parsing the monolingual documents. The parsing results are represented by dependency which are denoted as Relation Type, For example, the sentence “I declined the invitation” is transformed into three triples after parsing: &lt;decline, SUBJ, I&gt;, &lt;decline, OBJ, invitation&gt; and &lt;invitation, DET, the&gt;. If we name Type, as an attribute of the word the verb “decline” in the above sentence has two &lt;OBJ, invitation&gt; and &lt;SUBJ, I&gt; the contexts of a word can be expressed using its attributes. In this case, two words are synonymous if they have similar attributes. We use a weighted version of the Dice measure to calculate the similarity of two words. tt k ) A w 2) where , , stands for attributes of words. , the association strength the attribute the word the attribute set of the word The measure used to measure the association strength between a word and its attributes is weighted mutual information (WMI) (Fung and Mckeown, 1997) as described in (5). p ) ( ) i w where , , frequency of the triples having relation the word frequency of the triples including of triples in the corpus. We use it instead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. Weighted mutual information meliothis effect by adding , 2.4 Combining the Three Extractors In terms of combining the outputs of the different methods, the ensemble method is a good candidate. Originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance (Dietterich, 2000). It has been successfully used in many NLP tasks. For example, 2002) proved that the ensembles of indisim = atti (4) 2 * i p 1 J i � ( A Î , att ) j * = , ) p log , ) , ) (5) *, *) N vidual extractors using different contexts in the monolingual corpus improve the performance of synonym extraction. In fact, we can consider the extractors in the previous sections as binary classifiers. Thus, we use the ensemble method to combine the output of the individual extractors described in the previous sections for synonym extraction. The method is described in Equation (6). 3 where (i 2, 3) for the different similarity measure using different resources described in the previous sections. individual extractors. The reasons that we use the weighted ensemble method are as follows: (1) If the majority of three extractors select the same word as a synonym of a investigated word, it tend to be a real synonym. This method can ensure it has a high similarity score. Thus, it will improve the precision of the extracted synonyms. (2) With this method, it can improve the coverage of the extracted synonyms. This is because if the similarity score of a candidate with the investigated word is higher than a threshold, our method can select the candidate as a synonym even though it is only suggested by one extractor. of Individual Extractors For the extractor employing a monolingual dictionary, we use the same online dictionary as in (Blondel and Sennelart, 2002), which is named the Online Plain Text Dictionary. The dictionary consists of 27 HTML files, which is available the web sitehttp://www.gutenberg.net/.With the method described in Section 2.1, the result for the extracted synonyms is shown in Table 1 when the similarity threshold is set to 0.04. An example is shown as follows: acclimatize: (acclimate, 0.1481; habituate, 0.0976) The numbers in the example are the similarity scores of two words. Table 1. Synonyms Extracted from the Monolingual Dictionary Category # Entries # Synonyms Noun 16963 4.7 Verb 5084 7.1 For synonym extraction from the bilingual corpus, we use an English-Chinese lexicon, which includes 219,404 English words with each source word having 3 translations on average. The word translation probabilities are estimated from a bilingual corpus that obtains 170,025 pairs of Chinese-English sentences, including about 2.1 million English words and about 2.5 million Chinese words. With the method described in Section 2.2, we extracted synonyms as shown in Table 2 when the similarity threshold is set to 0.04. Table 2. Synonyms Extracted from the Bilingual corpus Category #Entries Synonyms Noun 26253 10.2 Verb 7364 14.8 For synonym extraction from a monolingual corpus, we use the Wall Street Journal from 1987 to 1992, the size of which is about 500M bytes. In order to get contexts of words, we parse the corpus an English parser –NLPWIN From the parsing results, we extracted the following four types of dependency triples. (a) &lt;verb, OBJ, noun&gt; (b) &lt;verb, SUBJ, noun&gt; (c) &lt;noun, ATTRIB, adjective&gt; (d) &lt;verb, MODS, adjunct&gt; statistics are shown in Table 3. means the total number of triples in the triple set a unique instance of triple in the corpus. These triples are used as contexts of words to calculate the similarity between words as described in Section 2.3. The result is shown in Table 4 when the similarity threshold is set to 0.1. ai i = � ( 1, 2, 3, and ai = is the weight for the 1) NLPWIN parser is developed at Microsoft Research. Its output can be a phrase structure parse tree or a logical form which is represented with dependency triples. Table 3. Statistics for Triples OBJ 7,041,382 1,487,543 SUBJ 7,180,572 2,116,761 ATTRIB 4,976,822 1,393,188 MODS 3,557,737 94,004 Total 22,756,512 5,937,496 Table 4. Synonyms Extracted from the Monolingual Corpus Category Entries Synonyms Noun 16963 4.6 Verb 5084 7.1 4.1 The Gold Standard The simplest evaluation measure is direct comparison of the extracted synonyms with the manually created thesaurus. However, the thesaurus coverage is a problem. In this paper, we combined two thesauri as a gold stardard: WordNet 1.6 http://www.cogsci.princeton.edu/~wn/)and Roget (Roget’s II: The New Thesaurus, 1995. http://www.bartleby.com/thesauri/). In WordNet, one synset consists of several synonyms which represent a single sense. Therefore, a polysemous word occurs in more than one synsets. For example, the polysemous word “abandon” occur in five different synsets: (abandon, forsake, desolate, desert, lurch) (vacate, empty, abandon) (abandon, give up, give) (abandon, give up) (abandon) For a given word, we combine its synonyms from all synsets including the word. Thus, we get the synonyms of the word “ abandon” as follows: desolate, desert, lurch, vacate, empty, give up, give For synonyms in Roget, we also combine the synonyms in different synsets into one set as we do for WordNet. Thus, we get the synonyms of the word “abandon”as follows: off, desist, discontinue, give up, leave off, quit, relinquish, remit, stop, desert, forsake, leave, throw over, abdicate, cede, demit, forswear, hand over, quitclaim, render, renounce, resign, surrender, waive, yield, give over, forgo, lay down Combining the results of WordNet and Roget, we can get the synonyms of the word “abandon” as follows. lurch, vacate, empty, give, abdicate, break off, cede, demit, desert, desist, discontinue, forgo, forsake, forswear, give up, give over, hand over, lay down, lay off, leave off, leave, quit, quitclaim, relinquish, remit, stop, swear off, throw over, render, renounce, resign, surrender, waive, yield 4.2 Evaluation Measures The evaluation metrics are precision, recall, and f-measure. If we use S to indicate the synonyms our method extracts for a word and to denote the synonyms of the word in WordNet and Roget, the methods to calculate the precision, recall, and f-measure of our methods are shown in Equation (7), (8), and (9). To investigate the results of more than one word, we calculate the average precision, recall and f-measure, which sum the individual values divided by the number of the investigated words. |  |S | S |  |S | G f measure = 2 precision  precision recall + 4.3 Test Set In order to evaluate our methods, we build up a test set which includes three parts: (a) high-frequency words: occurring more than 100 times; (b) middle-frequency words: occurring more than 10 times and not greater than 100 times; (c) low-frequency words: occurring no greater than 10 times. Table 5. Statistics for Nouns and Verbs High Frequency Frequency quency Noun 600 2000 1000 Verb 340 1300 800 The frequency counts are estimated from Wall Street Journal (1987-1992), from which we randomly extracted 3600 nouns and 2440 verbs. These</abstract>
<title confidence="0.799377">Table 6. Evaluation Results for Nouns High-Frequency Nouns Middle-Frequency Nouns Low-Frequency Nouns Pre Rec F Pre Rec F Pre Rec F</title>
<phone confidence="0.7937885">1 0.174 0.140 0.155 0.212 0.137 0.167 0.198 0.119 0.149 2 0.225 0.209 0.217 0.242 0.212 0.226 0.207 0.212 0.209 3 0.118 0.109 0.114 0.117 0.104 0.109 0.099 0.096 0.098 1+2+3 0.240 0.201 0.219 0.271 0.220 0.243 0.222 0.232 0.227</phone>
<title confidence="0.862987333333333">Table 7. Evaluation Results for Verbs High-Frequency Verbs Middle-Frequency Verbs Low-Frequency Verbs Pre Rec F Pre Rec F Pre Rec F</title>
<phone confidence="0.71567125">1 0.228 0.243 0.235 0.272 0.233 0.251 0.209 0.216 0.212 2 0.226 0.312 0.262 0.224 0.292 0.253 0.184 0.275 0.220 3 0.143 0.166 0.154 0.162 0.127 0.142 0.128 0.135 0.132 1+2+3 0.295 0.323 0.308 0.311 0.304 0.307 0.238 0.302 0.266</phone>
<abstract confidence="0.7808415">Note: 1, 2, and 3 represent the extractor using the monolingual dictionary, the bilingual corpus, and the monolingual corpus respectively. The symbols “Pre”, “Rec”, and “F” represent precision, recall, and f-measure scores.</abstract>
<note confidence="0.825324892857143">Precision 0.45 0.35 0.25 0.15 0.05 0.2 0.1 0.4 0.3 0 0 0.1 0.2 0.3 0.4 0.5 Recall 2 1 3 1+2+3 Precision 0.45 0.35 0.25 0.15 0.05 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall 2 1 3 1+2+3</note>
<abstract confidence="0.984032703225806">Figure 1. Recall-Precision curves for nouns Figure 2. Recall-Precision curves for verbs words have synonyms both in our results extracted from the three resources and in the thesauri WordNet and Roget. The statistics of the test set are shown in Table 5. 4.4 Experimental Results In this section, we compare the extracted synonyms of the nouns and verbs in the test set with those in WordNet and Roget. For each method, we select those as synonyms whose similarity scores with the investigated word are larger than a given threshold. A development set is used to determine the thresholds of each method. The thresholds for getting highest f-measure scores on the development set are selected. In our experiments, we get 0.04, 0.04, 0.1 and 0.04 for Method 1, Method 2, Method 3 and the combined approach, respectively. The evaluation results for the individual extractors and the ensemble extractor are shown in Table 6 Table 7. We set and in Equation (6) for the ensemble to combine the results from the three resources. The weights are also obtained with the development set. In order to examine the performance of each method in more details, we also get the precisions and recalls under different thresholds. Figure 1 and Figure 2 shows the precision values under different recall values (different thresholds) for all nouns and verbs, respectively. Among all of the methods, the method combining all of the three resources gets the best results in terms of both precision and recall. The effect is similar to the ensemble methods for synonym extraction in (Curran 2002). However, our method uses an ensemble of different resources instead of one single resource. During the experiments, we also find the ensemble combining all of the three extractors outperforms the ensembles only combining any two of the three extractors. This indicates that the extractors using the three different resources are complementary to each other. For example, the extractor using the monolingual dictionary gets a high precision and the extractor using the bilingual corpus gets a high recall. Although the extractor using the monolingual corpus achieved much lower precision and recall on synonym extraction, it is still useful to be included in the ensemble. This shows that the monolingual corpus is complementary to the other two resources on synonym extraction. The success of our method also indicates that our ensemble method by weighting all extractors is effective for synonym extraction. Among the methods only using one kind of resource, Method 2, which uses the bilingual corpus, has the highest f-measure scores on both nouns and verbs. From the results in Figure 1 and Figure 2, we can see that the coverage of synonyms extracted by Method 2 is the highest. Although it has lower precisions than Method 1 under low recalls, its precisions are higher than those of Method 1 under higher recalls. This shows that Method 2 can get a good compromise between precision and recall. We also note that the maximum recall of Method 2 is much larger than that of Method 1. This is because (1) in Method 1, the words used in the definitions are highly limited. Thus, the coverage of the synonyms is limited; (2) the advantage of Method 2 is that the coverage of extracted synonyms is high because it can extract the synonyms not occurring in the corpus. It is different from the method in (Barzilay and Mckeown, 2001; Shimohata and Sumita, 2002), which can only extract the synonyms in the bilingual corpus. The performance of Method 3 is the worst. It is caused by two factors: (1) the context model of Method 3 introduces much noise because of the errors of the parser; (2) this method is unable to distinguish synonyms, antonyms, and similar words because they tend to have similar contexts. From the contexts it uses, method 3 is suitable to extract related words which have the similar usages from the view of syntax. This paper uses three different methods and resources for synonym extraction. By using the corpus-based method, we can get some synonyms or near synonyms which can not be found in the thesauri. For Example: “handspring “audiology “roisterer and “parmesan These kinds of synonyms are difficult for hand-built thesauri to cover because they occur too infrequent to be caught by humans. In addition, this corpus-based method can get synonyms in specific domains while the general thesauri don’t provide such fine-grained knowledge. Comparing the results with the human-built thesauri is not the best way to evaluate synonym extraction because the coverage of the human-built thesaurus is also limited. However, manually evaluating the results is time consuming. And it also cannot get the precise evaluation of the extracted synonyms. Although the human-built thesauri cannot help to precisely evaluate the results, they can still be used to detect the effectiveness of extraction methods. Conclusion This paper proposes a new method to extract synonyms from three resources: a monolingual dictionary, a bilingual corpus, and a large monolingual corpus. This method uses a weighted ensemble to combine all of the results of the individual extractors using one of the three resources respectively. Experimental results prove that the three resources are complementary to each other on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls when the results are compared with the manually-built thesauri WordNet and Roget. Further, we also propose a new method to extract synonyms from a bilingual corpus. This method uses the translations of a word to represent its meaning. The translation probabilities are trained with the bilingual corpus. The advantage of this method is that it can improve the coverage of the extracted synonyms. Experiments indicate that this method outperforms the other methods using a monolingual corpus or a monolingual dictionary. The contribution of this work lies in three aspects: (1) develop a method to combine the results of individual extractors using the three resources on synonym extraction; (2) investigate the performance of the three extraction methods using different resources, exposing the merits and demerits of each method; (3) propose a new method to extract synonyms from a bilingual corpus, which greatly improves the coverage of the extracted synonyms. References R. and Elhadad M. 1997. lexical chains text summarization. proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pp10-17. R. and McKeown K. 2001. Parafrom a Parallel Corpus. Proc. of ACL/EACL. V. D. and Sennelart P. 2002. exof synonyms in a In Proc. of the SIAM Workshop on Text Mining. C. J. and Yang B. 1992. in autostatistical thesaurus construction. Proc. of Annual International ACM SIGIR conference on Research and Development in Information</abstract>
<note confidence="0.953814764705882">Retrieval, pp77-88. J. 2002 Methods for Automatic The- Extraction. Proc. of the Conference on Empirical Methods in Natural Language Processing. pp. 222-229. T. 2000. Methods in Machine In Proc. of the First International Workshop on Multiple Classier Systems. pp 1-15. P., Mckeown K. 1997. Technical Wordand Term- Translation Aid Using Noisy Parallel Corpora Language Groups. Machine Translation, Vol.1-2 (special issue), pp53-87. Gasperin C., Gamallo P., Agustini A., Lopes G., Lima 2001 Syntactic Contexts for Measuring Similarity. on Knowledge Acquisition &amp; Categorization, ESSLLI. G. 1994 in Automatic The- Discovery. Academic Press. D. 1990. Classification from Predi- Structure. Proc. of the Annual Meeting of the Association for Computational Linguistics. Y., Kurohashi S., Kido F. 2002. Navigator&amp;quot;: A Question Answering System Based on Text Knowledge In Proc. of the 19th International Conference on Computational Linguistics. I. and Knight K. 1998. that Ex- Corpus-based Statistical In Proc. of the COLING-ACL. D. 1998 Retrieval and Clustering of Words. Proc. of the Annual Meeting of the Association for Computational Linguistics. R., Tokunaga T. Tanaka H. 1999.</note>
<title confidence="0.830616">Multiple Evidence from Different Type of Thesaurus</title>
<abstract confidence="0.493614">Query In Proc. of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. Y.C. and Choi K. S. 1997. Thesaurus Using Baysian Networks.</abstract>
<note confidence="0.6838731">Processing &amp; Management. Vol. 32. Radev D., Qi H., Zheng Z., Goldensohn S., Zhang Z., W., Prager J. 2001. the Web for Answers Natural Language In the Tenth International ACM Conference on Information and Knowl- M. and Sumita E. 2002. Paraphrasing Based on Parallel Corpus for Normalization. In Proc. of the Third International Conference on Language Resources and Evaluation. W., Huang J., Zhou M., Huang C.N. 2001. Find-</note>
<title confidence="0.820665">ing Target Language Correspondence for Lexicalized System. Proc. of the Natural Language Processing Pacific Rim Symposium.</title>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="1663" citStr="Barzilay and Elhadad, 1997" startWordPosition="245" endWordPosition="248"> of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem </context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Barzilay R. and Elhadad M. 1997. Using lexical chains for text summarization. In proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pp10-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Extracting Paraphrases from a Parallel Corpus.</title>
<date>2001</date>
<booktitle>In Proc. of ACL/EACL.</booktitle>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Barzilay R. and McKeown K. 2001. Extracting Paraphrases from a Parallel Corpus. In Proc. of ACL/EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V D Blondel</author>
<author>P Sennelart</author>
</authors>
<title>Automatic extraction of synonyms in a dictionary.</title>
<date>2002</date>
<booktitle>In Proc. of the SIAM Workshop on Text Mining.</booktitle>
<contexts>
<context position="2764" citStr="Blondel and Sennelart (2002)" startWordPosition="417" endWordPosition="420">rin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002) showed that the ensemble extractors outperformed the individual extractors, it still cannot overcome the deficiency of the methods using the monolingual corpus. To overcome the deficiencies of the methods using</context>
<context position="5521" citStr="Blondel and Sennelart, 2002" startWordPosition="850" endWordPosition="853">ine the individual extractors. The advantage of our approach is that it can combine the merits of the individual extractors to improve the precision and recalls of the extracted synonyms. 2.1 Synonym Extraction with a Monolingual Dictionary This section proposes a method to extract synonyms from a monolingual dictionary. In a monolingual dictionary, each entry is defined by other words and may also be used in the definitions for other words. For a word in the dictionary, the words used to define it are called hubs and the words whose definitions include this word are called authorities as in (Blondel and Sennelart, 2002). We use the hubs and authorities of a word to represent its meaning. The assumption behind this method is that two words are similar if they have common hubs and authorities. In this paper, we only use content words as members of hubs and authorities. We take these hubs and authorities as features of a word. The vector constructed with them is referred to as the feature vector of a word. The similarity between two words is calculated through their feature vectors with the cosine measure as shown in Equation (1). v2 2 j (1) where Fi =&lt; (wi1, vi1 ), (wi2 , vi2 ), ... (wim ,vim) &gt; Fi is the feat</context>
<context position="12150" citStr="Blondel and Sennelart, 2002" startWordPosition="2070" endWordPosition="2073">nym of a investigated word, it tend to be a real synonym. This method can ensure it has a high similarity score. Thus, it will improve the precision of the extracted synonyms. (2) With this method, it can improve the coverage of the extracted synonyms. This is because if the similarity score of a candidate with the investigated word is higher than a threshold, our method can select the candidate as a synonym even though it is only suggested by one extractor. 3 Implementation of Individual Extractors For the extractor employing a monolingual dictionary, we use the same online dictionary as in (Blondel and Sennelart, 2002), which is named the Online Plain Text Dictionary. The dictionary consists of 27 HTML files, which is available from the web site http://www.gutenberg.net/. With the method described in Section 2.1, the result for the extracted synonyms is shown in Table 1 when the similarity threshold is set to 0.04. An example is shown as follows: acclimatize: (acclimate, 0.1481; habituate, 0.0976) The numbers in the example are the similarity scores of two words. Table 1. Synonyms Extracted from the Monolingual Dictionary Category # Entries # Average Synonyms Noun 16963 4.7 Verb 5084 7.1 For synonym extract</context>
</contexts>
<marker>Blondel, Sennelart, 2002</marker>
<rawString>Blondel V. D. and Sennelart P. 2002. Automatic extraction of synonyms in a dictionary. In Proc. of the SIAM Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Crouch</author>
<author>B Yang</author>
</authors>
<title>Experiments in automatic statistical thesaurus construction.</title>
<date>1992</date>
<booktitle>In Proc. of the 15th Annual International ACM SIGIR conference on Research and Development in Information Retrieval,</booktitle>
<pages>77--88</pages>
<contexts>
<context position="2088" citStr="Crouch and Yang, 1992" startWordPosition="307" endWordPosition="310">yota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thu</context>
</contexts>
<marker>Crouch, Yang, 1992</marker>
<rawString>Crouch C. J. and Yang B. 1992. Experiments in automatic statistical thesaurus construction. In Proc. of the 15th Annual International ACM SIGIR conference on Research and Development in Information Retrieval, pp77-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Ensemble Methods for Automatic Thesaurus Extraction.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>222--229</pages>
<contexts>
<context position="3034" citStr="Curran (2002)" startWordPosition="464" endWordPosition="465">ut not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002) showed that the ensemble extractors outperformed the individual extractors, it still cannot overcome the deficiency of the methods using the monolingual corpus. To overcome the deficiencies of the methods using only one resource, our approach combines both monolingual and bilingual resources to automatically extract synonymous words. By combining the synonyms extracted by the individual extractors using the three resources, our approach can combine the merits of the individua</context>
<context position="10637" citStr="Curran, 2002" startWordPosition="1795" endWordPosition="1796">nstead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. Weighted mutual information meliorates this effect by adding p(wi , attj) . 2.4 Combining the Three Extractors In terms of combining the outputs of the different methods, the ensemble method is a good candidate. Originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance (Dietterich, 2000). It has been successfully used in many NLP tasks. For example, (Curran, 2002) proved that the ensembles of indisim = atti ÎA (4) 2 * i p 1 J i � ( A Î , att ) j w2 * = , ) p (wi log attj , ) attj , ) (wi (5) att j *, *) N vidual extractors using different contexts in the monolingual corpus improve the performance of synonym extraction. In fact, we can consider the extractors in the previous sections as binary classifiers. Thus, we use the ensemble method to combine the output of the individual extractors described in the previous sections for synonym extraction. The method is described in Equation (6). 3 sim(w1 , w2) = E (ai  simi (w1 , w2)) (6) i=1 where simi (w1, w2</context>
<context position="20390" citStr="Curran 2002" startWordPosition="3438" endWordPosition="3439">e ensemble to combine the results from the three resources. The weights are also obtained with the development set. In order to examine the performance of each method in more details, we also get the precisions and recalls under different thresholds. Figure 1 and Figure 2 shows the precision values under different recall values (different thresholds) for all nouns and verbs, respectively. Among all of the methods, the method combining all of the three resources gets the best results in terms of both precision and recall. The effect is similar to the ensemble methods for synonym extraction in (Curran 2002). However, our method uses an ensemble of different resources instead of one single resource. During the experiments, we also find the ensemble combining all of the three extractors outperforms the ensembles only combining any two of the three extractors. This indicates that the extractors using the three different resources are complementary to each other. For example, the extractor using the monolingual dictionary gets a high precision and the extractor using the bilingual corpus gets a high recall. Although the extractor using the monolingual corpus achieved much lower precision and recall </context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>Curran J. 2002 Ensemble Methods for Automatic Thesaurus Extraction. In Proc. of the Conference on Empirical Methods in Natural Language Processing. pp. 222-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Ensemble Methods in Machine Learning.</title>
<date>2000</date>
<booktitle>In Proc. of the First International Workshop on Multiple Classier Systems.</booktitle>
<pages>1--15</pages>
<contexts>
<context position="10559" citStr="Dietterich, 2000" startWordPosition="1781" endWordPosition="1782"> of the triples including word wi. N: number of triples in the corpus. We use it instead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. Weighted mutual information meliorates this effect by adding p(wi , attj) . 2.4 Combining the Three Extractors In terms of combining the outputs of the different methods, the ensemble method is a good candidate. Originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance (Dietterich, 2000). It has been successfully used in many NLP tasks. For example, (Curran, 2002) proved that the ensembles of indisim = atti ÎA (4) 2 * i p 1 J i � ( A Î , att ) j w2 * = , ) p (wi log attj , ) attj , ) (wi (5) att j *, *) N vidual extractors using different contexts in the monolingual corpus improve the performance of synonym extraction. In fact, we can consider the extractors in the previous sections as binary classifiers. Thus, we use the ensemble method to combine the output of the individual extractors described in the previous sections for synonym extraction. The method is described in Equ</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Dietterich T. 2000. Ensemble Methods in Machine Learning. In Proc. of the First International Workshop on Multiple Classier Systems. pp 1-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K Mckeown</author>
</authors>
<title>A Technical Word- and Term- Translation Aid Using Noisy Parallel Corpora across Language Groups. In:</title>
<date>1997</date>
<booktitle>Machine Translation, Vol.1-2 (special issue),</booktitle>
<pages>53--87</pages>
<contexts>
<context position="9739" citStr="Fung and Mckeown, 1997" startWordPosition="1642" endWordPosition="1645"> expressed using its attributes. In this case, two words are synonymous if they have similar attributes. We use a weighted version of the Dice measure to calculate the similarity of two words. Y_ (W (w,att ) +W (w,att )) 1 k 2 k tt a k W (w1,atti)+ E W( w1 ) attj A w Î ( 2 ) where atti , attj , attk stands for attributes of words. W (wi , attj) indicates the association strength between the attribute attj with the word wi. A(wi) denotes the attribute set of the word wi. The measure used to measure the association strength between a word and its attributes is weighted mutual information (WMI) (Fung and Mckeown, 1997) as described in (5). W(wi,attj) =WMI(wi p ) ( ) i p w *p (attj where p(wi) count(wi , , count(*, r, w) : frequency of the triples having dependency relation r with the word w. count( wi,*,*) : frequency of the triples including word wi. N: number of triples in the corpus. We use it instead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. Weighted mutual information meliorates this effect by adding p(wi , attj) . 2.4 Combining the Three Extractors In terms of combining the outputs of the different me</context>
</contexts>
<marker>Fung, Mckeown, 1997</marker>
<rawString>Fung P., Mckeown K. 1997. A Technical Word- and Term- Translation Aid Using Noisy Parallel Corpora across Language Groups. In: Machine Translation, Vol.1-2 (special issue), pp53-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gasperin</author>
<author>P Gamallo</author>
<author>A Agustini</author>
<author>G Lopes</author>
<author>V Lima</author>
</authors>
<title>Using Syntactic Contexts for Measuring Word Similarity.</title>
<date>2001</date>
<booktitle>Workshop on Knowledge Acquisition &amp; Categorization, ESSLLI.</booktitle>
<contexts>
<context position="2152" citStr="Gasperin et al., 2001" startWordPosition="318" endWordPosition="321">words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Senn</context>
</contexts>
<marker>Gasperin, Gamallo, Agustini, Lopes, Lima, 2001</marker>
<rawString>Gasperin C., Gamallo P., Agustini A., Lopes G., Lima V. 2001 Using Syntactic Contexts for Measuring Word Similarity. Workshop on Knowledge Acquisition &amp; Categorization, ESSLLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Press.</publisher>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette G. 1994 Explorations in Automatic Thesaurus Discovery. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structure.</title>
<date>1990</date>
<booktitle>In Proc. of the 28th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2065" citStr="Hindle, 1990" startWordPosition="305" endWordPosition="306"> al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in th</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle D. 1990. Noun Classification from Predicate-Argument Structure. In Proc. of the 28th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kiyota</author>
<author>S Kurohashi</author>
<author>F Kido</author>
</authors>
<title>Dialog Navigator&amp;quot;: A Question Answering System Based on Large Text Knowledge Base.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th</booktitle>
<contexts>
<context position="1485" citStr="Kiyota et al., 2002" startWordPosition="217" endWordPosition="220">ction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. 1 Introduction This paper addresses the problem of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1</context>
</contexts>
<marker>Kiyota, Kurohashi, Kido, 2002</marker>
<rawString>Kiyota Y., Kurohashi S., Kido F. 2002. &amp;quot;Dialog Navigator&amp;quot;: A Question Answering System Based on Large Text Knowledge Base. In Proc. of the 19th</rawString>
</citation>
<citation valid="false">
<booktitle>International Conference on Computational Linguistics.</booktitle>
<marker></marker>
<rawString>International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that Exploits Corpus-based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of the COLING-ACL.</booktitle>
<contexts>
<context position="1767" citStr="Langkilde and Knight, 1998" startWordPosition="261" endWordPosition="264">parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many w</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. 1998. Generation that Exploits Corpus-based Statistical Knowledge. In Proc. of the COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2167" citStr="Lin, 1998" startWordPosition="323" endWordPosition="324">tify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) us</context>
<context position="10077" citStr="Lin (1998)" startWordPosition="1708" endWordPosition="1709">cates the association strength between the attribute attj with the word wi. A(wi) denotes the attribute set of the word wi. The measure used to measure the association strength between a word and its attributes is weighted mutual information (WMI) (Fung and Mckeown, 1997) as described in (5). W(wi,attj) =WMI(wi p ) ( ) i p w *p (attj where p(wi) count(wi , , count(*, r, w) : frequency of the triples having dependency relation r with the word w. count( wi,*,*) : frequency of the triples including word wi. N: number of triples in the corpus. We use it instead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. Weighted mutual information meliorates this effect by adding p(wi , attj) . 2.4 Combining the Three Extractors In terms of combining the outputs of the different methods, the ensemble method is a good candidate. Originally, the ensemble method is a machine learning technique of combining the outputs of several classifiers to improve the classification performance (Dietterich, 2000). It has been successfully used in many NLP tasks. For example, (Curran, 2002) proved that the ensembles of indisim = </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin D. 1998 Automatic Retrieval and Clustering of Similar Words. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mandala</author>
<author>Tokunaga T Tanaka H</author>
</authors>
<title>Combining Multiple Evidence from Different Type of Thesaurus for Query Expansion.</title>
<date>1999</date>
<booktitle>In Proc. of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<marker>Mandala, H, 1999</marker>
<rawString>Mandala R., Tokunaga T. Tanaka H. 1999. Combining Multiple Evidence from Different Type of Thesaurus for Query Expansion. In Proc. of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Park</author>
<author>K S Choi</author>
</authors>
<title>Automatic Thesaurus Construction Using Baysian Networks.</title>
<date>1997</date>
<journal>Information Processing &amp; Management.</journal>
<volume>32</volume>
<contexts>
<context position="2129" citStr="Park and Choi, 1997" startWordPosition="314" endWordPosition="317">rization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Be</context>
</contexts>
<marker>Park, Choi, 1997</marker>
<rawString>Park Y.C. and Choi K. S. 1997. Automatic Thesaurus Construction Using Baysian Networks. Information Processing &amp; Management. Vol. 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Qi</author>
<author>Z Zheng</author>
<author>S Goldensohn</author>
<author>Z Zhang</author>
<author>W Fan</author>
<author>J Prager</author>
</authors>
<title>Mining the Web for Answers to Natural Language Questions.</title>
<date>2001</date>
<booktitle>In the Tenth International ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="1463" citStr="Radev et al., 2001" startWordPosition="213" endWordPosition="216">her on synonym extraction, and that the ensemble method we used is very effective to improve both precisions and recalls of extracted synonyms. 1 Introduction This paper addresses the problem of extracting synonymous English words (synonyms) from multiple resources: a monolingual dictionary, a parallel bilingual corpus, and a monolingual corpus. The extracted synonyms can be used in a number of NLP applications. In information retrieval and question answering, the synonymous words are employed to bridge the expressions gaps between the query space and the document space (Mandala et al., 1999; Radev et al., 2001; Kiyota et al., 2002). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). In language generation, synonyms are employed to create more varied texts (Langkilde and Knight, 1998). Up to our knowledge, there are few studies investigating the combination of different resources for synonym extraction. However, many studies investigate synonym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 19</context>
</contexts>
<marker>Radev, Qi, Zheng, Goldensohn, Zhang, Fan, Prager, 2001</marker>
<rawString>Radev D., Qi H., Zheng Z., Goldensohn S., Zhang Z., Fan W., Prager J. 2001. Mining the Web for Answers to Natural Language Questions. In the Tenth International ACM Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shimohata</author>
<author>E Sumita</author>
</authors>
<title>Automatic Paraphrasing Based on Parallel Corpus for Normalization.</title>
<date>2002</date>
<booktitle>In Proc. of the Third International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="2554" citStr="Shimohata and Sumita (2002)" startWordPosition="386" endWordPosition="390">nym extraction from only one resource. The most frequently used resource for synonym extraction is large monolingual corpora (Hindle, 1990; Crouch and Yang, 1992; Grefenstatte, 1994; Park and Choi, 1997; Gasperin et al., 2001 and Lin, 1998). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. Other resources are also used for synonym extraction. Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used bilingual corpora to extract synonyms. However, these methods can only extract synonyms which occur in the bilingual corpus. Thus, the extracted synonyms are limited. Besides, Blondel and Sennelart (2002) used monolingual dictionaries to extract synonyms. Although the precision of this method is high, the coverage is low because the result of this method heavily depends on the definitions of words. In order to improve the performance of synonym extraction, Curran (2002) used an ensemble method to combine the results of different methods using a monolingual corpus. Although Curran (2002) </context>
<context position="22251" citStr="Shimohata and Sumita, 2002" startWordPosition="3751" endWordPosition="3754">der low recalls, its precisions are higher than those of Method 1 under higher recalls. This shows that Method 2 can get a good compromise between precision and recall. We also note that the maximum recall of Method 2 is much larger than that of Method 1. This is because (1) in Method 1, the words used in the definitions are highly limited. Thus, the coverage of the synonyms is limited; (2) the advantage of Method 2 is that the coverage of extracted synonyms is high because it can extract the synonyms not occurring in the corpus. It is different from the method in (Barzilay and Mckeown, 2001; Shimohata and Sumita, 2002), which can only extract the synonyms in the bilingual corpus. The performance of Method 3 is the worst. It is caused by two factors: (1) the context model of Method 3 introduces much noise because of the errors of the parser; (2) this method is unable to distinguish synonyms, antonyms, and similar words because they tend to have similar contexts. From the contexts it uses, method 3 is suitable to extract related words which have the similar usages from the view of syntax. 5 Discussions This paper uses three different methods and resources for synonym extraction. By using the corpus-based meth</context>
</contexts>
<marker>Shimohata, Sumita, 2002</marker>
<rawString>Shimohata M. and Sumita E. 2002. Automatic Paraphrasing Based on Parallel Corpus for Normalization. In Proc. of the Third International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Huang</author>
<author>M Zhou</author>
<author>C N Huang</author>
</authors>
<title>Finding Target Language Correspondence for Lexicalized EBMT System.</title>
<date>2001</date>
<booktitle>In Proc. of the 6th Natural Language Processing Pacific Rim Symposium.</booktitle>
<contexts>
<context position="6807" citStr="Wang et al., 2001" startWordPosition="1082" endWordPosition="1085">e word wi; else, vij = 0 ; 2.2 Synonym Extraction with a Bilingual Corpus This section proposes a novel method to extract synonyms from a bilingual corpus. It uses the translations of a word to express its meaning. The assumption of this method is that two words are synonymous if their translations are similar. Given an English word, we get their translations with an English-Chinese bilingual dictionary. Each translation is assigned a translation probability, which is trained with a bilingual English-Chinese corpus based on the result of word alignment. The aligner use the model described in (Wang et al., 2001). In order to deal with the problem of data sparseness, we conduct a simple smoothing by adding 0.5 to the counts of each translation pair as in (2). count c e ( , )0.5 + e) 0.5* | + trans c _ j where count(c, e) represents the co-occurring frequency of the Chinese word c and the English word e in the sentence pairs. count(e) represents the frequency of the English word e occurring in the bilingual corpus. 1 sim ) cos( (w1 F1, F2 , w2) * (v1 v2 i ) j j I w1i =w2 �v 2* 1 i i ( |e c ) p ( count | (2) F F 1 , 2 cos( , pij is the translation probability of the word wi is translated into cij For ex</context>
</contexts>
<marker>Wang, Huang, Zhou, Huang, 2001</marker>
<rawString>Wang W., Huang J., Zhou M., Huang C.N. 2001. Finding Target Language Correspondence for Lexicalized EBMT System. In Proc. of the 6th Natural Language Processing Pacific Rim Symposium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>