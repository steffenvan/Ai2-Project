<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000176">
<title confidence="0.96201">
The Computational Lexical Semantics of Syntagmatic Relations
</title>
<author confidence="0.960721">
Evelyne Viegas, Stephen Beale and Sergei Nirenburg
</author>
<affiliation confidence="0.910815">
New Mexico State University
Computing Research Lab,
</affiliation>
<address confidence="0.7899775">
Las Cruces, NM 88003,
USA
</address>
<email confidence="0.944541">
viegas,sb,sergeiOcrl.nmsu.edu
</email>
<sectionHeader confidence="0.99403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999735461538461">
In this paper, we address the issue of syntagmatic
expressions from a computational lexical semantic
perspective. From a representational viewpoint, we
argue for a hybrid approach combining linguistic and
conceptual paradigms, in order to account for the
continuum we find in natural languages from free
combining words to frozen expressions. In particu-
lar, we focus on the place of lexical and semantic
restricted co-occurrences. From a processing view-
point, we show how to generate/analyze syntag-
matic expressions by using an efficient constraint-
based processor, well fitted for a knowledge-driven
approach.
</bodyText>
<sectionHeader confidence="0.997894" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.970849233333333">
You can take advantage of the chambermaid&apos; is not a
collocation one would like to generate in the context
of a hotel to mean &amp;quot;use the services of.&amp;quot; This is why
collocations should constitute an important part in
the design of Machine Translation or Multilingual
Generation systems.
In this paper, we address the issue of syntagmatic
expressions from a computational lexical semantic
perspective. From a representational viewpoint, we
argue for a hybrid approach combining linguistic and
conceptual paradigms, in order to account for the
continuum we find in natural languages from free
combining words to frozen expressions (such as in
idioms kick the (proverbial) bucket). In particular,
we focus on the representation of restricted seman-
tic and lexical co-occurrences, such as heavy smoker
and professor ... students respectively, that we de-
fine later. From a processing viewpoint, we show
how to generate/analyze syntagmatic expressions by
using an efficient constraint-based processor, well fit-
ted for a knowledge-driven approach. In the follow-
ing, we first compare different approaches to collo-
cations. Second, we present our approach in terms
of representation and processing. Finally, we show
how to facilitate the acquisition of co-occurrences by
using 1) the formalism of lexical rules (LRs), 2) an
&apos;Lederer, R. 1990. Anguished English A Laurel Book, Dell
Publishing.
inheritance hierarchy of Lexical Semantic Functions
(LSFs).
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="method">
2 Approaches to Syntagmatic
Relations
</sectionHeader>
<bodyText confidence="0.999866974358974">
Syntagmatic relations, also known as collocations,
are used differently by lexicographers, linguists and
statisticians denoting almost similar but not identi-
cal classes of expressions.
The traditional approach to collocations has been
lexicographic. Here dictionaries provide infor-
mation about what is unpredictable or idiosyn-
cratic. Benson (1989) synthesizes Hausmann&apos;s stud-
ies on collocations, calling expressions such as com-
mit murder, compile a dictionary, inflict a wound,
etc. &amp;quot;fixed combinations, recurrent combinations&amp;quot;
or &amp;quot;collocations&amp;quot;. In Hausmann&apos;s terms (1979) a
collocation is composed of two elements, a base (&amp;quot;Ba-
sis&amp;quot;) and a collocate (&amp;quot;Kollokator&amp;quot; ); the base is se-
mantically autonomous whereas the collocate cannot
be semantically interpreted in isolation. In other
words, the set of lexical collocates which can com-
bine with a given basis is not predictable and there-
fore collocations must be listed in dictionaries.
It is hard to say that there has been a real focus
on collocations from a linguistic perspective. The
lexicon has been broadly sacrificed by both English-
speaking schools and continental European schools.
The scientific agenda of the former has been largely
dominated by syntactic issues until recently, whereas
the latter was more concerned with pragmatic as-
pects of natural languages. The focus has been on
grammatical collocations such as adapt to, aim at,
look for. Lakoff (1970) distinguishes a class of ex-
pressions which cannot undergo certain operations,
such as nominalization, causativization: the problem
is hard; *the hardness of the problem; *the problem
hardened. The restriction on the application of cer-
tain syntactic operations can help define collocations
such as hard problem, for example. Mel&apos;euk&apos;s treat-
ment of collocations will be detailed below.
In recent years, there has been a resurgence of
statistical approaches applied to the study of nat-
ural languages. Sinclair (1991) states that &amp;quot;a word
</bodyText>
<page confidence="0.988198">
1328
</page>
<bodyText confidence="0.996023029411765">
which occurs in close proximity to a word under in-
vestigation is called a collocate of it. ... Collocation
is the occurrence of two or more words within a
short space of each other in a text&amp;quot;. The prob-
lem is that with such a definition of collocations,
even when improved,2 one identifies not only collo-
cations but free-combining pairs frequently appear-
ing together such as lawyer-client; doctor-hospital.
However, nowadays, researchers seem to agree that
combining statistic with symbolic approaches lead
to quantifiable improvements (Klavans and Resnik,
1996).
The Meaning Text Theory Approach The
Meaning Text Theory (MTT) is a generator-oriented
lexical grammatical formalism. Lexical knowledge is
encoded in an entry of the Explanatory Combina-
torial Dictionary (ECD), each entry being divided
into three zones: the semantic zone (a semantic net-
work representing the meaning of the entry in terms
of more primitive words), the syntactic zone (the
grammatical properties of the entry) and the lexi-
cal combinatorics zone (containing the values of the
Lexical Functions (LFs) 3). LFs are central to the
study of collocations:
A lexical function F is a correspondence
which associates a lexical item L, called the
key word of F, with a set of lexical items
F(L)-the value of F. (Mel&apos;euk, 1988)4
We focus here on syntagmatic LFs describing co-
occurrence relations such as pay attention, legitimate
complaint; from a distance.5
Heylen et al. (1993) have worked out some cases
which help license a starting point for assigning LFs.
They distinguish four types of syntagmatic LFs:
</bodyText>
<listItem confidence="0.996658">
• evaluative qualifier
Magn(bleed) = profusely
• distributional qualifier
Mult (sheep) = flock
• co-occurrence
Loc-in(distance)= at a distance
• verbal operator
</listItem>
<bodyText confidence="0.9487404">
Oper 1 (attention) = pay
The MTT approach is very interesting as it pro-
vides a model of production well suited for genera-
tion with its different strata and also a lot of lexical-
semantic information. It seems nevertheless that all
</bodyText>
<footnote confidence="0.9143268">
2Church and Hanks (1989), Smadja (1993) use statistics
in their algorithms to extract collocations from texts.
3See (Iordanskaja et al., 1991) and (Ramos et al., 1994)
for their use of LFs in MTT and NLG respectively.
4(Heid, 1989) contrasts Hausman&apos;s base and collate to
Mel&apos;euk&apos;s keyword and LF values.
6There are about 60 LFs listed said to be universal; the
lexicographic approach of Mel&apos;Zuk and Zolkovsky has been
applied among other languages to Russian, French, German
and English.
</footnote>
<bodyText confidence="0.9993472">
the collocational information is listed in a static way.
We believe that one of the main drawbacks of the ap-
proach is the lack of any predictable calculi on the
possible expressions which can collocate with each
other semantically.
</bodyText>
<sectionHeader confidence="0.90741" genericHeader="method">
3 The Computational Lexical
</sectionHeader>
<subsectionHeader confidence="0.848924">
Semantic Approach
</subsectionHeader>
<bodyText confidence="0.990764833333333">
In order to account for the continuum we find in nat-
ural languages, we argue for a continuum perspec-
tive, spanning the range from free-combining words
to idioms, with semantic collocations and idiosyn-
crasies in between as defined in (Viegas and Bouil-
lon, 1994):
</bodyText>
<listItem confidence="0.99941625">
• free-combining words (the girl ate candies)
• semantic collocations (fast car; long book)6
• idiosyncrasies (large coke; green jealousy)
• idioms (to kick the (proverbial) bucket)
</listItem>
<bodyText confidence="0.99870975">
Formally, we go from a purely compositional
approach in &amp;quot;free-combining words&amp;quot; to a non-
compositional approach in idioms. In between, a
(semi-)compositional approach is still possible. (Vie-
gas and Bouillon, 1994) showed that we can reduce
the set of what are conventionally considered as id-
iosyncrasies by differentiating &amp;quot;true&amp;quot; idiosyncrasies
(difficult to derive or calculate) from expressions
which have well-defined calculi, being compositional
in nature, and that have been called semantic collo-
cations. In this paper, we further distinguish their
idiosyncrasies into:
</bodyText>
<listItem confidence="0.896278666666667">
• restricted semantic co-occurrence, where
the meaning of the co-occurrence is semi-
compositional between the base and the collo-
cate (strong coffee, pay attention, heavy smoker,
•-•)
• restricted lexical co-occurrence, where the
meaning of the collocate is compositional but
has a lexical idiosyncratic behavior (lecture ...
student; rancid butter; sour milk).
</listItem>
<bodyText confidence="0.984605909090909">
We provide below examples of restricted seman-
tic co-occurrences in (1), and restricted lexical co-
occurrences in (2).
Restricted semantic co-occurrence The se-
mantics of the combination of the entries is semi-
compositional. In other words, there is an entry in -
the lexicon for the base, (the semantic collocate is
encoded inside the base), whereas we cannot directly
refer to the sense of the semantic collocate in the
lexicon, as it is not part of its senses. We assign
the co-occurrence a new semi-compositional sense,
</bodyText>
<footnote confidence="0.985977">
6See (Pustejovsky, 1995) for his account of such expres-
sions using a coercion operator.
</footnote>
<page confidence="0.994278">
1329
</page>
<bodyText confidence="0.9987195">
where the sense of the base is composed with a new
sense for the collocate.
</bodyText>
<table confidence="0.9829175">
(la) #0=[key: &amp;quot;smoker&amp;quot;,
rel: [syntagmatic: LSFIntensity
[base: #0, collocate:
[key: &amp;quot;heavy&amp;quot;,
gram: [subCat: Attributive,
freq: [value: 8]]]]] ...]
(lb) #0=[key: &amp;quot;attention&amp;quot;,
rel: [syntagmatic: LSFOper
[base: #0, collocate:
[key: &amp;quot;pay&amp;quot;,
gram: [subCat: SupportVerb,
freq: [value: 5]]]]] ...]
</table>
<bodyText confidence="0.991765730769231">
In examples (1), the LSFs (LSFIntensity, LS-
FOper, ...) are equivalent (and some identical) to
the LFs provided in the ECD. The notion of LSF
is the same as that of LFs. However, LSFs and
LFs are different in two ways: i) conceptually, LSFs
are organized into an inheritance hierarchy; ii) for-
mally, they are rules, and produce a new entry com-
posed of two entries, the base with the collocate.
As such, the new composed entry is ready for pro-
cessing. These LSFs signal a compositional syntax
and a semi-compositional semantics. For instance,
in (la), a heavy smoker is somebody who smokes a
lot, and not a &amp;quot;fat&amp;quot; person. It has been shown that
one cannot code in the lexicon all uses of heavy for
heavy smoker, heavy drinker, .... Therefore, we do
not have in our lexicon for heavy a sense for &amp;quot;a lot&amp;quot;,
or a sense for &amp;quot;strong&amp;quot; to be composed with wine,
etc... It is well known that such co-occurrences are
lexically marked; if we allowed in our lexicons a pro-
liferation of senses, multiplying ambiguities in anal-
ysis and choices in generation, then there would be
no limit to what could be combined and we could
end up generating *heavy coffee with the sense of
&amp;quot;strong&amp;quot; for heavy, in our lexicon.
The left hand-side of the rule LSFIntensity spec-
ifies an &amp;quot;Intensity-Attribute&amp;quot; applied to an event
which accepts aspectual features of duration. In
(la), the event is smoke. The LSFIntensity also
provides the syntax-semantic interface, allowing for
an Adj-Noun construction to be either predicative
(the car is red) or attributive (the red car). We
need therefore to restrict the co-occurrence to the
Attributive use only, as the predicative use is not
allowed: (the smoker is heavy) has a literal meaning
or figurative, but not collocational.
In (lb) again, there is no sense in the dictionary
for pay which would mean concentrate. The rule LS-
FOper makes the verb a verbal operator. No further
restriction is required.
Restricted lexical co-occurrence The seman-
tics of the combination of the entries is composi-
tional. In other words, there are entries in the lex-
icon for the base and the collocate, with the same
senses as in the co-occurrence. Therefore, we can di-
rectly refer to the senses of the co-occurring words.
What we are capturing here is a lexical idiosyncrasy
or in other words, we specify that we should prefer
this particular combination of words. This is useful
for analysis, where it can help disambiguate a sense,
and is most relevant for generation; it can be viewed
as a preference among the paradigmatic family of
the co-occurrence.
</bodyText>
<table confidence="0.920078533333333">
(2a) #0=[key: &amp;quot;truth&amp;quot;,
rel: [syntagmatic: LSFSyn
[base: #0, collocate:
[key: &amp;quot;plain&amp;quot;, sense: adj2,
lr: [comp:no, superl:no]]]] ...]
(2b) #0=[key: &amp;quot;pupil&amp;quot;,
rel: (syntagmatic: LSFSyn
[base: #0, collocate:
[key: &amp;quot;teacher&amp;quot;, sense: n2,
freq: [value: 511]]...]
(2c) #0=[key: &amp;quot;conference&amp;quot;,
rel: [syntagmatic: LSFSyn
[base: #0, collocate:
[key: &amp;quot;student&amp;quot;, sense: ml,
freq: [value: 9]]]] ...]
</table>
<bodyText confidence="0.999653964285714">
In examples (2), the LSFSyn produces a new en-
try composed of two or more entries. As such, the
new entry is ready for processing. LSFSyn signals
a compositional syntax and a compositional seman-
tics, and restricts the use of lexemes to be used in
the composition. We can directly refer to the sense
of the collocate, as it is part of the lexicon.
In (2a) the entry for truth specifies one co-
occurrence (plain truth), where the sense of plain
here is adj2 (obvious), and not say adj3 (fiat). The
syntagmatic expression inherits all the zones of the
entry for &amp;quot;plain&amp;quot;, sense adj2, we only code here the
irregularities. For instance, &amp;quot;plain&amp;quot; can be used
as &amp;quot;plainer&amp;quot; &amp;quot;plainest&amp;quot; in its &amp;quot;plain&amp;quot; sense in its
adj2 entry, but not as such within the lexical co-
occurrence &amp;quot;*plainer truth&amp;quot;, &amp;quot;*plainest truth&amp;quot;, we
therefore must block it in the collocate, as expressed
in (comp: no, super!: no). In other words, we will
not generate &amp;quot;plainer/plainest truth&amp;quot;. Examples
(2b) and (2c) illustrate complex entries as there is
no direct grammatical dependency between the base
and the collocate. In (2b) for instance, we prefer
to associate teacher in the context of a pupil rather
than any other element belonging to the paradig-
matic family of teacher such as professor, instructor.
Formally, there is no difference between the two
types of co-occurrences. In both cases, we specify
the base (which is the word described in the en-
</bodyText>
<page confidence="0.977859">
1330
</page>
<bodyText confidence="0.9997948">
try itself), the collocate, the frequency of the co-
occurrence in some corpus, and the LSF which links
the base with the collocate. Using the formalism
of typed feature structures, both cases are of type
Co-occurrence as defined below:
</bodyText>
<note confidence="0.602325666666667">
Co-occurrence = [base: Entry,
collocate: Entry,
freq: Frequency];
</note>
<subsectionHeader confidence="0.999938">
3.1 Processing of Syntagmatic Relations
</subsectionHeader>
<bodyText confidence="0.999954093023256">
We utilize an efficient constraint-based control mech-
anism called Hunter-Gatherer (HG) (Beale, 1997).
HG allows us to mark certain compositions as be-
ing dependent on each other and then forget about
them. Thus, once we have two lexicon entries
that we know go together, HG will ensure that
they do. HG also gives preference to co-occurring
compositions. In analysis, meaning representations
constructed using co-occurrences are preferred over
those that are not, and, in generation, realizations
involving co-occurrences are preferred over equally
correct, but non-cooccurring realizations.&apos;
The real work in processing is making sure that we
have the correct two entries to put together. In re-
stricted semantic co-occurrences, the co-occurrence
does not have the correct sense in the lexicon. For
example, when the phrase heavy smoker is encoun-
tered, the lexicon entry for heavy would not contain
the correct sense. (la) could be used to create the
correct entry. In (la), the entry for smoker contains
the key, or trigger, heavy. This signals the analyzer
to produce another sense for heavy smoker. This
sense will contain the same syntactic information
present in the &amp;quot;old&amp;quot; heavy, except for any modifi-
cations listed in the &amp;quot;gram&amp;quot; section (see (la)). The
semantics of the new sense comes directly from the
LSF. Generation works the same, except the trig-
ger is different. The input to generation will be a
SMOKE event along with an Intensity-Attribute.
(la), which would be used to realize the SMOKE
event, would trigger LSFIntensify which has the
Intensity-Attribute in the left hand-side, thus con-
firming the production of heavy.
Restricted lexical co-occurrences are easier in the
sense that the correct entry already exists in the lexi-
con. The analyzer/generator simply needs to detect
the co-occurrence and add the constraint that the
corresponding senses be used together. In examples
like (2b), there is no direct grammatical or semantic
relationship between the words that co-occur. Thus,
the entire clause, sentence or even text may have to
be searched for the co-occurrence. In practice, we
limit such searches to the sentence level.
</bodyText>
<footnote confidence="0.7281444">
7The selection of co-occurrences is part of the lexical pro-
cess, in other words, if there are reasons not to choose a co-
occurrence because of the presence of modifiers or because
of stylistics reasons, the generator will not generate the co-
occurrence.
</footnote>
<subsectionHeader confidence="0.998531">
3.2 Acquisition of Syntagmatic Relations
</subsectionHeader>
<bodyText confidence="0.984231090909091">
The acquisition of syntagmatic relations is knowl-
edge intensive as it requires human intervention. In
order to minimize this cost we rely on conceptual
tools such as lexical rules, on the LSF inheritance
hierarchy.
Lexical Rules in Acquisition The acquisition of
restricted semantic co-occurrences can be minimized
by detecting rules between different classes of co-
occurrences (modulo presence of derived forms in the
lexicon with same or subsumed semantics). Looking
at the following example,
</bodyText>
<table confidence="0.420895857142857">
A + N &lt;=&gt; V + Adv
bitter resentment resent bitterly
heavy smoker smoke heavily
big eater eat *bigly
V + Adv &lt;=&gt; Adv + Adj-ed
oppose strongly strongly opposed
oblige morally morally obliged
</table>
<bodyText confidence="0.946995136363637">
we see that after having acquired with human in-
tervention co-occurrences belonging to the A + N
class, we can use lexical rules to derive the V + Adv
class and also Adv + Adj-ed class.
Lexical rules are a useful conceptual tool to extend
a dictionary. (Viegas et al., 1996) used derivational
lexical rules to extend a Spanish lexicon. We ap-
ply their approach to the production of restricted
semantic co-occurrences. Note that eat bigly will be
produced but then rejected, as the form bigly does
not exist in a dictionary. The rules overgenerate co-
occurrences. This is a minor problem for analysis
than for generation. To use these derived restricted
co-occurrences in generation, the output of the lexi-
cal rule processor must be checked. This can be done
in different ways: dictionary check, corpus check and
ultimately human check.
Other classes, such as the ones below can be
extracted using lexico-statistical tools, such as in
(Smadja, 1993), and then checked by a human.
V + N pay attention, meet an obligation,
commit an offence, • • •
</bodyText>
<subsectionHeader confidence="0.5618645">
N + N dance marathon, marriage ceremony
object of derision, ...
</subsectionHeader>
<bodyText confidence="0.998866444444445">
LSFs and Inheritance We take advantage of 1)
the semantics encoded in the lexemes, and 2) an in-
heritance hierarchy of LSFs. We illustrate briefly
this notion of LSF inheritance hierarchy. For in-
stance, the left hand-side of LSFChangeState spec-
ifies that it applies to foods (solid or liquid) which
are human processed, and produces the collocates
rancid, rancio (Spanish). Therefore it could apply
to milk, butter, or wine. The rule would end up
</bodyText>
<page confidence="0.972803">
1331
</page>
<bodyText confidence="0.999689090909091">
producing rancid milk, rancid butter, or vino rancio
(rancid wine) which is fine in Spanish. We therefore
need to further distinguish LSFChangeState into
LSFChangeStateSolid and LSFChangeStateLiquid.
This restricts the application of the rule to produce
rancid butter, by going down the hierarchy. This
enables us to factor out information common to sev-
eral entries, and can be applied to both types of
co-occurrences. We only have to code in the co-
occurrence information relevant to the combination,
the rest is inherited from its entry in the dictionary.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999855">
In this paper, we built on a continuum perspec-
tive, knowledge-based, spanning the range from free-
combining words to idioms. We further distin-
guished the notion of idiosyncrasies as defined in
(Viegas and Bouillon, 1994), into restricted semantic
co-occurrences and restricted lexical co-occurrences.
We showed that they were formally equivalent, thus
facilitating the processing of strictly compositional
and semi-compositional expressions. Moreover, by
considering the information in the lexicon as con-
straints, the linguistic difference between composi-
tionality and semi-compositionality becomes a vir-
tual difference for Hunter-Gatherer. We showed
ways of minimizing the acquisition costs, by 1) using
lexical rules as a way of expanding co-occurrences, 2)
taking advantage of the LSF inheritance hierarchy.
The main advantage of our approach over the ECD
approach is to use the semantics coded in the lex-
emes along with the language independent LSF in-
heritance hierarchy to propagate restricted semantic
co-occurrences. The work presented here is complete
concerning representational aspects and processing
aspects (analysis and generation): it has been tested
on the translations of on-line unrestricted texts. The
large-scale acquisition of restricted co-occurrences is
in progress.
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997212">
This work has been supported in part by DoD under
contract number MDA-904-92-C-5189. We would
like to thank Pierrette Bouillon, Leo Wanner and
Remi Zajac for helpful discussions and the anony-
mous reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999829163934427">
S. Beale. 1997. HUNTER-GATHERER: Applying
Constraint Satisfaction, Branch-and-Bound and
Solution Synthesis to Computational Semantics.
Ph.D. Diss., Carnegie Mellon University.
M. Benson. 1989. The Structure of the Colloca-
tional Dictionary. In International Journal of Lex-
icography.
K.W. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicogra-
phy. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics.
F.J. Hausmann. 1979. Un dictionnaire des colloca-
tions est-il possible ? In Travaux de Linguistique
et de Litterature XVII, 1.
U. Heid. 1979. Decrire les collocations : deux ap-
proches lexicographigues et leur application dans
un outil informatise. Internal Report, Stuttgart
University.
D. Heylen. 1993. Collocations and the Lexicalisa-
tion of Semantic Information. In Collocations, TR
ET-10/75, Taaltechnologie, Utrecht.
L. Iordanskaja, R. Kittredge and A. Folguere. 1991.
Lexical Selection and Paraphrase in a Meaning-
text Generation Model. In C. L. Paris, W.
Swartout and W. Mann (eds), NLG in Al and
CL. Kluwer Academic Publishers.
J. Klavans and P. Resnik. 1996. The Balancing Act,
Combining Symbolic and Statistical Approaches to
Language. MIT Press, Cambridge Mass., London
England.
G. Lakoff. 1970. Irregularities in Syntax. New York:
Holt, Rinehart and Winston, Inc.
I. Mel&apos;euk. 1988. Paraphrase et lexique dans la
theorie Sens-Texte. In Bes &amp; Fuchs (ed) Lexigue6.
S. Nirenburg and I. Nirenburg. 1988. A Framework
for Lexical Selection in NLG. In Proceedings of
COLING 88.
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
M. Ramos, A. Tutin and G. Lapalme. 1994. Lexical
Functions of Explanatory Combinatorial Dictio-
nary for Lexicalization in Text Generation. In P.
St-Dizier &amp; E. Viegas (Ed) Computational Lexical
Semantics: CUP.
J. Sinclair. 1991. Corpus, Concordance, Colloca-
tions. Oxford University Press.
F. Smadja. 1993. Retrieving Collocations from
Texts: Xtract. Computational Linguistics, 19(1).
E. Viegas and P. Bouillon. 1994. Semantic Lexi-
cons: the Cornerstone for Lexical Choice in Nat-
ural Language Generation. In Proceedings of the
7th INLG, Kennebunkport.
E. Viegas, B. Onyshkevych, V. Raskin and S. Niren-
burg. 1996. From Submit to Submitted via Sub-
mission: on Lexical Rules in Large-scale Lexi-
con Acquisition. In Proceedings of the 34th An-
nual Meeting of the Association for Computa-
tional Linguists.
L. Wanner. 1996. Lexical Functions in Lexicography
and Natural Language Processing. John Benjamin
Publishing Company.
</reference>
<page confidence="0.99447">
1332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890889">
<title confidence="0.999772">The Computational Lexical Semantics of Syntagmatic Relations</title>
<author confidence="0.998051">Evelyne Viegas</author>
<author confidence="0.998051">Stephen Beale</author>
<author confidence="0.998051">Sergei Nirenburg</author>
<affiliation confidence="0.9996205">New Mexico State University Computing Research Lab,</affiliation>
<address confidence="0.9965075">Las Cruces, NM 88003, USA</address>
<email confidence="0.999612">viegas,sb,sergeiOcrl.nmsu.edu</email>
<abstract confidence="0.992602928571429">In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective. From a representational viewpoint, we argue for a hybrid approach combining linguistic and conceptual paradigms, in order to account for the continuum we find in natural languages from free combining words to frozen expressions. In particular, we focus on the place of lexical and semantic restricted co-occurrences. From a processing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraintbased processor, well fitted for a knowledge-driven approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Beale</author>
</authors>
<title>HUNTER-GATHERER: Applying Constraint Satisfaction, Branch-and-Bound and Solution Synthesis to Computational Semantics.</title>
<date>1997</date>
<institution>Mellon University.</institution>
<location>Ph.D. Diss., Carnegie</location>
<contexts>
<context position="14169" citStr="Beale, 1997" startWordPosition="2227" endWordPosition="2228">essor, instructor. Formally, there is no difference between the two types of co-occurrences. In both cases, we specify the base (which is the word described in the en1330 try itself), the collocate, the frequency of the cooccurrence in some corpus, and the LSF which links the base with the collocate. Using the formalism of typed feature structures, both cases are of type Co-occurrence as defined below: Co-occurrence = [base: Entry, collocate: Entry, freq: Frequency]; 3.1 Processing of Syntagmatic Relations We utilize an efficient constraint-based control mechanism called Hunter-Gatherer (HG) (Beale, 1997). HG allows us to mark certain compositions as being dependent on each other and then forget about them. Thus, once we have two lexicon entries that we know go together, HG will ensure that they do. HG also gives preference to co-occurring compositions. In analysis, meaning representations constructed using co-occurrences are preferred over those that are not, and, in generation, realizations involving co-occurrences are preferred over equally correct, but non-cooccurring realizations.&apos; The real work in processing is making sure that we have the correct two entries to put together. In restrict</context>
</contexts>
<marker>Beale, 1997</marker>
<rawString>S. Beale. 1997. HUNTER-GATHERER: Applying Constraint Satisfaction, Branch-and-Bound and Solution Synthesis to Computational Semantics. Ph.D. Diss., Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
</authors>
<title>The Structure of the Collocational Dictionary.</title>
<date>1989</date>
<journal>In International Journal of Lexicography.</journal>
<contexts>
<context position="2676" citStr="Benson (1989)" startWordPosition="387" endWordPosition="388">cilitate the acquisition of co-occurrences by using 1) the formalism of lexical rules (LRs), 2) an &apos;Lederer, R. 1990. Anguished English A Laurel Book, Dell Publishing. inheritance hierarchy of Lexical Semantic Functions (LSFs). 2 Approaches to Syntagmatic Relations Syntagmatic relations, also known as collocations, are used differently by lexicographers, linguists and statisticians denoting almost similar but not identical classes of expressions. The traditional approach to collocations has been lexicographic. Here dictionaries provide information about what is unpredictable or idiosyncratic. Benson (1989) synthesizes Hausmann&apos;s studies on collocations, calling expressions such as commit murder, compile a dictionary, inflict a wound, etc. &amp;quot;fixed combinations, recurrent combinations&amp;quot; or &amp;quot;collocations&amp;quot;. In Hausmann&apos;s terms (1979) a collocation is composed of two elements, a base (&amp;quot;Basis&amp;quot;) and a collocate (&amp;quot;Kollokator&amp;quot; ); the base is semantically autonomous whereas the collocate cannot be semantically interpreted in isolation. In other words, the set of lexical collocates which can combine with a given basis is not predictable and therefore collocations must be listed in dictionaries. It is hard t</context>
</contexts>
<marker>Benson, 1989</marker>
<rawString>M. Benson. 1989. The Structure of the Collocational Dictionary. In International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6277" citStr="Church and Hanks (1989)" startWordPosition="952" endWordPosition="955">s pay attention, legitimate complaint; from a distance.5 Heylen et al. (1993) have worked out some cases which help license a starting point for assigning LFs. They distinguish four types of syntagmatic LFs: • evaluative qualifier Magn(bleed) = profusely • distributional qualifier Mult (sheep) = flock • co-occurrence Loc-in(distance)= at a distance • verbal operator Oper 1 (attention) = pay The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information. It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts. 3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in MTT and NLG respectively. 4(Heid, 1989) contrasts Hausman&apos;s base and collate to Mel&apos;euk&apos;s keyword and LF values. 6There are about 60 LFs listed said to be universal; the lexicographic approach of Mel&apos;Zuk and Zolkovsky has been applied among other languages to Russian, French, German and English. the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any p</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>K.W. Church and P. Hanks. 1989. Word Association Norms, Mutual Information and Lexicography. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Hausmann</author>
</authors>
<title>Un dictionnaire des collocations est-il possible ?</title>
<date>1979</date>
<booktitle>In Travaux de Linguistique et de Litterature XVII,</booktitle>
<volume>1</volume>
<marker>Hausmann, 1979</marker>
<rawString>F.J. Hausmann. 1979. Un dictionnaire des collocations est-il possible ? In Travaux de Linguistique et de Litterature XVII, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Heid</author>
</authors>
<title>Decrire les collocations : deux approches lexicographigues et leur application dans un outil informatise.</title>
<date>1979</date>
<tech>Internal Report,</tech>
<institution>Stuttgart University.</institution>
<marker>Heid, 1979</marker>
<rawString>U. Heid. 1979. Decrire les collocations : deux approches lexicographigues et leur application dans un outil informatise. Internal Report, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Heylen</author>
</authors>
<title>Collocations and the Lexicalisation of Semantic Information.</title>
<date>1993</date>
<booktitle>In Collocations, TR ET-10/75,</booktitle>
<location>Taaltechnologie, Utrecht.</location>
<marker>Heylen, 1993</marker>
<rawString>D. Heylen. 1993. Collocations and the Lexicalisation of Semantic Information. In Collocations, TR ET-10/75, Taaltechnologie, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Iordanskaja</author>
<author>R Kittredge</author>
<author>A Folguere</author>
</authors>
<title>Lexical Selection and Paraphrase in a Meaningtext Generation Model. In</title>
<date>1991</date>
<booktitle>NLG in Al and CL.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="6395" citStr="Iordanskaja et al., 1991" startWordPosition="969" endWordPosition="972">license a starting point for assigning LFs. They distinguish four types of syntagmatic LFs: • evaluative qualifier Magn(bleed) = profusely • distributional qualifier Mult (sheep) = flock • co-occurrence Loc-in(distance)= at a distance • verbal operator Oper 1 (attention) = pay The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information. It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts. 3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in MTT and NLG respectively. 4(Heid, 1989) contrasts Hausman&apos;s base and collate to Mel&apos;euk&apos;s keyword and LF values. 6There are about 60 LFs listed said to be universal; the lexicographic approach of Mel&apos;Zuk and Zolkovsky has been applied among other languages to Russian, French, German and English. the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other semantically. 3 The Computational L</context>
</contexts>
<marker>Iordanskaja, Kittredge, Folguere, 1991</marker>
<rawString>L. Iordanskaja, R. Kittredge and A. Folguere. 1991. Lexical Selection and Paraphrase in a Meaningtext Generation Model. In C. L. Paris, W. Swartout and W. Mann (eds), NLG in Al and CL. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>P Resnik</author>
</authors>
<title>The Balancing Act, Combining Symbolic and Statistical Approaches to Language.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge Mass., London England.</location>
<contexts>
<context position="4847" citStr="Klavans and Resnik, 1996" startWordPosition="722" endWordPosition="725"> of natural languages. Sinclair (1991) states that &amp;quot;a word 1328 which occurs in close proximity to a word under investigation is called a collocate of it. ... Collocation is the occurrence of two or more words within a short space of each other in a text&amp;quot;. The problem is that with such a definition of collocations, even when improved,2 one identifies not only collocations but free-combining pairs frequently appearing together such as lawyer-client; doctor-hospital. However, nowadays, researchers seem to agree that combining statistic with symbolic approaches lead to quantifiable improvements (Klavans and Resnik, 1996). The Meaning Text Theory Approach The Meaning Text Theory (MTT) is a generator-oriented lexical grammatical formalism. Lexical knowledge is encoded in an entry of the Explanatory Combinatorial Dictionary (ECD), each entry being divided into three zones: the semantic zone (a semantic network representing the meaning of the entry in terms of more primitive words), the syntactic zone (the grammatical properties of the entry) and the lexical combinatorics zone (containing the values of the Lexical Functions (LFs) 3). LFs are central to the study of collocations: A lexical function F is a correspo</context>
</contexts>
<marker>Klavans, Resnik, 1996</marker>
<rawString>J. Klavans and P. Resnik. 1996. The Balancing Act, Combining Symbolic and Statistical Approaches to Language. MIT Press, Cambridge Mass., London England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Irregularities in Syntax.</title>
<date>1970</date>
<publisher>and Winston, Inc.</publisher>
<location>New York: Holt, Rinehart</location>
<contexts>
<context position="3746" citStr="Lakoff (1970)" startWordPosition="554" endWordPosition="555">ical collocates which can combine with a given basis is not predictable and therefore collocations must be listed in dictionaries. It is hard to say that there has been a real focus on collocations from a linguistic perspective. The lexicon has been broadly sacrificed by both Englishspeaking schools and continental European schools. The scientific agenda of the former has been largely dominated by syntactic issues until recently, whereas the latter was more concerned with pragmatic aspects of natural languages. The focus has been on grammatical collocations such as adapt to, aim at, look for. Lakoff (1970) distinguishes a class of expressions which cannot undergo certain operations, such as nominalization, causativization: the problem is hard; *the hardness of the problem; *the problem hardened. The restriction on the application of certain syntactic operations can help define collocations such as hard problem, for example. Mel&apos;euk&apos;s treatment of collocations will be detailed below. In recent years, there has been a resurgence of statistical approaches applied to the study of natural languages. Sinclair (1991) states that &amp;quot;a word 1328 which occurs in close proximity to a word under investigatio</context>
</contexts>
<marker>Lakoff, 1970</marker>
<rawString>G. Lakoff. 1970. Irregularities in Syntax. New York: Holt, Rinehart and Winston, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel&apos;euk</author>
</authors>
<title>Paraphrase et lexique dans la theorie Sens-Texte.</title>
<date>1988</date>
<booktitle>In Bes &amp; Fuchs (ed) Lexigue6.</booktitle>
<contexts>
<context position="5579" citStr="Mel&apos;euk, 1988" startWordPosition="843" endWordPosition="844">. Lexical knowledge is encoded in an entry of the Explanatory Combinatorial Dictionary (ECD), each entry being divided into three zones: the semantic zone (a semantic network representing the meaning of the entry in terms of more primitive words), the syntactic zone (the grammatical properties of the entry) and the lexical combinatorics zone (containing the values of the Lexical Functions (LFs) 3). LFs are central to the study of collocations: A lexical function F is a correspondence which associates a lexical item L, called the key word of F, with a set of lexical items F(L)-the value of F. (Mel&apos;euk, 1988)4 We focus here on syntagmatic LFs describing cooccurrence relations such as pay attention, legitimate complaint; from a distance.5 Heylen et al. (1993) have worked out some cases which help license a starting point for assigning LFs. They distinguish four types of syntagmatic LFs: • evaluative qualifier Magn(bleed) = profusely • distributional qualifier Mult (sheep) = flock • co-occurrence Loc-in(distance)= at a distance • verbal operator Oper 1 (attention) = pay The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and </context>
</contexts>
<marker>Mel&apos;euk, 1988</marker>
<rawString>I. Mel&apos;euk. 1988. Paraphrase et lexique dans la theorie Sens-Texte. In Bes &amp; Fuchs (ed) Lexigue6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
<author>I Nirenburg</author>
</authors>
<title>A Framework for Lexical Selection in NLG.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING 88.</booktitle>
<marker>Nirenburg, Nirenburg, 1988</marker>
<rawString>S. Nirenburg and I. Nirenburg. 1988. A Framework for Lexical Selection in NLG. In Proceedings of COLING 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8941" citStr="Pustejovsky, 1995" startWordPosition="1367" endWordPosition="1368">l idiosyncratic behavior (lecture ... student; rancid butter; sour milk). We provide below examples of restricted semantic co-occurrences in (1), and restricted lexical cooccurrences in (2). Restricted semantic co-occurrence The semantics of the combination of the entries is semicompositional. In other words, there is an entry in - the lexicon for the base, (the semantic collocate is encoded inside the base), whereas we cannot directly refer to the sense of the semantic collocate in the lexicon, as it is not part of its senses. We assign the co-occurrence a new semi-compositional sense, 6See (Pustejovsky, 1995) for his account of such expressions using a coercion operator. 1329 where the sense of the base is composed with a new sense for the collocate. (la) #0=[key: &amp;quot;smoker&amp;quot;, rel: [syntagmatic: LSFIntensity [base: #0, collocate: [key: &amp;quot;heavy&amp;quot;, gram: [subCat: Attributive, freq: [value: 8]]]]] ...] (lb) #0=[key: &amp;quot;attention&amp;quot;, rel: [syntagmatic: LSFOper [base: #0, collocate: [key: &amp;quot;pay&amp;quot;, gram: [subCat: SupportVerb, freq: [value: 5]]]]] ...] In examples (1), the LSFs (LSFIntensity, LSFOper, ...) are equivalent (and some identical) to the LFs provided in the ECD. The notion of LSF is the same as that of L</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>J. Pustejovsky. 1995. The Generative Lexicon. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ramos</author>
<author>A Tutin</author>
<author>G Lapalme</author>
</authors>
<title>Lexical Functions of Explanatory Combinatorial Dictionary for Lexicalization in Text Generation.</title>
<date>1994</date>
<booktitle>In P. St-Dizier &amp; E. Viegas (Ed) Computational Lexical Semantics: CUP.</booktitle>
<contexts>
<context position="6420" citStr="Ramos et al., 1994" startWordPosition="974" endWordPosition="977">signing LFs. They distinguish four types of syntagmatic LFs: • evaluative qualifier Magn(bleed) = profusely • distributional qualifier Mult (sheep) = flock • co-occurrence Loc-in(distance)= at a distance • verbal operator Oper 1 (attention) = pay The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information. It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts. 3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in MTT and NLG respectively. 4(Heid, 1989) contrasts Hausman&apos;s base and collate to Mel&apos;euk&apos;s keyword and LF values. 6There are about 60 LFs listed said to be universal; the lexicographic approach of Mel&apos;Zuk and Zolkovsky has been applied among other languages to Russian, French, German and English. the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other semantically. 3 The Computational Lexical Semantic Approach </context>
</contexts>
<marker>Ramos, Tutin, Lapalme, 1994</marker>
<rawString>M. Ramos, A. Tutin and G. Lapalme. 1994. Lexical Functions of Explanatory Combinatorial Dictionary for Lexicalization in Text Generation. In P. St-Dizier &amp; E. Viegas (Ed) Computational Lexical Semantics: CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sinclair</author>
</authors>
<title>Corpus, Concordance, Collocations.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4260" citStr="Sinclair (1991)" startWordPosition="631" endWordPosition="632">ges. The focus has been on grammatical collocations such as adapt to, aim at, look for. Lakoff (1970) distinguishes a class of expressions which cannot undergo certain operations, such as nominalization, causativization: the problem is hard; *the hardness of the problem; *the problem hardened. The restriction on the application of certain syntactic operations can help define collocations such as hard problem, for example. Mel&apos;euk&apos;s treatment of collocations will be detailed below. In recent years, there has been a resurgence of statistical approaches applied to the study of natural languages. Sinclair (1991) states that &amp;quot;a word 1328 which occurs in close proximity to a word under investigation is called a collocate of it. ... Collocation is the occurrence of two or more words within a short space of each other in a text&amp;quot;. The problem is that with such a definition of collocations, even when improved,2 one identifies not only collocations but free-combining pairs frequently appearing together such as lawyer-client; doctor-hospital. However, nowadays, researchers seem to agree that combining statistic with symbolic approaches lead to quantifiable improvements (Klavans and Resnik, 1996). The Meaning</context>
</contexts>
<marker>Sinclair, 1991</marker>
<rawString>J. Sinclair. 1991. Corpus, Concordance, Collocations. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Retrieving Collocations from Texts: Xtract.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="6292" citStr="Smadja (1993)" startWordPosition="956" endWordPosition="957">te complaint; from a distance.5 Heylen et al. (1993) have worked out some cases which help license a starting point for assigning LFs. They distinguish four types of syntagmatic LFs: • evaluative qualifier Magn(bleed) = profusely • distributional qualifier Mult (sheep) = flock • co-occurrence Loc-in(distance)= at a distance • verbal operator Oper 1 (attention) = pay The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information. It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts. 3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in MTT and NLG respectively. 4(Heid, 1989) contrasts Hausman&apos;s base and collate to Mel&apos;euk&apos;s keyword and LF values. 6There are about 60 LFs listed said to be universal; the lexicographic approach of Mel&apos;Zuk and Zolkovsky has been applied among other languages to Russian, French, German and English. the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any predictable calc</context>
<context position="18136" citStr="Smadja, 1993" startWordPosition="2868" endWordPosition="2869">n. We apply their approach to the production of restricted semantic co-occurrences. Note that eat bigly will be produced but then rejected, as the form bigly does not exist in a dictionary. The rules overgenerate cooccurrences. This is a minor problem for analysis than for generation. To use these derived restricted co-occurrences in generation, the output of the lexical rule processor must be checked. This can be done in different ways: dictionary check, corpus check and ultimately human check. Other classes, such as the ones below can be extracted using lexico-statistical tools, such as in (Smadja, 1993), and then checked by a human. V + N pay attention, meet an obligation, commit an offence, • • • N + N dance marathon, marriage ceremony object of derision, ... LSFs and Inheritance We take advantage of 1) the semantics encoded in the lexemes, and 2) an inheritance hierarchy of LSFs. We illustrate briefly this notion of LSF inheritance hierarchy. For instance, the left hand-side of LSFChangeState specifies that it applies to foods (solid or liquid) which are human processed, and produces the collocates rancid, rancio (Spanish). Therefore it could apply to milk, butter, or wine. The rule would </context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>F. Smadja. 1993. Retrieving Collocations from Texts: Xtract. Computational Linguistics, 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Viegas</author>
<author>P Bouillon</author>
</authors>
<title>Semantic Lexicons: the Cornerstone for Lexical Choice in Natural Language Generation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 7th INLG, Kennebunkport.</booktitle>
<contexts>
<context position="7280" citStr="Viegas and Bouillon, 1994" startWordPosition="1114" endWordPosition="1118">olkovsky has been applied among other languages to Russian, French, German and English. the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other semantically. 3 The Computational Lexical Semantic Approach In order to account for the continuum we find in natural languages, we argue for a continuum perspective, spanning the range from free-combining words to idioms, with semantic collocations and idiosyncrasies in between as defined in (Viegas and Bouillon, 1994): • free-combining words (the girl ate candies) • semantic collocations (fast car; long book)6 • idiosyncrasies (large coke; green jealousy) • idioms (to kick the (proverbial) bucket) Formally, we go from a purely compositional approach in &amp;quot;free-combining words&amp;quot; to a noncompositional approach in idioms. In between, a (semi-)compositional approach is still possible. (Viegas and Bouillon, 1994) showed that we can reduce the set of what are conventionally considered as idiosyncrasies by differentiating &amp;quot;true&amp;quot; idiosyncrasies (difficult to derive or calculate) from expressions which have well-defin</context>
<context position="19537" citStr="Viegas and Bouillon, 1994" startWordPosition="3096" endWordPosition="3099">StateSolid and LSFChangeStateLiquid. This restricts the application of the rule to produce rancid butter, by going down the hierarchy. This enables us to factor out information common to several entries, and can be applied to both types of co-occurrences. We only have to code in the cooccurrence information relevant to the combination, the rest is inherited from its entry in the dictionary. 4 Conclusion In this paper, we built on a continuum perspective, knowledge-based, spanning the range from freecombining words to idioms. We further distinguished the notion of idiosyncrasies as defined in (Viegas and Bouillon, 1994), into restricted semantic co-occurrences and restricted lexical co-occurrences. We showed that they were formally equivalent, thus facilitating the processing of strictly compositional and semi-compositional expressions. Moreover, by considering the information in the lexicon as constraints, the linguistic difference between compositionality and semi-compositionality becomes a virtual difference for Hunter-Gatherer. We showed ways of minimizing the acquisition costs, by 1) using lexical rules as a way of expanding co-occurrences, 2) taking advantage of the LSF inheritance hierarchy. The main </context>
</contexts>
<marker>Viegas, Bouillon, 1994</marker>
<rawString>E. Viegas and P. Bouillon. 1994. Semantic Lexicons: the Cornerstone for Lexical Choice in Natural Language Generation. In Proceedings of the 7th INLG, Kennebunkport.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Viegas</author>
<author>B Onyshkevych</author>
<author>V Raskin</author>
<author>S Nirenburg</author>
</authors>
<title>From Submit to Submitted via Submission: on Lexical Rules in Large-scale Lexicon Acquisition.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguists.</booktitle>
<contexts>
<context position="17464" citStr="Viegas et al., 1996" startWordPosition="2758" endWordPosition="2761">tween different classes of cooccurrences (modulo presence of derived forms in the lexicon with same or subsumed semantics). Looking at the following example, A + N &lt;=&gt; V + Adv bitter resentment resent bitterly heavy smoker smoke heavily big eater eat *bigly V + Adv &lt;=&gt; Adv + Adj-ed oppose strongly strongly opposed oblige morally morally obliged we see that after having acquired with human intervention co-occurrences belonging to the A + N class, we can use lexical rules to derive the V + Adv class and also Adv + Adj-ed class. Lexical rules are a useful conceptual tool to extend a dictionary. (Viegas et al., 1996) used derivational lexical rules to extend a Spanish lexicon. We apply their approach to the production of restricted semantic co-occurrences. Note that eat bigly will be produced but then rejected, as the form bigly does not exist in a dictionary. The rules overgenerate cooccurrences. This is a minor problem for analysis than for generation. To use these derived restricted co-occurrences in generation, the output of the lexical rule processor must be checked. This can be done in different ways: dictionary check, corpus check and ultimately human check. Other classes, such as the ones below ca</context>
</contexts>
<marker>Viegas, Onyshkevych, Raskin, Nirenburg, 1996</marker>
<rawString>E. Viegas, B. Onyshkevych, V. Raskin and S. Nirenburg. 1996. From Submit to Submitted via Submission: on Lexical Rules in Large-scale Lexicon Acquisition. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguists.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wanner</author>
</authors>
<title>Lexical Functions in Lexicography and Natural Language Processing.</title>
<date>1996</date>
<publisher>John Benjamin Publishing Company.</publisher>
<marker>Wanner, 1996</marker>
<rawString>L. Wanner. 1996. Lexical Functions in Lexicography and Natural Language Processing. John Benjamin Publishing Company.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>