<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000392">
<title confidence="0.9751285">
A Comparison of Rankings Produced by Summarization
Evaluation Measures
</title>
<author confidence="0.989517">
Robert L. Donaway
</author>
<affiliation confidence="0.982855">
Department of Defense
</affiliation>
<address confidence="0.8703725">
9800 Savage Rd. STE 6409
Ft. Meade, MD 20755-6409
</address>
<email confidence="0.982039">
rldonaw@super. org
</email>
<author confidence="0.516352">
Kevin W. Drummey
</author>
<affiliation confidence="0.555191">
Department of Defense
</affiliation>
<address confidence="0.6266965">
9800 Savage Rd. STE 6341
Ft. Meade, MD 20755-6341
</address>
<email confidence="0.938299">
kwdrumin@super. org
</email>
<note confidence="0.915518333333333">
Laura A. Mather
La Jolla Research Lab
Britannica.com, Inc.
</note>
<address confidence="0.87077">
3253 Holiday Ct. Suite 208
La Jolla, CA 92037
</address>
<email confidence="0.472818">
math er@us .britannica. con&apos;
</email>
<sectionHeader confidence="0.904804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.909085833333333">
Summary evaluation measures produce a ranking
of all possible extract summaries of a document.,
Recall-based evaluation measures, which depend on
costly human-generated ground truth summaries,
produce uncorrelated rankings when ground truth
is varied. This paper proposes using sentence-rank-
based and content-based measures for evaluating ex-
tract summaries, and compares these with recall-
based evaluation measures. Content-based measures
increase the correlation of rankings induced by syn-
onymous ground truths, and exhibit other desirable
properties.
</bodyText>
<sectionHeader confidence="0.992609" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986357428571429">
The bulk of active research in the automatic
text summarization community centers on de-
veloping algorithms to produce extract sum-
maries, e. g. (Schwarz, 1990), (Paice and Jones,
.1993), (Kupiec et al., 1995), (Marcu, 1997),
(Strzalkowski et al., 1998), and (Goldstein et
. al., 1999). Yet understanding how to evalu-
ate their output has received less attention. In.
• 1997, TIPSTER sponsored a conference (SUM-
MAC) where various text summarization algo-
rithms were evaluated for their performance in
various tasks (Mani et al., 1999; Firmin and
Chrzanowski, 1999). While extrinsic evalua-
tion measures such as these are often very con-
crete, the act of designing the task and scor-
ing the results of the task introduces bias and
subject-based variability. These factors may
confound the comparison of summarization al-
gorithms. Machine-generated summaries also
may be evaluated intrinsically by comparing
them with &amp;quot;ideal&amp;quot; human-generated summaries.
However, there is often little agreement as to
what constitutes the ideal summary of a docu-
ment.
Both intrinsic and extrinsic methods require
time consuming, expert human input in order
to evaluate summaries. While the traditional
methods have many advantages, they are costly,
and human assessors cannot always agree on
summary quality. If a numerical measure were
available which did not depend on human judge-
ment, researchers and developers would be able
to immediately assess the effect of modifications
to summary generation algorithms. Also, such
a measure might be free of the bias that is in-
troduced by human assessment.
This paper investigates the properties of vari-
ous numerical measures for evaluating the qual-
ity of generic, indicative document summaries.
As explained by Mani et al. (1999), a generic
summary is not topic-related, but &amp;quot;is aimed at
a broad readership community&amp;quot; and an indica-
tive summary tells &amp;quot;what topics are addressed
in the source text, and thus can be used to
alert the user as to source content.&amp;quot; Section 2
discusses the properties of numerical evaluation
measures, points out several drawbacks associ-
ated with intrinsic measures and introduces new
measures developed by the authors. An exper-
iment was devised to compare the new evalua-
tion measures with the traditional ones. The de-
sign of this experiment is discussed in Section 3
and its results are presented in Section 4. The
final section includes conclusions and a state-
ment of the future work related to these evalu-
ation measures.
</bodyText>
<sectionHeader confidence="0.985666" genericHeader="introduction">
2 Evaluation Measures
</sectionHeader>
<bodyText confidence="0.999896428571429">
An evaluation measure produces a numerical
score which can be used to compare different
summaries of the same document. The scores
are used to assess summary quality across a col-
lection of test documents in order to produce
an average for an algorithm or system. How-
ever, it must be emphasized that the scores are
</bodyText>
<page confidence="0.998693">
69
</page>
<bodyText confidence="0.999901203703704">
most significant when considered per document.
For example, two different summaries of a doc-
ument may have been produced by two differ-
ent summarization algorithms. Presumably, the
summary with the higher score indicates that
the system which produced it performed bet-
ter than the other system. Obviously, if one
system consistently produces higher scores than
another system, its average score will be higher,
and one has reason to believe that it is a bet-
ter system. Thus, the important feature of any
summary evaluation measure is not the value of
its score, but rather the ranking its score im-
poses on a set of extracts of a document.
To compare two evaluation measures, whose
scores may have very different ranges and distri-
butions, one must compare the order in which
the measures rank various summaries of a docu-
ment. For instance, suppose a summary scoring
function Y is completely dependent upon the
output of another scoring function X, such as
Y = 2. Since Y is an increasing function of X,
both X and Y will produce the same ranking of
any set of summaries. However, the scores pro-
duced by Y will have a very different distribu-
tion than those of X and the two sets of scores
will not be correlated since the dependence of Y
on X is non-linear. Therefore, in order to com-
pare the scores two different measures assign to
a set of summaries, one must compare the ranks
, they assign, not the actual scores.
The ranks assigned by an evaluation mea-
sure produce equivalence classes of extract
summaries; each rank equivalence class con-
tains summaries which received the same score.
When a measure produces the same score for
two different summaries of a document, there is
a tie, and the equivalence class will contain more
than one summary. All summaries in an equiv-
alence class must share the same rank; let this
rank be the midrank of the range of ranks that
would have be assigned if each score were dis-
tinct. An evaluation measure should posses the
following properties: (i) higher-ranking sum-
maries are more effective or are of higher quality
than lower-ranking summaries, and (ii) all of the
summaries in a rank equivalence class are more-
or-less equally effective..
The following sections contrast the ranking
properties of three types of evaluation measures:
recall-based measures, a sentence-rank-based
measure and content-based measures. These
types of measures are defined, their properties
are described and their use is explained.
</bodyText>
<subsectionHeader confidence="0.998953">
2.1 Recall-Based Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.995907531914894">
Recall-based evaluation measures are intrin-
sic. They compare machine-generated sum-
maries with sentences previously extracted by
human assessors or judges. From each docu-
ment, the judges extract sentences that they
believe make up the best extract summary of
the document. A summary of a document gen-
erated by a summarization algorithm is typi-
cally compared to one of these &amp;quot;ground truth&amp;quot;
, summaries by counting the number of sentences
the ground truth summary and the algorithm&apos;s
summary have in common. Thus, the more sen-
tences a summary has recalled from the ground
truth, the higher its score will be. See work by
Goldstein et al. (1999) and Jing et al. (1998)
for examples of the use of this measure.
The recall-based measures introduce a bias
since they are based on the opinions of a small
number of assessors. It is widely acknowledged
(Jing et al., 1998; Kupiec et al., 1995; Voorhees,
1998) that assessor agreement is typically quite
low. There are at least two sources of this dis-
agreement. First, it is possible that one human
assessor will pick a particular sentence for in-
clusion in their summary when the content of
another sentence or set of sentences is approx-
imately equivalent. Jing et al. (1998) agree:
&amp;quot;...precision and recall are not the best mea-
sures for computing document quality. This is
due to the fact that a small change in the sum-
mary output (e.g., replacing one sentence with
an equally good equivalent which happens not
to match majority opinion [of the assessors]) can
dramatically affect a system&apos;s score.&amp;quot; We call
this source of summary disagreement &apos;disagree-
ment due to synonymy.&apos; Here is an example of
two human-generated extracts from the same
1991 Wall Street Journal article which contain
different sentences, but still seem to be describ-
ing an article about violin playing in a film:
EXTRACT 1: Both Ms. Streisand&apos;s film
husband, played by Jeroen Krabbe, and
her film son, played by her real son Ja-
son Gould, are, for the purposes of the
screenplay, violinists. The actual sound
— what might be called a fiddle over — was
produced off camera by Pinchas Zucker-
</bodyText>
<page confidence="0.996202">
70
</page>
<bodyText confidence="0.985837489361702">
man. The violin program in &amp;quot;Prince of
Tides&amp;quot; eliminates the critic&apos;s usual edge
and makes everyone fall back on his basic
pair of ears.
EXTRACT 2: Journalistic ethics forbid
me from saying if I think &amp;quot;Prince of Tides&amp;quot;
is as good as &amp;quot;Citizen Kane,&amp;quot; but I don&apos;t
think it&apos;s wrong to reveal that the film
has some very fine violin playing. But
moviegoers will hear Mr. Zuckerman cast
off the languor that too often makes him
seem like the most bored of great violin-
ists. With each of these pieces, Mr. Zuck-
erman takes over the movie and shows
what it means to play his instrument with
supreme dash.
Another source of disagreement can arise from
judges&apos; differing opinions about the true focus of
the original document. In other words, judges
disagree on what the document is about. We
call this second source &apos;disagreement due to fo-
cus.&apos; Here is a human-generated extract of the
same article which seems to differ in focus:
EXTRACT 3: Columbia Pictures has de-
layed the New York City and Los Angeles
openings of &amp;quot;Prince of Tides&amp;quot; for a week.
So Gothamites and Angelenos, along with
the rest of the country, will have to wait
until Christmas Day to see this film ver-
sion of the Pat Conroy novel about a
Southern football coach (Nick Nolte) dal-
lying with a Jewish female psychothera-
pist (Barbra Streisand) in the Big Apple.
Perhaps the postponement is a sign that
the studio is looking askance at this ex-
pensive product directed and co-produced
&apos;by its female lead.
Whatever the source, disagreements at the
sentence level are prevalent. This has seri-
ous implications for measures based on a sin-
gle opinion, when a slightly different opinion
would result in a significantly different score
(and rank) for many summaries.
For example, consider the following three-
sentence ground truth extract of a 37-sentence
1994 Los Angeles Times article from the TREC
collection. It contains sentences 1, 2 and 13.
</bodyText>
<listItem confidence="0.830703">
(1) Clinton Trade Initiative Sinks Under
G-7 Criticism. (2) President Clinton came
</listItem>
<bodyText confidence="0.973637188679245">
to the high-profile Group of Seven sum-
mit to demonstrate new strength in for-
eign policy but instead watched his pre-
mier initiative sink Saturday under a wave
of sharp criticism. (13) The negative re-
action to the president&apos;s trade proposal
came as a jolt after administration offi-
cials had built it up under the forward-
looking name of &amp;quot;Markets 2000&amp;quot; and had
portrayed it as evidence of his interest in
leading the other nations to more open
trade practices.
An extract that replaces sentence 13 with sen-
tence 5:
(5) In its most elementary form, it would
have set up a one-year examination of im-
prediments to world trade, but it would
have also set an agenda for liberalizing
trade rules in entirely new areas, such as
financial services, telecommunications and
investment.
will receive the same recall score as one which
replaces sentence 13 with sentence 32:
(32) Most nations have yet to go through
this process, which they hope to complete
by January.
These two alternative summaries both have the
same recall rank, but are obviously of very dif-
ferent quality.
Considered quantitatively, the only impor-
tant component of either precision or recall is
the &apos;sentence agreement&apos; J, the number of sen-
tences a summary has in common with the
ground truth summary. Following Goldstein
et al. (1999), let M be the number of sen-
tences in a ground truth extract summary and
let K be the number of sentences in a sum-
mary to be evaluated. With precision P
JIK and recall R = JIM as usual, and F1 =
2PRI(P R), then elementary algebra shows
that F1 = 2JI(M+K). Often, a fixed summary
length K is used. (In terms of word count, this
represents varying compression rates.) When a
particular ground truth of a given document is
chosen, then precision, recall and F1 are all con-
stant multiples of J. As such, these measures
produce different scores, but the same ranking
of all the K-sentence extracts from the docu-
ment. Since only this ranking is of interest, it is
not necessary to examine more than one of P,
Rand F1.
The sentence agreement J can only take on
integer values between 0 and M, so J, P, R,
</bodyText>
<page confidence="0.992228">
71
</page>
<bodyText confidence="0.999974958333333">
and F1 are all discrete variables. Therefore, al-
though there may be thousands of possible ex-
tract summaries of a document, only M + 1 dif-
ferent scores are possible. This will obviously
create a large number of ties in rankings pro-
duced by the P, R, and F1 scores, and will
greatly increase the probability that radically
different summaries will be given the same score
and rank. On the other hand, two summaries
which express the same ideas using different sen-
tences will be given very different scores. Both
of these problems are illustrated by the exam- -
ple above. Furthermore, if a particular ground
truth includes a large proportion of the doc-
ument&apos;s sentences (perhaps it is a, very con-,
cise document), shorter summaries will likely in-
clude only sentences which appear in the ground
truth. Consequently, even a randomly selected
collection of sentences might obtain the largest
possible score. Thus, recall-based measures are
likely to violate both properties (i) and (ii), dis-
cussed at the beginning of Section 2. These in-
herent weaknesses in recall-based measures will
be further explored in Section 4.
</bodyText>
<subsectionHeader confidence="0.999447">
2.2 A Sentence-Rank-Based Measure
</subsectionHeader>
<bodyText confidence="0.999985589285714">
One way to produce ground truth summaries is
to ask judges to rank the sentences of a docu-
ment in order of their importance in a generic,
indicative summary. This is often a difficult
task for which it is nearly impossible to obtain
consistent results. However, sentences which
appear early in a document are often more in-
dicative of the content of the document than
are other sentences. This is particularly true
in newspaper articles, whose authors frequently
trY to give the main points in the first para-
graph (Brandow et al., 1995). Similarly, adja-
cent sentences are more likely to be related to
each other than to those which appear further
away in the text. Thus, sentence position alone
may be an effective way to rank the importance
of sentences.
To account for sentence importance within a
ground truth, a summary comparison measure
was developed which treats an extract as a rank-
ing of the sentences of the document. For ex-
ample, a document with five sentences can be
expressed as (1, 2, 3, 4,5). A particular extract
may include sentences 2 and 3. Then if sen-
tence 2 is more important than sentence 3, the
sentence ranks are given by (4, 1, 2, 4, 4). Sen-
tences 1, 4, and 5 all rank fourth, since 4 is the
midrank of ranks 3, 4 and 5. Such rank vectors
can be compared using Kendall&apos;s tau statistic
(Sheskin, 1997), thus quantifying a summary&apos;s
agreement with a particular ground truth. As
will be shown in Section 4, sentence rank mea-
sures result in a smaller number of ties than do
recall-based evaluation measures.
Although it is also essentially recall-based,
the sentence rank measure has another slight
advantage over recall. Suppose a ground truth
summary of a 20-sentence document consists
of sentences {2, 3, 5}. The machine-generated
summaries consisting of sentences {2, 3, 4} and
{2, 3, 9} would receive the same recall score, but
{2, 3, 4} would receive a higher tau score (5 is
closer to 4 than to 9). Of course, this higher
score may not be warranted if the content of
sentence 9 is more similar to that of sentence 5.
The use of the tau statistic may be more ap-
propriate for ground truths produced by classi-
fying all of the sentences of the original docu-
ment in terms of their importance to an indica-
tive summary. Perhaps four different categories
could be used, ranging from &apos;very important&apos; to
&apos;not important.&apos; This would allow comparison
of a ranking with four equivalence classes (rep-
resenting the document) to one with just two
equivalence classes (representing inclusion and
exclusion from the summary to be evaluated).
</bodyText>
<subsectionHeader confidence="0.941915">
2.3 Content-Based Measures
</subsectionHeader>
<bodyText confidence="0.9999816">
Since indicative summaries alert users to doc-
ument content, any measure that evaluates the
quality of an indicative summary ought to con-
sider the similarity of the content of the sum-
mary to the content of the full document. This
consideration should be independent of exactly
which sentences are chosen for the summary.
The content of the summary need only cap-
ture the general ideas of the original docu-
ment. If human-generated extracts are avail-
able, machine-generated extracts may be evalu-
ated alternatively by comparing their contents
to these ground truths. This section defines
content-based measures by comparing the term
frequency (tf) vectors of extracts to tf vectors
of the full text or to tf vectors of a ground truth
extract. When the texts and summaries are to-
kenized and token aliases are determined by a
thesaurus, summaries that disagree due to syn-
onymy are likely to have similarly-distributed
</bodyText>
<page confidence="0.991808">
72
</page>
<bodyText confidence="0.997260313725491">
term frequencies. Also, summaries which hap-
pen to use synonyms appearing infrequently in
the text will not be penalized in a summary-
to-full-document comparison. Note that term
frequencies can always be used to compare an
extract with its full text, since the two will al-
ways have terms in common, but without a the-
saurus or some form of term aliasing, term fre-
quencies cannot be used to compare abstracts
with extracts.
The vector space model of information re-
trieval as described by Salton (1989) uses the -
inner product of document vectors to measure
the content similarity sim(di, d2) of two docu-
ments d1 and d2. Geometrically, this similarity •
measure gives the cosine of the angle between
the two document vectors. Since cos 0 = 1, doc-
uments with high cosine similarity are deemed
similar. We apply this concept to summary
evaluation by computing document-summary
content similarity sim(d, s) or ground truth-
summary content similarity sim(g, s).
Note that when comparing a summary with
its document, a prior human assessment is not
necessary. This may serve to eliminate the am-
biguity of a human assessor&apos;s bias towards cer-
tain types of summaries or sentences. How-
ever, the scores produced by such evaluation
Measures cannot be used reliably to compare
summaries of drastically different lengths, since
a much longer summary is more likely than a
short summary to produce a term frequency
•vector which is similar to the full document&apos;s
&apos;tf vector, despite the normalization of the two
vectors. (This contrasts with the bias of recall •
towards short summaries.)
This similarity measure can be enhanced in a
number of ways. For example, using term fre-
quency counts for a large corpus of documents,
term weighting (such as log-entropy (Dumais,
1991) or tf-idf (Salton, 1989)) can be used to
weight the terms in the document and summary
vectors. This may improve the performance of
the similarity measure by increasing the weights
of content-indicative terms and decreasing the
weights of those terms that are not indicative
of content. It is demonstrated in Section 4 that
term weighting caused a significant increase in
the correlation of the rankings produced by dif-
ferent ground truths; however, it is not clear
that this weighting increases the scores of high
quality summaries.
There are two potential problems with using -
the cosine measure to evaluate the performance
of a summarization algorithm. First of all, it
is likely that the summary vector will be very
sparse compared to the document vector since
the summary will probably contain many fewer
terms than the document. Second, it is possi-
ble that the summary will use key terms that
are not used often in the document. For exam-
ple, a document about the merger of two banks,
may use the term &amp;quot;bank&amp;quot; frequently, and use the
related (yet not exactly synonymous) term &amp;quot;fi-
nancial institution&amp;quot; only a few times. It is pos-
sible that a high quality extract would have a
low cosine similarity with the full document if it
contained only those few sentences that use the
term &amp;quot;financial institution&amp;quot; instead of &amp;quot;bank.&amp;quot;
Both of these problems can be addressed with
another common tool in information retrieval:
latent semantic indexing or LSI (Deerwester et
al., 1990).
LSI is a method of reducing the dimension
of the vector space model using the singular
value decomposition. Given a corpus of doc-
uments, create a term-by-document matrix A
where each row corresponds to a term in the
document set and each column corresponds to
a document. Thus, the columns of A represent
all the documents from the corpus, expressed
in a particular term-weighting scheme. (In our
testing, the document vectors&apos; entries are the
relative frequencies of the terms.) Compute the
singular value decomposition (SVD) of this ma-
trix (for details see Golub and van Loan (1989)).
Retain some number of the largest singular val-
ues of A and discard the rest. In general, re-
moving singular values serves as a dimension
reduction technique. While the SVD computa-
tion may be time-consuming when the corpus is
large, it needs to be performed only once to pro-
duce a new term-document matrix and a pro-
jection matrix. To calculate the similarity of a
summary and a document, the summary vector
s must also be mapped to this low-dimensional
space. This involves computing a vector-matrix
product, which can be done quickly.
The effect of using the scaled, reduced-
dimension document and summary vectors is
two-fold. First, each coordinate of both the doc-
ument and summary vector will contribute to
</bodyText>
<page confidence="0.997525">
73
</page>
<bodyText confidence="0.999978047619048">
the overall similarity of the summary to the doc-
ument (unlike the original vector space model,
where only terms that occur in the summary
contribute to the cosine similarity score). Sec-
ond, the purpose of using LSI is to reduce the
effect of near-synonymy on the similarity score.
If a term occurs infrequently in the document
but is highly indicative of the content of the
document, as in the case where the infrequent
term is synonymous with a frequent term, the
summary will be penalized less in the reduced-
dimension model for using the infrequent term -
than it would be in the original vector space
model. This reduction in penalty occurs be-
cause LSI essentially averages the weights of,
terms that co-occur frequently with other terms
(both &amp;quot;bank&amp;quot; and &amp;quot;financial institution&amp;quot; often
occur with the term &amp;quot;account&amp;quot;). This should
improve the accuracy of the Cosine similarity
measure for determining the quality of a sum-
mary of a document.
</bodyText>
<sectionHeader confidence="0.993905" genericHeader="method">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.995825783783784">
This section describes the experiment that tests
how well these summary evaluation metrics per-
form. Fifteen documents from the Text Re-
trieval Conference (TREC) collection were used
in the experiment. These documents are part of
a corpus of 103 newspaper articles. Each of the
documents was tokenized by a language process-
ing algorithm, which performed token aliasing.
In our experiments, the term set was comprised
of all the aliases appearing in the full corpus of
103 documents. This corpus was used for the
purposes of term weighting. Four expert judges
created extract summaries (ground truths) for
each of the documents. A list of the first 15
documents, along with some of their numeri-
cal features is found in Table 1. The judges
were instructed to select as many sentences as
were necessary to make an &amp;quot;ideal&amp;quot; indicative ex-
tract summary of the document. In terms of
the count of sentences in the ground truth, the
lengths of the summaries varied from document
to document. Ground truth compression rates
were generally between 10 and 20 percent. The
inter-assessor agreement also varied, but was of-
ten quite high. We measured this by calculating
the average pairwise recall in the collection of
four ground truths.
A suite of summary evaluation measures {Ek}
which produce a score for a summary was de-
veloped. These measures may depend on none,
one, or all of the collection of ground truth &amp;quot;
summaries {M. Measures which do not de-
pend on ground truth compute the summary-
document similarity sim(s, d). Content-based
measures which depend on a single ground truth
gi compute the summary-ground truth similar-
ity sim(s, gi). A measure which depends on
all of the ground truths ,g, computes
a summary&apos;s similarity with each ground truth
separately and averages these values. Table 2
enumerates the 28 different evaluation measures
that were compared in this experiment. Note
that the Recall and Kendall measures require a
ground truth.
In this study, the measures will be used to
evaluate extract summaries of a fixed sentence
length K. In all of our tests, K = 3 for rea-
sons of scale which will become clear. A sum-
mary length of three sentences represents vary-
ing proportions of the number of sentences in
the full text document, but this length was usu-
ally comparable to the lengths of the human-
generated ground truths. For each document,
the collection {Si} was generated. This is the
set of all possible K-sentence extracts from the
document. If the document has N sentences
total, there will be N choose K
( N \N!
K) _ K! (N — K)!
extracts in the exhaustive collection {S3}. The
focus now is only on the set of all possible sum-
maries and the evaluation measures, and not on
any particular summarization algorithm. For
each document, each of the measures in {Ek}
was used to rank the sets {S}. (Note that the
measures which do not depend on ground truths
could, in fact, be used to generate summaries if
it were possible to produce and rank the exhaus-
tive set of fixed-length summaries in real time.
Despite the authors&apos; access to impressive com-
puting power, the process took several hours
for each document!) The next section compares
these different rankings of the exhaustive set of
extracts for each document.
</bodyText>
<sectionHeader confidence="0.988922" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9868895">
One way to compare the different rankings pro-
duced by two different evaluation measures is to
</bodyText>
<page confidence="0.999326">
74
</page>
<tableCaption confidence="0.999658">
Table 1: Test Document St Summary Statistics
</tableCaption>
<table confidence="0.999960176470588">
Doc. TREC File Name Sent. Token Gnd. Truth Gnd. Truth
No. Count Count Sent. Cnt. Avg. Recall
1 WSJ911211-0057 34 667 3, 4, 12, 3 44%
2 WSJ900608-0126 34 603 4, 4, 9, 3 54%
3 WSJ900712-0047 - 18 364 2, 3, 5, 2 78%
4 1atwp940604.0027 23 502 4, 5, 5, 4 69%
5 1atwp940621.0116 27 579 12, 11, 10 84%
6 1atwp940624.0094 17 460 5, 5, 5, 4 79%
7 1atwp940707.0400 33 503 6, 9, 8, 8 52%
8 1atwp940709.0051 37 . 877 3, 5, 5, 4 53%
9 latwp940713.0013 34 702 9, 4, 5, 8 35%
10 1atwp940713.0014 30 528 6, 5, 7, 5 . 88%
11 1atwp940721.0080 28 , 793 3, 3, 5, 2 88%
12 1atwp940725.0030 36 690 9, 2, 7, 5 45%
13 1atwp940725.0128 18 438 6, 3, 5, 5 63%
14 1atwp940729.0109 25 682 4, 3, 4, 3 96%
15 latwp940801.0010 28 474 4, 5, 4, 5 43%
</table>
<tableCaption confidence="0.911206">
Table 2: Evaluation Measures
</tableCaption>
<table confidence="0.990557111111111">
Similarity Details Ground Truth Dependency
Measure
None gi 92 93 g4 All
Recall Ji/M, Ji = #(s n gi) N.A. E1 E2 E3 E4 E5
Kendall Tau see Section 2.2 N.A. E6 E7 E8 E9 E10
tf Cosine sim(s, d or gi) on tf vectors E11 E12 E13 E14 E15 Els
tf-idf Cosine sim(s, -) on tf-idf weighted vectors E17 E18 E19 E20 E21 E22
SVD tf Cosine sim(s, -)I E23 E24 E25 E26 E27 E28
on _ow-dim. vectors
</table>
<bodyText confidence="0.99650219047619">
calculate their Spearman rank correlation coef-
fcient. When two evaluation measures produce
nearly the same ranking of the summary set, the
. rank correlation will be near 1 and a scatterplot
of the two rankings will show points nearly ly-
• ing on a line with slope 1. When there is little
corielation between two rankings, the statistic
will be near 0 and the scatterplot will appear to
have randomly-distributed points. A negative
correlation indicates that one ranking often re-
verses the rankings of the other and in this case
a rank scatterplot will show points nearly lying
on a line with negative slope.
Table 3 compares the Spearman correlation
of the rankings produced by a specific pair of
ground truths. The first row contains the cor-
relations of two highly similar ground truth ex-
tracts of document 14. Both of these extracts
consisted of three sentences; two of the sen-
tences were common to both extracts. Not sur-
prisingly, the correlation is high regardless of
what measure produced the rankings. The sec-
ond row demonstrates an increase (across the
row) in correlation between rankings produced
by two different ground truth summaries of doc-
ument 8. These two ground truths did not dis-
agree in focus, but did disagree due to synonymy
- they contain just one sentence in common.
In general, the correlation among the rankings
produced by synonymous ground truths was in-
creased most by using the SVD content-based
comparison. Figure 1 illustrates the correla-
tion increase graphically for this pair of ground
truths. By contrast, the third row of Table 3
displays a decrease (across the row) in correla-
tion between rankings produced by two differ-
ent ground truths. In this case, the two ground
truths disagreed in focus: they are Extracts 2
and 3 contrasted in Section 2.1. Again, the cor-
relation among the rankings produced by the
four ground truths was decreased most by us-
ing a weighted content-based comparison such
</bodyText>
<page confidence="0.999407">
75
</page>
<tableCaption confidence="0.999825">
Table 3: Correlation of Ground Truths Depends on Level of Disagreement
</tableCaption>
<table confidence="0.94550525">
recall tau tf cosine tf-idf SVD
Agree Sentences 0.87 0.96 0.95 0.87 0.99
Disagree synonymy 0.34 0.37 0.53 0.72 0.96
Disagree focus 0.22 0.31 0.32 0.20 -0.29
</table>
<bodyText confidence="0.998771">
as tf-idf or SVD. These patterns were typical for
rankings produced by ground truths which dif-
fered in focus, allaying the fear that applying
the SVD weighting would produce correlated
rankings based on any two ground truths.
Of course, the lack of correlation among
recall-based rankings whenever ground truths
did not contain exactly the same sentences im-
plies that a different collection of extracts would
rank highly if one ground truth were replaced
with the other. This effect would surely carry
through to system averages across a set of doc-
uments. To exemplify the size of this effect,
for each document, the summaries which scored
highest using one ground truth were scored (us-
ing recall) against a second ground truth. With
the first ground truths, these high-scoring sum-
maries averaged over 75% recall; using the sec-
ond ground truths, the same summaries aver-
aged just over 25% recall. Thus, by simply
changing judges, an automatic system which
•produced these summaries would appear to
have a very different success rate. This dispar-
ity is lessened when content-based measures are
used, but the outcomes are still disparate.
Evidence suggests that the content-based
measures which do not rely on a ground truth
may be an acceptable substitute to those which
dos: Over the set of 15 documents, the aver-
age within-document inter-assessor correlation
is 0.61 using term frequency, 0.72 using tf-idf,
and 0.67 using SVD. The average correlation
of the ground truth dependent measures with
those that perform summary-document com-
parisons is 0.48 using term frequency, 0.70 using
tf-idf, and 0.56 using SVD. This means that on
average, the rankings based on single ground
truths are only slightly more correlated to each
other than they are to the rankings that do not
depend on any ground truth.
As noted in Section 2.1, the recall-based
measures exhibit unfavorable scoring proper-
ties. Figure 2 shows the histogram of scores
assigned to the exhaustive summary set for doc-
</bodyText>
<figureCaption confidence="0.973828">
Figure 1: Synonymy: Content-based Measures
Increase Rank Correlation
</figureCaption>
<figure confidence="0.946114416666667">
Duffed 8Rankbelptt GT1 va G12 IRO
a
bargibi
Dactmat Ranh katelpt Gli IS GT2 Verriall)
a a IXI
lawfsEiad
Domed 8 Pari ScalorfIct Gll Gi2 (If Coire)
Dcand Rat &amp;argil GT1 la GT2 FIDF Cosire)
a XII WI WI
9ccisEllflai
bad 8 Radaterpla GT1 as GT219,0 Coke)
WI WI WI
</figure>
<page confidence="0.955356">
76
</page>
<bodyText confidence="0.99995424137931">
ument 14 by five different measures. Each of
these measures was based on the same ground
truth summary of this document, which con-
tained four sentences. Clearly, the measures
based on a more sophisticated parsing method
have a much greater ability to discriminate be-
tween summaries. By contrast, the recall met-
ric can assign one of only four scores to a length
3 summary, based on the value of J. Elemen-
tary combinatorics shows that 4 extracts will
receive the highest possible score (and thus will
rank first), 126 summaries will rank second, 840
summaries will rank third, and 1330 summaries
will rank last (with a score of 0). This accounts
for all of the 2300 three-sentence extracts that
are possible. It seems very unlikely that all of
the second-ranking summaries are equally effec-
tive. The histogram depicting this distribution
is shown at the top of Figure 2. This is fol-
lowed by the histograms for the Kendall met-
ric, and the content-based metrics using term
frequency, tf-idf, and SVD weighted vectors, re-
spectively. The tf-idf and SVD weighted mea-
sures produced a very fine distribution of scores,
particularly near the top of the range. That is,
these metrics are able to distinguish between
different high-scoring summaries. These pat-
terns in the score histograms were typical across
the 15 documents.
</bodyText>
<sectionHeader confidence="0.997798" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9997923">
There is wide variation in the rankings pro-
duced by recall scores from non-identical ground
truths. This difference in scores is reflected in
averages computed across documents. The low.
inter-assessor correlation of ranks based on re-
call measures is distressing, and indicates that
these measures cannot be effectively used to
compare performances of summarization sys-
tems. Measures which gauge content similarity
produce more highly correlated rankings when-
ever ground truths do not disagree in focus.
Content-based measures assign different rank-
ings when ground truths do disagree in focus. In
addition, these measures provide a finer grained
score with which to compare summaries.
Moreover, the content-based measures which
rely on a ground truth are only slightly more
correlated to each other than they are to the
measures which perform summary-document
comparisons. This suggests that the effective-
</bodyText>
<figureCaption confidence="0.973958">
Figure 2: Score Histograms for Document 14
</figureCaption>
<figure confidence="0.6890664">
kurettl4ScaeHdcgratrt GT4 (kcal)
Dowel 14 Swe Kagan GT4 (TFCceite)
__.libliU
thdollellOiliimm
Ithindhh
</figure>
<bodyText confidence="0.999655210526316">
ness of summarization algorithms could be mea-
sured without the use of human judges. Since
the cosine measure is easy to calculate, feed-
back of summary quality can be almost instan-
taneous.
The properties of these content-based mea-
sures need to be further investigated. For ex-
ample, it is not clear that content-based mea-
sures satisfy properties (i) and (ii), discussed in
Section 2. Also, while they do produce far fewer
ties than either recall or tau, such a fine distinc-
tion in summary quality is probably not justi-
fied. When human-generated ground truths are
available, perhaps some combination of recall
and the content-based measures could be used.
For instance, whenever recall is not perfect, the
content of the non-overlapping sentences could
be compared with the missed ground truth sen-
tences. Also, the effects of compression rate,
</bodyText>
<figure confidence="0.99835775">
Dowel 14 Scare Ktsloyant GT4 &amp;Man
02
Is
— —
11 01
Mon
to
II
12 II
1:11501
Nand 14 Soo 1.164ta00 614 (TROFGogre)
Ccoorm 14 Son %Wan GT4 (SO Oosine)
I
to
Ii
!&apos;
</figure>
<page confidence="0.992006">
77
</page>
<bodyText confidence="0.999769545454545">
summary length, and document style are not
known.
The authors are currently performing further
experiments to see if users prefer summaries
that rank highly with content-based measures
over other summaries. Also, the outcomes
of extrinsic evaluation techniques will be com-
pared with each of these scoring methods. In
other words, do the high-ranking summaries
help users to perform various tasks better than
lower-ranking summaries do?
</bodyText>
<sectionHeader confidence="0.998312" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99991675">
The authors would like to thank Mary Ellen
Okurowski and Duncan Buell for their sup-,
port, encouragement, and advice throughout
this project. Thanks go also to Tomek Strza-
lkowski, Inderjeet Mani, Donna Harman, and
Hal Wilson for their suggestions of how to im-
prove the design of the experiment. We greatly
appreciate the fine editing advice Oksana Las-
sowsky provided. Finally, we are especially
grateful to the four expert judges, Benay, Ed,
MEO, and Toby, who produced our ground
truth summaries.
</bodyText>
<sectionHeader confidence="0.987231" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.485478">
Ronald Brandow, Karl Mitze, and Lisa F. Rau.
</bodyText>
<listItem confidence="0.9567345">
• 1995. Automatic condensation of electronic
publications by sentence selection. Informa-
• tion Processing and Management, 31(5):675-
• 685.
</listItem>
<reference confidence="0.985471553846154">
Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
dlarshman. 1990. Indexing by latent seman-
tic analysis. Journal of the American Society
for Information Science, 41(6):391-407.
Susan T. Dumais. 1991. Improving the retrieval
of information from external sources. Behav-
ior Research Methods, Instruments e.4 Com-
puters, 23(2):229-236.
Therese Firmin and Michael J. Chrzanowski.
1999. An evaluation of automatic text sum-
marization systems. In Advances in Au-
tomatic Text Summarization, chapter 21,
pages 325-336. MIT Press, Cambridge, Mas-
sachusetts.
Jade Goldstein, Mark Kantrowitz, Vibhu Mit-
tal, and Jaime Carbonell. 1999. Summa-
rizing text documents: Sentence selection
and evaluation metrics. In Proceedings of the
ACM SIGIR, pages 121-128.
Gene H. Golub and Charles F. van Loan. 1989.
Matrix Computations. The Johns Hopkins
University Press, Baltimore.
Hongyan Jing, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998. Sum-
marization evaluation methods: Experiments
and analysis. In American Association for
Artificial Intelligence Spring Symposium Se-
ries, pages 60-68.
Julian Kupiec, Jan Pederson, and Francine
Chen. 1995. A trainable document summa-
rizer. In Proceedings of the ACM SIGIR,
&apos; pages 68-73.
Inderjeet Mani, David House, Gary Klein,
Lynette Hirschman, Therese Firmin, and
Beth Sundheim. 1999. The TIPSTER SUM-
MAC text summarization evaluation. In Pro-
ceedings of the Ninth Conference of the Euro-
pean Chapter of the ACL, pages 77-85.
Daniel Marcu. 1997. From discourse struc-
ture to text summaries. In Proceedings of
the ACL&apos;97/EACL&apos;97 Workshop on Intelli-
gent Scalable Text Summarization, pages 82-
88.
C. D. Paice and P. A. Jones. 1993. The identifi-
cation of important concepts in highly struc-
tured technical papers. In Proceedings of the
ACM SIGIR, pages 69-78.
Gerard Salton. 1989. Automatic Text Pro-
cessing. Addison-Wesley Publishers, Mas-
sachusetts.
C. Schwarz. 1990. Content based text han-
dling. Information Processing and Manage-
ment, 26(2):219-226.
David J. Sheskin. 1997. Handbook of Paramet-
ric and Nonparametric Statistical Procedures.
CRC Press LLC, United States.
T. Strzalkowski, J. Wang, and B. Wise. 1998.
A robust practical text summarizaton sys-
tem. In AAAI Intelligent Text Summariza-
tion Workshop, pages 26-30.
Ellen M. Voorhees. 1998. Variations in rele-
vance judgements and the measurement of
retrieval effectiveness. In Proceedings of the
ACM SIGIR, pages 315-323.
</reference>
<page confidence="0.998822">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.030206">
<title confidence="0.9984485">A Comparison of Rankings Produced by Evaluation Measures</title>
<author confidence="0.999989">Robert L Donaway</author>
<affiliation confidence="0.99525">Department of</affiliation>
<address confidence="0.7609275">9800 Savage Rd. STE Ft. Meade, MD</address>
<email confidence="0.668265">rldonaw@super.org</email>
<author confidence="0.999136">Kevin W Drummey</author>
<affiliation confidence="0.995416">Department of</affiliation>
<address confidence="0.995356">9800 Savage Rd. STE</address>
<author confidence="0.645852">kwdruminsuper org Laura A Meade</author>
<affiliation confidence="0.815891">Research Britannica.com,</affiliation>
<address confidence="0.974275">3253 Holiday Ct. Suite La Jolla, CA</address>
<email confidence="0.301577">.britannica.con'</email>
<abstract confidence="0.995537923076923">Summary evaluation measures produce a ranking of all possible extract summaries of a document., Recall-based evaluation measures, which depend on costly human-generated ground truth summaries, produce uncorrelated rankings when ground truth is varied. This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recallbased evaluation measures. Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard dlarshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="20334" citStr="Deerwester et al., 1990" startWordPosition="3405" endWordPosition="3408">possible that the summary will use key terms that are not used often in the document. For example, a document about the merger of two banks, may use the term &amp;quot;bank&amp;quot; frequently, and use the related (yet not exactly synonymous) term &amp;quot;financial institution&amp;quot; only a few times. It is possible that a high quality extract would have a low cosine similarity with the full document if it contained only those few sentences that use the term &amp;quot;financial institution&amp;quot; instead of &amp;quot;bank.&amp;quot; Both of these problems can be addressed with another common tool in information retrieval: latent semantic indexing or LSI (Deerwester et al., 1990). LSI is a method of reducing the dimension of the vector space model using the singular value decomposition. Given a corpus of documents, create a term-by-document matrix A where each row corresponds to a term in the document set and each column corresponds to a document. Thus, the columns of A represent all the documents from the corpus, expressed in a particular term-weighting scheme. (In our testing, the document vectors&apos; entries are the relative frequencies of the terms.) Compute the singular value decomposition (SVD) of this matrix (for details see Golub and van Loan (1989)). Retain some</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, dlarshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard dlarshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
</authors>
<title>Improving the retrieval of information from external sources.</title>
<date>1991</date>
<journal>Behavior Research Methods, Instruments e.4 Computers,</journal>
<pages>23--2</pages>
<contexts>
<context position="18860" citStr="Dumais, 1991" startWordPosition="3160" endWordPosition="3161">f summaries or sentences. However, the scores produced by such evaluation Measures cannot be used reliably to compare summaries of drastically different lengths, since a much longer summary is more likely than a short summary to produce a term frequency •vector which is similar to the full document&apos;s &apos;tf vector, despite the normalization of the two vectors. (This contrasts with the bias of recall • towards short summaries.) This similarity measure can be enhanced in a number of ways. For example, using term frequency counts for a large corpus of documents, term weighting (such as log-entropy (Dumais, 1991) or tf-idf (Salton, 1989)) can be used to weight the terms in the document and summary vectors. This may improve the performance of the similarity measure by increasing the weights of content-indicative terms and decreasing the weights of those terms that are not indicative of content. It is demonstrated in Section 4 that term weighting caused a significant increase in the correlation of the rankings produced by different ground truths; however, it is not clear that this weighting increases the scores of high quality summaries. There are two potential problems with using - the cosine measure t</context>
</contexts>
<marker>Dumais, 1991</marker>
<rawString>Susan T. Dumais. 1991. Improving the retrieval of information from external sources. Behavior Research Methods, Instruments e.4 Computers, 23(2):229-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Therese Firmin</author>
<author>Michael J Chrzanowski</author>
</authors>
<title>An evaluation of automatic text summarization systems.</title>
<date>1999</date>
<booktitle>In Advances in Automatic Text Summarization, chapter 21,</booktitle>
<pages>325--336</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1559" citStr="Firmin and Chrzanowski, 1999" startWordPosition="225" endWordPosition="228">round truths, and exhibit other desirable properties. 1 Introduction The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (Schwarz, 1990), (Paice and Jones, .1993), (Kupiec et al., 1995), (Marcu, 1997), (Strzalkowski et al., 1998), and (Goldstein et . al., 1999). Yet understanding how to evaluate their output has received less attention. In. • 1997, TIPSTER sponsored a conference (SUMMAC) where various text summarization algorithms were evaluated for their performance in various tasks (Mani et al., 1999; Firmin and Chrzanowski, 1999). While extrinsic evaluation measures such as these are often very concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability. These factors may confound the comparison of summarization algorithms. Machine-generated summaries also may be evaluated intrinsically by comparing them with &amp;quot;ideal&amp;quot; human-generated summaries. However, there is often little agreement as to what constitutes the ideal summary of a document. Both intrinsic and extrinsic methods require time consuming, expert human input in order to evaluate summaries. While th</context>
</contexts>
<marker>Firmin, Chrzanowski, 1999</marker>
<rawString>Therese Firmin and Michael J. Chrzanowski. 1999. An evaluation of automatic text summarization systems. In Advances in Automatic Text Summarization, chapter 21, pages 325-336. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACM SIGIR,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="6931" citStr="Goldstein et al. (1999)" startWordPosition="1112" endWordPosition="1115">ation measures are intrinsic. They compare machine-generated summaries with sentences previously extracted by human assessors or judges. From each document, the judges extract sentences that they believe make up the best extract summary of the document. A summary of a document generated by a summarization algorithm is typically compared to one of these &amp;quot;ground truth&amp;quot; , summaries by counting the number of sentences the ground truth summary and the algorithm&apos;s summary have in common. Thus, the more sentences a summary has recalled from the ground truth, the higher its score will be. See work by Goldstein et al. (1999) and Jing et al. (1998) for examples of the use of this measure. The recall-based measures introduce a bias since they are based on the opinions of a small number of assessors. It is widely acknowledged (Jing et al., 1998; Kupiec et al., 1995; Voorhees, 1998) that assessor agreement is typically quite low. There are at least two sources of this disagreement. First, it is possible that one human assessor will pick a particular sentence for inclusion in their summary when the content of another sentence or set of sentences is approximately equivalent. Jing et al. (1998) agree: &amp;quot;...precision and </context>
<context position="11662" citStr="Goldstein et al. (1999)" startWordPosition="1924" endWordPosition="1927">zing trade rules in entirely new areas, such as financial services, telecommunications and investment. will receive the same recall score as one which replaces sentence 13 with sentence 32: (32) Most nations have yet to go through this process, which they hope to complete by January. These two alternative summaries both have the same recall rank, but are obviously of very different quality. Considered quantitatively, the only important component of either precision or recall is the &apos;sentence agreement&apos; J, the number of sentences a summary has in common with the ground truth summary. Following Goldstein et al. (1999), let M be the number of sentences in a ground truth extract summary and let K be the number of sentences in a summary to be evaluated. With precision P JIK and recall R = JIM as usual, and F1 = 2PRI(P R), then elementary algebra shows that F1 = 2JI(M+K). Often, a fixed summary length K is used. (In terms of word count, this represents varying compression rates.) When a particular ground truth of a given document is chosen, then precision, recall and F1 are all constant multiples of J. As such, these measures produce different scores, but the same ranking of all the K-sentence extracts from th</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Proceedings of the ACM SIGIR, pages 121-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Charles F van Loan</author>
</authors>
<title>Matrix Computations. The Johns Hopkins</title>
<date>1989</date>
<publisher>University Press,</publisher>
<location>Baltimore.</location>
<marker>Golub, van Loan, 1989</marker>
<rawString>Gene H. Golub and Charles F. van Loan. 1989. Matrix Computations. The Johns Hopkins University Press, Baltimore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and analysis.</title>
<date>1998</date>
<booktitle>In American Association for Artificial Intelligence Spring Symposium Series,</booktitle>
<pages>60--68</pages>
<contexts>
<context position="6954" citStr="Jing et al. (1998)" startWordPosition="1117" endWordPosition="1120">. They compare machine-generated summaries with sentences previously extracted by human assessors or judges. From each document, the judges extract sentences that they believe make up the best extract summary of the document. A summary of a document generated by a summarization algorithm is typically compared to one of these &amp;quot;ground truth&amp;quot; , summaries by counting the number of sentences the ground truth summary and the algorithm&apos;s summary have in common. Thus, the more sentences a summary has recalled from the ground truth, the higher its score will be. See work by Goldstein et al. (1999) and Jing et al. (1998) for examples of the use of this measure. The recall-based measures introduce a bias since they are based on the opinions of a small number of assessors. It is widely acknowledged (Jing et al., 1998; Kupiec et al., 1995; Voorhees, 1998) that assessor agreement is typically quite low. There are at least two sources of this disagreement. First, it is possible that one human assessor will pick a particular sentence for inclusion in their summary when the content of another sentence or set of sentences is approximately equivalent. Jing et al. (1998) agree: &amp;quot;...precision and recall are not the best</context>
</contexts>
<marker>Jing, McKeown, Barzilay, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Kathleen McKeown, Regina Barzilay, and Michael Elhadad. 1998. Summarization evaluation methods: Experiments and analysis. In American Association for Artificial Intelligence Spring Symposium Series, pages 60-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pederson</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACM SIGIR, &apos;</booktitle>
<pages>68--73</pages>
<contexts>
<context position="1206" citStr="Kupiec et al., 1995" startWordPosition="170" endWordPosition="173">uman-generated ground truth summaries, produce uncorrelated rankings when ground truth is varied. This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recallbased evaluation measures. Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties. 1 Introduction The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (Schwarz, 1990), (Paice and Jones, .1993), (Kupiec et al., 1995), (Marcu, 1997), (Strzalkowski et al., 1998), and (Goldstein et . al., 1999). Yet understanding how to evaluate their output has received less attention. In. • 1997, TIPSTER sponsored a conference (SUMMAC) where various text summarization algorithms were evaluated for their performance in various tasks (Mani et al., 1999; Firmin and Chrzanowski, 1999). While extrinsic evaluation measures such as these are often very concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability. These factors may confound the comparison of summarizatio</context>
<context position="7173" citStr="Kupiec et al., 1995" startWordPosition="1156" endWordPosition="1159">cument. A summary of a document generated by a summarization algorithm is typically compared to one of these &amp;quot;ground truth&amp;quot; , summaries by counting the number of sentences the ground truth summary and the algorithm&apos;s summary have in common. Thus, the more sentences a summary has recalled from the ground truth, the higher its score will be. See work by Goldstein et al. (1999) and Jing et al. (1998) for examples of the use of this measure. The recall-based measures introduce a bias since they are based on the opinions of a small number of assessors. It is widely acknowledged (Jing et al., 1998; Kupiec et al., 1995; Voorhees, 1998) that assessor agreement is typically quite low. There are at least two sources of this disagreement. First, it is possible that one human assessor will pick a particular sentence for inclusion in their summary when the content of another sentence or set of sentences is approximately equivalent. Jing et al. (1998) agree: &amp;quot;...precision and recall are not the best measures for computing document quality. This is due to the fact that a small change in the summary output (e.g., replacing one sentence with an equally good equivalent which happens not to match majority opinion [of t</context>
</contexts>
<marker>Kupiec, Pederson, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pederson, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the ACM SIGIR, &apos; pages 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>David House</author>
<author>Gary Klein</author>
<author>Lynette Hirschman</author>
<author>Therese Firmin</author>
<author>Beth Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC text summarization evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the ACL,</booktitle>
<pages>77--85</pages>
<contexts>
<context position="1528" citStr="Mani et al., 1999" startWordPosition="221" endWordPosition="224">ced by synonymous ground truths, and exhibit other desirable properties. 1 Introduction The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (Schwarz, 1990), (Paice and Jones, .1993), (Kupiec et al., 1995), (Marcu, 1997), (Strzalkowski et al., 1998), and (Goldstein et . al., 1999). Yet understanding how to evaluate their output has received less attention. In. • 1997, TIPSTER sponsored a conference (SUMMAC) where various text summarization algorithms were evaluated for their performance in various tasks (Mani et al., 1999; Firmin and Chrzanowski, 1999). While extrinsic evaluation measures such as these are often very concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability. These factors may confound the comparison of summarization algorithms. Machine-generated summaries also may be evaluated intrinsically by comparing them with &amp;quot;ideal&amp;quot; human-generated summaries. However, there is often little agreement as to what constitutes the ideal summary of a document. Both intrinsic and extrinsic methods require time consuming, expert human input in order </context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Firmin, Sundheim, 1999</marker>
<rawString>Inderjeet Mani, David House, Gary Klein, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 1999. The TIPSTER SUMMAC text summarization evaluation. In Proceedings of the Ninth Conference of the European Chapter of the ACL, pages 77-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From discourse structure to text summaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<contexts>
<context position="1221" citStr="Marcu, 1997" startWordPosition="174" endWordPosition="175">ruth summaries, produce uncorrelated rankings when ground truth is varied. This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recallbased evaluation measures. Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties. 1 Introduction The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (Schwarz, 1990), (Paice and Jones, .1993), (Kupiec et al., 1995), (Marcu, 1997), (Strzalkowski et al., 1998), and (Goldstein et . al., 1999). Yet understanding how to evaluate their output has received less attention. In. • 1997, TIPSTER sponsored a conference (SUMMAC) where various text summarization algorithms were evaluated for their performance in various tasks (Mani et al., 1999; Firmin and Chrzanowski, 1999). While extrinsic evaluation measures such as these are often very concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability. These factors may confound the comparison of summarization algorithms. M</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. From discourse structure to text summaries. In Proceedings of the ACL&apos;97/EACL&apos;97 Workshop on Intelligent Scalable Text Summarization, pages 82-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
<author>P A Jones</author>
</authors>
<title>The identification of important concepts in highly structured technical papers.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACM SIGIR,</booktitle>
<pages>69--78</pages>
<marker>Paice, Jones, 1993</marker>
<rawString>C. D. Paice and P. A. Jones. 1993. The identification of important concepts in highly structured technical papers. In Proceedings of the ACM SIGIR, pages 69-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing.</title>
<date>1989</date>
<publisher>Addison-Wesley Publishers,</publisher>
<location>Massachusetts.</location>
<contexts>
<context position="17602" citStr="Salton (1989)" startWordPosition="2953" endWordPosition="2954"> are determined by a thesaurus, summaries that disagree due to synonymy are likely to have similarly-distributed 72 term frequencies. Also, summaries which happen to use synonyms appearing infrequently in the text will not be penalized in a summaryto-full-document comparison. Note that term frequencies can always be used to compare an extract with its full text, since the two will always have terms in common, but without a thesaurus or some form of term aliasing, term frequencies cannot be used to compare abstracts with extracts. The vector space model of information retrieval as described by Salton (1989) uses the - inner product of document vectors to measure the content similarity sim(di, d2) of two documents d1 and d2. Geometrically, this similarity • measure gives the cosine of the angle between the two document vectors. Since cos 0 = 1, documents with high cosine similarity are deemed similar. We apply this concept to summary evaluation by computing document-summary content similarity sim(d, s) or ground truthsummary content similarity sim(g, s). Note that when comparing a summary with its document, a prior human assessment is not necessary. This may serve to eliminate the ambiguity of a </context>
<context position="18885" citStr="Salton, 1989" startWordPosition="3164" endWordPosition="3165"> However, the scores produced by such evaluation Measures cannot be used reliably to compare summaries of drastically different lengths, since a much longer summary is more likely than a short summary to produce a term frequency •vector which is similar to the full document&apos;s &apos;tf vector, despite the normalization of the two vectors. (This contrasts with the bias of recall • towards short summaries.) This similarity measure can be enhanced in a number of ways. For example, using term frequency counts for a large corpus of documents, term weighting (such as log-entropy (Dumais, 1991) or tf-idf (Salton, 1989)) can be used to weight the terms in the document and summary vectors. This may improve the performance of the similarity measure by increasing the weights of content-indicative terms and decreasing the weights of those terms that are not indicative of content. It is demonstrated in Section 4 that term weighting caused a significant increase in the correlation of the rankings produced by different ground truths; however, it is not clear that this weighting increases the scores of high quality summaries. There are two potential problems with using - the cosine measure to evaluate the performanc</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. 1989. Automatic Text Processing. Addison-Wesley Publishers, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schwarz</author>
</authors>
<title>Content based text handling.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>26--2</pages>
<contexts>
<context position="1157" citStr="Schwarz, 1990" startWordPosition="164" endWordPosition="165">aluation measures, which depend on costly human-generated ground truth summaries, produce uncorrelated rankings when ground truth is varied. This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recallbased evaluation measures. Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties. 1 Introduction The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (Schwarz, 1990), (Paice and Jones, .1993), (Kupiec et al., 1995), (Marcu, 1997), (Strzalkowski et al., 1998), and (Goldstein et . al., 1999). Yet understanding how to evaluate their output has received less attention. In. • 1997, TIPSTER sponsored a conference (SUMMAC) where various text summarization algorithms were evaluated for their performance in various tasks (Mani et al., 1999; Firmin and Chrzanowski, 1999). While extrinsic evaluation measures such as these are often very concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability. These fa</context>
</contexts>
<marker>Schwarz, 1990</marker>
<rawString>C. Schwarz. 1990. Content based text handling. Information Processing and Management, 26(2):219-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Sheskin</author>
</authors>
<title>Handbook of Parametric and Nonparametric Statistical Procedures.</title>
<date>1997</date>
<publisher>CRC Press LLC, United States.</publisher>
<contexts>
<context position="14936" citStr="Sheskin, 1997" startWordPosition="2508" endWordPosition="2509">ive way to rank the importance of sentences. To account for sentence importance within a ground truth, a summary comparison measure was developed which treats an extract as a ranking of the sentences of the document. For example, a document with five sentences can be expressed as (1, 2, 3, 4,5). A particular extract may include sentences 2 and 3. Then if sentence 2 is more important than sentence 3, the sentence ranks are given by (4, 1, 2, 4, 4). Sentences 1, 4, and 5 all rank fourth, since 4 is the midrank of ranks 3, 4 and 5. Such rank vectors can be compared using Kendall&apos;s tau statistic (Sheskin, 1997), thus quantifying a summary&apos;s agreement with a particular ground truth. As will be shown in Section 4, sentence rank measures result in a smaller number of ties than do recall-based evaluation measures. Although it is also essentially recall-based, the sentence rank measure has another slight advantage over recall. Suppose a ground truth summary of a 20-sentence document consists of sentences {2, 3, 5}. The machine-generated summaries consisting of sentences {2, 3, 4} and {2, 3, 9} would receive the same recall score, but {2, 3, 4} would receive a higher tau score (5 is closer to 4 than to 9)</context>
</contexts>
<marker>Sheskin, 1997</marker>
<rawString>David J. Sheskin. 1997. Handbook of Parametric and Nonparametric Statistical Procedures. CRC Press LLC, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>J Wang</author>
<author>B Wise</author>
</authors>
<title>A robust practical text summarizaton system.</title>
<date>1998</date>
<booktitle>In AAAI Intelligent Text Summarization Workshop,</booktitle>
<pages>26--30</pages>
<contexts>
<context position="1250" citStr="Strzalkowski et al., 1998" startWordPosition="176" endWordPosition="179"> produce uncorrelated rankings when ground truth is varied. This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recallbased evaluation measures. Content-based measures increase the correlation of rankings induced by synonymous ground truths, and exhibit other desirable properties. 1 Introduction The bulk of active research in the automatic text summarization community centers on developing algorithms to produce extract summaries, e. g. (Schwarz, 1990), (Paice and Jones, .1993), (Kupiec et al., 1995), (Marcu, 1997), (Strzalkowski et al., 1998), and (Goldstein et . al., 1999). Yet understanding how to evaluate their output has received less attention. In. • 1997, TIPSTER sponsored a conference (SUMMAC) where various text summarization algorithms were evaluated for their performance in various tasks (Mani et al., 1999; Firmin and Chrzanowski, 1999). While extrinsic evaluation measures such as these are often very concrete, the act of designing the task and scoring the results of the task introduces bias and subject-based variability. These factors may confound the comparison of summarization algorithms. Machine-generated summaries al</context>
</contexts>
<marker>Strzalkowski, Wang, Wise, 1998</marker>
<rawString>T. Strzalkowski, J. Wang, and B. Wise. 1998. A robust practical text summarizaton system. In AAAI Intelligent Text Summarization Workshop, pages 26-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Variations in relevance judgements and the measurement of retrieval effectiveness.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACM SIGIR,</booktitle>
<pages>315--323</pages>
<contexts>
<context position="7190" citStr="Voorhees, 1998" startWordPosition="1160" endWordPosition="1161">a document generated by a summarization algorithm is typically compared to one of these &amp;quot;ground truth&amp;quot; , summaries by counting the number of sentences the ground truth summary and the algorithm&apos;s summary have in common. Thus, the more sentences a summary has recalled from the ground truth, the higher its score will be. See work by Goldstein et al. (1999) and Jing et al. (1998) for examples of the use of this measure. The recall-based measures introduce a bias since they are based on the opinions of a small number of assessors. It is widely acknowledged (Jing et al., 1998; Kupiec et al., 1995; Voorhees, 1998) that assessor agreement is typically quite low. There are at least two sources of this disagreement. First, it is possible that one human assessor will pick a particular sentence for inclusion in their summary when the content of another sentence or set of sentences is approximately equivalent. Jing et al. (1998) agree: &amp;quot;...precision and recall are not the best measures for computing document quality. This is due to the fact that a small change in the summary output (e.g., replacing one sentence with an equally good equivalent which happens not to match majority opinion [of the assessors]) ca</context>
</contexts>
<marker>Voorhees, 1998</marker>
<rawString>Ellen M. Voorhees. 1998. Variations in relevance judgements and the measurement of retrieval effectiveness. In Proceedings of the ACM SIGIR, pages 315-323.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>