<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.647035">
STRUCTURAL MATCHING OF PARALLEL TEXTS
</title>
<author confidence="0.847267">
Yuji Matsumoto
</author>
<affiliation confidence="0.983673">
Graduate School of Information Science
Advanced Institute of Science and Technology, Nara
</affiliation>
<address confidence="0.805804">
Takayama-cho, Ikoma-shi, Nara 630-01 Japan
</address>
<email confidence="0.913664">
matsuOis.aist-nara.acjp
</email>
<author confidence="0.994101">
Hiroyuki Ishimoto Takehito Utsuro
</author>
<affiliation confidence="0.999065">
Department of Electrical Engineering
Kyoto University
</affiliation>
<address confidence="0.545598">
Sakyo-ku, Kyoto 606 Japan
</address>
<email confidence="0.947404">
{ ishimoto, utsuro}@pine.kuee.kyoto-u.acjp
</email>
<sectionHeader confidence="0.985577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998874">
This paper describes a method for finding struc-
tural matching between parallel sentences of two
languages, (such as Japanese and English). Par-
allel sentences are analyzed based on unification
grammars, and structural matching is performed
by making use of a similarity measure of word pairs
in the two languages. Syntactic ambiguities are re-
solved simultaneously in the matching process. The
results serve as a useful source for extracting lin-
guistic and lexical knowledge.
</bodyText>
<sectionHeader confidence="0.995128" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.997648163265306">
Bilingual (or parallel) texts are useful resources for
acquisition of linguistic knowledge as well as for ap-
plications such as machine translation. Intensive
research has been done for aligning bilingual texts
at the sentence level using statistical techniques by
measuring sentence lengths in words or in charac-
ters (Brown 91), (Gale 91a). Those works are quite
successful in that far more than 90% of sentences
in bilingual corpora are aligned correctly.
Although such parallel texts are shown to be use-
ful in real applications such as machine translation
(Brown 90) and word sense disambiguation (Dagan
91), structured bilingual sentences are undoubtedly
more informative and important for future natural
language researches. Structured bilingual or multi-
lingual corpora serve as richer sources for extract-
ing linguistic knowledge (Kaji 92), (Mayans 90),
(Sadler 91), (Utsuro 92).
Phrase level or word level alignment has also
been done by several researchers. The Textual
Knowledge Bank Project (Sadler 91) is building
monolingual and multilingual text bases structured
by linking the elements with grammatical (depen-
dency), referential, and bilingual relations. (Kaji
92) reports a method to obtain phrase level corre-
spondence of parallel texts by coupling phrases of
two languages obtained in CKY parsing processes.
This paper presents another method to obtain
structural matching of bilingual texts. Sentences in
both languages are parsed to produce (disjunctive)
feature structures, from which dependency struc-
tures are extracted. Ambiguities are represented as
disjunction. Then, the two structures are matched
to establish a one-to-one correspondence between
their substructures. The result of the match is ob-
tained as a set of pairs of minimal corresponding
substructures of the dependency structures. Exam-
ples of the results are shown in Figures 1, 2 and 3.
A dependency structure is represented as a tree, in
which ambiguity is specified by a disjunctive node
(OR. node). Circles in the figure show substruc-
tures and bidirectional arrows show corresponding
substructures.
Our technique and the results are different from
those of other methods mentioned above. (Kaji 92)
identifies corresponding phrases and aims at pro-
ducing translation templates by abstracting those
corresponding phrases. In the Bilingual Knowledge
Bank (Sadler 91), the correspondence is shown by
</bodyText>
<page confidence="0.996425">
23
</page>
<bodyText confidence="0.999722">
links between words in two sentences, equating two
whole subtrees headed by the words. We prefer
the minimal substructure correspondence and the
relationship between substructures. Such a mini-
mal substructure stands for the minimal meaning-
ful component in the sentence, which we believe is
very useful for our target application of extracting
lexical knowledge from bilingual corpora.
</bodyText>
<sectionHeader confidence="0.998036666666667" genericHeader="method">
SPECIFICATION OF
STRUCTURAL MATCHING
PROBLEM
</sectionHeader>
<bodyText confidence="0.999986714285714">
Although the structural matching method shown
in this paper is language independent, we deal with
parallel texts of Japanese and English. We assume
that alignment at the sentence level is already pre-
processed manually or by other methods such as
those in (Brown 91), (Gale 91a). Throughout this
paper, we assume to match simple sentences.&apos;
</bodyText>
<sectionHeader confidence="0.992236" genericHeader="method">
DEFINITIONS OF DATA STRUCTURES
</sectionHeader>
<bodyText confidence="0.995017304347826">
A pair ofJapanese and English sentences are parsed
independently into (disjunctive) feature structures.
For our present purpose, a part of a feature struc-
ture is taken out as a dependency structure consist-
ing of the content words2 that appear in the original
sentence. Ambiguity is represented by disjunctive
feature structures (Kasper 87). Since any relation
other than modifier-modifyee dependencies is not
considered here, path equivalence is not taken into
consideration. Both of value disjunction and gen-
eral disjunction are allowed.
We are currently using LFG-like grammars for
both Japanese and English, where the value of the
`pred&apos; label in an f-structure is the content word
that is the head of the corresponding c-structure.
We start with the definitions of simplified dis-
junctive feature structures, and then disjunctive
dependency structures, that are extracted from the
disjunctive feature structures obtained by the pars-
ing process.
Definition 1 Simple feature structures (U&apos;S) (L is
the set of feature labels, and A is the set of atomic
values) are defined recursively:
</bodyText>
<tableCaption confidence="0.601646">
1Matching of compound sentences are done by cutting
them up into simple sentence fragments.
&apos;In the present system, nouns, pronouns. verbs, adjec-
tives, and adverbs are regarded as content words.
</tableCaption>
<figure confidence="0.991316">
NIL
a where a E A
l:qm where I L, E FS
0 AO where 0,11) E FS
0 V 11, where 0, E FS
</figure>
<bodyText confidence="0.9546958">
To define (Disjunctive) Dependency Structures
as a special case of an FS, we first require the fol-
lowing definitions.
Definition 2 Top label set of an FS 0, written as
t1(0), is defined:
</bodyText>
<listItem confidence="0.960224833333333">
1. If 0 = : 01, then t1(0) = {I},
2. If 0 = 01 A2 or 0 = i 2, then t1(0) =
t1(01)U t1(02).
Definition 3 A relation &apos;sibling&apos; between feature
labels in 0 is defined:
1. If 0 = 1 : 01, then 1 and labels in 01 are not
sibling, and sibling relation holding in 01 also
holds in 0.
2. If 0 = A 02, then labels in t1(01) and labels
in t1(02) are sibling.
3. If 0 = 01V 02, then labels in q51 and labels in
02 are not sibling.
</listItem>
<bodyText confidence="0.995350631578947">
Note that the sibling relation is not an equiva-
lence relation. We refer to a set of feature labels
illq5 that are mutually sibling as a sibling label set
of 0. Now, we are ready to define a dependency
structure (DS).
Definition 4 A dependency structure is an FS
that satisfies the following condition:
Condition: Every sibling label set of 11) includes ex-
actly one `pred&apos; label.
The idea behind those are that the value of a
&apos;pre&amp; label is a content word appearing in the orig-
inal sentence, and that a sibling label set defines
the dependency relation between content words.
Among the labels in a sibling label set, the values
of the labels other than `pred&apos; are dependent on
(i.e., modify) the value of the `pred&apos; label. A DS
can be drawn as a tree structure where the nodes
are either a content word or disjunction operator
and the edges represent the dependency relation.
</bodyText>
<listItem confidence="0.9561252">
Definition 5 A substructure of an FS 0 is defined
(sitb(0) stands for the set of all substructures of
0):
1. NIL and itself are substructures of 0.
2. If g = a (a. E A), then a is a substructure of
</listItem>
<page confidence="0.995592">
24
</page>
<figure confidence="0.972526833333333">
English: She has long hair.
Japanese: fIVC - (1) - KO
she - GEN hair - TOP long
she
long
hair
</figure>
<figureCaption confidence="0.999804">
Figure 1: Example of structural matching, No.1
</figureCaption>
<figure confidence="0.992790818181818">
English: This child is starving for parental love.
Japanese: 0) - - 1- IC 01,i..-006
this child - TOP parent - GEN love - DAT be-starving
1CT-i-----C‘ this =UK
t be -1 OR child = 7-
Q. 1 • love =
hiicD I If
1 t tlove
I
this love starve t
parental 431
</figure>
<figureCaption confidence="0.999864">
Figure 2: Example of structural matching, No.2
</figureCaption>
<bodyText confidence="0.61129">
English: Japan benefits from free trade.
</bodyText>
<equation confidence="0.952686714285714">
Japanese: H* - - Q-
Japan - TOP free-trade - GEN benefit - ACC receive
I---------- ---------
OIR
Japan = 1321K
benefit =
trade = alWg
</equation>
<figureCaption confidence="0.99979">
Figure 3: Example of structural matching, No.3
</figureCaption>
<figure confidence="0.9872149">
japan :---jaloan
--------
---------- ----- T
;benefit
trade benefit t
;
free
.••
..free ................... ......
&apos;trade%
</figure>
<page confidence="0.932202">
25
</page>
<listItem confidence="0.970371666666667">
3. If = 1 : eh, then sub(01) are substructures of
0.
4. If (/) = A 02, then for any E sub(01) and
</listItem>
<bodyText confidence="0.823889388888889">
for any -11)2 E sub(02), 01A02 is a substructure
of 0.
5. If el) = V 02, then for for any E sub(ch)
and for any 02 E sub(02), i V IP, is a sub-
structure of 0.
The DS derived from an FS is the maximum sub-
structure of the FS that satisfies the condition in
Definition 4. The DS is uniquely determined from
an FS.
Definition 6 A disjunction-free maximal sub-
structure of an FS ch, is called a complete FS of
0-
An FS does not usually have a unique complete
FS. This concept is important since the selection of
a complete FS corresponds to ambiguity resolution.
Naturally, a maximal disjunction-free substructure
of a DS is again a DS and is called a complete
DS of 0.
</bodyText>
<construct confidence="0.374659">
Definition 7 A semi-complete DS of a DS 1,1) is a
substructure of a complete DS of Y, that satisfies
the condition in Definition 4.
</construct>
<bodyText confidence="0.996756857142857">
Note that a substructure of a DS is not neces-
sarily a DS. This is why the definition requires the
condition in Definition 4.
A complete DS V, can be decomposed into a. set
of non-overlapping semi-complete DSs. Such a de-
composition defines the units of structural match-
ing and plays the key role in our problem.
</bodyText>
<equation confidence="0.846507">
Definition 8 A set of semi-complete DS of a DS
D = • • • ,„,is called a decomposition of
</equation>
<bodyText confidence="0.913105285714286">
every in the set contains at least one oc-
currence of `pred&apos; feature label, and every content
word at the `pred&apos; feature label appearing in 1,b is
contained in exactly one
Definition 9 The reduced DS of a DS Y., with re-
spect to a decomposition D NI, • • • is con-
structed as follows:
</bodyText>
<listItem confidence="0.7907714">
1. oi is transformed to a DS, preci : Si&apos;, where
Si is the set of all content words appearing in
This DS is referred to as red(iki).
2. If there is a direct dependency relation between
two content words tv1 and iv, that are in
</listItem>
<bodyText confidence="0.9750706">
and iPj (i j), then the dependency relation.
is allotted between j and
Although this definition should be described pre-
cisely, we leave it with this more intuitive descrip-
tion. Examples of dependency structures and re-
duced dependency structures are found in Figures
1, 2 and 3, where the decompositions are indicated
by circles.
It is not difficult to show that the reduced DS
satisfies the condition of Definition 4.
</bodyText>
<sectionHeader confidence="0.9993015" genericHeader="method">
STRUCTURAL MATCHING OF BILIN-
GUAL DEPENDENCY STRUCTURES
</sectionHeader>
<bodyText confidence="0.999909742857143">
Structural matching problem of bilingual sentences
is now defined formally.
Parsing parallel English and Japanese sentences
results in feature structures, from which depen-
dency structures are derived by removing unrelated
features.
Assume that OE and 0.7 are dependency struc-
tures of English and Japanese sentences. The struc-
tural matching is to find the most plausible one-to-
one mapping between a decomposition of a com-
plete DS of OE and a decomposition of a complete
DS of 0j, provided that the reduced DS of OE and
the reduced DS of 0./ w.r.t. the decompositions
are isomorphic over the dependency relation. The
isomorphism imposes a natural one-to-one corre-
spondence on the dependency relations between the
reduced DSs.
Generally, the mapping need not always be one-
to-one, i.e., all elements in a decomposition need
not map into another decomposition. When the
mapping is not one-to-one, we assume that dummy
nodes are inserted in the dependency structures so
that the mapping naturally extends to be one-to-
one.
When the decompositions of parallel sentences
have such an isomorphic one-to-one mapping, we
assume that there are systematic methods to com-
pute similarity between corresponding elements in
the decompositions and to compute similarity be-
tween the corresponding dependency relations&apos;.
We write the function defining the former sim-
ilarity as f, and that of the latter as g. Then, f
is a function over semi-complete DSs derived from
English and Japanese parallel sentences into a real
number, and g is a function over feature label sets
</bodyText>
<footnote confidence="0.9978025">
3in the case of similarity between dependency relations,
the original feature labels are taken into account.
</footnote>
<page confidence="0.995333">
26
</page>
<bodyText confidence="0.885458">
of English and Japanese into a. real number.
</bodyText>
<construct confidence="0.985284625">
Definition 10 Given dependency structures, D,51
and D.52, of two languages, the structural match-
ing problem is to find an. isomorphic one-to-one
mapping in between decompositions of D.S&apos;i and
DS2 that maximizes the sum, of the values of simi-
larity functions, f and g.
That is, the problem is to find the function 771 that
maximizes
</construct>
<equation confidence="0.705476">
Ed( (d, m(d)) g(l in(l)))
</equation>
<bodyText confidence="0.975881285714286">
where d varies over semi-complete DS of DS1 and
1 varies over feature labels in DSi.
The similarity functions can be defined in vari-
ous ways. \Are assume some similarity measure be-
tween Japanese and English words. For instance,
we assume that the similarity function f satisfies
the following principles:
</bodyText>
<listItem confidence="0.996893625">
1. f is a. simple function defined by the similar-
ity measure between content \vol.&amp; of two lan-
guages.
2. Fine-grained decompositions get larger simi-
larity measure than coarse-grained decompo-
sitions.
3. Dummy nodes should give some negative value
to f.
</listItem>
<bodyText confidence="0.999977294117647">
The first principle is to simplify the complexity
of the structural matching algorithm. The second
is to obtain detailed structural matching between
parallel sentences and to avoid trivial results, e.g.,
the whole DSs are matched. The third is to avoid
the introduction of dummy nodes when it is possi-
ble.
The function g should be defined according to
the language pair. Although feature labels repre-
sent grammatical relation between content words
or phrases and may provide useful information for
measuring similarity, we do not use the informa-
tion at our current stage. The reason is that we
found it difficult to have a clear view on the re-
lationship between feature labels of English and
Japanese and on the meaning of feature labels be-
tween semi-complete dependency structures.
</bodyText>
<sectionHeader confidence="0.996394" genericHeader="method">
STRUCTURAL MATCHING
ALGORITHM
</sectionHeader>
<bodyText confidence="0.9999205">
The structural matching of two dependency struc-
tures are combinatorially difficult, problem. We
apply the branch-and-bound method to solve the
problem.
The branch-and-bound algorithm is a top-down
depth-first backtracking algorithm for search prob-
lems. It looks for the answers with the BEST score.
In each new step, it estimates the maximum value
of the expected scores along the current path and
compares it with the currently known best score.
The maximum expected score is usually calculated
by a. simplified problem that guarantees to give a
value not less than the best score attainable along
the current path. If the maximum expectation is
less than the currently known best score, it means
that there is no chance to find better answers by
pursuing the path. Then, it gives up the current
path and backtracks to try remaining paths.
We regard a dependency structure as a tree
structure that includes disjunction (OR nodes),
and call a content word and a dependency rela-
tion as a node and an edge, respectively. Then
a semi-complete dependency structure corresponds
to a connected subgraph in the tree.
The matching of two dependency trees starts
from the top nodes and the matching process goes
along edges of the trees. During the matching pro-
cess, three types of nondeterminism arise:
</bodyText>
<listItem confidence="0.996505333333333">
1. Selection of top-most subgraphs in both of the
trees (i.e., selection of a semi-complete DS)
2. Selection of edges in both of the trees to decide
the correspondence of dependency relations
3. Selection of one of the disjuncts at an &apos;OR&apos;
node
</listItem>
<bodyText confidence="0.999804785714286">
While the matching is done top-down, the exact
score of the matched subgraphs is calculated us-
ing the similarity function f.4 When the matching
process proceeds to the selection of the second type,
it selects an edge in each of the dependency trees.
The maximum expected score of matching the sub-
trees under the selected edges are calculated from
the sets of content words in the subtrees. The cal-
culation method of the maximum expected score is
defined in some relation with the similarity func-
tion f.
Suppose h is the function that gives the maxi-
mum expected score of two subgraphs. Also, sup-
pose B and P be the currently known best score
</bodyText>
<footnote confidence="0.980083">
4We do not take into account the similarity measure
between dependency relations as stated in the preceding
section.
</footnote>
<page confidence="0.999103">
27
</page>
<bodyText confidence="0.9890318">
and the total score of the already matched sub-
graphs, respectively. If s and t are the subgraphs
under the selected edges and s&apos; and 1&apos; are the whole
remaining subgraphs, the matching under s and t
will be undertaken further only when the following
inequation holds:
P h(s,t) h(s&apos; , &gt; B
Any selection of edges that does not satisfy this
inequality cannot provide better matching than the
currently known best ones.
All of the three types of nondeterminism are sim-
ply treated as the nondeterminism in the algorithm.
The syntactic ambiguities in the dependency
structures are resolved spontaneously when the
matching with the best score is obtained.
</bodyText>
<sectionHeader confidence="0.809774" genericHeader="evaluation">
EXPERIMENTS
</sectionHeader>
<bodyText confidence="0.993496380952381">
We have tested the structural matching algorithm
with 82 pairs of sample sentences randomly selected
from a Japanese-English dictionary.
We used a machine readable Japanese-English
dictionary (Shimizu 79) and Roget&apos;s thesaurus (Ro-
get 11) to measure the similarity of pairs of content
words, which are used to define the function f.
Similarity of word pairs
Given a pair of Japanese and English sentences,
we take two methods to measure the similarity be-
tween Japanese and English content. words appear-
ing in the sentences.
For each Japanese content word wj appearing in
the Japanese sentence, we can find a. set of translat-
able English words from the Japanese-English dic-
tionary. When the Japanese word is a. polysemous
word, we select an English word from each polyse-
mous entry. Let CEJ be the set of such translat-
able English words of wj. Suppose CE is the set of
contents words in the English sentence. The trans-
latable pairs of tvj, Tp(wj), is defined as follows:
Tp(wi) = (w , WE) WE E CEJ n CEI
We use Roget&apos;s thesaurus to measure similarity
of other word pairs. Roget&apos;s thesaurus is regarded
as a. tree structure where words are allocated at the
leaves of the tree: For each Japanese content word
wj appearing in the Japanese sentence, we can de-
fine the set of translatable English words of MI,
CEj. From each English word in the set, the mini-
mum distance to each of the English content words
appearing in the English sentence is measured.&apos;
This minimum distance defines the similarity be-
tween pairs of Japanese and English words.
We decided to use this similarity only for esti-
mating dissimilarity between Japanese and English
word pairs. We set a predetermined threshold dis-
tance. If the minimal distance exceeds the thresh-
old, the exceeded distance is counted as the nega-
tive similarity.
The similarity of two words tv1 and w2 appear-
ing in the given pair of sentences, sim((wi, w2)), is
defined as follows:
</bodyText>
<equation confidence="0.992636">
sim((wi,w2)) =
w2) E Tp(wi) or (w2, wi) E Tp(w2)
</equation>
<bodyText confidence="0.994030142857143">
(wi w2) Tp(wi) and (w2, w) Tp(w2)
and the distance between tv1 and w2
exceeds the threshold by k.
otherwise
Similarity of semi-complete DSs
The similarity between corresponding semi-
complete DSs is defined based on the similarity be-
tween the content words. Suppose that s and t are
semi-complete DSs to be matched, and that V, and
V2 are the sets of content words in s and t. Let A
be the less larger set of V, and 172 and B be the
other A B For each injection p from A
into B, the set of word pairs D derived from p can
be defined as follows.
</bodyText>
<equation confidence="0.619514">
D= {(a,p(a)) I a E
</equation>
<bodyText confidence="0.976072615384615">
Now, we define the similarity function f over
Japanese and English semi-complete DSs to give
the maximum value to the following expression for
all possible injections:
f (s,t) = max{ dED Sint(d)/ X 0.95111.1+1Vii-2
The summation gives the maximum sum of the
similarity of the content words in s and t. 0.95 is
the penalty when the semi-complete DSs with more
than one content words are used in the matching.
Figures 1, 2 and 3 shows the results of the struc-
tural matching algorithm, in which the translatable
pairs obtained from the Japanese-English dictio-
nary are shown by the equations.
</bodyText>
<footnote confidence="0.715926">
5 The distance between words is the length of the shortest
path in the thesaurus tree.
</footnote>
<page confidence="0.552183">
6
—k
28
</page>
<bodyText confidence="0.661396">
The match with the best score includes
Correct matching 47 89% (47/53)
no correct matching 6 11% (6/53)
Single correct matching 34 64% (34/53)
</bodyText>
<subsectionHeader confidence="0.877524">
Results of the experiments
</subsectionHeader>
<bodyText confidence="0.999990428571429">
We used 82 pairs of Japanese and English sen-
tences appearing in a. Japanese-English dictionary.
The results were checked and examined in detail by
hand. Some of the sentences are not parsable be-
cause of the limited coverage of our current gram-
mars. Although 59 pairs of them are parsable, 6
out of them do not, include correct. parse results.
The structural matching algorithm with the set-
ting described above is applied to the 53 pairs. The
cases where the correct matching is not included in
the best rated answers are 6 out of them. The
remaining 47 pairs include the correct matching,
of which 31 pairs result in the correct matching
uniquely. Table 1 summarizes the results.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="evaluation">
EVALUATION AND DISCUSSION
</sectionHeader>
<bodyText confidence="0.999917370370371">
Although the number of sentences used in the ex-
periments is small, the result shows that about,
two third of the pairs give the unique matching,
in which every syntactic ambiguity is resolved.
The cases where no correct matching was ob-
tained needs be examined. Some sentences contain
an idiomatic expression that has completely differ-
ent syntactic structures from the sentence struc-
ture of the other. Such an expression will no way
be matched correctly except that the whole struc-
tures are matched intact. Other cases are caused by
complex sentences that, include an embedded sen-
tence. When the verbs at. the roots of the depen-
dency trees are irrelevant, extraordinary matchings
are produced. We intend not to use our method to
match complex or compound sentences as a whole.
We will rather use our method to find structural
matching between simple sentences or verb phrases
of two languages.
The matching problem of complex sentences are
regarded as a different problem though the simi-
lar technique is usable. We think that the scores
of matched phrases will help to identify the cor-
responding phrases when we match complex sen-
tences.
Taking the sources of other errors into consider-
ation, possible improvements are:
</bodyText>
<listItem confidence="0.992024545454545">
1. Enhancement of English and Japanese gram-
mars for wider coverage and lower error rate.
2. Introduction of more precise similarity mea-
surement of content words.
3. Utilization of grammatical information:
• Feature labels, for estimating matching
plausibility of dependency relations
• Part of speech, for measuring matching
plausibility of content words
• Other grammatical information: mood,
voice, etc.
</listItem>
<bodyText confidence="0.999880392857143">
The first two improvements are undoubtedly im-
portant. As for the similarity measurement of con-
tent words, completely different approaches such
as statistical methods may be useful to get good
translatable pairs (Brown 90), (Gale 91).
Various grammatical information is kept in the
feature descriptions produced in the parsing pro-
cess. However, we should be very prudent in using
it. Since English and Japanese are grammatically
quite different, some grammatical relation may not
be preserved between them. In Figure 3, solid ar-
rows and circles show the correct matching. While
&apos;benefit&apos; matches with the structure consisting of
Est &apos; and &apos;, their dependent words &apos;trade&apos;
and HEIIIRV) &apos; modify them as a verb modifier
and as a noun modifier, the grammatical relation
of which are quite different.
This example highlights another interesting
point. Dotted arrows and circles show another
matching with the same highest score. In this case,
&apos;japan&apos; is taken as a. verb. This rather strange in-
terpretation insists that &apos;japan&apos; matches with 11*
&apos; and &apos; 11:11 &apos;. Since &apos;japan&apos; as a verb has little se-
mantic relation with &apos; as a. country, discrim-
ination of part-of-speech seems to be useful. On
the other hand, the correspondence between &apos;ben-
efit&apos; and &apos; &apos; is found in their noun entry in the
dictionary. Since &apos;benefit&apos; is used as a verb in the
</bodyText>
<tableCaption confidence="0.918967">
Table 1: Results of experiments
</tableCaption>
<figure confidence="0.9184299">
Parsing Japanese and English sentences
Number of sentences
Parse failure
Parsa.ble
Correct parsa.bility
Correct parse 53 89.8% (53/59)
Incorrect parse 6 10.2% (6/59)
82
23
59
</figure>
<page confidence="0.996808">
29
</page>
<bodyText confidence="0.999655666666667">
sentence, taking part-of-speech into consideration
may jeopardize the correct matching, either. The
fact that the verb and noun usages of &apos;benefit&apos; bear
common concept implies that more precise similar-
ity measurement will solve this particular problem.
Since the interpretations of the sample English sen-
tences are in different mood, imperative and declar-
ative, the mood of a sentence is also useful to re-
move irrelevant interpretations.
</bodyText>
<sectionHeader confidence="0.991566" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999993375">
The structural matching problem of parallel texts
is formally defined and our current implementation
and experiments are introduced. Although the re-
search is at the preliminary stage and has a very
simple setting, the experiments have shown a. num-
ber of interesting results. The method is easily
enhanced by improving the grammars and by in-
corporating more accurate similarity measurement.
Number of other researches of building transla-
tion dictionaries and of determining similarity re-
lationship between words are useful to improve our
method.
To extract useful information from bilingual cor-
pora, structural matching is inevitable for language
pairs like English and Japanese that have quite dif-
ferent linguistic structure. Incidentally, we have
found that this dissimilarity plays an important
role in resolving syntactic ambiguities since the
sources of ambiguities in English and Japanese sen-
tences are in many cases do not coincide (Utsuro
92). We a.re currently working on extracting verbal
case frames of Japanese from the results of struc-
tural matching of a Japanese-English corpus (Ut-
suro 93). The same technique is naturally applica,
hie to acquire verbal case frames of English as well.
Another application we are envisaging is to extract
translation pattern from the results of structural
matching.
We plan to work on possible improvements dis-
cussed in the preceding section, and will make large
scale experiments using translated newspaper arti-
cles, based on the phrase matching strategy.
</bodyText>
<sectionHeader confidence="0.997924" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.983314333333333">
This work is partly supported by the Grants
from Ministry of Education, &amp;quot;knowledge Science&amp;quot;
(#03245103).
</bodyText>
<sectionHeader confidence="0.987095" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9993624">
Brown, P.F., et al., A Statistical Approach to Ma-
chine Translation, Computational Linguistics,
Vol.16, No.2, pp.79-85, 1990.
Brown, P.F., Lai, J.C. and Mercer, R.L., Align-
ing Sentences in Parallel Corpora, ACL-91,
pp.169-176, 1991.
Dagan, I., Itai, A. and Schwa11, U., Two Lan-
guages are More Informative than One, ACL-
91, pp.130-137, 1991a.
Gale. W.A. and Church, K.W., A Program
for Aligning Sentences in Bilingual Corpora,
ACL-91, pp.177-184, 1991b.
Gale. W.A. and Church, K.W., Identifying
Word Correspondences in Parallel Texts, &apos;91
DARPA Speech and Natural Language Work-
shop, pp.152-157, 1991.
Kaji, H., Kida., Y., and Morimoto, Y., Learning
Translation Templates from Bilingual Text,
COLING-92, pp.672-678, 1992.
Kasper, R., A Unification Method for Disjunc-
tive Feature Descriptions, ACL-87, pp.235-
242, 1987.
Klavans, J. and Tzoukermann, E., The BICORD
System: Combining Lexical Information from
Bilingual Corpora and Machine Readable Dic-
tionaries, COLING-90, pp.174-179, 1990.
Miller, G.A., et al., Five Papers on WordNet, Cog-
nitive Science Laboratory, Princeton Univer-
sity, CSL Report 4.9, July 1990.
Roget, S.R., Roget&apos;s Thesaurus, Crowell Co.,
1911.
Sadler, V., The Textual Knowledge Bank: De-
sign, Construction, Applications, Proc. Inter-
national Workshop on Fundamental Research
for the Future Generation of Natural Language
Processing (FGNLP), pp.17-32, Kyoto, Japan,
1991.
Shimizu, M., et al. (ed.), Japanese-English Dictio-
nary, Kodansha, 1979.
Utsuro, T., Matsumoto, Y., and Nagao, M., Lexi-
cal Knowledge Acquisition from Bilingual Cor-
pora, COLING-92, pp.581-587, 1992.
Utsuro, T., Matsumoto, Y., and Naga.o, M., Ver-
bal Case Frame Acquisition from Bilingual
Corpora, to appear LICAI-93, 1993.
</reference>
<page confidence="0.998802">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721259">
<title confidence="0.999053">STRUCTURAL MATCHING OF PARALLEL TEXTS</title>
<author confidence="0.996338">Yuji Matsumoto</author>
<affiliation confidence="0.996629">Graduate School of Information Science Advanced Institute of Science and Technology, Nara</affiliation>
<address confidence="0.930913">Takayama-cho, Ikoma-shi, Nara 630-01 Japan</address>
<email confidence="0.965674">matsuOis.aist-nara.acjp</email>
<author confidence="0.948232">Hiroyuki Ishimoto Takehito Utsuro</author>
<affiliation confidence="0.999526">Department of Electrical Engineering Kyoto University</affiliation>
<address confidence="0.980058">Sakyo-ku, Kyoto 606 Japan</address>
<email confidence="0.959621">ishimoto@pine.kuee.kyoto-u.acjp</email>
<email confidence="0.959621">utsuro@pine.kuee.kyoto-u.acjp</email>
<abstract confidence="0.991057">This paper describes a method for finding structural matching between parallel sentences of two languages, (such as Japanese and English). Parallel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages. Syntactic ambiguities are resolved simultaneously in the matching process. The results serve as a useful source for extracting linguistic and lexical knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>A Statistical Approach to</title>
<date>1990</date>
<booktitle>Machine Translation, Computational Linguistics, Vol.16, No.2,</booktitle>
<pages>79--85</pages>
<marker>Brown, 1990</marker>
<rawString>Brown, P.F., et al., A Statistical Approach to Machine Translation, Computational Linguistics, Vol.16, No.2, pp.79-85, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<date>1991</date>
<booktitle>Aligning Sentences in Parallel Corpora, ACL-91,</booktitle>
<pages>169--176</pages>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Brown, P.F., Lai, J.C. and Mercer, R.L., Aligning Sentences in Parallel Corpora, ACL-91, pp.169-176, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
<author>U Schwa11</author>
</authors>
<date>1991</date>
<booktitle>Two Languages are More Informative than One, ACL91,</booktitle>
<pages>130--137</pages>
<marker>Dagan, Itai, Schwa11, 1991</marker>
<rawString>Dagan, I., Itai, A. and Schwa11, U., Two Languages are More Informative than One, ACL91, pp.130-137, 1991a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A</author>
<author>K W Church</author>
</authors>
<title>A Program for Aligning Sentences</title>
<date>1991</date>
<booktitle>in Bilingual Corpora, ACL-91,</booktitle>
<pages>177--184</pages>
<marker>A, Church, 1991</marker>
<rawString>Gale. W.A. and Church, K.W., A Program for Aligning Sentences in Bilingual Corpora, ACL-91, pp.177-184, 1991b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A</author>
<author>K W Church</author>
</authors>
<title>Identifying Word Correspondences</title>
<date>1991</date>
<booktitle>in Parallel Texts, &apos;91 DARPA Speech and Natural Language Workshop,</booktitle>
<pages>152--157</pages>
<marker>A, Church, 1991</marker>
<rawString>Gale. W.A. and Church, K.W., Identifying Word Correspondences in Parallel Texts, &apos;91 DARPA Speech and Natural Language Workshop, pp.152-157, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kaji</author>
<author>Y Kida</author>
<author>Y Morimoto</author>
</authors>
<date>1992</date>
<booktitle>Learning Translation Templates from Bilingual Text, COLING-92,</booktitle>
<pages>672--678</pages>
<marker>Kaji, Kida, Morimoto, 1992</marker>
<rawString>Kaji, H., Kida., Y., and Morimoto, Y., Learning Translation Templates from Bilingual Text, COLING-92, pp.672-678, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kasper</author>
</authors>
<title>A Unification Method for Disjunctive Feature Descriptions,</title>
<date>1987</date>
<volume>87</volume>
<pages>235--242</pages>
<marker>Kasper, 1987</marker>
<rawString>Kasper, R., A Unification Method for Disjunctive Feature Descriptions, ACL-87, pp.235-242, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<date>1990</date>
<booktitle>The BICORD System: Combining Lexical Information from Bilingual Corpora and Machine Readable Dictionaries, COLING-90,</booktitle>
<pages>174--179</pages>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>Klavans, J. and Tzoukermann, E., The BICORD System: Combining Lexical Information from Bilingual Corpora and Machine Readable Dictionaries, COLING-90, pp.174-179, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Five Papers on WordNet,</title>
<date>1990</date>
<tech>CSL Report 4.9,</tech>
<institution>Cognitive Science Laboratory, Princeton University,</institution>
<marker>Miller, 1990</marker>
<rawString>Miller, G.A., et al., Five Papers on WordNet, Cognitive Science Laboratory, Princeton University, CSL Report 4.9, July 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Roget</author>
</authors>
<title>Roget&apos;s Thesaurus,</title>
<date>1911</date>
<location>Crowell Co.,</location>
<marker>Roget, 1911</marker>
<rawString>Roget, S.R., Roget&apos;s Thesaurus, Crowell Co., 1911.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>The Textual Knowledge Bank: Design, Construction, Applications,</title>
<date>1991</date>
<booktitle>Proc. International Workshop on Fundamental Research for the Future Generation of Natural Language Processing (FGNLP),</booktitle>
<pages>17--32</pages>
<location>Kyoto, Japan,</location>
<marker>Sadler, 1991</marker>
<rawString>Sadler, V., The Textual Knowledge Bank: Design, Construction, Applications, Proc. International Workshop on Fundamental Research for the Future Generation of Natural Language Processing (FGNLP), pp.17-32, Kyoto, Japan, 1991.</rawString>
</citation>
<citation valid="false">
<date>1979</date>
<editor>Shimizu, M., et al. (ed.), Japanese-English</editor>
<publisher>Dictionary, Kodansha,</publisher>
<marker>1979</marker>
<rawString>Shimizu, M., et al. (ed.), Japanese-English Dictionary, Kodansha, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>Y Matsumoto</author>
<author>M Nagao</author>
</authors>
<date>1992</date>
<journal>Lexical Knowledge Acquisition from Bilingual Corpora,</journal>
<volume>92</volume>
<pages>581--587</pages>
<marker>Utsuro, Matsumoto, Nagao, 1992</marker>
<rawString>Utsuro, T., Matsumoto, Y., and Nagao, M., Lexical Knowledge Acquisition from Bilingual Corpora, COLING-92, pp.581-587, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>Y Matsumoto</author>
<author>M Naga o</author>
</authors>
<title>Verbal Case Frame Acquisition from Bilingual Corpora, to appear LICAI-93,</title>
<date>1993</date>
<marker>Utsuro, Matsumoto, o, 1993</marker>
<rawString>Utsuro, T., Matsumoto, Y., and Naga.o, M., Verbal Case Frame Acquisition from Bilingual Corpora, to appear LICAI-93, 1993.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>