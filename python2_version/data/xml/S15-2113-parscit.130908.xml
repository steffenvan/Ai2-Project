<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.063079">
<title confidence="0.962113">
LLT-PolyU: Identifying Sentiment Intensity in Ironic Tweets
</title>
<author confidence="0.949709">
Hongzhi Xu, Enrico Santus, Anna Laszlo and Chu-Ren Huang
</author>
<affiliation confidence="0.686251">
The Department of Chinese and Bilingual Studies
The Hong Kong Polytechnic University
</affiliation>
<email confidence="0.985431333333333">
{hongz.xu, esantus}@gmail.com
mandarin1985@yahoo.de
churenhuang@gmail.com
</email>
<sectionHeader confidence="0.995523" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999385117647059">
In this paper, we describe the system we built
for Task 11 of SemEval2015, which aims at i-
dentifying the sentiment intensity of figurative
language in tweets. We use various features,
including those specially concerned with the
identification of irony and sarcasm. The fea-
tures are evaluated through a decision tree re-
gression model and a support vector regres-
sion model. The experiment result of the five-
cross validation on the training data shows that
the tree regression model outperforms the sup-
port vector regression model. The former is
therefore used for the final evaluation of the
task. The results show that our model per-
forms especially well in predicting the senti-
ment intensity of tweets involving irony and
sarcasm.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999743833333333">
Sentiment analysis aims to identify the polarity and
intensity of certain texts in order to shed light on
people’s sentiments, perceptions, opinions, and be-
liefs about a particular product, service, scheme, etc.
Knowing what people think can, in fact, help com-
panies, political parties, and other public entities in
strategizing and decision making.
While impressive results have been achieved in
analysing literal texts (Abbasi et al., 2008; Yan et
al., 2014), the study of polarity shifting in sentiment
analysis still requires much research. For example,
Li, et.al. (2010), explores the polarity shifters in En-
glish which significantly improve the performance
of sentiment analysis. Besides, figurative uses of
language, such as irony or sarcasm, are also able
to invert the polarity of the surface text. Theoreti-
cal research in irony and sarcasm often emphasize
that humans have difficulties in deciphering mes-
sages with underlying meaning (Hay, 2001; Kot-
thoff, 2003; Kreuz and Caucci, 2007). Factors that
can facilitate the understanding of these messages
include prosody (e.g. stress or intonation), kinesics
(e.g. facial gestures), co-text (i.e. immediate textual
environment) and context (i.e. wider environment),
as well as cultural background. Computers, howev-
er, cannot always rely on this kind of information.
Currently, there is no method that can guaran-
tee the unequivocal recognition of irony or sarcas-
m. Training a computer to perform such a high-
ly pragmatic task does indeed pose a challenge to
computational linguists. A good number of studies
have been recently devoted to finding a solution to
the problem. Most of them have focused on tweet-
s (Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013;
Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et
al., 2014; Vanzo et al., 2014).
Identifying figurative language in short messages
(generally consisting of no more than 140 character-
s) that do not make use of conventional language,
but employ “little space-consuming” elements, such
as emoticons (“:D”), abbreviations (“abbr.”) and s-
lang (“slng”) is not a self-evident task. The reason
why none of these studies has proved to be the rep-
resentative method that could widely be adopted and
applied by other researchers is that they have not
yet reached optimal results. Thus, the devising of
a computational model able to accurately detect po-
larity is very much on-going.
</bodyText>
<page confidence="0.990777">
673
</page>
<bodyText confidence="0.8822202">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 673–678,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
This paper describes the model we developed
for Task 11 of SemEval-2015 (Ghosh et al., 2015),
which is concerned with the Sentiment Analysis of
Figurative Language in Twitter. Our model came
first in the SemEval-2015 task for irony and third
in the overall ranking, showing that the features we
proposed produce more reliable results in sentiment
analysis of ironic tweets.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974538461538">
Irony is defined by Quintilian in the first century CE
as “saying the opposite of what you mean” (Quintil-
ian, 1922). It violates the expectations of the listen-
er by flouting the maxim of quality (Grice, 1975;
Stringfellow Jr, 1994; Gibbs and Colston, 2007;
Tungthamthiti et al., 2014). In the same fashion, sar-
casm is generally understood as the use of irony “to
mock or convey contempt” (Stevenson, 2010).
While irony and sarcasm are well studied in lin-
guistics and psychology, their automatic identifica-
tion through Natural Language Processing methods
is a relatively novel task (Pang and Lee, 2008). Not
to mention that irony and sarcasm pose a difficult
problem in Sentiment Analysis of micro blogging
and social media (Barbieri et al., 2014).
Up to this date, several approaches have been pro-
posed to automatically identify irony and sarcasm
in tweets and comments. Carvalho et al. (2009),
for example, proposed to identify irony in com-
ments to newspaper articles by relying on the pres-
ence of emoticons, onomatopoeic expressions, and
heavy punctuation in the text surface. Hao and
Veale (2010) have investigated similes of the form
“x as y” in a large corpus, proposing a method to au-
tomatically discriminate ironic from non-ironic sim-
iles. Tsur et al. (2010) proposed a semi-supervised
approach for the automatic recognition of sarcasm in
Amazon product reviews, exploiting some features
that were specific to Amazon. Their method em-
ployed two modules: a semi-supervised acquisition
of sarcastic patterns and a classifier. This method
was then applied to tweets by Davidov et al. (2010),
achieving even better results. Gonz´alez-Ib´a˜nez et
al. (2011) constructed a corpus of sarcastic tweets
and used it to compare judgements made by human-
s and machine learning algorithms, concluding that
none of them performed well.
More recently, Reyes et al. (2013) defined a com-
plex model for identifying sarcasm which goes far
behind the surface of the text and takes into accoun-
t features on four levels: signatures, degree of un-
expectedness, style, and emotional scenarios. They
have demonstrated that these features do not help
the identification in isolation. However, they do if
they are combined in a complex framework. Bar-
bieri and Saggion (2014) focused their approach on
the use of lexical and semantic features, such as the
frequency of the words in different reference corpo-
ra, the length of the words, and the number of related
synsets in WordNet (Miller and Fellbaum, 1998).
Finally, Buschmeier et al. (2014) assessed the im-
pact of features used in previous studies, and they
provide an important baseline for irony detection in
English.
Many datasets for the study of irony and sarcasm
in Twitter are nowadays available. Thanks to the use
of hashtags, it is easier to collect data with specific
characteristics in Twitter. Reyes et al. (2013), for
example, created a corpus of 40.000 tweets with four
categories: Irony, Education, Humour, and Politics.
Among the other resources, it is worth mentioning
the sarcastic Amazon product reviews collected by
Filatova (2012) and the Italian examples collected
and annotated by Gianti et al. (2012), later used in
Bosco et al. (2013).
</bodyText>
<sectionHeader confidence="0.999713" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999602">
3.1 Data Pre-processing
</subsectionHeader>
<bodyText confidence="0.999981375">
Considering the unregulated and arbitrary nature of
the texts we are working with, we use some heuristic
rules to pre-process them. These rules help us get
more reliable syntactic structures when calling the
syntactic parser.
Twitter users often use repeated vowels (e.g.
“loooove”) or capitalization (e.g. “LOVE”) to em-
phasize certain sentiments or emotions. The nor-
malization consists of removing the repeated vowels
(e.g. from “loooove” to “love”) and the capitaliza-
tion (e.g. from “LOVE” to “love”). The normal-
ized forms can help improve the parsing accuracy.
Moreover, they are saved in a special feature bag
as they are important indicators of sentiments, es-
pecially when they are in sentiment lexicons. Other
special uses of language in tweets include the so-
</bodyText>
<page confidence="0.995645">
674
</page>
<bodyText confidence="0.999981222222222">
called heavy punctuation and emoticons. In our sys-
tem, we substitute every combination of exclama-
tion and question marks (e.g. “?!?!!”) with the for-
m “?!”. We also compiled an emoticon dictionary
based on training data and internet resources.
Another step that we considered relevant at this
point is the maximal matching segmentation. The
segmentation is, in fact, often lost in tweets, as white
spaces and punctuation are not always used in their
customary format (e.g. “yeahright”). In order to get
rid of this problem, we tried to segment all the out
of vocabulary tokens through a maximal matching
algorithm according to an English dictionary (e.g.
the token “yeahright” would be segmented as “yeah
right”).
Finally, we use Stanford parser (Klein and Man-
ning, 2003) to get the POS tags and dependency
structures of the normalized tweets.
</bodyText>
<subsectionHeader confidence="0.997184">
3.2 Feature Set
</subsectionHeader>
<bodyText confidence="0.999787933333333">
After the pre-processing, we then extract features of
the following kinds.
UniToken Token uni-grams are the basic features
in our approach. The normalized forms of the em-
phasized tokens are put in a special bag with tags de-
scribing their emphasis types {duplicate vowel, cap-
italized, heavy punctuation, emoticon}
BiToken Bi-grams of the normalized tokens are
also used as features.
DepTokenPair The “parent-child” pairs based on
dependency structures are also used as features.
PolarityWin In order to identify the polarity val-
ues of tokens, we used four sentiment dictio-
naries: Opinion Lexicon (Hu and Liu, 2004),
Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005),
and SentiWordnet (Baccianella et al., 2010). Their
union and their intersection are also used as two ad-
ditional dictionaries. A window size of five is used
to verify whether negations are present. If a nega-
tion is present the resulting value is set to zero. Six
features are used to save the sum polarity values of
a tweet based on the six dictionaries respectively.
Besides, we also use features recording the polarity
contribution of different POS tags. For example, one
possible feature-value pair can be (adj-mpqa, 1.0)
meaning that according to the dictionary MPQA,
the sum of the polarity contributed by adjectives in
the current tweet is 1.0.
PolarityDep This feature set is similar to
PolarityWin, but it differs in that the negation is
checked in the dependency structure.
PolarShiftWin This feature set is designed for
irony which has been discussed in (Riloff et al.,
2013). Let us consider the tweets (1) “I love work-
ing for eight hours without any break” and (2) “I
hate people giving me such a big surprise”. In these
tweets the verbs “love” (positive) and “hate” (neg-
ative) are used with reference to a negative and a
positive clause (“working for eight hours without
any break” and “people giving me such a big sur-
prise”) respectively. Based on a 5-window we check
whether a shift of polarity is present.
PolarShiftDep This feature set is similar to
Polar5hiftWin, but it differs in that the shift is
checked in the dependency structure.
</bodyText>
<subsectionHeader confidence="0.999182">
3.3 Feature Normalization and Evaluation
</subsectionHeader>
<bodyText confidence="0.999896666666667">
In order to avoid noise and sparseness, only features
that occur at least 3 times are kept. All the feature
values are normalized into the range [-1, 1] accord-
ing to the formula shown in Equation 1, where fi,j
is the value of feature j in the ith example, and N is
the sample size.
</bodyText>
<equation confidence="0.994955">
norm(fi,j) = max i |fk,j |(1)
1 &lt;k&lt;N
</equation>
<bodyText confidence="0.999988142857143">
We use the correlative coefficient (Pearson’s r)
measure to rank all the features. Then, we can use
the threshold value of r to rule out less important
features. The calculation of r is described in Equa-
tion 2, where X and Y are the two variables that are
evaluated, Xi is the ith sample value of X, Yi is the
ith sample value of Y and N is the sample size.
</bodyText>
<equation confidence="0.921728">
√∑Ni�1 (Xi −�X )2&apos;, /EN 1 (Yi − Y�)2
�/ (2)
</equation>
<bodyText confidence="0.999707666666667">
The goal of the first experiment is to find the opti-
mal threshold value of r with all the features as listed
in 3.2. Two different models are used: Decision Tree
</bodyText>
<equation confidence="0.996828">
r(X, Y ) =
∑N 1 (Xi − X) (Yi − Y�)
</equation>
<page confidence="0.997054">
675
</page>
<table confidence="0.999566266666667">
Feature Set Features mse cosine
Baseline N/A 1.9847 0.8184
UniToken 136 1.6821 0.8507
+BiToken 410 1.7007 0.8485
+DepTokenPair 409 1.6733 0.8514
+PolarityWin 582 1.6573 0.8524
+PolarityDep 748 1.6436 0.8536
+PolarShiftWin 825 1.6403 0.8542
+PolarShiftDep 841 1.6393 0.8543
category mse cosine
Sarcasm 0.997 0.896
Irony 0.671 0.918
Metaphor 3.917 0.535
Other 4.617 0.290
Overall 2.602 0.687
</table>
<tableCaption confidence="0.957445666666667">
Table 2: Test result of SemEval Task 11.
Table 1: Experiment result of the 5-fold cross validation
by RegTree and SVR on the training data.
</tableCaption>
<bodyText confidence="0.9981843">
Regression model (RepTree) implemented in We-
ka (Hall et al., 2009) and Support Vector Regression
model (SVR) implemented in LibSVM (Chang and
Lin, 2011). The result is shown in Figure 1. The best
performance is obtained with the value of r between
0.03 and 0.04 with the RepTree model. The exper-
iment also shows that RepTree always outperforms
SVR (i.e. higher cosine value and lower rmse val-
ue). Therefore, in the following experiments and in
the evaluation the RepTree model is adopted.
</bodyText>
<figureCaption confidence="0.997198">
Figure 1: Effect of Pearson value threshold on the overall
performance in cosine (left) and root mean squared error
(right).
</figureCaption>
<bodyText confidence="0.9999465">
In the second experiment, we use r = 0.035 as
threshold for feature selection by testing how differ-
ent kinds of features contribute to the overall perfor-
mance. The features listed in Section 3.2 are gradu-
ally added and their contribution is assessed. If the
new feature does not improve the performance, it is
removed in the next running. The results of the sec-
ond experiment are shown in Table 1. The baseline
is obtained with a naive prediction using the aver-
age polarity value of the training data. As can be
seen, only BiToken harms the performance, while
all other features contribute to its improvement.
</bodyText>
<subsectionHeader confidence="0.981381">
3.4 Evaluation Result
</subsectionHeader>
<bodyText confidence="0.99997725">
Based on the described analysis, for the final test
we used RepTree and all the feature sets, except for
BiToken. The threshold for feature frequency is
set to 3 and the r value for feature selection is set
to 0.035. Finally, the trained model on the 8,000
tweets is used to predict the sentiment intensity of
the evaluation dataset which includes 4,000 tweets.
The results are shown in Table 2. Among the fifteen
participants in the SemEval task on Sentiment Anal-
ysis of Figurative Language in Twitter, our model
achieves the best performance in the identification
of irony, and ranks third in the overall performance.
</bodyText>
<sectionHeader confidence="0.997447" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999612214285714">
In this paper, we introduced our model for the Senti-
mentAnalysis of Figurative Language in Twitter fol-
lowing the track of Task 11 of SemEval 2015. We
first used heuristic rules to pre-process the tweets
by identifying and normalizing the emphasized to-
kens. Then, features were extracted based on both
window and dependency structures. We adopted
polarity shift features with special consideration on
the identification of irony. As expected, our system
performed best in predicting the sentiment intensity
of tweets containing irony according to the evalua-
tion. This confirms the robustness of our design and
points to promising development of automatic pro-
cessing of irony in the future.
</bodyText>
<sectionHeader confidence="0.989775" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998048">
The work is supported by a General Research Fund
(GRF) sponsored by the Research Grants Council
(Project no. 543512 &amp; 543810). This work is par-
tially supported by HK PhD Fellowship Scheme, un-
der PF11-00122 and PF12-13656.
</bodyText>
<page confidence="0.998908">
676
</page>
<sectionHeader confidence="0.949003" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.949123858490566">
Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008.
Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums.
ACM Transactions on Information Systems (TOIS),
26(3):12:1–12:34.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC, volume 10, pages 2200–2204.
Francesco Barbieri and Horacio Saggion. 2014. Mod-
elling irony in twitter. In Proceedings of the Student
Research Workshop at the 14th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 56–64.
Francesco Barbieri, Francesco Ronzano, and Horacio
Saggion. 2014. Italian irony detection in twitter: a
first approach. In The First Italian Conference on
Computational Linguistics CLiC-it 2014 &amp; the Fourth
International Workshop EVALITA 2014, pages 28–32.
Cristina Bosco, Viviana Patti, and Andrea Bolioli. 2013.
Developing corpora for sentiment analysis and opinion
mining: the case of irony and senti-tut. IEEE Intelli-
gent Systems, 28(2):55–63.
Konstantin Buschmeier, Philipp Cimiano, and Roman K-
linger. 2014. An impact analysis of features in a
classification approach to irony detection in produc-
t reviews.
Paula Carvalho, Lu´ıs Sarmento, M´ario J Silva, and Eu-
g´enio de Oliveira. 2009. Clues for detecting irony in
user-generated contents: oh...!! it’s so easy;-). In Pro-
ceedings of the 1st international CIKM workshop on
Topic-sentiment analysis for mass opinion, pages 53–
56. ACM.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:1–27.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107–116.
Elena Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In Proceed-
ings of Language Resources and Evaluation Confer-
ence, pages 392–398.
Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso,
Ekaterina Shutova, Antonio Reyes, and John Barn-
den. 2015. Semeval-2015 task 11: Sentiment anal-
ysis of figurative language in twitter. In Proceedings
of the International Workshop on Semantic Evaluation
(SemEval-2015), Co-located with NAACL and *SEM,
Denver, Colorado, US, June 4-5.
Andrea Gianti, Cristina Bosco, Viviana Patti, Andrea Bo-
lioli, and Luigi Di Caro. 2012. Annotating irony in
a novel italian corpus for sentiment analysis. In Pro-
ceedings of the 4th Workshop on Corpora for Research
on Emotion Sentiment and Social Signals, pages 1–7.
Raymond W Gibbs and Herbert L Colston. 2007. Irony
in language and thought: A cognitive science reader.
Psychology Press.
Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: a
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistic-
s: Human Language Technologies: short papers, vol-
ume 2, pages 581–586.
H Paul Grice, 1975. Logic and conversation, volume 3 of
Syntax and Semantics, pages 41–58. Academic Press,
New York.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard P-
fahringer, Peter Reutemann, and Ian H. Witten. 2009.
The weka data mining software: An update. SIGKDD
Explorations, 11.
Yanfen Hao and Tony Veale. 2010. An ironic fist
in a velvet glove: Creative mis-representation in the
construction of ironic similes. Minds and Machines,
20(4):635–650.
Jennifer Hay. 2001. The pragmatics of humor support.
International Journal of Humor Research, 14:1–27.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computation-
al Linguistics-Volume 1, pages 423–430.
Helga Kotthoff. 2003. Responding to irony in differen-
t contexts: On cognition in conversation. Journal of
pragmatics, 35(9):1387–1411.
Roger J Kreuz and Gina M Caucci. 2007. Lexical influ-
ences on the perception of sarcasm. In Proceedings of
the Workshop on computational approaches to Figura-
tive Language, pages 1–4.
Shoushan Li, Sophia Yat Mei Lee, Ying Chen, Chu-Ren
Huang, and Guodong Zhou. 2010. Sentiment clas-
sification and polarity shifting. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 635–643.
Christine Liebrecht, Florian Kunneman, and Antal
van den Bosch. 2013. The perfect solution for de-
tecting sarcasm in tweets# not. In Proceedings of the
4th Workshop on Computational Approaches to Sub-
jectivity, Sentiment and Social Media Analysis. New
Brunswick, NJ: ACL.
</reference>
<page confidence="0.980307">
677
</page>
<reference confidence="0.999773555555556">
George Miller and Christiane Fellbaum. 1998. Wordnet:
An electronic lexical database.
Finn ˚Arup Nielsen. 2011. Anew anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ‘Making
Sense of Microposts’: Big things come in small pack-
ages.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Quintilian. 1922. With An English Translation. Harold
Edgeworth Butler. Cambridge, Mass., Harvard Uni-
versity Press; London, William Heinemann, Ltd.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A
multidimensional approach for detecting irony in twit-
ter. Language Resources and Evaluation, 47(1):239–
268.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De
Silva, Nathan Gilbert, and Ruihong Huang. 2013.
Sarcasm as contrast between a positive sentiment and
negative situation. In Proceedings of Conference on
Emperical Methods for Natural Language Processing
(EMNLP), pages 704–714.
Angus Stevenson. 2010. Oxford dictionary of English.
Oxford University Press.
Frank Stringfellow Jr. 1994. The meaning of irony: A
psychoanalytic investigation. SUNY Press.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm-a great catchy name: Semi-supervised recogni-
tion of sarcastic sentences in online product reviews.
In In International AAAI Conference on Web and So-
cial Media.
Piyoros Tungthamthiti, Shirai Kiyoaki, and Masnizah
Mohd. 2014. Recognition of sarcasms in tweets
based on concept level sentiment analysis and super-
vised learning approaches. In Proceedings of Pacific
Asia Conference on Language, Information and Com-
puting, Phuket, Thailand.
Andrea Vanzo, Giuseppe Castellucci, Danilo Croce, and
Roberto Basili. 2014. A context based model for sen-
timent analysis in twitter for the italian language. In
R. Basili, A. Lenci, and B. Magnini, editors, Proceed-
ings of the First Italian Conference on Computational
Linguistics CLiC-it &amp; the Fourth International Work-
shop EVALITA, pages 379–383, Pisa. Pisa University
Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language resources and evaluation, 39(2-
3):165–210.
Gongjun Yan, Wu He, Jiancheng Shen, and Chuany-
i Tang. 2014. A bilingual approach for conducting
chinese and english social media sentiment analysis.
Computer Networks, 75:491–503.
</reference>
<page confidence="0.997715">
678
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629033">
<title confidence="0.883007333333333">LLT-PolyU: Identifying Sentiment Intensity in Ironic Tweets Hongzhi Xu, Enrico Santus, Anna Laszlo and Chu-Ren The Department of Chinese and Bilingual</title>
<author confidence="0.902283">The Hong Kong Polytechnic</author>
<email confidence="0.999146">churenhuang@gmail.com</email>
<abstract confidence="0.9946055">In this paper, we describe the system we built for Task 11 of SemEval2015, which aims at identifying the sentiment intensity of figurative language in tweets. We use various features, including those specially concerned with the identification of irony and sarcasm. The features are evaluated through a decision tree regression model and a support vector regression model. The experiment result of the fivecross validation on the training data shows that the tree regression model outperforms the support vector regression model. The former is therefore used for the final evaluation of the task. The results show that our model performs especially well in predicting the sentiment intensity of tweets involving irony and sarcasm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
<author>Arab Salem</author>
</authors>
<title>Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums.</title>
<date>2008</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="1472" citStr="Abbasi et al., 2008" startWordPosition="218" endWordPosition="221">inal evaluation of the task. The results show that our model performs especially well in predicting the sentiment intensity of tweets involving irony and sarcasm. 1 Introduction Sentiment analysis aims to identify the polarity and intensity of certain texts in order to shed light on people’s sentiments, perceptions, opinions, and beliefs about a particular product, service, scheme, etc. Knowing what people think can, in fact, help companies, political parties, and other public entities in strategizing and decision making. While impressive results have been achieved in analysing literal texts (Abbasi et al., 2008; Yan et al., 2014), the study of polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of language, such as irony or sarcasm, are also able to invert the polarity of the surface text. Theoretical research in irony and sarcasm often emphasize that humans have difficulties in deciphering messages with underlying meaning (Hay, 2001; Kotthoff, 2003; Kreuz and Caucci, 2007). Factors that can facilitate the understandin</context>
</contexts>
<marker>Abbasi, Chen, Salem, 2008</marker>
<rawString>Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums. ACM Transactions on Information Systems (TOIS), 26(3):12:1–12:34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>2200--2204</pages>
<contexts>
<context position="9566" citStr="Baccianella et al., 2010" startWordPosition="1507" endWordPosition="1510"> Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify the polarity values of tokens, we used four sentiment dictionaries: Opinion Lexicon (Hu and Liu, 2004), Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005), and SentiWordnet (Baccianella et al., 2010). Their union and their intersection are also used as two additional dictionaries. A window size of five is used to verify whether negations are present. If a negation is present the resulting value is set to zero. Six features are used to save the sum polarity values of a tweet based on the six dictionaries respectively. Besides, we also use features recording the polarity contribution of different POS tags. For example, one possible feature-value pair can be (adj-mpqa, 1.0) meaning that according to the dictionary MPQA, the sum of the polarity contributed by adjectives in the current tweet i</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Barbieri</author>
<author>Horacio Saggion</author>
</authors>
<title>Modelling irony in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>56--64</pages>
<contexts>
<context position="6253" citStr="Barbieri and Saggion (2014)" startWordPosition="976" endWordPosition="980">. Gonz´alez-Ib´a˜nez et al. (2011) constructed a corpus of sarcastic tweets and used it to compare judgements made by humans and machine learning algorithms, concluding that none of them performed well. More recently, Reyes et al. (2013) defined a complex model for identifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are combined in a complex framework. Barbieri and Saggion (2014) focused their approach on the use of lexical and semantic features, such as the frequency of the words in different reference corpora, the length of the words, and the number of related synsets in WordNet (Miller and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (20</context>
</contexts>
<marker>Barbieri, Saggion, 2014</marker>
<rawString>Francesco Barbieri and Horacio Saggion. 2014. Modelling irony in twitter. In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 56–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Barbieri</author>
<author>Francesco Ronzano</author>
<author>Horacio Saggion</author>
</authors>
<title>Italian irony detection in twitter: a first approach.</title>
<date>2014</date>
<booktitle>In The First Italian Conference on Computational Linguistics CLiC-it 2014 &amp; the Fourth International Workshop EVALITA 2014,</booktitle>
<pages>28--32</pages>
<contexts>
<context position="2814" citStr="Barbieri et al., 2014" startWordPosition="429" endWordPosition="432"> textual environment) and context (i.e. wider environment), as well as cultural background. Computers, however, cannot always rely on this kind of information. Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very muc</context>
<context position="4748" citStr="Barbieri et al., 2014" startWordPosition="737" endWordPosition="740">ates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et al. (2009), for example, proposed to identify irony in comments to newspaper articles by relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically discriminate ironic from non-ironic similes. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in </context>
</contexts>
<marker>Barbieri, Ronzano, Saggion, 2014</marker>
<rawString>Francesco Barbieri, Francesco Ronzano, and Horacio Saggion. 2014. Italian irony detection in twitter: a first approach. In The First Italian Conference on Computational Linguistics CLiC-it 2014 &amp; the Fourth International Workshop EVALITA 2014, pages 28–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Bosco</author>
<author>Viviana Patti</author>
<author>Andrea Bolioli</author>
</authors>
<title>Developing corpora for sentiment analysis and opinion mining: the case of irony and senti-tut.</title>
<date>2013</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="7190" citStr="Bosco et al. (2013)" startWordPosition="1130" endWordPosition="1133">studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and annotated by Gianti et al. (2012), later used in Bosco et al. (2013). 3 Methodology 3.1 Data Pre-processing Considering the unregulated and arbitrary nature of the texts we are working with, we use some heuristic rules to pre-process them. These rules help us get more reliable syntactic structures when calling the syntactic parser. Twitter users often use repeated vowels (e.g. “loooove”) or capitalization (e.g. “LOVE”) to emphasize certain sentiments or emotions. The normalization consists of removing the repeated vowels (e.g. from “loooove” to “love”) and the capitalization (e.g. from “LOVE” to “love”). The normalized forms can help improve the parsing accura</context>
</contexts>
<marker>Bosco, Patti, Bolioli, 2013</marker>
<rawString>Cristina Bosco, Viviana Patti, and Andrea Bolioli. 2013. Developing corpora for sentiment analysis and opinion mining: the case of irony and senti-tut. IEEE Intelligent Systems, 28(2):55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantin Buschmeier</author>
<author>Philipp Cimiano</author>
<author>Roman Klinger</author>
</authors>
<title>An impact analysis of features in a classification approach to irony detection in product reviews.</title>
<date>2014</date>
<contexts>
<context position="6521" citStr="Buschmeier et al. (2014)" startWordPosition="1022" endWordPosition="1025">ifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are combined in a complex framework. Barbieri and Saggion (2014) focused their approach on the use of lexical and semantic features, such as the frequency of the words in different reference corpora, the length of the words, and the number of related synsets in WordNet (Miller and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and</context>
</contexts>
<marker>Buschmeier, Cimiano, Klinger, 2014</marker>
<rawString>Konstantin Buschmeier, Philipp Cimiano, and Roman Klinger. 2014. An impact analysis of features in a classification approach to irony detection in product reviews.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Carvalho</author>
<author>Lu´ıs Sarmento</author>
<author>M´ario J Silva</author>
<author>Eug´enio de Oliveira</author>
</authors>
<title>Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-).</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion,</booktitle>
<pages>53--56</pages>
<publisher>ACM.</publisher>
<marker>Carvalho, Sarmento, Silva, de Oliveira, 2009</marker>
<rawString>Paula Carvalho, Lu´ıs Sarmento, M´ario J Silva, and Eug´enio de Oliveira. 2009. Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-). In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 53– 56. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--1</pages>
<contexts>
<context position="12622" citStr="Chang and Lin, 2011" startWordPosition="2045" endWordPosition="2048">Token 136 1.6821 0.8507 +BiToken 410 1.7007 0.8485 +DepTokenPair 409 1.6733 0.8514 +PolarityWin 582 1.6573 0.8524 +PolarityDep 748 1.6436 0.8536 +PolarShiftWin 825 1.6403 0.8542 +PolarShiftDep 841 1.6393 0.8543 category mse cosine Sarcasm 0.997 0.896 Irony 0.671 0.918 Metaphor 3.917 0.535 Other 4.617 0.290 Overall 2.602 0.687 Table 2: Test result of SemEval Task 11. Table 1: Experiment result of the 5-fold cross validation by RegTree and SVR on the training data. Regression model (RepTree) implemented in Weka (Hall et al., 2009) and Support Vector Regression model (SVR) implemented in LibSVM (Chang and Lin, 2011). The result is shown in Figure 1. The best performance is obtained with the value of r between 0.03 and 0.04 with the RepTree model. The experiment also shows that RepTree always outperforms SVR (i.e. higher cosine value and lower rmse value). Therefore, in the following experiments and in the evaluation the RepTree model is adopted. Figure 1: Effect of Pearson value threshold on the overall performance in cosine (left) and root mean squared error (right). In the second experiment, we use r = 0.035 as threshold for feature selection by testing how different kinds of features contribute to the</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>107--116</pages>
<contexts>
<context position="5595" citStr="Davidov et al. (2010)" startWordPosition="872" endWordPosition="875">relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically discriminate ironic from non-ironic similes. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in Amazon product reviews, exploiting some features that were specific to Amazon. Their method employed two modules: a semi-supervised acquisition of sarcastic patterns and a classifier. This method was then applied to tweets by Davidov et al. (2010), achieving even better results. Gonz´alez-Ib´a˜nez et al. (2011) constructed a corpus of sarcastic tweets and used it to compare judgements made by humans and machine learning algorithms, concluding that none of them performed well. More recently, Reyes et al. (2013) defined a complex model for identifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are co</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
</authors>
<title>Irony and sarcasm: Corpus generation and analysis using crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of Language Resources and Evaluation Conference,</booktitle>
<pages>392--398</pages>
<contexts>
<context position="7082" citStr="Filatova (2012)" startWordPosition="1113" endWordPosition="1114">and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and annotated by Gianti et al. (2012), later used in Bosco et al. (2013). 3 Methodology 3.1 Data Pre-processing Considering the unregulated and arbitrary nature of the texts we are working with, we use some heuristic rules to pre-process them. These rules help us get more reliable syntactic structures when calling the syntactic parser. Twitter users often use repeated vowels (e.g. “loooove”) or capitalization (e.g. “LOVE”) to emphasize certain sentiments or emotions. The normalization consists of removing the repeated vowels (e.g. from “loooove” to “love”) a</context>
</contexts>
<marker>Filatova, 2012</marker>
<rawString>Elena Filatova. 2012. Irony and sarcasm: Corpus generation and analysis using crowdsourcing. In Proceedings of Language Resources and Evaluation Conference, pages 392–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniruddha Ghosh</author>
<author>Guofu Li</author>
<author>Tony Veale</author>
<author>Paolo Rosso</author>
<author>Ekaterina Shutova</author>
<author>Antonio Reyes</author>
<author>John Barnden</author>
</authors>
<title>Semeval-2015 task 11: Sentiment analysis of figurative language in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2015), Co-located with NAACL and *SEM,</booktitle>
<pages>4--5</pages>
<location>Denver, Colorado, US,</location>
<contexts>
<context position="3705" citStr="Ghosh et al., 2015" startWordPosition="567" endWordPosition="570">d slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very much on-going. 673 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 673–678, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics This paper describes the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashi</context>
</contexts>
<marker>Ghosh, Li, Veale, Rosso, Shutova, Reyes, Barnden, 2015</marker>
<rawString>Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, Antonio Reyes, and John Barnden. 2015. Semeval-2015 task 11: Sentiment analysis of figurative language in twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval-2015), Co-located with NAACL and *SEM, Denver, Colorado, US, June 4-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gianti</author>
<author>Cristina Bosco</author>
<author>Viviana Patti</author>
<author>Andrea Bolioli</author>
<author>Luigi Di Caro</author>
</authors>
<title>Annotating irony in a novel italian corpus for sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 4th Workshop on Corpora for Research on Emotion Sentiment and Social Signals,</booktitle>
<pages>1--7</pages>
<marker>Gianti, Bosco, Patti, Bolioli, Di Caro, 2012</marker>
<rawString>Andrea Gianti, Cristina Bosco, Viviana Patti, Andrea Bolioli, and Luigi Di Caro. 2012. Annotating irony in a novel italian corpus for sentiment analysis. In Proceedings of the 4th Workshop on Corpora for Research on Emotion Sentiment and Social Signals, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond W Gibbs</author>
<author>Herbert L Colston</author>
</authors>
<title>Irony in language and thought: A cognitive science reader.</title>
<date>2007</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="4257" citStr="Gibbs and Colston, 2007" startWordPosition="658" endWordPosition="661">the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweet</context>
</contexts>
<marker>Gibbs, Colston, 2007</marker>
<rawString>Raymond W Gibbs and Herbert L Colston. 2007. Irony in language and thought: A cognitive science reader. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: a closer look.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<volume>2</volume>
<pages>581--586</pages>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: a closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers, volume 2, pages 581–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation,</title>
<date>1975</date>
<booktitle>of Syntax and Semantics,</booktitle>
<volume>3</volume>
<pages>41--58</pages>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="4209" citStr="Grice, 1975" startWordPosition="653" endWordPosition="654">al Linguistics This paper describes the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to a</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H Paul Grice, 1975. Logic and conversation, volume 3 of Syntax and Semantics, pages 41–58. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<contexts>
<context position="12536" citStr="Hall et al., 2009" startWordPosition="2032" endWordPosition="2035">Xi − X) (Yi − Y�) 675 Feature Set Features mse cosine Baseline N/A 1.9847 0.8184 UniToken 136 1.6821 0.8507 +BiToken 410 1.7007 0.8485 +DepTokenPair 409 1.6733 0.8514 +PolarityWin 582 1.6573 0.8524 +PolarityDep 748 1.6436 0.8536 +PolarShiftWin 825 1.6403 0.8542 +PolarShiftDep 841 1.6393 0.8543 category mse cosine Sarcasm 0.997 0.896 Irony 0.671 0.918 Metaphor 3.917 0.535 Other 4.617 0.290 Overall 2.602 0.687 Table 2: Test result of SemEval Task 11. Table 1: Experiment result of the 5-fold cross validation by RegTree and SVR on the training data. Regression model (RepTree) implemented in Weka (Hall et al., 2009) and Support Vector Regression model (SVR) implemented in LibSVM (Chang and Lin, 2011). The result is shown in Figure 1. The best performance is obtained with the value of r between 0.03 and 0.04 with the RepTree model. The experiment also shows that RepTree always outperforms SVR (i.e. higher cosine value and lower rmse value). Therefore, in the following experiments and in the evaluation the RepTree model is adopted. Figure 1: Effect of Pearson value threshold on the overall performance in cosine (left) and root mean squared error (right). In the second experiment, we use r = 0.035 as thresh</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanfen Hao</author>
<author>Tony Veale</author>
</authors>
<title>An ironic fist in a velvet glove: Creative mis-representation in the construction of ironic similes.</title>
<date>2010</date>
<journal>Minds and Machines,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="5101" citStr="Hao and Veale (2010)" startWordPosition="794" endWordPosition="797">their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et al. (2009), for example, proposed to identify irony in comments to newspaper articles by relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically discriminate ironic from non-ironic similes. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in Amazon product reviews, exploiting some features that were specific to Amazon. Their method employed two modules: a semi-supervised acquisition of sarcastic patterns and a classifier. This method was then applied to tweets by Davidov et al. (2010), achieving even better results. Gonz´alez-Ib´a˜nez et al. (2011) constructed a corpus of sarcastic tweets</context>
</contexts>
<marker>Hao, Veale, 2010</marker>
<rawString>Yanfen Hao and Tony Veale. 2010. An ironic fist in a velvet glove: Creative mis-representation in the construction of ironic similes. Minds and Machines, 20(4):635–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Hay</author>
</authors>
<title>The pragmatics of humor support.</title>
<date>2001</date>
<journal>International Journal of Humor Research,</journal>
<pages>14--1</pages>
<contexts>
<context position="1985" citStr="Hay, 2001" startWordPosition="299" endWordPosition="300">ing. While impressive results have been achieved in analysing literal texts (Abbasi et al., 2008; Yan et al., 2014), the study of polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of language, such as irony or sarcasm, are also able to invert the polarity of the surface text. Theoretical research in irony and sarcasm often emphasize that humans have difficulties in deciphering messages with underlying meaning (Hay, 2001; Kotthoff, 2003; Kreuz and Caucci, 2007). Factors that can facilitate the understanding of these messages include prosody (e.g. stress or intonation), kinesics (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background. Computers, however, cannot always rely on this kind of information. Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of </context>
</contexts>
<marker>Hay, 2001</marker>
<rawString>Jennifer Hay. 2001. The pragmatics of humor support. International Journal of Humor Research, 14:1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9471" citStr="Hu and Liu, 2004" startWordPosition="1493" endWordPosition="1496">Set After the pre-processing, we then extract features of the following kinds. UniToken Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify the polarity values of tokens, we used four sentiment dictionaries: Opinion Lexicon (Hu and Liu, 2004), Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005), and SentiWordnet (Baccianella et al., 2010). Their union and their intersection are also used as two additional dictionaries. A window size of five is used to verify whether negations are present. If a negation is present the resulting value is set to zero. Six features are used to save the sum polarity values of a tweet based on the six dictionaries respectively. Besides, we also use features recording the polarity contribution of different POS tags. For example, one possible feature-value pair can be (adj-mpqa, 1.0) meaning that according t</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="8769" citStr="Klein and Manning, 2003" startWordPosition="1381" endWordPosition="1385">orm “?!”. We also compiled an emoticon dictionary based on training data and internet resources. Another step that we considered relevant at this point is the maximal matching segmentation. The segmentation is, in fact, often lost in tweets, as white spaces and punctuation are not always used in their customary format (e.g. “yeahright”). In order to get rid of this problem, we tried to segment all the out of vocabulary tokens through a maximal matching algorithm according to an English dictionary (e.g. the token “yeahright” would be segmented as “yeah right”). Finally, we use Stanford parser (Klein and Manning, 2003) to get the POS tags and dependency structures of the normalized tweets. 3.2 Feature Set After the pre-processing, we then extract features of the following kinds. UniToken Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helga Kotthoff</author>
</authors>
<title>Responding to irony in different contexts: On cognition in conversation.</title>
<date>2003</date>
<journal>Journal of pragmatics,</journal>
<volume>35</volume>
<issue>9</issue>
<contexts>
<context position="2001" citStr="Kotthoff, 2003" startWordPosition="301" endWordPosition="303">impressive results have been achieved in analysing literal texts (Abbasi et al., 2008; Yan et al., 2014), the study of polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of language, such as irony or sarcasm, are also able to invert the polarity of the surface text. Theoretical research in irony and sarcasm often emphasize that humans have difficulties in deciphering messages with underlying meaning (Hay, 2001; Kotthoff, 2003; Kreuz and Caucci, 2007). Factors that can facilitate the understanding of these messages include prosody (e.g. stress or intonation), kinesics (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background. Computers, however, cannot always rely on this kind of information. Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have bee</context>
</contexts>
<marker>Kotthoff, 2003</marker>
<rawString>Helga Kotthoff. 2003. Responding to irony in different contexts: On cognition in conversation. Journal of pragmatics, 35(9):1387–1411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger J Kreuz</author>
<author>Gina M Caucci</author>
</authors>
<title>Lexical influences on the perception of sarcasm.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on computational approaches to Figurative Language,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="2026" citStr="Kreuz and Caucci, 2007" startWordPosition="304" endWordPosition="307">ts have been achieved in analysing literal texts (Abbasi et al., 2008; Yan et al., 2014), the study of polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of language, such as irony or sarcasm, are also able to invert the polarity of the surface text. Theoretical research in irony and sarcasm often emphasize that humans have difficulties in deciphering messages with underlying meaning (Hay, 2001; Kotthoff, 2003; Kreuz and Caucci, 2007). Factors that can facilitate the understanding of these messages include prosody (e.g. stress or intonation), kinesics (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background. Computers, however, cannot always rely on this kind of information. Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to fin</context>
</contexts>
<marker>Kreuz, Caucci, 2007</marker>
<rawString>Roger J Kreuz and Gina M Caucci. 2007. Lexical influences on the perception of sarcasm. In Proceedings of the Workshop on computational approaches to Figurative Language, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Sophia Yat Mei Lee</author>
<author>Ying Chen</author>
<author>Chu-Ren Huang</author>
<author>Guodong Zhou</author>
</authors>
<title>Sentiment classification and polarity shifting.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>635--643</pages>
<marker>Li, Lee, Chen, Huang, Zhou, 2010</marker>
<rawString>Shoushan Li, Sophia Yat Mei Lee, Ying Chen, Chu-Ren Huang, and Guodong Zhou. 2010. Sentiment classification and polarity shifting. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 635–643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Liebrecht</author>
<author>Florian Kunneman</author>
<author>Antal van den Bosch</author>
</authors>
<title>The perfect solution for detecting sarcasm in tweets# not.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Computational Approaches to</booktitle>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ:</location>
<marker>Liebrecht, Kunneman, van den Bosch, 2013</marker>
<rawString>Christine Liebrecht, Florian Kunneman, and Antal van den Bosch. 2013. The perfect solution for detecting sarcasm in tweets# not. In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. New Brunswick, NJ: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: An electronic lexical database.</title>
<date>1998</date>
<contexts>
<context position="6486" citStr="Miller and Fellbaum, 1998" startWordPosition="1017" endWordPosition="1020">13) defined a complex model for identifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are combined in a complex framework. Barbieri and Saggion (2014) focused their approach on the use of lexical and semantic features, such as the frequency of the words in different reference corpora, the length of the words, and the number of related synsets in WordNet (Miller and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and</context>
</contexts>
<marker>Miller, Fellbaum, 1998</marker>
<rawString>George Miller and Christiane Fellbaum. 1998. Wordnet: An electronic lexical database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>Anew anew: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things</booktitle>
<note>come in small packages.</note>
<contexts>
<context position="9494" citStr="Nielsen, 2011" startWordPosition="1498" endWordPosition="1499">ng, we then extract features of the following kinds. UniToken Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify the polarity values of tokens, we used four sentiment dictionaries: Opinion Lexicon (Hu and Liu, 2004), Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005), and SentiWordnet (Baccianella et al., 2010). Their union and their intersection are also used as two additional dictionaries. A window size of five is used to verify whether negations are present. If a negation is present the resulting value is set to zero. Six features are used to save the sum polarity values of a tweet based on the six dictionaries respectively. Besides, we also use features recording the polarity contribution of different POS tags. For example, one possible feature-value pair can be (adj-mpqa, 1.0) meaning that according to the dictionary MPQA, </context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. Anew anew: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="4603" citStr="Pang and Lee, 2008" startWordPosition="713" endWordPosition="716">s. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et al. (2009), for example, proposed to identify irony in comments to newspaper articles by relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quintilian</author>
</authors>
<title>With An English Translation. Harold Edgeworth Butler.</title>
<date>1922</date>
<publisher>University Press;</publisher>
<location>Cambridge, Mass., Harvard</location>
<contexts>
<context position="4117" citStr="Quintilian, 1922" startWordPosition="636" endWordPosition="638">SemEval 2015), pages 673–678, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics This paper describes the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and socia</context>
</contexts>
<marker>Quintilian, 1922</marker>
<rawString>Quintilian. 1922. With An English Translation. Harold Edgeworth Butler. Cambridge, Mass., Harvard University Press; London, William Heinemann, Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Tony Veale</author>
</authors>
<title>A multidimensional approach for detecting irony in twitter. Language Resources and Evaluation,</title>
<date>2013</date>
<volume>47</volume>
<issue>1</issue>
<pages>268</pages>
<contexts>
<context position="2746" citStr="Reyes et al., 2013" startWordPosition="417" endWordPosition="420">nation), kinesics (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background. Computers, however, cannot always rely on this kind of information. Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of </context>
<context position="5863" citStr="Reyes et al. (2013)" startWordPosition="913" endWordPosition="916">les. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in Amazon product reviews, exploiting some features that were specific to Amazon. Their method employed two modules: a semi-supervised acquisition of sarcastic patterns and a classifier. This method was then applied to tweets by Davidov et al. (2010), achieving even better results. Gonz´alez-Ib´a˜nez et al. (2011) constructed a corpus of sarcastic tweets and used it to compare judgements made by humans and machine learning algorithms, concluding that none of them performed well. More recently, Reyes et al. (2013) defined a complex model for identifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are combined in a complex framework. Barbieri and Saggion (2014) focused their approach on the use of lexical and semantic features, such as the frequency of the words in different reference corpora, the length of the words, and the number of related synsets in WordNet (Mil</context>
</contexts>
<marker>Reyes, Rosso, Veale, 2013</marker>
<rawString>Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony in twitter. Language Resources and Evaluation, 47(1):239– 268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Ashequl Qadir</author>
<author>Prafulla Surve</author>
<author>Lalindra De Silva</author>
<author>Nathan Gilbert</author>
<author>Ruihong Huang</author>
</authors>
<title>Sarcasm as contrast between a positive sentiment and negative situation.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Emperical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>704--714</pages>
<marker>Riloff, Qadir, Surve, De Silva, Gilbert, Huang, 2013</marker>
<rawString>Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In Proceedings of Conference on Emperical Methods for Natural Language Processing (EMNLP), pages 704–714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angus Stevenson</author>
</authors>
<title>Oxford dictionary of English.</title>
<date>2010</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4407" citStr="Stevenson, 2010" startWordPosition="685" endWordPosition="686">r model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et al. (2009), for example, proposed to identify irony in comments to newspaper articles by relying on the presence of emotic</context>
</contexts>
<marker>Stevenson, 2010</marker>
<rawString>Angus Stevenson. 2010. Oxford dictionary of English. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Stringfellow Jr</author>
</authors>
<title>The meaning of irony: A psychoanalytic investigation.</title>
<date>1994</date>
<publisher>SUNY Press.</publisher>
<contexts>
<context position="4232" citStr="Jr, 1994" startWordPosition="656" endWordPosition="657">describes the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify i</context>
</contexts>
<marker>Jr, 1994</marker>
<rawString>Frank Stringfellow Jr. 1994. The meaning of irony: A psychoanalytic investigation. SUNY Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews.</title>
<date>2010</date>
<booktitle>In In International AAAI Conference on Web and Social Media.</booktitle>
<contexts>
<context position="5267" citStr="Tsur et al. (2010)" startWordPosition="823" endWordPosition="826"> difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et al. (2009), for example, proposed to identify irony in comments to newspaper articles by relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically discriminate ironic from non-ironic similes. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in Amazon product reviews, exploiting some features that were specific to Amazon. Their method employed two modules: a semi-supervised acquisition of sarcastic patterns and a classifier. This method was then applied to tweets by Davidov et al. (2010), achieving even better results. Gonz´alez-Ib´a˜nez et al. (2011) constructed a corpus of sarcastic tweets and used it to compare judgements made by humans and machine learning algorithms, concluding that none of them performed well. More recently, Reyes et al. (2013) def</context>
</contexts>
<marker>Tsur, Davidov, Rappoport, 2010</marker>
<rawString>Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010. Icwsm-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews. In In International AAAI Conference on Web and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piyoros Tungthamthiti</author>
<author>Shirai Kiyoaki</author>
<author>Masnizah Mohd</author>
</authors>
<title>Recognition of sarcasms in tweets based on concept level sentiment analysis and supervised learning approaches.</title>
<date>2014</date>
<booktitle>In Proceedings of Pacific Asia Conference on Language, Information and Computing,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="4286" citStr="Tungthamthiti et al., 2014" startWordPosition="662" endWordPosition="665">r Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et a</context>
</contexts>
<marker>Tungthamthiti, Kiyoaki, Mohd, 2014</marker>
<rawString>Piyoros Tungthamthiti, Shirai Kiyoaki, and Masnizah Mohd. 2014. Recognition of sarcasms in tweets based on concept level sentiment analysis and supervised learning approaches. In Proceedings of Pacific Asia Conference on Language, Information and Computing, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Vanzo</author>
<author>Giuseppe Castellucci</author>
<author>Danilo Croce</author>
<author>Roberto Basili</author>
</authors>
<title>A context based model for sentiment analysis in twitter for the italian language. In</title>
<date>2014</date>
<booktitle>Proceedings of the First Italian Conference on Computational Linguistics CLiC-it &amp; the Fourth International Workshop EVALITA,</booktitle>
<pages>379--383</pages>
<editor>R. Basili, A. Lenci, and B. Magnini, editors,</editor>
<publisher>Pisa University Press.</publisher>
<location>Pisa.</location>
<contexts>
<context position="2835" citStr="Vanzo et al., 2014" startWordPosition="433" endWordPosition="436">nd context (i.e. wider environment), as well as cultural background. Computers, however, cannot always rely on this kind of information. Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very much on-going. 673 Proce</context>
</contexts>
<marker>Vanzo, Castellucci, Croce, Basili, 2014</marker>
<rawString>Andrea Vanzo, Giuseppe Castellucci, Danilo Croce, and Roberto Basili. 2014. A context based model for sentiment analysis in twitter for the italian language. In R. Basili, A. Lenci, and B. Magnini, editors, Proceedings of the First Italian Conference on Computational Linguistics CLiC-it &amp; the Fourth International Workshop EVALITA, pages 379–383, Pisa. Pisa University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="9521" citStr="Wiebe et al., 2005" startWordPosition="1501" endWordPosition="1504">atures of the following kinds. UniToken Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify the polarity values of tokens, we used four sentiment dictionaries: Opinion Lexicon (Hu and Liu, 2004), Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005), and SentiWordnet (Baccianella et al., 2010). Their union and their intersection are also used as two additional dictionaries. A window size of five is used to verify whether negations are present. If a negation is present the resulting value is set to zero. Six features are used to save the sum polarity values of a tweet based on the six dictionaries respectively. Besides, we also use features recording the polarity contribution of different POS tags. For example, one possible feature-value pair can be (adj-mpqa, 1.0) meaning that according to the dictionary MPQA, the sum of the polarity con</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gongjun Yan</author>
<author>Wu He</author>
<author>Jiancheng Shen</author>
<author>Chuanyi Tang</author>
</authors>
<title>A bilingual approach for conducting chinese and english social media sentiment analysis.</title>
<date>2014</date>
<journal>Computer Networks,</journal>
<pages>75--491</pages>
<contexts>
<context position="1491" citStr="Yan et al., 2014" startWordPosition="222" endWordPosition="225">e task. The results show that our model performs especially well in predicting the sentiment intensity of tweets involving irony and sarcasm. 1 Introduction Sentiment analysis aims to identify the polarity and intensity of certain texts in order to shed light on people’s sentiments, perceptions, opinions, and beliefs about a particular product, service, scheme, etc. Knowing what people think can, in fact, help companies, political parties, and other public entities in strategizing and decision making. While impressive results have been achieved in analysing literal texts (Abbasi et al., 2008; Yan et al., 2014), the study of polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of language, such as irony or sarcasm, are also able to invert the polarity of the surface text. Theoretical research in irony and sarcasm often emphasize that humans have difficulties in deciphering messages with underlying meaning (Hay, 2001; Kotthoff, 2003; Kreuz and Caucci, 2007). Factors that can facilitate the understanding of these messages</context>
</contexts>
<marker>Yan, He, Shen, Tang, 2014</marker>
<rawString>Gongjun Yan, Wu He, Jiancheng Shen, and Chuanyi Tang. 2014. A bilingual approach for conducting chinese and english social media sentiment analysis. Computer Networks, 75:491–503.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>