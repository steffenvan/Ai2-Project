<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000135">
<title confidence="0.9989315">
Automatic Text Summarization Based on
the Global Document Annotation
</title>
<author confidence="0.95364">
Katashi Nagao
</author>
<affiliation confidence="0.863359">
Sony Computer Science Laboratory Inc.
</affiliation>
<address confidence="0.7765145">
3-14-13 Higashi-gotanda, Shinagawa—ku,
Tokyo 141-0022, Japan
</address>
<email confidence="0.809563">
nagao©csl.sony.co.jp
</email>
<author confidence="0.863679">
Koiti Hasida
</author>
<affiliation confidence="0.852386">
Electrotechnical Laboratory
</affiliation>
<address confidence="0.7505975">
1-1-4 Umezono, Tukuba,
Ibaraki 305-8568, Japan
</address>
<email confidence="0.87286">
hasida©etl.go.jp
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999671095238095">
The GDA (Global Document Annotation) project
proposes a tag set which allows machines to auto-
matically infer the underlying semantic/pragmatic
structure of documents. Its objectives are to pro-
mote development and spread of NLP/AI applica-
tions to render GDA-tagged documents versatile and
intelligent contents, which should motivate WWW
(World Wide Web) users to tag their documents as
part of content authoring. This paper discusses au-
tomatic text summarization based on GDA. Its main
features are a domain/style-free algorithm and per-
sonalization on summarization which reflects read-
ers&apos; interests and preferences. In order to calcu-
late the importance score of a text element, the
algorithm uses spreading activation on an intra-
document network which connects text elements via
thematic, rhetorical, and coreferential relations. The
proposed method is flexible enough to dynamically
generate summaries of various sizes. A summary
browser supporting personalization is reported as
well.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984006">
The WWW has opened up an era in which an un-
restricted number of people publish their messages
electronically through their online documents. How-
ever, it is still very hard to automatically process
contents of those documents. The reasons include
the following:
</bodyText>
<listItem confidence="0.9943107">
1. HTML (HyperText Markup Language) tags
mainly specify the physical layout of docu-
ments. They address very few content-related
annotations.
2. Hypertext links cannot very much help readers
recognize the content of a document.
3. The WWW authors tend to be less careful
about wording and readability than in tradi-
tional printed media. Currently there is no sys-
tematic means for quality control in the WWW.
</listItem>
<bodyText confidence="0.99948435">
Although HTML is a flexible tool that allows you
to freely write and read messages on the WWW, it
is neither very convenient to readers nor suitable for
automatic processing of contents.
We have been developing an integrated platform
for document authoring, publishing, and reuse by
combining natural language and WWW technolo-
gies. As the first step of our project, we defined a
new tag set and developed tools for editing tagged
texts and browsing these texts. The browser has the
functionality of summarization and content-based
retrieval of tagged documents.
This paper focuses on summarization based on
this system. The main features of our summariza-
tion method are a domain/style-free algorithm and
personalization to reflect readers&apos; interests and pref-
erences. This method naturally outperforms the tra-
ditional summarization methods, which just pick out
sentences highly scored on the basis of superficial
clues such as word count, and so on.
</bodyText>
<sectionHeader confidence="0.992105" genericHeader="method">
2 Global Document Annotation
</sectionHeader>
<bodyText confidence="0.998921833333334">
GDA (Global Document Annotation) is a chal-
lenging project to make WWW texts machine-
understandable on the basis of a new tag set,
and to develop content-based presentation, retrieval,
question-answering, summarization, and translation
systems with much higher quality than before. GDA
thus proposes an integrated global platform for elec-
tronic content authoring, presentation, and reuse.
The GDA tag set is based on XML (Extensible
Markup Language), and designed as compatible as
possible with HTML, TEI, EAGLES, and so forth.
An example of a GDA-tagged sentence is as follows:
</bodyText>
<equation confidence="0.6433464">
&lt;su&gt;&lt;np sem=time0&gt;time&lt;/np&gt;
&lt;vp&gt;&lt;v sem=flyl&gt;flies&lt;/v&gt;
&lt;adp&gt;&lt;ad sem=like0&gt;like&lt;/ad&gt; &lt;np&gt;an
&lt;n sem=arrow0&gt;arrow&lt;/n&gt;&lt;/np&gt;
&lt;/adp&gt;&lt;/vp&gt;.&lt;/su&gt;
</equation>
<bodyText confidence="0.458766">
&lt;su&gt; means sentential unit.
&lt;n&gt;. &lt;np&gt;. &lt;v&gt;, &lt;vp&gt;. &lt;ad&gt; and &lt;adp&gt; mean noun.
</bodyText>
<page confidence="0.99479">
917
</page>
<bodyText confidence="0.982545866666667">
noun phrase, verb, verb phrase, adnoun or adverb
(including preposition and postposition), and ad-
nominal or adverbial phrase, respectivelyl.
The GDA initiative aims at having many WWW
authors annotate their on-line documents with this
common tag set so that machines can automatically
recognize the underlying semantic and pragmatic
structures of those documents much more easily
than by analyzing traditional HTML files. A huge
amount of annotated data is expected to emerge,
which should serve not just as tagged linguistic cor-
pora but also as a worldwide, self-extending knowl-
edge base, mainly consisting of examples showing
how our knowledge is manifested.
GDA has three main steps:
</bodyText>
<listItem confidence="0.99928275">
1. Propose an XML tag set which allows machines
to automatically infer the underlying structure
of documents.
2. Promote development and spread of NLP/AI
applications to turn tagged texts to versatile
and intelligent contents.
3. Motivate thereby the authors of WWW files to
annotate their documents using those tags.
</listItem>
<subsectionHeader confidence="0.997097">
2.1 Themant ic /Rhet orical Relations
</subsectionHeader>
<bodyText confidence="0.989363366666667">
The rel attribute encodes a relationship in which
the current element stands with respect to the ele-
ment that it semantically depends on. Its value is
called a relational term. A relational term denotes a
binary relation, which may be a thematic role such
as agent, patient, recipient. etc., or a rhetorical rela-
tion such as cause, concession. etc. Thus we conflate
thematic roles and rhetorical relations here, because
the distinction between them is often vague. For in-
stance, concession may be both intrasentential and
intersentential relation.
Here is an example of a rel attribute:
&lt;su ctyp=fd&gt;&lt;name rel=agt&gt;Tom&lt;/name&gt;
&lt;vp&gt;came&lt;/vp&gt;.&lt;/su&gt;
ctyp=fd means that the first element
&lt;name rel=agt&gt;Tom&lt;/name&gt; depends on the second
element &lt;vp&gt;came&lt;/vp&gt;. rel=agt means that Tom
has the agent role with respect to the event denoted
by came.
rel is an open-class attribute, potentially encom-
passing all the binary relations lexicalized in nat-
ural languages. An exhaustive listing of thematic
roles and rhetorical relations appears impossible. as
widely recognized. We are not yet sure about how
lA more detailed description of the GDA tag set can be
found at http://www.etl.go.jp/etl/n1/GDA/tagset.html.
many thematic roles and rhetorical relations are suf-
ficient for engineering applications. However, the
appropriate granularity of classification will be de-
termined by the current level of technology.
</bodyText>
<subsectionHeader confidence="0.998035">
2.2 Anaphora and Coreference
</subsectionHeader>
<bodyText confidence="0.989950269230769">
Each element may have an identifier as the value of
the id attribute. Anaphoric expression should have
the ana attribute with its antecedent&apos;s id value. An
example follows:
&lt;name id=1&gt;John&lt;/name&gt; beats
&lt;adp ana=1&gt;his&lt;/adp&gt; dog.
A non-anaphoric coreference is marked by the crf
attribute, whose usage is the same as the ana at-
tribute.
When the coreference is at the level of type (kind.
sort, etc.) which the referents of the antecedent
and the anaphor are tokens of, we use the cotyp
attribute as below:
You bought &lt;np id=11&gt;a car&lt;/np&gt;.
I bought &lt;np cotyp=11&gt;one&lt;/np&gt;, too.
A zero anaphora is encoded by using the appro-
priate relational term as an attribute name with the
referent&apos;s id value. Zero anaphors of compulsory el-
ements, which describe the internal structure of the
events represented by the verbs of adjectives are re-
quired to be resolved. Zero anaphors of optional ele-
ments such as with reason and means roles may not.
Here is an example of a zero anaphora concerning
an optional thematic role ben (for beneficiary):
Tom visited &lt;name id=111&gt;Mary&lt;/name&gt;.
He &lt;v ben=111&gt;brought&lt;/v&gt; a present.
</bodyText>
<sectionHeader confidence="0.981923" genericHeader="method">
3 Text Summarization
</sectionHeader>
<bodyText confidence="0.999813166666667">
As an example of a basic application of GDA, we
have developed an automatic text summarization
system. Summarization generally requires deep se-
mantic processing and a lot of background knowl-
edge. However, most previous works use several su-
perficial clues and heuristics on specific styles or con-
figurations of documents to summarize.
For example, clues for determining the importance
of a sentence include (1) sentence length, (2) key-
word count, (3) tense, (4) sentence type (such as
fact, conjecture and assertion), (5) rhetorical rela-
tion (such as reason and example), and (6) position
of sentence in the whole text. Most of these are ex-
tracted by a shallow processing of the text. Such a
computation is rather robust.
Present summarization systems (Watanabe, 1996:
Hovy and Lin, 1997) use such clues to calculate an
importance score for each sentence, choose sentences
</bodyText>
<page confidence="0.990959">
918
</page>
<bodyText confidence="0.988313464285714">
according to the score, and simply put the selected
sentences together in order of their occurrences in
the original document. In a sense, these systems are
successful enough to be practical, and are based on
reliable technologies. However, the quality of sum-
marization cannot be improved beyond this basic
level without any deep content-based processing.
We propose a new summarization method based
on GDA. This method employs a spreading activa-
tion technique (Hasida et al., 1987) to calculate the
importance values of elements in the text. Since the
method does not employ any heuristics dependent on
the domain and style of documents, it is applicable
to any GDA-tagged documents. The method also
can trim sentences in the summary because impor-
tance scores are assigned to elements smaller than
sentences.
A GDA-tagged document naturally defines an
intra-document network in which nodes corre-
spond to elements and links represent the seman-
tic relations mentioned in the previous section.
This network consists of sentence trees (syntactic
head-daughter hierarchies of subsentential elements
such as words or phrases), coreference/anaphora
links, document/subdivision/paragraph nodes, and
rhetorical relation links.
Figure 1 shows a graphical representation of the
intra-document network.
</bodyText>
<figureCaption confidence="0.998497">
Figure 1: Intra-Document Network
</figureCaption>
<bodyText confidence="0.993057">
The summarization algorithm is the following:
</bodyText>
<listItem confidence="0.94196572">
1. Spreading activation is performed in such a
way that two elements have the same activa-
tion value if they are coreferent or One of them
is the syntactic head of the other.
2. The unmarked element with the highest activa-
tion value is marked for inclusion in the sum-
mary.
3. When an element is marked, other elements
listed below are recursively marked as well, until
no more element may be marked.
• its head
• its antecedent
• its compulsory or a priori important
daughters, the values of whose relational
attributes are agt. pat. obj. pos, cnt, cau,
cnd, sbm, and so forth.
• the antecedent of a zero anaphor in it with
some of the above values for the relational
attribute
4. All marked elements in the intra-document net-
work are generated preserving the order of their
positions in the original document.
5. If a size of the summary reaches the user-
specified value, then terminate; otherwise go
back to Step 2.
</listItem>
<bodyText confidence="0.998546547619047">
The following article of the Wall Street Journal
was used for testing this algorithm.
During its centennial year. The Wall Street
Journal will report events of the past century
that stand as milestones of American busi-
ness history. THREE COMPUTERS THAT
CHANGED the face of personal computing
were launched in 1977. That year the Ap-
ple II. Commodore Pet and Tandy TRS came
to market. The computers were crude by to-
day&apos;s standards. Apple II owners, for exam-
ple, had to use their television sets as screens
and stored data on audiocassettes. But Apple
II was a major advance from Apple I, which
was built in a garage by Stephen Wozniak and
Steven Jobs for hobbyists such as the Home-
brew Computer Club. In addition, the Ap-
ple II was an affordable $1,298. Crude as
they were, these early PCs triggered explosive
product development in desktop models for the
home and office. Big mainframe computers for
business had been around for years. But the
new 1977 PCs — unlike earlier built-from-kit
types such as the Altair, Sol and IMSAI — had
keyboards and could store about two pages of
data in their memories. Current PCs are more
than 50 times faster and have memory capac-
ity 500 times greater than their 1977 counter-
parts. There were many pioneer PC contrib-
utors. William Gates and Paul Allen in 1975
developed an early language-housekeeper sys-
tem for PCs, and Gates became an industry
billionaire six years after IBM adapted one of
these versions in 1981. Alan F. Shugart, cur-
rently chairman of Seagate Technology, led the
team that developed the disk drives for PCs.
Dennis Hayes and Dale Heatherington, two At-
lanta engineers, were co-developers of the in-
ternal modems that allow PCs to share data
via the telephone. IBM, the world leader in
computers, didn&apos;t offer its first PC until Au-
gust 1981 as many other companies entered the
</bodyText>
<figure confidence="0.998947733333333">
• • • •
normal
link
---- reference
link
subsentential
segment
•
•
document
subdivision
(optional)
paragraph
(optional)
sentence
</figure>
<page confidence="0.856305">
919
</page>
<figureCaption confidence="0.913058444444444">
market. Today. PC shipments annually total
some $38.3 billion world-wide.
Here is a short, computer-generated summary of
this sample article:
THREE COMPUTERS THAT
CHANGED the face of personal computing
were launched. Crude as they were, these
early PCs triggered explosive product de-
velopment. Current PCs are more than 50
times faster and have memory capacity 500
times greater than their counterparts.
The proposed method is flexible enough to dy-
namically generate summaries of various sizes. If a
longer summary is needed. the user can change the
window size of the summary browser, as described
in Section 3.1. Then. the summary changes its size
to fit into the new window. An example of a longer
summary follows:
</figureCaption>
<bodyText confidence="0.975063828571428">
THREE COMPUTERS THAT
CHANGED the face of personal comput-
ing were launched. The Apple IL Com-
modore Pet and Tandy TRS came to mar-
ket. The computers were crude. Apple II
owners had to use their television sets and
stored data on audiocassettes. The Ap-
ple II was an affordable $1.298. Crude as
they were, these early PCs triggered explo-
sive product development. The new PCs
had keyboards and could store about two
pages of data in their memories. Current
PCs are more than 50 times faster and have
memory capacity 500 times greater than
their counterparts. There were many pi-
oneer PC contributors. William Gates and
Paul Allen developed an early language-
housekeeper system. and Gates became an
industry billionaire after IBM adapted one
of these versions. IBM didn&apos;t offer its first
PC.
An observation obtained from this experiment is
that tags for coreferences and thematic and rhetori-
cal relations are almost enough to make a summary.
In particular, coreferences and rhetorical relations
help summarization very much.
GDA tags allow us to apply more sophisticated
natural language processing technologies to come up
with better summaries. It is straightforward to in-
corporate sentence generation technologies to para-
phrase parts of the document, rather than just se-
lecting or pruning them. Annotations on anaphora
can be exploited to produce context-dependent para-
phrases. Also the summary could be itemized to fit
in a slide presentation.
</bodyText>
<subsectionHeader confidence="0.99984">
3.1 Summary Browser
</subsectionHeader>
<bodyText confidence="0.931498368421053">
We developed a summary browser using a Java-
capable WWW browser. Figure 2 shows an example
screen of the summary browser.
ZES lactpc GU Storl Ermrsor
Document: I &amp;quot;1 Keywords:
; During its centenrial year, The Wall Street Journal will report everts of the past century that
i stand as milestones of Ameican business history. THREE COMPUTERS THAT CHANGED the
I face of personal computing were launched in 1977. That year the Apple U, Commodore Pet
I and Tandy TRS came to market. The computers were crude by today&apos;s standards. Apple II
1 owners, for example, had to use their television sets as screens and stored data on
iaudiocassettes. But Apple II was a major advance from Apple I, which was built in a garage by i
I Stephen Wozriak and Steven Jobs for hobbyists such as the Homebrew Computer Club. In
I addition, the Apple U was an affordable 1 1,293. Crude as they were, these early PCs
i triggered explosive product development in desktop models for the home and office. Big .
i mainframe computers for business had been around for years. But the new 1977 PCs — unlike $..:!
i earlier built-from-kit types such as the Altair, Sol arid IMSAI — had keyboards and could store
i about two pages of data in their memories. Current PCs are more than 50 times faster and
Ihave memory capacity 500 times greater than their 1977 counterparts. There were many
i pioneer PC contributors. William Gates and Paul Allen in 1975 developed an early
</bodyText>
<table confidence="0.987086636363636">
1 language-housekeeper system for Ks, and Gates became an industry billionaire six years i
iafter IBM adapted one of these versions in 1931. Alan F. Shugart, currently chairman of
1 Seagate Technology, led the team that developed the disk drives for PCs. Dernis Hayes and
i Dale Heatherington, two Atlanta engineers, were co-developers of the internal modems that
Iallow PCs to share data via the telephone. IBM, the world leader in computers, didn&apos;t offer its
., , . - .... &apos; , ,. • • &amp;
; THREE COMPUTERS THAT CHANGED the face of personal computing were launched. Crude as
; they were, these early PCs triggered explosive product development. Current PCs are more
! than 50 times faster and have memory capacity 500 times greater than their counterparts. 1
1 1
airilretNet eA
</table>
<figureCaption confidence="0.883555">
Figure 2: Summary Browser
It has the following functionalities:
</figureCaption>
<bodyText confidence="0.723073666666667">
1. A screen is divided into three parts (frames).
One frame provides a user input form through
which you can select documents and type key-
words. The other frames are for displaying the
original document and its summary.
2. The frame for the summary text is resizable
by sliding the boundary with the original doc-
ument frame. The size of the summary frame
influences the size of the summary itself. Thus
you can see the summary in a preferred size and
change the size in an easy and intuitive way.
3. The frame for the original document is mouse
sensitive. You can select any element of text in
this frame. This function is used for the cus-
tomization of the summary, as described later.
4. HTML tags are also handled by the browser.
So, images are viewed and hyperlinks are man-
aged both in the summary. If a hyperlink
is clicked in the original document frame, the
linked document appears on the same frame.
The hyperlinks are kept in the summary.
</bodyText>
<sectionHeader confidence="0.990677" genericHeader="method">
4 Personalization
</sectionHeader>
<bodyText confidence="0.971728">
A good summary might depend on the background
knowledge of its creator. It also should change ac-
</bodyText>
<page confidence="0.984297">
920
</page>
<bodyText confidence="0.999976454545455">
cording to the interests or preferences of its reader.
Let us refer to the adaptation of the summariza-
tion process to a particular user as personalization.
GDA-based summarization can be easily personal-
ized because our method is flexible enough to bias
a summary toward the user&apos;s concerns. You can se-
lect any elements in the original document during
summarization, to interactively provide information
concerning your personal interests.
We have been developing the following techniques
for personalized summarization:
</bodyText>
<listItem confidence="0.744003">
• Keyword-based customization
</listItem>
<bodyText confidence="0.999668888888889">
The user can input any words of interest.
The system relates those words with those in
the document using cooccurrence statistics ac-
quired from a corpus and a dictionary such as
WordNet (Miller, 1995). The related words in
the document are assigned numeric values that
reflect closeness to the input words. These val-
ues are used in spreading activation for calcu-
lating importance scores.
</bodyText>
<listItem confidence="0.686291">
• Interactive customization by selecting any ele-
ments from a document
</listItem>
<bodyText confidence="0.972653142857143">
The user can mark any words, phrases, and sen-
tences to be included in the summary. The sum-
mary browser allows the user to select those el-
ements by pointing devices such as mouse and
stylus pen. The user can easily select elements
by clicking on them. The click count corre-
sponds to the level of elements. That is, the
first click means the word, the second the next
larger element containing it, and so on. The se-
lected elements will have higher activation val-
ues in spreading activation.
• Learning user interests by observation of WWW
browsing
The summarization system can customize the
summary according to the user without any ex-
plicit user inputs. We implemented a learning
mechanism for user personalization. The mech-
anism uses a weighted feature vector. The fea-
ture corresponds to the category or topic of doc-
uments. The category is defined according to a
WWW directory such as Yahoo. The topic is
detected using the summarization technique.
Learning is roughly divided into data acquisi-
tion and model modification. The user&apos;s behav-
ioral data is acquired by detecting her informa-
tion access on the WWW. This data includes
the time and duration of that information ac-
cess and features related to that information.
The first step of model modification is to esti-
mate the degree of relevance between the input
feature vector assigned to the information ac-
cessed by the user and the model of the user&apos;s
interests acquired from previous data. The sec-
ond step is to adjust the weights of features in
the user model.
</bodyText>
<sectionHeader confidence="0.987273" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999977">
We have discussed the GDA project, which aims at
supporting versatile and intelligent contents. Our
focus in the present paper is one of its applications
to automatic text summarization. We are evaluating
our summarization method using online Japanese ar-
ticles with GDA tags. We are also extending text
summarization to that of hypertext. For example, a
summary of a hypertext document will include re-
cursively embedding linked documents in summary,
which should be useful for encyclopedic entries, too.
Future work includes construction of a large-scale
GDA corpus and system evaluation by open exper-
imentation. GDA tools including a tagging editor
and a browser will soon be publicly available on the
WWW. Our main current concern is interactive and
intelligent presentation, as an extension of text sum-
marization. This may turn out to be a killer appli-
cation of GDA. because it does not just presuppose
rather small amount of tagged document but also
makes the effect of tagging immediately visible to
the author. We hope that our project revolutionize
global and intercultural communications.
</bodyText>
<sectionHeader confidence="0.998277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999058055555556">
KOiti Hasida, Syun Ishizaki, and Hitoshi Isahara.
1987. A connectionist approach to the generation
of abstracts. In Gerard Kempen, editor. Natural
Language Generation: New Results in Artificial
Intelligence, Psychology, and Linguistics, pages
149-156. Martinus Nijhoff.
Eduard Hovy and Chin Yew Lin. 1997. Automated
text summarization in SUMMARIST. In Proceed-
ings of ACL Workshop on Intelligent Scalable Text
Summarization..
George Miller. 1995. WordNet: A lexical database
for English. Communications of the ACM,
38(11):39-41.
Hideo Watanabe. 1996. A method for abstract-
ing newspaper articles by using surface clues. In
Proceedings of the Sixteenth International Con-
ference on Computational Linguistics (COLING-
96), pages 974-979.
</reference>
<page confidence="0.997819">
921
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.764371">
<title confidence="0.9980635">Automatic Text Summarization Based on the Global Document Annotation</title>
<author confidence="0.999134">Katashi Nagao</author>
<affiliation confidence="0.999776">Sony Computer Science Laboratory Inc.</affiliation>
<address confidence="0.9912505">3-14-13 Higashi-gotanda, Shinagawa—ku, Tokyo 141-0022, Japan</address>
<email confidence="0.963512">nagao©csl.sony.co.jp</email>
<author confidence="0.981262">Koiti Hasida</author>
<affiliation confidence="0.999987">Electrotechnical Laboratory</affiliation>
<address confidence="0.948598">1-1-4 Umezono, Tukuba, Ibaraki 305-8568, Japan</address>
<email confidence="0.975952">hasida©etl.go.jp</email>
<abstract confidence="0.9970965">The GDA (Global Document Annotation) project proposes a tag set which allows machines to automatically infer the underlying semantic/pragmatic structure of documents. Its objectives are to promote development and spread of NLP/AI applications to render GDA-tagged documents versatile and intelligent contents, which should motivate WWW (World Wide Web) users to tag their documents as part of content authoring. This paper discusses automatic text summarization based on GDA. Its main features are a domain/style-free algorithm and personalization on summarization which reflects readers&apos; interests and preferences. In order to calculate the importance score of a text element, the algorithm uses spreading activation on an intradocument network which connects text elements via thematic, rhetorical, and coreferential relations. The proposed method is flexible enough to dynamically generate summaries of various sizes. A summary browser supporting personalization is reported as well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>KOiti Hasida</author>
<author>Syun Ishizaki</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A connectionist approach to the generation of abstracts.</title>
<date>1987</date>
<booktitle>Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics,</booktitle>
<pages>149--156</pages>
<editor>In Gerard Kempen, editor.</editor>
<publisher>Martinus Nijhoff.</publisher>
<contexts>
<context position="8711" citStr="Hasida et al., 1987" startWordPosition="1324" endWordPosition="1327">tion systems (Watanabe, 1996: Hovy and Lin, 1997) use such clues to calculate an importance score for each sentence, choose sentences 918 according to the score, and simply put the selected sentences together in order of their occurrences in the original document. In a sense, these systems are successful enough to be practical, and are based on reliable technologies. However, the quality of summarization cannot be improved beyond this basic level without any deep content-based processing. We propose a new summarization method based on GDA. This method employs a spreading activation technique (Hasida et al., 1987) to calculate the importance values of elements in the text. Since the method does not employ any heuristics dependent on the domain and style of documents, it is applicable to any GDA-tagged documents. The method also can trim sentences in the summary because importance scores are assigned to elements smaller than sentences. A GDA-tagged document naturally defines an intra-document network in which nodes correspond to elements and links represent the semantic relations mentioned in the previous section. This network consists of sentence trees (syntactic head-daughter hierarchies of subsentent</context>
</contexts>
<marker>Hasida, Ishizaki, Isahara, 1987</marker>
<rawString>KOiti Hasida, Syun Ishizaki, and Hitoshi Isahara. 1987. A connectionist approach to the generation of abstracts. In Gerard Kempen, editor. Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics, pages 149-156. Martinus Nijhoff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin Yew Lin</author>
</authors>
<title>Automated text summarization in SUMMARIST.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL Workshop on Intelligent Scalable Text Summarization..</booktitle>
<contexts>
<context position="8140" citStr="Hovy and Lin, 1997" startWordPosition="1234" endWordPosition="1237">ot of background knowledge. However, most previous works use several superficial clues and heuristics on specific styles or configurations of documents to summarize. For example, clues for determining the importance of a sentence include (1) sentence length, (2) keyword count, (3) tense, (4) sentence type (such as fact, conjecture and assertion), (5) rhetorical relation (such as reason and example), and (6) position of sentence in the whole text. Most of these are extracted by a shallow processing of the text. Such a computation is rather robust. Present summarization systems (Watanabe, 1996: Hovy and Lin, 1997) use such clues to calculate an importance score for each sentence, choose sentences 918 according to the score, and simply put the selected sentences together in order of their occurrences in the original document. In a sense, these systems are successful enough to be practical, and are based on reliable technologies. However, the quality of summarization cannot be improved beyond this basic level without any deep content-based processing. We propose a new summarization method based on GDA. This method employs a spreading activation technique (Hasida et al., 1987) to calculate the importance </context>
</contexts>
<marker>Hovy, Lin, 1997</marker>
<rawString>Eduard Hovy and Chin Yew Lin. 1997. Automated text summarization in SUMMARIST. In Proceedings of ACL Workshop on Intelligent Scalable Text Summarization..</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--11</pages>
<contexts>
<context position="18746" citStr="Miller, 1995" startWordPosition="3008" endWordPosition="3009">s personalization. GDA-based summarization can be easily personalized because our method is flexible enough to bias a summary toward the user&apos;s concerns. You can select any elements in the original document during summarization, to interactively provide information concerning your personal interests. We have been developing the following techniques for personalized summarization: • Keyword-based customization The user can input any words of interest. The system relates those words with those in the document using cooccurrence statistics acquired from a corpus and a dictionary such as WordNet (Miller, 1995). The related words in the document are assigned numeric values that reflect closeness to the input words. These values are used in spreading activation for calculating importance scores. • Interactive customization by selecting any elements from a document The user can mark any words, phrases, and sentences to be included in the summary. The summary browser allows the user to select those elements by pointing devices such as mouse and stylus pen. The user can easily select elements by clicking on them. The click count corresponds to the level of elements. That is, the first click means the wo</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Watanabe</author>
</authors>
<title>A method for abstracting newspaper articles by using surface clues.</title>
<date>1996</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Computational Linguistics (COLING96),</booktitle>
<pages>974--979</pages>
<contexts>
<context position="8119" citStr="Watanabe, 1996" startWordPosition="1232" endWordPosition="1233">ocessing and a lot of background knowledge. However, most previous works use several superficial clues and heuristics on specific styles or configurations of documents to summarize. For example, clues for determining the importance of a sentence include (1) sentence length, (2) keyword count, (3) tense, (4) sentence type (such as fact, conjecture and assertion), (5) rhetorical relation (such as reason and example), and (6) position of sentence in the whole text. Most of these are extracted by a shallow processing of the text. Such a computation is rather robust. Present summarization systems (Watanabe, 1996: Hovy and Lin, 1997) use such clues to calculate an importance score for each sentence, choose sentences 918 according to the score, and simply put the selected sentences together in order of their occurrences in the original document. In a sense, these systems are successful enough to be practical, and are based on reliable technologies. However, the quality of summarization cannot be improved beyond this basic level without any deep content-based processing. We propose a new summarization method based on GDA. This method employs a spreading activation technique (Hasida et al., 1987) to calc</context>
</contexts>
<marker>Watanabe, 1996</marker>
<rawString>Hideo Watanabe. 1996. A method for abstracting newspaper articles by using surface clues. In Proceedings of the Sixteenth International Conference on Computational Linguistics (COLING96), pages 974-979.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>