<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.9986915">
Augmenting a Bilingual Lexicon with Information
for Word Translation Disambiguation
</title>
<author confidence="0.997127">
Takashi Tsunakawa
</author>
<affiliation confidence="0.9982205">
Faculty of Informatics
Shizuoka University
</affiliation>
<email confidence="0.998708">
tuna@inf.shizuoka.ac.jp
</email>
<author confidence="0.995903">
Hiroyuki Kaji
</author>
<affiliation confidence="0.9982215">
Faculty of Informatics
Shizuoka University
</affiliation>
<email confidence="0.998839">
kaji@inf.shizuoka.ac.jp
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842833333333">
We describe a method for augmenting
a bilingual lexicon with additional in-
formation for selecting an appropriate
translation word. For each word in the
source language, we calculate a corre-
lation matrix of its association words
versus its translation candidates. We
estimate the degree of correlation by
using comparable corpora based on
these assumptions: “parallel word as-
sociations” and “one sense per word
association.” In our word translation
disambiguation experiment, the results
show that our method achieved 42%
recall and 49% precision for Japa-
nese-English newspaper texts, and 45%
recall and 76% precision for Chi-
nese-Japanese technical documents.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955405405406">
The bilingual lexicon, or bilingual dictionary,
is a fundamental linguistic resource for multi-
lingual natural language processing (NLP). For
each word, multiword, or expression in the
source language, the bilingual lexicon provides
translation candidates representing the original
meaning in the target language.
Selecting the right words for translation is a
serious problem in almost all of multilingual
NLP. One word in the source language almost
always has two or more translation candidates
in the target language by looking up them in
the bilingual lexicon. Because each translation
candidate has a distinct meaning and property,
we must be careful in selecting the appropriate
translation candidate that has the same sense as
the word inputted. This task is often called
word translation disambiguation.
In this paper, we describe a method for add-
ing information for word translation disam-
biguation into the bilingual lexicon. Compara-
ble corpora can be used to determine which
word associations suggest which translations
of the word (Kaji and Morimoto, 2002). First,
we extract word associations in each language
corpus and align them by using a bilingual dic-
tionary. Then, we construct a word correlation
matrix for each word in the source language.
This correlation matrix works as information
for word translation disambiguation.
We carried out word translation experiments
on two settings: English-to-Japanese and Chi-
nese-to-Japanese. In the experiments, we tested
Dice/Jaccard coefficients, pointwise mutual
information, log-likelihood ratio, and Student’s
t-score as the association measures for extract-
ing word associations.
</bodyText>
<sectionHeader confidence="0.653695333333333" genericHeader="introduction">
2 Constructing word correlation ma-
trices for word translation disam-
biguation
</sectionHeader>
<subsectionHeader confidence="0.980145">
2.1 Outline of our method
</subsectionHeader>
<bodyText confidence="0.999875153846154">
In this section, we describe the method for
calculating a word correlation matrix for each
word in the source language. The correlation
matrix for a word f consists of its association
words and its translation candidates. Among
the translation candidates, we choose the most
acceptable one that is strongly suggested by its
association words occurring around f.
We use two assumptions for this framework:
(i) Parallel word associations:
Translations of words associated with
each other in a language are also asso-
ciated with each other in another language
</bodyText>
<page confidence="0.978369">
30
</page>
<note confidence="0.9572475">
Proceedings of the 8th Workshop on Asian Language Resources, pages 30–37,
Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing
</note>
<bodyText confidence="0.996432785714286">
(Rapp, 1995). For example, two English
words “tank” and “soldier” are associated
with each other and their Japanese trans-
lations “ᡓ㌴ (sensha)” and “රኈ (hei-
shi)” are also associated with each other.
(ii) One sense per word association:
A polysemous word exhibits only one
sense of a word per word association
(Yarowsky, 1993). For example, a poly-
semous word “tank” exhibits the “military
vehicle” sense of a word when it is asso-
ciated with “soldier,” while it exhibits the
“container for liquid or gas” sense when it
is associated with “gasoline.”
Under these assumptions, we determine which
of the words associated with an input word
suggests which of its translations by aligning
word associations by using a bilingual dictio-
nary. Consider the associated English words
(tank, soldier) and their Japanese translations
(ᡓ㌴ (sensha), රኈ (heishi)). When we
translate the word “tank” into Japanese, the
associated word “soldier” helps us to translate
it into “ᡓ㌴ (sensha)”, not to translate it into
“ࢱࣥࢡ (tanku)” which means “a storage
tank.”
This naive method seems to suffer from the
following difficulties:
</bodyText>
<listItem confidence="0.968389166666667">
• A disparity in topical coverage between
two corpora in two languages
• A shortage in the bilingual dictionary
• The existence of polysemous associated
words that cannot determine the correct
sense of the input word
</listItem>
<bodyText confidence="0.994144117647059">
For these difficulties, we use the tendency that
the two words associated with a third word are
likely to suggest the same sense of the third
word when they are also associated with each
other. For example, consider an English asso-
ciated word pair (tank, troop). The word “troop”
cannot distinguish the different meanings be-
cause it can co-occur with the word “tank” in
both senses of the word. The third word “sol-
dier,” which is associated with both “tank” and
“troop,” can suggest the translation “ ᡓ㌴
(sensha).”
The overview of our method is shown in
Figure 1. We first extract associated word pairs
in the source and target languages from com-
parable corpora. Using a bilingual dictionary,
we obtain alignments of these word associa-
</bodyText>
<figureCaption confidence="0.993945">
Figure 1. Overview of our method.
</figureCaption>
<bodyText confidence="0.9770473">
tions. Then, we iteratively calculate a correla-
tion matrix for each word in the source lan-
guage. Finally, we select the translation with
the highest correlation from the translation
candidates of the input word and the
co-occurring words.
For each input word in the source language,
we calculate correlation values between their
translation candidates and their association
words. The algorithm is shown in Figure 2.
In Algorithm 1, the initialization of correla-
tion values is based on word associations,
where D is a set of word pairs in the bilingual
dictionary, and Af and Ae are the sets of asso-
ciated word pairs. First, we retain associated
words f’(i) when its translation e’ exists and
when e’ is associated with e. In the iteration,
the correlation values of associated words f’(i)
that suggest e(j) increase relatively by using
association scores a(f&apos;(i), f) and
</bodyText>
<page confidence="0.999375">
31
</page>
<bodyText confidence="0.9815795">
a(f&apos;(i), f&amp;quot;). In our experiments, we set the
number of iterations Nr to 10.
</bodyText>
<subsectionHeader confidence="0.911573">
2.2 Alternative association measures for
extracting word associations
</subsectionHeader>
<bodyText confidence="0.999825882352941">
We extract co-occurring word pairs and calcu-
late their association scores. In this paper, we
focus on some frequently used metrics for
finding word associations based on their oc-
currence/co-occurrence frequencies.
Suppose that words x and y frequently
co-occur. Let n1 and n2 be the occurrence fre-
quencies of x and y respectively, and let m be
the frequency that x and y co-occur between w
content words. The parameter w is a window
size that adjusts the range of co-occurrences.
Let N and M be the sum of occur-
rences/co-occurrences of all words/word pairs,
respectively. The frequencies are summarized
in Table 1.
The word association scores a(x, y) are de-
fined as follows:
</bodyText>
<listItem confidence="0.905311">
• Dice coefficient (Smadja, 1993)
2m
</listItem>
<equation confidence="0.9160366">
Dice(x,y) = (1)
�� + ��
• Jaccard coefficient (Smadja et al., 1996)
M
Jaccard(x,y) = n,+ �� � � (2)
</equation>
<listItem confidence="0.9763905">
• Pointwise mutual information (pMI)
(Church and Hanks, 1990)
</listItem>
<equation confidence="0.9001885">
m/M
pMI(x,Y) = logZ (n1 IN)(nz IN)
</equation>
<listItem confidence="0.602739">
(3)
• Log-likelihood ratio (LLR) (Dunning,
</listItem>
<equation confidence="0.987875363636364">
1993)
LLR(x, y)
= —2(logL(m, n1, r)
+ logL(n2 — m, N — n1, r)
— logL(m, n1, r1)
— logL(n2 — m, N — n1, r2)); (4)
logL(k, n, r) =
k loge r + (n — k) loge(1 — r), (5)
nZ _M
, � =
� � ��
</equation>
<listItem confidence="0.9461915">
• Student’s t-score (TScore) (Church et al.,
1991)
</listItem>
<equation confidence="0.972952">
� � ���� ��
TScore(�,�) =
(7)
��
</equation>
<bodyText confidence="0.999795">
We calculate association scores for all pairs
of words when their occurrence frequencies
are not less than a threshold Tf and when their
</bodyText>
<figureCaption confidence="0.9942705">
Figure 2. Algorithm for calculating correlation
matrices.
</figureCaption>
<table confidence="0.987397142857143">
x x not Total
occur occur
y m n2 – m n2
occur
y not n1 – m M – n1 N – n2
occur – n2 + m
Total n1 N – n1 N
</table>
<tableCaption confidence="0.9390035">
Table 1. Contingency matrix of occurrence
frequencies.
</tableCaption>
<figure confidence="0.932923703703704">
Algorithm 1:
Input:
f: an input word
f’(1), ..., f’(I): associated words of f
e(1), ..., e(J): translation candidates
of f
Nr: number of iterations
A bilingual lexicon
Word association scores a for both
languages
Output:
Cf = [Cf (f’(i), e(j))]: a correlation ma-
trix for f
1: if Ek Sf(f*(i), e(k)) - 0 then
2: Cf(f*(i), e(i)) . 345f6(7),8(9)&lt;
E&gt; 34(f6(7),8(k))
3: else
4: Cf(f*(i), e(j)) . 0
5: end
6: i . 0
7: while i &lt; Nr
8: i . i + 1
9: Cf(f*(i),e(i)) . a(f&apos;(i),f)
E.a(f&apos;(i), f&amp;quot;) . Cf(� e(j))
×max E. a(f&apos;(i),f&amp;quot;) . Cf(f&amp;quot;, e(k))
�
10: end
</figure>
<equation confidence="0.953773555555556">
�!6f(f&apos;, e) =
1 (&amp;quot;e&apos;: (e, e&apos;) # $e, (f&apos;, e&apos;) # %)
0 (otherwise)
(?�: = ?{f**|(f,f**)#@4,(f*(7),f**)#@4})
M
r1 = , rz =
n1
��
N (6)
</equation>
<page confidence="0.984947">
32
</page>
<table confidence="0.78517">
&lt;home&gt; Q *V-- � n� *r an
(kuni) (honrui) (ie) (jitaku) (katei) (shisetsu)
[country] [home base] [house] [my home] [homeplace] [facilities]
base 0.009907 0.017649 0.005495 0.006117 0.005186 0.005597
game 0.043507 0.048358 0.025145 0.028208 0.019987 0.023014
Kansas 0.010514 0.003786 0.004280 0.007307 0.004320 0.005459
run 0.023468 0.042035 0.014430 0.015765 0.012061 0.012986
season 0.044855 0.050952 0.025406 0.028506 0.020716 0.023631
</table>
<tableCaption confidence="0.934243">
Table 2. Word correlation matrix for a word “home,”
as information for word translation disambiguation
</tableCaption>
<bodyText confidence="0.98500675">
threshold Tc.
We handle word pairs whose association
scores are not less than a predefined value TA;
some of the thresholds were evaluated in our
experiment. The associated word pair sets Af
and Ae in Algorithm 1 includes only word pairs
whose scores are not less than TA in the source
and target language, respectively.
</bodyText>
<sectionHeader confidence="0.99137" genericHeader="method">
3 Word Translation Disambiguation
</sectionHeader>
<bodyText confidence="0.99995445">
Consider that a translator changes a word in an
input sentence. Usually, two or more transla-
tion candidates are enumerated in the bilingual
dictionary for a word. The translator should
select a translation word that is grammatically/
syntactically correct, semantically equivalent
to the input, and pragmatically appropriate.
We assume that the translation word e for an
input word f tends to be selected if words oc-
curring around f are strongly correlated with e.
Using the correlation matrices, we select e as a
translation if the associated word f’ occurs
around f and the score Cf(f’,e) is large. In addi-
tion, we take distance between f and f’ into
account.
We define the score of the translation word
e(f0) for an input word f0 as follows. Consider
an input word f0 that occurs in the context of
“... f-2 f-1 f0 f1 f2 ....” The score for a trans-
lation word e(fo) for an input word f0 is
</bodyText>
<equation confidence="0.72480325">
Score(e(fo))
� �1
Cf. (fp, e (fo )) � (8)
i!51pl!5Yy 1PI
</equation>
<bodyText confidence="0.980015">
defined as where p is the relative position of
the words surrounding f0, Cfo (fp, e(fo)) is
the value of the correlation matrix for f0, and y
</bodyText>
<figureCaption confidence="0.708093571428571">
A career .284 hitter, Beltran batted .267
in the regular season, split between
Kansas City and Houston, but came
alive in the playoffs. He hit .435 in 12
postseason games, with six stolen bases,
eight home runs and 14 runs batted in.
Figure 3. An example of an input word “home”
</figureCaption>
<bodyText confidence="0.996804571428571">
is the window size for word translation disam-
biguation.
A simple example is shown in Figure 3. The
word “home” in this context means the sense
of “home base” used in baseball games, not
“house” or “hometown.” The surrounding
words such as “games,” “bases,” and “runs”
can be clues for indicating the correct sense of
the word. By using the correlation matrix (Ta-
ble 2) and formula (8), we calculate a score for
each translation candidate and select the best
translation with the largest score. In this case,
Score(*V-- (honrui)) = 0.1134 was the best
score and the correct translation was selected.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.997792">
4.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.9999629">
We carried out word translation experiments
on two different settings. In the first experi-
ment (Experiment A), we used large-scale
comparable corpora from English and Japanese
newspaper texts. The second experiment (Ex-
periment B) was targeted at translating tech-
nical terms by using Chinese and Japanese
domain-specific corpora.
We used the following linguistic resources
for the experiments:
</bodyText>
<listItem confidence="0.995246">
• Experiment A
</listItem>
<page confidence="0.984425">
33
</page>
<table confidence="0.900171818181818">
■ Training comparable corpora
— The New York Times texts from
English Gigaword Corpus
Fourth Edition (LDC2009T13):
1.6 billion words
— The Mainichi Shimbun Corpus
(2000-2005): 195 million words
■ Test corpus
— A part of The New York Times
(January 2005): 157 paragraphs,
1,420 input words
</table>
<listItem confidence="0.992049">
• Experiment B1
■ Training comparable corpus
— In-house Chinese-Japanese pa-
rallel corpus in the environment
domain: 53,027 sentence pairs1
■ Test corpus
— A part of the training data: 1,443
sentences, 668 input words
• Experiment B2
■ Training comparable corpus
— In-house Chinese-Japanese pa-
rallel corpus in the medical do-
main: 123,175 sentence pairs
■ Test corpus
— A part of the training data: 940
sentences, 3,582 input words
• Dictionaries
■ Japanese-English bilingual dictiona-
ries: Total 333,656 term pairs
— EDR Electronic Dictionary
— Eijiro, Third Edition
— EDICT (Breen, 1995)
■ Chinese-English bilingual dictionary
— Chinese-English Translation
Lexicon Version 3.0 (LDC
2002L27): 54,170 term pairs
— Wanfang Data Chinese-English
Science/Technology Bilingual
Dictionary: 525,259 term pairs
For the Chinese-Japanese translation, we gen-
erated a Chinese-Japanese bilingual dictionary
by merging Chinese-English and Japa-
nese-English dictionaries. The Chi-
nese-Japanese bilingual dictionary includes
</listItem>
<bodyText confidence="0.943500956521739">
1 We could prepare only parallel corpora for Chi-
nese-Japanese language pair as training corpora.
For our experiments, we assumed them as compa-
rable corpora and did not use the correspondence of
sentence pairs.
every Chinese-Japanese term pair (tC, tJ) when
(tC, tE) and (tJ, tE) were present in the dictiona-
ries for one or more English terms tE. This
merged dictionary contains about two million
term pairs. While these Chinese-Japanese term
pairs include wrong translations, it was not a
serious problem in our experiments because
wrong translations were excluded in the pro-
cedure of our method.
We applied morphological analysis and
part-of-speech tagging by using TreeTagger
(Schmid, 1994) for English, JUMAN for Jap-
anese, and mma (Kruengkrai et al., 2009) for
Chinese, respectively.
In the test corpus, we manually annotated
reference translations for each target word.2
The parameters we used were as follows:
Experiment A:
</bodyText>
<equation confidence="0.98471">
Tf= 100 (Japanese), Tf = 1000 (English),
Tc = 4, w = 30, y = 30.
Experiment B1/B2:
Tf= 100, Tc = 4, w = 10, y = 25.
</equation>
<bodyText confidence="0.9995358">
Some of the parameters were empirically ad-
justed.
In the experiments, the matrices could be
obtained for 9103 English words (A), 674
Chinese words (B1) and 1258 Chinese words
(B2), respectively. In average one word had
3.24 (A), 1.15 (B1) and 1.51 (B2) translation
candidates3 by using the best setting. Table 2
is the resulted matrix for the word “home” in
the Experiment A.
</bodyText>
<sectionHeader confidence="0.712391" genericHeader="method">
4.2 Results of English-Japanese word
translation
</sectionHeader>
<bodyText confidence="0.9524356875">
Table 3 shows the results of Experiment A. We
classified the translation results for 1,420
target English words into four categories: True,
False, R, and M. When the translation was
output, the result was True if the output is
included in the reference translations, and it
was False otherwise. The result was R when all
the associated words in the correlation matrix
2 We prepared multiple references for several tar-
get words. The average numbers of reference trans-
lations for an input word are 1.84 (A), 1.50 (B1),
and 1.48 (B2), respectively.
3 From each matrix, we cut off the columns with
translations that do not have the best scores for any
associated words, because such translations are
never selected.
</bodyText>
<page confidence="0.998088">
34
</page>
<bodyText confidence="0.9829996">
did not occur around the input word. The result
was M when no correlation matrix existed for
the input word. We did not select a translation
output in these cases. The recall and precision
are shown in the parentheses below.
</bodyText>
<listItem confidence="0.999937">
• Recall = (True) / (Number of input words)
• Precision = (True) / (True + False)
</listItem>
<bodyText confidence="0.999763285714286">
Among the settings, we obtained the best
results, 42% recall and 49% precision, when
we used the Jaccard coefficient for association
scores and TA = 0, which means all pairs were
taken into consideration. Among other settings,
the Dice coefficient achieved a comparable
performance with Jaccard.
</bodyText>
<subsectionHeader confidence="0.500023">
4.3 Results of Chinese-Japanese word
translation
</subsectionHeader>
<bodyText confidence="0.999962272727273">
Tables 4 and 5 show the results of Experiment
B. In each domain, we tested only the settings
on Dice, pMI, and LLR with TA = 0.
In the environmental domain, the pointwise
mutual information score achieved the best
performance, 45% recall and 76% precision.
However, the Dice coefficient gave the best
recall (55%) for the medical domain. This re-
sult indicates that Experiment B1/B2 had
higher precision and more words without the
correlation matrix than Experiment A had.
</bodyText>
<subsectionHeader confidence="0.979854">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9999012">
As a result, we could generate bilingual lex-
icons with word translation disambiguation
information for 9103 English words and 1932
Chinese words. Although the number of words
might be augmented by changing the settings,
the size does not seem to be sufficient as bi-
lingual dictionaries. The availability of larger
output should be investigated.
The experimental results show that our me-
thod selected correct translations for at least
half of the input words if a correlation matrix
existed and if the associated words co-occur.
Among all input words, at least 40% of the
input words can be translated. The bilingual
dictionaries included 24.4, 38.6, and 52.0
translation candidates for one input word in
Experiment A, B1, and B2, respectively. When
we select the most frequent word, the preci-
sions were 7%, 1%, and 1%, respectively.
Meanwhile, the average numbers of translation
</bodyText>
<table confidence="0.9993854">
Score TA True False R M
Dice 0 588 627 90 115
(41%/48%)
0.001 586 619 94 121
(41%/49%)
0.01 479 507 243 191
(34%/49%)
Jacc- 0 594 621 90 115
ard (42%/49%)
0.001 584 609 105 122
(41%/49%)
0.01 348 378 374 320
(25%/48%)
pMI 0 292 309 704 115
(21%/49%)
1 293 308 703 116
(21%/49%)
LLR 10 530 747 28 115
(37%/42%)
100 529 744 32 115
(37%/42%)
T- 1 486 793 26 115
Score (34%/38%)
4 489 787 26 118
(34%/38%)
</table>
<tableCaption confidence="0.7888195">
Table 3. Results of English-Japanese word
translation (A)
</tableCaption>
<table confidence="0.993538625">
Score TA True False R M
Dice 0 1984 895 82 621
(55%/69%)
pMI 0 1886 804 271 621
(53%/70%)
LLR 0 1652 1246 63 621
(46%/57%)
Table 4. Results of Chinese-Japanese word
translation for environmental domain (B1).
Score TA True False R M
Dice 0 277 124 14 253
(41%/69%)
pMI 0 303 95 17 253
(45%/76%)
LLR 0 269 131 15 253
(40%/67%)
</table>
<tableCaption confidence="0.992156">
Table 5. Results of Chinese-Japanese word
translation for medical domain (B2).
</tableCaption>
<bodyText confidence="0.999337333333333">
candidates in the correlation matrices for one
input word are shown in Table 6. These indi-
cate that our method effectively removed noisy
</bodyText>
<page confidence="0.9983">
35
</page>
<bodyText confidence="0.993042235294118">
translations from the Chinese-Japanese dictio-
nary merged Japanese-English and Chi-
nese-English dictionaries, and that the associa-
tion scores contributed word translation dis-
ambiguation.
Among the settings, the Jaccard/Dice coef-
ficients were proven to be effective, although
pointwise mutual information (pMI) was also
effective for technical domains and the Chi-
nese-Japanese language pair. Because the Jac-
card/Dice coefficients were originally used for
measuring the proximity of sets, these might
be effective for collecting related words by
using the similarity of kinds of co-occurring
words. However, pMI tends to emphasize
low-frequency words as associated words. The
consequence of this tendency might be that
low-frequency associated words do not appear
around the input word in the newspaper text.4
In most metrics for the association score, the
lowest threshold value TA achieved the best
performance. This result indicates that the
cut-off of associated words by some thresholds
was not effective, although it requires more
time and memory space to obtain correlation
matrices without cut-off. How to optimize oth-
er parameters in our method remains unsolved.
More words without the correlation matrix
were present in Experiment B1/B2 than in Ex-
periment A because the input word was often a
technical term that was not in the bilingual dic-
tionary. The better recall and precision of Ex-
periment B1/B2 came from several reasons,
including difference of test sets and language
pairs. In addition, it might have an impact on
this result that the fact that word translation
disambiguation of technical terms is easier
than word translation disambiguation of com-
mon words.
We handled only nouns as input words and
associated words in this study. Considering
only the co-occurrence in a fixed window
would be insufficient to apply this method to
the translation of verbs and other parts of
speech. In future work, we will consider syn-
4 We limited the maximum number of association
words for one word to 400 in descending order of
their association scores because of restriction of
computational resources. In future work, we may
alleviate the drawback of pMI by enlarging or de-
leting this limitation.
</bodyText>
<table confidence="0.89074875">
Score TA Exp.A Exp.B1 Exp.B2
Dice 0 1.83 0.61 0.91
pMI 0 1.27 0.50 0.79
LLR 10/0 1.34 1.48 2.75
</table>
<bodyText confidence="0.9282789">
Table 6. Average numbers of translation can-
didates in the correlation matrices for one input
word.
tactic co-occurrence, which is obtained by
conducting dependency parsing of the results
of a sentence. The correlation between asso-
ciated words and translation candidates also
needs to be re-examined. Similarly, we will
handle verbs as associated words to the input
nouns by using syntactic co-occurrence.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999945787878788">
Statistical machine translation (Brown et al.,
1990) automatically acquires knowledge for
word translation disambiguation from parallel
corpora. Word translation disambiguation is
based on probabilities calculated from the
word alignment, phrase pair extraction, and the
language model. However, much broad con-
text/domain information is not considered.
Carpuat and Wu (2007) proposed con-
text-dependent phrasal translation lexicons by
introducing context-dependent features into
statistical machine translation.
Unsupervised methods using dictionaries
and corpora were proposed for monolingual
WSD (Ide and Veronis, 1998). They used
grammatical information including
parts-of-speech, syntactically related words,
and co-occurring words as the clues for the
WSD. Our method uses a part of the clues for
bilingual WSD and word translation disam-
biguation.
Li and Li (2002) constructed a classifier for
word translation disambiguation by using a
bilingual dictionary with bootstrapping tech-
niques. We also conducted recursive calcula-
tion by dealing with the bilingual dictionary as
the seeds of the iteration.
Vickrey et al. (2005) introduced a context as
a feature for a statistical MT system and they
generated word-level translations. How to in-
troduce the word-level translation disambigua-
tion into sentence-level translation is a consi-
derable problem.
</bodyText>
<page confidence="0.997532">
36
</page>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99996752">
In this paper, we described a method for add-
ing information for word translation disam-
biguation into the bilingual lexicon, by consi-
dering the associated words that co-occur with
the input word. We based our method on the
following two assumptions: “parallel word
associations” and “one sense per word associa-
tion.” We aligned word associations by using a
bilingual dictionary, and constructed a correla-
tion matrix for each word in the source lan-
guage for word translation disambiguation.
Experiments showed that our method was ap-
plicable for both common newspaper text and
domain-specific text and for two language
pairs. The Jaccard/Dice coefficients were
proven to be more effective than the other me-
trics as word association scores. Future work
includes extending our method to handle verbs
as input words by introducing syntactic
co-occurrence. The comparisons with other
disambiguation methods and machine transla-
tion systems would strengthen the effective-
ness of our method. We consider also evalua-
tions on real NLP tasks including machine
translation.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99706175">
This work was partially supported by Japa-
nese/Chinese Machine Translation Project in
Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
</bodyText>
<sectionHeader confidence="0.999052" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940954545455">
Brown, Peter F., John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
John D. Lafferty, Robert L. Mercer, and Paul S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79-85.
Carpuat, Marine and Dekai Wu. 2007. Con-
text-dependent phrasal translation lexicons for
statistical machine translation. In Proc. of Ma-
chine Translation Summit XI, pages 73-80.
Church, Kenneth W., William Gale, Patrick Hanks
and Donald Hindle. 1991. Using statistics in
lexical analysis. Lexical Acquisition: Using
On-line Resources to Build a Lexicon, pages
115-164.
Church, Kenneth W. and Patrick Hanks. 1990. Word
association norms, mutual information, and lex-
icography. Computational Linguistics,
16(1):22-29.
Dunning, Ted. 1993. Accurate methods for the sta-
tistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61-74.
Ide, Nancy and Jean Veronis. 1998. Introduction to
the special issue on word sense disambiguation:
the state of the art. Computational Linguistics,
24(1):1-40.
Kaji, Hiroyuki and Yasutsugu Morimoto. 2002.
Unsupervised word sense disambiguation using
bilingual comparable corpora. In Proc. of the
19th International Conference on Computational
Linguistics, pages 411-417.
Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and
Hitoshi Isahara. 2009. An error-driven
word-character hybrid model for joint Chinese
word segmentation and POS tagging. In Proc. of
the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the
AFNLP, pages 513-521.
Li, Cong and Hang Li. 2002. Word translation
disambiguation using bilingual bootstrapping. In
Proc. of the 40th Annual Meeting of Association
for Computational Linguistics, pages 343-351.
Rapp, Reinhard. 1995. Identifying word transla-
tions in non-parallel texts. In Proc. of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 320-322.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of the 1st
International Conference on New Methods in
Natural Language Processing.
Smadja, Frank. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics,
19(1):143-177.
Smadja, Frank, Kathleen R. McKeown and Vasi-
leios Hatzivassiloglou. 1996. Translating collo-
cations or bilingual lexicons: A statistical ap-
proach. Computational Linguistics, 22(1):3-38.
Vickrey, David, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-sense disambigua-
tion for machine translation. In Proc. of the
Conference on HLT/EMNLP, pages 771-778.
Yarowsky, David. 1993. One sense per collocation.
In Proc. of ARPA Human Language Technology
Workshop, pages 266-271.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259805">
<title confidence="0.999618">Augmenting a Bilingual Lexicon with Information for Word Translation Disambiguation</title>
<author confidence="0.860013">Takashi</author>
<affiliation confidence="0.9553095">Faculty of Shizuoka University</affiliation>
<email confidence="0.852711">tuna@inf.shizuoka.ac.jp</email>
<author confidence="0.415955">Hiroyuki</author>
<affiliation confidence="0.9602625">Faculty of Shizuoka University</affiliation>
<email confidence="0.938561">kaji@inf.shizuoka.ac.jp</email>
<abstract confidence="0.996897105263158">We describe a method for augmenting a bilingual lexicon with additional information for selecting an appropriate translation word. For each word in the source language, we calculate a correlation matrix of its association words versus its translation candidates. We estimate the degree of correlation by using comparable corpora based on these assumptions: “parallel word associations” and “one sense per word association.” In our word translation disambiguation experiment, the results show that our method achieved 42% recall and 49% precision for Japanese-English newspaper texts, and 45% recall and 76% precision for Chinese-Japanese technical documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<contexts>
<context position="21348" citStr="Brown et al., 1990" startWordPosition="3499" endWordPosition="3502">k of pMI by enlarging or deleting this limitation. Score TA Exp.A Exp.B1 Exp.B2 Dice 0 1.83 0.61 0.91 pMI 0 1.27 0.50 0.79 LLR 10/0 1.34 1.48 2.75 Table 6. Average numbers of translation candidates in the correlation matrices for one input word. tactic co-occurrence, which is obtained by conducting dependency parsing of the results of a sentence. The correlation between associated words and translation candidates also needs to be re-examined. Similarly, we will handle verbs as associated words to the input nouns by using syntactic co-occurrence. 5 Related Work Statistical machine translation (Brown et al., 1990) automatically acquires knowledge for word translation disambiguation from parallel corpora. Word translation disambiguation is based on probabilities calculated from the word alignment, phrase pair extraction, and the language model. However, much broad context/domain information is not considered. Carpuat and Wu (2007) proposed context-dependent phrasal translation lexicons by introducing context-dependent features into statistical machine translation. Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998). They used grammatical informat</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Context-dependent phrasal translation lexicons for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of Machine Translation Summit XI,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="21670" citStr="Carpuat and Wu (2007)" startWordPosition="3541" endWordPosition="3544">f the results of a sentence. The correlation between associated words and translation candidates also needs to be re-examined. Similarly, we will handle verbs as associated words to the input nouns by using syntactic co-occurrence. 5 Related Work Statistical machine translation (Brown et al., 1990) automatically acquires knowledge for word translation disambiguation from parallel corpora. Word translation disambiguation is based on probabilities calculated from the word alignment, phrase pair extraction, and the language model. However, much broad context/domain information is not considered. Carpuat and Wu (2007) proposed context-dependent phrasal translation lexicons by introducing context-dependent features into statistical machine translation. Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998). They used grammatical information including parts-of-speech, syntactically related words, and co-occurring words as the clues for the WSD. Our method uses a part of the clues for bilingual WSD and word translation disambiguation. Li and Li (2002) constructed a classifier for word translation disambiguation by using a bilingual dictionary with bootstr</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Carpuat, Marine and Dekai Wu. 2007. Context-dependent phrasal translation lexicons for statistical machine translation. In Proc. of Machine Translation Summit XI, pages 73-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William Gale</author>
<author>Patrick Hanks</author>
<author>Donald Hindle</author>
</authors>
<title>Using statistics in lexical analysis. Lexical Acquisition: Using On-line Resources to Build a Lexicon,</title>
<date>1991</date>
<pages>115--164</pages>
<contexts>
<context position="7699" citStr="Church et al., 1991" startWordPosition="1225" endWordPosition="1228">vely. The frequencies are summarized in Table 1. The word association scores a(x, y) are defined as follows: • Dice coefficient (Smadja, 1993) 2m Dice(x,y) = (1) �� + �� • Jaccard coefficient (Smadja et al., 1996) M Jaccard(x,y) = n,+ �� � � (2) • Pointwise mutual information (pMI) (Church and Hanks, 1990) m/M pMI(x,Y) = logZ (n1 IN)(nz IN) (3) • Log-likelihood ratio (LLR) (Dunning, 1993) LLR(x, y) = —2(logL(m, n1, r) + logL(n2 — m, N — n1, r) — logL(m, n1, r1) — logL(n2 — m, N — n1, r2)); (4) logL(k, n, r) = k loge r + (n — k) loge(1 — r), (5) nZ _M , � = � � �� • Student’s t-score (TScore) (Church et al., 1991) � � ���� �� TScore(�,�) = (7) �� We calculate association scores for all pairs of words when their occurrence frequencies are not less than a threshold Tf and when their Figure 2. Algorithm for calculating correlation matrices. x x not Total occur occur y m n2 – m n2 occur y not n1 – m M – n1 N – n2 occur – n2 + m Total n1 N – n1 N Table 1. Contingency matrix of occurrence frequencies. Algorithm 1: Input: f: an input word f’(1), ..., f’(I): associated words of f e(1), ..., e(J): translation candidates of f Nr: number of iterations A bilingual lexicon Word association scores a for both languag</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1991</marker>
<rawString>Church, Kenneth W., William Gale, Patrick Hanks and Donald Hindle. 1991. Using statistics in lexical analysis. Lexical Acquisition: Using On-line Resources to Build a Lexicon, pages 115-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--1</pages>
<contexts>
<context position="7386" citStr="Church and Hanks, 1990" startWordPosition="1153" endWordPosition="1156">y co-occur. Let n1 and n2 be the occurrence frequencies of x and y respectively, and let m be the frequency that x and y co-occur between w content words. The parameter w is a window size that adjusts the range of co-occurrences. Let N and M be the sum of occurrences/co-occurrences of all words/word pairs, respectively. The frequencies are summarized in Table 1. The word association scores a(x, y) are defined as follows: • Dice coefficient (Smadja, 1993) 2m Dice(x,y) = (1) �� + �� • Jaccard coefficient (Smadja et al., 1996) M Jaccard(x,y) = n,+ �� � � (2) • Pointwise mutual information (pMI) (Church and Hanks, 1990) m/M pMI(x,Y) = logZ (n1 IN)(nz IN) (3) • Log-likelihood ratio (LLR) (Dunning, 1993) LLR(x, y) = —2(logL(m, n1, r) + logL(n2 — m, N — n1, r) — logL(m, n1, r1) — logL(n2 — m, N — n1, r2)); (4) logL(k, n, r) = k loge r + (n — k) loge(1 — r), (5) nZ _M , � = � � �� • Student’s t-score (TScore) (Church et al., 1991) � � ���� �� TScore(�,�) = (7) �� We calculate association scores for all pairs of words when their occurrence frequencies are not less than a threshold Tf and when their Figure 2. Algorithm for calculating correlation matrices. x x not Total occur occur y m n2 – m n2 occur y not n1 – m</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, Kenneth W. and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="7470" citStr="Dunning, 1993" startWordPosition="1169" endWordPosition="1170"> the frequency that x and y co-occur between w content words. The parameter w is a window size that adjusts the range of co-occurrences. Let N and M be the sum of occurrences/co-occurrences of all words/word pairs, respectively. The frequencies are summarized in Table 1. The word association scores a(x, y) are defined as follows: • Dice coefficient (Smadja, 1993) 2m Dice(x,y) = (1) �� + �� • Jaccard coefficient (Smadja et al., 1996) M Jaccard(x,y) = n,+ �� � � (2) • Pointwise mutual information (pMI) (Church and Hanks, 1990) m/M pMI(x,Y) = logZ (n1 IN)(nz IN) (3) • Log-likelihood ratio (LLR) (Dunning, 1993) LLR(x, y) = —2(logL(m, n1, r) + logL(n2 — m, N — n1, r) — logL(m, n1, r1) — logL(n2 — m, N — n1, r2)); (4) logL(k, n, r) = k loge r + (n — k) loge(1 — r), (5) nZ _M , � = � � �� • Student’s t-score (TScore) (Church et al., 1991) � � ���� �� TScore(�,�) = (7) �� We calculate association scores for all pairs of words when their occurrence frequencies are not less than a threshold Tf and when their Figure 2. Algorithm for calculating correlation matrices. x x not Total occur occur y m n2 – m n2 occur y not n1 – m M – n1 N – n2 occur – n2 + m Total n1 N – n1 N Table 1. Contingency matrix of occur</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Jean Veronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: the state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="21916" citStr="Ide and Veronis, 1998" startWordPosition="3570" endWordPosition="3573">k Statistical machine translation (Brown et al., 1990) automatically acquires knowledge for word translation disambiguation from parallel corpora. Word translation disambiguation is based on probabilities calculated from the word alignment, phrase pair extraction, and the language model. However, much broad context/domain information is not considered. Carpuat and Wu (2007) proposed context-dependent phrasal translation lexicons by introducing context-dependent features into statistical machine translation. Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998). They used grammatical information including parts-of-speech, syntactically related words, and co-occurring words as the clues for the WSD. Our method uses a part of the clues for bilingual WSD and word translation disambiguation. Li and Li (2002) constructed a classifier for word translation disambiguation by using a bilingual dictionary with bootstrapping techniques. We also conducted recursive calculation by dealing with the bilingual dictionary as the seeds of the iteration. Vickrey et al. (2005) introduced a context as a feature for a statistical MT system and they generated word-level t</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>Ide, Nancy and Jean Veronis. 1998. Introduction to the special issue on word sense disambiguation: the state of the art. Computational Linguistics, 24(1):1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Kaji</author>
<author>Yasutsugu Morimoto</author>
</authors>
<title>Unsupervised word sense disambiguation using bilingual comparable corpora.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>411--417</pages>
<contexts>
<context position="1999" citStr="Kaji and Morimoto, 2002" startWordPosition="285" endWordPosition="288"> almost always has two or more translation candidates in the target language by looking up them in the bilingual lexicon. Because each translation candidate has a distinct meaning and property, we must be careful in selecting the appropriate translation candidate that has the same sense as the word inputted. This task is often called word translation disambiguation. In this paper, we describe a method for adding information for word translation disambiguation into the bilingual lexicon. Comparable corpora can be used to determine which word associations suggest which translations of the word (Kaji and Morimoto, 2002). First, we extract word associations in each language corpus and align them by using a bilingual dictionary. Then, we construct a word correlation matrix for each word in the source language. This correlation matrix works as information for word translation disambiguation. We carried out word translation experiments on two settings: English-to-Japanese and Chinese-to-Japanese. In the experiments, we tested Dice/Jaccard coefficients, pointwise mutual information, log-likelihood ratio, and Student’s t-score as the association measures for extracting word associations. 2 Constructing word correl</context>
</contexts>
<marker>Kaji, Morimoto, 2002</marker>
<rawString>Kaji, Hiroyuki and Yasutsugu Morimoto. 2002. Unsupervised word sense disambiguation using bilingual comparable corpora. In Proc. of the 19th International Conference on Computational Linguistics, pages 411-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<contexts>
<context position="14130" citStr="Kruengkrai et al., 2009" startWordPosition="2295" endWordPosition="2298">s comparable corpora and did not use the correspondence of sentence pairs. every Chinese-Japanese term pair (tC, tJ) when (tC, tE) and (tJ, tE) were present in the dictionaries for one or more English terms tE. This merged dictionary contains about two million term pairs. While these Chinese-Japanese term pairs include wrong translations, it was not a serious problem in our experiments because wrong translations were excluded in the procedure of our method. We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al., 2009) for Chinese, respectively. In the test corpus, we manually annotated reference translations for each target word.2 The parameters we used were as follows: Experiment A: Tf= 100 (Japanese), Tf = 1000 (English), Tc = 4, w = 30, y = 30. Experiment B1/B2: Tf= 100, Tc = 4, w = 10, y = 25. Some of the parameters were empirically adjusted. In the experiments, the matrices could be obtained for 9103 English words (A), 674 Chinese words (B1) and 1258 Chinese words (B2), respectively. In average one word had 3.24 (A), 1.15 (B1) and 1.51 (B2) translation candidates3 by using the best setting. Table 2 is</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513-521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Li</author>
<author>Hang Li</author>
</authors>
<title>Word translation disambiguation using bilingual bootstrapping.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>343--351</pages>
<contexts>
<context position="22164" citStr="Li and Li (2002)" startWordPosition="3608" endWordPosition="3611">xtraction, and the language model. However, much broad context/domain information is not considered. Carpuat and Wu (2007) proposed context-dependent phrasal translation lexicons by introducing context-dependent features into statistical machine translation. Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998). They used grammatical information including parts-of-speech, syntactically related words, and co-occurring words as the clues for the WSD. Our method uses a part of the clues for bilingual WSD and word translation disambiguation. Li and Li (2002) constructed a classifier for word translation disambiguation by using a bilingual dictionary with bootstrapping techniques. We also conducted recursive calculation by dealing with the bilingual dictionary as the seeds of the iteration. Vickrey et al. (2005) introduced a context as a feature for a statistical MT system and they generated word-level translations. How to introduce the word-level translation disambiguation into sentence-level translation is a considerable problem. 36 6 Conclusion In this paper, we described a method for adding information for word translation disambiguation into </context>
</contexts>
<marker>Li, Li, 2002</marker>
<rawString>Li, Cong and Hang Li. 2002. Word translation disambiguation using bilingual bootstrapping. In Proc. of the 40th Annual Meeting of Association for Computational Linguistics, pages 343-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>320--322</pages>
<contexts>
<context position="3412" citStr="Rapp, 1995" startWordPosition="496" endWordPosition="497">orrelation matrix for a word f consists of its association words and its translation candidates. Among the translation candidates, we choose the most acceptable one that is strongly suggested by its association words occurring around f. We use two assumptions for this framework: (i) Parallel word associations: Translations of words associated with each other in a language are also associated with each other in another language 30 Proceedings of the 8th Workshop on Asian Language Resources, pages 30–37, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing (Rapp, 1995). For example, two English words “tank” and “soldier” are associated with each other and their Japanese translations “ᡓ㌴ (sensha)” and “රኈ (heishi)” are also associated with each other. (ii) One sense per word association: A polysemous word exhibits only one sense of a word per word association (Yarowsky, 1993). For example, a polysemous word “tank” exhibits the “military vehicle” sense of a word when it is associated with “soldier,” while it exhibits the “container for liquid or gas” sense when it is associated with “gasoline.” Under these assumptions, we determine which of the words associat</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Rapp, Reinhard. 1995. Identifying word translations in non-parallel texts. In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 320-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proc. of the 1st International Conference on New Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="14063" citStr="Schmid, 1994" startWordPosition="2285" endWordPosition="2286">training corpora. For our experiments, we assumed them as comparable corpora and did not use the correspondence of sentence pairs. every Chinese-Japanese term pair (tC, tJ) when (tC, tE) and (tJ, tE) were present in the dictionaries for one or more English terms tE. This merged dictionary contains about two million term pairs. While these Chinese-Japanese term pairs include wrong translations, it was not a serious problem in our experiments because wrong translations were excluded in the procedure of our method. We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al., 2009) for Chinese, respectively. In the test corpus, we manually annotated reference translations for each target word.2 The parameters we used were as follows: Experiment A: Tf= 100 (Japanese), Tf = 1000 (English), Tc = 4, w = 30, y = 30. Experiment B1/B2: Tf= 100, Tc = 4, w = 10, y = 25. Some of the parameters were empirically adjusted. In the experiments, the matrices could be obtained for 9103 English words (A), 674 Chinese words (B1) and 1258 Chinese words (B2), respectively. In average one word had 3.24 (A), 1.15 (B1) and 1.51</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, Helmut. 1994. Probabilistic part-of-speech tagging using decision trees. In Proc. of the 1st International Conference on New Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<booktitle>Xtract. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="7221" citStr="Smadja, 1993" startWordPosition="1124" endWordPosition="1125">cus on some frequently used metrics for finding word associations based on their occurrence/co-occurrence frequencies. Suppose that words x and y frequently co-occur. Let n1 and n2 be the occurrence frequencies of x and y respectively, and let m be the frequency that x and y co-occur between w content words. The parameter w is a window size that adjusts the range of co-occurrences. Let N and M be the sum of occurrences/co-occurrences of all words/word pairs, respectively. The frequencies are summarized in Table 1. The word association scores a(x, y) are defined as follows: • Dice coefficient (Smadja, 1993) 2m Dice(x,y) = (1) �� + �� • Jaccard coefficient (Smadja et al., 1996) M Jaccard(x,y) = n,+ �� � � (2) • Pointwise mutual information (pMI) (Church and Hanks, 1990) m/M pMI(x,Y) = logZ (n1 IN)(nz IN) (3) • Log-likelihood ratio (LLR) (Dunning, 1993) LLR(x, y) = —2(logL(m, n1, r) + logL(n2 — m, N — n1, r) — logL(m, n1, r1) — logL(n2 — m, N — n1, r2)); (4) logL(k, n, r) = k loge r + (n — k) loge(1 — r), (5) nZ _M , � = � � �� • Student’s t-score (TScore) (Church et al., 1991) � � ���� �� TScore(�,�) = (7) �� We calculate association scores for all pairs of words when their occurrence frequencies</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Smadja, Frank. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations or bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="7292" citStr="Smadja et al., 1996" startWordPosition="1136" endWordPosition="1139">s based on their occurrence/co-occurrence frequencies. Suppose that words x and y frequently co-occur. Let n1 and n2 be the occurrence frequencies of x and y respectively, and let m be the frequency that x and y co-occur between w content words. The parameter w is a window size that adjusts the range of co-occurrences. Let N and M be the sum of occurrences/co-occurrences of all words/word pairs, respectively. The frequencies are summarized in Table 1. The word association scores a(x, y) are defined as follows: • Dice coefficient (Smadja, 1993) 2m Dice(x,y) = (1) �� + �� • Jaccard coefficient (Smadja et al., 1996) M Jaccard(x,y) = n,+ �� � � (2) • Pointwise mutual information (pMI) (Church and Hanks, 1990) m/M pMI(x,Y) = logZ (n1 IN)(nz IN) (3) • Log-likelihood ratio (LLR) (Dunning, 1993) LLR(x, y) = —2(logL(m, n1, r) + logL(n2 — m, N — n1, r) — logL(m, n1, r1) — logL(n2 — m, N — n1, r2)); (4) logL(k, n, r) = k loge r + (n — k) loge(1 — r), (5) nZ _M , � = � � �� • Student’s t-score (TScore) (Church et al., 1991) � � ���� �� TScore(�,�) = (7) �� We calculate association scores for all pairs of words when their occurrence frequencies are not less than a threshold Tf and when their Figure 2. Algorithm fo</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Smadja, Frank, Kathleen R. McKeown and Vasileios Hatzivassiloglou. 1996. Translating collocations or bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):3-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Luke Biewald</author>
<author>Marc Teyssier</author>
<author>Daphne Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the Conference on HLT/EMNLP,</booktitle>
<pages>771--778</pages>
<contexts>
<context position="22422" citStr="Vickrey et al. (2005)" startWordPosition="3646" endWordPosition="3649">tion. Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998). They used grammatical information including parts-of-speech, syntactically related words, and co-occurring words as the clues for the WSD. Our method uses a part of the clues for bilingual WSD and word translation disambiguation. Li and Li (2002) constructed a classifier for word translation disambiguation by using a bilingual dictionary with bootstrapping techniques. We also conducted recursive calculation by dealing with the bilingual dictionary as the seeds of the iteration. Vickrey et al. (2005) introduced a context as a feature for a statistical MT system and they generated word-level translations. How to introduce the word-level translation disambiguation into sentence-level translation is a considerable problem. 36 6 Conclusion In this paper, we described a method for adding information for word translation disambiguation into the bilingual lexicon, by considering the associated words that co-occur with the input word. We based our method on the following two assumptions: “parallel word associations” and “one sense per word association.” We aligned word associations by using a bil</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne Koller. 2005. Word-sense disambiguation for machine translation. In Proc. of the Conference on HLT/EMNLP, pages 771-778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proc. of ARPA Human Language Technology Workshop,</booktitle>
<pages>266--271</pages>
<contexts>
<context position="3724" citStr="Yarowsky, 1993" startWordPosition="547" endWordPosition="548">ns: Translations of words associated with each other in a language are also associated with each other in another language 30 Proceedings of the 8th Workshop on Asian Language Resources, pages 30–37, Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing (Rapp, 1995). For example, two English words “tank” and “soldier” are associated with each other and their Japanese translations “ᡓ㌴ (sensha)” and “රኈ (heishi)” are also associated with each other. (ii) One sense per word association: A polysemous word exhibits only one sense of a word per word association (Yarowsky, 1993). For example, a polysemous word “tank” exhibits the “military vehicle” sense of a word when it is associated with “soldier,” while it exhibits the “container for liquid or gas” sense when it is associated with “gasoline.” Under these assumptions, we determine which of the words associated with an input word suggests which of its translations by aligning word associations by using a bilingual dictionary. Consider the associated English words (tank, soldier) and their Japanese translations (ᡓ㌴ (sensha), රኈ (heishi)). When we translate the word “tank” into Japanese, the associated word “soldier”</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, David. 1993. One sense per collocation. In Proc. of ARPA Human Language Technology Workshop, pages 266-271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>