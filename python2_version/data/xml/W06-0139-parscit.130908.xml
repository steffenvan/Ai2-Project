<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.058324">
<title confidence="0.999601">
Description of the NCU Chinese Word Segmentation and Named Entity
Recognition System for SIGHAN Bakeoff 2006
</title>
<author confidence="0.999404">
Yu-Chieh Wu Jie-Chi Yang Qian-Xiang Lin
</author>
<affiliation confidence="0.97484425">
Dept. of Computer Science and Graduate Institute of Net- Dept. of Computer Science and
Information Engineering work Learning Technology Information Engineering
National Central University National Central University National Central University
Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan
</affiliation>
<email confidence="0.992411">
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw 93522083@cc.ncu.edu.tw
</email>
<sectionHeader confidence="0.994958" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9728019375">
Asian languages are far from most west-
ern-style in their non-separate word se-
quence especially Chinese. The
preliminary step of Asian-like language
processing is to find the word boundaries
between words. In this paper, we present
a general purpose model for both Chinese
word segmentation and named entity rec-
ognition. This model was built on the
word sequence classification with prob-
ability model, i.e., conditional random
fields (CRF). We used a simple feature set
for CRF which achieves satisfactory clas-
sification result on the two tasks. Our
model achieved 91.00 in F rate in UPUC-
Treebank data, and 78.71 for NER task.
</bodyText>
<sectionHeader confidence="0.999123" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845642857143">
With the rapid expansion of text media sources
such as news articles, technical reports, there is an
increasing demand for text mining and processing.
Among different cultures and countries, the Asian
languages are far from the other languages, there is
not an explicit boundary between words, for exam-
ple Chinese. Similar to English, the preliminary
step of most natural language processing is to “to-
kenize” each word. In Chinese, the word tokeniza-
tion is also known as word segmentation or
Chinese word tokenization.
To support the above targets, it is necessary to
detect the boundaries between words in a given
sentence. In tradition, the Chinese word segmenta-
tion technologies can be categorized into three
types, (heuristic) rule-based, machine learning, and
hybrid. Among them, the machine learning-based
techniques showed excellent performance in many
research studies (Peng et al., 2004; Zhou et al.,
2005; Gao et al., 2004). This method treats the
word segmentation problem as a sequence of word
classification. The classifier online assigns either
“boundary” or “non-boundary” label to each word
by learning from the large annotated corpora. Ma-
chine learning-based word segmentation method is
quite similar to the word sequence inference tech-
niques, such as part-of-speech (POS) tagging,
phrase chunking (Wu et al., 2006a) and named en-
tity recognition (Wu et al., 2006b).
In this paper, we present a prototype for Chinese
word segmentation and named entity recognition
based on the word sequence inference model.
Unlike previous researches (Zhou et al., 2005; Shi,
2005), we argue that without using the word seg-
mentation information, Chinese named entity rec-
ognition task can also be viewed as a variant word
segmentation technique. Therefore, the two tasks
can be accomplished without adapting the word
sequence inference model. The preliminary ex-
perimental result show that in the word segmenta-
tion task, our method can achieve 91.00 in F rate
for the UPUC Chinese Treebank data, while it at-
</bodyText>
<page confidence="0.995711">
209
</page>
<bodyText confidence="0.475504">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 209–212,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<note confidence="0.320852">
CP: Chinese word phrase LOC: Location ORG: Organization O: Non-named entity word
</note>
<figureCaption confidence="0.99852">
Figure 1: Sequence of word classification model
</figureCaption>
<bodyText confidence="0.973206625">
tends 78.76 F rate for the Microsoft Chinese
named entity recognition task.
The rest of this paper is organized as follows.
Section 2 describes the word sequence inference
model and the used learner. Experimental result
and evaluations are reported in section 3. Finally,
in section 4, we draw conclusion and future re-
marks.
</bodyText>
<sectionHeader confidence="0.975211" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9999042">
In this section, we firstly describe the overall sys-
tem architecture for the word segmentation and
named entity recognition tasks. In section 2.2, the
employed classification model- conditional random
fields (CRF) is then presented.
</bodyText>
<subsectionHeader confidence="0.997106">
2.1 Word Sequence Classification
</subsectionHeader>
<bodyText confidence="0.999930366666667">
Similar to English text chunking (Ramshaw and
Marcus, 1995; Wu et al., 2006a), the word se-
quence classification model aims to classify each
word via encoding its context features. An example
can be shown in Figure 1. In Figure1, the model is
classifying the Chinese character “國” (country).
The second row in Figure 1 means the correspond-
ing category of each in the word-segmentation
(WS) task, while the third row indicates the class
in the named entity recognition (NER) task. For
the WS task, there are only two word types, B-CP
(Begin of Chinese phrase) and I-CP (Interior of
Chinese phrase). In contrast, the word types in the
NER task depend on the pre-defined named class.
For example, both in MSR and CityU datasets,
person, location, and organization should be identi-
fied. In this paper, we used the similar IOB2 repre-
sentation style (Wu et al., 2006a) to express the
Chinese word structures.
By encoding with IOB style, both WS and NER
problems can be viewed as a sequence of word
classification. During testing, we seek to find the
optimal word type for each Chinese character.
These types strongly reflect the actual word
boundaries for Chinese words or named entity
phrases.
To effect classify each character, in this paper,
we employ 13 feature templates to capture the con-
text information of it. Table 1 lists the adopted fea-
ture templates.
</bodyText>
<tableCaption confidence="0.973691666666667">
Table 1: Feature template used for both Chi-
nese word segmentation and named entity rec-
ognition tasks
</tableCaption>
<table confidence="0.998217777777778">
Feature Examples Feature Type Examples
Type
W-2 領 W0 + W+1 國+愛
W-1 中 W+1 + W+2 愛+樂
W0 國 W+1 + W+2 愛+樂
W+1 愛 W-2+W-1+W0 領+中+國
W+2 樂 W-1+W0+W+1 中+國+愛
W-2 + W-1 領+中 W0+W+1+W+2 國+愛+樂
W-1 + W0 中+國
</table>
<subsectionHeader confidence="0.98801">
2.2 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.999807733333333">
Conditional random field (CRF) was an extension
of both Maximum Entropy Model (MEMs) and
Hidden Markov Models (HMMs) that was firstly
introduced by (Lafferty et al., 2001). CRF defined
conditional probability distribution P(Y|X) of given
sequence given input sentence where Y is the
“class label” sequence and X denotes as the obser-
vation word sequence.
A CRF on (X,Y) is specified by a feature vector
F of local context and the corresponding feature
weight λ. The F can be treated as the combination
of state transition and observation value in conven-
tional HMM. To determine the optimal label se-
quence, the CRF uses the following equation to
estimate the most probability.
</bodyText>
<equation confidence="0.994113">
y = arg max (  |, ) arg max ( , )
P y x A, = A, F y x
y y
</equation>
<page confidence="0.987785">
210
</page>
<bodyText confidence="0.999965142857143">
The most probable label sequence y can be effi-
ciently extracted via the Viterbi algorithm. How-
ever, training a CRF is equivalent to estimate the
parameter setλfor the feature set. In this paper, we
directly use the quasi-Newton L-BFGS 1 method
(Nocedal and Wright, 1999) to iterative update the
parameters.
</bodyText>
<sectionHeader confidence="0.991666" genericHeader="method">
3 Evaluations and Experimental Result
</sectionHeader>
<subsectionHeader confidence="0.987984">
3.1 Dataset and Evaluations
</subsectionHeader>
<bodyText confidence="0.999974833333333">
We evaluated our model in the close track on
UPUC Chinese Treebank for Chinese word seg-
mentation task, and CityU corpus for Chinese NER
task. Both settings are the same for the two tasks.
The evaluations of the two tasks were mainly
measured by the three metrics, namely recall, pre-
cision, and f1-measurement. However, the evalua-
tion style for the NER and WS is quite different. In
WS, participant should reformulate the testing data
into sentence level whereas the NER was evaluated
in the token-level. Table 2 lists the results of the
two tasks with our preliminary model.
</bodyText>
<tableCaption confidence="0.9842825">
Table 2: Official results on the word segmenta-
tion and named entity recognition tasks
</tableCaption>
<table confidence="0.998315333333333">
Dataset F1-measure
Word segmentation UPUC 91.00
Named entity recognition CityU 78.71
</table>
<tableCaption confidence="0.996267">
Table 3: Experimental results for the three
Chinese word segmentation datasets
</tableCaption>
<table confidence="0.9982775">
Closed Task CityU MSR UPUC
Recall 0.958 0.940 0.917
Precision 0.926 0.906 0.904
F-measure 0.942 0.923 0.910
</table>
<subsectionHeader confidence="0.995707">
3.2 Experimental Result on Word Segmenta-
tion Task
</subsectionHeader>
<bodyText confidence="0.999848428571428">
To explore the effectiveness of our method, we go
on extend our model to the other three tasks for the
WS track, namely CityU, MSR. Table3 shows the
experimental results of our model in the all close
WS track except for CKIP corpus. These results do
not officially provided by the SIGHAN due to the
time limitation.
</bodyText>
<footnote confidence="0.955113">
1 http://www-unix.mcs.anl.gov/tao/
</footnote>
<subsectionHeader confidence="0.985732">
3.3 Experimental Result on Named Entity
Recognition Task
</subsectionHeader>
<bodyText confidence="0.999893285714286">
In the second experiment, we focus on directly
adapting our method for the NER track. Table 4
lists the experimental result of our method in the
CityU and MSR datasets. It is worth to note that
due to the different evaluation style in NER tracks,
our tokenization rules did not consistent with the
SIGHAN provided testing tokens. Our preliminary
tokenization rules produced 371814 characters for
the testing data, while there are 364356 tokens in
the official provided testing set. Such a big trouble
deeply earns the actual performance of our model.
To propose a reliable and actual result, we directly
evaluate our method in the official provided testing
set again. As shown in Table 4, the our method
achieved 0.787 in F rate with non-correct version.
In contrast, after correcting the Chinese tokeniza-
tion rules as well as SIGHAN official provided
tokens, our method significantly improved from
0.787 to 0.868. Similarly, our method performed
very on the MSR track which reached 0.818 in F
rate.
</bodyText>
<tableCaption confidence="0.890686">
Table 4: Experimental results for MSR and
City closed NER tasks
</tableCaption>
<table confidence="0.9993034">
Closed Task City (official City MSR
result) (correct)
Recall 0.697 0.931 0.752
Precision 0.935 0.814 0.896
F-measure 0.787 0.868 0.818
</table>
<sectionHeader confidence="0.997094" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999970625">
Chinese word segmentation is the most important
foundations for many Chinese linguistic technolo-
gies such as text categorization and information
retrieval. In this paper, we present simple Chinese
word segmentation and named entity recognition
models based on the conventional sequence classi-
fication technique. The main focus of our work is
to provide a light-weight and simple model that
could be easily ported to different domains and
languages. Without any prior knowledge and rules,
such a simple technique shows satisfactory results
on both word segmentation and named entity rec-
ognition tasks. To reach state-of-the-art this model
still needs to employed more detail feature engines
and analysis. In the future, one of the main direc-
tions is to extend this model toward full unsuper-
</bodyText>
<page confidence="0.994279">
211
</page>
<bodyText confidence="0.9999295">
vised learning from large un-annotated text. Min-
ing from large unlabeled data have been showed
benefits to improve the original accuracy. Thus,
not only the more stochastic feature analysis, but
also adjust the learner from unlabeled data are im-
portant future remarks.
</bodyText>
<sectionHeader confidence="0.998532" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719071428571">
Lafferty, J., McCallum, A., and Pereira, F. 2001.
Conditional Random Field: Probabilistic models
for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Ma-
chine Learning.
Gao, J., Wu, A., Li, M., Huang, C. N., Li, H., Xia, X.,
and Qin, H. 2004. Adaptive Chinese word segmen-
tation. In Proceedings the 41st Annual Meeting of
the Association for Computational Linguistics, pp.
21-26.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the 3rd Workshop on Very
Large Corpora, pages 82-94.
Nocedal, J., and Wright, S. 1999. Numerical optimi-
zation. Springer.
Peng, F., Feng, F., and McCallum, A. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Porceedings of the Com-
putational Linguistics, pp. 562-568.
Shi, W. 2005. Chinese Word Segmentation Based On
Direct Maximum Entropy Model. In Proceedings
of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Wu, Y. C., Chang, C. H. and Lee, Y. S. 2006a. A
general and multi-lingual phrase chunking model
based on masking method. Lecture Notes in Com-
puter Science (LNCS): Computational Linguistics
and Intelligent Text Processing, 3878: 144-155.
Wu, Y. C., Fan, T. K., Lee Y. S. and Yen, S. J. 2006b.
Extracting named entities using support vector
machines,&amp;quot; Lecture Notes in Bioinformatics
(LNBI): Knowledge Discovery in Life Science
Literature, (3886): 91-103.
Wu, Y. C., Lee, Y. S., and Yang, J. C. 2006c. The
Exploration of Deterministic and Efficient De-
pendency Parsing. In Proceedings of the 10th Con-
ference on Natural Language Learning (CoNLL).
Zhou, J., Dai, X., Ni, R., Chen, J. 2005. .A Hybrid
Approach to Chinese Word Segmentation around
CRFs. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing.
</reference>
<page confidence="0.99849">
212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262851">
<title confidence="0.900171">Description of the NCU Chinese Word Segmentation and Named Recognition System for SIGHAN Bakeoff 2006</title>
<author confidence="0.996894">Wu Jie-Chi Yang Lin</author>
<affiliation confidence="0.997287333333333">Dept. of Computer Science and Graduate Institute of Net- Dept. of Computer Science and Information Engineering work Learning Technology Information Engineering National Central University National Central University National Central University</affiliation>
<address confidence="0.986006">Taoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwan</address>
<email confidence="0.704853">bcbb@db.csie.ncu.edu.twyang@cl.ncu.edu.tw93522083@cc.ncu.edu.tw</email>
<abstract confidence="0.964878882352941">Asian languages are far from most western-style in their non-separate word sequence especially Chinese. The preliminary step of Asian-like language processing is to find the word boundaries between words. In this paper, we present a general purpose model for both Chinese word segmentation and named entity recognition. This model was built on the word sequence classification with probability model, i.e., conditional random fields (CRF). We used a simple feature set for CRF which achieves satisfactory classification result on the two tasks. Our model achieved 91.00 in F rate in UPUC- Treebank data, and 78.71 for NER task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Field: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5913" citStr="Lafferty et al., 2001" startWordPosition="931" endWordPosition="934">is paper, we employ 13 feature templates to capture the context information of it. Table 1 lists the adopted feature templates. Table 1: Feature template used for both Chinese word segmentation and named entity recognition tasks Feature Examples Feature Type Examples Type W-2 領 W0 + W+1 國+愛 W-1 中 W+1 + W+2 愛+樂 W0 國 W+1 + W+2 愛+樂 W+1 愛 W-2+W-1+W0 領+中+國 W+2 樂 W-1+W0+W+1 中+國+愛 W-2 + W-1 領+中 W0+W+1+W+2 國+愛+樂 W-1 + W0 中+國 2.2 Conditional Random Fields Conditional random field (CRF) was an extension of both Maximum Entropy Model (MEMs) and Hidden Markov Models (HMMs) that was firstly introduced by (Lafferty et al., 2001). CRF defined conditional probability distribution P(Y|X) of given sequence given input sentence where Y is the “class label” sequence and X denotes as the observation word sequence. A CRF on (X,Y) is specified by a feature vector F of local context and the corresponding feature weight λ. The F can be treated as the combination of state transition and observation value in conventional HMM. To determine the optimal label sequence, the CRF uses the following equation to estimate the most probability. y = arg max ( |, ) arg max ( , ) P y x A, = A, F y x y y 210 The most probable label sequence y </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional Random Field: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>A Wu</author>
<author>M Li</author>
<author>C N Huang</author>
<author>H Li</author>
<author>X Xia</author>
<author>H Qin</author>
</authors>
<title>Adaptive Chinese word segmentation.</title>
<date>2004</date>
<booktitle>In Proceedings the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>21--26</pages>
<contexts>
<context position="2090" citStr="Gao et al., 2004" startWordPosition="308" endWordPosition="311">imilar to English, the preliminary step of most natural language processing is to “tokenize” each word. In Chinese, the word tokenization is also known as word segmentation or Chinese word tokenization. To support the above targets, it is necessary to detect the boundaries between words in a given sentence. In tradition, the Chinese word segmentation technologies can be categorized into three types, (heuristic) rule-based, machine learning, and hybrid. Among them, the machine learning-based techniques showed excellent performance in many research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004). This method treats the word segmentation problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the word sequence inference model. Unlike pr</context>
</contexts>
<marker>Gao, Wu, Li, Huang, Li, Xia, Qin, 2004</marker>
<rawString>Gao, J., Wu, A., Li, M., Huang, C. N., Li, H., Xia, X., and Qin, H. 2004. Adaptive Chinese word segmentation. In Proceedings the 41st Annual Meeting of the Association for Computational Linguistics, pp. 21-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="4121" citStr="Ramshaw and Marcus, 1995" startWordPosition="620" endWordPosition="623">amed entity recognition task. The rest of this paper is organized as follows. Section 2 describes the word sequence inference model and the used learner. Experimental result and evaluations are reported in section 3. Finally, in section 4, we draw conclusion and future remarks. 2 System Description In this section, we firstly describe the overall system architecture for the word segmentation and named entity recognition tasks. In section 2.2, the employed classification model- conditional random fields (CRF) is then presented. 2.1 Word Sequence Classification Similar to English text chunking (Ramshaw and Marcus, 1995; Wu et al., 2006a), the word sequence classification model aims to classify each word via encoding its context features. An example can be shown in Figure 1. In Figure1, the model is classifying the Chinese character “國” (country). The second row in Figure 1 means the corresponding category of each in the word-segmentation (WS) task, while the third row indicates the class in the named entity recognition (NER) task. For the WS task, there are only two word types, B-CP (Begin of Chinese phrase) and I-CP (Interior of Chinese phrase). In contrast, the word types in the NER task depend on the pre</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the 3rd Workshop on Very Large Corpora, pages 82-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S Wright</author>
</authors>
<title>Numerical optimization.</title>
<date>1999</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6748" citStr="Nocedal and Wright, 1999" startWordPosition="1084" endWordPosition="1087"> by a feature vector F of local context and the corresponding feature weight λ. The F can be treated as the combination of state transition and observation value in conventional HMM. To determine the optimal label sequence, the CRF uses the following equation to estimate the most probability. y = arg max ( |, ) arg max ( , ) P y x A, = A, F y x y y 210 The most probable label sequence y can be efficiently extracted via the Viterbi algorithm. However, training a CRF is equivalent to estimate the parameter setλfor the feature set. In this paper, we directly use the quasi-Newton L-BFGS 1 method (Nocedal and Wright, 1999) to iterative update the parameters. 3 Evaluations and Experimental Result 3.1 Dataset and Evaluations We evaluated our model in the close track on UPUC Chinese Treebank for Chinese word segmentation task, and CityU corpus for Chinese NER task. Both settings are the same for the two tasks. The evaluations of the two tasks were mainly measured by the three metrics, namely recall, precision, and f1-measurement. However, the evaluation style for the NER and WS is quite different. In WS, participant should reformulate the testing data into sentence level whereas the NER was evaluated in the token-</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Nocedal, J., and Wright, S. 1999. Numerical optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>F Feng</author>
<author>A McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Porceedings of the Computational Linguistics,</booktitle>
<pages>562--568</pages>
<contexts>
<context position="2052" citStr="Peng et al., 2004" startWordPosition="300" endWordPosition="303"> between words, for example Chinese. Similar to English, the preliminary step of most natural language processing is to “tokenize” each word. In Chinese, the word tokenization is also known as word segmentation or Chinese word tokenization. To support the above targets, it is necessary to detect the boundaries between words in a given sentence. In tradition, the Chinese word segmentation technologies can be categorized into three types, (heuristic) rule-based, machine learning, and hybrid. Among them, the machine learning-based techniques showed excellent performance in many research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004). This method treats the word segmentation problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the wo</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Peng, F., Feng, F., and McCallum, A. 2004. Chinese segmentation and new word detection using conditional random fields. In Porceedings of the Computational Linguistics, pp. 562-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Shi</author>
</authors>
<title>Chinese Word Segmentation Based On Direct Maximum Entropy Model.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2738" citStr="Shi, 2005" startWordPosition="409" endWordPosition="410">tion problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the word sequence inference model. Unlike previous researches (Zhou et al., 2005; Shi, 2005), we argue that without using the word segmentation information, Chinese named entity recognition task can also be viewed as a variant word segmentation technique. Therefore, the two tasks can be accomplished without adapting the word sequence inference model. The preliminary experimental result show that in the word segmentation task, our method can achieve 91.00 in F rate for the UPUC Chinese Treebank data, while it at209 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 209–212, Sydney, July 2006. c�2006 Association for Computational Linguistics CP: Chinese word</context>
</contexts>
<marker>Shi, 2005</marker>
<rawString>Shi, W. 2005. Chinese Word Segmentation Based On Direct Maximum Entropy Model. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>C H Chang</author>
<author>Y S Lee</author>
</authors>
<title>A general and multi-lingual phrase chunking model based on masking method.</title>
<date>2006</date>
<booktitle>Lecture Notes in Computer Science (LNCS): Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>3878</volume>
<pages>144--155</pages>
<contexts>
<context position="2488" citStr="Wu et al., 2006" startWordPosition="367" endWordPosition="370">s, (heuristic) rule-based, machine learning, and hybrid. Among them, the machine learning-based techniques showed excellent performance in many research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004). This method treats the word segmentation problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the word sequence inference model. Unlike previous researches (Zhou et al., 2005; Shi, 2005), we argue that without using the word segmentation information, Chinese named entity recognition task can also be viewed as a variant word segmentation technique. Therefore, the two tasks can be accomplished without adapting the word sequence inference model. The preliminary experimental result show that in the word segmentation task, our method c</context>
<context position="4138" citStr="Wu et al., 2006" startWordPosition="624" endWordPosition="627">sk. The rest of this paper is organized as follows. Section 2 describes the word sequence inference model and the used learner. Experimental result and evaluations are reported in section 3. Finally, in section 4, we draw conclusion and future remarks. 2 System Description In this section, we firstly describe the overall system architecture for the word segmentation and named entity recognition tasks. In section 2.2, the employed classification model- conditional random fields (CRF) is then presented. 2.1 Word Sequence Classification Similar to English text chunking (Ramshaw and Marcus, 1995; Wu et al., 2006a), the word sequence classification model aims to classify each word via encoding its context features. An example can be shown in Figure 1. In Figure1, the model is classifying the Chinese character “國” (country). The second row in Figure 1 means the corresponding category of each in the word-segmentation (WS) task, while the third row indicates the class in the named entity recognition (NER) task. For the WS task, there are only two word types, B-CP (Begin of Chinese phrase) and I-CP (Interior of Chinese phrase). In contrast, the word types in the NER task depend on the pre-defined named cl</context>
</contexts>
<marker>Wu, Chang, Lee, 2006</marker>
<rawString>Wu, Y. C., Chang, C. H. and Lee, Y. S. 2006a. A general and multi-lingual phrase chunking model based on masking method. Lecture Notes in Computer Science (LNCS): Computational Linguistics and Intelligent Text Processing, 3878: 144-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>T K Fan</author>
<author>Y S Lee</author>
<author>S J Yen</author>
</authors>
<title>Extracting named entities using support vector machines,&amp;quot;</title>
<date>2006</date>
<booktitle>Lecture Notes in Bioinformatics (LNBI): Knowledge Discovery in Life Science Literature,</booktitle>
<volume>3886</volume>
<pages>91--103</pages>
<contexts>
<context position="2488" citStr="Wu et al., 2006" startWordPosition="367" endWordPosition="370">s, (heuristic) rule-based, machine learning, and hybrid. Among them, the machine learning-based techniques showed excellent performance in many research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004). This method treats the word segmentation problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the word sequence inference model. Unlike previous researches (Zhou et al., 2005; Shi, 2005), we argue that without using the word segmentation information, Chinese named entity recognition task can also be viewed as a variant word segmentation technique. Therefore, the two tasks can be accomplished without adapting the word sequence inference model. The preliminary experimental result show that in the word segmentation task, our method c</context>
<context position="4138" citStr="Wu et al., 2006" startWordPosition="624" endWordPosition="627">sk. The rest of this paper is organized as follows. Section 2 describes the word sequence inference model and the used learner. Experimental result and evaluations are reported in section 3. Finally, in section 4, we draw conclusion and future remarks. 2 System Description In this section, we firstly describe the overall system architecture for the word segmentation and named entity recognition tasks. In section 2.2, the employed classification model- conditional random fields (CRF) is then presented. 2.1 Word Sequence Classification Similar to English text chunking (Ramshaw and Marcus, 1995; Wu et al., 2006a), the word sequence classification model aims to classify each word via encoding its context features. An example can be shown in Figure 1. In Figure1, the model is classifying the Chinese character “國” (country). The second row in Figure 1 means the corresponding category of each in the word-segmentation (WS) task, while the third row indicates the class in the named entity recognition (NER) task. For the WS task, there are only two word types, B-CP (Begin of Chinese phrase) and I-CP (Interior of Chinese phrase). In contrast, the word types in the NER task depend on the pre-defined named cl</context>
</contexts>
<marker>Wu, Fan, Lee, Yen, 2006</marker>
<rawString>Wu, Y. C., Fan, T. K., Lee Y. S. and Yen, S. J. 2006b. Extracting named entities using support vector machines,&amp;quot; Lecture Notes in Bioinformatics (LNBI): Knowledge Discovery in Life Science Literature, (3886): 91-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>Y S Lee</author>
<author>J C Yang</author>
</authors>
<title>The Exploration of Deterministic and Efficient Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="2488" citStr="Wu et al., 2006" startWordPosition="367" endWordPosition="370">s, (heuristic) rule-based, machine learning, and hybrid. Among them, the machine learning-based techniques showed excellent performance in many research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004). This method treats the word segmentation problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the word sequence inference model. Unlike previous researches (Zhou et al., 2005; Shi, 2005), we argue that without using the word segmentation information, Chinese named entity recognition task can also be viewed as a variant word segmentation technique. Therefore, the two tasks can be accomplished without adapting the word sequence inference model. The preliminary experimental result show that in the word segmentation task, our method c</context>
<context position="4138" citStr="Wu et al., 2006" startWordPosition="624" endWordPosition="627">sk. The rest of this paper is organized as follows. Section 2 describes the word sequence inference model and the used learner. Experimental result and evaluations are reported in section 3. Finally, in section 4, we draw conclusion and future remarks. 2 System Description In this section, we firstly describe the overall system architecture for the word segmentation and named entity recognition tasks. In section 2.2, the employed classification model- conditional random fields (CRF) is then presented. 2.1 Word Sequence Classification Similar to English text chunking (Ramshaw and Marcus, 1995; Wu et al., 2006a), the word sequence classification model aims to classify each word via encoding its context features. An example can be shown in Figure 1. In Figure1, the model is classifying the Chinese character “國” (country). The second row in Figure 1 means the corresponding category of each in the word-segmentation (WS) task, while the third row indicates the class in the named entity recognition (NER) task. For the WS task, there are only two word types, B-CP (Begin of Chinese phrase) and I-CP (Interior of Chinese phrase). In contrast, the word types in the NER task depend on the pre-defined named cl</context>
</contexts>
<marker>Wu, Lee, Yang, 2006</marker>
<rawString>Wu, Y. C., Lee, Y. S., and Yang, J. C. 2006c. The Exploration of Deterministic and Efficient Dependency Parsing. In Proceedings of the 10th Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhou</author>
<author>X Dai</author>
<author>R Ni</author>
<author>J Chen</author>
</authors>
<title>A Hybrid Approach to Chinese Word Segmentation around CRFs.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2071" citStr="Zhou et al., 2005" startWordPosition="304" endWordPosition="307"> example Chinese. Similar to English, the preliminary step of most natural language processing is to “tokenize” each word. In Chinese, the word tokenization is also known as word segmentation or Chinese word tokenization. To support the above targets, it is necessary to detect the boundaries between words in a given sentence. In tradition, the Chinese word segmentation technologies can be categorized into three types, (heuristic) rule-based, machine learning, and hybrid. Among them, the machine learning-based techniques showed excellent performance in many research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004). This method treats the word segmentation problem as a sequence of word classification. The classifier online assigns either “boundary” or “non-boundary” label to each word by learning from the large annotated corpora. Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging, phrase chunking (Wu et al., 2006a) and named entity recognition (Wu et al., 2006b). In this paper, we present a prototype for Chinese word segmentation and named entity recognition based on the word sequence inferen</context>
</contexts>
<marker>Zhou, Dai, Ni, Chen, 2005</marker>
<rawString>Zhou, J., Dai, X., Ni, R., Chen, J. 2005. .A Hybrid Approach to Chinese Word Segmentation around CRFs. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>