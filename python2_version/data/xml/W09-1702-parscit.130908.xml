<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.990444">
Utilizing Contextually Relevant Terms in Bilingual Lexicon Extraction
</title>
<author confidence="0.993792">
Azniah Ismail
</author>
<affiliation confidence="0.9978495">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.926757">
York YO10 5DD UK
</address>
<email confidence="0.998319">
azniah@cs.york.ac.uk
</email>
<author confidence="0.990162">
Suresh Manandhar
</author>
<affiliation confidence="0.997858">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.926922">
York YO10 5DD UK
</address>
<email confidence="0.998724">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999343461538461">
This paper demonstrates one efficient tech-
nique in extracting bilingual word pairs from
non-parallel but comparable corpora. Instead
of using the common approach of taking high
frequency words to build up the initial bilin-
gual lexicon, we show contextually relevant
terms that co-occur with cognate pairs can be
efficiently utilized to build a bilingual dictio-
nary. The result shows that our models using
this technique have significant improvement
over baseline models especially when highest-
ranked translation candidate per word is con-
sidered.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997828617647059">
Bilingual lexicons or dictionaries are invaluable
knowledge resources for language processing tasks.
The compilation of such bilingual lexicons remains
as a substantial issue to linguistic fields. In gen-
eral practice, many linguists and translators spend
huge amounts of money and effort to compile this
type of knowledge resources either manually, semi-
automatically or automatically. Thus, obtaining the
data is expensive.
In this paper, we demonstrate a technique that uti-
lizes contextually relevant terms that co-occur with
cognate pairs to expand an initial bilingual lexi-
con. We use unannotated resources that are freely
available such as English-Spanish Europarl corpus
(Koehn, 2005) and another different set of cognate
pairs as seed words.
We show that this technique is able to achieve
high precision score for bilingual lexicon extracted
10
from non-parallel but comparable corpora. Our
model using this technique with spelling similarity
approach obtains 85.4 percent precision at 50.0 per-
cent recall. Precision of 79.0 percent at 50.0 percent
recall is recorded when using this technique with
context similarity approach. Furthermore, by using
a string edit-distance vs. precision curve, we also
reveal that the latter model is able to capture words
efficiently compared to a baseline model.
Section 2 is dedicated to mention some of the re-
lated works. In Section 3, the technique that we used
is explained. Section 4 describes our experimental
setup followed by the evaluation results in Section
5. Discussion and conclusion are in Section 6 and 7
respectively.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999957705882353">
Koehn and Knight (2002) describe few potential
clues that may help in extracting bilingual lexi-
con from two monolingual corpora such as identi-
cal words, similar spelling, and similar context fea-
tures. In reporting our work, we treat both identical
word pairs and similar spelling word pairs as cog-
nate pairs.
Koehn and Knight (2002) map 976 identical
word pairs that are found in their two monolin-
gual German-English corpora and report that 88.0
percent of them are correct. They propose to re-
strict the word length, at least of length 6, to in-
crease the accuracy of the collected word pairs.
Koehn and Knight (2002) mention few related works
that use different measurement to compute the sim-
ilarity, such as longest common subsequence ratio
(Melamed, 1995) and string edit distance (Mann
</bodyText>
<note confidence="0.579656">
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 10–17,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921913043478">
and Yarowski, 2001). However, Koehn and Knight
(2002) point out that majority of their word pairs
do not show much resemblance at all since they
use German-English language pair. Haghighi et al.
(2008) mention one disadvantage of using edit dis-
tance, that is, precision quickly degrades with higher
recall. Instead, they propose assigning a feature to
each substring of length of three or less for each
word.
For approaches based on contextual features or
context similarity, we assume that for a word that
occurs in a certain context, its translation equivalent
also occurs in equivalent contexts. Contextual fea-
tures are the frequency counts of context words oc-
curring in the surrounding of target word W. A con-
text vector for each W is then constructed, with only
context words found in the seed lexicon. The context
vectors are then translated into the target language
before their similarity is measured.
Fung and Yee (1998) point out that not only the
number of common words in context gives some
similarity clue to a word and its translation, but the
actual ranking of the context word frequencies also
provides important clue to the similarity between a
bilingual word pair. This fact has motivated Fung
and Yee (1998) to use tfidf weighting to compute the
vectors. This idea is similar to Rapp (1999) who
proposed to transform all co-occurrence vectors us-
ing log likelihood ratio instead of just using the
frequency counts of the co-occurrences. These val-
ues are used to define whether the context words are
highly associated with the W or not.
Earlier work relies on a large bilingual dictionary
as their seed lexicon (Rapp, 1999; Fung and Yee,
1998; among others). Koehn and Knight (2002)
present one interesting idea of using extracted cog-
nate pairs from corpus as the seed words in order
to alleviate the need of huge, initial bilingual lex-
icon. Haghighi et al. (2008), amongst a few oth-
ers, propose using canonical correlation analysis to
reduce the dimension. Haghighi et al (2008) only
use a small-sized bilingual lexicon containing 100
word pairs as seed lexicon. They obtain 89.0 percent
precision at 33.0 percent recall for their English-
Spanish induction with best feature set, using top-
ically similar but non-parallel corpora.
</bodyText>
<sectionHeader confidence="0.978177" genericHeader="method">
3 The Utilizing Technique
</sectionHeader>
<bodyText confidence="0.99892425">
Most works in bilingual lexicon extraction use lists
of high frequency words that are obtained from
source and target language corpus to be their source
and target word lists respectively. In our work, we
aim to extract a high precision bilingual lexicon us-
ing different approach. Instead, we use list of con-
textually relevant terms that co-occur with cognate
pairs.
</bodyText>
<figureCaption confidence="0.999483">
Figure 1: Cognate pair extraction
</figureCaption>
<bodyText confidence="0.880146588235294">
These cognate pairs can be derived automatically
by mapping or finding identical words occur in two
high frequency list of two monolingual corpora (see
Figure 1). They are used to acquire list of source
word W3 and target word Wt. W3 and Wt are contex-
tually relevant terms that highly co-occur with the
cognate pairs in the same context. Thus, log likeli-
hood measure can be used to identify them.
Next, bilingual word pairs are extracted among
words in these W3 and Wt list using either context
similarity or spelling similarity. Figure 2 shows
some examples of potential bilingual word pairs,
of W3 and Wt, co-occurring with identical cognate
pairs of word ’civil’.
As we are working on English-Spanish language
pair, we extract bilingual lexicon using string edit
distance to identify spelling similarity between W3
</bodyText>
<page confidence="0.955501">
11
</page>
<bodyText confidence="0.940153355555555">
and Wt. Figure 3 outlines the algorithm using
spelling similarity in more detail.
Using the same W3 and Wt lists, we extract bilin-
gual lexicon by computing the context similarity be-
tween each {W3,Wt} pair. To identify the context
similarity, the relation between each {W3, Wt} pair
can be detected automatically using a vector similar-
ity measure such as cosine measure as in (1). The A
and B are the elements in the context vectors, con-
taining either zero or non-zero seed word values for
W3 and Wt, respectively.
A × B
Cosine similarity = cos(θ) = ||A ||× ||B ||(1)
The cosine measure favors {W3,Wt} pairs that
share the most number of non-zero seed word val-
ues. However, one disadvantage of this measure is
that the cosine value directly proportional to the ac-
tual W3 and Wt values. Even though W3 and Wt
might not closely correlated with the same set of
seed words, the matching score could be high if W3
or Wt has high seed word values everywhere. Thus,
we transform the context vectors from real value
into binary vectors before the similarity is computed.
Figure 4 outlines the algorithm using context simi-
larity in more detail.
In the algorithm, after the W3 and Wt lists are ob-
tained, each W3 and Wt units is represented by their
context vector containing log likelihood (LL) values
of contextually relevant words, occurring in the seed
lexicon, that highly co-occur with the W3 and Wt re-
spectively. To get this context vector, for each W3
and Wt, all sentences in the English or Spanish cor-
pora containing the respective word are extracted to
form a particular sub corpus, e.g. sub corpus soci-
ety is a collection of sentences containing the source
word society.
Using window size of a sentence, the LL value
of term occurring with the word W3 or Wt in their
respective sub corpora is computed. Term that is
highly associated with the W3 or Wt is called con-
textually relevant term. However, we consider each
term with LL value higher than certain threshold
(e.g. threshold ≥ 15.0) to be contextually relevant.
Contextually relevant terms occurring in the seed
lexicon are used to build the context vector for the
</bodyText>
<page confidence="0.211008">
12
</page>
<figureCaption confidence="0.999938">
Figure 2: Bilingual word pairs are found within context
of cognate word civil
Figure 3: Utilizing technique with spelling similarity
</figureCaption>
<bodyText confidence="0.965142">
1996 - 1999, year 2000 - 2003 and year 2004
- 2006.
• We only take the first part, about 400k sen-
tences of Europarl Spanish (year 1996 - 1999)
and 2nd part, also about 400k from Europarl
English (year 2000 - 2003). We refer the partic-
ular part taken from the source language corpus
as S and the other part of the target language
corpus as T.
This approach is quite common in order to ob-
tain non-parallel but comparable (or same domain)
corpus. Examples can be found in Fung and Che-
ung (2004), followed by Haghighi et al. (2008).
For corpus pre-processing, we only use sentence
boundary detection and tokenization on raw text.
We decided that large quantities of raw text requir-
ing minimum processing could also be considered as
minimal since they are inexpensive and not limited.
These should contribute to low or medium density
languages for which annotated resources are limited.
We also clean all tags and filter out stop words from
</bodyText>
<table confidence="0.892571135135135">
the corpus.
4.2 Evaluation
We extracted our evaluation lexicon from Word Ref-
erence∗ free online dictionary . For this work, the
word types are not restricted but mostly are con-
tent words. We have two sets of evaluation. In one,
we take high ranked candidate pairs where W3 could
have multiple translations. In the other, we only con-
sider highest-ranked Wt for each W3. For evalua-
tion purposes, we take only the top 2000 candidate
ranked-pairs from the output. From that list, only
candidate pairs with words found in the evaluation
lexicon are proposed. We use F1-measure to evalu-
ate proposed lexicon against the evaluation lexicon.
The recall is defined as the proportion of the high
ranked candidate pairs. The precision is given as the
number of correct candidate pairs divided by the to-
tal number of proposed candidate pairs.
4.3 Other Setups
The following were also setup and used:
• List of cognate pairs
We obtained 79 identical cognate pairs from the
Figure 4: Utilizing technique with context similarity
W3 or Wt respectively. For example, word participa-
tion and education occurring in the seed lexicon are
contextually relevant terms for source word society.
Thus, they become elements of the context vector.
Then, we transform the context vectors, from real
value into binary, before we compute the similarity
with cosine measure.
4 Experimental Setup
4.1 Data
For source and target monolingual corpus, we de-
rive English and Spanish sentences from parallel Eu-
roparl corpora (Koehn, 2005).
• We split each of them into three parts; year ∗from website http://www.wordreference.com
13
</table>
<bodyText confidence="0.972495">
top 2000 high frequency lists of our S and T but
we chose 55 of these that have at least 100 con-
textually relevant terms that are highly associ-
ated with each of them.
</bodyText>
<listItem confidence="0.926617">
• Seed lexicon
</listItem>
<bodyText confidence="0.999915157894737">
We also take a set of cognate pairs to be our
seed lexicon. We defined the size of a small
seed lexicon ranges between 100 to 1k word
pairs. Hence, our seed lexicon containing 700
cognate pairs are still considered as a small-
sized seed lexicon. However, instead of acquir-
ing this set of cognate pairs automatically, we
compiled the cognate pairs from a few Learn-
ing Spanish Cognates websites †. This ap-
proach is a simple alternative to replace the
10-20k general dictionaries (Rapp, 1999; Fung
and McKeown, 2004) or automatic seed words
(Koehn and Knight, 2002; Haghighi et al.,
2008). However, this approach can only be
used if the source and target language are fairly
related and both share lexically similar words
that most likely have same meaning. Other-
wise, we have to rely on general bilingual dic-
tionaries.
</bodyText>
<listItem confidence="0.844183">
• Stop list
</listItem>
<bodyText confidence="0.981412142857143">
Previously (Rapp, 1999; Koehn and Knight,
2002; among others) suggested filtering out
commonly occurring words that do not help
in processing natural language data. This idea
sometimes seem as a negative approach to the
natural articles of language, however various
studies have proven that it is sensible to do so.
</bodyText>
<listItem confidence="0.78809">
• Baseline system
</listItem>
<bodyText confidence="0.955455">
We build baseline systems using basic context
similarity and spelling similarity features.
</bodyText>
<sectionHeader confidence="0.993909" genericHeader="method">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.991725666666667">
For the first evaluation, candidate pairs are ranked
after being measured either with cosine for context
similarity or edit distance for spelling similarity. In
this evaluation, we take the first 2000 of {W3, Wt}
candidate pairs from the proposed lexicon where W3
may have multiple translations or multiple Wt. See
</bodyText>
<tableCaption confidence="0.843086">
Table 1.
</tableCaption>
<footnote confidence="0.986715">
†such as http://www.colorincolorado.org and
http://www.language-learning-advisor.com
</footnote>
<table confidence="0.959416875">
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
ContextSim (CS) 42.9 69.6 60.7 58.7 49.6
SpellingSim (SS) 90.5 74.2 69.9 64.6 50.9
(a) from baseline models
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
E-ContextSim (ECS) 78.3 73.5 71.8 64.0 51.2
E-SpellingSim (ESS) 95.8 75.6 71.8 63.4 51.5
(b) from our proposed models
</table>
<tableCaption confidence="0.9936425">
Table 1: Performance of baseline and our model for top
2000 candidates below certain threshold and ranked
</tableCaption>
<table confidence="0.94282925">
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
ContextSim-Top1 (CST) 58.3 61.2 64.8 55.2 52.6
SpellingSim-Top1 (SST) 84.9 66.4 52.7 34.5 37.0
(a) from baseline models
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
E-ContextSim-Top1 (ECST) 85.0 81.1 79.7 79.0 57.1
E-SpellingSim-Top1 (ESST) 100.0 93.6 91.6 85.4 59.0
(b) from our proposed models
</table>
<tableCaption confidence="0.991267">
Table 2: Performance of baseline and our model for top
2000 candidates of top 1
</tableCaption>
<bodyText confidence="0.999852545454545">
Using either context or spelling similarity ap-
proach on S and T (labeled ECS and ESS respec-
tively), our models achieved about 51.2 percent of
best F1 measure. Those are not a significant im-
provement with only 1.0 to 2.0 percent error reduc-
tion over the baseline models (labeled CS and SS).
For the second evaluation, we take the first 2000
of {W3, Wt} pairs where W3 may only have the high-
est ranked Wt as translation candidates (See Table
2). This time, both of our models (with context
similarity and spelling similarity, labeled ECST and
ESST respectively) yielded almost 60.0 percent of
best F1 measure. It is noted that using ESST alone
recorded a significant improvement of 20.0 percent
in the F1 score compared to SST baseline model.
ESST obtained 85.4 percent precision at 50.0 per-
cent recall. Precision of 79.0 percent at 50.0 percent
recall is recorded when using ECST. However, the
ECST has not recorded a significant difference over
CST baseline model (57.1 and 52.6 percent respec-
tively) in the second evaluation. The overall perfor-
mances, represented by precision scores for different
</bodyText>
<page confidence="0.980511">
14
</page>
<bodyText confidence="0.999020666666667">
recorded higher F1 scores especially when consid-
ering only the highest-ranked candidates.
We also experiment with context similarity ap-
proach. We would like to see how far this approach
helps to add to the candidate scores from our corpus
S and T. The other reason is sometimes a correct tar-
get is not always a cognate even though a cognate
for it is available. Our ECST model has not recorded
significant improvement over CST baseline model in
the F1-measure. However, we were able to show that
by utilizing contextually relevant terms, ECST gath-
ers more correct candidate pairs especially when it
comes to words with dissimilar spelling. This means
that ECST is able to add more to the candidate scores
compared to CST. Thus, more correct translation
pairs can be expected with a good combination of
ECST and ESST.
The following are the advantages of our utilizing
</bodyText>
<listItem confidence="0.914346166666667">
technique:
• Reduced errors, hence able to improve preci-
sion scores.
• Extraction is more efficient in the contextual
boundaries (see Appendix B for examples).
• Context similarity approach within our tech-
</listItem>
<bodyText confidence="0.994632984848485">
nique has a potential to add more to the can-
didate scores.
Yet, our attempt using cognate pairs as seed words is
more appropriate for language pairs that share large
number of cognates or similar spelling words with
same meaning. Otherwise, one may have to rely on
bilingual dictionaries.
There may be some possible supporting strate-
gies, which we could use to help improve further
the precision score within the utilizing technique.
For example, dimension reduction using canonical
correlation analysis (CCA), resemblance detection,
measure of dispersion, reference corpus and further
noise reduction. However, we do not include a re-
ranking method, as we are using collection of cog-
nate pairs instead of a general bilingual dictionary.
Since our corpus S and T is in similar domain, we
might still not have seen the potential of this tech-
nique in its entirety. One may want to test the tech-
nique with different type of corpora for future works.
Figure 5: String Edit Distance vs. Precision curve
range of recalls, for these four models are illustrated
in Appendix A.
It is important to see the inner performance of the
ECST model with further analysis. We present a
string edit distance value (EDv) vs. precision curve
for ECST and CST in Figure 5 to measure the per-
formance of the ECST model in capturing bilingual
pairs with less similar orthographic features, those
that may not be captured using spelling similarity.
The graph in Figure 5 shows that even though
CST has higher precision score than ECST at EDv
of 2, it is not significant (the difference is less than
5.0 percent) and the spelling is still similar. On the
other hand, precision for proposed lexicon with EDv
above 3 (where the Ws and the proposed translation
equivalent Wt spelling becoming more dissimilar)
using ECST is higher than CST. The most significant
difference of the precision is almost 35.0 percent,
where ECST achieved almost 75.0 percent precision
compared to CST with 40.0 percent precision at EDv
of 4. It is followed by ECST with almost 50.0 per-
cent precision compared to CST with precision less
than 35.0 percent, offering about 15.0 percent preci-
sion improvement at EDv of 5.
6 Discussion
As we are working on English-Spanish language
pair, we could have focused on spelling similar-
ity feature only. Performance of the model using
this feature usually record higher accuracy other-
wise they may not be commonly occurring in a cor-
pus. Our models with this particular feature have
15
Nevertheless, we are still concerned that many
spurious translation equivalents were proposed be-
cause the words actually have higher correlation
with the input source word compared to the real
target word. Otherwise, the translation equivalents
may not be in the boundaries or in the corpus from
which translation equivalents are to be extracted.
Haghighi et al (2008) have reported that the most
common errors detected in their analysis on top 100
errors were from semantically related words, which
had strong context feature correlations. Thus, the
issue remains. We leave all these for further discus-
sion in future works.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986222222222">
We present a bilingual lexicon extraction technique
that utilizes contextually relevant terms that co-
occur with cognate pairs to expand an initial bilin-
gual lexicon. We show that this utilizing technique
is able to achieve high precision score for bilingual
lexicon extracted from non-parallel but comparable
corpora. We demonstrate this technique using unan-
notated resources that are freely available.
Our model using this technique with spelling sim-
ilarity obtains 85.4 percent precision at 50.0 percent
recall. Precision of 79.0 percent at 50.0 percent re-
call is recorded when using this technique with con-
text similarity approach. We also reveal that the
latter model with context similarity is able to cap-
ture words efficiently compared to a baseline model.
Thus, we show contextually relevant terms that co-
occur with cognate pairs can be efficiently utilized
to build a bilingual dictionary.
</bodyText>
<sectionHeader confidence="0.998546" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.997273878048781">
Cranias, L., Papageorgiou, H, and Piperidis, S. 1994.
A matching technique in Example-Based Machine
Translation. In International Conference On Compu-
tational Linguistics Proceedings, 15th conference on
Computational linguistics, Kyoto, Japan.
Diab, M., and Finch, S. 2000. A statistical word-level
translation model for comparable corpora. In Proceed-
ings of the Conference on Content-based multimedia
information access (RIAO).
Fung, P., and Cheung, P. 2004. Mining very non-parallel
corpora: Parallel sentence and lexicon extraction via
bootstrapping and EM. In Proceedings of the 2004
Conference on Empirical Method in Natural Language
Processing (EMNLP), Barcelona, Spain.
Fung, P., and Yee, L.Y. 1998. An IR Approach for
Translating New Words from Nonparallel, Compara-
ble Texts. In Proceedings of COLING-ACL98, Mon-
treal, Canada, 1998.
Fung, P., and McKeown, K. 1997. Finding Terminology
Translations from Non-parallel Corpora. In The 5th
Annual Workshop on Very Large Corpora, Hong Kong,
Aug 1997.
Haghighi, A., Liang, P., Berg-Krikpatrick, T., and Klein,
D. 2008. Learning bilingual lexicons from monolin-
gual corpora. In Proceedings of The ACL 2008, June
15 -20 2008, Columbus, Ohio
Koehn, P. 2005. Europarl: a parallel corpus for statistical
machine translation. In MT Summit
Koehn, P., and Knight , K. 2001. Knowledge sources
for word-level translation models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Koehn, P., and Knight, K. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
ACL 2002, July 2002, Philadelphia, USA, pp. 9-16.
Rapp, R. 1995. Identifying word translations in non-
parallel texts. In Proceedings of ACL 33, pages 320-
322.
Rapp, R. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of ACL 37, pages 519-526.
</reference>
<page confidence="0.993558">
16
</page>
<sectionHeader confidence="0.615574" genericHeader="method">
Appendix A. Precision scores with different recalls
</sectionHeader>
<figure confidence="0.99143786">
CS vs. ECS
SS vs. ESS
Context Similarity Approach
90
80
70
60
SO
40
30
20
10
0
CS
-0- ECS
OA 0/ 03 OA 0.5 0.6 0.7 0.8 0.9 1
Recall
Context Similarity Approach - Top 1
100
80
60
40
20
0
-a- ECST
OA 02 03 04 CO 04 07 03 09 1
Recall
Spelling Similarity Approach
0
0.1 02 03 0.4 0.5 0.6 0.7 08 09 1
Recall
120
100
SO
60
40
20
- SS
1- .&apos;&amp;quot; ES&apos;S
Spelling Similarity Approach -Top 1
120 -4)- SST
100 - ESST
80
60
40
20
0
01 0.2 0.3 0.4 03 0.6 0.7 0,8 09 I
Recall
CST vs. ECST SST vs. ESST
</figure>
<table confidence="0.974210518518518">
Appendix B. Some examples of effective extraction via utilizing technique
Source Target ECVT CV Rank
Candidate found Sim. value Candidate found Sim. value
clause clansula clausula 0.402015126 au/ant/ca 0.447213595 1
fortalecimiento 0.430331483 2
economic° 0.412478956 &lt;&gt;
respeto 0.40824829 &lt;&gt;
vigor 0.402015126 &lt;&gt;
clausu1a 0_402015126 &lt;&gt;
pillar p/Tar pllar 0.547722558 daramente 0.632455532 1
pt/ar 0.547722558 2
basada 0.53935989 3
comercial 0.516397779 4
iniciado 0.516397779 4
exterior 0.478091444 5
agricola 0.447213595 6
state es/ado estado 0_433012702 derecho 0.43519414 1
es/ado 0.433012702 2
respeto 0.412478956 &lt; &gt;
confidence confianza confianza 0.424264069 errores 0.447213595 1
desarrollo 0.447213595 1
haberse 0.447213595 1
denmestran 0.447213595 1
deficiencias 0.447213595 1
confianza 0_424264069 2
welfare bienestar bienestar 0.40824829 hubiera 0.500000000 1
bienestar 0.40824829 &lt;&gt;
</table>
<page confidence="0.993592">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.378127">
<title confidence="0.99942">Utilizing Contextually Relevant Terms in Bilingual Lexicon Extraction</title>
<author confidence="0.906466">Azniah</author>
<affiliation confidence="0.9997">Department of Computer University of</affiliation>
<address confidence="0.973139">York YO10 5DD</address>
<email confidence="0.993845">azniah@cs.york.ac.uk</email>
<author confidence="0.57697">Suresh</author>
<affiliation confidence="0.9996415">Department of Computer University of</affiliation>
<address confidence="0.971615">York YO10 5DD</address>
<email confidence="0.998001">suresh@cs.york.ac.uk</email>
<abstract confidence="0.983133857142857">This paper demonstrates one efficient technique in extracting bilingual word pairs from non-parallel but comparable corpora. Instead of using the common approach of taking high frequency words to build up the initial bilingual lexicon, we show contextually relevant terms that co-occur with cognate pairs can be efficiently utilized to build a bilingual dictionary. The result shows that our models using this technique have significant improvement over baseline models especially when highestranked translation candidate per word is considered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Cranias</author>
<author>H Papageorgiou</author>
<author>S Piperidis</author>
</authors>
<title>A matching technique in Example-Based Machine Translation.</title>
<date>1994</date>
<booktitle>In International Conference On Computational Linguistics Proceedings, 15th conference on Computational linguistics, Kyoto,</booktitle>
<marker>Cranias, Papageorgiou, Piperidis, 1994</marker>
<rawString>Cranias, L., Papageorgiou, H, and Piperidis, S. 1994. A matching technique in Example-Based Machine Translation. In International Conference On Computational Linguistics Proceedings, 15th conference on Computational linguistics, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>S Finch</author>
</authors>
<title>A statistical word-level translation model for comparable corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Content-based multimedia information access (RIAO).</booktitle>
<marker>Diab, Finch, 2000</marker>
<rawString>Diab, M., and Finch, S. 2000. A statistical word-level translation model for comparable corpora. In Proceedings of the Conference on Content-based multimedia information access (RIAO).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>P Cheung</author>
</authors>
<title>Mining very non-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Method in Natural Language Processing (EMNLP),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="9671" citStr="Fung and Cheung (2004)" startWordPosition="1582" endWordPosition="1586">gure 2: Bilingual word pairs are found within context of cognate word civil Figure 3: Utilizing technique with spelling similarity 1996 - 1999, year 2000 - 2003 and year 2004 - 2006. • We only take the first part, about 400k sentences of Europarl Spanish (year 1996 - 1999) and 2nd part, also about 400k from Europarl English (year 2000 - 2003). We refer the particular part taken from the source language corpus as S and the other part of the target language corpus as T. This approach is quite common in order to obtain non-parallel but comparable (or same domain) corpus. Examples can be found in Fung and Cheung (2004), followed by Haghighi et al. (2008). For corpus pre-processing, we only use sentence boundary detection and tokenization on raw text. We decided that large quantities of raw text requiring minimum processing could also be considered as minimal since they are inexpensive and not limited. These should contribute to low or medium density languages for which annotated resources are limited. We also clean all tags and filter out stop words from the corpus. 4.2 Evaluation We extracted our evaluation lexicon from Word Reference∗ free online dictionary . For this work, the word types are not restrict</context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Fung, P., and Cheung, P. 2004. Mining very non-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and EM. In Proceedings of the 2004 Conference on Empirical Method in Natural Language Processing (EMNLP), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>L Y Yee</author>
</authors>
<title>An IR Approach for Translating New Words from Nonparallel, Comparable Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL98,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="4357" citStr="Fung and Yee (1998)" startWordPosition="672" endWordPosition="675">tead, they propose assigning a feature to each substring of length of three or less for each word. For approaches based on contextual features or context similarity, we assume that for a word that occurs in a certain context, its translation equivalent also occurs in equivalent contexts. Contextual features are the frequency counts of context words occurring in the surrounding of target word W. A context vector for each W is then constructed, with only context words found in the seed lexicon. The context vectors are then translated into the target language before their similarity is measured. Fung and Yee (1998) point out that not only the number of common words in context gives some similarity clue to a word and its translation, but the actual ranking of the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors. This idea is similar to Rapp (1999) who proposed to transform all co-occurrence vectors using log likelihood ratio instead of just using the frequency counts of the co-occurrences. These values are used to define whether the context words are highly assoc</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Fung, P., and Yee, L.Y. 1998. An IR Approach for Translating New Words from Nonparallel, Comparable Texts. In Proceedings of COLING-ACL98, Montreal, Canada, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K McKeown</author>
</authors>
<title>Finding Terminology Translations from Non-parallel Corpora.</title>
<date>1997</date>
<booktitle>In The 5th Annual Workshop on Very Large Corpora,</booktitle>
<location>Hong Kong,</location>
<marker>Fung, McKeown, 1997</marker>
<rawString>Fung, P., and McKeown, K. 1997. Finding Terminology Translations from Non-parallel Corpora. In The 5th Annual Workshop on Very Large Corpora, Hong Kong, Aug 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>P Liang</author>
<author>T Berg-Krikpatrick</author>
<author>D Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of The ACL</booktitle>
<volume>15</volume>
<location>Columbus, Ohio</location>
<contexts>
<context position="3629" citStr="Haghighi et al. (2008)" startWordPosition="552" endWordPosition="555"> collected word pairs. Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 10–17, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics and Yarowski, 2001). However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. Haghighi et al. (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. Instead, they propose assigning a feature to each substring of length of three or less for each word. For approaches based on contextual features or context similarity, we assume that for a word that occurs in a certain context, its translation equivalent also occurs in equivalent contexts. Contextual features are the frequency counts of context words occurring in the surrounding of target word W. A context vector for each W is then constructed, with only context words found in the seed le</context>
<context position="5309" citStr="Haghighi et al. (2008)" startWordPosition="835" endWordPosition="838">e the vectors. This idea is similar to Rapp (1999) who proposed to transform all co-occurrence vectors using log likelihood ratio instead of just using the frequency counts of the co-occurrences. These values are used to define whether the context words are highly associated with the W or not. Earlier work relies on a large bilingual dictionary as their seed lexicon (Rapp, 1999; Fung and Yee, 1998; among others). Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. Haghighi et al. (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. They obtain 89.0 percent precision at 33.0 percent recall for their EnglishSpanish induction with best feature set, using topically similar but non-parallel corpora. 3 The Utilizing Technique Most works in bilingual lexicon extraction use lists of high frequency words that are obtained from source and target language corpus to be their source and target word lists respectively. In our work, we aim</context>
<context position="9707" citStr="Haghighi et al. (2008)" startWordPosition="1589" endWordPosition="1592">nd within context of cognate word civil Figure 3: Utilizing technique with spelling similarity 1996 - 1999, year 2000 - 2003 and year 2004 - 2006. • We only take the first part, about 400k sentences of Europarl Spanish (year 1996 - 1999) and 2nd part, also about 400k from Europarl English (year 2000 - 2003). We refer the particular part taken from the source language corpus as S and the other part of the target language corpus as T. This approach is quite common in order to obtain non-parallel but comparable (or same domain) corpus. Examples can be found in Fung and Cheung (2004), followed by Haghighi et al. (2008). For corpus pre-processing, we only use sentence boundary detection and tokenization on raw text. We decided that large quantities of raw text requiring minimum processing could also be considered as minimal since they are inexpensive and not limited. These should contribute to low or medium density languages for which annotated resources are limited. We also clean all tags and filter out stop words from the corpus. 4.2 Evaluation We extracted our evaluation lexicon from Word Reference∗ free online dictionary . For this work, the word types are not restricted but mostly are content words. We </context>
<context position="12475" citStr="Haghighi et al., 2008" startWordPosition="2053" endWordPosition="2056">ted with each of them. • Seed lexicon We also take a set of cognate pairs to be our seed lexicon. We defined the size of a small seed lexicon ranges between 100 to 1k word pairs. Hence, our seed lexicon containing 700 cognate pairs are still considered as a smallsized seed lexicon. However, instead of acquiring this set of cognate pairs automatically, we compiled the cognate pairs from a few Learning Spanish Cognates websites †. This approach is a simple alternative to replace the 10-20k general dictionaries (Rapp, 1999; Fung and McKeown, 2004) or automatic seed words (Koehn and Knight, 2002; Haghighi et al., 2008). However, this approach can only be used if the source and target language are fairly related and both share lexically similar words that most likely have same meaning. Otherwise, we have to rely on general bilingual dictionaries. • Stop list Previously (Rapp, 1999; Koehn and Knight, 2002; among others) suggested filtering out commonly occurring words that do not help in processing natural language data. This idea sometimes seem as a negative approach to the natural articles of language, however various studies have proven that it is sensible to do so. • Baseline system We build baseline syst</context>
<context position="19402" citStr="Haghighi et al (2008)" startWordPosition="3188" endWordPosition="3191">nguage pair, we could have focused on spelling similarity feature only. Performance of the model using this feature usually record higher accuracy otherwise they may not be commonly occurring in a corpus. Our models with this particular feature have 15 Nevertheless, we are still concerned that many spurious translation equivalents were proposed because the words actually have higher correlation with the input source word compared to the real target word. Otherwise, the translation equivalents may not be in the boundaries or in the corpus from which translation equivalents are to be extracted. Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. Thus, the issue remains. We leave all these for further discussion in future works. 7 Conclusion We present a bilingual lexicon extraction technique that utilizes contextually relevant terms that cooccur with cognate pairs to expand an initial bilingual lexicon. We show that this utilizing technique is able to achieve high precision score for bilingual lexicon extracted from non-parallel but comparable corpora. We demonstra</context>
</contexts>
<marker>Haghighi, Liang, Berg-Krikpatrick, Klein, 2008</marker>
<rawString>Haghighi, A., Liang, P., Berg-Krikpatrick, T., and Klein, D. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of The ACL 2008, June 15 -20 2008, Columbus, Ohio</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: a parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="1536" citStr="Koehn, 2005" startWordPosition="219" endWordPosition="220">guage processing tasks. The compilation of such bilingual lexicons remains as a substantial issue to linguistic fields. In general practice, many linguists and translators spend huge amounts of money and effort to compile this type of knowledge resources either manually, semiautomatically or automatically. Thus, obtaining the data is expensive. In this paper, we demonstrate a technique that utilizes contextually relevant terms that co-occur with cognate pairs to expand an initial bilingual lexicon. We use unannotated resources that are freely available such as English-Spanish Europarl corpus (Koehn, 2005) and another different set of cognate pairs as seed words. We show that this technique is able to achieve high precision score for bilingual lexicon extracted 10 from non-parallel but comparable corpora. Our model using this technique with spelling similarity approach obtains 85.4 percent precision at 50.0 percent recall. Precision of 79.0 percent at 50.0 percent recall is recorded when using this technique with context similarity approach. Furthermore, by using a string edit-distance vs. precision curve, we also reveal that the latter model is able to capture words efficiently compared to a b</context>
<context position="11614" citStr="Koehn, 2005" startWordPosition="1902" endWordPosition="1903"> • List of cognate pairs We obtained 79 identical cognate pairs from the Figure 4: Utilizing technique with context similarity W3 or Wt respectively. For example, word participation and education occurring in the seed lexicon are contextually relevant terms for source word society. Thus, they become elements of the context vector. Then, we transform the context vectors, from real value into binary, before we compute the similarity with cosine measure. 4 Experimental Setup 4.1 Data For source and target monolingual corpus, we derive English and Spanish sentences from parallel Europarl corpora (Koehn, 2005). • We split each of them into three parts; year ∗from website http://www.wordreference.com 13 top 2000 high frequency lists of our S and T but we chose 55 of these that have at least 100 contextually relevant terms that are highly associated with each of them. • Seed lexicon We also take a set of cognate pairs to be our seed lexicon. We defined the size of a small seed lexicon ranges between 100 to 1k word pairs. Hence, our seed lexicon containing 700 cognate pairs are still considered as a smallsized seed lexicon. However, instead of acquiring this set of cognate pairs automatically, we comp</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, P. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>Knight</author>
</authors>
<title>Knowledge sources for word-level translation models.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Koehn, Knight, 2001</marker>
<rawString>Koehn, P., and Knight , K. 2001. Knowledge sources for word-level translation models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>9--16</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="2460" citStr="Koehn and Knight (2002)" startWordPosition="365" endWordPosition="368">t 50.0 percent recall. Precision of 79.0 percent at 50.0 percent recall is recorded when using this technique with context similarity approach. Furthermore, by using a string edit-distance vs. precision curve, we also reveal that the latter model is able to capture words efficiently compared to a baseline model. Section 2 is dedicated to mention some of the related works. In Section 3, the technique that we used is explained. Section 4 describes our experimental setup followed by the evaluation results in Section 5. Discussion and conclusion are in Section 6 and 7 respectively. 2 Related Work Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. In reporting our work, we treat both identical word pairs and similar spelling word pairs as cognate pairs. Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. They propose to restrict the word length, at least of length 6, to increase the accuracy of the collected word pairs. Koehn and Knight (2002) mentio</context>
<context position="5127" citStr="Koehn and Knight (2002)" startWordPosition="803" endWordPosition="806">the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors. This idea is similar to Rapp (1999) who proposed to transform all co-occurrence vectors using log likelihood ratio instead of just using the frequency counts of the co-occurrences. These values are used to define whether the context words are highly associated with the W or not. Earlier work relies on a large bilingual dictionary as their seed lexicon (Rapp, 1999; Fung and Yee, 1998; among others). Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. Haghighi et al. (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. They obtain 89.0 percent precision at 33.0 percent recall for their EnglishSpanish induction with best feature set, using topically similar but non-parallel corpora. 3 The Utilizing Technique Most works in bilingual le</context>
<context position="12451" citStr="Koehn and Knight, 2002" startWordPosition="2049" endWordPosition="2052"> that are highly associated with each of them. • Seed lexicon We also take a set of cognate pairs to be our seed lexicon. We defined the size of a small seed lexicon ranges between 100 to 1k word pairs. Hence, our seed lexicon containing 700 cognate pairs are still considered as a smallsized seed lexicon. However, instead of acquiring this set of cognate pairs automatically, we compiled the cognate pairs from a few Learning Spanish Cognates websites †. This approach is a simple alternative to replace the 10-20k general dictionaries (Rapp, 1999; Fung and McKeown, 2004) or automatic seed words (Koehn and Knight, 2002; Haghighi et al., 2008). However, this approach can only be used if the source and target language are fairly related and both share lexically similar words that most likely have same meaning. Otherwise, we have to rely on general bilingual dictionaries. • Stop list Previously (Rapp, 1999; Koehn and Knight, 2002; among others) suggested filtering out commonly occurring words that do not help in processing natural language data. This idea sometimes seem as a negative approach to the natural articles of language, however various studies have proven that it is sensible to do so. • Baseline syste</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Koehn, P., and Knight, K. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of ACL 2002, July 2002, Philadelphia, USA, pp. 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Identifying word translations in nonparallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL 33,</booktitle>
<pages>320--322</pages>
<marker>Rapp, 1995</marker>
<rawString>Rapp, R. 1995. Identifying word translations in nonparallel texts. In Proceedings of ACL 33, pages 320-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL 37,</booktitle>
<pages>519--526</pages>
<contexts>
<context position="4737" citStr="Rapp (1999)" startWordPosition="740" endWordPosition="741">t word W. A context vector for each W is then constructed, with only context words found in the seed lexicon. The context vectors are then translated into the target language before their similarity is measured. Fung and Yee (1998) point out that not only the number of common words in context gives some similarity clue to a word and its translation, but the actual ranking of the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors. This idea is similar to Rapp (1999) who proposed to transform all co-occurrence vectors using log likelihood ratio instead of just using the frequency counts of the co-occurrences. These values are used to define whether the context words are highly associated with the W or not. Earlier work relies on a large bilingual dictionary as their seed lexicon (Rapp, 1999; Fung and Yee, 1998; among others). Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. Haghighi et al. (2008), amongst a few others, prop</context>
<context position="12378" citStr="Rapp, 1999" startWordPosition="2039" endWordPosition="2040">5 of these that have at least 100 contextually relevant terms that are highly associated with each of them. • Seed lexicon We also take a set of cognate pairs to be our seed lexicon. We defined the size of a small seed lexicon ranges between 100 to 1k word pairs. Hence, our seed lexicon containing 700 cognate pairs are still considered as a smallsized seed lexicon. However, instead of acquiring this set of cognate pairs automatically, we compiled the cognate pairs from a few Learning Spanish Cognates websites †. This approach is a simple alternative to replace the 10-20k general dictionaries (Rapp, 1999; Fung and McKeown, 2004) or automatic seed words (Koehn and Knight, 2002; Haghighi et al., 2008). However, this approach can only be used if the source and target language are fairly related and both share lexically similar words that most likely have same meaning. Otherwise, we have to rely on general bilingual dictionaries. • Stop list Previously (Rapp, 1999; Koehn and Knight, 2002; among others) suggested filtering out commonly occurring words that do not help in processing natural language data. This idea sometimes seem as a negative approach to the natural articles of language, however v</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Rapp, R. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of ACL 37, pages 519-526.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>