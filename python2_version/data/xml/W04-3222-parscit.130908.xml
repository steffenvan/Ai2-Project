<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9971045">
The Leaf Projection Path View of Parse Trees: Exploring String Kernels for
HPSG Parse Selection
</title>
<author confidence="0.945063">
Kristina Toutanova
</author>
<affiliation confidence="0.85223">
CS Dept, Stanford University
</affiliation>
<address confidence="0.730861333333333">
353 Serra Mall
Stanford 94305, CA
USA,
</address>
<email confidence="0.998842">
kristina@cs.stanford.edu
</email>
<author confidence="0.43554">
Penka Markova
</author>
<affiliation confidence="0.34458">
EE Dept, Stanford University
</affiliation>
<address confidence="0.577243">
350 Serra Mall
Stanford 94305, CA,
USA,
</address>
<email confidence="0.999111">
penka@cs.stanford.edu
</email>
<author confidence="0.900491">
Christopher Manning
</author>
<affiliation confidence="0.823144">
CS Dept, Stanford University
</affiliation>
<address confidence="0.753189666666667">
353 Serra Mall
Stanford 94305, CA,
USA,
</address>
<email confidence="0.999355">
manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989">
We present a novel representation of parse trees as
lists of paths (leafprojection paths) from leaves to
the top level of the tree. This representation allows
us to achieve significantly higher accuracy in the
task of HPSG parse selection than standard models,
and makes the application of string kernels natural.
We define tree kernels via string kernels on projec-
tion paths and explore their performance in the con-
text of parse disambiguation. We apply SVM rank-
ing models and achieve an exact sentence accuracy
of 85.40% on the Redwoods corpus.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938197183099">
In this work we are concerned with building sta-
tistical models for parse disambiguation – choos-
ing a correct analysis out of the possible analyses
for a sentence. Many machine learning algorithms
for classification and ranking require data to be rep-
resented as real-valued vectors of fixed dimension-
ality. Natural language parse trees are not readily
representable in this form, and the choice of repre-
sentation is extremely important for the success of
machine learning algorithms.
For a large class of machine learning algorithms,
such an explicit representation is not necessary, and
it suffices to devise a kernel function which
measures the similarity between inputs and . In
addition to achieving efficient computation in high
dimensional representation spaces, the use of ker-
nels allows for an alternative view on the mod-
elling problem as defining a similarity between in-
puts rather than a set of relevant features.
In previous work on discriminative natural lan-
guage parsing, one approach has been to define fea-
tures centered around lexicalized local rules in the
trees (Collins, 2000; Shen and Joshi, 2003), simi-
lar to the features of the best performing lexicalized
generative parsing models (Charniak, 2000; Collins,
1997). Additionally non-local features have been
defined measuring e.g. parallelism and complexity
of phrases in discriminative log-linear parse ranking
models (Riezler et al., 2000).
Another approach has been to define tree kernels:
for example, in (Collins and Duffy, 2001), the all-
subtrees representation of parse trees (Bod, 1998)
is effectively utilized by the application of a fast
dynamic programming algorithm for computing the
number of common subtrees of two trees. Another
tree kernel, more broadly applicable to Hierarchi-
cal Directed Graphs, was proposed in (Suzuki et al.,
2003). Many other interesting kernels have been de-
vised for sequences and trees, with application to se-
quence classification and parsing. A good overview
of kernels for structured data can be found in (Gaert-
ner et al., 2002).
Here we propose a new representation of parse
trees which (i) allows the localization of broader
useful context, (ii) paves the way for exploring ker-
nels, and (iii) achieves superior disambiguation ac-
curacy compared to models that use tree representa-
tions centered around context-free rules.
Compared to the usual notion of discriminative
models (placing classes on rich observed data) dis-
criminative PCFG parsing with plain context free
rule features may look naive, since most of the fea-
tures (in a particular tree) make no reference to ob-
served input at all. The standard way to address this
problem is through lexicalization, which puts an el-
ement of the input on each tree node, so all features
do refer to the input. This paper explores an alterna-
tive way of achieving this that gives a broader view
of tree contexts, extends naturally to exploring ker-
nels, and performs better.
We represent parse trees as lists of paths (leafpro-
jection paths) from words to the top level of the tree,
which includes both the head-path (where the word
is a syntactic head) and the non-head path. This al-
lows us to capture for example cases of non-head
dependencies which were also discussed by (Bod,
1998) and were used to motivate large subtree fea-
tures, such as “more careful than his sister” where
“careful” is analyzed as head of the adjective phrase,
but “more” licenses the “than” comparative clause.
This representation of trees as lists of projection
</bodyText>
<figure confidence="0.991242344827586">
HCOMPverb HCOMP verb
PLAN ON V2
plan
HCOMP prep*
ON
THAT DEIX
IMPER verb
HCOMP verb
HCOMP verb
LET V1
let (v sorb)
IMPER verb
HCOMP verb
HCOMP verb
PLAN ON V2
plan (v e p itrs)
IMPER verb
HCOMP verb
HCOMP verb
HCOMP prep*
ON
on (p reg)
LET V1
let
US
us
IMPER verb
HCOMPverb
on that
</figure>
<figureCaption confidence="0.9738165">
Figure 1: Derivation tree for the sentence Let us
plan on that.
</figureCaption>
<bodyText confidence="0.996195384615385">
paths (strings) allows us to explore string kernels on
these paths and combine them into tree kernels.
We apply these ideas in the context of parse
disambiguation for sentence analyses produced by
a Head-driven Phrase Structure Grammar (HPSG),
the grammar formalism underlying the Redwoods
corpus (Oepen et al., 2002). HPSG is a modern
constraint-based lexicalist (or “unification”) gram-
mar formalism.1 We build discriminative mod-
els using Support Vector Machines for ranking
(Joachims, 1999). We compare our proposed rep-
resentation to previous approaches and show that it
leads to substantial improvements in accuracy.
</bodyText>
<sectionHeader confidence="0.975887" genericHeader="method">
2 The Leaf Projection Paths View of Parse
Trees
</sectionHeader>
<subsectionHeader confidence="0.99926">
2.1 Representing HPSG Signs
</subsectionHeader>
<bodyText confidence="0.999963111111111">
In HPSG, sentence analyses are given in the form
of HPSG signs, which are large feature structures
containing information about syntactic and seman-
tic properties of the phrases.
As in some of the previous work on the Red-
woods corpus (Toutanova et al., 2002; Toutanova
and Manning, 2002), we use the derivation trees as
the main representation for disambiguation. Deriva-
tion trees record the combining rule schemas of
the HPSG grammar which were used to license
the sign by combining initial lexical types. The
derivation tree is also the fundamental data stored
in the Redwoods treebank, since the full sign can
be reconstructed from it by reference to the gram-
mar. The internal nodes represent, for example,
head-complement, head-specifier, and head-adjunct
schemas, which were used to license larger signs
out of component parts. A derivation tree for the
</bodyText>
<footnote confidence="0.994644">
1For an introduction to HPSG, see (Pollard and Sag, 1994).
</footnote>
<figureCaption confidence="0.793738666666667">
Figure 2: Paths to top for three leaves. The nodes
in bold are head nodes for the leaf word and the rest
are non-head nodes.
</figureCaption>
<bodyText confidence="0.996437448275862">
sentence Let us plan on that is shown in Figure 1. 2
Additionally, we annotate the nodes of the deriva-
tion trees with information extracted from the HPSG
sign. The annotation of nodes is performed by ex-
tracting values of feature paths from the feature
structure or by propagating information from chil-
dren or parents of a node. In theory with enough
annotation at the nodes of the derivation trees, we
can recover the whole HPSG signs.
Here we describe three node annotations that
proved very useful for disambiguation. One is
annotation with the values of the feature path
synsem.local.cat.head – its values are basic parts
of speech such as noun, verb, prep, adj, adv. An-
other is phrase structure category information asso-
ciated with the nodes, which summarizes the values
of several feature paths and is available in the Red-
woods corpus as Phrase-Structure trees. The third is
annotation with lexical type (le-type), which is the
type of the head word at a node. The preterminals in
Figure 1 are lexical item identifiers — identifiers of
the lexical entries used to construct the parse. The
le-types are about types in the HPSG type hier-
archy and are the direct super-types of the lexical
item identifiers. The le-types are not shown in this
figure, but can be seen at the leaves in Figure 2. For
example, the lexical type of LET V1 in the figure is
v sorb. In Figure 1, the only annotation performed
is with the values of synsem.local.cat.head.
</bodyText>
<subsectionHeader confidence="0.999414">
2.2 The Leaf Projection Paths View
</subsectionHeader>
<bodyText confidence="0.994645634146342">
The projection path of a leaf is the sequence of
nodes from the leaf to the root of the tree. In Figure
2, the leaf projection paths for three of the words
are shown.
We can see that a node in the derivation tree par-
2This sentence has three possible analyses depending on the
attachment of the preposition `on” and whether `on” is an ad-
junct or complement of `plan”.
ticipates in the projection paths of all words domi-
nated by that node. The original local rule config-
urations — a node and its children, do not occur
jointly in the projection paths; thus, if special anno-
tation is not performed to recover it, this informa-
tion is lost.
As seen in Figure 2, and as is always true for a
grammar that produces non-crossing lexical depen-
dencies, there is an initial segment of the projec-
tion path for which the leaf word is a syntactic head
(called head path from here on), and a final segment
for which the word is not a syntactic head (called
non-head path from here on). In HPSG non-local
dependencies are represented in the final semantic
representation, but can not be obtained via syntactic
head annotation.
If, in a traditional parsing model that estimates
the likelihood of a local rule expansion given a node
(such as e.g (Collins, 1997)), the tree nodes are an-
notated with the word of the lexical head, some in-
formation present in the word projection paths can
be recovered. However, this is only the information
in the head path part of the projection path. In fur-
ther experiments we show that the non-head part of
the projection path is very helpful for disambigua-
tion.
Using this representation of derivation trees, we
can apply string kernels to the leaf projection paths
and combine those to obtain kernels on trees. In the
rest of this paper we explore the application of string
kernels to this task, comparing the performance of
the new models to models using more standard rule
features.
</bodyText>
<sectionHeader confidence="0.969327" genericHeader="method">
3 Tree and String Kernels
</sectionHeader>
<subsectionHeader confidence="0.99788">
3.1 Kernels and SVM ranking
</subsectionHeader>
<bodyText confidence="0.999251606060606">
From a machine learning point of view, the parse se-
lection problem can be formulated as follows: given
training examples ( ,
where each is a natural language sentence, is
the number of such sentences, ,is
a parse tree for ,is the number of parses for a
given sentence ,is a feature representation
for the parse tree , and we are given the training
information which of all is the correct parse –
learn how to correctly identify the correct parse of
an unseen test sentence.
One approach for solving this problem is via
representing it as an SVM (Vapnik, 1998) ranking
problem, where (without loss of generality) is
assumed to be the correct parse for . The goal is
to learn a parameter vector , such that the score of
( ) is higher than the scores of all other
parses for the sentence. Thus we optimize for:
The are slack variables used to handle the
non-separable case. The same formulation has been
used in (Collins, 2001) and (Shen and Joshi, 2003).
This problem can be solved by solving the dual,
and thus we would only need inner products of the
feature vectors. This allows for using the kernel
trick, where we replace the inner product in the
representation space by inner product in some fea-
ture space, usually different from the representation
space. The advantage of using a kernel is associ-
ated with the computational effectiveness of com-
puting it (it may not require performing the expen-
sive transformation explicitly).
We learn SVM ranking models using a tree kernel
defined via string kernels on projection paths.
</bodyText>
<subsectionHeader confidence="0.999615">
3.2 Kernels on Trees Based on Kernels on
Projection Paths
</subsectionHeader>
<bodyText confidence="0.989201805555555">
So far we have defined a representation of parse
trees as lists of strings corresponding to projection
paths of words. Now we formalize this representa-
tion and show how string kernels on projection paths
extend to tree kernels.
We introduce the notion of a keyed string — a
string that has a key, which is some letter from the
alphabet of the string. We can denote a keyed
string by a pair , where is the key,
and is the string. In our application, a key would
be a word , and the string would be the sequence
of derivation tree nodes on the head or non-head
part of the projection path of the word . Addi-
tionally, for reducing sparsity, for each keyed string
, we also include a keyed string
, if , and
, otherwise. We use this con-
struction for all string kernels applied in this work.
,
where is the le-type of the word . Thus each
projection path occurs twice in the list representa-
tion of the tree – once headed by the word, and
once by its le-type. In our application, the strings
are sequences of annotated derivation tree nodes,
e.g. =“LET V1:verb HCOMP:verb HCOMP:verb IM-
PER:verb” for the head projection path of let in Fig-
ure 2. The non-head projection path of let is empty.
For a given kernel on strings, we de-
fine its extension to keyed strings as follows:
Given a tree and a
tree , and a kernel
on keyed strings, we define a kernel on the trees
as follows:
, , is defined as follows: Let be
the left-most minimal contiguous substring of that
contains ,, where for indices
</bodyText>
<equation confidence="0.722683666666667">
, .Then
( )=
.
</equation>
<bodyText confidence="0.778907666666667">
For our previous example, if ,
,
, and
</bodyText>
<subsectionHeader confidence="0.584385">
This can be viewed as a convolution (Haussler,
</subsectionHeader>
<bodyText confidence="0.9963575">
1999) and therefore is a valid kernel (positive
definite symmetric), if is a valid kernel.
</bodyText>
<subsectionHeader confidence="0.999616">
3.3 String Kernels
</subsectionHeader>
<bodyText confidence="0.998085876712329">
We experimented with some of the string kernels
proposed in (Lodhi et al., 2000; Leslie and Kuang,
2003), which have been shown to perform very well
for indicating string similarity in other domains. In
particular we applied the N-gram kernel, Subse-
quence kernel, and Wildcard kernel. We refer the
reader to (Lodhi et al., 2000; Leslie and Kuang,
2003) for detailed formal definition of these ker-
nels, and restrict ourselves to an intuitive descrip-
tion here. In addition, we devised a new kernel,
called Repetition kernel, which we describe in de-
tail.
The kernels used here can be defined as the in-
ner product of the feature vectors of the two strings
( , )= x ( ), with feature map from the
space of all finite sequences from a string alpha-
bet to a vector space indexed by a set of sub-
sequences from . As a simple example, the -
gram string kernel maps each string to a
vector with dimensionality and each element in
the vector indicates the number of times the corre-
sponding symbol from occurs in . For example,
.
The Repetition kernel is similar to the 1-gram ker-
nel. It improves on the -gram kernel by better han-
dling cases with repeated occurrences of the same
symbol. Intuitively, in the context of our applica-
tion, this kernel captures the tendency of words to
take (or not take) repeated modifiers of the same
kind. For example, it may be likely that a ceratin
verb take one PP-modifier, but less likely for it to
take two or more.
More specifically, the Repetition kernel is defined
such that its vector space consists of all sequences
from composed of the same symbol. The fea-
ture map obtains matching of substrings of the in-
put string to features, allowing the occurrence of
gaps. There are two discount parameters and .
serves to discount features for the occurrence of
gaps, and discounts longer symbol sequences.
Formally, for an input string , the value of the
feature vector for the feature index sequence
.
The weighted Wildcard kernel performs match-
ing by permitting a restricted number of matches to
a wildcard character. A wildcard kernel has
as feature indices -grams with up to wildcard
characters. Any character matches a wildcard. For
example the 3-gram will match the feature in-
dex in a (3,1) wildcard kernel. The weighting
is based on the number of wildcard characters used
– the weight is multiplied by a discount for each
wildcard.
The Subsequence kernel was defined in (Lodhi
et al., 2000). We used a variation where the ker-
nel is defined by two integers and two dis-
count factors and for gaps and characters. A
subseq(k,g) kernel has as features all -grams with
. The is a restriction on the maximal span
of the -gram in the original string – e.g. if
and , the two letters of a -gram can be at
most letters apart in the original string.
The weight of a feature is multiplied by for each
gap, and by for each non-gap. For the exam-
ple above, if ,
. The feature in-
dex matches only once in the string with a span
at most – for the sequence with gap.
The details of the algorithms for computing the
kernels can be found in the fore-mentioned papers
(Lodhi et al., 2000; Leslie and Kuang, 2003). To
summarize, the kernels can be implemented effi-
ciently using tries.
</bodyText>
<sectionHeader confidence="0.999741" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996157487804878">
In this section we describe our experimental results
using different string kernels and different feature
annotation of parse trees. We learn Support Vector
Machine (SVM) ranking models using the software
package (Joachims, 1999). We also nor-
malized the kernels:
.
For all tree kernels implemented here, we first ex-
tract all features, generating an explicit map to the
space of the kernel, and learn SVM ranking models
using with a linear kernel in that space.
Since the feature maps are not especially expen-
sive for the kernels used here, we chose to solve
the problem in its primal form. We were not aware
of the existence of any fast software packages that
could solve SVM ranking problems in the dual for-
mulation. It is possible to convert the ranking prob-
lem into a classification problem using pairs of trees
as shown in (Shen and Joshi, 2003). We have taken
this approach in more recent work using string ker-
nels requiring very expensive feature maps.
We performed experiments using the version of
the Redwoods corpus which was also used in the
work of (Toutanova et al., 2002; Osborne and Bald-
bridge, 2004) and others. There are anno-
tated sentences in total, of which are ambigu-
ous. The average sentence length of the ambiguous
sentences is words and the average number of
parses per sentence is . We discarded the un-
ambiguous sentences from the training and test sets.
All models were trained and tested using 10-fold
cross-validation. Accuracy results are reported as
percentage of sentences where the correct analysis
was ranked first by the model.
The structure of the experiments section is as fol-
lows. First we describe the results from a controlled
experiment using a limited number of features, and
aimed at comparing models using local rule features
to models using leaf projection paths in Section 4.1.
Next we describe models using more sophisticated
string kernels on projection paths in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.9919405">
4.1 The Leaf Projection Paths View versus the
Context-Free Rule View
</subsectionHeader>
<bodyText confidence="0.999503625">
In order to evaluate the gains from the new repre-
sentation, we describe the features of three similar
models, one using the leaf projection paths, and two
using derivation tree rules. Additionally, we train
a model using only the features from the head-path
parts of the projection paths to illustrate the gain of
using the non-head path. As we will show, a model
using only the head-paths has almost the same fea-
tures as a rule-based tree model.
All models here use derivation tree nodes anno-
tated with only the rule schema name as in Figure
1 and the synsem.local.cat.head value. We will
define these models by their feature map from trees
to vectors. It will be convenient to define the feature
maps for all models by defining the set of features
through templates. The value for a feature
and tree , will be the number of times occurs in
the tree. It is easy to show that the kernels on trees
we introduce in Section 3.2, can be defined via a
feature map that is the sum of the feature maps of
the string kernels on projection paths.
As a concrete example, for each model we show
all features that contain the node [HCOMP:verb]
from Figure 1, which covers the phrase plan on that.
</bodyText>
<subsectionHeader confidence="0.799721">
Bi-gram Model on Projection Paths (2PP)
</subsectionHeader>
<bodyText confidence="0.9946375">
The features of this model use a projection path
representation, where the keys are not the words,
but the le-types of the words. The features of
this model are defined by the following template
: .
is a binary variable showing whether this feature
matches a head or a non-head path, is the
le-type of the path leaf, and is a bi-
gram from the path.
The node [HCOMP:verb] is part of the head-path
for plan, and part of the non-head path for on and
that. The le-types of the words let, plan, on, and that
are, with abbreviations, v sorb, v e p, p reg, and
n deic pro sg respectively. In the following exam-
ples, the node labels are abbreviated as well;
is a special symbol for end of path and is a
special symbol for start of path. Therefore the fea-
tures that contain the node will be:
</bodyText>
<equation confidence="0.7650305">
(v_e_p,[PLAN_ON:verb],[HCOMP:verb],1)
(v_e_p,[HCOMP:verb],EOP,1)
(p_reg,SOP,[HCOMP:verb],0)
(p_reg,[HCOMP:verb],[HCOMP:verb],0)
(n_deic_pro_sg,[HCOMP:prep*],[HCOMP:verb],0)
(n_deic_pro_sg,[HCOMP:verb],[HCOMP:verb],0)
</equation>
<bodyText confidence="0.915588428571429">
Bi-gram Model on only Head Projection Paths
(2HeadPP)
This model has a subset of the features of Model
2PP — only those obtained by the head path parts
of the projection paths. For our example, it contains
the subset of features of 2PP that have last bit ,
which will be only the following:
</bodyText>
<equation confidence="0.8144795">
(v_e_p,[PLAN_ON:verb],[HCOMP:verb],1)
(v_e_p,[HCOMP:verb],EOP,1)
</equation>
<sectionHeader confidence="0.367712" genericHeader="method">
Rule Tree Model I (Rule I)
</sectionHeader>
<bodyText confidence="0.9928985">
The features of this model are defined by the two
templates: and
. The last value in
the tuples is an indication of whether the tuple con-
tains the le-type of the head or the non-head child as
its first element. The features containing the node
[HCOMP:verb] are ones from the expansion at that
node and also from the expansion of its parent:
</bodyText>
<footnote confidence="0.493932">
(v_e_p,[HCOMP:verb],[PLAN_ON:verb],[HCOMP:prep*],1)
(p_reg,[HCOMP:verb],[PLAN_ON:verb],[HCOMP:prep*],0)
(v_sorb,[HCOMP:verb],[HCOMP:verb],[HCOMP:verb],1)
(v_e_p,[HCOMP:verb],[HCOMP:verb],[HCOMP:verb],0)
</footnote>
<table confidence="0.991005">
Model Features Accuracy
2PP 36,623 82.70
2HeadPP 11,490 80.14
Rule I 28,797 80.99
Rule II 16,318 81.07
</table>
<tableCaption confidence="0.9668935">
Table 1: Accuracy of models using the leaf projec-
tion path and rule representations.
</tableCaption>
<note confidence="0.241791">
Rule Tree Model II (Rule II)
</note>
<bodyText confidence="0.996414571428571">
This model splits the features of model Rule I in
two parts, to mimic the features of the projection
path models. It has features from the following tem-
plates: and
.
The features containing the [HCOMP:verb] node
are:
</bodyText>
<equation confidence="0.9782835">
(v_e_p,[HCOMP:verb],[PLAN_ON:verb],1)
(p_reg,[HCOMP:verb],[HCOMP:prep*],0)
(v_sorb,[HCOMP:verb],[HCOMP:verb],1)
(v_e_p,[HCOMP:verb],[HCOMP:verb],0)
</equation>
<bodyText confidence="0.9999655">
This model has less features than model Rule I,
because it splits each rule into its head and non-
head parts and does not have the two parts jointly.
We can note that this model has all the features of
2HeadPP, except the ones involving start and end
of path, due to the first template. The second tem-
plate leads to features that are not even in 2PP be-
cause they connect the head and non-head paths of
a word, which are represented as separate strings in
2PP.
Overall, we can see that models Rule I and Rule
II have the information used by 2HeadPP (and
some more information), but do not have the in-
formation from the non-head parts of the paths in
Model 2PP. Table 1 shows the average parse rank-
ing accuracy obtained by the four models as well as
the number of features used by each model. Model
Rule I did not do better than model Rule II, which
shows that joint representation of rule features was
not very important. The large improvement of 2PP
over 2HeadPP (13% error reduction) shows the
usefulness of the non-head projection paths. The er-
ror reduction of 2PP over Rule I is also large – 9%
error reduction. Further improvements over mod-
els using rule features were possible by considering
more sophisticated string kernels and word keyed
projection paths, as will be shown in the following
sections.
</bodyText>
<subsectionHeader confidence="0.9652505">
4.2 Experimental Results using String Kernels
on Projection Paths
</subsectionHeader>
<bodyText confidence="0.999937666666667">
In the present experiments, we have limited the
derivation tree node annotation to the features listed
in Table 2. Many other features from the HPSG signs
</bodyText>
<table confidence="0.998976333333333">
No. Name Example
0 Node Label HCOMP
1 synsem.local.cat.head verb
2 Label from Phrase Struct Tree S
3 Le Type of Lexical Head v sorb le
4 Lexical Head Word let
</table>
<tableCaption confidence="0.962909">
Table 2: Annotated features of derivation tree
</tableCaption>
<bodyText confidence="0.945331363636364">
nodes. The examples are from one node in the head
path of the word let in Figure 1.
are potentially helpful for disambiguation, and in-
corporating more useful features is a next step for
this work. However, given the size of the corpus,
a single model can not usefully profit from a large
number of features. Previous work (Osborne and
Baldbridge, 2004; Toutanova and Manning, 2002;
Toutanova et al., 2002) has explored combining
multiple classifiers using different features. We re-
port results from such an experiment as well.
</bodyText>
<subsectionHeader confidence="0.7902855">
Using Node Label and Head Category
Annotations
</subsectionHeader>
<bodyText confidence="0.999971">
The simplest derivation tree node representation
that we consider consists of features and -
schema name and category of the lexical head. All
experiments in this subsection section were per-
formed using this derivation tree annotation. We
briefly mention results from the best string-kernels
when using other node annotations, as well as a
combination of models using different features in
the following subsection.
To evaluate the usefulness of our Repetition Ker-
nel, defined in Section 3.3, we performed several
simple experiments. We compared it to a -gram
kernel, and to a -gram kernel. The results – num-
ber of features per model, and accuracy, are shown
in Table 3. The models shown in this table include
both features from projection paths keyed by words
and projection paths keyed by le-types. The results
show that the Repetition kernel achieves a notice-
able improvement over a -gram model ( error
reduction), with the addition of only a small number
of features. For most of the words, repeated sym-
bols will not occur in their paths, and the Repetition
kernel will behave like a -gram for the majority of
cases. The additional information it captures about
repeated symbols gives a sizable improvement. The
bi-gram kernel performs better but at the cost of the
addition of many features. It is likely that for large
alphabets and small training sets, the Repetition ker-
nel may outperform the bi-gram kernel.
From this point on, we will fix the string kernel
for projection paths keyed by words — it will be a
linear combination of a bi-gram kernel and a Rep-
</bodyText>
<table confidence="0.98420625">
Kernel Features Accuracy
-gram 44,278 82.21
Repetition 52,994 83.59
-gram 104,331 84.15
</table>
<tableCaption confidence="0.8423525">
Table 3: Comparison of the Repetition kernel to -
gram and -gram.
</tableCaption>
<bodyText confidence="0.998631531914894">
etition kernel. We found that, because lexical in-
formation is sparse, going beyond -grams for lex-
ically headed paths was not useful. The projection
paths keyed by le-types are much less sparse, but
still capture important sequence information about
the syntactic frames of words of particular lexical
types.
To study the usefulness of different string kernels
on projection paths, we first tested models where
only le-type keyed paths were represented, and then
tested the performance of the better models when
word keyed paths were added (with a fixed string
kernel that interpolates a bi-gram and a Repetition
kernel).
Table 4 shows the accuracy achieved by several
string kernels as well as the number of features (in
thousands) they use. As can be seen from the ta-
ble, the models are very sensitive to the discount
factors used. Many of the kernels that use some
combination of 1-grams and possibly discontinu-
ous bi-grams performed at approximately the same
accuracy level. Such are the wildcard(2,1, ) and
subseq(2, , , ) kernels. Kernels that use -
grams have many more parameters, and even though
they can be marginally better when using le-types
only, their advantage when adding word keyed paths
disappears. A limited amount of discontinuity in
the Subsequence kernels was useful. Overall Sub-
sequence kernels were slightly better than Wild-
card kernels. The major difference between the two
kinds of kernels as we have used them here is that
the Subsequence kernel unifies features that have
gaps in different places, and the Wildcard kernel
does not. For example, are different
features for Wildcard, but they are the same feature
for Subsequence – only the weighting of the fea-
ture depends on the position of the wildcard.
When projection paths keyed by words are
added, the accuracy increases significantly. sub-
seq(2,3,.5,2) achieved an accuracy of ,
which is much higher than the best previously pub-
lished accuracy from a single model on this corpus
( for a model that incorporates more sources
of information from the HPSG signs (Toutanova et
al., 2002)). The error reduction compared to that
model is . It is also higher than the best re-
sult from voting classifiers ( (Osborne and
</bodyText>
<table confidence="0.9988751875">
Model Features Accuracy
le w &amp; le le w &amp; le
1gram 13K - 81.43 -
2gram 37K 141K 82.70 84.11
wildcard (2,1,.7) 62K 167K 83.17 83.86
wildcard (2,1,.25) 62K 167K 82.97 -
wildcard (3,1,.5) 187K 291K 83.21 83.59
wildcard (3,2,.5) 220K 82.90 -
subseq (2,3,.5,2) 81K 185K 83.22 84.96
subseq (2,3,.25,2) 81K 185K 83.48 84.75
subseq (2,3,.25,1) 81K 185K 82.89 -
subseq (2,4,.5,2) 102K 206K 83.29 84.40
subseq (3,3,.5,2) 154K 259K 83.17 83.85
subseq (3,4,.25,2) 290K - 83.06 -
subseq (3,5,.25,2) 416K - 83.06 -
combination model 85.40
</table>
<tableCaption confidence="0.966771333333333">
Table 4: Accuracy of models using projection paths
keyed by le-type or both word and le-type. Numbers
offeatures are shown in thousands.
</tableCaption>
<bodyText confidence="0.959827545454546">
Baldbridge, 2004)).
Other Features and Model Combination
Finally, we trained several models using different
derivation tree annotations and built a model that
combined the scores from these models together
with the best model subseq(2,3,.5,2) from Table
4. The combined model achieved our best accuracy
of . The models combined were:
Model I A model that uses the Node Label and le-
type of non-head daughter for head projection paths,
and Node Label and sysnem.local.cat.head for
non-head projection paths. The model used the sub-
seq(2,3,.5,2) kernel for le-type keyed paths and bi-
gram + Repetition for word keyed paths as above.
Number of features of this model: 237K Accuracy:
.
Model II A model that uses, for head paths,
Node Label of node and Node Label and sys-
nem.local.cat.head of non-head daughter, and for
non-head paths PS category of node. The model
uses the same kernels as Model I. Number of fea-
tures: 311K. Accuracy: .
Model III This model uses PS label and sys-
nem.local.cat.head for head paths, and only PS
label for non-head paths. The kernels are the same
as Model I. Number of features: 165K Accuracy:
.
Model IV This is a standard model based on
rule features for local trees, with levels of grand-
parent annotation and back-off. The annotation
used at nodes was with Node Label and sys-
nem.local.cat.head. Number of features: 78K Ac-
curacy: .
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999997588235294">
We proposed a new representation of parse trees that
allows us to connect more tightly tree structures to
the words of the sentence. Additionally this repre-
sentation allows for the natural extension of string
kernels to kernels on trees. The major source of ac-
curacy improvement for our models was this rep-
resentation, as even with bi-gram features, the per-
formance was higher than previously achieved. We
were able to improve on these results by using more
sophisticated Subsequence kernels and by our Rep-
etition kernel which captures some salient proper-
ties of word projection paths.
In future work, we aim to explore the definition
of new string kernels that are more suitable for this
particular application and apply these ideas to Penn
Treebank parse trees. We also plan to explore anno-
tation with more features from HPSG signs.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998012">
We would like to thank the anonymous review-
ers for helpful comments. This work was car-
ried out under the Edinburgh-Stanford Link pro-
gramme, funded by Scottish Enterprise, ROSIE
project R36763.
</bodyText>
<sectionHeader confidence="0.999079" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999648633802817">
Rens Bod. 1998. Beyond Grammar: An Experience
Based Theory ofLanguage. CSLI Publications.
Eugene Charniak. 2000. A maximum entropy in-
spired parser. In Proceedings of NAACL, pages
132–139.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proceedings
ofNIPS.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Proceed-
ings of the ACL, pages 16 – 23.
Michael Collins. 2000. Discriminative reranking
for natural language parsing. In Proceedings of
ICML, pages 175–182.
Michael Collins. 2001. Parameter estimation for
statistical parsing models: Theory and practice of
distribution-free methods. In IWPT. Paper writ-
ten to accompany invited talk at IWPT 2001.
Thomas Gaertner, John W. Lloyd, and Peter A.
Flach. 2002. Kernels for structured data. In
ILP02, pages 66–83.
David Haussler. 1999. Convolution kernels on dis-
crete structures. In UC Santa Cruz Technical Re-
port UCS-CRL-99-10.
Thorsten Joachims. 1999. Making large-scale
SVM learning practical. In B. Scholkopf,
C. Burges, and A. Smola, editors, Advances in
Kernel Methods - Support Vector Learning.
Christina Leslie and Rui Kuang. 2003. Fast ker-
nels for inexact string matching. In COLT 2003,
pages 114–128.
Huma Lodhi, John Shawe-Taylor, Nello Cristianini,
and Christopher J. C. H. Watkins. 2000. Text
classification using string kernels. In Proceed-
ings of NIPS, pages 563–569.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Chris Manning, and Dan Flickinger. 2002. The
LinGo Redwoods treebank: Motivation and pre-
liminary apllications. In Proceedings of COLING
19, pages 1253—1257.
Miles Osborne and Jason Baldbridge. 2004.
Ensemble-based active learning for parse selec-
tion. In Proceedings ofHLT-NAACL.
Carl Pollard and Ivan A. Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and
Mark Johnson. 2000. Lexicalized stochastic
modeling of constraint-based grammars using
log-linear measures and EM training. In Pro-
ceedings of the ACL, pages 480—487.
Libin Shen and Aravind K. Joshi. 2003. An SVM-
based voting algorithm with application to parse
reranking. In Proceedings of CoNLL, pages 9–
16.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical directed
acyclic graph kernel: Methods for structured nat-
ural language data. In Proceedings of the ACL,
pages 32–39.
Kristina Toutanova and Christopher D. Manning.
2002. Feature selection for a rich HPSG gram-
mar using decision trees. In Proceedings of
CoNLL.
Kristina Toutanova, Christopher D. Manning, Stu-
art Shieber, Dan Flickinger, and Stephan Oepen.
2002. Parse disambiguation for a rich HPSG
grammar. In Proceedings of Treebanks and Lin-
guistic Theories, pages 253–263.
Vladimir Vapnik. 1998. Statistical Learning The-
ory. Wiley, New York.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.059672">
<title confidence="0.999764">The Leaf Projection Path View of Parse Trees: Exploring String Kernels for HPSG Parse Selection</title>
<author confidence="0.975864">Kristina</author>
<affiliation confidence="0.868785">CS Dept, Stanford</affiliation>
<address confidence="0.7939045">353 Serra Stanford 94305,</address>
<email confidence="0.999188">kristina@cs.stanford.edu</email>
<author confidence="0.780733">Penka</author>
<affiliation confidence="0.617355">EE Dept, Stanford</affiliation>
<address confidence="0.76968">350 Serra Stanford 94305,</address>
<email confidence="0.998686">penka@cs.stanford.edu</email>
<author confidence="0.979268">Christopher</author>
<affiliation confidence="0.935281">CS Dept, Stanford</affiliation>
<address confidence="0.990602">353 Serra</address>
<author confidence="0.463665">Stanford</author>
<email confidence="0.998237">manning@cs.stanford.edu</email>
<abstract confidence="0.990672666666666">We present a novel representation of parse trees as of paths from leaves to the top level of the tree. This representation allows us to achieve significantly higher accuracy in the of selection than standard models, and makes the application of string kernels natural. We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation. We apply SVM ranking models and achieve an exact sentence accuracy of 85.40% on the Redwoods corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond Grammar: An Experience Based Theory ofLanguage.</title>
<date>1998</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="2559" citStr="Bod, 1998" startWordPosition="390" endWordPosition="391">ative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002). Here we propose a new representation of parse trees which (i) allows the localization of broader useful context, (ii) pave</context>
<context position="4232" citStr="Bod, 1998" startWordPosition="670" endWordPosition="671">ress this problem is through lexicalization, which puts an element of the input on each tree node, so all features do refer to the input. This paper explores an alternative way of achieving this that gives a broader view of tree contexts, extends naturally to exploring kernels, and performs better. We represent parse trees as lists of paths (leafprojection paths) from words to the top level of the tree, which includes both the head-path (where the word is a syntactic head) and the non-head path. This allows us to capture for example cases of non-head dependencies which were also discussed by (Bod, 1998) and were used to motivate large subtree features, such as “more careful than his sister” where “careful” is analyzed as head of the adjective phrase, but “more” licenses the “than” comparative clause. This representation of trees as lists of projection HCOMPverb HCOMP verb PLAN ON V2 plan HCOMP prep* ON THAT DEIX IMPER verb HCOMP verb HCOMP verb LET V1 let (v sorb) IMPER verb HCOMP verb HCOMP verb PLAN ON V2 plan (v e p itrs) IMPER verb HCOMP verb HCOMP verb HCOMP prep* ON on (p reg) LET V1 let US us IMPER verb HCOMPverb on that Figure 1: Derivation tree for the sentence Let us plan on that. </context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>Rens Bod. 1998. Beyond Grammar: An Experience Based Theory ofLanguage. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2214" citStr="Charniak, 2000" startWordPosition="341" endWordPosition="342">evise a kernel function which measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 200</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy inspired parser. In Proceedings of NAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="2500" citStr="Collins and Duffy, 2001" startWordPosition="379" endWordPosition="382">uts rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002). Here we propose a new representation of parse trees which (i) a</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2230" citStr="Collins, 1997" startWordPosition="343" endWordPosition="344">unction which measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other i</context>
<context position="9296" citStr="Collins, 1997" startWordPosition="1542" endWordPosition="1543">s lost. As seen in Figure 2, and as is always true for a grammar that produces non-crossing lexical dependencies, there is an initial segment of the projection path for which the leaf word is a syntactic head (called head path from here on), and a final segment for which the word is not a syntactic head (called non-head path from here on). In HPSG non-local dependencies are represented in the final semantic representation, but can not be obtained via syntactic head annotation. If, in a traditional parsing model that estimates the likelihood of a local rule expansion given a node (such as e.g (Collins, 1997)), the tree nodes are annotated with the word of the lexical head, some information present in the word projection paths can be recovered. However, this is only the information in the head path part of the projection path. In further experiments we show that the non-head part of the projection path is very helpful for disambiguation. Using this representation of derivation trees, we can apply string kernels to the leaf projection paths and combine those to obtain kernels on trees. In the rest of this paper we explore the application of string kernels to this task, comparing the performance of </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the ACL, pages 16 – 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="2089" citStr="Collins, 2000" startWordPosition="322" endWordPosition="323">ms. For a large class of machine learning algorithms, such an explicit representation is not necessary, and it suffices to devise a kernel function which measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2001</date>
<booktitle>In IWPT. Paper written</booktitle>
<contexts>
<context position="10934" citStr="Collins, 2001" startWordPosition="1834" endWordPosition="1835"> tree , and we are given the training information which of all is the correct parse – learn how to correctly identify the correct parse of an unseen test sentence. One approach for solving this problem is via representing it as an SVM (Vapnik, 1998) ranking problem, where (without loss of generality) is assumed to be the correct parse for . The goal is to learn a parameter vector , such that the score of ( ) is higher than the scores of all other parses for the sentence. Thus we optimize for: The are slack variables used to handle the non-separable case. The same formulation has been used in (Collins, 2001) and (Shen and Joshi, 2003). This problem can be solved by solving the dual, and thus we would only need inner products of the feature vectors. This allows for using the kernel trick, where we replace the inner product in the representation space by inner product in some feature space, usually different from the representation space. The advantage of using a kernel is associated with the computational effectiveness of computing it (it may not require performing the expensive transformation explicitly). We learn SVM ranking models using a tree kernel defined via string kernels on projection pat</context>
</contexts>
<marker>Collins, 2001</marker>
<rawString>Michael Collins. 2001. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In IWPT. Paper written to accompany invited talk at IWPT 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Gaertner</author>
<author>John W Lloyd</author>
<author>Peter A Flach</author>
</authors>
<title>Kernels for structured data. In</title>
<date>2002</date>
<booktitle>ILP02,</booktitle>
<pages>66--83</pages>
<contexts>
<context position="3035" citStr="Gaertner et al., 2002" startWordPosition="464" endWordPosition="468">her approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002). Here we propose a new representation of parse trees which (i) allows the localization of broader useful context, (ii) paves the way for exploring kernels, and (iii) achieves superior disambiguation accuracy compared to models that use tree representations centered around context-free rules. Compared to the usual notion of discriminative models (placing classes on rich observed data) discriminative PCFG parsing with plain context free rule features may look naive, since most of the features (in a particular tree) make no reference to observed input at all. The standard way to address this pro</context>
</contexts>
<marker>Gaertner, Lloyd, Flach, 2002</marker>
<rawString>Thomas Gaertner, John W. Lloyd, and Peter A. Flach. 2002. Kernels for structured data. In ILP02, pages 66–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<booktitle>In UC Santa Cruz</booktitle>
<tech>Technical Report UCS-CRL-99-10.</tech>
<contexts>
<context position="13186" citStr="Haussler, 1999" startWordPosition="2248" endWordPosition="2249">he strings are sequences of annotated derivation tree nodes, e.g. =“LET V1:verb HCOMP:verb HCOMP:verb IMPER:verb” for the head projection path of let in Figure 2. The non-head projection path of let is empty. For a given kernel on strings, we define its extension to keyed strings as follows: Given a tree and a tree , and a kernel on keyed strings, we define a kernel on the trees as follows: , , is defined as follows: Let be the left-most minimal contiguous substring of that contains ,, where for indices , .Then ( )= . For our previous example, if , , , and This can be viewed as a convolution (Haussler, 1999) and therefore is a valid kernel (positive definite symmetric), if is a valid kernel. 3.3 String Kernels We experimented with some of the string kernels proposed in (Lodhi et al., 2000; Leslie and Kuang, 2003), which have been shown to perform very well for indicating string similarity in other domains. In particular we applied the N-gram kernel, Subsequence kernel, and Wildcard kernel. We refer the reader to (Lodhi et al., 2000; Leslie and Kuang, 2003) for detailed formal definition of these kernels, and restrict ourselves to an intuitive description here. In addition, we devised a new kernel</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. In UC Santa Cruz Technical Report UCS-CRL-99-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Scholkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="5324" citStr="Joachims, 1999" startWordPosition="855" endWordPosition="856">N on (p reg) LET V1 let US us IMPER verb HCOMPverb on that Figure 1: Derivation tree for the sentence Let us plan on that. paths (strings) allows us to explore string kernels on these paths and combine them into tree kernels. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al., 2002). HPSG is a modern constraint-based lexicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redwoods corpus (Toutanova et al., 2002; Toutanova and Manning, 2002), we use the derivation trees as the main representation for disambiguation. Derivation trees record the combi</context>
<context position="16723" citStr="Joachims, 1999" startWordPosition="2875" endWordPosition="2876">ap, and by for each non-gap. For the example above, if , . The feature index matches only once in the string with a span at most – for the sequence with gap. The details of the algorithms for computing the kernels can be found in the fore-mentioned papers (Lodhi et al., 2000; Leslie and Kuang, 2003). To summarize, the kernels can be implemented efficiently using tries. 4 Experiments In this section we describe our experimental results using different string kernels and different feature annotation of parse trees. We learn Support Vector Machine (SVM) ranking models using the software package (Joachims, 1999). We also normalized the kernels: . For all tree kernels implemented here, we first extract all features, generating an explicit map to the space of the kernel, and learn SVM ranking models using with a linear kernel in that space. Since the feature maps are not especially expensive for the kernels used here, we chose to solve the problem in its primal form. We were not aware of the existence of any fast software packages that could solve SVM ranking problems in the dual formulation. It is possible to convert the ranking problem into a classification problem using pairs of trees as shown in (S</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Scholkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Leslie</author>
<author>Rui Kuang</author>
</authors>
<title>Fast kernels for inexact string matching.</title>
<date>2003</date>
<booktitle>In COLT</booktitle>
<pages>114--128</pages>
<contexts>
<context position="13395" citStr="Leslie and Kuang, 2003" startWordPosition="2281" endWordPosition="2284"> empty. For a given kernel on strings, we define its extension to keyed strings as follows: Given a tree and a tree , and a kernel on keyed strings, we define a kernel on the trees as follows: , , is defined as follows: Let be the left-most minimal contiguous substring of that contains ,, where for indices , .Then ( )= . For our previous example, if , , , and This can be viewed as a convolution (Haussler, 1999) and therefore is a valid kernel (positive definite symmetric), if is a valid kernel. 3.3 String Kernels We experimented with some of the string kernels proposed in (Lodhi et al., 2000; Leslie and Kuang, 2003), which have been shown to perform very well for indicating string similarity in other domains. In particular we applied the N-gram kernel, Subsequence kernel, and Wildcard kernel. We refer the reader to (Lodhi et al., 2000; Leslie and Kuang, 2003) for detailed formal definition of these kernels, and restrict ourselves to an intuitive description here. In addition, we devised a new kernel, called Repetition kernel, which we describe in detail. The kernels used here can be defined as the inner product of the feature vectors of the two strings ( , )= x ( ), with feature map from the space of all</context>
<context position="16408" citStr="Leslie and Kuang, 2003" startWordPosition="2827" endWordPosition="2830">scount factors and for gaps and characters. A subseq(k,g) kernel has as features all -grams with . The is a restriction on the maximal span of the -gram in the original string – e.g. if and , the two letters of a -gram can be at most letters apart in the original string. The weight of a feature is multiplied by for each gap, and by for each non-gap. For the example above, if , . The feature index matches only once in the string with a span at most – for the sequence with gap. The details of the algorithms for computing the kernels can be found in the fore-mentioned papers (Lodhi et al., 2000; Leslie and Kuang, 2003). To summarize, the kernels can be implemented efficiently using tries. 4 Experiments In this section we describe our experimental results using different string kernels and different feature annotation of parse trees. We learn Support Vector Machine (SVM) ranking models using the software package (Joachims, 1999). We also normalized the kernels: . For all tree kernels implemented here, we first extract all features, generating an explicit map to the space of the kernel, and learn SVM ranking models using with a linear kernel in that space. Since the feature maps are not especially expensive f</context>
</contexts>
<marker>Leslie, Kuang, 2003</marker>
<rawString>Christina Leslie and Rui Kuang. 2003. Fast kernels for inexact string matching. In COLT 2003, pages 114–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Christopher J C H Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2000</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>563--569</pages>
<contexts>
<context position="13370" citStr="Lodhi et al., 2000" startWordPosition="2277" endWordPosition="2280">ction path of let is empty. For a given kernel on strings, we define its extension to keyed strings as follows: Given a tree and a tree , and a kernel on keyed strings, we define a kernel on the trees as follows: , , is defined as follows: Let be the left-most minimal contiguous substring of that contains ,, where for indices , .Then ( )= . For our previous example, if , , , and This can be viewed as a convolution (Haussler, 1999) and therefore is a valid kernel (positive definite symmetric), if is a valid kernel. 3.3 String Kernels We experimented with some of the string kernels proposed in (Lodhi et al., 2000; Leslie and Kuang, 2003), which have been shown to perform very well for indicating string similarity in other domains. In particular we applied the N-gram kernel, Subsequence kernel, and Wildcard kernel. We refer the reader to (Lodhi et al., 2000; Leslie and Kuang, 2003) for detailed formal definition of these kernels, and restrict ourselves to an intuitive description here. In addition, we devised a new kernel, called Repetition kernel, which we describe in detail. The kernels used here can be defined as the inner product of the feature vectors of the two strings ( , )= x ( ), with feature </context>
<context position="15709" citStr="Lodhi et al., 2000" startWordPosition="2689" endWordPosition="2692">ts longer symbol sequences. Formally, for an input string , the value of the feature vector for the feature index sequence . The weighted Wildcard kernel performs matching by permitting a restricted number of matches to a wildcard character. A wildcard kernel has as feature indices -grams with up to wildcard characters. Any character matches a wildcard. For example the 3-gram will match the feature index in a (3,1) wildcard kernel. The weighting is based on the number of wildcard characters used – the weight is multiplied by a discount for each wildcard. The Subsequence kernel was defined in (Lodhi et al., 2000). We used a variation where the kernel is defined by two integers and two discount factors and for gaps and characters. A subseq(k,g) kernel has as features all -grams with . The is a restriction on the maximal span of the -gram in the original string – e.g. if and , the two letters of a -gram can be at most letters apart in the original string. The weight of a feature is multiplied by for each gap, and by for each non-gap. For the example above, if , . The feature index matches only once in the string with a span at most – for the sequence with gap. The details of the algorithms for computing</context>
</contexts>
<marker>Lodhi, Shawe-Taylor, Cristianini, Watkins, 2000</marker>
<rawString>Huma Lodhi, John Shawe-Taylor, Nello Cristianini, and Christopher J. C. H. Watkins. 2000. Text classification using string kernels. In Proceedings of NIPS, pages 563–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Chris Manning</author>
<author>Dan Flickinger</author>
</authors>
<title>The LinGo Redwoods treebank: Motivation and preliminary apllications.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING 19,</booktitle>
<pages>1253--1257</pages>
<contexts>
<context position="5149" citStr="Oepen et al., 2002" startWordPosition="829" endWordPosition="832">rep* ON THAT DEIX IMPER verb HCOMP verb HCOMP verb LET V1 let (v sorb) IMPER verb HCOMP verb HCOMP verb PLAN ON V2 plan (v e p itrs) IMPER verb HCOMP verb HCOMP verb HCOMP prep* ON on (p reg) LET V1 let US us IMPER verb HCOMPverb on that Figure 1: Derivation tree for the sentence Let us plan on that. paths (strings) allows us to explore string kernels on these paths and combine them into tree kernels. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al., 2002). HPSG is a modern constraint-based lexicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redw</context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Chris Manning, and Dan Flickinger. 2002. The LinGo Redwoods treebank: Motivation and preliminary apllications. In Proceedings of COLING 19, pages 1253—1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Jason Baldbridge</author>
</authors>
<title>Ensemble-based active learning for parse selection.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="17608" citStr="Osborne and Baldbridge, 2004" startWordPosition="3030" endWordPosition="3034">re not especially expensive for the kernels used here, we chose to solve the problem in its primal form. We were not aware of the existence of any fast software packages that could solve SVM ranking problems in the dual formulation. It is possible to convert the ranking problem into a classification problem using pairs of trees as shown in (Shen and Joshi, 2003). We have taken this approach in more recent work using string kernels requiring very expensive feature maps. We performed experiments using the version of the Redwoods corpus which was also used in the work of (Toutanova et al., 2002; Osborne and Baldbridge, 2004) and others. There are annotated sentences in total, of which are ambiguous. The average sentence length of the ambiguous sentences is words and the average number of parses per sentence is . We discarded the unambiguous sentences from the training and test sets. All models were trained and tested using 10-fold cross-validation. Accuracy results are reported as percentage of sentences where the correct analysis was ranked first by the model. The structure of the experiments section is as follows. First we describe the results from a controlled experiment using a limited number of features, and</context>
<context position="24302" citStr="Osborne and Baldbridge, 2004" startWordPosition="4131" endWordPosition="4134">he features listed in Table 2. Many other features from the HPSG signs No. Name Example 0 Node Label HCOMP 1 synsem.local.cat.head verb 2 Label from Phrase Struct Tree S 3 Le Type of Lexical Head v sorb le 4 Lexical Head Word let Table 2: Annotated features of derivation tree nodes. The examples are from one node in the head path of the word let in Figure 1. are potentially helpful for disambiguation, and incorporating more useful features is a next step for this work. However, given the size of the corpus, a single model can not usefully profit from a large number of features. Previous work (Osborne and Baldbridge, 2004; Toutanova and Manning, 2002; Toutanova et al., 2002) has explored combining multiple classifiers using different features. We report results from such an experiment as well. Using Node Label and Head Category Annotations The simplest derivation tree node representation that we consider consists of features and - schema name and category of the lexical head. All experiments in this subsection section were performed using this derivation tree annotation. We briefly mention results from the best string-kernels when using other node annotations, as well as a combination of models using different</context>
</contexts>
<marker>Osborne, Baldbridge, 2004</marker>
<rawString>Miles Osborne and Jason Baldbridge. 2004. Ensemble-based active learning for parse selection. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6443" citStr="Pollard and Sag, 1994" startWordPosition="1033" endWordPosition="1036">e the derivation trees as the main representation for disambiguation. Derivation trees record the combining rule schemas of the HPSG grammar which were used to license the sign by combining initial lexical types. The derivation tree is also the fundamental data stored in the Redwoods treebank, since the full sign can be reconstructed from it by reference to the grammar. The internal nodes represent, for example, head-complement, head-specifier, and head-adjunct schemas, which were used to license larger signs out of component parts. A derivation tree for the 1For an introduction to HPSG, see (Pollard and Sag, 1994). Figure 2: Paths to top for three leaves. The nodes in bold are head nodes for the leaf word and the rest are non-head nodes. sentence Let us plan on that is shown in Figure 1. 2 Additionally, we annotate the nodes of the derivation trees with information extracted from the HPSG sign. The annotation of nodes is performed by extracting values of feature paths from the feature structure or by propagating information from children or parents of a node. In theory with enough annotation at the nodes of the derivation trees, we can recover the whole HPSG signs. Here we describe three node annotatio</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. HeadDriven Phrase Structure Grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>480--487</pages>
<contexts>
<context position="2407" citStr="Riezler et al., 2000" startWordPosition="364" endWordPosition="367">lows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be foun</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proceedings of the ACL, pages 480—487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVMbased voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2112" citStr="Shen and Joshi, 2003" startWordPosition="324" endWordPosition="327"> class of machine learning algorithms, such an explicit representation is not necessary, and it suffices to devise a kernel function which measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tre</context>
<context position="10961" citStr="Shen and Joshi, 2003" startWordPosition="1837" endWordPosition="1840">iven the training information which of all is the correct parse – learn how to correctly identify the correct parse of an unseen test sentence. One approach for solving this problem is via representing it as an SVM (Vapnik, 1998) ranking problem, where (without loss of generality) is assumed to be the correct parse for . The goal is to learn a parameter vector , such that the score of ( ) is higher than the scores of all other parses for the sentence. Thus we optimize for: The are slack variables used to handle the non-separable case. The same formulation has been used in (Collins, 2001) and (Shen and Joshi, 2003). This problem can be solved by solving the dual, and thus we would only need inner products of the feature vectors. This allows for using the kernel trick, where we replace the inner product in the representation space by inner product in some feature space, usually different from the representation space. The advantage of using a kernel is associated with the computational effectiveness of computing it (it may not require performing the expensive transformation explicitly). We learn SVM ranking models using a tree kernel defined via string kernels on projection paths. 3.2 Kernels on Trees Ba</context>
<context position="17343" citStr="Shen and Joshi, 2003" startWordPosition="2986" endWordPosition="2989">). We also normalized the kernels: . For all tree kernels implemented here, we first extract all features, generating an explicit map to the space of the kernel, and learn SVM ranking models using with a linear kernel in that space. Since the feature maps are not especially expensive for the kernels used here, we chose to solve the problem in its primal form. We were not aware of the existence of any fast software packages that could solve SVM ranking problems in the dual formulation. It is possible to convert the ranking problem into a classification problem using pairs of trees as shown in (Shen and Joshi, 2003). We have taken this approach in more recent work using string kernels requiring very expensive feature maps. We performed experiments using the version of the Redwoods corpus which was also used in the work of (Toutanova et al., 2002; Osborne and Baldbridge, 2004) and others. There are annotated sentences in total, of which are ambiguous. The average sentence length of the ambiguous sentences is words and the average number of parses per sentence is . We discarded the unambiguous sentences from the training and test sets. All models were trained and tested using 10-fold cross-validation. Accu</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVMbased voting algorithm with application to parse reranking. In Proceedings of CoNLL, pages 9– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Tsutomu Hirao</author>
<author>Yutaka Sasaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Hierarchical directed acyclic graph kernel: Methods for structured natural language data.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>32--39</pages>
<contexts>
<context position="2816" citStr="Suzuki et al., 2003" startWordPosition="428" endWordPosition="431">ls (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002). Here we propose a new representation of parse trees which (i) allows the localization of broader useful context, (ii) paves the way for exploring kernels, and (iii) achieves superior disambiguation accuracy compared to models that use tree representations centered around context-free rules. Compared to the usual notion of discriminative models (placing classes on rich observed</context>
</contexts>
<marker>Suzuki, Hirao, Sasaki, Maeda, 2003</marker>
<rawString>Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku Maeda. 2003. Hierarchical directed acyclic graph kernel: Methods for structured natural language data. In Proceedings of the ACL, pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Feature selection for a rich HPSG grammar using decision trees.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="5814" citStr="Toutanova and Manning, 2002" startWordPosition="933" endWordPosition="936">exicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redwoods corpus (Toutanova et al., 2002; Toutanova and Manning, 2002), we use the derivation trees as the main representation for disambiguation. Derivation trees record the combining rule schemas of the HPSG grammar which were used to license the sign by combining initial lexical types. The derivation tree is also the fundamental data stored in the Redwoods treebank, since the full sign can be reconstructed from it by reference to the grammar. The internal nodes represent, for example, head-complement, head-specifier, and head-adjunct schemas, which were used to license larger signs out of component parts. A derivation tree for the 1For an introduction to HPSG</context>
<context position="24331" citStr="Toutanova and Manning, 2002" startWordPosition="4135" endWordPosition="4138"> Many other features from the HPSG signs No. Name Example 0 Node Label HCOMP 1 synsem.local.cat.head verb 2 Label from Phrase Struct Tree S 3 Le Type of Lexical Head v sorb le 4 Lexical Head Word let Table 2: Annotated features of derivation tree nodes. The examples are from one node in the head path of the word let in Figure 1. are potentially helpful for disambiguation, and incorporating more useful features is a next step for this work. However, given the size of the corpus, a single model can not usefully profit from a large number of features. Previous work (Osborne and Baldbridge, 2004; Toutanova and Manning, 2002; Toutanova et al., 2002) has explored combining multiple classifiers using different features. We report results from such an experiment as well. Using Node Label and Head Category Annotations The simplest derivation tree node representation that we consider consists of features and - schema name and category of the lexical head. All experiments in this subsection section were performed using this derivation tree annotation. We briefly mention results from the best string-kernels when using other node annotations, as well as a combination of models using different features in the following su</context>
</contexts>
<marker>Toutanova, Manning, 2002</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2002. Feature selection for a rich HPSG grammar using decision trees. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Stuart Shieber</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Parse disambiguation for a rich HPSG grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of Treebanks and Linguistic Theories,</booktitle>
<pages>253--263</pages>
<contexts>
<context position="5784" citStr="Toutanova et al., 2002" startWordPosition="929" endWordPosition="932">odern constraint-based lexicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redwoods corpus (Toutanova et al., 2002; Toutanova and Manning, 2002), we use the derivation trees as the main representation for disambiguation. Derivation trees record the combining rule schemas of the HPSG grammar which were used to license the sign by combining initial lexical types. The derivation tree is also the fundamental data stored in the Redwoods treebank, since the full sign can be reconstructed from it by reference to the grammar. The internal nodes represent, for example, head-complement, head-specifier, and head-adjunct schemas, which were used to license larger signs out of component parts. A derivation tree for th</context>
<context position="17577" citStr="Toutanova et al., 2002" startWordPosition="3026" endWordPosition="3029">Since the feature maps are not especially expensive for the kernels used here, we chose to solve the problem in its primal form. We were not aware of the existence of any fast software packages that could solve SVM ranking problems in the dual formulation. It is possible to convert the ranking problem into a classification problem using pairs of trees as shown in (Shen and Joshi, 2003). We have taken this approach in more recent work using string kernels requiring very expensive feature maps. We performed experiments using the version of the Redwoods corpus which was also used in the work of (Toutanova et al., 2002; Osborne and Baldbridge, 2004) and others. There are annotated sentences in total, of which are ambiguous. The average sentence length of the ambiguous sentences is words and the average number of parses per sentence is . We discarded the unambiguous sentences from the training and test sets. All models were trained and tested using 10-fold cross-validation. Accuracy results are reported as percentage of sentences where the correct analysis was ranked first by the model. The structure of the experiments section is as follows. First we describe the results from a controlled experiment using a </context>
<context position="24356" citStr="Toutanova et al., 2002" startWordPosition="4139" endWordPosition="4142"> HPSG signs No. Name Example 0 Node Label HCOMP 1 synsem.local.cat.head verb 2 Label from Phrase Struct Tree S 3 Le Type of Lexical Head v sorb le 4 Lexical Head Word let Table 2: Annotated features of derivation tree nodes. The examples are from one node in the head path of the word let in Figure 1. are potentially helpful for disambiguation, and incorporating more useful features is a next step for this work. However, given the size of the corpus, a single model can not usefully profit from a large number of features. Previous work (Osborne and Baldbridge, 2004; Toutanova and Manning, 2002; Toutanova et al., 2002) has explored combining multiple classifiers using different features. We report results from such an experiment as well. Using Node Label and Head Category Annotations The simplest derivation tree node representation that we consider consists of features and - schema name and category of the lexical head. All experiments in this subsection section were performed using this derivation tree annotation. We briefly mention results from the best string-kernels when using other node annotations, as well as a combination of models using different features in the following subsection. To evaluate the</context>
<context position="28311" citStr="Toutanova et al., 2002" startWordPosition="4792" endWordPosition="4795">s that the Subsequence kernel unifies features that have gaps in different places, and the Wildcard kernel does not. For example, are different features for Wildcard, but they are the same feature for Subsequence – only the weighting of the feature depends on the position of the wildcard. When projection paths keyed by words are added, the accuracy increases significantly. subseq(2,3,.5,2) achieved an accuracy of , which is much higher than the best previously published accuracy from a single model on this corpus ( for a model that incorporates more sources of information from the HPSG signs (Toutanova et al., 2002)). The error reduction compared to that model is . It is also higher than the best result from voting classifiers ( (Osborne and Model Features Accuracy le w &amp; le le w &amp; le 1gram 13K - 81.43 - 2gram 37K 141K 82.70 84.11 wildcard (2,1,.7) 62K 167K 83.17 83.86 wildcard (2,1,.25) 62K 167K 82.97 - wildcard (3,1,.5) 187K 291K 83.21 83.59 wildcard (3,2,.5) 220K 82.90 - subseq (2,3,.5,2) 81K 185K 83.22 84.96 subseq (2,3,.25,2) 81K 185K 83.48 84.75 subseq (2,3,.25,1) 81K 185K 82.89 - subseq (2,4,.5,2) 102K 206K 83.29 84.40 subseq (3,3,.5,2) 154K 259K 83.17 83.85 subseq (3,4,.25,2) 290K - 83.06 - subse</context>
</contexts>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Stuart Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse disambiguation for a rich HPSG grammar. In Proceedings of Treebanks and Linguistic Theories, pages 253–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="10569" citStr="Vapnik, 1998" startWordPosition="1767" endWordPosition="1768">3 Tree and String Kernels 3.1 Kernels and SVM ranking From a machine learning point of view, the parse selection problem can be formulated as follows: given training examples ( , where each is a natural language sentence, is the number of such sentences, ,is a parse tree for ,is the number of parses for a given sentence ,is a feature representation for the parse tree , and we are given the training information which of all is the correct parse – learn how to correctly identify the correct parse of an unseen test sentence. One approach for solving this problem is via representing it as an SVM (Vapnik, 1998) ranking problem, where (without loss of generality) is assumed to be the correct parse for . The goal is to learn a parameter vector , such that the score of ( ) is higher than the scores of all other parses for the sentence. Thus we optimize for: The are slack variables used to handle the non-separable case. The same formulation has been used in (Collins, 2001) and (Shen and Joshi, 2003). This problem can be solved by solving the dual, and thus we would only need inner products of the feature vectors. This allows for using the kernel trick, where we replace the inner product in the represent</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. Wiley, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>