<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000162">
<title confidence="0.991149">
Stochastic Finite-State models for Spoken Language Machine
Translation
</title>
<author confidence="0.704188">
Srinivas Bangalore Giuseppe Riccardi
</author>
<affiliation confidence="0.579581">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.86981">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.926351">
{srini,dsp3}Oresearch.att.com
</email>
<sectionHeader confidence="0.992574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854428571428">
Stochastic finite-state models are efficiently learn-
able from data, effective for decoding and are asso-
ciated with a calculus for composing models which
allows for tight integration of constraints from var-
ious levels of language processing. In this paper,
we present a method for stochastic finite-state ma-
chine translation that is trained automatically from
pairs of source and target utterances. We use this
method to develop models for English-Japanese and
Japanese-English translation. We have embedded
the Japanese-English translation system in a call
routing task of unconstrained speech utterances. We
evaluate the efficacy of the translation system in the
context of this application.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995893281690141">
Finite state models have been extensively applied
to many aspects of language processing including,
speech recognition (Pereira and Riley, 1997; Riccardi
et al., 1996), phonology (Kaplan and Kay, 1994),
morphology (Koskenniemi, 1984), chunking (Abney,
1991; Srinivas, 1997) and parsing (Roche, 1999).
Finite-state models are attractive mechanisms for
language processing since they are (a) efficiently
learnable from data (b) generally effective for de-
coding (c) associated with a calculus for composing
models which allows for straightforward integration
of constraints from various levels of language pro-
cessing.&apos;
In this paper, we develop stochastic finite-state
models (SFSM) for statistical machine transla-
tion (SMT) and explore the performance limits of
such models in the context of translation in limited
domains. We are also interested in these models
since they allow for a tight integration with a speech
recognizer for speech-to-speech translation. In par-
ticular we are interested in one-pass decoding and
translation of speech as opposed to the more preva-
lent approach of translation of speech lattices.
The problem of machine translation can be viewed
as consisting of two phases: (a) lexical choice phase
&apos;Furthermore, software implementing the finite-state cal-
culus is available for research purposes.
where appropriate target language lexical items are
chosen for each source language lexical item and (b)
reordering phase where the chosen target language
lexical items are reordered to produce a meaning-
ful target language string. In our approach, we will
represent these two phases using stochastic finite-
state models which can be composed together to
result in a single stochastic finite-state model for
SMT. Thus our method can be viewed as a direct
translation approach of transducing strings of the
source language to strings of the target language.
There are other approaches to statistical machine
translation where translation is achieved through
transduction of source language structure to tar-
get language structure (Alshawi et al., 1998b; Wu,
1997). There are also large international multi-site
projects such as VERBMOBIL (Verbmobil, 2000)
and CSTAR (Woszczyna et al., 1998; Lavie et al.,
1999) that are involved in speech-to-speech trans-
lation in limited domains. The systems developed
in these projects employ various techniques ranging
from example-based to interlingua-based translation
methods for translation between English, French,
German, Italian, Japanese, and Korean.
Finite-state models for SMT have been previ-
ously suggested in the literature (Vilar et al., 1999;
Knight and Al-Onaizan, 1998). In (Vilar et al.,
1999), a deterministic transducer is used to imple-
ment an English-Spanish speech translation system.
In (Knight and Al-Onaizan, 1998), finite-state ma-
chine translation is based on (Brown et al., 1993)
and is used for decoding the target language string.
However, no experimental results are reported using
this approach.
Our approach differs from the previous approaches
in both the lexical choice and the reordering phases.
Unlike the previous approaches, the lexical choice
phase in our approach is decomposed into phrase-
level and sentence-level translation models. The
phrase-level translation is learned based on joint en-
tropy reduction of the source and target languages
and a variable length n-gram model (VNSA) (Ric-
cardi et al., 1995; Riccardi et al., 1996) is learned
for the sentence-level translation. For the construc-
</bodyText>
<page confidence="0.997112">
52
</page>
<bodyText confidence="0.999967571428571">
tion of the bilingual lexicon needed for lexical choice,
we use the alignment algorithm presented in (Al-
shawi et al., 1998b) which takes advantage of hi-
erarchical decomposition of strings and thus per-
forms a structure-based alignment. In the previ-
ous approaches, a bilingual lexicon is constructed
using a string-based alignment. Another difference
between our approach and the previous approaches
is in the reordering of the target language lexical
items. In (Knight and Al-Onaizan, 1998), an FSM
that represents all strings resulting from the per-
mutations of the lexical items produced by lexical
choice is constructed and the most likely translation
is retrieved using a target language model. In (Vilar
et al., 1999), the lexical items are associated with
markers that allow for reconstruction of the target
language string. Our reordering step is similar to
that proposed in (Knight and Al-Onaizan, 1998) but
does not incur the expense of creating a permutation
lattice. We use a phrase-based VNSA target lan-
guage model to retrieve the most likely translation
from the lattice.
In addition, we have used the resulting finite-
state translation method to implement an English-
Japanese speech and text translation system and
a Japanese-English text translation system. We
present evaluation results for these systems and dis-
cuss their limitations. We also evaluate the efficacy
of this translation model in the context of a telecom
application such as call routing.
The layout of the paper is as follows. In Section 2
we discuss the architecture of the finite-state trans-
lation system. We discuss the algorithm for learning
lexical and phrasal translation in Section 3. The de-
tails of the translation model are presented in Sec-
tion 4 and our method for reordering the output
is presented in Section 5. In Section 6 we discuss
the call classification application and present moti-
vations for embedding translation in such an applica-
tion. In Section 6.1 we present the experiments and
evaluation results for the various translation systems
on text input.
</bodyText>
<sectionHeader confidence="0.96108" genericHeader="introduction">
2 Stochastic Machine Translation
</sectionHeader>
<bodyText confidence="0.988905">
In machine translation, the objective is to map a
source symbol sequence Ws = w1, • • • , wNs (wi
Ls) into a target sequence WT = Z, • • • ,x1sI1. (xi E
LT). The statistical machine translation approach
is based on the noisy channel paradigm and the
Maximum-A-Posteriori decoding algorithm (Brown
et al., 1993). The sequence Ws is thought as a noisy
version of WT and the best guess W4&apos;, is then com-
puted as
</bodyText>
<equation confidence="0.8957995">
= arg max P(WT11415)
WT
= arg P(WsIWT)P(WT) (1)
WT
</equation>
<bodyText confidence="0.998275">
In (Brown et al., 1993) they propose a method for
maximizing P(WTIWs) by estimating P (WT) and
P(WsIWT) and solving the problem in equation 1.
Our approach to statistical machine translation dif-
fers from the model proposed in (Brown et al., 1993)
in that:
</bodyText>
<listItem confidence="0.622385333333333">
• We compute the joint model P(Ws, WT) from
the bilanguage corpus to account for the direct
mapping of the source sentence Ws into the tar-
get sentence WT that is ordered according to the
source language word order. The target string
W. is then chosen from all possible reorderings2
</listItem>
<equation confidence="0.96881775">
of WT •
= arg max P (Ws ,WT) (2)
WT
P(WT I AT) (3)
</equation>
<bodyText confidence="0.999465">
where AT is the target language model and A Wr
are the different reorderings of WT.
</bodyText>
<listItem confidence="0.983091666666667">
• We decompose the translation problem into
local (phrase-level) and global (sentence-level)
source-target string transduction.
• We automatically learn stochastic automata
and transducers to perform the sentence-level
and phrase-level translation.
</listItem>
<bodyText confidence="0.999466666666667">
As shown in Figure 1, the stochastic machine
translation system consists of two phases, the lexical
choice phase and the reordering phase. In the next
sections we describe the finite-state machine com-
ponents and the operation cascade that implements
this translation algorithm.
</bodyText>
<sectionHeader confidence="0.972234" genericHeader="method">
3 Acquiring Lexical Translations
</sectionHeader>
<bodyText confidence="0.998414823529412">
In the problem of speech recognition the alignment
between the words and their acoustics is relatively
straightforward since the words appear in the same
order as their corresponding acoustic events. In con-
trast, in machine translation, the linear order of
words in the source language, in general is not main-
tained in the target language.
The first stage in the process of bilingual phrase
acquisition is obtaining an alignment function that
given a pair of source and target language sentences,
maps source language word subsequences into target
language word subsequences. For this purpose, we
use the alignment algorithm described in (Alshawi et
2 Note that computing the exact set of all possible reorder-
ings is computationally expensive. In Section 5 we discuss
an approximation for the set of all possible reorderings that
serves for our application.
</bodyText>
<figure confidence="0.785140166666667">
VITT
VYT
= arg max
WTEAWT
53
max PØ4) Reorder
</figure>
<figureCaption confidence="0.997358">
Figure 1: A block diagram of the stochastic machine translation system
</figureCaption>
<figure confidence="0.692829111111111">
English: I need to make a collect call
Japanese: WI I- iP tON101.) 711&apos;
Alignment: 1 5 0 3 0 2 4
English: A T and T calling card
Japanese: 1.4 -7- q— 7 -7- 4 — —1&gt;&apos; 7 —
Alignment: 1 2 3 4 5 6
English: I&apos;d like to charge this to my home phone
Japanese: Val CL- tt. t. 14(7) *0&apos;) c&apos;—i 0)-c-4-
Alignment: 1 7 0 6 2 0 3 4 5
</figure>
<tableCaption confidence="0.995073">
Table 1: Example bitexts and with alignment information
</tableCaption>
<bodyText confidence="0.978101555555555">
al., 1998a). The result of the alignment procedure
is shown in Table 1.3
Although the search for bilingual phrases of length
more than two words can be incorporated in a
straight-forward manner in the alignment module,
we find that doing so is computationally prohibitive.
We first transform the output of the alignment
into a representation conducive for further manip-
ulation. We call this a bilanguage TB. A string
</bodyText>
<equation confidence="0.838882">
R E TB is represented as follows:
R = wi-xi, tv2-x2, • • • 7 WISrXN (4)
</equation>
<bodyText confidence="0.9918684375">
where wi E LS U e, xE LT U e, e is the empty
string and wi_xi is the symbol pair (colons are the
delimiters) drawn from the source and target lan-
guage.
A string in a bilanguage corpus consists of se-
quences of tokens where each token (wi_xi) is repre-
sented with two components: a source word (possi-
bly an empty word) as the first component and the
target word (possibly an empty word) that is the
translation of the source word as the second com-
ponent. Note that the tokens of a bilanguage could
be either ordered according to the word order of the
source language or ordered according to the word
order of the target language. Thus an alignment
of a pair of source and target language sentences
will result in two bilanguage strings. Table 2 shows
</bodyText>
<footnote confidence="0.658696333333333">
3 The Japanese string was translated and segmented so
that a token boundary in Japanese corresponds to some token
boundary in English.
</footnote>
<bodyText confidence="0.999021181818182">
an example alignment and the source-word-ordered
bilanguage strings corresponding to the alignment
shown in Table 1.
Having transformed the alignment for each sen-
tence pair into a bilanguage string (source word-
ordered or target word-ordered), we proceed to seg-
ment the corpus into bilingual phrases which can be
acquired from the corpus TB by minimizing the joint
entropy H (L s LT) = —11 M log P (TB). The proba-
bility P (W s WT) = P(Ft) is computed in the same
way as n-gram model:
</bodyText>
<equation confidence="0.997936">
P(R) = H pe.„,,,IWi-n+1-Xi-n+1) • • • Wi—l-Xi-1)
</equation>
<bodyText confidence="0.9704195">
(5)
Using the phrase segmented corpus, we construct
a phrase-based variable n-gram translation model as
discussed in the following section.
</bodyText>
<sectionHeader confidence="0.9606535" genericHeader="method">
4 Learning Phrase-based Variable
N-gram Translation Models
</sectionHeader>
<bodyText confidence="0.9999455">
Our approach to stochastic language modeling is
based on the Variable Ngram Stochastic Automaton
(VNSA) representation and learning algorithms
introduced in (Riccardi et al., 1995; Riccardi et al.,
1996). A VNSA is a non-deterministic Stochastic
Finite-State Machine (SFSM) that allows for pars-
ing any possible sequence of words drawn from a
given vocabulary V. In its simplest implementation
the state q in the VNSA encapsulates the lexical
(word sequence) history of a word sequence. Each
</bodyText>
<page confidence="0.991946">
54
</page>
<table confidence="0.718005">
need_!OW.hcA 9 at to_%EPS% make_ a_%EPS% collect_ n call_b, it
I&apos;d_f/it like_ L, t,: NorC&apos;t to_%EPS% charge r— this_ tt to_%EPS% myJM home_.,*op phone:_Cit
A_.m -I — and_7&gt; 4 — calling— U:/,&apos;&amp;quot; card_b
</table>
<tableCaption confidence="0.9610545">
Table 2: Bilanguage strings resulting from alignments shown in Table 1.
(%EPS% represents the null symbol e).
</tableCaption>
<bodyText confidence="0.977298833333333">
state recognizes a symbol w, E Vu {e}, where e is the
empty string. The probability of going from state qi
to qj (and recognizing the symbol associated to qj)
is given by the state transition probability, P(qjlqi).
Stochastic finite-state machines represent in a
compact way the probability distribution over all
possible word sequences. The probability of a word
sequence W can be associated to a state sequence
= qi,...,qj and to the probability P(,). For
a non-deterministic finite-state machine the prob-
ability of W is then given by P(W) = P4).
Moreover, by appropriately defining the state space
to incorporate lexical and extra-lexical information,
the VNSA formalism can generate a wide class of
probability distribution (i.e., standard word n-gram,
class-based, phrase-based, etc.) (Riccardi et al.,
1996; Riccardi et al., 1997; Riccardi and Bangalore,
1998). In Fig. 2, we plot a fragment of a VNSA
trained with word classes and phrases. State 0 is
the initial state and final states are double circled.
The c transition from state 0 to state 1 carries
the membership probability P(C), where the class
C contains the two elements {collect, calling
card}. The e transition from state 4 to state 6
is a back-off transition to a lower order n-gram
probability. State 2 carries the information about
the phrase calling card. The state transition
function, the transition probabilities and state
space are learned via the self-organizing algorithms
presented in (Riccardi et al., 1996).
4.1 Extending VNSAs to Stochastic
Transducers
Given the monolingual corpus T, the VNSA learning
algorithm provides an automaton that recognizes an
input string W (W E VN) and computes P(W) 0
for each W. Learning VNSAs from the bilingual cor-
pus TB leads to the notion of stochastic transducers
rsT. Stochastic transducers rsT : Ls x LT [0,1]
map the string WS E Ls into WT E LT and assign
a probability to the transduction WS 74 WT. In
our case, the VNSA&apos;s model will estimate P(WS
WT) = P(WS, WT) and the symbol pair w, : xi
will be associated to each transducer state q with
input label wi and output label xi. The model
rsT provides a sentence-level transduction from Ws
into WT. The integrated sentence and phrase-level
transduction is then trained directly on the phrase-
segmented corpus Tz described in section 3.
</bodyText>
<sectionHeader confidence="0.973967" genericHeader="method">
5 Reordering the output
</sectionHeader>
<bodyText confidence="0.999989727272727">
The stochastic transducers rsT takes as input a sen-
tence Ws and outputs a set of candidate strings in
the target language with source language word or-
der. Recall that the one-to-many mapping comes
from the non-determinism of rsT. The maximiza-
tion step in equation 2 is carried out with Viterbi al-
gorithm over the hypothesized strings in LT and WT
is selected. The last step to complete the translation
process is to apply the monolingual target language
model AT to re-order the sentence WT to produce
W. The re-order operation is crucial especially
in the case the bilanguage phrases in Tr, are not
sorted in the target language. For the re-ordering
operation, the exact approach would be to search
through all possible permutations of the words in
WT and select the most likely. However, that op-
eration is computationally very expensive. To over-
come this problem, we approximate the set of the
permutations with the word lattice 4, represent-
ing (x1 I x2I ... xN)N, where xi are the words in
WT. The most likely string Wit in the word lattice
is then decoded as follows:
</bodyText>
<equation confidence="0.864769">
= arg max(AT o
arg max P(WT I AT)
WT
(6)
</equation>
<bodyText confidence="0.9999245">
where o is the composition operation defined for
weighted finite-state machines (Pereira and Riley,
1997). The complete operation cascade for the ma-
chine translation process is shown in Figure 3.
</bodyText>
<sectionHeader confidence="0.998115" genericHeader="method">
6 Embedding Translation in an
Application
</sectionHeader>
<bodyText confidence="0.999953166666667">
In this section, we describe an application in
which we have embedded our translation model and
present some of the motivations for doing so. The
application that we are interested in is a call type
classification task called How May I Help You (Gorin
et al., 1997). The goal is to sufficiently understand
</bodyText>
<page confidence="0.998569">
55
</page>
<figureCaption confidence="0.998452">
Figure 2: Example of a Variable Ngram Stochastic Automaton (VNSA).
</figureCaption>
<figure confidence="0.9972755">
ST AT
&lt;epsilon&gt;/0.2
yes/0.8
calling/O.5 card/1
&lt;epsilon&gt;/0.9
max Pails ,W,.)
(x, Ix, I. x„)N
max (N0,14)
*T ,
Reorder
</figure>
<figureCaption confidence="0.999991">
Figure 3: The Machine Translation architecture
</figureCaption>
<bodyText confidence="0.9998568125">
caller&apos;s responses to the open-ended prompt How
May I Help You? and route such a call based on the
meaning of the response. Thus we aim at extracting
a relatively small number of semantic actions from
the utterances of a very large set of users who are not
trained to the system&apos;s capabilities and limitations.
The first utterance of each transaction has been
transcribed and marked with a call-type by label-
ers. There are 14 call-types plus a class other for
the complement class. In particular, we focused our
study on the classification of the caller&apos;s first utter-
ance in these dialogs. The spoken sentences vary
widely in duration, with a distribution distinctively
skewed around a mean value of 5.3 seconds corre-
sponding to 19 words per utterance. Some examples
of the first utterances are given below:
</bodyText>
<listItem confidence="0.9999495">
• Yes ma&apos;am where is area code two zero
one?
• I&apos;m tryn&apos;a call and I can&apos;t get it to
go through I wondered if you could try
it for me please?
• Hello
</listItem>
<bodyText confidence="0.999981181818182">
We trained a classifer on the training set of En-
glish sentences each of which was annotated with a
call type. The classifier searches for phrases that are
strongly associated with one of the call types (Gorin
et al., 1997) and in the test phase the classifier ex-
tracts these phrases from the output of the speech
recognizer and classifies the user utterance. This is
how the system works when the user speaks English.
However, if the user does not speak the language
that the classifier is trained on, English, in our
case, the system is unusable. We propose to solve
this problem by translating the user&apos;s utterance,
Japanese, in our case, to English. This extends the
usability of the system to new user groups.
An alternate approach could be to retrain the
classifier on Japanese text. However, this approach
would result in replicating the system for each pos-
sible input language, a very expensive proposition
considering, in general, that the system could have
sophisticated natural language understanding and
dialog components which would have to be repli-
cated also.
</bodyText>
<subsectionHeader confidence="0.997083">
6.1 Experiments and Evaluation
</subsectionHeader>
<bodyText confidence="0.998673631578947">
In this section, we discuss issues concerning evalu-
ation of the translation system. The data for the
experiments reported in this section were obtained
from the customer side of operator-customer con-
versations, with the customer-care application de-
scribed above and detailed in (Riccardi and Gorin,
January 2000; Gorin et al., 1997). Each of the cus-
tomer&apos;s utterance transcriptions were then manually
translated into Japanese. A total of 15,457 English-
Japanese sentence pairs was split into 12,204 train-
ing sentence pairs and 3,253 test sentence pairs.
The objective of this experiment is to measure
the performance of a translation system in the con-
text of an application. In an automated call router
there are two important performance measures. The
first is the probability of false rejection, where a
call is falsely rejected. Since such calls would be
transferred to a human agent, this corresponds to
a missed opportunity for automation. The second
</bodyText>
<page confidence="0.9925">
56
</page>
<bodyText confidence="0.99988692">
measure is the probability of correct classification.
Errors in this dimension lead to misinterpretations
that must be resolved by a dialog manager (Abella
and Gorin, 1997).
Using our approach described in the previous
sections, we have trained a unigram, bigram and
trigram VNSA based translation models with and
without phrases. Table 3 shows lexical choice (bag-
of-tokens) accuracy for these different translation
models measured in terms of recall, precision and
F-measure.
In order to measure the effectiveness of our trans-
lation models for this task we classify Japanese ut-
terances based on their English translations. Fig-
ure 4 plots the false rejection rate against the correct
classification rate of the classifier on the English gen-
erated by three different Japanese to English trans-
lation models for the set of Japanese test sentences.
The figure also shows the performance of the classi-
fier using the correct English text as input.
There are a few interesting observations to be
made from the Figure 4. Firstly, the task per-
formance on the text data is asymptotically simi-
lar to the task performance on the translation out-
put. In other words, the system performance is not
significantly affected by the translation process; a
Japanese transcription would most often be associ-
ated with the same call type after translation as if
the original were English. This result is particu-
larly interesting inspite of the impoverished reorder-
ing phase of the target language words. We believe
that this result is due to the nature of the application
where the classifier is mostly relying on the existence
of certain key words and phrases, not necessarily in
any particular order.
The task performance improved from the
unigram-based translation model to phrase unigram-
based translation model corresponding to the im-
provement in the lexical choice accuracy in Table 3.
Also, at higher false rejection rates, the task perfor-
mance is better for trigram-based translation model
than the phrase trigram-based translation model
since the precision of lexical choice is better than
that of the phrase trigram-based model as shown in
Table 3. This difference narrows at lower false rejec-
tion rate.
We are currently working on evaluating the
translation system in an application independent
method and developing improved models of reorder-
ing needed for better translation system.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999532">
We have presented an architecture for speech trans-
lation in limited domains based on the simple ma-
chinery of stochastic finite-state transducers. We
have implemented stochastic finite-state models for
English-Japanese and Japanese-English translation
in limited domains. These models have been trained
automatically from source-target utterance pairs.
We have evaluated the effectiveness of such a transla-
tion model in the context of a call-type classification
task.
</bodyText>
<sectionHeader confidence="0.992581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.825390574468085">
A. AbeIla and A. L. Gorin. 1997. Generating se-
mantically consistent inputs to a dialog man-
ager. In Proceedings of European Conference on
Speech Communication and Technology, pages
1879-1882.
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-based parsing. Kluwer Academic Pub-
lishers.
H. &apos;Alshawi, S. Bangalore, and S. Douglas. 1998a.
Learning Phrase-based Head Transduction Mod-
els for Translation of Spoken Utterances. In The
fifth International Conference on Spoken Lan-
guage Processing (ICSLP98), Sydney.
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 1998b. Automatic acquisition of hierarchi-
cal transduction models for machine translation.
In Proceedings of the 36&amp;quot; Annual Meeting of the
Association for Computational Linguistics, Mon-
treal, Canada.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mer-
cer. 1993. The Mathematics of Machine Transla-
tion: Parameter Estimation. Computational Lin-
guistics, 16(2):263-312.
E. Giachin. 1995. Phrase Bigrams for Continuous
Speech Recognition. In Proceedings of ICASSP,
pages 225-228, Detroit.
A. L. Gorin, G. Riccardi, and J. H Wright. 1997.
How May I Help You? Speech Communication,
23:113-127.
R.M. Kaplan and M. Kay. 1994. Regular models
of phonological rule systems. Computational Lin-
guistics, 20(3):331-378.
Kevin Knight and Y. Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Machine trans-
lation and the information soup, Langhorne, PA,
October.
K. K. Koskenniemi&apos;. 1984. Two-level morphology: a
general computation model for word-form recogni-
tion and production. Ph.D. thesis, University of
Helsinki.
Alon Lavie, Lori Levin, Monika Woszczyna, Donna
Gates, Marsal Gavalda„ and Alex Waibel.
1999. The janus-iii translation system: Speech-
to-speech translation in multiple domains. In
Proceedings of CSTAR Workshop, Schwetzingen,
Germany, September.
</reference>
<bodyText confidence="0.750007333333333">
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech recognition by composition of weighted
finite automata. In E. Roche and Schabes Y.,
</bodyText>
<page confidence="0.994695">
57
</page>
<table confidence="0.999895625">
Trans Recall Precision F-Measure
VNSA order (R) (P) (2*P*R/(P+R))
Unigram 24.5 83.6 37.9
Bigram 55.3 87.3 67.7
Trigram 61.8 86.4 72.1
Phrase Unigram 43.7 80.3 56.6
Phrase Bigram 62.5 86.3 72.5
Phrase Trigram 65.5 85.5 74.2
</table>
<tableCaption confidence="0.999816">
Table 3: Lexical choice accuracy of the Japanese to English Translation System with and without phrases
</tableCaption>
<figure confidence="0.991636588235294">
ROC curve tor English test set
100
95
9°
5
P. 85
8 80
Trigram
Text
--a
Phrase-YrIgram
Phrase-Unigram
Unigram
75
700
10 20 30 40 50 60 70 80 90
False rejection rate (%)
</figure>
<figureCaption confidence="0.992945">
Figure 4: Plots for the false rejection rate against the correct classification rate of the classifier on the English
generated by three different Japanese to English translation models
</figureCaption>
<reference confidence="0.997173659090909">
editors, Finite State Devices for Natural Lan-
guage Processing. MIT Press, Cambridge, Mas-
sachusetts.
G. Riccardi and S. Bangalore. 1998. Automatic ac-
quisition of phrase grammars for stochastic lan-
guage modeling. In Proceedings of ACL Workshop
on Very Large Corpora, pages 188-196, Montreal.
G. Riccardi and A.L. Gorin. January, 2000.
Stochastic Language Adaptation over Time and
State in Natural Spoken Dialogue Systems. IEEE
Transactions on Speech and Audio, pages 3-10.
G. Riccardi, E. Bocchieri, and R. Pieraccini. 1995.
Non deterministic stochastic language models for
speech recognition. In Proceedings of ICASSP,
pages 247-250, Detroit.
G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996.
Stochastic Automata for Language Modeling.
Computer Speech and Language, 10(4):265-293.
G. Riccardi, A. L. Gorin, A. Ljolje, and M. Riley.
1997. A spoken language system for automated
call routing. In Proceedings of ICASSP, pages
1143-1146, Munich.
K. Ries, F.D. Buo, and T. Wang. 1995. Improved
Language Modeling by Unsupervised Acquisition
of Structure. In Proceedings of ICASSP, pages
193-196, Detroit.
Emmanuel Roche. 1999. Finite state transducers:
parsing free and frozen sentences. In Andras Ko-
rnai, editor, Extened Finite State Models of Lan-
guage. Cambridge University Press.
B. Srinivas. 1997. Complexity of Lexical Descrip-
tions and its Relevance to Partial Parsing. Ph.D.
thesis, University of Pennsylvania, Philadelphia,
PA, August.
yerbmobil. 2000. Verbmobil Web page.
http://verbmobil.dfki.de/.
J. Vilar, V.M. Jimenez, J. Amengual, A. Castel-
lanos, D. Llorens, and E. Vidal. 1999. Text
and speech translation by means of subsequential
transducers. In Andras Kornai, editor, Extened
Finite State Models of Language. Cambridge Uni-
versity Press.
Monika Woszczyna, Matthew Broadhead, Donna
Gates, Marsal Gavalda, Alon Lavie, Lori Levin,
</reference>
<page confidence="0.98385">
58
</page>
<reference confidence="0.989328714285714">
and Alex Waibel. 1998. A modular approach to
spoken language translation for large domains. In
Proceedings of AMTA-98, Langhorne, Pennsylva-
nia, October.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-404.
</reference>
<page confidence="0.999261">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897610">
<title confidence="0.999428">Stochastic Finite-State models for Spoken Language Machine Translation</title>
<author confidence="0.999049">Srinivas Bangalore Giuseppe Riccardi</author>
<affiliation confidence="0.99291">AT&amp;T Labs -</affiliation>
<address confidence="0.9559235">180 Park Florham Park, NJ</address>
<email confidence="0.995274">sriniOresearch.att.com</email>
<email confidence="0.995274">dsp3Oresearch.att.com</email>
<abstract confidence="0.999614933333333">Stochastic finite-state models are efficiently learnable from data, effective for decoding and are associated with a calculus for composing models which allows for tight integration of constraints from various levels of language processing. In this paper, we present a method for stochastic finite-state machine translation that is trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system in the context of this application.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A AbeIla</author>
<author>A L Gorin</author>
</authors>
<title>Generating semantically consistent inputs to a dialog manager.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology,</booktitle>
<pages>1879--1882</pages>
<marker>AbeIla, Gorin, 1997</marker>
<rawString>A. AbeIla and A. L. Gorin. 1997. Generating semantically consistent inputs to a dialog manager. In Proceedings of European Conference on Speech Communication and Technology, pages 1879-1882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<editor>In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1171" citStr="Abney, 1991" startWordPosition="162" endWordPosition="163"> from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translation in limited domains. We are also interested in these mo</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-based parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H &apos;Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning Phrase-based Head Transduction Models for Translation of Spoken Utterances.</title>
<date>1998</date>
<booktitle>In The fifth International Conference on Spoken Language Processing (ICSLP98),</booktitle>
<location>Sydney.</location>
<marker>&apos;Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>H. &apos;Alshawi, S. Bangalore, and S. Douglas. 1998a. Learning Phrase-based Head Transduction Models for Translation of Spoken Utterances. In The fifth International Conference on Spoken Language Processing (ICSLP98), Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Automatic acquisition of hierarchical transduction models for machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36&amp;quot; Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2980" citStr="Alshawi et al., 1998" startWordPosition="432" endWordPosition="435"> the chosen target language lexical items are reordered to produce a meaningful target language string. In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a d</context>
<context position="4523" citStr="Alshawi et al., 1998" startWordPosition="663" endWordPosition="667"> differs from the previous approaches in both the lexical choice and the reordering phases. Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level translation models. The phrase-level translation is learned based on joint entropy reduction of the source and target languages and a variable length n-gram model (VNSA) (Riccardi et al., 1995; Riccardi et al., 1996) is learned for the sentence-level translation. For the construc52 tion of the bilingual lexicon needed for lexical choice, we use the alignment algorithm presented in (Alshawi et al., 1998b) which takes advantage of hierarchical decomposition of strings and thus performs a structure-based alignment. In the previous approaches, a bilingual lexicon is constructed using a string-based alignment. Another difference between our approach and the previous approaches is in the reordering of the target language lexical items. In (Knight and Al-Onaizan, 1998), an FSM that represents all strings resulting from the permutations of the lexical items produced by lexical choice is constructed and the most likely translation is retrieved using a target language model. In (Vilar et al., 1999), </context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 1998b. Automatic acquisition of hierarchical transduction models for machine translation. In Proceedings of the 36&amp;quot; Annual Meeting of the Association for Computational Linguistics, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>16--2</pages>
<contexts>
<context position="3770" citStr="Brown et al., 1993" startWordPosition="546" endWordPosition="549">volved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Our approach differs from the previous approaches in both the lexical choice and the reordering phases. Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level translation models. The phrase-level translation is learned based on joint entropy reduction of the source and target languages and a variable length n-gram model (VNSA) (Riccardi et al., 1995; Riccardi et al., 1996) is learned for the sentence-level t</context>
<context position="6796" citStr="Brown et al., 1993" startWordPosition="1032" endWordPosition="1035">s presented in Section 5. In Section 6 we discuss the call classification application and present motivations for embedding translation in such an application. In Section 6.1 we present the experiments and evaluation results for the various translation systems on text input. 2 Stochastic Machine Translation In machine translation, the objective is to map a source symbol sequence Ws = w1, • • • , wNs (wi Ls) into a target sequence WT = Z, • • • ,x1sI1. (xi E LT). The statistical machine translation approach is based on the noisy channel paradigm and the Maximum-A-Posteriori decoding algorithm (Brown et al., 1993). The sequence Ws is thought as a noisy version of WT and the best guess W4&apos;, is then computed as = arg max P(WT11415) WT = arg P(WsIWT)P(WT) (1) WT In (Brown et al., 1993) they propose a method for maximizing P(WTIWs) by estimating P (WT) and P(WsIWT) and solving the problem in equation 1. Our approach to statistical machine translation differs from the model proposed in (Brown et al., 1993) in that: • We compute the joint model P(Ws, WT) from the bilanguage corpus to account for the direct mapping of the source sentence Ws into the target sentence WT that is ordered according to the source l</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 16(2):263-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Giachin</author>
</authors>
<title>Phrase Bigrams for Continuous Speech Recognition.</title>
<date>1995</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>225--228</pages>
<location>Detroit.</location>
<marker>Giachin, 1995</marker>
<rawString>E. Giachin. 1995. Phrase Bigrams for Continuous Speech Recognition. In Proceedings of ICASSP, pages 225-228, Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How May I Help You? Speech Communication,</title>
<date>1997</date>
<pages>23--113</pages>
<contexts>
<context position="16385" citStr="Gorin et al., 1997" startWordPosition="2668" endWordPosition="2671">The most likely string Wit in the word lattice is then decoded as follows: = arg max(AT o arg max P(WT I AT) WT (6) where o is the composition operation defined for weighted finite-state machines (Pereira and Riley, 1997). The complete operation cascade for the machine translation process is shown in Figure 3. 6 Embedding Translation in an Application In this section, we describe an application in which we have embedded our translation model and present some of the motivations for doing so. The application that we are interested in is a call type classification task called How May I Help You (Gorin et al., 1997). The goal is to sufficiently understand 55 Figure 2: Example of a Variable Ngram Stochastic Automaton (VNSA). ST AT &lt;epsilon&gt;/0.2 yes/0.8 calling/O.5 card/1 &lt;epsilon&gt;/0.9 max Pails ,W,.) (x, Ix, I. x„)N max (N0,14) *T , Reorder Figure 3: The Machine Translation architecture caller&apos;s responses to the open-ended prompt How May I Help You? and route such a call based on the meaning of the response. Thus we aim at extracting a relatively small number of semantic actions from the utterances of a very large set of users who are not trained to the system&apos;s capabilities and limitations. The first utt</context>
<context position="17841" citStr="Gorin et al., 1997" startWordPosition="2921" endWordPosition="2924"> utterance in these dialogs. The spoken sentences vary widely in duration, with a distribution distinctively skewed around a mean value of 5.3 seconds corresponding to 19 words per utterance. Some examples of the first utterances are given below: • Yes ma&apos;am where is area code two zero one? • I&apos;m tryn&apos;a call and I can&apos;t get it to go through I wondered if you could try it for me please? • Hello We trained a classifer on the training set of English sentences each of which was annotated with a call type. The classifier searches for phrases that are strongly associated with one of the call types (Gorin et al., 1997) and in the test phase the classifier extracts these phrases from the output of the speech recognizer and classifies the user utterance. This is how the system works when the user speaks English. However, if the user does not speak the language that the classifier is trained on, English, in our case, the system is unusable. We propose to solve this problem by translating the user&apos;s utterance, Japanese, in our case, to English. This extends the usability of the system to new user groups. An alternate approach could be to retrain the classifier on Japanese text. However, this approach would resu</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. L. Gorin, G. Riccardi, and J. H Wright. 1997. How May I Help You? Speech Communication, 23:113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>M Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="1116" citStr="Kaplan and Kay, 1994" startWordPosition="154" endWordPosition="157">ic finite-state machine translation that is trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translation</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>R.M. Kaplan and M. Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Translation with finite-state devices.</title>
<date>1998</date>
<booktitle>In Machine translation and the information soup,</booktitle>
<location>Langhorne, PA,</location>
<contexts>
<context position="3550" citStr="Knight and Al-Onaizan, 1998" startWordPosition="512" endWordPosition="515">ructure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Our approach differs from the previous approaches in both the lexical choice and the reordering phases. Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level translation models. The phrase-l</context>
<context position="4890" citStr="Knight and Al-Onaizan, 1998" startWordPosition="718" endWordPosition="721">ength n-gram model (VNSA) (Riccardi et al., 1995; Riccardi et al., 1996) is learned for the sentence-level translation. For the construc52 tion of the bilingual lexicon needed for lexical choice, we use the alignment algorithm presented in (Alshawi et al., 1998b) which takes advantage of hierarchical decomposition of strings and thus performs a structure-based alignment. In the previous approaches, a bilingual lexicon is constructed using a string-based alignment. Another difference between our approach and the previous approaches is in the reordering of the target language lexical items. In (Knight and Al-Onaizan, 1998), an FSM that represents all strings resulting from the permutations of the lexical items produced by lexical choice is constructed and the most likely translation is retrieved using a target language model. In (Vilar et al., 1999), the lexical items are associated with markers that allow for reconstruction of the target language string. Our reordering step is similar to that proposed in (Knight and Al-Onaizan, 1998) but does not incur the expense of creating a permutation lattice. We use a phrase-based VNSA target language model to retrieve the most likely translation from the lattice. In add</context>
</contexts>
<marker>Knight, Al-Onaizan, 1998</marker>
<rawString>Kevin Knight and Y. Al-Onaizan. 1998. Translation with finite-state devices. In Machine translation and the information soup, Langhorne, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K K Koskenniemi&apos;</author>
</authors>
<title>Two-level morphology: a general computation model for word-form recognition and production.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<marker>Koskenniemi&apos;, 1984</marker>
<rawString>K. K. Koskenniemi&apos;. 1984. Two-level morphology: a general computation model for word-form recognition and production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Lori Levin</author>
<author>Monika Woszczyna</author>
<author>Donna Gates</author>
<author>Marsal Gavalda„</author>
<author>Alex Waibel</author>
</authors>
<title>The janus-iii translation system: Speechto-speech translation in multiple domains.</title>
<date>1999</date>
<booktitle>In Proceedings of CSTAR Workshop,</booktitle>
<location>Schwetzingen, Germany,</location>
<marker>Lavie, Levin, Woszczyna, Gates, Gavalda„, Waibel, 1999</marker>
<rawString>Alon Lavie, Lori Levin, Monika Woszczyna, Donna Gates, Marsal Gavalda„ and Alex Waibel. 1999. The janus-iii translation system: Speechto-speech translation in multiple domains. In Proceedings of CSTAR Workshop, Schwetzingen, Germany, September.</rawString>
</citation>
<citation valid="false">
<title>State Devices for Natural Language Processing.</title>
<editor>editors, Finite</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker></marker>
<rawString>editors, Finite State Devices for Natural Language Processing. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>S Bangalore</author>
</authors>
<title>Automatic acquisition of phrase grammars for stochastic language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL Workshop on Very Large Corpora,</booktitle>
<pages>188--196</pages>
<location>Montreal.</location>
<contexts>
<context position="13302" citStr="Riccardi and Bangalore, 1998" startWordPosition="2134" endWordPosition="2137">e machines represent in a compact way the probability distribution over all possible word sequences. The probability of a word sequence W can be associated to a state sequence = qi,...,qj and to the probability P(,). For a non-deterministic finite-state machine the probability of W is then given by P(W) = P4). Moreover, by appropriately defining the state space to incorporate lexical and extra-lexical information, the VNSA formalism can generate a wide class of probability distribution (i.e., standard word n-gram, class-based, phrase-based, etc.) (Riccardi et al., 1996; Riccardi et al., 1997; Riccardi and Bangalore, 1998). In Fig. 2, we plot a fragment of a VNSA trained with word classes and phrases. State 0 is the initial state and final states are double circled. The c transition from state 0 to state 1 carries the membership probability P(C), where the class C contains the two elements {collect, calling card}. The e transition from state 4 to state 6 is a back-off transition to a lower order n-gram probability. State 2 carries the information about the phrase calling card. The state transition function, the transition probabilities and state space are learned via the self-organizing algorithms presented in </context>
</contexts>
<marker>Riccardi, Bangalore, 1998</marker>
<rawString>G. Riccardi and S. Bangalore. 1998. Automatic acquisition of phrase grammars for stochastic language modeling. In Proceedings of ACL Workshop on Very Large Corpora, pages 188-196, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>A L Gorin</author>
</authors>
<title>Stochastic Language Adaptation over Time and State in Natural Spoken Dialogue Systems.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio,</journal>
<pages>3--10</pages>
<marker>Riccardi, Gorin, 2000</marker>
<rawString>G. Riccardi and A.L. Gorin. January, 2000. Stochastic Language Adaptation over Time and State in Natural Spoken Dialogue Systems. IEEE Transactions on Speech and Audio, pages 3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>E Bocchieri</author>
<author>R Pieraccini</author>
</authors>
<title>Non deterministic stochastic language models for speech recognition.</title>
<date>1995</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>247--250</pages>
<location>Detroit.</location>
<contexts>
<context position="4310" citStr="Riccardi et al., 1995" startWordPosition="628" endWordPosition="632">Onaizan, 1998), finite-state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Our approach differs from the previous approaches in both the lexical choice and the reordering phases. Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level translation models. The phrase-level translation is learned based on joint entropy reduction of the source and target languages and a variable length n-gram model (VNSA) (Riccardi et al., 1995; Riccardi et al., 1996) is learned for the sentence-level translation. For the construc52 tion of the bilingual lexicon needed for lexical choice, we use the alignment algorithm presented in (Alshawi et al., 1998b) which takes advantage of hierarchical decomposition of strings and thus performs a structure-based alignment. In the previous approaches, a bilingual lexicon is constructed using a string-based alignment. Another difference between our approach and the previous approaches is in the reordering of the target language lexical items. In (Knight and Al-Onaizan, 1998), an FSM that repres</context>
<context position="11811" citStr="Riccardi et al., 1995" startWordPosition="1898" endWordPosition="1901">l phrases which can be acquired from the corpus TB by minimizing the joint entropy H (L s LT) = —11 M log P (TB). The probability P (W s WT) = P(Ft) is computed in the same way as n-gram model: P(R) = H pe.„,,,IWi-n+1-Xi-n+1) • • • Wi—l-Xi-1) (5) Using the phrase segmented corpus, we construct a phrase-based variable n-gram translation model as discussed in the following section. 4 Learning Phrase-based Variable N-gram Translation Models Our approach to stochastic language modeling is based on the Variable Ngram Stochastic Automaton (VNSA) representation and learning algorithms introduced in (Riccardi et al., 1995; Riccardi et al., 1996). A VNSA is a non-deterministic Stochastic Finite-State Machine (SFSM) that allows for parsing any possible sequence of words drawn from a given vocabulary V. In its simplest implementation the state q in the VNSA encapsulates the lexical (word sequence) history of a word sequence. Each 54 need_!OW.hcA 9 at to_%EPS% make_ a_%EPS% collect_ n call_b, it I&apos;d_f/it like_ L, t,: NorC&apos;t to_%EPS% charge r— this_ tt to_%EPS% myJM home_.,*op phone:_Cit A_.m -I — and_7&gt; 4 — calling— U:/,&apos;&amp;quot; card_b Table 2: Bilanguage strings resulting from alignments shown in Table 1. (%EPS% repres</context>
</contexts>
<marker>Riccardi, Bocchieri, Pieraccini, 1995</marker>
<rawString>G. Riccardi, E. Bocchieri, and R. Pieraccini. 1995. Non deterministic stochastic language models for speech recognition. In Proceedings of ICASSP, pages 247-250, Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>R Pieraccini</author>
<author>E Bocchieri</author>
</authors>
<title>Stochastic Automata for Language Modeling. Computer Speech and Language,</title>
<date>1996</date>
<pages>10--4</pages>
<contexts>
<context position="1082" citStr="Riccardi et al., 1996" startWordPosition="149" endWordPosition="152">r, we present a method for stochastic finite-state machine translation that is trained automatically from pairs of source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such mo</context>
<context position="4334" citStr="Riccardi et al., 1996" startWordPosition="633" endWordPosition="636">state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Our approach differs from the previous approaches in both the lexical choice and the reordering phases. Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level translation models. The phrase-level translation is learned based on joint entropy reduction of the source and target languages and a variable length n-gram model (VNSA) (Riccardi et al., 1995; Riccardi et al., 1996) is learned for the sentence-level translation. For the construc52 tion of the bilingual lexicon needed for lexical choice, we use the alignment algorithm presented in (Alshawi et al., 1998b) which takes advantage of hierarchical decomposition of strings and thus performs a structure-based alignment. In the previous approaches, a bilingual lexicon is constructed using a string-based alignment. Another difference between our approach and the previous approaches is in the reordering of the target language lexical items. In (Knight and Al-Onaizan, 1998), an FSM that represents all strings resulti</context>
<context position="11835" citStr="Riccardi et al., 1996" startWordPosition="1902" endWordPosition="1905">acquired from the corpus TB by minimizing the joint entropy H (L s LT) = —11 M log P (TB). The probability P (W s WT) = P(Ft) is computed in the same way as n-gram model: P(R) = H pe.„,,,IWi-n+1-Xi-n+1) • • • Wi—l-Xi-1) (5) Using the phrase segmented corpus, we construct a phrase-based variable n-gram translation model as discussed in the following section. 4 Learning Phrase-based Variable N-gram Translation Models Our approach to stochastic language modeling is based on the Variable Ngram Stochastic Automaton (VNSA) representation and learning algorithms introduced in (Riccardi et al., 1995; Riccardi et al., 1996). A VNSA is a non-deterministic Stochastic Finite-State Machine (SFSM) that allows for parsing any possible sequence of words drawn from a given vocabulary V. In its simplest implementation the state q in the VNSA encapsulates the lexical (word sequence) history of a word sequence. Each 54 need_!OW.hcA 9 at to_%EPS% make_ a_%EPS% collect_ n call_b, it I&apos;d_f/it like_ L, t,: NorC&apos;t to_%EPS% charge r— this_ tt to_%EPS% myJM home_.,*op phone:_Cit A_.m -I — and_7&gt; 4 — calling— U:/,&apos;&amp;quot; card_b Table 2: Bilanguage strings resulting from alignments shown in Table 1. (%EPS% represents the null symbol e).</context>
<context position="13248" citStr="Riccardi et al., 1996" startWordPosition="2126" endWordPosition="2129"> probability, P(qjlqi). Stochastic finite-state machines represent in a compact way the probability distribution over all possible word sequences. The probability of a word sequence W can be associated to a state sequence = qi,...,qj and to the probability P(,). For a non-deterministic finite-state machine the probability of W is then given by P(W) = P4). Moreover, by appropriately defining the state space to incorporate lexical and extra-lexical information, the VNSA formalism can generate a wide class of probability distribution (i.e., standard word n-gram, class-based, phrase-based, etc.) (Riccardi et al., 1996; Riccardi et al., 1997; Riccardi and Bangalore, 1998). In Fig. 2, we plot a fragment of a VNSA trained with word classes and phrases. State 0 is the initial state and final states are double circled. The c transition from state 0 to state 1 carries the membership probability P(C), where the class C contains the two elements {collect, calling card}. The e transition from state 4 to state 6 is a back-off transition to a lower order n-gram probability. State 2 carries the information about the phrase calling card. The state transition function, the transition probabilities and state space are le</context>
</contexts>
<marker>Riccardi, Pieraccini, Bocchieri, 1996</marker>
<rawString>G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996. Stochastic Automata for Language Modeling. Computer Speech and Language, 10(4):265-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>A L Gorin</author>
<author>A Ljolje</author>
<author>M Riley</author>
</authors>
<title>A spoken language system for automated call routing.</title>
<date>1997</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>1143--1146</pages>
<location>Munich.</location>
<contexts>
<context position="13271" citStr="Riccardi et al., 1997" startWordPosition="2130" endWordPosition="2133"> Stochastic finite-state machines represent in a compact way the probability distribution over all possible word sequences. The probability of a word sequence W can be associated to a state sequence = qi,...,qj and to the probability P(,). For a non-deterministic finite-state machine the probability of W is then given by P(W) = P4). Moreover, by appropriately defining the state space to incorporate lexical and extra-lexical information, the VNSA formalism can generate a wide class of probability distribution (i.e., standard word n-gram, class-based, phrase-based, etc.) (Riccardi et al., 1996; Riccardi et al., 1997; Riccardi and Bangalore, 1998). In Fig. 2, we plot a fragment of a VNSA trained with word classes and phrases. State 0 is the initial state and final states are double circled. The c transition from state 0 to state 1 carries the membership probability P(C), where the class C contains the two elements {collect, calling card}. The e transition from state 4 to state 6 is a back-off transition to a lower order n-gram probability. State 2 carries the information about the phrase calling card. The state transition function, the transition probabilities and state space are learned via the self-orga</context>
</contexts>
<marker>Riccardi, Gorin, Ljolje, Riley, 1997</marker>
<rawString>G. Riccardi, A. L. Gorin, A. Ljolje, and M. Riley. 1997. A spoken language system for automated call routing. In Proceedings of ICASSP, pages 1143-1146, Munich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ries</author>
<author>F D Buo</author>
<author>T Wang</author>
</authors>
<title>Improved Language Modeling by Unsupervised Acquisition of Structure.</title>
<date>1995</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>193--196</pages>
<location>Detroit.</location>
<marker>Ries, Buo, Wang, 1995</marker>
<rawString>K. Ries, F.D. Buo, and T. Wang. 1995. Improved Language Modeling by Unsupervised Acquisition of Structure. In Proceedings of ICASSP, pages 193-196, Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Finite state transducers: parsing free and frozen sentences.</title>
<date>1999</date>
<editor>In Andras Kornai, editor, Extened</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1214" citStr="Roche, 1999" startWordPosition="168" endWordPosition="169">s. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translation in limited domains. We are also interested in these models since they allow for a tight integrati</context>
</contexts>
<marker>Roche, 1999</marker>
<rawString>Emmanuel Roche. 1999. Finite state transducers: parsing free and frozen sentences. In Andras Kornai, editor, Extened Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Complexity of Lexical Descriptions and its Relevance to Partial Parsing.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1188" citStr="Srinivas, 1997" startWordPosition="164" endWordPosition="165">f source and target utterances. We use this method to develop models for English-Japanese and Japanese-English translation. We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances. We evaluate the efficacy of the translation system in the context of this application. 1 Introduction Finite state models have been extensively applied to many aspects of language processing including, speech recognition (Pereira and Riley, 1997; Riccardi et al., 1996), phonology (Kaplan and Kay, 1994), morphology (Koskenniemi, 1984), chunking (Abney, 1991; Srinivas, 1997) and parsing (Roche, 1999). Finite-state models are attractive mechanisms for language processing since they are (a) efficiently learnable from data (b) generally effective for decoding (c) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of language processing.&apos; In this paper, we develop stochastic finite-state models (SFSM) for statistical machine translation (SMT) and explore the performance limits of such models in the context of translation in limited domains. We are also interested in these models since they a</context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>B. Srinivas. 1997. Complexity of Lexical Descriptions and its Relevance to Partial Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>yerbmobil</author>
</authors>
<title>Verbmobil Web page.</title>
<date>2000</date>
<note>http://verbmobil.dfki.de/.</note>
<marker>yerbmobil, 2000</marker>
<rawString>yerbmobil. 2000. Verbmobil Web page. http://verbmobil.dfki.de/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vilar</author>
<author>V M Jimenez</author>
<author>J Amengual</author>
<author>A Castellanos</author>
<author>D Llorens</author>
<author>E Vidal</author>
</authors>
<title>Text and speech translation by means of subsequential transducers.</title>
<date>1999</date>
<editor>In Andras Kornai, editor, Extened</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3520" citStr="Vilar et al., 1999" startWordPosition="508" endWordPosition="511">f source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state machine translation is based on (Brown et al., 1993) and is used for decoding the target language string. However, no experimental results are reported using this approach. Our approach differs from the previous approaches in both the lexical choice and the reordering phases. Unlike the previous approaches, the lexical choice phase in our approach is decomposed into phraselevel and sentence-level tr</context>
<context position="5121" citStr="Vilar et al., 1999" startWordPosition="756" endWordPosition="759">(Alshawi et al., 1998b) which takes advantage of hierarchical decomposition of strings and thus performs a structure-based alignment. In the previous approaches, a bilingual lexicon is constructed using a string-based alignment. Another difference between our approach and the previous approaches is in the reordering of the target language lexical items. In (Knight and Al-Onaizan, 1998), an FSM that represents all strings resulting from the permutations of the lexical items produced by lexical choice is constructed and the most likely translation is retrieved using a target language model. In (Vilar et al., 1999), the lexical items are associated with markers that allow for reconstruction of the target language string. Our reordering step is similar to that proposed in (Knight and Al-Onaizan, 1998) but does not incur the expense of creating a permutation lattice. We use a phrase-based VNSA target language model to retrieve the most likely translation from the lattice. In addition, we have used the resulting finitestate translation method to implement an EnglishJapanese speech and text translation system and a Japanese-English text translation system. We present evaluation results for these systems and</context>
</contexts>
<marker>Vilar, Jimenez, Amengual, Castellanos, Llorens, Vidal, 1999</marker>
<rawString>J. Vilar, V.M. Jimenez, J. Amengual, A. Castellanos, D. Llorens, and E. Vidal. 1999. Text and speech translation by means of subsequential transducers. In Andras Kornai, editor, Extened Finite State Models of Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monika Woszczyna</author>
<author>Matthew Broadhead</author>
<author>Donna Gates</author>
<author>Marsal Gavalda</author>
<author>Alon Lavie</author>
<author>Lori Levin</author>
<author>Alex Waibel</author>
</authors>
<title>A modular approach to spoken language translation for large domains. In</title>
<date>1998</date>
<booktitle>Proceedings of AMTA-98,</booktitle>
<location>Langhorne, Pennsylvania,</location>
<contexts>
<context position="3118" citStr="Woszczyna et al., 1998" startWordPosition="452" endWordPosition="455">nt these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic transducer is used to implement an English-Spanish speech translation system. In (Knight and Al-Onaizan, 1998), finite-state </context>
</contexts>
<marker>Woszczyna, Broadhead, Gates, Gavalda, Lavie, Levin, Waibel, 1998</marker>
<rawString>Monika Woszczyna, Matthew Broadhead, Donna Gates, Marsal Gavalda, Alon Lavie, Lori Levin, and Alex Waibel. 1998. A modular approach to spoken language translation for large domains. In Proceedings of AMTA-98, Langhorne, Pennsylvania, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="2992" citStr="Wu, 1997" startWordPosition="436" endWordPosition="437">uage lexical items are reordered to produce a meaningful target language string. In our approach, we will represent these two phases using stochastic finitestate models which can be composed together to result in a single stochastic finite-state model for SMT. Thus our method can be viewed as a direct translation approach of transducing strings of the source language to strings of the target language. There are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure (Alshawi et al., 1998b; Wu, 1997). There are also large international multi-site projects such as VERBMOBIL (Verbmobil, 2000) and CSTAR (Woszczyna et al., 1998; Lavie et al., 1999) that are involved in speech-to-speech translation in limited domains. The systems developed in these projects employ various techniques ranging from example-based to interlingua-based translation methods for translation between English, French, German, Italian, Japanese, and Korean. Finite-state models for SMT have been previously suggested in the literature (Vilar et al., 1999; Knight and Al-Onaizan, 1998). In (Vilar et al., 1999), a deterministic</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-404.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>