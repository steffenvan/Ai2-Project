<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006319">
<title confidence="0.925213">
Entity Extraction Without Language-Specific Resources
</title>
<author confidence="0.845488">
Paul McNamee James Mayfield
</author>
<note confidence="0.593119333333333">
The Johns Hopkins University Applied Physics Laboratory
11100 Johns Hopkins Road, Laurel, Maryland 20723-6099 USA
{ mcnamee,mayfield} Ajhuapl.edu
</note>
<sectionHeader confidence="0.927975" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824705882353">
We describe a named-entity tagging system
that requires minimal linguistic knowledge and
thus may be applied to new target languages
without significant adaptation. To maintain a
language- neutral posture, the system is lin-
guistically nave, and in fact, reduces the tag-
ging problem to supervised machine learning.
A large number of binary features are extracted
from labeled data to train classifiers and compu-
tationally expensive features are eschewed. We
have initially focused our attention on linear
support vectors machines (SVMs); SVMs are
known to work well when a large number of fea-
tures is used as long as the individual vectors
are sparse. We call our system SNOOD (Hop-
kins APL Inductive Retargetable Named Entity
Tagger).
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922">
Text processing research at the Johns Hop-
kins University Applied Physics Laboratory
(JHU/APL) has focused on information re-
trieval tasks. We have built a language-
independent retrieval engine, HAIRCUT, to ex-
plore effective, knowledge-light approaches to
the multilingual text retrieval problem (Mc-
Namee et al., 2001). Through participation
in cross- language evaluation conferences like
TREC, CLEF, and NTCIR, we have evalu-
ated retrieval effectiveness over document col-
lections containing Arabic, Chinese, Dutch,
English, Finnish, French, German, Italian,
Japanese, Korean, Spanish, and Swedish. We
have found that a system based on weak-
methods can achieve high retrieval effectiveness,
both monolingually and translingually, in all
of these languages. HAIRCUT uses a statis-
tical language model of retrieval and simple to-
kenization methods such as unnormalized word
forms and character n-grams. We approached
the CoNLL- 2002 shared task in Language-
Independent Named Entity Recognition with
the same philosophy of attempting to maxi-
mize performance with judicious use of simple,
language-neutral methods; the promise of such
approaches has previously been demonstrated
for this task (Cucerzan and Yarowsky, 1999).
</bodyText>
<sectionHeader confidence="0.96366" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999942458333333">
Our initial approach was to cast the labeling
problem as a binary decision problem of deter-
mining whether the current token belongs to
one of the eight classes (e.g., B-LOC). Thus we
built eight individual classifiers. The algorithm,
features, and learning parameters were identical
for each classifier. For each, we used a linear
kernel support vector machine that was trained
using all positive and negative exemplars from
the training set. The labels for this problem are
mutually exclusive of one another, so we applied
a rudimentary deterministic method to select a
single label when multiple classifiers predicted
membership in multiple categories.
We computed features pertaining to each to-
ken, and used a window of w tokens, with x to-
kens before and y tokens following the current
token. Our submitted results used w=7 and
x=y=3; we examined other window sizes briefly,
but did not find significant differences. Binary
features were used and were of two kinds: those
derived from orthography and punctuation and
those related to the language under question.
For each token, the basic features were:
</bodyText>
<listItem confidence="0.998227">
1. Token length equal to 0-9 (10)
2. Token length between 10 and 15, or greater
than 15 (2)
3. Whether the token was a comma, a full
stop, a question mark, an exclamation
mark, or other punctuation (5)
4. If the token contained all digits, letters and
digits, all capitals, any internal capitaliza-
tion, and an initial capital (5)
5. Whether the lst/2nd/nth-1/nth character
exists (4 x 1)
6. Whether the lst/2nd/nth-1/nth character
is ASCII code 33, ..., 90 (4 x 58)
</listItem>
<bodyText confidence="0.997988103448276">
for a total of 258 features. The use of lead-
ing and trailing characters was motivated by a
desire to use morphological clues, such as if a
word ended in &apos;ly&apos; or &apos;ing&apos;. In a non-alphabetic
language, different features would be required,
for example, in Chinese the stroke count of a
character might be used, or perhaps a translit-
erated form such as that produced by the Pinyin
system could be applied first. In Japanese, the
kana (hiragani and katakana) would be useful.
This rudimentary approach would less applica-
ble with infix morphology, but consonant triples
might be useful for a Semitic language like Ara-
bic.
One thousand features that were specific
to the language under consideration were also
used:
1. If the token is the ith most frequent word
in the language
Very common words were used as an approx-
imation to more grammatically rich features,
such as whether the token under consideration
is a closed class word or a particular part of
speech. In total, 1258 features were used for
each token, and a window of 7 tokens was used,
so each classifier used exemplars with 8806 bi-
nary features. The Spanish training set con-
tained roughly 270k tokens, and about 33k la-
bels (see Table 1).
</bodyText>
<table confidence="0.99574775">
LOC PER ORG MISC
B 4913 4321 7390 2173
I 1891 3903 4992 3212
ALL 6804 8224 12382 5385
</table>
<tableCaption confidence="0.999894">
Table 1: Spanish training exemplars by class.
</tableCaption>
<bodyText confidence="0.977099">
The Dutch training data consists of roughly
220k tokens, and about 19k labels (see Table 2).
</bodyText>
<sectionHeader confidence="0.966539" genericHeader="method">
3 Learning Algorithm
</sectionHeader>
<bodyText confidence="0.99665075">
Many empirical approaches to information
extraction have examined, including hidden
Markov Models (Seymore et al., 1999), Maxi-
mum Entropy Models (McCallum et al., 2000),
</bodyText>
<table confidence="0.99830125">
LOC PER ORG MISC
B 3192 4734 2091 3319
I 466 2880 1183 1381
ALL 3658 7614 3274 4700
</table>
<tableCaption confidence="0.6973875">
Table 2: Dutch training exemplars by class.
and Support Vector Machines (Kudo and Mat-
</tableCaption>
<bodyText confidence="0.994270944444444">
sumoto, 2001). Support Vector Machines
(SVMs) are known to be good choices for sparse,
high-dimensional problems. A typical approach
is to minimize error by maximizing the margin
between two classes. This is done by identifying
a hyperplane that separates the classes; support
vectors of examples close to the other class are
used to define the separating hyperplane.
Linear kernels appear to be applicable to text
classification problems and we investigate their
use in this study; polynomials and radial-basis
functions are other popular choices. We used a
Java Native Interface (JNI) API with the SVM-
Light toolkit (Joachims, 2002). Various learn-
ing parameters can significantly affect the per-
formance of the resulting classifiers. With these
data, there are many more negative then posi-
tive examples, so it is appropriate to give higher
importance to learning the positive class. This
can be achieved by increasing the cost ratio pa-
rameter, J (we used J={1,3} for our models).
Multiclass SVMs are being developed, and these
might be better suited for this problem. Com-
bining Multiple Binary Classifiers To produce
a single label for each token, we first identi-
fied the set, S, of possible labels predicted by
the classifiers. If S was empty, the &amp;quot;0&amp;quot; la-
bel was assigned. Otherwise, the most frequent
category was determined (ORG, PER, LOC, or
MISC) and selected; if both a beginning and
inside tag were in S, the beginning tag was cho-
sen. More sophisticated methods are almost
certainly warranted; this scheme can produce
invalid sequences, for example, where an inside
tag might precede an initial one, such as: &amp;quot;Es-
tados I-ORG&amp;quot; followed by &amp;quot;Unidos B-ORG&amp;quot;.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999962875">
We developed our methods based on the Span-
ish training and test data files (esp.train and
esp.testa) and then utilized the same methods
and parameter settings when building classifiers
for the Dutch data. For reasons of symmetry we
did not wish to use the part-of-speech indicators
in the Dutch text; these were removed with a
simple awk script.
</bodyText>
<sectionHeader confidence="0.805916" genericHeader="method">
5 Influence of Learning Parameters
</sectionHeader>
<bodyText confidence="0.782560333333333">
When using both types of features and a linear
kernel SVM with J set to 1.0, the evaluation
script reported:
</bodyText>
<table confidence="0.999193333333333">
Spanish dev. Precision Recall F0-1
LOC 78.68% 43.45% 55.98
MISC 7.50% 0.67% 1.24
ORG 65.47% 48.41% 55.66
PER 77.43% 55.32% 64.53
Overall 71.11% 44.35% 54.63
</table>
<tableCaption confidence="0.999966">
Table 3: Results with J=1 for esp.testa
</tableCaption>
<bodyText confidence="0.997560333333333">
Clearly the MISC class has not been learned
well at all. Since there are so many more neg-
ative examples than positive ones, the machine
can focus on identifying only the easy positive
cases and still achieve low error. We have begun
examining the use of different learning param-
eters, in particular, the effects of changing the
cost ratio. Using J = 3.0, we observed some
improvement (see Table 4).
</bodyText>
<table confidence="0.997190333333333">
Spanish dev. Precision Recall F0-1
LOC 58.36% 59.90% 59.12
MISC 20.00% 9.44% 12.82
ORG 52.90% 65.53% 58.54
PER 68.13% 67.51% 67.82
Overall 56.65% 59.08% 57.84
</table>
<tableCaption confidence="0.999833">
Table 4: Improvement when J=3 was used
</tableCaption>
<bodyText confidence="0.9999383125">
In this case, the MISC category is still diffi-
cult to learn. On the whole, recall is enhanced
in the other categories, but at a cost in preci-
sion.
Many of the mistakes that the system makes
are due to mis-tagging an entity; that is, an
entity is labeled with one tag when another
was appropriate. For example, distinctions were
made in the test set where the country Brazil
could refer to either an organization (e.g., the
government of Brazil) or a location, depend-
ing on context; these can be difficult to learn.
When the problem is reduced to simply tagging
a word or not, we see much better performance.
This suggests that additional effort to establish
proper tags would be beneficial.
</bodyText>
<sectionHeader confidence="0.4464435" genericHeader="method">
6 Contribution of Common Word
Features
</sectionHeader>
<bodyText confidence="0.995029375">
To assess the benefit from using common words
as features, we attempted the task using only
the basic orthographic and punctuation fea-
tures. As expected, performance was worse
when these features were not included (Table
5). We selected 1000 features expecting that
this number would be sufficient to include most
closed class words.
</bodyText>
<table confidence="0.998652666666667">
Spanish dev. Precision Recall F0-1
LOC 47.80% 61.73% 53.88
MISC 11.52% 14.83% 12.97
ORG 48.35% 62.06% 54.35
PER 60.43% 71.36% 65.44
Overall 47.55% 59.77% 52.96
</table>
<tableCaption confidence="0.855028333333333">
Table 5: Baseline performance when common
words were not used as features. Compare with
the results in Table 4.
</tableCaption>
<sectionHeader confidence="0.908212" genericHeader="method">
7 Chunk Detection
</sectionHeader>
<bodyText confidence="0.99997275">
In an attempt to improve overall performance,
we explored whether separate classifiers could
be combined for different portions of the prob-
lem. We trained a classifier to detect when a
given token should be labeled at all, and used all
labeled tokens as positive examples and the un-
labelled tokens as negative examples. We then
used a second processing phase where the deci-
sion problem was to identify the correct label,
given that some label should be assigned. The
problem of detecting chunks is far simpler as is
revealed in the following table:
</bodyText>
<table confidence="0.9906495">
Spanish dev. Precision Recall F0-1
J = I 85.97% 85.75% 85.86
J = 3 84.91% 88.32% 86.58
Basic features 83.39% 86.90% 85.11
</table>
<tableCaption confidence="0.973854">
Table 6: Accuracy on the chunk detection sub-
problem (esp.testa). Common word features
contribute very little.
</tableCaption>
<bodyText confidence="0.99961595">
Our hope was that separation of the two
problems would lead to better overall perfor-
mance. Using a J = 3.0 classifier to iden-
tify tokens which should be labeled, we then
made a decision about the correct label type
(LOC/MISC/ORG/PER), and then made as-
signments for beginning or internal tags. We
also changed our method for combining multi-
ple binary classifiers. Here, when it was deter-
mined that a token should be labeled, we sim-
ply picked the class whose model produced the
highest score.
Little change was seen using this new ap-
proach; recall was enhanced, but precision
dropped slightly (see Table 7 below). This is
the method that we then applied to the Dutch
data set. Performance was very similar across
the two languages; however, the MISC class was
apparently much easier to learn in Dutch as can
be seen in the tables on the right.
</bodyText>
<table confidence="0.998209">
Spanish dev. Precision Recall F0-1
LOC 51.97% 67.11% 58.57
MISC 17.67% 23.15% 20.04
ORG 54.46% 64.94% 59.24
PER 65.73% 74.71% 69.93
Overall 52.75% 63.90% 57.80
</table>
<tableCaption confidence="0.9571545">
Table 7: Results when classes were assigned
subsequent to chunk detection
</tableCaption>
<sectionHeader confidence="0.991009" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999774529411765">
The approach we investigated relied on a min-
imum of language-specific support. Only some
knowledge of the character encoding (e.g.,
ASCII features) and a list of common terms
were utilized. We believe simple techniques con-
tinue to show promise, but that there is room
for significant improvement. It may be possi-
ble to refine this approach and achieve higher
performance while retaining a language-neutral
focus. But it may be instead, that a large drop
in accuracy will result from avoiding linguis-
tic resources such as morphological analyzers,
gazetteers, and POS-taggers.
We also found that the values of learning pa-
rameters are important; this agrees with results
reported by Joachims on a text classification
problem (Joachims, 2000).
</bodyText>
<sectionHeader confidence="0.996948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.939683">
S. Cucerzan and D. Yarowsky. 1999. In Joint SIG-
DAT Conference on EMNLP and VLC.
</reference>
<table confidence="0.994805">
Spanish dev. Precision Recall F0-1
LOC 51.97% 67.11% 58.57
MISC 17.67% 23.15% 20.04
ORG 54.46% 64.94% 59.24
PER 65.73% 74.71% 69.93
Overall 52.75% 63.90% 57.80
Spanish test Precision Recall F0-1
LOC 63.54% 63.19% 63.37
MISC 15.11% 16.18% 15.62
ORG 56.39% 72.79% 63.55
PER 63.53% 82.72% 71.87
Overall 56.28% 66.51% 60.97
Dutch devel. Precision Recall F0-1
LOC 62.86% 64.71% 63.77
MISC 58.55% 66.09% 62.09
ORG 55.26% 39.47% 46.05
PER 49.45% 75.99% 59.91
Overall 55.32% 61.59% 58.29
Dutch test Precision Recall F0-1
LOC 66.58% 66.67% 66.62
MISC 58.67% 63.27% 60.88
ORG 54.72% 44.24% 48.93
PER 50.18% 75.91% 60.42
Overall 56.22% 63.24% 59.52
</table>
<tableCaption confidence="0.9187265">
Table 8: Our results for the four CoNLL-2002
Shared Task test sets.
</tableCaption>
<reference confidence="0.996053666666667">
T. Joachims. 2000. Estimating the generalization
performance of a SVM efficiently.
T. Joachims. 2002. http://svmlight.joachims.org/.
T. Kudo and Y. Matsumoto. 2001. Chunking with
support vector machines. In NAACL-2001.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum entropy markov models for information
extraction and segmentation. In ICML-2000.
P. McNamee, J. Mayfield, and C. Piatko. 2001. A
language-independent approach to european text
retrieval. In CLEF-2000 Workshop.
K. Seymore, A. McCallum, and R. Rosenfeld.
1999. Learning hidden markov model structure
for information extraction. In Proceedings of the
AAAI-99 Workshop on ML for IE.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237209">
<title confidence="0.999924">Entity Extraction Without Language-Specific Resources</title>
<author confidence="0.999972">Paul McNamee James Mayfield</author>
<affiliation confidence="0.997104">The Johns Hopkins University Applied Physics Laboratory</affiliation>
<address confidence="0.978567">Johns Hopkins Road, Laurel, Maryland 20723-6099</address>
<email confidence="0.987289">mcnameeAjhuapl.edu</email>
<email confidence="0.987289">mayfieldAjhuapl.edu</email>
<abstract confidence="0.965162352941176">a named-entity tagging system that requires minimal linguistic knowledge and thus may be applied to new target languages without significant adaptation. To maintain a languageneutral posture, the system is linguistically nave, and in fact, reduces the tagging problem to supervised machine learning. A large number of binary features are extracted from labeled data to train classifiers and computationally expensive features are eschewed. We have initially focused our attention on linear support vectors machines (SVMs); SVMs are known to work well when a large number of features is used as long as the individual vectors are sparse. We call our system SNOOD (Hopkins APL Inductive Retargetable Named Entity</abstract>
<intro confidence="0.513262">Tagger).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on EMNLP and VLC.</booktitle>
<contexts>
<context position="2196" citStr="Cucerzan and Yarowsky, 1999" startWordPosition="316" endWordPosition="319">and Swedish. We have found that a system based on weakmethods can achieve high retrieval effectiveness, both monolingually and translingually, in all of these languages. HAIRCUT uses a statistical language model of retrieval and simple tokenization methods such as unnormalized word forms and character n-grams. We approached the CoNLL- 2002 shared task in LanguageIndependent Named Entity Recognition with the same philosophy of attempting to maximize performance with judicious use of simple, language-neutral methods; the promise of such approaches has previously been demonstrated for this task (Cucerzan and Yarowsky, 1999). 2 Approach Our initial approach was to cast the labeling problem as a binary decision problem of determining whether the current token belongs to one of the eight classes (e.g., B-LOC). Thus we built eight individual classifiers. The algorithm, features, and learning parameters were identical for each classifier. For each, we used a linear kernel support vector machine that was trained using all positive and negative exemplars from the training set. The labels for this problem are mutually exclusive of one another, so we applied a rudimentary deterministic method to select a single label whe</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>S. Cucerzan and D. Yarowsky. 1999. In Joint SIGDAT Conference on EMNLP and VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Estimating the generalization performance of a SVM efficiently.</title>
<date>2000</date>
<journal>T. Joachims.</journal>
<booktitle>In NAACL-2001.</booktitle>
<marker>Joachims, 2000</marker>
<rawString>T. Joachims. 2000. Estimating the generalization performance of a SVM efficiently. T. Joachims. 2002. http://svmlight.joachims.org/. T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In NAACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In ICML-2000.</booktitle>
<contexts>
<context position="5390" citStr="McCallum et al., 2000" startWordPosition="856" endWordPosition="859">sed for each token, and a window of 7 tokens was used, so each classifier used exemplars with 8806 binary features. The Spanish training set contained roughly 270k tokens, and about 33k labels (see Table 1). LOC PER ORG MISC B 4913 4321 7390 2173 I 1891 3903 4992 3212 ALL 6804 8224 12382 5385 Table 1: Spanish training exemplars by class. The Dutch training data consists of roughly 220k tokens, and about 19k labels (see Table 2). 3 Learning Algorithm Many empirical approaches to information extraction have examined, including hidden Markov Models (Seymore et al., 1999), Maximum Entropy Models (McCallum et al., 2000), LOC PER ORG MISC B 3192 4734 2091 3319 I 466 2880 1183 1381 ALL 3658 7614 3274 4700 Table 2: Dutch training exemplars by class. and Support Vector Machines (Kudo and Matsumoto, 2001). Support Vector Machines (SVMs) are known to be good choices for sparse, high-dimensional problems. A typical approach is to minimize error by maximizing the margin between two classes. This is done by identifying a hyperplane that separates the classes; support vectors of examples close to the other class are used to define the separating hyperplane. Linear kernels appear to be applicable to text classification</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In ICML-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>J Mayfield</author>
<author>C Piatko</author>
</authors>
<title>A language-independent approach to european text retrieval.</title>
<date>2001</date>
<booktitle>In CLEF-2000 Workshop.</booktitle>
<contexts>
<context position="1301" citStr="McNamee et al., 2001" startWordPosition="187" endWordPosition="191"> eschewed. We have initially focused our attention on linear support vectors machines (SVMs); SVMs are known to work well when a large number of features is used as long as the individual vectors are sparse. We call our system SNOOD (Hopkins APL Inductive Retargetable Named Entity Tagger). 1 Introduction Text processing research at the Johns Hopkins University Applied Physics Laboratory (JHU/APL) has focused on information retrieval tasks. We have built a languageindependent retrieval engine, HAIRCUT, to explore effective, knowledge-light approaches to the multilingual text retrieval problem (McNamee et al., 2001). Through participation in cross- language evaluation conferences like TREC, CLEF, and NTCIR, we have evaluated retrieval effectiveness over document collections containing Arabic, Chinese, Dutch, English, Finnish, French, German, Italian, Japanese, Korean, Spanish, and Swedish. We have found that a system based on weakmethods can achieve high retrieval effectiveness, both monolingually and translingually, in all of these languages. HAIRCUT uses a statistical language model of retrieval and simple tokenization methods such as unnormalized word forms and character n-grams. We approached the CoN</context>
</contexts>
<marker>McNamee, Mayfield, Piatko, 2001</marker>
<rawString>P. McNamee, J. Mayfield, and C. Piatko. 2001. A language-independent approach to european text retrieval. In CLEF-2000 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seymore</author>
<author>A McCallum</author>
<author>R Rosenfeld</author>
</authors>
<title>Learning hidden markov model structure for information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the AAAI-99 Workshop on ML for IE.</booktitle>
<contexts>
<context position="5342" citStr="Seymore et al., 1999" startWordPosition="848" endWordPosition="851"> part of speech. In total, 1258 features were used for each token, and a window of 7 tokens was used, so each classifier used exemplars with 8806 binary features. The Spanish training set contained roughly 270k tokens, and about 33k labels (see Table 1). LOC PER ORG MISC B 4913 4321 7390 2173 I 1891 3903 4992 3212 ALL 6804 8224 12382 5385 Table 1: Spanish training exemplars by class. The Dutch training data consists of roughly 220k tokens, and about 19k labels (see Table 2). 3 Learning Algorithm Many empirical approaches to information extraction have examined, including hidden Markov Models (Seymore et al., 1999), Maximum Entropy Models (McCallum et al., 2000), LOC PER ORG MISC B 3192 4734 2091 3319 I 466 2880 1183 1381 ALL 3658 7614 3274 4700 Table 2: Dutch training exemplars by class. and Support Vector Machines (Kudo and Matsumoto, 2001). Support Vector Machines (SVMs) are known to be good choices for sparse, high-dimensional problems. A typical approach is to minimize error by maximizing the margin between two classes. This is done by identifying a hyperplane that separates the classes; support vectors of examples close to the other class are used to define the separating hyperplane. Linear kernel</context>
</contexts>
<marker>Seymore, McCallum, Rosenfeld, 1999</marker>
<rawString>K. Seymore, A. McCallum, and R. Rosenfeld. 1999. Learning hidden markov model structure for information extraction. In Proceedings of the AAAI-99 Workshop on ML for IE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>