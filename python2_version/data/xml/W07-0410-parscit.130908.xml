<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000627">
<title confidence="0.9967835">
A Walk on the Other Side:
Adding Statistical Components to a Transfer-Based Translation System
</title>
<author confidence="0.968629">
Ariadna Font Llitjós
</author>
<affiliation confidence="0.97942">
Carnegie Mellon University
</affiliation>
<address confidence="0.897715">
5000 Forbes Ave.
Pittsburgh, PA, 15213
</address>
<email confidence="0.999748">
aria@cs.cmu.edu
</email>
<sectionHeader confidence="0.993921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999601333333333">
This paper seeks to complement the cur-
rent trend of adding more structure to Sta-
tistical Machine Translation systems, by
exploring the opposite direction: adding
statistical components to a Transfer-Based
MT system. Initial results on the BTEC
data show significant improvement ac-
cording to three automatic evaluation
metrics (BLEU, NIST and METEOR).
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999210666666667">
In recent years the machine translation research
community has seen a remarkable paradigm shift.
It is not the first one, but it has been a very dra-
matic one: statistical machine translation has taken
the center stage. Conferences like ACL or HLT are
virtually flooded with papers on various flavors of
SMT. In international machine translation evalua-
tion like NIST (NIST MT Evaluation), TC-Star
(TC-STAR Evaluation) or IWSLT (IWSLT 2006)
evaluations, most participating systems are SMT
systems, with a few Example-Based systems sprin-
kled in. Rule-Based systems seem to have for the
most part disappeared. There may be many reasons
for this paradigm shift. One obvious reason is the
comparable ease, which with data-driven systems
can be built once some parallel data is available.
Another reason is that the performance of statisti-
cal translation systems has dramatically improved
over the last 5 to 10 years.
Does this mean that work on grammar-based
systems should be stopped? Should all the insight
into the structure of languages be neglected? This
might be too drastic a reaction. Actually, now that
SMT has reached some maturity, we see several
</bodyText>
<author confidence="0.61865">
Stephan Vogel
</author>
<affiliation confidence="0.631942">
Carnegie Mellon University
</affiliation>
<address confidence="0.615456">
5000 Forbes Ave.
Pittsburgh, PA, 15213
</address>
<email confidence="0.994944">
vogel+@cs.cmu.edu
</email>
<bodyText confidence="0.9999035">
attempts to integrate more structure into these sys-
tems, ranging from simple hierarchical alignment
models (Wu 1997, Chiang 2005) to syntax-based
statistical systems (Yamada and Knight 2001,
Zollmann and Venugopal 2006). What can tradi-
tional Rule-Based translation systems learn from
these approaches? And would it not make sense to
work from both sides towards that common goal:
structurally rich statistical translation models. In
this paper we study some enhancements for a
Transfer-Based translation system, using tech-
niques and even components developed for statisti-
cal machine translation. While the core engine
remains virtually untouched, additional features are
added to re-score the n-best list generated by the
transfer engine. Statistical alignment techniques
are used to lower the burden in building a lexicon
for a new domain. Minimum error rate training is
used to optimize the system. We show that this
leads to significant improvements in performance.
</bodyText>
<sectionHeader confidence="0.996997" genericHeader="introduction">
2 A Transfer-Based Translation System
</sectionHeader>
<subsectionHeader confidence="0.999655">
2.1 The Lexicon and Grammar
</subsectionHeader>
<bodyText confidence="0.9999654">
In our Rule-Based MT (RBMT) system, translation
rules include parsing, transfer, and generation in-
formation, similar to the modified transfer ap-
proach used in the early Metal system (Hutchins
and Somers, 1992).
The initial lexicon (479 entries) and grammar
(40 rules) used in our experiments were manually
written to cover the syntactic structures and the
vocabulary of the first 400 sentences of the
AVENUE Elicitation Corpus (Probst et al 2001).
The Elicitation Corpus contains sets of minimal
pairs in English and it was designed to cover a va-
riety of linguistic phenomena. Building these two
language-dependent components took a computa-
tional linguist 2-3 months. Figures 1 and 2 show
</bodyText>
<page confidence="0.983694">
72
</page>
<note confidence="0.5350035">
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 72–79,
Rochester, New York, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9596735">
examples of a translation rules in the grammar and
the lexicon.
</bodyText>
<equation confidence="0.9935803">
{S,4}
S::S : [NP VP] -&gt; [NP VP]
( (X1::Y1) (X2::Y2)
(x0 = x2)
((y2 subj) = -)
((y1 case) = nom)
((y1 agr) = (x1 agr))
((y2 tense) = (x2 tense))
((y2 agr pers) = (y1 agr pers))
((y2 agr num) = (y1 agr num)) )
</equation>
<figureCaption confidence="0.795057">
Figure 1: English4Spanish translation rule with
agreement constraints for subject (NP) and verb
(VP).
</figureCaption>
<equation confidence="0.999678">
V::V |: [&amp;quot;prefer&amp;quot;] -&gt; [&amp;quot;prefiero&amp;quot;]
((X1::Y1)
((x0 form) = prefer)
((x0 tense) = pres)
((y0 agr pers) = 1)
((y0 agr num) = sg))
</equation>
<figureCaption confidence="0.989993">
Figure 2: English4Spanish lexical entry for the
verb “prefer”.
</figureCaption>
<subsectionHeader confidence="0.997293">
2.2 Refined MT System
</subsectionHeader>
<bodyText confidence="0.9999778">
The original grammar and lexicon were automati-
cally improved with an Automatic Rule Refiner,
guided by a few bilingual speaker corrections
(Font Llitjós &amp; Ridmann 2007). In this approach,
automatic refinements only affect the target lan-
guage side of translation rules, namely transfer and
generation information.
The refined MT system used in our experiments
is the result of adding 30 agreement constraints to
the grammar rules, which makes the grammar
tighter (leading to an increase in precision), as well
as adding three new rules to cover new syntactic
structures and five lexical entries for new senses
and forms of existing words (leading to an increase
in recall).
</bodyText>
<subsectionHeader confidence="0.999844">
2.3 The Transfer Engine
</subsectionHeader>
<bodyText confidence="0.999914">
The Transfer Engine, or Xfer engine for short,
combines the translation grammar and lexicon in
order to produce translations of a source language
sentence into a target language. The Xfer engine
incorporates the three main processes involved in
Transfer-based MT: parsing of the source language
input, transfer of the parsed constituents of the
source sentence to their corresponding structured
constituents on the target language side, and gen-
eration of the target sentence.
The currently implemented algorithm is similar
to bottom-up chart parsing as described for exam-
ple in Allen (1995). A chart is first populated with
all constituent structures that were created in the
course of parsing the source language sentence
with the source-side portion of the transfer gram-
mar. Transfer and generation are applied to each
constituent entry. The transfer rules associated
with each entry in the chart are used in order to
determine the corresponding constituent structure
on the target language side. At the word level, lexi-
cal transfer rules are used in order to get the differ-
ent lexical choices.
Often, no parse for the entire source sentence
can be found. Partial parses are concatenated se-
quentially to generate complete translations.
In the current version of the Xfer system, the
output can be a first-best translation or a n-best list,
which can be used for additional n-best list rescor-
ing. The alternatives arise from lexical ambiguity
and multiple synonymous choices for lexical items
in the dictionary, but also from syntactic ambiguity
and multiple competing hypotheses from the
grammar.
For our experiments, we used version 3 of the
Xfer engine. An older version of the Xfer engine is
described in detail in Peterson (2002).
</bodyText>
<subsectionHeader confidence="0.999069">
2.4 Ranking Translations
</subsectionHeader>
<bodyText confidence="0.999863">
The Xfer engine can generate multiple translations.
This requires a quality score to be assigned to all
the alternatives. Based on these scores, the 1-best
translation will be selected by the system.
</bodyText>
<subsectionHeader confidence="0.955194">
Fragmentation Penalty
</subsectionHeader>
<bodyText confidence="0.999988888888889">
In the original Xfer system the only score used to
rank translation alternatives was a heuristic frag-
mentation penalty. The fragmentation penalty is
essentially the number of different chunks (rules or
lexical entries not embedded in another rule) that
span the whole translation. The intuition behind
this score is that the more partial parses are neces-
sary to span the entire sentence the less likely the
resulting translation will be a good one.
</bodyText>
<subsectionHeader confidence="0.832289">
N-gram LM
</subsectionHeader>
<bodyText confidence="0.99891075">
The fragmentation feature is rather weak. It does
not distinguish between words which are more
likely to be seen in the target language and words
which are less likely to be used. To generate sen-
</bodyText>
<page confidence="0.993439">
73
</page>
<bodyText confidence="0.999799769230769">
tences which are not only grammatically correct,
but also use words and word sequences that are
more natural and more common, data-driven ma-
chine translation systems use a n-gram language
model. To get the same benefit in the Xfer system,
an n-gram LM has been integrated with the engine.
This has the advantage that in the case of prun-
ing, the LM score can be used to avoid pruning
good hypotheses, in addition to re re-rank the final
translations.
For our experiments, a suffix array language
model based on the SALM toolkit (Zhang &amp; Vo-
gel, 2006) is used.
</bodyText>
<subsectionHeader confidence="0.902224">
Length Model
</subsectionHeader>
<bodyText confidence="0.999994">
To adjust for the length of the translations gener-
ated by the system, the difference between the
number of words generated and the expected num-
ber of words is added as a very simple feature. The
expected length is calculated by multiplying the
source sentence length by the ratio of the number
of target and source words in the training corpus.
The effect of this feature is to balance globally the
length of the translations.
</bodyText>
<subsectionHeader confidence="0.993147">
2.5 Pruning
</subsectionHeader>
<bodyText confidence="0.999986625">
To deal with the combinatorial explosion during
the parsing/translation process, pruning has to be
applied. Only the n top-ranking hypotheses are
kept in each cell of the chart. The ranking of these
partial translations is based on their language
model score, which at this time is only an ap-
proximation, as the true history has not been seen
and cannot be taken into account.
</bodyText>
<sectionHeader confidence="0.6382025" genericHeader="method">
3 Building a Xfer System for a New Do-
main
</sectionHeader>
<bodyText confidence="0.999969090909091">
A major bottleneck in developing a RBMT system
for a new translation task (a new language pair or a
new domain) is writing the grammar and building
the lexicon. Automatic grammar induction using
statistical alignments has been studied in (Probst
2005).
Here, we start with an existing grammar and
augment the baseline lexicon with entries to cover
the new domain. We explore semi-automatic lexi-
con generation for fast adaptation to the travel do-
main (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999752">
3.1 Test Data: BTEC Corpus
</subsectionHeader>
<bodyText confidence="0.999963133333333">
For initial evaluation on unseen data, we selected
the Basic Travel Expression Corpus (BTEC)
(Takezawa et al. 2002), which has been used in the
evaluation campaigns in connection with the Inter-
national Workshop on Spoken Language Transla-
tion (IWSLT 2006). Besides still being currently
used to build real systems (Shimizu et al. 2006;
Nakamura, et al. 2006), this corpus contains rela-
tively simple sentences that are comparable to the
ones initially corrected by users, and which are
covered by the baseline manual grammar.
As our test set, we used 506 English sentences
for which two sets of Spanish reference transla-
tions were available. Table 1 shows corpus statis-
tics for the BTEC data.
</bodyText>
<table confidence="0.999105333333333">
Data English
Sentences Pairs 123,416
Train Sentence Length 7.3
BTEC Word Tokens 903,525
Word Types 12,578
Sentence Pairs 506
Test Word Tokens 3,764
Word Types 776
Coverage Test 756 (97%)
</table>
<tableCaption confidence="0.999587">
Table 1: Corpus Statistics for the BTEC corpus
</tableCaption>
<subsectionHeader confidence="0.9541375">
3.2 Semi-Automatic Generation of the
Transfer Lexicon
</subsectionHeader>
<bodyText confidence="0.999969473684211">
The Transfer-Based system relies on a lexicon that
contains POS, gender and number agreement,
among other linguistic features. To adjust the sys-
tem quickly to a new task, we decided to leverage
from statistical alignment models to generate word
and phrase alignments as candidates for the trans-
fer lexicon.
In the first step, we trained statistical lexicons
using the well-known IBM1 word alignment
model: one for the directions Spanish to English,
and one for the direction English to Spanish. As
multi-word entries, are often needed ([valuables]
4 [objetos de valor], [reception desk] 4[recep-
ción], [air conditioner]4[aire acondicionado]), we
used phrase alignment techniques to create transla-
tion candidates for words and 2-word phrases. The
phrase alignment also generates multi-word trans-
lations for single source words. With reasonably
tight pruning, a manageable phrase translation ta-
</bodyText>
<page confidence="0.993544">
74
</page>
<bodyText confidence="0.99991225">
ble was generated. This first step took about 5
hours.
The next step, manually cleaning the translation
table, annotating them with parts-of-speech, and
with agreement and tense constraints, was initially
restricted to those items that overlapped with the
vocabulary of our development test set, and took
two days.
The statistically generated lexicon comprises
1,248 lexical entries, whereas the initial manual
lexicon contained 479 lexical entries. For our
BTEC experiments, we combined both lexicons.
</bodyText>
<subsectionHeader confidence="0.992525">
3.3 Xfer Results with No Ranking
</subsectionHeader>
<bodyText confidence="0.999834375">
To determine how the Xfer system would perform
only on the basis of the lexicon and grammar, we
ran one translation experiment in which no lan-
guage model was used. This experiment was also
intended to see if the refined grammar would lead
to better translations. We took the first-best transla-
tion output by the system without using any statis-
tical components to rank alternative translations.
</bodyText>
<table confidence="0.998378">
System METEOR BLEU NIST
Baseline 0.5666 0.2745 5.88
Refined 0.5676 0.2559 5.62
</table>
<tableCaption confidence="0.970376">
Table 2: Automatic metric scores for a purely
Rule-Based MT System.
</tableCaption>
<bodyText confidence="0.991746157894737">
Table 2 shows that, in this crude setting, differ-
ent automatic metrics do not agree on the transla-
tion accuracy of both systems. On one hand,
METEOR (Lavie et al. 2004), which has been
shown to correlate well with human judgments
(Snover et al. 2006), indicates that the refined sys-
tem outperforms the baseline system (as measured
by the latest version v0.5.1,). On the other hand,
both BLEU (Papineni et al., 2002) and NIST
(Doddington 2002) scores are higher for the base-
line system (mteval-v11b.pl).
However, human inspection revealed that the re-
fined grammar is able to augment the n-best list
with correct translations that the baseline system
was not able to generate. This suggests that these
results reflect poor re-ranking and not n-best list
quality. In the next section, we describe an oracle
experiment to measure n-best list quality of both
systems.
</bodyText>
<subsectionHeader confidence="0.95311">
3.4 Oracle Experiment
</subsectionHeader>
<bodyText confidence="0.982003266666667">
Oracle scores provide an upper-bound in perform-
ance. For the BTEC test set, we approximated a
human oracle by calculating automatic metric
scores for METEOR and for BLEU and NIST.
Given 100-best lists for each source language
sentence, we selected the best translation hypothe-
sis for each automatic metric separately.
These scores reflect the fact that automatic re-
finements are able to feed the n-best list with better
translations, as evulated by comparison against
human reference translations. Even with a small set
of independent user corrections, the refined system
shows potential improved translation quality as
indicated by higher scores for all three automatic
evaluation metrics in Table 3.
</bodyText>
<table confidence="0.998676666666667">
System METEOR BLEU NIST
Baseline 0.6863 0.4068 7.42
Refined 0.6954 0.4215 7.51
</table>
<tableCaption confidence="0.9791085">
Table 3: Automatic metric oracle scores based on a
100-best list
</tableCaption>
<bodyText confidence="0.993658666666667">
Moreover, oracle scores provide the margin that
we can gain when improving on the re-ranking of
the n-best list produced by the Xfer engine.
</bodyText>
<subsectionHeader confidence="0.824603">
3.5 Xfer Results with Initial Ranking
</subsectionHeader>
<bodyText confidence="0.985031166666667">
As expected, when the Xfer system is run in com-
bination with a LM1 as well as the fragmentation
penalty, automatic metric scores for the 1-best hy-
pothesis are significantly higher (Table 4), than
when just using the first translation output by the
Xfer system alone (Table 2).
</bodyText>
<table confidence="0.998211">
System METEOR BLEU NIST
Baseline 0.6176 0.3425 6.53
Refined 0.6222 0.3513 6.56
</table>
<tableCaption confidence="0.9863875">
Table 4: Automatic metric scores for 1-best de-
coder hypothesis.
</tableCaption>
<bodyText confidence="0.999670142857143">
These results are lower than the oracle scores for
both the baseline and the refined system (Table 3),
which is also to be expected. However, the impor-
tant thing to notice from these results is that, like in
the oracle case, the refined system consistently
outperforms the baseline MT system for all three
automatic metrics.
</bodyText>
<footnote confidence="0.940329">
1 The Suffix Array Language Model (SALM) was built using
the 123,416 Spanish sentences from the training data.
</footnote>
<page confidence="0.999245">
75
</page>
<bodyText confidence="0.999969571428571">
The difference between the baseline and the re-
fined system in terms of 1-best scores is slightly
smaller than the difference between oracle scores,
which means that the decoder can not fully lever-
age the improvements made in the grammar. This
indicates that the decoder fails to select the best
translation in most cases.
</bodyText>
<sectionHeader confidence="0.9705915" genericHeader="method">
4 Adding Statistical Components to a Re-
Ranker
</sectionHeader>
<bodyText confidence="0.999962461538462">
The information used in the Xfer system to rank
alternative translations is limited. Essentially, it is
the n-gram LM, which is the most important com-
ponent, a simple sentence length model, and the
fragmentation score, which measures if a com-
pletely spanning parse could be found or if the
translation is glued together from partial parses.
Given an n-best list of translations for each source
sentence, we can apply additional models to re-
rank these n-best list, hopefully pushing more good
translations into the first rank. We studied the ef-
fect of adding different features to the n-best lists:
lexical features and rule (type) probability features.
</bodyText>
<subsectionHeader confidence="0.94924">
4.1 Word-To-Word Probabilities
</subsectionHeader>
<bodyText confidence="0.995598166666667">
In SMT systems, rescoring with an IBM1 model-
like word alignment score has become a standard
feature. We use two word-to-word lexicons (Eng-
lish4Spanish and Spanish4English) to calculate
sentence translation probabilities, based on word-
to-word probabilities:
</bodyText>
<equation confidence="0.98936575">
P(e  |s) = 1 ∏ ∑ p(ei  |s j) Eq.1
JI
and:
P(s  |e) = IJ ∏ ∑ p(sj  |ei) Eq.2
</equation>
<bodyText confidence="0.999806888888889">
Here, we denote the English words with e, the
Spanish words with s, the sentence lengths are
given by I and J. In the IBM1 alignment model,
the position alignment is a uniform distribution p( i
 |j ) = 1/I for Spanish to English and p( j  |i ) = 1/J
for English to Spanish. For Spanish to English, we
have the additional factor of (1/I)J, i.e. longer
translations get a smaller probability, and for En-
Sp we have (1/J)I, which again gives a bias to-
wards shorter translations. To compensate for this
bias, we use probabilities normalized to the sen-
tence length. Table 5 shows that adding the lexical
probabilities improves the 1-best translation score.
However, there is no significant difference when
using different normalization of the lexicon prob-
abilities. The length bias introduced by different
lexicon features can be balanced by the decoder’s
length feature.
</bodyText>
<table confidence="0.995729">
BLEU NIST
Refined 0.3513 6.56
+Lex Prob 0.3755 6.88
</table>
<tableCaption confidence="0.707303333333333">
Table 5: Comparing 1-best scores with scores
result of rescoring the n-best list with lexical fea-
tures.
</tableCaption>
<subsectionHeader confidence="0.963685">
4.2 Rule Probabilities
</subsectionHeader>
<bodyText confidence="0.999953476190476">
The Xfer MT system can display the derivation
tree showing the rules applied during translation.
This allows rescoring the translations with rule
probabilities. However, there is no annotated cor-
pus from which the rule probabilities could be es-
timated. As an approximation to such a training
corpus, we decided to run the Xfer system over the
training data and to generate n-best lists with trans-
lations and translation trees. Overall, about 6 mil-
lion parse trees were generated. Using this data to
estimate rule probabilities is definitely not ideal, as
the translation on the training data are far from per-
fect, especially as not all the vocabulary has so far
been added to the Xfer lexicon. By averaging over
all n-best translations a reasonable smoothing is to
be expected.
We used this information in three ways. We es-
timated conditional probabilities rule r given rule-
type R, i.e. the distribution over different VP rules
or NP rules. For each derivation D the overall
probability was then calculated as:
</bodyText>
<equation confidence="0.920059">
P(D) = ∏ p(r  |R) Eq. 3
</equation>
<bodyText confidence="0.984934666666667">
As an alternative, we just build n-gram language
models, one on the rule level and on the rule type
level:
</bodyText>
<equation confidence="0.999047">
P(D) = ∏ p(r  |r− n...r − 1) Eq. 4
P(D) = ∏ p(R  |R−n ...R−1) Eq. 5
</equation>
<bodyText confidence="0.999758666666667">
Overall, 1,685 different rules and 19 rule types
were seen in the training data. For models 2 and 3,
we used the suffix array LM once again to allow
for arbitrary long histories. Even though it often
backs-off to 3-gram, 2-gram or even unigram prob-
abilities.
</bodyText>
<page confidence="0.962491">
76
</page>
<bodyText confidence="0.998312666666667">
In Table 6, we can see the effect of adding these
LMs as additional features to the system and run-
ning MER training.
</bodyText>
<table confidence="0.9985795">
BLEU NIST
Refined 0.3513 6.56
Lex. Prob. 0.3755 6.88
Cond. Prob. 0.3728 6.81
Rule LM 0.3717 6.74
Rule Type LM 0.3736 6.78
</table>
<tableCaption confidence="0.975834">
Table 6: BLEU scores when rescoring the n-
</tableCaption>
<bodyText confidence="0.508235">
best list with different rule probability features (as
well as the n-gram LM).
</bodyText>
<sectionHeader confidence="0.96968" genericHeader="method">
5 MER Training
</sectionHeader>
<bodyText confidence="0.999946653846154">
Like in SMT systems, in the Xfer engine transla-
tions are ranked to their total cost, which is a
weighted linear combination of the individual
costs. When adding more features to the translation
system, a careful balancing of the individual con-
tributions can make a significant difference. How-
ever, with each feature added, manually tuning the
system becomes less and less practical, and auto-
matic optimization becomes necessary.
Different optimization techniques are available,
like the Simplex algorithm or the special Minimum
Error Training as described in (Och 2003). In
Minimum Error Rate (MER) training, the n-best
list generated by the translation system is used to
find feature weight, thereby re-ranking the n-best
list. This improves the match between the 1-best
translation and given reference translations. Opti-
mization can use any metric as objective function.
Typically, systems are tuned towards high BLEU
or high NIST scores, more recently also towards
METEOR or TER (Snover et al. 2006).
We used a MER training module (Venugopal),
originally developed for an SMT system, to run
MER training on the n-best lists generated by the
Xfer system. This implementation allows for opti-
mization towards BLEU and NIST mteval metrics.
</bodyText>
<subsectionHeader confidence="0.704431">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999961833333333">
In Table 7, we summarize some of the results from
different n-best list rescoring experiments. Using
only the Xfer engine, without language model,
gives a very low score, as the selection is based
only on the fragmentation score.
Adding the n-gram language model gives a huge
improvement. Adding additional features leads to
more then 2 BLEU points improvement. However,
there is not much difference when using different
feature combinations. It seems that the rather small
size of the n-best list is a limiting factor.
When setting the optimal weights in the Xfer
engine for the LM and fragmentation penalty
scores obtained from MER training, both the base-
line and the refined system get higher scores, not
only according to BLEU, which was used as the
objective function, but also according to METEOR
and NIST automatic evaluation metrics (Table 8).
</bodyText>
<table confidence="0.999695875">
System + Statistical Components 1-best
Rule Based Xfer 0.2559
+ Stat. Comp. Xfer + LM + Frag 0.3513
Optimizing POS LM 0.3180
weights
with
MER training
Rule Probabilities (Prob.) 0.2593
LM + Rule Type LM 0.3736
LM + Frag/Len + Rule Type LM 0.3737
LM + POS + Rule LM 0.3744
LM + Frag + Rule Type LM + Cond. Rule Prob. 0.3743
LM + Len + Rule Type LM + Cond. Rule Prob. 0.3745
LM + POS + Rule LM + Cond. Rule Prob. 0.3741
LM + Frag + Len + Rule Type LM + Rule Prob. 0.3746
LM + Frag + Len + POS + Rule LM + Rule Prob. 0.3741
</table>
<tableCaption confidence="0.991691">
Table 7: BLEU scores for the Refined MT System as the weights for the different statistical components
described in Section 2.4 and 4 are optimized with MER Training.
</tableCaption>
<page confidence="0.998536">
77
</page>
<bodyText confidence="0.99820525">
Moreover, the difference between the Baseline
and the Refined system after MER training is sta-
tistically significant2, whereas this was not the case
for the initial ranking results (Table 4).
</bodyText>
<table confidence="0.997291666666667">
System METEOR BLEU NIST
Baseline 0.6184 0.3609 6.68
Refined 0.6231 0.3780 6.79
</table>
<tableCaption confidence="0.953572">
Table 8: Automatic metric scores for 1-best de-
coder hypothesis, after LM and Fragmentation
weights have been optimized.
</tableCaption>
<bodyText confidence="0.996145583333333">
Table 9 shows a few examples from the BTEC cor-
pus with 1-best translations output by the Refined
MT system before (No Optimization) and after
(With Optimization) MER training, given LM and
Fragmentation penalty scores. From these exam-
ples, it can be observed that re-ranking improves
after optimizing the LM and fragmentation
weights. In particular, order issues get resolved
(examples 1, 2 and 4), which result in correct de-
terminer agreement (1 and 2); determiner insertion
(3); correct verb form (5 and 7) and omission of
incorrect pronouns (6 and 7).
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.995432310344828">
Starting from a Transfer-Based translation system,
we explored techniques currently used in statistical
translation systems to rapidly adapt to a new do-
main and to improve its performance. Using word
and phrase alignment techniques allowed us to
quickly augment the transfer lexicon. Adding a
statistical language model is crucial in selecting
good translations from the n-best lists generated by
the Xfer engine. Adding additional features, such
as word-to-word probabilities and rule (type) prob-
abilities, further improves performance.
While this information would ideally be used in
the parsing and transfer steps of the translation sys-
tem, our initial experiments were targeted at using
this in an n-best list rescoring setup. As rule prob-
abilities were estimated from noisy training data,
these models are far from optimal.
To facilitate the experiments with the Xfer sys-
tem, especially when adding more and more fea-
tures, we added a Minimum Error Rate training
2 According to the standard paired two-tailed t-Test, the de-
coder METEOR scores with optimized weights are statisti-
cally significant, with a p value of 0.0051.
component. Having such a component will defi-
nitely boost the development of the Xfer engine.
We see statistically significant improvements
over the baseline system when using optimized
weights for the word-level language model and the
fragmentation score.
</bodyText>
<table confidence="0.999702">
1 Source: where is the boarding gate ?
NO: dónde está el embarque puerta ?
WO: dónde está la puerta embarque ?
2 Src: where is the bus stop for city hall ?
NO: dónde está el autobús parada para ayuntamiento ?
WO: dónde está la parada autobús para ayuntamiento ?
3 Src: i would like a twin room with a bath please .
NO: me gustaría habitación una cama doble con un
baño por favor .
WO: me gustaría una habitación cama doble con un
baño por favor .
4 Src: i would like to buy some duty-free items .
NO: me gustaría comprar algunos duty-free productos.
WO: me gustaría comprar algunos artículos duty-free .
5 Src: does he speak japanese ?
NO: él hablar a japonés ?
WO: habla japonés ?
6 Src: it is just round the corner .
NO: lo es simplemente a la vuelta de la esquina .
WO: es simplemente a la vuelta de la esquina .
7 Src: do you sell duty-free items ?
NO: te venden artículos duty-free ?
WO: vendéis artículos duty-free ?
</table>
<tableCaption confidence="0.983254">
Table 9: 1-best translations from the BTEC test set
</tableCaption>
<bodyText confidence="0.5293015">
output by the Refined MT system before and after
MER training. NO stands for No Optimization of
LM and Fragmentation weights, and WO stands
for With Optimization of weights.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="acknowledgments">
7 Future Work
</sectionHeader>
<bodyText confidence="0.9999588">
Using rule probabilities has shown to be a promis-
ing extension to the current Xfer system. We plan
to improve these models by selecting the oracle
best translations from the n-best list generated on
the training data. This will reduce the noise in the
training stage. Ultimately, the rule probabilities
should be applied not as an n-best list rescoring
step, but directly in the Xfer engine decoder.
Analyzing the translation results, one important
shortcoming became obvious. Currently the trans-
lation lexicon only covers about 88% of the words
that appear in the reference translations. This se-
verely limits as to what kind of BLEU score we
can achieve. When we generated the phrasal lexi-
con from the BTEC training data, we deliberately
</bodyText>
<page confidence="0.992447">
78
</page>
<bodyText confidence="0.9998642">
chose to only include few alternatives, mainly to
limit the manual labor when adding POS and con-
straint. We expect that the Xfer system will sig-
nificantly benefit from further expanding the
lexicon.
</bodyText>
<sectionHeader confidence="0.996456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797227848101">
Allen, J. 1995. Natural Language Understanding. Sec-
ond Edition ed. Benjamin Cummings.
Chiang, D. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), Ann Arbor, USA.
Doddington G. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence sta-
tistics. In Proc. of the HLT 2002, San Diego, USA.
Hutchins, W. J., and H. L. Somers. 1992. An Introduc-
tion to Machine Translation. London: Academic
Press.
Font Llitjós, A. and W. Ridmann. 2007 The Inner
Works of an Automatic Rule Refiner for Machine
Translation. METIS-II Workshop, Leuven, Belgium.
IWSLT 2006: http://www.slt.atr.jp/IWSLT2006/
Lavie, A., K. Sagae and S. Jayaraman. 2004. The Sig-
nificance of Recall in Automatic Metrics for MT
Evaluation. AMTA, Washington DC, USA.
Nakamura, S., K. Markov, H. Nakaiwa, G. Kikui, H.
Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E.
Sumita, and S. Yamamoto. 2006. The ATR multilin-
gual speech-to-speech translation system. IEEE
Trans. on Audio, Speech, and Language Processing,
14, No.2:365–376.
NIST MT Evaluations:
http://www.nist.gov/speech/tests/mt/
Och, F. J. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan.
Papineni, K, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th ACL, Phila-
delphia, USA.
Peterson, E. 2002. Adapting a transfer engine for rapid
machine translation development. M.S. Thesis,
Georgetown University.
Probst, K., Brown, R., Carbonell, J., Lavie, A. Levin,
and L., Peterson, E., 2001. Design and Implementa-
tion of Controlled Elicitation for Machine Transla-
tion of Low density Languages. Proceedings of the
MT2001 workshop at MT Summit, Santiago de
Compostela, Spain.
SALM Toolkit:
http://projectile.is.cs.cmu.edu/research/public/tools/s
alm/salm.htm
Shimizu T., Y. Ashikari, E. Sumita, H. Kashioka and S.
Nakamura. 2006. Development of client-server
speech translation system on a multi-lingual speech
communication platform. IWSLT, Kyoto, Japan.
Snover, M; B. Dorr, R. Schwartz, L. Micciulla, 2006.
Targeted Human Annotation. AMTA, Boston, USA.
Takezawa, T, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto, 2002. Toward a Broad-Coverage Bi-
lingual Corpus for Speech Translation of Travel
Conversations in the Real World. In Proceedings of
3rd LREC, Las Palmas, Spain.
TC-STAR Evaluations: http://www.tc-star.org/
Venugopal, A.: MER Training Toolkit.
http://www.cs.cmu.edu/~ashishv/mer.html
Wu, D. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377–404.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting of the ACL, Toulouse, France.
Zhang, Y and S. Vogel. 2006. Suffix Array and its Ap-
plications in Empirical Natural Language Process-
ing,. Technical Report CMU-LTI-06-010, Pittsburgh
PA, USA.
Zhang, Y, A. S. Hildebrand and S. Vogel. 2006. Dis-
tributed Language Modeling for N-best List Re-
ranking. Empirical Methods in Natural Language
Processing (EMNLP), Sydney, Australia.
Zollmann A. and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
Proc. of NAACL 2006 - Workshop on Statistical
Machine Translation, New York, USA.
</reference>
<page confidence="0.999046">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823666">
<title confidence="0.9983395">A Walk on the Other Side: Adding Statistical Components to a Transfer-Based Translation System</title>
<author confidence="0.999451">Ariadna Font</author>
<affiliation confidence="0.998072">Carnegie Mellon</affiliation>
<address confidence="0.998208">5000 Forbes Pittsburgh, PA, 15213</address>
<email confidence="0.996794">aria@cs.cmu.edu</email>
<abstract confidence="0.9823394">This paper seeks to complement the current trend of adding more structure to Statistical Machine Translation systems, by exploring the opposite direction: adding statistical components to a Transfer-Based MT system. Initial results on the BTEC data show significant improvement according to three automatic evaluation metrics (BLEU, NIST and METEOR).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>Natural Language Understanding. Second Edition</title>
<date>1995</date>
<editor>ed. Benjamin Cummings.</editor>
<contexts>
<context position="5622" citStr="Allen (1995)" startWordPosition="878" endWordPosition="879">3 The Transfer Engine The Transfer Engine, or Xfer engine for short, combines the translation grammar and lexicon in order to produce translations of a source language sentence into a target language. The Xfer engine incorporates the three main processes involved in Transfer-based MT: parsing of the source language input, transfer of the parsed constituents of the source sentence to their corresponding structured constituents on the target language side, and generation of the target sentence. The currently implemented algorithm is similar to bottom-up chart parsing as described for example in Allen (1995). A chart is first populated with all constituent structures that were created in the course of parsing the source language sentence with the source-side portion of the transfer grammar. Transfer and generation are applied to each constituent entry. The transfer rules associated with each entry in the chart are used in order to determine the corresponding constituent structure on the target language side. At the word level, lexical transfer rules are used in order to get the different lexical choices. Often, no parse for the entire source sentence can be found. Partial parses are concatenated </context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>Allen, J. 1995. Natural Language Understanding. Second Edition ed. Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1952" citStr="Chiang 2005" startWordPosition="296" endWordPosition="297"> is available. Another reason is that the performance of statistical translation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfe</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, D. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. of the HLT 2002,</booktitle>
<location>San Diego, USA.</location>
<contexts>
<context position="12986" citStr="Doddington 2002" startWordPosition="2081" endWordPosition="2082"> alternative translations. System METEOR BLEU NIST Baseline 0.5666 0.2745 5.88 Refined 0.5676 0.2559 5.62 Table 2: Automatic metric scores for a purely Rule-Based MT System. Table 2 shows that, in this crude setting, different automatic metrics do not agree on the translation accuracy of both systems. On one hand, METEOR (Lavie et al. 2004), which has been shown to correlate well with human judgments (Snover et al. 2006), indicates that the refined system outperforms the baseline system (as measured by the latest version v0.5.1,). On the other hand, both BLEU (Papineni et al., 2002) and NIST (Doddington 2002) scores are higher for the baseline system (mteval-v11b.pl). However, human inspection revealed that the refined grammar is able to augment the n-best list with correct translations that the baseline system was not able to generate. This suggests that these results reflect poor re-ranking and not n-best list quality. In the next section, we describe an oracle experiment to measure n-best list quality of both systems. 3.4 Oracle Experiment Oracle scores provide an upper-bound in performance. For the BTEC test set, we approximated a human oracle by calculating automatic metric scores for METEOR </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>Doddington G. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. of the HLT 2002, San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Hutchins</author>
<author>H L Somers</author>
</authors>
<title>An Introduction to Machine Translation.</title>
<date>1992</date>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="3067" citStr="Hutchins and Somers, 1992" startWordPosition="461" endWordPosition="464">remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine. Statistical alignment techniques are used to lower the burden in building a lexicon for a new domain. Minimum error rate training is used to optimize the system. We show that this leads to significant improvements in performance. 2 A Transfer-Based Translation System 2.1 The Lexicon and Grammar In our Rule-Based MT (RBMT) system, translation rules include parsing, transfer, and generation information, similar to the modified transfer approach used in the early Metal system (Hutchins and Somers, 1992). The initial lexicon (479 entries) and grammar (40 rules) used in our experiments were manually written to cover the syntactic structures and the vocabulary of the first 400 sentences of the AVENUE Elicitation Corpus (Probst et al 2001). The Elicitation Corpus contains sets of minimal pairs in English and it was designed to cover a variety of linguistic phenomena. Building these two language-dependent components took a computational linguist 2-3 months. Figures 1 and 2 show 72 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 72–79, </context>
</contexts>
<marker>Hutchins, Somers, 1992</marker>
<rawString>Hutchins, W. J., and H. L. Somers. 1992. An Introduction to Machine Translation. London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Font Llitjós</author>
<author>A</author>
<author>W Ridmann</author>
</authors>
<title>The Inner Works of an Automatic Rule Refiner for Machine Translation. METIS-II Workshop,</title>
<date>2007</date>
<location>Leuven, Belgium. IWSLT</location>
<note>http://www.slt.atr.jp/IWSLT2006/</note>
<marker>Llitjós, A, Ridmann, 2007</marker>
<rawString>Font Llitjós, A. and W. Ridmann. 2007 The Inner Works of an Automatic Rule Refiner for Machine Translation. METIS-II Workshop, Leuven, Belgium. IWSLT 2006: http://www.slt.atr.jp/IWSLT2006/</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>K Sagae</author>
<author>S Jayaraman</author>
</authors>
<date>2004</date>
<booktitle>The Significance of Recall in Automatic Metrics for MT Evaluation. AMTA,</booktitle>
<location>Washington DC, USA.</location>
<contexts>
<context position="12712" citStr="Lavie et al. 2004" startWordPosition="2034" endWordPosition="2037">, we ran one translation experiment in which no language model was used. This experiment was also intended to see if the refined grammar would lead to better translations. We took the first-best translation output by the system without using any statistical components to rank alternative translations. System METEOR BLEU NIST Baseline 0.5666 0.2745 5.88 Refined 0.5676 0.2559 5.62 Table 2: Automatic metric scores for a purely Rule-Based MT System. Table 2 shows that, in this crude setting, different automatic metrics do not agree on the translation accuracy of both systems. On one hand, METEOR (Lavie et al. 2004), which has been shown to correlate well with human judgments (Snover et al. 2006), indicates that the refined system outperforms the baseline system (as measured by the latest version v0.5.1,). On the other hand, both BLEU (Papineni et al., 2002) and NIST (Doddington 2002) scores are higher for the baseline system (mteval-v11b.pl). However, human inspection revealed that the refined grammar is able to augment the n-best list with correct translations that the baseline system was not able to generate. This suggests that these results reflect poor re-ranking and not n-best list quality. In the </context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Lavie, A., K. Sagae and S. Jayaraman. 2004. The Significance of Recall in Automatic Metrics for MT Evaluation. AMTA, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nakamura</author>
<author>K Markov</author>
<author>H Nakaiwa</author>
<author>G Kikui</author>
<author>H Kawai</author>
<author>T Jitsuhiro</author>
<author>J Zhang</author>
<author>H Yamamoto</author>
<author>E Sumita</author>
<author>S Yamamoto</author>
</authors>
<title>The ATR multilingual speech-to-speech translation system.</title>
<date>2006</date>
<journal>IEEE Trans. on Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<pages>2--365</pages>
<contexts>
<context position="9951" citStr="Nakamura, et al. 2006" startWordPosition="1600" endWordPosition="1603">studied in (Probst 2005). Here, we start with an existing grammar and augment the baseline lexicon with entries to cover the new domain. We explore semi-automatic lexicon generation for fast adaptation to the travel domain (Section 3.2). 3.1 Test Data: BTEC Corpus For initial evaluation on unseen data, we selected the Basic Travel Expression Corpus (BTEC) (Takezawa et al. 2002), which has been used in the evaluation campaigns in connection with the International Workshop on Spoken Language Translation (IWSLT 2006). Besides still being currently used to build real systems (Shimizu et al. 2006; Nakamura, et al. 2006), this corpus contains relatively simple sentences that are comparable to the ones initially corrected by users, and which are covered by the baseline manual grammar. As our test set, we used 506 English sentences for which two sets of Spanish reference translations were available. Table 1 shows corpus statistics for the BTEC data. Data English Sentences Pairs 123,416 Train Sentence Length 7.3 BTEC Word Tokens 903,525 Word Types 12,578 Sentence Pairs 506 Test Word Tokens 3,764 Word Types 776 Coverage Test 756 (97%) Table 1: Corpus Statistics for the BTEC corpus 3.2 Semi-Automatic Generation of</context>
</contexts>
<marker>Nakamura, Markov, Nakaiwa, Kikui, Kawai, Jitsuhiro, Zhang, Yamamoto, Sumita, Yamamoto, 2006</marker>
<rawString>Nakamura, S., K. Markov, H. Nakaiwa, G. Kikui, H. Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E. Sumita, and S. Yamamoto. 2006. The ATR multilingual speech-to-speech translation system. IEEE Trans. on Audio, Speech, and Language Processing, 14, No.2:365–376.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NIST MT</author>
</authors>
<note>Evaluations: http://www.nist.gov/speech/tests/mt/</note>
<marker>MT, </marker>
<rawString>NIST MT Evaluations: http://www.nist.gov/speech/tests/mt/</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="20149" citStr="Och 2003" startWordPosition="3277" endWordPosition="3278"> well as the n-gram LM). 5 MER Training Like in SMT systems, in the Xfer engine translations are ranked to their total cost, which is a weighted linear combination of the individual costs. When adding more features to the translation system, a careful balancing of the individual contributions can make a significant difference. However, with each feature added, manually tuning the system becomes less and less practical, and automatic optimization becomes necessary. Different optimization techniques are available, like the Simplex algorithm or the special Minimum Error Training as described in (Och 2003). In Minimum Error Rate (MER) training, the n-best list generated by the translation system is used to find feature weight, thereby re-ranking the n-best list. This improves the match between the 1-best translation and given reference translations. Optimization can use any metric as objective function. Typically, systems are tuned towards high BLEU or high NIST scores, more recently also towards METEOR or TER (Snover et al. 2006). We used a MER training module (Venugopal), originally developed for an SMT system, to run MER training on the n-best lists generated by the Xfer system. This impleme</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. J. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th ACL,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="12959" citStr="Papineni et al., 2002" startWordPosition="2075" endWordPosition="2078">ny statistical components to rank alternative translations. System METEOR BLEU NIST Baseline 0.5666 0.2745 5.88 Refined 0.5676 0.2559 5.62 Table 2: Automatic metric scores for a purely Rule-Based MT System. Table 2 shows that, in this crude setting, different automatic metrics do not agree on the translation accuracy of both systems. On one hand, METEOR (Lavie et al. 2004), which has been shown to correlate well with human judgments (Snover et al. 2006), indicates that the refined system outperforms the baseline system (as measured by the latest version v0.5.1,). On the other hand, both BLEU (Papineni et al., 2002) and NIST (Doddington 2002) scores are higher for the baseline system (mteval-v11b.pl). However, human inspection revealed that the refined grammar is able to augment the n-best list with correct translations that the baseline system was not able to generate. This suggests that these results reflect poor re-ranking and not n-best list quality. In the next section, we describe an oracle experiment to measure n-best list quality of both systems. 3.4 Oracle Experiment Oracle scores provide an upper-bound in performance. For the BTEC test set, we approximated a human oracle by calculating automati</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the 40th ACL, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Peterson</author>
</authors>
<title>Adapting a transfer engine for rapid machine translation development.</title>
<date>2002</date>
<tech>M.S. Thesis,</tech>
<institution>Georgetown University.</institution>
<contexts>
<context position="6767" citStr="Peterson (2002)" startWordPosition="1064" endWordPosition="1065"> entire source sentence can be found. Partial parses are concatenated sequentially to generate complete translations. In the current version of the Xfer system, the output can be a first-best translation or a n-best list, which can be used for additional n-best list rescoring. The alternatives arise from lexical ambiguity and multiple synonymous choices for lexical items in the dictionary, but also from syntactic ambiguity and multiple competing hypotheses from the grammar. For our experiments, we used version 3 of the Xfer engine. An older version of the Xfer engine is described in detail in Peterson (2002). 2.4 Ranking Translations The Xfer engine can generate multiple translations. This requires a quality score to be assigned to all the alternatives. Based on these scores, the 1-best translation will be selected by the system. Fragmentation Penalty In the original Xfer system the only score used to rank translation alternatives was a heuristic fragmentation penalty. The fragmentation penalty is essentially the number of different chunks (rules or lexical entries not embedded in another rule) that span the whole translation. The intuition behind this score is that the more partial parses are ne</context>
</contexts>
<marker>Peterson, 2002</marker>
<rawString>Peterson, E. 2002. Adapting a transfer engine for rapid machine translation development. M.S. Thesis, Georgetown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Probst</author>
<author>R Brown</author>
<author>J Carbonell</author>
<author>A Levin Lavie</author>
<author>L Peterson</author>
<author>E</author>
</authors>
<date>2001</date>
<booktitle>Design and Implementation of Controlled Elicitation for Machine Translation of Low density Languages. Proceedings of the MT2001 workshop at MT Summit,</booktitle>
<location>Santiago de Compostela,</location>
<contexts>
<context position="3304" citStr="Probst et al 2001" startWordPosition="499" endWordPosition="502">ining is used to optimize the system. We show that this leads to significant improvements in performance. 2 A Transfer-Based Translation System 2.1 The Lexicon and Grammar In our Rule-Based MT (RBMT) system, translation rules include parsing, transfer, and generation information, similar to the modified transfer approach used in the early Metal system (Hutchins and Somers, 1992). The initial lexicon (479 entries) and grammar (40 rules) used in our experiments were manually written to cover the syntactic structures and the vocabulary of the first 400 sentences of the AVENUE Elicitation Corpus (Probst et al 2001). The Elicitation Corpus contains sets of minimal pairs in English and it was designed to cover a variety of linguistic phenomena. Building these two language-dependent components took a computational linguist 2-3 months. Figures 1 and 2 show 72 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 72–79, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics examples of a translation rules in the grammar and the lexicon. {S,4} S::S : [NP VP] -&gt; [NP VP] ( (X1::Y1) (X2::Y2) (x0 = x2) ((y2 subj) = -) ((y1 case) = </context>
</contexts>
<marker>Probst, Brown, Carbonell, Lavie, Peterson, E, 2001</marker>
<rawString>Probst, K., Brown, R., Carbonell, J., Lavie, A. Levin, and L., Peterson, E., 2001. Design and Implementation of Controlled Elicitation for Machine Translation of Low density Languages. Proceedings of the MT2001 workshop at MT Summit, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="false">
<note>SALM Toolkit: http://projectile.is.cs.cmu.edu/research/public/tools/s alm/salm.htm</note>
<marker></marker>
<rawString>SALM Toolkit: http://projectile.is.cs.cmu.edu/research/public/tools/s alm/salm.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Shimizu</author>
<author>Y Ashikari</author>
<author>E Sumita</author>
<author>H Kashioka</author>
<author>S Nakamura</author>
</authors>
<title>Development of client-server speech translation system on a multi-lingual speech communication platform. IWSLT,</title>
<date>2006</date>
<location>Kyoto, Japan.</location>
<contexts>
<context position="9927" citStr="Shimizu et al. 2006" startWordPosition="1596" endWordPosition="1599"> alignments has been studied in (Probst 2005). Here, we start with an existing grammar and augment the baseline lexicon with entries to cover the new domain. We explore semi-automatic lexicon generation for fast adaptation to the travel domain (Section 3.2). 3.1 Test Data: BTEC Corpus For initial evaluation on unseen data, we selected the Basic Travel Expression Corpus (BTEC) (Takezawa et al. 2002), which has been used in the evaluation campaigns in connection with the International Workshop on Spoken Language Translation (IWSLT 2006). Besides still being currently used to build real systems (Shimizu et al. 2006; Nakamura, et al. 2006), this corpus contains relatively simple sentences that are comparable to the ones initially corrected by users, and which are covered by the baseline manual grammar. As our test set, we used 506 English sentences for which two sets of Spanish reference translations were available. Table 1 shows corpus statistics for the BTEC data. Data English Sentences Pairs 123,416 Train Sentence Length 7.3 BTEC Word Tokens 903,525 Word Types 12,578 Sentence Pairs 506 Test Word Tokens 3,764 Word Types 776 Coverage Test 756 (97%) Table 1: Corpus Statistics for the BTEC corpus 3.2 Semi</context>
</contexts>
<marker>Shimizu, Ashikari, Sumita, Kashioka, Nakamura, 2006</marker>
<rawString>Shimizu T., Y. Ashikari, E. Sumita, H. Kashioka and S. Nakamura. 2006. Development of client-server speech translation system on a multi-lingual speech communication platform. IWSLT, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
</authors>
<title>Targeted Human Annotation.</title>
<date>2006</date>
<location>AMTA, Boston, USA.</location>
<contexts>
<context position="12794" citStr="Snover et al. 2006" startWordPosition="2048" endWordPosition="2051">eriment was also intended to see if the refined grammar would lead to better translations. We took the first-best translation output by the system without using any statistical components to rank alternative translations. System METEOR BLEU NIST Baseline 0.5666 0.2745 5.88 Refined 0.5676 0.2559 5.62 Table 2: Automatic metric scores for a purely Rule-Based MT System. Table 2 shows that, in this crude setting, different automatic metrics do not agree on the translation accuracy of both systems. On one hand, METEOR (Lavie et al. 2004), which has been shown to correlate well with human judgments (Snover et al. 2006), indicates that the refined system outperforms the baseline system (as measured by the latest version v0.5.1,). On the other hand, both BLEU (Papineni et al., 2002) and NIST (Doddington 2002) scores are higher for the baseline system (mteval-v11b.pl). However, human inspection revealed that the refined grammar is able to augment the n-best list with correct translations that the baseline system was not able to generate. This suggests that these results reflect poor re-ranking and not n-best list quality. In the next section, we describe an oracle experiment to measure n-best list quality of b</context>
<context position="20582" citStr="Snover et al. 2006" startWordPosition="3343" endWordPosition="3346">and automatic optimization becomes necessary. Different optimization techniques are available, like the Simplex algorithm or the special Minimum Error Training as described in (Och 2003). In Minimum Error Rate (MER) training, the n-best list generated by the translation system is used to find feature weight, thereby re-ranking the n-best list. This improves the match between the 1-best translation and given reference translations. Optimization can use any metric as objective function. Typically, systems are tuned towards high BLEU or high NIST scores, more recently also towards METEOR or TER (Snover et al. 2006). We used a MER training module (Venugopal), originally developed for an SMT system, to run MER training on the n-best lists generated by the Xfer system. This implementation allows for optimization towards BLEU and NIST mteval metrics. 5.1 Results In Table 7, we summarize some of the results from different n-best list rescoring experiments. Using only the Xfer engine, without language model, gives a very low score, as the selection is based only on the fragmentation score. Adding the n-gram language model gives a huge improvement. Adding additional features leads to more then 2 BLEU points im</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, 2006</marker>
<rawString>Snover, M; B. Dorr, R. Schwartz, L. Micciulla, 2006. Targeted Human Annotation. AMTA, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>H Yamamoto</author>
<author>S Yamamoto</author>
</authors>
<title>Toward a Broad-Coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World.</title>
<date>2002</date>
<booktitle>In Proceedings of 3rd LREC,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="9709" citStr="Takezawa et al. 2002" startWordPosition="1561" endWordPosition="1564"> a New Domain A major bottleneck in developing a RBMT system for a new translation task (a new language pair or a new domain) is writing the grammar and building the lexicon. Automatic grammar induction using statistical alignments has been studied in (Probst 2005). Here, we start with an existing grammar and augment the baseline lexicon with entries to cover the new domain. We explore semi-automatic lexicon generation for fast adaptation to the travel domain (Section 3.2). 3.1 Test Data: BTEC Corpus For initial evaluation on unseen data, we selected the Basic Travel Expression Corpus (BTEC) (Takezawa et al. 2002), which has been used in the evaluation campaigns in connection with the International Workshop on Spoken Language Translation (IWSLT 2006). Besides still being currently used to build real systems (Shimizu et al. 2006; Nakamura, et al. 2006), this corpus contains relatively simple sentences that are comparable to the ones initially corrected by users, and which are covered by the baseline manual grammar. As our test set, we used 506 English sentences for which two sets of Spanish reference translations were available. Table 1 shows corpus statistics for the BTEC data. Data English Sentences P</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Takezawa, T, E. Sumita, F. Sugaya, H. Yamamoto, and S. Yamamoto, 2002. Toward a Broad-Coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World. In Proceedings of 3rd LREC, Las Palmas, Spain.</rawString>
</citation>
<citation valid="false">
<authors>
<author>TC-STAR</author>
</authors>
<title>Evaluations: http://www.tc-star.org/ Venugopal, A.: MER Training Toolkit.</title>
<note>http://www.cs.cmu.edu/~ashishv/mer.html</note>
<marker>TC-STAR, </marker>
<rawString>TC-STAR Evaluations: http://www.tc-star.org/ Venugopal, A.: MER Training Toolkit. http://www.cs.cmu.edu/~ashishv/mer.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--377</pages>
<contexts>
<context position="1938" citStr="Wu 1997" startWordPosition="294" endWordPosition="295">llel data is available. Another reason is that the performance of statistical translation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Wu, D. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23:377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="2012" citStr="Yamada and Knight 2001" startWordPosition="302" endWordPosition="305">nce of statistical translation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine. Statistical alignment techniques are used to lower</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the ACL, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>Suffix Array and its Applications in Empirical Natural Language Processing,.</title>
<date>2006</date>
<tech>Technical Report CMU-LTI-06-010,</tech>
<location>Pittsburgh PA, USA.</location>
<contexts>
<context position="8221" citStr="Zhang &amp; Vogel, 2006" startWordPosition="1306" endWordPosition="1310"> language and words which are less likely to be used. To generate sen73 tences which are not only grammatically correct, but also use words and word sequences that are more natural and more common, data-driven machine translation systems use a n-gram language model. To get the same benefit in the Xfer system, an n-gram LM has been integrated with the engine. This has the advantage that in the case of pruning, the LM score can be used to avoid pruning good hypotheses, in addition to re re-rank the final translations. For our experiments, a suffix array language model based on the SALM toolkit (Zhang &amp; Vogel, 2006) is used. Length Model To adjust for the length of the translations generated by the system, the difference between the number of words generated and the expected number of words is added as a very simple feature. The expected length is calculated by multiplying the source sentence length by the ratio of the number of target and source words in the training corpus. The effect of this feature is to balance globally the length of the translations. 2.5 Pruning To deal with the combinatorial explosion during the parsing/translation process, pruning has to be applied. Only the n top-ranking hypothe</context>
</contexts>
<marker>Zhang, Vogel, 2006</marker>
<rawString>Zhang, Y and S. Vogel. 2006. Suffix Array and its Applications in Empirical Natural Language Processing,. Technical Report CMU-LTI-06-010, Pittsburgh PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>A S Hildebrand</author>
<author>S Vogel</author>
</authors>
<title>Distributed Language Modeling for N-best List Reranking.</title>
<date>2006</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Sydney, Australia.</location>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Zhang, Y, A. S. Hildebrand and S. Vogel. 2006. Distributed Language Modeling for N-best List Reranking. Empirical Methods in Natural Language Processing (EMNLP), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax Augmented Machine Translation via Chart Parsing.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL 2006 - Workshop on Statistical Machine Translation,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="2042" citStr="Zollmann and Venugopal 2006" startWordPosition="306" endWordPosition="309">lation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine. Statistical alignment techniques are used to lower the burden in building a lexi</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Zollmann A. and A. Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In Proc. of NAACL 2006 - Workshop on Statistical Machine Translation, New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>