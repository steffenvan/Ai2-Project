<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.497557">
SemEval-2015 Task 8: SpaceEval
</note>
<author confidence="0.9780115">
James Pustejovsky(1), Parisa Kordjamshidi(2,3), Marie-Francine Moens(2),
Aaron Levine(1), Seth Dworman(1), Zachary Yocum(1)
</author>
<affiliation confidence="0.938214333333333">
(1)Brandeis University, Waltham, MA
(2)Katholieke Universiteit Leuven, Belgium
(3)University of Illinois, Urbana/Champaign, IL
</affiliation>
<email confidence="0.9916505">
{jamesp,zyocum,aclevine,sdworman}@brandeis.edu,
Sien.Moens@cs.kuleuven.be, kordjam@illinois.edu
</email>
<sectionHeader confidence="0.997282" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999121230769231">
Human languages exhibit a variety of strate-
gies for communicating spatial information,
including toponyms, spatial nominals, loca-
tions that are described in relation to other lo-
cations, and movements along paths. SpaceE-
val is a combined information extraction and
classification task with the goal of identify-
ing and categorizing such spatial information.
In this paper, we describe the SpaceEval task,
annotation schema, and corpora, and evalu-
ate the performance of several supervised and
semi-supervised machine learning systems de-
veloped with the goal of automating this task.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999373982142857">
SpaceEval builds on the Spatial Role Labeling
(SpRL) task introduced in SemEval 2012 (Kord-
jamshidi et al., 2012) and used in SemEval 2013
(Kolomiyets et al., 2013). The base annotation
scheme of the previous tasks was introduced in (Ko-
rdjamshidi et al., 2010), with empirical practices
in (Kordjamshidi et al., 2011; Kordjamshidi and
Moens, 2015). While those previous tasks are
similar in their goal, SpacEval adopts the annota-
tion specification from ISOspace (Pustejovsky et al.,
2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC
37/SC 4/WG 2, 2014), a new standard for capturing
spatial information. The SpRL in SemEval 2012 had
a focus on the main roles of trajectors, landmarks,
spatial indicators, and the links between these roles
which form spatial relations. The formal semantics
of the relations were considered at a course-grained
level, consisting of three types: directional, regional
(topological), and distal. The related annotated data,
CLEF IAPR TC-12 Image Benchmark (Grubinger et
al., 2006), contained mostly static spatial relations.
In SemEval 2013, the SpRL task was extended to the
recognition of motion indicators and paths, which
are applied to the more dynamic spatial relations.
Accordingly, the data set was expanded and the text
from the Degree Confluence Project (Jarrett, 2013)
webpages were annotated.
SpaceEval extends the task in several dimensions,
first by enriching the granularity of the semantics in
both static and dynamic spatial configurations, and
secondly by broadening the variety of annotated data
and the domains considered. In SpaceEval the con-
cept of place is distinguished from the concept of
spatial entity as a fundamental typing distinction.
That is, the roles of trajector (figure) and landmark
(ground) are roles that are assigned to spatial enti-
ties and places when occurring in spatial relations.
Places, however, are inherently typed as such, and
remain places, regardless of what spatial roles they
may occupy. Obviously, an individual may assume
multiple role assignments, and in both ISOspace and
SpRL this is assumed to be the case. However, be-
cause SpRL focuses on role assignment, it does not
introduce the general concept of spatial entity.
There are other differences in the relational
schemas of SpRL and SpaceEval which can be eas-
ily mapped to each other. For example, in SpRL
the general concept of spatial relation is defined
and the semantics of the relationship (e.g., direc-
tional, regional) is added as an attribute of the re-
lation while in SpceEval these semantics introduce
new types of relations (e.g., QSLINK and OLINK).
In addition to the variations in relational schemas,
there are some additional extensions in the SpaceE-
val annotation. These include augmenting the main
elements with more fine-grained attributes. These
</bodyText>
<page confidence="0.977829">
884
</page>
<note confidence="0.9531275">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 884–894,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9994278125">
attributes, in turn, impact the way the spatial seman-
tics are interpreted. For example, the spatial entities
are described with their dimensionality, form, etc.
SpaceEval, also strongly highlights the concepts in-
volved in dynamic spatial relations by introducing
movelink relations and motion tags for annotating
motion verbs or nominal motion events and their
category from the perspective of spatial semantics.
These fine-grained annotations of all the relevant
concepts that contribute to grasping spatial seman-
tics makes this scheme and the accompanying cor-
pus unique. The details of the task, including the an-
notation schema, evaluation configurations, break-
down of the sub-tasks, data set, participant systems,
and evaluation results are described in the rest of the
paper.
</bodyText>
<sectionHeader confidence="0.993999" genericHeader="method">
2 The Task
</sectionHeader>
<bodyText confidence="0.997665">
The goals of SpaceEval include identifying and clas-
sifying items from an inventory of spatial concepts:
</bodyText>
<listItem confidence="0.997011466666667">
• Places: toponyms, geographic and geopolitical
regions, locations.
• Spatial Entities: entities participating in spatial
relations.
• Paths: routes, lines, turns, arcs.
• Topological relations: in, connected, discon-
nected.
• Orientational relations: North, left, down, be-
hind.
• Object properties: intrinsic orientation, dimen-
sionality.
• Frames of reference: absolute, intrinsic, rela-
tive.
• Motion: tracking objects through space over
time.
</listItem>
<bodyText confidence="0.9968375">
Participants were offered three test configurations
for this task.
Configuration 1 Only unannotated test data was
provided.
Configuration 2 Manually annotated spatial ele-
ments, without attributes, were provided.
Configuration 3 Manually annotated spatial ele-
ments, with attributes, were provided.
The SpacEval task is broken down into the fol-
lowing sub-tasks:
</bodyText>
<sectionHeader confidence="0.509881" genericHeader="method">
Spatial Elements (SE)
</sectionHeader>
<listItem confidence="0.956140114285714">
a. Identify spans of spatial elements includ-
ing locations, paths, events and other spa-
tial entities.
b. Classify spatial elements according to
type: PATH (road, river, highway), PLACE
(mountain, village), MOTION (walk, fly),
NONMOTION EVENT (sit, read), SPA-
TIAL ENTITY (any entity in a spatial re-
lation).
c. Identify their attributes according to type.
Spatial Signal Identification (SS)
a. Identify spans of spatial signals (in, on,
above).
b. Identify their attributes.
Motion Signal Identification (MI)
a. Identify spans of path-of-motion and
manner-of-motion signals (arrive, leave,
drive, walk).
b. Identify their attributes.
Motion Relation Identification (MoveLink)
a. Identify relations between motion-event
triggers, motion signals, and motion-
event participants (source, goal, landmark,
path).
b. Identify their attributes.
Spatial Configuration Identification (QSLink)
a. Identify qualitative spatial relations be-
tween spatial signals and spatial elements
(connected, unconnected, part-of, etc.).
b. Identify their attributes.
Spatial Orientation Identification (OLink)
a. Identify orientational relations between
spatial signals and spatial elements
(above, under, in front of, etc.).
b. Identify their attributes.
</listItem>
<sectionHeader confidence="0.97216" genericHeader="method">
3 The SpaceBank Corpus
</sectionHeader>
<bodyText confidence="0.99946875">
The data for this task are comprised of annotated
textual descriptions of spatial entities, places, paths,
motions, localized non-motion events, and spatial
relations. The data set selected for this task, a subset
of the SpaceBank corpus first described in (Puste-
jovsky and Yocum, 2013), consists of submissions
retrieved from the Degree Confluence Project (DCP)
(Jarrett, 2013), Berlitz Travel Guides retrieved from
</bodyText>
<page confidence="0.997468">
885
</page>
<bodyText confidence="0.96970375">
the American National Corpus (ANC) (Reppen et
al., 2005), and entries retrieved from a travel we-
blog, Ride for Climate (RFC) (Kroosma, 2012). The
DCP documents are the same set as those annotated
with Spatial Role Labeling (SpRL) for SemEval-
2013 Task 3 (Kolomiyets et al., 2013), however, for
this task, the DCP texts were re-annotated according
to ISO-Space.
</bodyText>
<subsectionHeader confidence="0.999919">
3.1 Annotation Schema
</subsectionHeader>
<bodyText confidence="0.996635421052632">
The annotation of spatial information in text in-
volves at least the following: a PLACE tag (for
locations and regions participating in spatial rela-
tions); a PATH tag (for paths and boundaries be-
tween regions); a SPATIAL ENTITY tag (for spatial
objects whose location changes over time); link tags
(for topological relations, direction and orientation,
frames of reference, and motion event participants);
and signal tags (for spatial prepositions)1. ISO-
Space has been designed to capture both spatial and
spatio-temporal information as expressed in natural
language texts (Pustejovsky et al., 2012). We have
followed a strict methodology of specification devel-
opment, as adopted by ISO TC37/SC4 and outlined
in (Bunt, 2010) and (Ide and Romary, 2004), and as
implemented with the development of ISO-TimeML
(Pustejovsky et al., 2005) and others in the family of
SemAF standards.
SpaceEval’s three link tags are as follows:
</bodyText>
<listItem confidence="0.998279">
1. MOVELINK – for movement relations;
2. OLINK – orientation relations;
3. QSLINK – qualitative spatial relations;
</listItem>
<bodyText confidence="0.987132076923077">
QSLINKs are used in ISO-Space to capture topo-
logical relationships between tagged elements. The
relType attribute values come from an extension
to the RCC8 set of relations that was first used
by SpatialML (Mani et al., 2010). The possible
RCC8+ values include the RCC8 values (Randell et
al., 1992), in addition to IN, a disjunction of TPP
and NTPP.
Orientation links describe non-topological rela-
tionships. A SPATIAL SIGNAL with a DIRECTIONAL
semantic type triggers such a link. In contrast
to topological spatial relations, OLINK relations are
built around a specific frame of reference type and
</bodyText>
<footnote confidence="0.887194">
1For more information, cf. (Pustejovsky et al., 2012).
</footnote>
<bodyText confidence="0.999833833333333">
a reference point. The referencePt value de-
pends on the frame type of the link. The ABSO-
LUTE frame type stipulates that the referencePt
is a cardinal direction. For INTRINSIC OLINKs,
the referencePt is the same identifier that is
given in the landmark attribute. For OLINKs
with a RELATIVE frame of reference, the identi-
fier for the viewer should be provided as to the
referencePt.
The following samples from the RFC and ANC
sub-corpora have been annotated with a subset of
ISO-Space for the SpaceEval task2:
</bodyText>
<construct confidence="0.948412055555555">
1. [Arrivingm1] [inms1] the [town of Juanjuipl1], near the
[parkpl2], [Ise1] learned that my map had lied to me.
&lt;MOTION id=m1 extent=‘‘Arriving’’
motion type=PATH motion class=REACH
motion sense=LITERAL&gt;
&lt;MOTION SIGNAL id=ms1 extent=‘‘in’’
motion signal type=PATH&gt;
&lt;PLACE id=pl1 extent=‘‘town of
Juanjui’’ form=NAM countable=TRUE
dimensionality=AREA&gt;
&lt;PLACE id=pl2 extent=‘‘park’’ form=NAM
countable=TRUE dimensionality=AREA&gt;
&lt;SPATIAL ENTITY id=se1 extent=‘‘I’’
form=NOM countable=TRUE
dimensionality=VOLUME&gt;
&lt;MOVELINK id=mvl1 trigger=m1
goal=pl1 mover=se1 goal reached=TRUE
motion signalID=ms1&gt;
</construct>
<listItem confidence="0.850301">
2. Just [south ofs1] [Ginzapl3] itself, as [youse2] [walkm2]
[towardms2] the [baypl4], you see [ons2] your [leftpl5]
the red [lanternsse4] and long [bannersse5] of the
[Kabuki-zapl6]. . .
</listItem>
<construct confidence="0.82165">
&lt;SPATIAL SIGNAL id=s1 extent=‘‘south
of’’ semantic type=DIRECTIONAL&gt;
&lt;PLACE id=pl3 extent=‘‘Ginza’’
form=NAM countable=TRUE
dimensionality=AREA&gt;
&lt;SPATIAL ENTITY id=se2 extent=‘‘you’’
form=NOM countable=TRUE
dimensionality=VOLUME&gt;
&lt;MOTION id=m2 extent=‘‘walk’’
motion type=COMPOUND
motion class=REACH
motion sense=LITERAL&gt;
&lt;MOTION SIGNAL id=ms2
extent=‘‘toward’’
motion signal type=PATH&gt;
&lt;PLACE id=pl4 extent=‘‘bay’’ form=NAM
countable=TRUE dimensionality=AREA&gt;
&lt;PLACE id=pl5 extent=‘‘left’’ form=NAM
countable=TRUE dimensionality=AREA&gt;
&lt;SPATIAL ENTITY id=se4
</construct>
<footnote confidence="0.953468">
2The MEASURE and MLINK tags were not a part of this task.
</footnote>
<page confidence="0.970562">
886
</page>
<equation confidence="0.982061055555555">
extent=“lanterns”form=NAM
countable=TRUE dimensionality=VOLUME&gt;
&lt;SPATIAL ENTITY id=se5
extent=“banners”form=NAM
countable=TRUE mod=“long”dimensionality=VOLUME&gt;
&lt;PLACE id=pl6 extent=“Kabuki-za”form=NAM countable=TRUE
dimensionality=VOLUME&gt;
&lt;OLINK id=ol1 trajector=m2
landmark=pl3 trigger=s1
frame type=ABSOLUTE referencePt=SOUTH
projective=FALSE&gt;
&lt;MOVELINK id=mvl2 trigger=m2
mover=se2 goal=pl4 goal reached=NO
motion signalID=ms2&gt;
&lt;QSLINK id=qsl1 trigger=s2
trajector=se5 landmark=pl5 relType=IN&gt;
&lt;QSLINK id=qsl2 trigger=s2
trajector=se6 landmark=pl5 relType=IN&gt;
</equation>
<bodyText confidence="0.9997184">
Since SpaceEval is building on the SpRL shared
tasks, we opted to retain the trajector and
landmark attributes for labeling the participants
in QSLINK and OLINK relations. This is a devia-
tion from the ISO-Space (Pustejovsky et al., 2011b)
standard, which specifies figure and ground
labels based on cognitive-semantic categories ex-
plored in the semantics of motion and location by
Leonard Talmy (Talmy, 1978; Talmy, 2000) and
others. ISO-Space adopted the figure/ground
terminology to identify the potentially asymmetric
roles played by participants within spatial relations.
For MOVELINKs, however, we distinguish the no-
tion of a figure/trajector with the ISO-Space
mover attribute label.
</bodyText>
<subsectionHeader confidence="0.999929">
3.2 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999623">
Table 1 includes corpus statistics broken down into
the ANC, DCP, and RFC sub-corpora in addition to
the train:test partition (∼3:1). The counts of docu-
ment, sentence, and lexical tokens are tabulated as
well as counts of each annotation tag type.
</bodyText>
<subsectionHeader confidence="0.99986">
3.3 Annotation and Adjudication
</subsectionHeader>
<bodyText confidence="0.981935285714286">
All annotations for this task were of English lan-
guage texts and all annotations were created and ad-
judicated by native English speakers. Due to depen-
dencies of link tag elements on extent tag elements,
the annotation and adjudication tasks were broken
down into the following phases:
Phase 1 Extent tag span and attribute annotation.
</bodyText>
<table confidence="0.996726333333333">
Sub-corpus Partition
ANC DCP RFC Train Test Total
words 1577 7673 21048 24150 6148 30298
sents 61 369 821 1001 250 1251
docs 3 22 44 55 14 69
pl 148 691 1250 1661 428 2089
se 34 461 1175 1347 323 1670
qsl 69 348 693 886 224 1110
Mvl 15 345 614 779 195 974
M 16 330 588 751 183 934
s 39 216 550 653 152 805
Ms 17 260 365 508 134 642
p 19 246 278 415 128 543
e 14 66 301 321 60 381
ol 14 82 191 225 62 287
</table>
<tableCaption confidence="0.499157">
pl=PLACE; se=SPATIAL ENTITY; qsl=QSLINK;
mvl=MOVELINK; m=MOTION; s=SPATIAL SIGNAL;
ms=MOTION SIGNAL; p=PATH; e=NONMOTION EVENT;
ol=OLINK
Table 1: Corpus Statistics
</tableCaption>
<bodyText confidence="0.99591737037037">
Phase 2 Extent tag adjudication.
Phase 3 Link tag argument and attribute annotation.
Phase 4 Link tag adjudication.
Phases 2 and 4 produced gold standards from an-
notations in the preceding annotation phases. This
annotation strategy ensured that the intermediate
gold standard extent tag set was adjudicated before
any link tag annotations were performed.
The annotation and adjudication effort was
conducted at Brandeis University using Multi-
document Annotation Environment (MAE) and
Multi-annotator Adjudication Interface (MAI)
(Stubbs, 2011). We used MAE to perform each
phase of the annotation procedure and MAI to
adjudicate and produce gold standard standoff
annotations in XML format. In addition to the
ISO-Space annotation tags and attributes, as a
post-process, we also provided sentence and lexical
tokenization as a separate standoff annotation layer
in the XML data for the training and test sets.
Each document was covered by a minimum of
three annotators for each annotation phase (though
not necessarily the same annotators per phase). As
such, we report inter-annotator agreement (IAA) as
a mean Fleiss’s κ coefficient for all extent tag types
annotated in Phase 1, and individual kappa scores
for each of the three link tag types annotated in
</bodyText>
<page confidence="0.987448">
887
</page>
<bodyText confidence="0.999802">
Phase 3 in Table 2. The scores for extent tags and
MOVELINK indicate high agreement, however link
tag annotation was less consistent for the remaining
link tags. Though the OLINK and QSLINK tag agree-
ment is better than chance, it is not high. We believe
the lower agreement for these link tags reflects the
complexity of the annotation task.
</bodyText>
<table confidence="0.571114">
Extent Tags Link Tags
All Types MOVELINK OLINK QSLINK
0.85 0.91 0.39 0.33
</table>
<tableCaption confidence="0.980773">
Table 2: Overall Fleiss’s κ Scores
</tableCaption>
<sectionHeader confidence="0.99877" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.998422">
Participant systems were evaluated for each enumer-
ated configuration as follows:
</bodyText>
<listItem confidence="0.862064">
1 a. SE.a precision, recall, and F1.
b. SE.b precision, recall, and F1 for each
type, and an overall precision, recall, and
F1.
c. SE.c precision, recall, and F1 for each at-
tribute, and an overall precision, recall,
and F1.
d. MoveLink.a, QSLink.a, OLink.a preci-
</listItem>
<bodyText confidence="0.949530176470588">
sion, recall, and F1.
e. MoveLink.b, QSLink.b, OLink.b preci-
sion, recall, and F1 for each attribute, and
an overall precision, recall, and F1.
2 a. SE.b and SE.c precision, recall, and F1 for
each type and its attributes, and an overall
precision, recall, and F1.
b. MoveLink.a, QSLink.a, OLink.a preci-
sion, recall, and F1.
c. MoveLink.b, QSLink.b, OLink.b preci-
sion, recall, and F1 for each attribute, and
an overall precision, recall, and F1.
3 a. MoveLink.a, QSLink.a, OLink.a preci-
sion, recall, and F1.
b. MoveLink.b, QSLink.b, OLink.b preci-
sion, recall, and F1 for each attribute, and
an overall precision, recall, and F1.
</bodyText>
<sectionHeader confidence="0.991626" genericHeader="evaluation">
5 Submissions and Results
</sectionHeader>
<bodyText confidence="0.968416658536585">
In this section we evaluate results from runs of five
systems. Three systems were submitted by outside
groups including Honda Research Institute Japan
(HRIJP-CRF-VW), Ixa Group in the University of
the Basque Country (IXA), and University of Texas,
Dallas (UTD)3. We also present results for two sys-
tems developed internally at Brandeis University: a
suite of logistic regression classifiers with minimal
feature engineering intended as a performance base-
line covering all sub-tasks in addition to a CRF sys-
tem with more advanced features, but limited to sub-
tasks 1a and 1b for Configuration 1.
BASELINE A suite of logistic regression models
using Scikit-learn (Pedregosa et al., 2011) with
simple bag-of-words and n-gram features.4
BRANDEIS-CRF A system using a conditional
random field (CRF) model (Okazaki, 2007)
with features including Stanford POS and NER
tags (Toutanova et al., 2003) (Finkel et al.,
2005) in combination with Sparser (McDonald,
1996) tags.5
HRIJP-CRF-VW A system using a CRF model us-
ing CoreNLP, (Manning et al., 2014), CRF-
Suite (Okazaki, 2007) and Vowpal Wabbit
(Langford et al., 2007) with lemmatization,
POS, NER, GloVe word vector (Pennington et
al., 2014) and dependency parse features.
IXA X-Space: A system using a binary sup-
port vector machine model from SVM-light
(Joachims, 1999) and a pipeline architecture
using ClearNLP (Choi and Adviser-Palmer,
2012), OpenNLP (OpenNLP, 2014), and lever-
aging computational linguistic resources in-
cluding WordNet (Fellbaum, 1998), PropBank
(Palmer et al., 2003) and the Predicate Matrix
(de la Calle et al., 2014).
UTD A suite of 13 classifiers for classifying spatial
roles and relations including classifiers for sta-
tionary spatial relations and their participants in
addition to classification of participants of mo-
tion events and their attributes.
</bodyText>
<footnote confidence="0.978369875">
3UTD submitted three runs, however, after evaluating all the
data, all three runs achieved similar scores; the results reported
here are for their third and final submitted run.
4These baseline classifiers were developed at Brandeis Uni-
versity by Aaron Levine and Zachary Yocum. Cf. Section 5.1
for full description.
5This system was developed at Brandeis University by Seth
Dworman. Cf. Section 5.2 for full description.
</footnote>
<page confidence="0.986784">
888
</page>
<subsectionHeader confidence="0.735286">
5.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999889">
Our baseline classification system (BASE-
LINE) consists of a suite of 47 classifiers built
from Scikit-learn’s (Pedregosa et al., 2011)
sklearn.linear model logistic regression
package. The system builds a collection of extent
objects from the annotation and lexical tokeniza-
tions provided in the SpaceEval XML distribution
data. Each extent instance has attributes for further
feature and label extraction: the target chunk used
to form the extent instance; any annotation tag
associated with the chunk; lists of all surrounding
tokens in the sentence, split between tokens preced-
ing the target and those following, and a pointer to
the original annotation XML for the purposes of
global feature extraction and generating new XML
tags based on the eventual model predictions.
Some extent attributes are optional, depending on
the sub-task. E.g., in sub-task 1a, no attributes are
required since this sub-task is a simple classification
task. For link tags, extent objects are instantiated
using the text chunks associated with the extent tags
that serve as the link trigger. After pre-processing,
the system has a complete collection of extent in-
stances for the corpus.
Subsequent to pre-processing, the extent data are
further processed for label and feature extraction.
The label and feature extractors were hand-tweaked
for each sub-task:
</bodyText>
<listItem confidence="0.9717535">
• For extent tag identification, the label extractor
checks if a given token occurs at the end of a
chunk, and the feature extractors include capi-
talization and POS tags.
• For classifying extent tag types, the feature
extractors include the target chunk string,
POS tag, and a seven-token context window
(bounded by the sentence) centered on the tar-
get token.
• For extent tag attribute classification, the only
feature extracted was the text of the chunk as-
sociated with the target tag.
• For link tag identification, a heuristic system
was developed to select candidate extent tags
</listItem>
<bodyText confidence="0.978967625">
for the trigger argument. The remaining argu-
ments in the relation were identified by their
distance and direction from the trigger. Fea-
ture extractors for this process included the text
of the trigger chunk, a count of the tags in lo-
cal context (the same sentence) before and after
the trigger, and the types of the extent tags that
occur in the context.
</bodyText>
<listItem confidence="0.8310134">
• For open-class link tag attributes, feature ex-
tractors included the count of extent tags be-
fore and after the trigger tag in the sentence.
For closed-class link tag attributes feature ex-
tractors were limited to the text of the trigger
chunk and the trigger tag type.6
• For link tag arguments that take an TDREF as a
value, a unique label function was created that
extracts the offsets of the candidate extent tags
in the same sentence as the trigger.
</listItem>
<bodyText confidence="0.999874363636364">
The label and feature vectors were maintained
using the DictVectorizer from Scikit-learn’s
feature extraction module. To train the sys-
tem, the vectors were used to fit the model to the
training data. For decoding, the tag labels and at-
tributes from the test data were discarded and the re-
maining feature vectors were transformed into a hy-
pothesis index based on the model, which was trans-
lated to a final value using a codebook. The hypothe-
ses were then written out to XML in accordance to
the task DTD.
</bodyText>
<subsectionHeader confidence="0.998317">
5.2 Brandeis CRF
</subsectionHeader>
<bodyText confidence="0.999984588235294">
In addition to the BASELINE system, we also devel-
oped a more advanced pipeline (BRANDEIS-CRF)
to automate the SpaceEval sub-tasks 1a and 1b us-
ing a linear-chain conditional random field model
using lexical, part-of-speech (POS), named-entity-
recognition (NER), and semantic labels. We re-
port overall F1 measures of 0.83 and 0.77 for tasks
1a and 1b, respectively, which are comparable to
other top results (cf. Section 5.3). Our imple-
mentation used the CRFSuite (Okazaki, 2007) open
source package, which facilitated rapid training and
model inspection. The hypotheses were written out
to XML in accordance to the task DTD.
We used a small set of 9 core features, augmented
with bigram contexts, resulting in a total of 27 fea-
tures. These features consist of lexical, syntac-
tic, and semantic information, many of which have
</bodyText>
<footnote confidence="0.56491125">
6We experimented with additional features for attribute clas-
sification, such as counting tags and their types in the local con-
text of the trigger, however additional features all resulted in
performance decreases.
</footnote>
<page confidence="0.998025">
889
</page>
<bodyText confidence="0.9999602">
been applied successfully in a variety of informa-
tion extraction tasks (Fei Huang et al., 2014), such
as named entity recognition (Vilain et al., 2009b) or
coreference resolution (Fernandes et al., 2014). The
complete set of features are outlined in Table 3.
</bodyText>
<subsectionHeader confidence="0.290402">
Type Id Value
</subsectionHeader>
<tableCaption confidence="0.982508">
Table 3: BRANDEIS-CRF Features
</tableCaption>
<bodyText confidence="0.9999652">
For part-of-speech (POS) and named entity (NE)
tags, we used the Stanford Log-linear Part-of-
Speech Tagger (Toutanova et al., 2003) and the Stan-
ford Named Entity Recognizer (Finkel et al., 2005).
Additionally, we made use of Sparser (McDonald,
1996), a rule-based natural language parser in order
to provide rich semantic features. Sparser parses un-
structured text in cycles, where a variety of hand-
written rules apply given the applications of pre-
vious rules or the current parse of the text. After
parsing, Sparser provides a set of edges, which pro-
vide both semantic and syntactic information. For
our purposes, we used the CATEGORY and FORM
attributes of the resulting edges. Table 4 shows
that the Sparser features can be informative for this
task, as five of the top ten positive weights are
from Sparser. As a disclaimer, we acknowledge that
model weights are not always sufficient for deter-
mining the most informative features (Vilain et al.,
2009a).
However, there were several problems using
Sparser. One issue is that Sparser performs its own
internal tokenization and chunking, as it expects un-
structured text as input, i.e. a string. To align the al-
ready tokenized sentences with a Sparser parse, we
used a matching algorithm that aligned a token with
its corresponding Sparser edge. A second problem
was that Sparser frequently fails on inputs, and the
points of failure can be difficult to identify due to the
interaction of its various phases and context based
</bodyText>
<footnote confidence="0.81669">
7Token character length is ≤ 5, (5..10], or &gt; 10.
</footnote>
<table confidence="0.988851083333333">
Weight Feature State
3.45 LCATEGORY=PATH-TYPE p
2.95 LCATEGORY=REGION-TYPE pl
2.66 word=( 0
2.66 LCATEGORY=BE 0
2.47 word=) 0
2.33 word=near me
2.28 word=border p
2.21 LCATEGORY=TIME-UNIT 0
2.17 LCATEGORY=NEAR me
2.16 pos=PRP se
p=PATH; pl=PLACE; me=MEASURE; se=SPATIAL ENTITY
</table>
<tableCaption confidence="0.999864">
Table 4: Top Ten Positive Feature Weights
</tableCaption>
<bodyText confidence="0.9994585">
rules. Thus, we were not able to get CATEGORY and
FORM for all tokens. As a remedy, we included lo-
cal forms of these Sparser features (prefixed with L),
which were collected by inputting tokens by them-
selves to Sparser. This suggests that word lists could
be very informative for this task.
</bodyText>
<subsectionHeader confidence="0.999495">
5.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999187190476191">
Table 5 shows mean precision (P), recall (R), F1,
and accuracy (ACC) scores for each group for each
evaluation configuration and sub-task that was at-
tempted. The overall precision and recall measures
we report are the arithmetic means of the precision
and recall for each tag label or attribute in the cor-
responding sub-task. The overall, macro-average F1
measures we report are the harmonic mean of the
overall P and R. Accuracy is computed as the num-
ber of correctly classified labels or attributes divided
by the total number of labels or attributes in the gold
standard. Overall accuracy and F1 are plotted in Ap-
pendix A.
Not all groups attempted all of the evaluation con-
figurations8. The HRIJP-CRF-VW system was eval-
uated only for Configuration 1 tasks 1a, 1b, 1d, and
1e (not 1c), and Configuration 3 sub-tasks 3a and
3b. HRIJP-CRF-VW was not evaluated for Config-
uration 2 since those sub-tasks were not attempted.
The UTD submission only covered Configuration 3,
thus was only evaluated for sub-tasks 3a and 3b.
</bodyText>
<footnote confidence="0.96806">
8The IXA system was the only one to complete all evalua-
tion configurations.
</footnote>
<figure confidence="0.986041076923077">
word[-1,0,1] string
isupper[-1,0,1] binary
wordlen[-1,0,1] ternary7
pos[-1,0,1] POS tag
ner[-1,0,1] NER tag
Lexical
Syntactic
Semantic
CATEGORY[-1,0,1] Sparser category
FORM[-1,0,1] Sparser form
LCATEGORY[-1,0,1] Sparser category
LFORM[-1,0,1] Sparser form
Sparser
</figure>
<page confidence="0.971398">
890
</page>
<table confidence="0.994103419354839">
System Task P R F1 ACC
BASELINE 1 a 0.55 0.52 0.53 0.75
b 0.55 0.51 0.53 0.86
c 0.10 0.02 0.04 0.05
d 0.50 0.50 0.50 0.50
e 0.05 0.02 0.02 0.06
2 a 0.27 0.28 0.27 0.76
b 0.79 0.58 0.67 0.90
c 0.19 0.20 0.19 0.66
3 a 0.86 0.84 0.85 0.98
b 0.26 0.26 0.26 0.79
BRANDEIS-CRF 1 a 0.85 0.80 0.83 0.89
b 0.78 0.76 0.77 0.92
HRIJP-CRF-VW 1 a 0.84 0.83 0.83 0.89
b 0.77 0.76 0.76 0.91
d 0.56 0.51 0.53 0.57
e 0.03 0.04 0.03 0.25
3 a 0.78 0.57 0.66 0.86
b 0.05 0.06 0.05 0.48
IXA 1 a 0.81 0.72 0.76 0.88
b 0.75 0.72 0.74 0.90
c 0.18 0.15 0.16 0.30
d 0.54 0.51 0.53 0.55
e 0.06 0.05 0.05 0.25
2 a 0.26 0.33 0.29 0.63
b 0.55 0.51 0.53 0.89
c 0.06 0.08 0.07 0.46
3 a 0.63 0.51 0.56 0.89
b 0.07 0.09 0.08 0.48
UTD 3 a 0.87 0.82 0.85 0.98
b 0.05 0.09 0.07 0.51
</table>
<tableCaption confidence="0.995972">
Table 5: Overall Performance
</tableCaption>
<sectionHeader confidence="0.993339" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975652173913">
It is clear from the participating system results
that recognizing spatial entities as a sub-task is a
fairly well-understood area, with reasonable per-
formance. All systems using CRF models for
recognizing places, paths, motion and non-motion
events, and spatial entities performed well. Fur-
thermore, MOVELINK recognition results were ex-
tremely promising, due to the general tendency for
movement to be accompanied by recognizable clues.
The overall poor performance for recognition of spa-
tial relations between entities, on the other hand
(QSLINKs and OLINKs) indicates that these are dif-
ficult relational identification tasks, reflected in the
lower IAA scores for these relations as well.
For the next SpaceEval evaluation, we believe that
a more focused task, possibly embedded within an
application, would lower the barrier to entry in the
competition. It would also permit us to use an extrin-
sic evaluation for performance of the systems. We
also hope to release the SpaceBank corpus through
LDC later this year. This would enable the commu-
nity to become more familiar with the dataset and
specification.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9862302">
This research was supported by grants from NSF’s
IIS-1017765 and NGA’s NURI HM1582-08-1-0018.
We would like to acknowledge all the annotators
and adjudicators who have contributed to the Space-
Bank annotation effort, including Adam Berger,
</bodyText>
<reference confidence="0.578526666666667">
Alexander Elias, Alison Marqusee, April Dobkin,
Benjamin Beaudett, Eric Benzschawel, Heather
Friedman, Katie Glanbock, Keelan Armstrong,
Kenyon Branan, Kiera Sarill, Lauren Weber, Martha
Schwarz, Meital Singer, Rebecca Loewenstein-
Harting, Sean Bethard, and Stephanie Grinley.
</reference>
<sectionHeader confidence="0.98769" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993797">
Harry Bunt. 2010. A methodology for designing seman-
tic annotation languages exploiting syntactic-semantic
iso-morphisms. In Proceedings of ICGL 2010, Second
International Conference on Global Interoperability
for Language Resources.
Jinho D Choi and Martha Adviser-Palmer. 2012. Op-
timization of natural language processing components
for robustness and scalability.
Maddalen L´opez de la Calle, Egoitz Laparra, and Ger-
man Rigau. 2014. First steps towards a predicate ma-
trix. In Proceedings of the Global WordNet Confer-
ence (GWC 2014), Tartu, Estonia, January. GWA.
Arun Ahuja Fei Huang, Doug Downey, Yi Yang, Yuhong
Guo, and Alexander Yates. 2014. Learning repre-
sentations for weakly supervised natural language pro-
cessing tasks. Computational Linguistics, 40:1(85-
120).
Christiane Fellbaum, editor. 1998. Wordnet: an elec-
tronic lexical database. MIT Press.
Eraldo Rezende Fernandes, Cicero Nogueira dos Santos,
and Ruy Luiz Milidi´u. 2014. Latent trees for corefer-
ence resolution. Computational Linguistics, 40:4(801-
835).
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363–370.
Association for Computational Linguistics.
Michael Grubinger, Paul Clough, Henning M¨uller, and
Thomas Deselaers. 2006. The IAPR benchmark: A
new evaluation resource for visual information sys-
tems. In Int. Conf. on Language Resources and Eval-
uation, LREC’06.
</reference>
<page confidence="0.992477">
891
</page>
<reference confidence="0.997271268518519">
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural Language Engineering, 10(3-4):211–225.
Kiyong Lee ISO/TC 37/SC 4/WG 2, Project leaders:
James Pustejovsky. 2014. Iso 24617-7:2014 language
resource management - part 7: Spatial information
(isospace). ISO/TC 37/SC 4/WG 2.
Alex Jarrett. 2013. The degree confluence project. Re-
trievedAugust, 2013, http://www.confiuence.org.
Thorsten Joachims. 1999. Svmlight: Support vec-
tor machine. SVM-Light Support Vector Machine
http://svmlight. joachims. org/, University of Dort-
mund, 19(4).
Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven
Bethard, and Marie-Francine Moens. 2013. Semeval-
2013 task 3: Spatial role labeling. In Second joint
conference on lexical and computational semantics (*
SEM), Volume 2: Proceedings of the seventh inter-
national workshop on semantic evaluation (SemEval
2013), pages 255–266.
Parisa Kordjamshidi and Marie-Francine Moens. 2015.
Global machine learning for spatial ontology popula-
tion. Web Semantics: Science, Services and Agents on
the World Wide Web, 30(0):3 – 21. Semantic Search.
Parisa Kordjamshidi, Marie-Francine Moens, and Mar-
tijn van Otterlo. 2010. Spatial role labeling: task
definition and annotation scheme. In Proceedings of
LREC 2010 - The sevent international conference on
langauge resources and evaluation.
Parisa Kordjamshidi, Martijn van Otterlo, and Marie-
Francine Moens. 2011. Spatial role labeling: towards
extraction of spatial relations from natural language.
ACM - Transactions on Speech and Language Process-
ing, 8:1–36.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012. Semeval-2012 task 3: Spatial
role labeling. In Proceedings of the First Joint Confer-
ence on Lexical and Computational Semantics-Volume
1: Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 365–
373. Association for Computational Linguistics.
David Kroosma. 2012. Ride for climate. Retrieved
September, 2012, http://rideforclimate.com/blog/.
John Langford, L Li, and A Strehl. 2007.
Vowpal wabbit. URL https://github.
com/JohnLangford/vowpal wabbit/wiki.
Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-
man, Rob Quimby, Justin Richer, Ben Wellner, Scott
Mardis, and Seamus Clancy. 2010. Spatialml: an-
notation scheme, resources, and evaluation. Language
Resources and Evaluation, 44(3):263–280, September.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
David McDonald. 1996. Internal and external evidence
in the identification and semantic categorization of
proper names. Corpus processing for lexical acqui-
sition, pages 21–39.
Jessica L. Moszkowicz and James Pustejovsky. 2010.
Iso-space: towards a spatial annotation framework for
natural language. Processing Romanian in Multilin-
gual, Interoperational and Scalable Environments.
Naoaki Okazaki. 2007. Crfsuite: a fast implementation
of conditional random fields (crfs).
Apache OpenNLP. 2014. Apache software foundation.
URL http://opennlp. apache. org.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003.
The proposition bank: an annotated corpus of semantic
roles. Computational Linguistics.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, et al. 2011. Scikit-learn: Machine
learning in python. The Journal of Machine Learning
Research, 12:2825–2830.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the Empiricial Methods in
Natural Language Processing (EMNLP 2014), 12.
James Pustejovsky and Zachary Yocum. 2013. Captur-
ing motion in iso-spacebank. In Workshop on Interop-
erable Semantic Annotation, page 25.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Eval-
uation, 39:123–164, May.
James Pustejovsky, Jessica L. Moszkowicz, and Marc
Verhagen. 2011a. Iso-space: the annotation of spa-
tial information in language. In Proceedings of ISA-6:
ACL-ISO International Workshop on Semantic Anno-
tation, Oxford, England, January.
James Pustejovsky, Jessica L. Moszkowicz, and Marc
Verhagen. 2011b. Iso-space: The annotation of spa-
tial information in language. In Proceedings of ISA-6:
ACL-ISO International Workshop on Semantic Anno-
tation, Oxford, England, January.
James Pustejovsky, Jessica Moszkowicz, and Marc Ver-
hagen. 2012. A linguistically grounded annotation
language for spatial information. TAL, 53(2).
David Randell, Zhan Cui, and Anthony Cohn. 1992.
A spatial logic based on regions and connections. In
Morgan Kaufmann, editor, Proceedings of the 3rd In-
ternation Conference on Knowledge Representation
and REasoning, pages 165–176, San Mateo.
</reference>
<page confidence="0.982563">
892
</page>
<reference confidence="0.999615230769231">
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
American national corpus (anc). Linguistic Data Con-
sortium, Philadelphia. Second release.
Amber Stubbs. 2011. Mae and mai: Lightweight an-
notation and adjudication tools. In Proceedings of the
5th Linguistic Annotation Workshop, pages 129–133.
Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 173–180. Association for Computational Lin-
guistics.
Marc Vilain, Jonathan Huggins, and Ben Wellner, 2009a.
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
chapter A simple feature-copying approach for long-
distance dependencies, pages 192–200. Association
for Computational Linguistics.
Marc Vilain, Jonathan Huggins, and Ben Wellner. 2009b.
Sources of performance in crf transfer training: a busi-
ness name-tagging case study. In Proceedings of the
International Conference RANLP-2009, pages 465–
470. Association for Computational Linguistics.
</reference>
<page confidence="0.998859">
893
</page>
<figure confidence="0.423770777777778">
A. Performance Plots
• BASELINE
• BRANDEIS-CRF
• HRUP-CR.F-VW
• IXA
• UTD
1111 011 MI Ill .ii 11 11 1111 1111
la lb lc id le 2a 2b 2c 3a 3b
Task
</figure>
<figureCaption confidence="0.998234">
Figure 1: Overall Accuracy for All Sub-tasks
</figureCaption>
<figure confidence="0.751072111111111">
• BASELINE
• BRANDEIS-CRF
•HRIJP-CRFIN
-V
• IXA
• UTD
di 1111 .1 iii ... 11 1. III
la lb lc id is 2a 2b 2c 3a 3b
Task
</figure>
<figureCaption confidence="0.937101">
Figure 2: Overall Fl for All Sub-tasks
</figureCaption>
<figure confidence="0.99955075">
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
</figure>
<page confidence="0.977994">
894
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605318">
<title confidence="0.956467">SemEval-2015 Task 8: SpaceEval</title>
<author confidence="0.886486">Parisa Marie-Francine</author>
<affiliation confidence="0.9243545">University, Waltham, Universiteit Leuven,</affiliation>
<address confidence="0.971272">of Illinois, Urbana/Champaign,</address>
<email confidence="0.995359">Sien.Moens@cs.kuleuven.be,kordjam@illinois.edu</email>
<abstract confidence="0.988217642857143">Human languages exhibit a variety of strategies for communicating spatial information, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alexander Elias</author>
<author>Alison Marqusee</author>
<author>April Dobkin</author>
<author>Benjamin Beaudett</author>
<author>Eric Benzschawel</author>
<author>Heather Friedman</author>
<author>Katie Glanbock</author>
<author>Keelan Armstrong</author>
<author>Kenyon Branan</author>
<author>Kiera Sarill</author>
<author>Lauren Weber</author>
<author>Martha Schwarz</author>
<author>Meital Singer</author>
<author>Rebecca LoewensteinHarting</author>
<author>Sean Bethard</author>
<author>Stephanie Grinley</author>
</authors>
<marker>Elias, Marqusee, Dobkin, Beaudett, Benzschawel, Friedman, Glanbock, Armstrong, Branan, Sarill, Weber, Schwarz, Singer, LoewensteinHarting, Bethard, Grinley, </marker>
<rawString>Alexander Elias, Alison Marqusee, April Dobkin, Benjamin Beaudett, Eric Benzschawel, Heather Friedman, Katie Glanbock, Keelan Armstrong, Kenyon Branan, Kiera Sarill, Lauren Weber, Martha Schwarz, Meital Singer, Rebecca LoewensteinHarting, Sean Bethard, and Stephanie Grinley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
</authors>
<title>A methodology for designing semantic annotation languages exploiting syntactic-semantic iso-morphisms.</title>
<date>2010</date>
<booktitle>In Proceedings of ICGL 2010, Second International Conference on Global Interoperability for Language Resources.</booktitle>
<contexts>
<context position="8464" citStr="Bunt, 2010" startWordPosition="1227" endWordPosition="1228">gions participating in spatial relations); a PATH tag (for paths and boundaries between regions); a SPATIAL ENTITY tag (for spatial objects whose location changes over time); link tags (for topological relations, direction and orientation, frames of reference, and motion event participants); and signal tags (for spatial prepositions)1. ISOSpace has been designed to capture both spatial and spatio-temporal information as expressed in natural language texts (Pustejovsky et al., 2012). We have followed a strict methodology of specification development, as adopted by ISO TC37/SC4 and outlined in (Bunt, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005) and others in the family of SemAF standards. SpaceEval’s three link tags are as follows: 1. MOVELINK – for movement relations; 2. OLINK – orientation relations; 3. QSLINK – qualitative spatial relations; QSLINKs are used in ISO-Space to capture topological relationships between tagged elements. The relType attribute values come from an extension to the RCC8 set of relations that was first used by SpatialML (Mani et al., 2010). The possible RCC8+ values include the RCC8 values (Randell </context>
</contexts>
<marker>Bunt, 2010</marker>
<rawString>Harry Bunt. 2010. A methodology for designing semantic annotation languages exploiting syntactic-semantic iso-morphisms. In Proceedings of ICGL 2010, Second International Conference on Global Interoperability for Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Adviser-Palmer</author>
</authors>
<title>Optimization of natural language processing components for robustness and scalability.</title>
<date>2012</date>
<contexts>
<context position="17919" citStr="Choi and Adviser-Palmer, 2012" startWordPosition="2637" endWordPosition="2640">ng a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3UTD submitted three runs, however, after evaluating all the data, all three runs achieved similar scores; the results reported here are for their third and final sub</context>
</contexts>
<marker>Choi, Adviser-Palmer, 2012</marker>
<rawString>Jinho D Choi and Martha Adviser-Palmer. 2012. Optimization of natural language processing components for robustness and scalability.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maddalen L´opez de la Calle</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>First steps towards a predicate matrix.</title>
<date>2014</date>
<booktitle>In Proceedings of the Global WordNet Conference (GWC 2014),</booktitle>
<publisher>GWA.</publisher>
<location>Tartu, Estonia,</location>
<contexts>
<context position="18114" citStr="Calle et al., 2014" startWordPosition="2666" endWordPosition="2669"> HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3UTD submitted three runs, however, after evaluating all the data, all three runs achieved similar scores; the results reported here are for their third and final submitted run. 4These baseline classifiers were developed at Brandeis University by Aaron Levine and Zachary Yocum. Cf. Section 5.1 for full description. 5This system was developed at Brandeis Unive</context>
</contexts>
<marker>Calle, Laparra, Rigau, 2014</marker>
<rawString>Maddalen L´opez de la Calle, Egoitz Laparra, and German Rigau. 2014. First steps towards a predicate matrix. In Proceedings of the Global WordNet Conference (GWC 2014), Tartu, Estonia, January. GWA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arun Ahuja Fei Huang</author>
<author>Doug Downey</author>
<author>Yi Yang</author>
<author>Yuhong Guo</author>
<author>Alexander Yates</author>
</authors>
<title>Learning representations for weakly supervised natural language processing tasks.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<pages>40--1</pages>
<contexts>
<context position="23185" citStr="Huang et al., 2014" startWordPosition="3489" endWordPosition="3492">apid training and model inspection. The hypotheses were written out to XML in accordance to the task DTD. We used a small set of 9 core features, augmented with bigram contexts, resulting in a total of 27 features. These features consist of lexical, syntactic, and semantic information, many of which have 6We experimented with additional features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. 889 been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Id Value Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear Part-ofSpeech Tagger (Toutanova et al., 2003) and the Stanford Named Entity Recognizer (Finkel et al., 2005). Additionally, we made use of Sparser (McDonald, 1996), a rule-based natural language parser in order to provide rich semantic features. Sparser parses unstructured text in cycles, where a variety </context>
</contexts>
<marker>Huang, Downey, Yang, Guo, Yates, 2014</marker>
<rawString>Arun Ahuja Fei Huang, Doug Downey, Yi Yang, Yuhong Guo, and Alexander Yates. 2014. Learning representations for weakly supervised natural language processing tasks. Computational Linguistics, 40:1(85-120).</rawString>
</citation>
<citation valid="true">
<title>Wordnet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. Wordnet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<pages>40--4</pages>
<marker>2014</marker>
<rawString>Eraldo Rezende Fernandes, Cicero Nogueira dos Santos, and Ruy Luiz Milidi´u. 2014. Latent trees for coreference resolution. Computational Linguistics, 40:4(801-835).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17443" citStr="Finkel et al., 2005" startWordPosition="2564" endWordPosition="2567">two systems developed internally at Brandeis University: a suite of logistic regression classifiers with minimal feature engineering intended as a performance baseline covering all sub-tasks in addition to a CRF system with more advanced features, but limited to subtasks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (P</context>
<context position="23587" citStr="Finkel et al., 2005" startWordPosition="3553" endWordPosition="3556">nd their types in the local context of the trigger, however additional features all resulted in performance decreases. 889 been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Id Value Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear Part-ofSpeech Tagger (Toutanova et al., 2003) and the Stanford Named Entity Recognizer (Finkel et al., 2005). Additionally, we made use of Sparser (McDonald, 1996), a rule-based natural language parser in order to provide rich semantic features. Sparser parses unstructured text in cycles, where a variety of handwritten rules apply given the applications of previous rules or the current parse of the text. After parsing, Sparser provides a set of edges, which provide both semantic and syntactic information. For our purposes, we used the CATEGORY and FORM attributes of the resulting edges. Table 4 shows that the Sparser features can be informative for this task, as five of the top ten positive weights </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Grubinger</author>
<author>Paul Clough</author>
<author>Henning M¨uller</author>
<author>Thomas Deselaers</author>
</authors>
<title>The IAPR benchmark: A new evaluation resource for visual information systems.</title>
<date>2006</date>
<booktitle>In Int. Conf. on Language Resources and Evaluation, LREC’06.</booktitle>
<marker>Grubinger, Clough, M¨uller, Deselaers, 2006</marker>
<rawString>Michael Grubinger, Paul Clough, Henning M¨uller, and Thomas Deselaers. 2006. The IAPR benchmark: A new evaluation resource for visual information systems. In Int. Conf. on Language Resources and Evaluation, LREC’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Laurent Romary</author>
</authors>
<title>International standard for a linguistic annotation framework.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<pages>10--3</pages>
<contexts>
<context position="8491" citStr="Ide and Romary, 2004" startWordPosition="1230" endWordPosition="1233">ng in spatial relations); a PATH tag (for paths and boundaries between regions); a SPATIAL ENTITY tag (for spatial objects whose location changes over time); link tags (for topological relations, direction and orientation, frames of reference, and motion event participants); and signal tags (for spatial prepositions)1. ISOSpace has been designed to capture both spatial and spatio-temporal information as expressed in natural language texts (Pustejovsky et al., 2012). We have followed a strict methodology of specification development, as adopted by ISO TC37/SC4 and outlined in (Bunt, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005) and others in the family of SemAF standards. SpaceEval’s three link tags are as follows: 1. MOVELINK – for movement relations; 2. OLINK – orientation relations; 3. QSLINK – qualitative spatial relations; QSLINKs are used in ISO-Space to capture topological relationships between tagged elements. The relType attribute values come from an extension to the RCC8 set of relations that was first used by SpatialML (Mani et al., 2010). The possible RCC8+ values include the RCC8 values (Randell et al., 1992), in addition </context>
</contexts>
<marker>Ide, Romary, 2004</marker>
<rawString>Nancy Ide and Laurent Romary. 2004. International standard for a linguistic annotation framework. Natural Language Engineering, 10(3-4):211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyong Lee</author>
</authors>
<title>ISO/TC 37/SC 4/WG 2, Project leaders: James Pustejovsky.</title>
<date>2014</date>
<booktitle>ISO/TC 37/SC 4/WG</booktitle>
<pages>2</pages>
<marker>Lee, 2014</marker>
<rawString>Kiyong Lee ISO/TC 37/SC 4/WG 2, Project leaders: James Pustejovsky. 2014. Iso 24617-7:2014 language resource management - part 7: Spatial information (isospace). ISO/TC 37/SC 4/WG 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Jarrett</author>
</authors>
<title>The degree confluence project. RetrievedAugust,</title>
<date>2013</date>
<note>http://www.confiuence.org.</note>
<contexts>
<context position="2288" citStr="Jarrett, 2013" startWordPosition="318" endWordPosition="319">dicators, and the links between these roles which form spatial relations. The formal semantics of the relations were considered at a course-grained level, consisting of three types: directional, regional (topological), and distal. The related annotated data, CLEF IAPR TC-12 Image Benchmark (Grubinger et al., 2006), contained mostly static spatial relations. In SemEval 2013, the SpRL task was extended to the recognition of motion indicators and paths, which are applied to the more dynamic spatial relations. Accordingly, the data set was expanded and the text from the Degree Confluence Project (Jarrett, 2013) webpages were annotated. SpaceEval extends the task in several dimensions, first by enriching the granularity of the semantics in both static and dynamic spatial configurations, and secondly by broadening the variety of annotated data and the domains considered. In SpaceEval the concept of place is distinguished from the concept of spatial entity as a fundamental typing distinction. That is, the roles of trajector (figure) and landmark (ground) are roles that are assigned to spatial entities and places when occurring in spatial relations. Places, however, are inherently typed as such, and rem</context>
<context position="7316" citStr="Jarrett, 2013" startWordPosition="1049" endWordPosition="1050">ntify their attributes. Spatial Orientation Identification (OLink) a. Identify orientational relations between spatial signals and spatial elements (above, under, in front of, etc.). b. Identify their attributes. 3 The SpaceBank Corpus The data for this task are comprised of annotated textual descriptions of spatial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from 885 the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE tag (for locations and regions participating in spatial relations); a PATH tag (for path</context>
</contexts>
<marker>Jarrett, 2013</marker>
<rawString>Alex Jarrett. 2013. The degree confluence project. RetrievedAugust, 2013, http://www.confiuence.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/,</title>
<date>1999</date>
<institution>University of Dortmund,</institution>
<contexts>
<context position="17844" citStr="Joachims, 1999" startWordPosition="2629" endWordPosition="2630">bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3UTD submitted three runs, however, after evaluating all the data, all three runs achieved </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/, University of Dortmund, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Semeval2013 task 3: Spatial role labeling.</title>
<date>2013</date>
<booktitle>In Second joint conference on lexical and computational semantics (* SEM), Volume 2: Proceedings of the seventh international workshop on semantic evaluation (SemEval</booktitle>
<pages>255--266</pages>
<contexts>
<context position="1145" citStr="Kolomiyets et al., 2013" startWordPosition="144" endWordPosition="147"> that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relation</context>
<context position="7637" citStr="Kolomiyets et al., 2013" startWordPosition="1100" endWordPosition="1103">patial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from 885 the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE tag (for locations and regions participating in spatial relations); a PATH tag (for paths and boundaries between regions); a SPATIAL ENTITY tag (for spatial objects whose location changes over time); link tags (for topological relations, direction and orientation, frames of reference, and motion event participants); and signal tags (for spatial prepositions)1. ISOSpace has been designed to capture both spa</context>
</contexts>
<marker>Kolomiyets, Kordjamshidi, Bethard, Moens, 2013</marker>
<rawString>Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens. 2013. Semeval2013 task 3: Spatial role labeling. In Second joint conference on lexical and computational semantics (* SEM), Volume 2: Proceedings of the seventh international workshop on semantic evaluation (SemEval 2013), pages 255–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Global machine learning for spatial ontology population. Web Semantics: Science,</title>
<date>2015</date>
<booktitle>Services and Agents on the World Wide Web, 30(0):3 – 21. Semantic Search.</booktitle>
<contexts>
<context position="1328" citStr="Kordjamshidi and Moens, 2015" startWordPosition="172" endWordPosition="175"> and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relations. The formal semantics of the relations were considered at a course-grained level, consisting of three types: directional, regional (topological), and distal. The related annotated d</context>
</contexts>
<marker>Kordjamshidi, Moens, 2015</marker>
<rawString>Parisa Kordjamshidi and Marie-Francine Moens. 2015. Global machine learning for spatial ontology population. Web Semantics: Science, Services and Agents on the World Wide Web, 30(0):3 – 21. Semantic Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Marie-Francine Moens</author>
<author>Martijn van Otterlo</author>
</authors>
<title>Spatial role labeling: task definition and annotation scheme.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Kordjamshidi, Moens, van Otterlo, 2010</marker>
<rawString>Parisa Kordjamshidi, Marie-Francine Moens, and Martijn van Otterlo. 2010. Spatial role labeling: task definition and annotation scheme. In Proceedings of LREC 2010 - The sevent international conference on langauge resources and evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Martijn van Otterlo</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Spatial role labeling: towards extraction of spatial relations from natural language.</title>
<date>2011</date>
<journal>ACM - Transactions on Speech and Language Processing,</journal>
<pages>8--1</pages>
<marker>Kordjamshidi, van Otterlo, Moens, 2011</marker>
<rawString>Parisa Kordjamshidi, Martijn van Otterlo, and MarieFrancine Moens. 2011. Spatial role labeling: towards extraction of spatial relations from natural language. ACM - Transactions on Speech and Language Processing, 8:1–36.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Semeval-2012 task 3: Spatial role labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>365--373</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1094" citStr="Kordjamshidi et al., 2012" startWordPosition="134" endWordPosition="138">tion, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the li</context>
</contexts>
<marker>Kordjamshidi, Bethard, Moens, 2012</marker>
<rawString>Parisa Kordjamshidi, Steven Bethard, and MarieFrancine Moens. 2012. Semeval-2012 task 3: Spatial role labeling. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 365– 373. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kroosma</author>
</authors>
<title>Ride for climate. Retrieved September,</title>
<date>2012</date>
<pages>http://rideforclimate.com/blog/.</pages>
<contexts>
<context position="7499" citStr="Kroosma, 2012" startWordPosition="1078" endWordPosition="1079">b. Identify their attributes. 3 The SpaceBank Corpus The data for this task are comprised of annotated textual descriptions of spatial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from 885 the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE tag (for locations and regions participating in spatial relations); a PATH tag (for paths and boundaries between regions); a SPATIAL ENTITY tag (for spatial objects whose location changes over time); link tags (for topological relations, direction and orientation, frames</context>
</contexts>
<marker>Kroosma, 2012</marker>
<rawString>David Kroosma. 2012. Ride for climate. Retrieved September, 2012, http://rideforclimate.com/blog/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Langford</author>
<author>L Li</author>
<author>A Strehl</author>
</authors>
<date>2007</date>
<note>Vowpal wabbit. URL https://github. com/JohnLangford/vowpal wabbit/wiki.</note>
<contexts>
<context position="17641" citStr="Langford et al., 2007" startWordPosition="2596" endWordPosition="2599">dition to a CRF system with more advanced features, but limited to subtasks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relati</context>
</contexts>
<marker>Langford, Li, Strehl, 2007</marker>
<rawString>John Langford, L Li, and A Strehl. 2007. Vowpal wabbit. URL https://github. com/JohnLangford/vowpal wabbit/wiki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Christy Doran</author>
<author>Dave Harris</author>
<author>Janet Hitzeman</author>
<author>Rob Quimby</author>
<author>Justin Richer</author>
<author>Ben Wellner</author>
<author>Scott Mardis</author>
<author>Seamus Clancy</author>
</authors>
<title>Spatialml: annotation scheme, resources, and evaluation.</title>
<date>2010</date>
<journal>Language Resources and Evaluation,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="9003" citStr="Mani et al., 2010" startWordPosition="1311" endWordPosition="1314">cification development, as adopted by ISO TC37/SC4 and outlined in (Bunt, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005) and others in the family of SemAF standards. SpaceEval’s three link tags are as follows: 1. MOVELINK – for movement relations; 2. OLINK – orientation relations; 3. QSLINK – qualitative spatial relations; QSLINKs are used in ISO-Space to capture topological relationships between tagged elements. The relType attribute values come from an extension to the RCC8 set of relations that was first used by SpatialML (Mani et al., 2010). The possible RCC8+ values include the RCC8 values (Randell et al., 1992), in addition to IN, a disjunction of TPP and NTPP. Orientation links describe non-topological relationships. A SPATIAL SIGNAL with a DIRECTIONAL semantic type triggers such a link. In contrast to topological spatial relations, OLINK relations are built around a specific frame of reference type and 1For more information, cf. (Pustejovsky et al., 2012). a reference point. The referencePt value depends on the frame type of the link. The ABSOLUTE frame type stipulates that the referencePt is a cardinal direction. For INTRIN</context>
</contexts>
<marker>Mani, Doran, Harris, Hitzeman, Quimby, Richer, Wellner, Mardis, Clancy, 2010</marker>
<rawString>Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitzeman, Rob Quimby, Justin Richer, Ben Wellner, Scott Mardis, and Seamus Clancy. 2010. Spatialml: annotation scheme, resources, and evaluation. Language Resources and Evaluation, 44(3):263–280, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="17573" citStr="Manning et al., 2014" startWordPosition="2585" endWordPosition="2588">ing intended as a performance baseline covering all sub-tasks in addition to a CRF system with more advanced features, but limited to subtasks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial rol</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McDonald</author>
</authors>
<title>Internal and external evidence in the identification and semantic categorization of proper names. Corpus processing for lexical acquisition,</title>
<date>1996</date>
<pages>21--39</pages>
<contexts>
<context position="17488" citStr="McDonald, 1996" startWordPosition="2572" endWordPosition="2573">rsity: a suite of logistic regression classifiers with minimal feature engineering intended as a performance baseline covering all sub-tasks in addition to a CRF system with more advanced features, but limited to subtasks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix </context>
<context position="23642" citStr="McDonald, 1996" startWordPosition="3563" endWordPosition="3564">additional features all resulted in performance decreases. 889 been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Id Value Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear Part-ofSpeech Tagger (Toutanova et al., 2003) and the Stanford Named Entity Recognizer (Finkel et al., 2005). Additionally, we made use of Sparser (McDonald, 1996), a rule-based natural language parser in order to provide rich semantic features. Sparser parses unstructured text in cycles, where a variety of handwritten rules apply given the applications of previous rules or the current parse of the text. After parsing, Sparser provides a set of edges, which provide both semantic and syntactic information. For our purposes, we used the CATEGORY and FORM attributes of the resulting edges. Table 4 shows that the Sparser features can be informative for this task, as five of the top ten positive weights are from Sparser. As a disclaimer, we acknowledge that </context>
</contexts>
<marker>McDonald, 1996</marker>
<rawString>David McDonald. 1996. Internal and external evidence in the identification and semantic categorization of proper names. Corpus processing for lexical acquisition, pages 21–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica L Moszkowicz</author>
<author>James Pustejovsky</author>
</authors>
<title>Iso-space: towards a spatial annotation framework for natural language. Processing Romanian in Multilingual, Interoperational and Scalable Environments.</title>
<date>2010</date>
<contexts>
<context position="1503" citStr="Moszkowicz and Pustejovsky, 2010" startWordPosition="197" endWordPosition="200">d and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relations. The formal semantics of the relations were considered at a course-grained level, consisting of three types: directional, regional (topological), and distal. The related annotated data, CLEF IAPR TC-12 Image Benchmark (Grubinger et al., 2006), contained mostly static spatial relations. In SemEval 2013, the SpRL task was extended to the recognition of mot</context>
</contexts>
<marker>Moszkowicz, Pustejovsky, 2010</marker>
<rawString>Jessica L. Moszkowicz and James Pustejovsky. 2010. Iso-space: towards a spatial annotation framework for natural language. Processing Romanian in Multilingual, Interoperational and Scalable Environments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>Crfsuite: a fast implementation of conditional random fields (crfs).</title>
<date>2007</date>
<contexts>
<context position="17346" citStr="Okazaki, 2007" startWordPosition="2550" endWordPosition="2551"> Basque Country (IXA), and University of Texas, Dallas (UTD)3. We also present results for two systems developed internally at Brandeis University: a suite of logistic regression classifiers with minimal feature engineering intended as a performance baseline covering all sub-tasks in addition to a CRF system with more advanced features, but limited to subtasks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), </context>
<context position="22525" citStr="Okazaki, 2007" startWordPosition="3384" endWordPosition="3385">translated to a final value using a codebook. The hypotheses were then written out to XML in accordance to the task DTD. 5.2 Brandeis CRF In addition to the BASELINE system, we also developed a more advanced pipeline (BRANDEIS-CRF) to automate the SpaceEval sub-tasks 1a and 1b using a linear-chain conditional random field model using lexical, part-of-speech (POS), named-entityrecognition (NER), and semantic labels. We report overall F1 measures of 0.83 and 0.77 for tasks 1a and 1b, respectively, which are comparable to other top results (cf. Section 5.3). Our implementation used the CRFSuite (Okazaki, 2007) open source package, which facilitated rapid training and model inspection. The hypotheses were written out to XML in accordance to the task DTD. We used a small set of 9 core features, augmented with bigram contexts, resulting in a total of 27 features. These features consist of lexical, syntactic, and semantic information, many of which have 6We experimented with additional features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. 889 been applied successfully in a vari</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apache OpenNLP</author>
</authors>
<title>Apache software foundation.</title>
<date>2014</date>
<note>URL http://opennlp. apache. org.</note>
<contexts>
<context position="17944" citStr="OpenNLP, 2014" startWordPosition="2642" endWordPosition="2643"> (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3UTD submitted three runs, however, after evaluating all the data, all three runs achieved similar scores; the results reported here are for their third and final submitted run. 4These baseli</context>
</contexts>
<marker>OpenNLP, 2014</marker>
<rawString>Apache OpenNLP. 2014. Apache software foundation. URL http://opennlp. apache. org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: an annotated corpus of semantic roles.</title>
<date>2003</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="18062" citStr="Palmer et al., 2003" startWordPosition="2656" endWordPosition="2659">) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3UTD submitted three runs, however, after evaluating all the data, all three runs achieved similar scores; the results reported here are for their third and final submitted run. 4These baseline classifiers were developed at Brandeis University by Aaron Levine and Zachary Yocum. Cf. Section 5.1 for full descr</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2003</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003. The proposition bank: an annotated corpus of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="17715" citStr="Pennington et al., 2014" startWordPosition="2607" endWordPosition="2610">sks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants o</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Zachary Yocum</author>
</authors>
<title>Capturing motion in iso-spacebank.</title>
<date>2013</date>
<booktitle>In Workshop on Interoperable Semantic Annotation,</booktitle>
<pages>25</pages>
<contexts>
<context position="7224" citStr="Pustejovsky and Yocum, 2013" startWordPosition="1034" endWordPosition="1038">ial relations between spatial signals and spatial elements (connected, unconnected, part-of, etc.). b. Identify their attributes. Spatial Orientation Identification (OLink) a. Identify orientational relations between spatial signals and spatial elements (above, under, in front of, etc.). b. Identify their attributes. 3 The SpaceBank Corpus The data for this task are comprised of annotated textual descriptions of spatial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from 885 the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PL</context>
</contexts>
<marker>Pustejovsky, Yocum, 2013</marker>
<rawString>James Pustejovsky and Zachary Yocum. 2013. Capturing motion in iso-spacebank. In Workshop on Interoperable Semantic Annotation, page 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Robert Knippen</author>
<author>Jessica Littman</author>
<author>Roser Sauri</author>
</authors>
<title>Temporal and event information in natural language text. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--123</pages>
<contexts>
<context position="8573" citStr="Pustejovsky et al., 2005" startWordPosition="1242" endWordPosition="1245">); a SPATIAL ENTITY tag (for spatial objects whose location changes over time); link tags (for topological relations, direction and orientation, frames of reference, and motion event participants); and signal tags (for spatial prepositions)1. ISOSpace has been designed to capture both spatial and spatio-temporal information as expressed in natural language texts (Pustejovsky et al., 2012). We have followed a strict methodology of specification development, as adopted by ISO TC37/SC4 and outlined in (Bunt, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005) and others in the family of SemAF standards. SpaceEval’s three link tags are as follows: 1. MOVELINK – for movement relations; 2. OLINK – orientation relations; 3. QSLINK – qualitative spatial relations; QSLINKs are used in ISO-Space to capture topological relationships between tagged elements. The relType attribute values come from an extension to the RCC8 set of relations that was first used by SpatialML (Mani et al., 2010). The possible RCC8+ values include the RCC8 values (Randell et al., 1992), in addition to IN, a disjunction of TPP and NTPP. Orientation links describe non-topological r</context>
</contexts>
<marker>Pustejovsky, Knippen, Littman, Sauri, 2005</marker>
<rawString>James Pustejovsky, Robert Knippen, Jessica Littman, and Roser Sauri. 2005. Temporal and event information in natural language text. Language Resources and Evaluation, 39:123–164, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jessica L Moszkowicz</author>
<author>Marc Verhagen</author>
</authors>
<title>Iso-space: the annotation of spatial information in language.</title>
<date>2011</date>
<booktitle>In Proceedings of ISA-6: ACL-ISO International Workshop on Semantic Annotation,</booktitle>
<location>Oxford, England,</location>
<contexts>
<context position="1468" citStr="Pustejovsky et al., 2011" startWordPosition="193" endWordPosition="196">rmance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relations. The formal semantics of the relations were considered at a course-grained level, consisting of three types: directional, regional (topological), and distal. The related annotated data, CLEF IAPR TC-12 Image Benchmark (Grubinger et al., 2006), contained mostly static spatial relations. In SemEval 2013, the SpRL task was</context>
<context position="12146" citStr="Pustejovsky et al., 2011" startWordPosition="1703" endWordPosition="1706">xtent=“Kabuki-za”form=NAM countable=TRUE dimensionality=VOLUME&gt; &lt;OLINK id=ol1 trajector=m2 landmark=pl3 trigger=s1 frame type=ABSOLUTE referencePt=SOUTH projective=FALSE&gt; &lt;MOVELINK id=mvl2 trigger=m2 mover=se2 goal=pl4 goal reached=NO motion signalID=ms2&gt; &lt;QSLINK id=qsl1 trigger=s2 trajector=se5 landmark=pl5 relType=IN&gt; &lt;QSLINK id=qsl2 trigger=s2 trajector=se6 landmark=pl5 relType=IN&gt; Since SpaceEval is building on the SpRL shared tasks, we opted to retain the trajector and landmark attributes for labeling the participants in QSLINK and OLINK relations. This is a deviation from the ISO-Space (Pustejovsky et al., 2011b) standard, which specifies figure and ground labels based on cognitive-semantic categories explored in the semantics of motion and location by Leonard Talmy (Talmy, 1978; Talmy, 2000) and others. ISO-Space adopted the figure/ground terminology to identify the potentially asymmetric roles played by participants within spatial relations. For MOVELINKs, however, we distinguish the notion of a figure/trajector with the ISO-Space mover attribute label. 3.2 Corpus Statistics Table 1 includes corpus statistics broken down into the ANC, DCP, and RFC sub-corpora in addition to the train:test partitio</context>
</contexts>
<marker>Pustejovsky, Moszkowicz, Verhagen, 2011</marker>
<rawString>James Pustejovsky, Jessica L. Moszkowicz, and Marc Verhagen. 2011a. Iso-space: the annotation of spatial information in language. In Proceedings of ISA-6: ACL-ISO International Workshop on Semantic Annotation, Oxford, England, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jessica L Moszkowicz</author>
<author>Marc Verhagen</author>
</authors>
<title>Iso-space: The annotation of spatial information in language.</title>
<date>2011</date>
<booktitle>In Proceedings of ISA-6: ACL-ISO International Workshop on Semantic Annotation,</booktitle>
<location>Oxford, England,</location>
<contexts>
<context position="1468" citStr="Pustejovsky et al., 2011" startWordPosition="193" endWordPosition="196">rmance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relations. The formal semantics of the relations were considered at a course-grained level, consisting of three types: directional, regional (topological), and distal. The related annotated data, CLEF IAPR TC-12 Image Benchmark (Grubinger et al., 2006), contained mostly static spatial relations. In SemEval 2013, the SpRL task was</context>
<context position="12146" citStr="Pustejovsky et al., 2011" startWordPosition="1703" endWordPosition="1706">xtent=“Kabuki-za”form=NAM countable=TRUE dimensionality=VOLUME&gt; &lt;OLINK id=ol1 trajector=m2 landmark=pl3 trigger=s1 frame type=ABSOLUTE referencePt=SOUTH projective=FALSE&gt; &lt;MOVELINK id=mvl2 trigger=m2 mover=se2 goal=pl4 goal reached=NO motion signalID=ms2&gt; &lt;QSLINK id=qsl1 trigger=s2 trajector=se5 landmark=pl5 relType=IN&gt; &lt;QSLINK id=qsl2 trigger=s2 trajector=se6 landmark=pl5 relType=IN&gt; Since SpaceEval is building on the SpRL shared tasks, we opted to retain the trajector and landmark attributes for labeling the participants in QSLINK and OLINK relations. This is a deviation from the ISO-Space (Pustejovsky et al., 2011b) standard, which specifies figure and ground labels based on cognitive-semantic categories explored in the semantics of motion and location by Leonard Talmy (Talmy, 1978; Talmy, 2000) and others. ISO-Space adopted the figure/ground terminology to identify the potentially asymmetric roles played by participants within spatial relations. For MOVELINKs, however, we distinguish the notion of a figure/trajector with the ISO-Space mover attribute label. 3.2 Corpus Statistics Table 1 includes corpus statistics broken down into the ANC, DCP, and RFC sub-corpora in addition to the train:test partitio</context>
</contexts>
<marker>Pustejovsky, Moszkowicz, Verhagen, 2011</marker>
<rawString>James Pustejovsky, Jessica L. Moszkowicz, and Marc Verhagen. 2011b. Iso-space: The annotation of spatial information in language. In Proceedings of ISA-6: ACL-ISO International Workshop on Semantic Annotation, Oxford, England, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jessica Moszkowicz</author>
<author>Marc Verhagen</author>
</authors>
<title>A linguistically grounded annotation language for spatial information.</title>
<date>2012</date>
<journal>TAL,</journal>
<volume>53</volume>
<issue>2</issue>
<contexts>
<context position="8339" citStr="Pustejovsky et al., 2012" startWordPosition="1205" endWordPosition="1208">ace. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE tag (for locations and regions participating in spatial relations); a PATH tag (for paths and boundaries between regions); a SPATIAL ENTITY tag (for spatial objects whose location changes over time); link tags (for topological relations, direction and orientation, frames of reference, and motion event participants); and signal tags (for spatial prepositions)1. ISOSpace has been designed to capture both spatial and spatio-temporal information as expressed in natural language texts (Pustejovsky et al., 2012). We have followed a strict methodology of specification development, as adopted by ISO TC37/SC4 and outlined in (Bunt, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005) and others in the family of SemAF standards. SpaceEval’s three link tags are as follows: 1. MOVELINK – for movement relations; 2. OLINK – orientation relations; 3. QSLINK – qualitative spatial relations; QSLINKs are used in ISO-Space to capture topological relationships between tagged elements. The relType attribute values come from an extension to the RCC8 set o</context>
</contexts>
<marker>Pustejovsky, Moszkowicz, Verhagen, 2012</marker>
<rawString>James Pustejovsky, Jessica Moszkowicz, and Marc Verhagen. 2012. A linguistically grounded annotation language for spatial information. TAL, 53(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Randell</author>
<author>Zhan Cui</author>
<author>Anthony Cohn</author>
</authors>
<title>A spatial logic based on regions and connections.</title>
<date>1992</date>
<booktitle>Proceedings of the 3rd Internation Conference on Knowledge Representation and REasoning,</booktitle>
<pages>165--176</pages>
<editor>In Morgan Kaufmann, editor,</editor>
<location>San Mateo.</location>
<contexts>
<context position="9077" citStr="Randell et al., 1992" startWordPosition="1323" endWordPosition="1326">t, 2010) and (Ide and Romary, 2004), and as implemented with the development of ISO-TimeML (Pustejovsky et al., 2005) and others in the family of SemAF standards. SpaceEval’s three link tags are as follows: 1. MOVELINK – for movement relations; 2. OLINK – orientation relations; 3. QSLINK – qualitative spatial relations; QSLINKs are used in ISO-Space to capture topological relationships between tagged elements. The relType attribute values come from an extension to the RCC8 set of relations that was first used by SpatialML (Mani et al., 2010). The possible RCC8+ values include the RCC8 values (Randell et al., 1992), in addition to IN, a disjunction of TPP and NTPP. Orientation links describe non-topological relationships. A SPATIAL SIGNAL with a DIRECTIONAL semantic type triggers such a link. In contrast to topological spatial relations, OLINK relations are built around a specific frame of reference type and 1For more information, cf. (Pustejovsky et al., 2012). a reference point. The referencePt value depends on the frame type of the link. The ABSOLUTE frame type stipulates that the referencePt is a cardinal direction. For INTRINSIC OLINKs, the referencePt is the same identifier that is given in the la</context>
</contexts>
<marker>Randell, Cui, Cohn, 1992</marker>
<rawString>David Randell, Zhan Cui, and Anthony Cohn. 1992. A spatial logic based on regions and connections. In Morgan Kaufmann, editor, Proceedings of the 3rd Internation Conference on Knowledge Representation and REasoning, pages 165–176, San Mateo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randi Reppen</author>
<author>Nancy Ide</author>
<author>Keith Suderman</author>
</authors>
<title>American national corpus (anc). Linguistic Data Consortium,</title>
<date>2005</date>
<location>Philadelphia.</location>
<note>Second release.</note>
<contexts>
<context position="7415" citStr="Reppen et al., 2005" startWordPosition="1062" endWordPosition="1065">relations between spatial signals and spatial elements (above, under, in front of, etc.). b. Identify their attributes. 3 The SpaceBank Corpus The data for this task are comprised of annotated textual descriptions of spatial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from 885 the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE tag (for locations and regions participating in spatial relations); a PATH tag (for paths and boundaries between regions); a SPATIAL ENTITY tag (for spatial objects whose location changes</context>
</contexts>
<marker>Reppen, Ide, Suderman, 2005</marker>
<rawString>Randi Reppen, Nancy Ide, and Keith Suderman. 2005. American national corpus (anc). Linguistic Data Consortium, Philadelphia. Second release.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amber Stubbs</author>
</authors>
<title>Mae and mai: Lightweight annotation and adjudication tools.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th Linguistic Annotation Workshop,</booktitle>
<pages>129--133</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14349" citStr="Stubbs, 2011" startWordPosition="2065" endWordPosition="2066">GNAL; p=PATH; e=NONMOTION EVENT; ol=OLINK Table 1: Corpus Statistics Phase 2 Extent tag adjudication. Phase 3 Link tag argument and attribute annotation. Phase 4 Link tag adjudication. Phases 2 and 4 produced gold standards from annotations in the preceding annotation phases. This annotation strategy ensured that the intermediate gold standard extent tag set was adjudicated before any link tag annotations were performed. The annotation and adjudication effort was conducted at Brandeis University using Multidocument Annotation Environment (MAE) and Multi-annotator Adjudication Interface (MAI) (Stubbs, 2011). We used MAE to perform each phase of the annotation procedure and MAI to adjudicate and produce gold standard standoff annotations in XML format. In addition to the ISO-Space annotation tags and attributes, as a post-process, we also provided sentence and lexical tokenization as a separate standoff annotation layer in the XML data for the training and test sets. Each document was covered by a minimum of three annotators for each annotation phase (though not necessarily the same annotators per phase). As such, we report inter-annotator agreement (IAA) as a mean Fleiss’s κ coefficient for all </context>
</contexts>
<marker>Stubbs, 2011</marker>
<rawString>Amber Stubbs. 2011. Mae and mai: Lightweight annotation and adjudication tools. In Proceedings of the 5th Linguistic Annotation Workshop, pages 129–133. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17421" citStr="Toutanova et al., 2003" startWordPosition="2560" endWordPosition="2563">also present results for two systems developed internally at Brandeis University: a suite of logistic regression classifiers with minimal feature engineering intended as a performance baseline covering all sub-tasks in addition to a CRF system with more advanced features, but limited to subtasks 1a and 1b for Configuration 1. BASELINE A suite of logistic regression models using Scikit-learn (Pedregosa et al., 2011) with simple bag-of-words and n-gram features.4 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellba</context>
<context position="23524" citStr="Toutanova et al., 2003" startWordPosition="3542" endWordPosition="3545">nal features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. 889 been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Id Value Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear Part-ofSpeech Tagger (Toutanova et al., 2003) and the Stanford Named Entity Recognizer (Finkel et al., 2005). Additionally, we made use of Sparser (McDonald, 1996), a rule-based natural language parser in order to provide rich semantic features. Sparser parses unstructured text in cycles, where a variety of handwritten rules apply given the applications of previous rules or the current parse of the text. After parsing, Sparser provides a set of edges, which provide both semantic and syntactic information. For our purposes, we used the CATEGORY and FORM attributes of the resulting edges. Table 4 shows that the Sparser features can be info</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>Jonathan Huggins</author>
<author>Ben Wellner</author>
</authors>
<title>A simple feature-copying approach for longdistance dependencies,</title>
<date>2009</date>
<booktitle>Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), chapter</booktitle>
<pages>192--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23240" citStr="Vilain et al., 2009" startWordPosition="3498" endWordPosition="3501"> written out to XML in accordance to the task DTD. We used a small set of 9 core features, augmented with bigram contexts, resulting in a total of 27 features. These features consist of lexical, syntactic, and semantic information, many of which have 6We experimented with additional features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. 889 been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Id Value Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear Part-ofSpeech Tagger (Toutanova et al., 2003) and the Stanford Named Entity Recognizer (Finkel et al., 2005). Additionally, we made use of Sparser (McDonald, 1996), a rule-based natural language parser in order to provide rich semantic features. Sparser parses unstructured text in cycles, where a variety of handwritten rules apply given the applications of pr</context>
</contexts>
<marker>Vilain, Huggins, Wellner, 2009</marker>
<rawString>Marc Vilain, Jonathan Huggins, and Ben Wellner, 2009a. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), chapter A simple feature-copying approach for longdistance dependencies, pages 192–200. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>Jonathan Huggins</author>
<author>Ben Wellner</author>
</authors>
<title>Sources of performance in crf transfer training: a business name-tagging case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>465--470</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23240" citStr="Vilain et al., 2009" startWordPosition="3498" endWordPosition="3501"> written out to XML in accordance to the task DTD. We used a small set of 9 core features, augmented with bigram contexts, resulting in a total of 27 features. These features consist of lexical, syntactic, and semantic information, many of which have 6We experimented with additional features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. 889 been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Id Value Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear Part-ofSpeech Tagger (Toutanova et al., 2003) and the Stanford Named Entity Recognizer (Finkel et al., 2005). Additionally, we made use of Sparser (McDonald, 1996), a rule-based natural language parser in order to provide rich semantic features. Sparser parses unstructured text in cycles, where a variety of handwritten rules apply given the applications of pr</context>
</contexts>
<marker>Vilain, Huggins, Wellner, 2009</marker>
<rawString>Marc Vilain, Jonathan Huggins, and Ben Wellner. 2009b. Sources of performance in crf transfer training: a business name-tagging case study. In Proceedings of the International Conference RANLP-2009, pages 465– 470. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>