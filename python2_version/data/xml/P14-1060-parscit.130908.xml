<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.982985">
Vector space semantics with frequency-driven motifs
</title>
<author confidence="0.994731">
Shashank Srivastava Eduard Hovy
</author>
<affiliation confidence="0.997243">
Carnegie Mellon University Carnegie Mellon University
</affiliation>
<address confidence="0.548289">
Pittsburgh, PA 15217 Pittsburgh, PA 15217
</address>
<email confidence="0.997406">
ssrivastava@cmu.edu hovy@cmu.edu
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833739130435">
Traditional models of distributional se-
mantics suffer from computational issues
such as data sparsity for individual lex-
emes and complexities of modeling se-
mantic composition when dealing with
structures larger than single lexical items.
In this work, we present a frequency-
driven paradigm for robust distributional
semantics in terms of semantically cohe-
sive lineal constituents, or motifs. The
framework subsumes issues such as dif-
ferential compositional as well as non-
compositional behavior of phrasal con-
situents, and circumvents some problems
of data sparsity by design. We design
a segmentation model to optimally par-
tition a sentence into lineal constituents,
which can be used to define distributional
contexts that are less noisy, semantically
more interpretable, and linguistically dis-
ambiguated. Hellinger PCA embeddings
learnt using the framework show competi-
tive results on empirical tasks.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890574074074">
Meaning in language is a confluence of experien-
tially acquired semantics of words or multi-word
phrases, and their semantic composition to create
new meanings. For instance, successfully inter-
preting a sentence such as
The old senator kicked the bucket.
requires the knowledge that the semantic conno-
tations of ‘kicking the bucket’ as a unit are the
same as those for ‘dying’. Short of explicit su-
pervision, such semantic mappings must be in-
ferred by a new language speaker through induc-
tive mechanisms operating on observed linguis-
tic usage. This perspective of acquired meaning
aligns with the ‘meaning is usage’ adage, conso-
nant with Wittgenstein’s view of semantics. At
the same time, the ability to adaptively commu-
nicate elaborate meanings can only be conciled
through Frege’s principle of compositionality, i.e.,
meanings of larger linguistic constructs can be
derived from the meanings of individual compo-
nents, modulated by their syntactic interrelations.
Indeed, most linguistic usage appears composi-
tional. This is supported by the fact even with
very limited vocabulary, children and non-native
speakers can often communicate surprisingly ef-
fectively.
It can be argued that to be sustainable, induc-
tive aspects of meaning must be recurrent enough
to be learnable by new users. That is, a non-
compositional phrase such as ‘kick the bucket’ is
likely to persist in common parlance only if it is
frequently used with its associated semantic map-
ping. If a usage-driven meaning of a motif is not
recurrent enough, learning this mapping is inef-
ficient in two ways. First, the sparseness of ob-
servations would severely limit accurate inductive
acquisition by new observers. Second, the value
of learning a very infrequent semantic mapping
is likely marginal. This motivates the need for
a frequency-driven view of lexical semantics. In
particular, such a perspective can be especially
advantageous for distributional semantics for rea-
sons we outline below.
Distributional semantic models (DSMs) that
represent words as distributions over neighbouring
contexts have been particularly effective in captur-
ing fine-grained lexical semantics (Turney et al.,
2010). Such models have engendered improve-
ments in diverse applications such as selectional
preference modeling (Erk, 2007), word-sense dis-
crimination (McCarthy and Carroll, 2003), auto-
matic dictionary building (Curran, 2003), and in-
formation retrieval (Manning et al., 2008). How-
ever, while conventional DSMs consider colloca-
</bodyText>
<page confidence="0.980467">
634
</page>
<note confidence="0.8605135">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634–643,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.499507666666667">
With the bad press in wake of the financial crisis, businesses are leaving our shores.
crisis: &lt;bad, businesses, financial, leaving, press, shores, wake&gt;
financial crisis: &lt;bad press, businesses, in wake of, leaving our shores&gt;
</bodyText>
<tableCaption confidence="0.996564">
Table 1: Meaning representation by conventional DSMs vs notional ideal
</tableCaption>
<bodyText confidence="0.997749675675676">
tion strengths (through counts and PMI scores) of
word neighbourhoods, they disregard much of the
regularity in human language. Most significantly,
word tokens that act as latent dimensions are of-
ten derived from arbitrary tokenization. The ex-
ample given in Table 1 succinctly describes this.
The first row in the table shows a representation
of the meaning of the token ‘crisis’ that a conven-
tional DSM might extract from the given sentence
after stopword removal. While helpful, the repre-
sentation seems unsatisfying since words such as
‘press’, ‘wake’ and ‘shores’ seem to have little to
do with a crisis. From a semantic perspective, a
representation similar to the second is more valu-
able: not only does it represent a semantic map-
ping for a more specific meaning, but the latent di-
mensions of the representation have are less noisy
(e.g., while ‘wake’ is semantically ambiguous, its
surrounding context in ‘in wake of’ disambiguates
it) and more intuitive in regards of semantic in-
terepretability. This is the overarching theme of
this work: we present a frequency driven paradigm
for extending distributional semantics to phrasal
and sentential levels in terms of such semantically
cohesive, recurrent lexical units or motifs.
We propose to identify such semantically
cohesive motifs in terms of features inspired
from frequency-characteristics, linguistic idiosyn-
crasies, and shallow syntactic analysis; and ex-
plore both supervised and semi-supervised mod-
els to optimally segment a sentence into such mo-
tifs. Through exploiting regularities in language
usage, the framework can efficiently account for
both compositional and non-compositional word
usage, while avoiding the issue of data-sparsity by
design. Our principal contributions in this paper
are:
</bodyText>
<listItem confidence="0.994653428571429">
• We present a framework for extending dis-
tributional semantics to learn semantic repre-
sentations of both words and phrases in terms
of recurrent motifs, rather than arbitrary word
tokens
• We present a simple model to segment a sen-
tence into such motifs using a feature-set
</listItem>
<bodyText confidence="0.621995333333333">
drawing from frequency statistics, informa-
tion theory, linguistic theories and shallow
syntactic analysis
</bodyText>
<listItem confidence="0.972193">
• Word and phrasal representations learnt
through the approach outperform conven-
tional DSM representations on empirical
tasks
</listItem>
<bodyText confidence="0.999954533333334">
This paper is organized as follows: In Sec-
tion 2, we briefly review related work in the do-
main of compositional distributional semantics,
and motivate our formulation. Section 3 describes
our methodology, which consists of a frequency-
driven segmentation model to partition text into
semantically meaningful recurring lineal-subunits,
a representation learning framework for learning
new semantic embeddings based on this segmen-
tation, and an approach to use such embeddings in
downstream applications. We present experiments
and empirical evaluations for our method in Sec-
tion 4. Finally, we conclude in Section 5 with a
summary of our principal findings, and a discus-
sion of possible directions for future work.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999909125">
While DSMs have been valuable in representing
semantics of single words, approaches to extend
them to represent the semantics of phrases and
sentences has met with only marginal success.
While there is considerable variety in approaches
and formulations, existing approaches for phrasal
level and sentential semantics can broadly be par-
titioned into two categories.
</bodyText>
<subsectionHeader confidence="0.993018">
2.1 Compositional approaches
</subsectionHeader>
<bodyText confidence="0.999303555555556">
These have aimed at using semantic representa-
tions for individual words to learn semantic rep-
resentations for larger linguistic structures. These
methods implicitly make an assumption of com-
positionality, and often include explicit computa-
tional models of compositionality. Notable among
such models are the additive and multiplicative
models of composition by Mitchell and Lapata
(2008), Grefenstette et al. (2010), Baroni and
</bodyText>
<page confidence="0.998278">
635
</page>
<bodyText confidence="0.999673103448276">
Zamparelli’s (2010) model that differentially mod-
els content and function words for semantic com-
position, and Goyal et al.’s SDSM model (2013)
that incorporates syntactic roles to model seman-
tic composition. Notable among the most effec-
tive distributional representations are the recent
deep-learning approaches by Socher et al. (2012),
that model vector composition through non-linear
transformations. While word embeddings and lan-
guage models from such methods have been use-
ful for tasks such as relation classification, polarity
detection, event coreference and parsing; much of
existing literature on composition is based on ab-
stract linguistic theory and conjecture, and there
is little evidence to support that learnt represen-
tations for larger linguistic units correspond to
their semantic meanings. While works such as
the SDSM model suffer from the problem of spar-
sity in composing structures beyond bigrams and
trigrams, methods such as Mitchell and Lapata
(2008)and (Socher et al., 2012) and Grefenstette
and Sadrzadeh (2011) are restricted by signifi-
cant model biases in representing semantic com-
position by generic algebraic operations. Finally,
the assumption that semantic meanings for sen-
tences could have representations similar to those
for smaller individual tokens is in some sense un-
intuitive, and not supported by linguistic or seman-
tic theories.
</bodyText>
<subsectionHeader confidence="0.998688">
2.2 Tree kernels
</subsectionHeader>
<bodyText confidence="0.99867972">
Tree Kernel methods have gained popularity in
the last decade for capturing syntactic information
in the structure of parse trees (Collins and Duffy,
2002; Moschitti, 2006). Instead of procuring ex-
plicit representations, the kernel paradigm directly
focuses on the larger goal of quantifying semantic
similarity of larger linguistic units. Structural ker-
nels for NLP are based on matching substructures
within two parse trees , consisting of word-nodes
with similar labels. These methods have been use-
ful for eclectic tasks such as parsing, NER, se-
mantic role labeling, and sentiment analysis. Re-
cent approaches such as by Croce et al. (2011)
and Srivastava et al. (2013) have attempted to pro-
vide formulations to incorporate semantics into
tree kernels through the use of distributional word
vectors at the individual word-nodes. While this
framework is attractive in the lack of assumptions
on representation that it makes, the use of distri-
butional embeddings for individual tokens means
that it suffers from the same shortcomings as de-
scribed for the example in Table 1, and hence these
methods model semantic relations between word-
nodes very weakly. Figure 1 shows an example of
the shortcomings of this general approach.
</bodyText>
<figureCaption confidence="0.972742">
Figure 1: Tokenwise syntactic and semantic simi-
larities don’t imply sentential semantic similarity
</figureCaption>
<bodyText confidence="0.9999389">
While the two sentences in consideration have
near-identical syntax and could be argued to have
semantically aligned words in similar positions,
the semantics of the complete sentences are widely
divergent. Specifically, the ‘bag of words’ as-
sumption in tree kernels doesn’t suffice for these
lexemes, and a stronger semantic model is needed
to capture phrasal semantics as well as diverging
inter-word relations such as in ‘coffee table’ and
‘water table’. Our hypothesis is that a model that
can even weakly identify recurrent motifs such as
‘water table’ or ‘breaking a fall’ would be help-
ful in building more effective semantic represen-
tations. A significant advantage of a frequency
driven view is that it makes the concern of com-
positionality of recurrent phrases immaterial. If a
motif occurs frequently enough in common par-
lance, its semantics could be captured with distri-
butional models irrespective of whether its associ-
ated semantics are compositional or acquired.
</bodyText>
<subsectionHeader confidence="0.999004">
2.3 Identifying multi-word expressions
</subsectionHeader>
<bodyText confidence="0.999981">
Several approaches have focused on supervised
identification of multi-word expressions (MWEs)
through statistical (Pecina, 2008; Villavicencio et
al., 2007) and linguistically motivated (Piao et al.,
2005) techniques. More recently, hybrid methods
based on both statistical as well as linguistic fea-
tures have been popular (Tsvetkov and Wintner,
2011). Ramisch et al. (2008) demonstrate that
adding part-of-speech tags to frequency counts
substantially improves performance. Other meth-
ods have attempted to exploit morphological, syn-
tactic and semantic characteristics of MWEs. In
</bodyText>
<page confidence="0.995446">
636
</page>
<bodyText confidence="0.999953">
particular, approaches such as Bannard (2007) use
syntactic rigidity to characterize MWEs. While
existing work has focused on the classification
task of categorizing a phrasal constituent as a
MWE or a non-MWE, the general ideas of most
of these works are in line with our current frame-
work, and the feature-set for our motif segmen-
tation model is designed to subsume most of
these ideas. It is worthwhile to point out that
the task of motif segmentation is slightly differ-
ent from MWE identification. Specifically, the
onus on recurrent occurrences means that non-
decomposibility is not an essential consideration
for a word to be considered a motif. In line with
the proposed paradigm, typical MWEs such as
‘shoot the breeze’, ‘sour note’ and ‘hot dog’ would
be considered valid lineal motifs. 1 In addition,
even decomposable recurrent lineal phrases such
as ‘love story’, ‘federal government’, and ‘mil-
lions of people’ are marked as meaningful recur-
rent motifs. Finally, and least interestingly, we
include common named entities such as ‘United
States’ and ‘Java Virtual Machine’ within the am-
bit of motifs.
</bodyText>
<sectionHeader confidence="0.981648" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999991166666667">
In this section, we define our frequency-driven
framework for distributional semantics in detail.
As just described above, our definition for motifs
is less specific than MWEs. With such a working
definition, contiguous motifs are likely to make
distributional representations less noisy and also
assist in disambiguating context. Also, the lack of
specificity ensures that such motifs are common
enough to meaningfully influence distributional
representation beyond single tokens. A method
towards frequency-driven distributional semantics
could involve the following principal components:
</bodyText>
<subsectionHeader confidence="0.99738">
3.1 Linear segmentation model
</subsectionHeader>
<bodyText confidence="0.999979875">
The segmentation model forms the core of the
framework. Ideally, it fragments a given sen-
tence into non-overlapping, semantically mean-
ingful, empirically frequent contiguous sub-units
or motifs. The model accounts for possible seg-
mentations of a sentence into potential motifs, and
prefers recurrent and cohesive motifs through fea-
tures that capture frequency-based and statistical
</bodyText>
<footnote confidence="0.632278">
1We note that since we take motifs as lineal units,
the current method doesn’t subsume several common non-
contiguous MWEs such as ‘let off’ in ‘let him off’.
</footnote>
<bodyText confidence="0.9999267">
features, as well as linguistic idiosyncracies. This
is accomplished using a very simple linear chain
model and a rich feature set consisting of a combi-
nation of frequency-driven, information theoretic
and linguistically motivated features.
Let an observed sentence be denoted by x, with
the individual tokens xi denoting the i’th token in
the sentence. The segmentation model is a chain
LVM (latent variable model) that aims to maxi-
mize a linear objective defined by:
</bodyText>
<equation confidence="0.934522">
J =� wifi(yk, yk−1, x)
i
</equation>
<bodyText confidence="0.999987029411764">
where fi are arbitrary Markov features that can
depend on segments (potential motifs) of the ob-
served sentence x, and contiguous latent states.
The features are chosen so as to best represent
frequency-based, statistical as well as linguistic
considerations for treating a segment as an ag-
glutinative unit, or a motif. In specific, these
features could encode characteristics such as fre-
quency statistics, collocation strengths and syn-
tactic distinctness, or inflectional rigidity of the
considered segments; described in detail in Sec-
tion 3.2. The model is an instantiation of a sim-
ple featurized HMM, and the weighted sum of fea-
tures corresponding to a segment is cognate with
an affinity score for the ‘stickiness’ of the segment,
i.e., the affinity for the segment to be treated as
holistic unit or a single motif.
We also associate a penalizing cost for each non
unary-motif to avoid aggressive agglutination of
tokens. In particular, for an ngram occurrence to
be considered a motif, the marginal contribution
due to the affinity of the prospective motif should
at minimum exceed this penalty. The weights for
the affinity functions as well as these penalties are
learnt from data using full as well as partial anno-
tations. The latent state-variables yk denotes the
membership of the token xk to a unary or a larger
motif; and the state-sequence collectively gives
the segmentation of the sentence. An individual
state-variable yk encodes a pairing of the size of
the encompassing ngram motif, and the position
of the word xk within it. For instance, yk = T3
denotes that the token xk is the final position in a
trigram motif.
</bodyText>
<subsectionHeader confidence="0.561344">
3.1.1 Inference of optimal segmentation
</subsectionHeader>
<bodyText confidence="0.993034">
If the optimal weights wi are known, inference
for the best motif segmentation can be performed
</bodyText>
<page confidence="0.9928">
637
</page>
<bodyText confidence="0.999951111111111">
in linear time (in the number of tokens) follow-
ing the generalized Viterbi algorithm. A slightly
modified version of Viterbi could also be used to
find segmentations that are constrained to agree
with some given motif boundaries, but can seg-
ment other parts of the sentence optimally under
these constraints. This is necessary for the sce-
nario of semi-supervised learning of weights with
partially annotated sentences, as described later.
</bodyText>
<subsectionHeader confidence="0.999875">
3.2 Learning motif affinities and penalties
</subsectionHeader>
<bodyText confidence="0.999502">
We briefly discuss data-driven learning of weights
for features that define the motif affinity scores
and penalties. We describe learning of the model
parameters with fully annotated training data, as
well as an approach for learning motif segmenta-
tion that requires only partial supervision.
Supervised learning: In the supervised case, op-
timal state sequences y(k) are fully observed for
the training set. For this purpose, we created a
dataset of 1000 sentences from the Simple En-
glish Wikipedia and the Gigaword Corpus, and
manually annotated it with motif boundaries us-
ing BRAT (Stenetorp et al., 2012). In this case,
learning can follow the online structured percep-
tron learning procedure by Collins (2002), where
weights updates for the k’th training example
(x(k), y(k)) are given as:
</bodyText>
<equation confidence="0.876396">
wi ← wi + α(fi(x(k),y(k)) − fi(x(k),y�))
</equation>
<bodyText confidence="0.986173195121951">
Here y&apos; = Decode(x(k), w) is the optimal
Viterbi decoding using the current estimates of
the weights. Updates are run for a large number
of iterations until the change in objective drops
below a threshold, and the learning rate α is
adaptively modified as described in Collins et al.
Implicitly, the weight learning algorithm can be
seen as a gradient descent procedure minimizing
the difference between the scores of highest
scoring (Viterbi) state sequences, and the label
state sequences.
Semi-supervised learning: In the semi-
supervised case, the labels y(k)
i are known
only for some of the tokens in x(k). This is a
commonplace scenario, where a part of a sentence
has clear motif-boundaries, whereas the rest of the
sentence is not annotated. For accumulating such
data, we looked for occurrences of 2500 expres-
sions from the WikiMWE dataset in sentences
from the combined Simple English Wikipedia
and Gigaword corpora. The query expressions in
the retrieved sentences were marked with motif
boundaries, while the remaining tokens in the
sentences were left unannotated.
While the Viterbi algorithm can be used for tag-
ging optimal state-sequences given the weights,
the structured perceptron can learn optimal model
weights given gold-standard sequence labels.
Hence, in this case, we use a variation of the hard
EM algorithm for learning. The algorithm pro-
ceeds as follows: in the E-step, we use the current
values of weights to compute hard-expectations,
i.e., the best scoring Viterbi sequences among
those consistent with the observed state labels. In
the M-step, we take the decoded state-sequences
in the E-step as observed, and run perceptron
learning to update feature weights wi. Pseudocode
of the learning algorithm for the partially labeled
case is given in Algorithm 1.
Algorithm 1
</bodyText>
<listItem confidence="0.991683090909091">
1: Input: Partially labeled data D = {(x, y)i}
2: Output: Weights w
3: Initialization: Set wi randomly, Vi
4: for i : 1 to maxIter do
5: Decode D with current w to find optimal
Viterbi paths that agree with (partial) ground
truths.
6: Run Structured Perceptron algorithm with de-
coded tag-sequences to update weights w
7: end for
8: return w
</listItem>
<bodyText confidence="0.999721083333333">
The semi-supervised approach enables incor-
poration of significantly more training data. In
particular, this method could be used in conjunc-
tion with a supervised approach. This would in-
volve initializing the weights prior to the semi-
supervised procedure with the weights from the
supervised learning model, so as to seed the semi-
supervised approach with reasonable model, and
use the partially annotated data to fine-tune the su-
pervised model. The sequential approach, akin to
annealing weights, can efficiently utilize both full
and partial annotations.
</bodyText>
<subsectionHeader confidence="0.681922">
3.2.1 Feature engineering
</subsectionHeader>
<bodyText confidence="0.994174">
In this section, we describe the principal features
used in the segmentation model
Transitional features and penalties:
</bodyText>
<listItem confidence="0.989566">
• Transitional features ftrans(yi−1, yi) =
</listItem>
<page confidence="0.984285">
638
</page>
<bodyText confidence="0.9103468">
Iyi−1,yi 2 describing the transitional affinities
of state pairs. Since our state definitions pre-
clude certain transitions (such as from state
T2 to T1), these weights are initialized to −oc
to expedite training.
</bodyText>
<listItem confidence="0.9769514">
• N-gram penalties: fngram We define a
penalty for tagging each non-unary motif as
described before. For a motif to be tagged,
the improvement in objective score should at
least exceed the corresponding penalty. e.g.,
fqgram(yi) = Iyi=Q4 denotes the penalty for
tagging a tetragram. 3
Frequency-based, information theoretic, and POS
features:
• Absolute and log-normalized motif frequen-
</listItem>
<bodyText confidence="0.871588125">
cies fngram(xi−n+1, ...xi−1, xi, yi). This
feature is associated with a particular token-
sequence and ngram-tag, and takes the
value of the motif-frequency if the motif
token-sequence matches the feature token-
sequence, and is marked as with a match-
ing tag. e.g., fbgram(xi−1 = love, xi =
story, yi = B2).
</bodyText>
<listItem confidence="0.664480181818182">
• Absolute and log-normalized motif frequen-
cies for a particular POS-sequence. This
feature is associated with a particular POS-
tag sequence and ngram-tag, and takes the
value of the motif-frequency if the motif
token-sequence gets a matching tag, and is
marked as with a matching ngram tag. e.g.,
fbgram(pi−1 = V B, pi = NN, yi = B2).
• Medians and maxima of pairwise collocation
statistics for tokens for a particular size of
ngram motifs: we use the following statis-
tics: pointwise mutual information, Chi-
square statistic, and conditional probability.
We also used POS sensitive versions of these,
which performed much better than plain ver-
sions in our evaluations.
• Histogram counts of inflectional forms of to-
ken sequence for the corresponding ngram
motif and POS sequence: this features takes
the value of the count of inflectional forms
of an ngram that account for 90% of occur-
rences of all inflectional forms.
</listItem>
<footnote confidence="0.865686666666667">
2Here, I denotes the indicator function
3It is straightforward to preclude partial n-gram annota-
tions near sentence boundaries with prohibitive penalties.
</footnote>
<listItem confidence="0.977699333333333">
• Entropies of histogram distributions of inflec-
tional variants (described above).
• Features encoding syntactic rigidity: ratios
and log-ratios of frequencies of an ngram mo-
tif and variations by replacing a token using
near synonyms from its synset.
</listItem>
<bodyText confidence="0.999649888888889">
Additionally, a few feature for the segmenta-
tions model contained minor orthographic features
based on word shape (length and capitalization
patterns). Also, all numbers, URLs, and cur-
rency symbols were normalized to the special NU-
MERIC, URL, and CURRENCY tokens respec-
tively. Finally, a gazetteer feature checked for oc-
currences of motifs in a gazetteer of named enti-
ties.
</bodyText>
<subsectionHeader confidence="0.997079">
3.3 Representation learning
</subsectionHeader>
<bodyText confidence="0.999926125">
With the segmentation model described in the pre-
vious section, we process text from the English Gi-
gaword corpus and the Simple English Wikipedia
to partition sentences into motifs. Since the seg-
mentation model accounts for the contexts of the
entire sentence in determining motifs, different in-
stances of the same token could evoke different
meaning representations. Consider the following
sentences tagged by the segmentation model, that
would correspond to different representations of
the token ‘remains’: once as a standalone motif,
and once as part of an encompassing bigram motif
(‘remains classified’).
Hog prices have declined sharply , while the
cost of corn remains relatively high.
Even with the release of such documents, ques-
tions are not answered, since only the agency
knows what remains classified
Given constituent motifs of each sentence in the
data, we can now define neighbourhood distribu-
tions for unary or phrasal motifs in terms of other
motifs (as envisioned in Table 1). In our experi-
ments, we use a window-length of 5 adjoining mo-
tifs on either side to define the neighbourhood of
a constituent. Naturally, in the presence of multi-
word motifs, the neighbourhood boundary could
be more extended than in a conventional DSM.
With such neighbourhood contexts, the distri-
butional paradigm posits that semantic similarity
between a pair of motifs can be given by a sense
of ‘distance’ between the two distributions. Most
popularly, traditional measures of vector distance
</bodyText>
<page confidence="0.998314">
639
</page>
<bodyText confidence="0.999972542857143">
such as the cosine similarity, Euclidean distance
and City-block distance have been used in sev-
eral distributional approaches. Additionally, sev-
eral distance measures between discrete distribu-
tions exist in statistical literature, most famously
the Kullback Leibler divergence, Bhattacharyya
distance and the Hellinger distance. Recent work
(Lebret and Lebret, 2013) has shown that the
Hellinger distance is an especially effective mea-
sure in learning distributional embeddings, with
Hellinger PCA being much more computationally
inexpensive than neural language modeling ap-
proaches, while performing much better than stan-
dard PCA, and competitive with the state-of-the-
art in downstream evaluations. Hence, we use the
Hellinger measure between neighbourhood motif
distributions in learning representations.
The Hellinger distance between two categorical
distributions P = (p1...pk) and Q = (q1...qk) is
defined as:
The Hellinger measure has intuitively desir-
able properties: specifically, it can be seen
as the Euclidean distance between the square-
roots transformed distributions, where both vec-
tors √P and √Q are length-normalized under the
same(Euclidean) norm. Finally, we perform SVD
on the motif similarity matrix (with size of the or-
der of the total vocabulary in the corpus), and re-
tain the first k principal eigenvectors to obtain low-
dimensional vector representations that are more
convenient to work with. In our preliminary ex-
periments, we found that k = 300 gave quanti-
tatively good results, with marginal change with
added dimensionality. We use this setting for all
our experiments.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999928428571429">
In this section, we describe some experimental
evaluations and findings for our approach. We first
quantitatively and qualitatively analyze the perfor-
mance of the segmentation model, and then evalu-
ate the distributional motif representations learnt
by the model through two downstream applica-
tions.
</bodyText>
<subsectionHeader confidence="0.998473">
4.1 Motif segmentation
</subsectionHeader>
<bodyText confidence="0.993035454545455">
In an evaluation of the motif segmentations model
within the perspective of our framework, we be-
lieve that exact correspondence to human judg-
ment is unrealistic, since guiding principles for
defining motifs, such as semantic cohesion, are
hard to define and only serve as working princi-
ples. However, for purposes of relative compar-
ison, we quantitatively evaluate the performance
of the motif segmentation models on the fully an-
notated dataset. For this experiment, the gold-
annotated corpus was split into a training and test
sets in a 9:1 proportion. A small fraction of the
training split was set apart for development and
validation. For this evaluation, we considered a
motif boundary as correct only for an exact match,
i.e., when both its boundaries (left and right) were
correctly predicted. Also, since a majority of mo-
tifs are unary tokens, including them into consider-
ation artificially boosts the accuracy, whereas we
are more interested in the prediction of larger n-
gram tokens. Hence we report results on the per-
formance on only non-unary motifs.
</bodyText>
<table confidence="0.9996008">
P R F
Rule-based baseline 0.85 0.10 0.18
Supervised 0.62 0.28 0.39
Semi-supervised 0.30 0.17 0.22
Supervised + annealing 0.69 0.38 0.49
</table>
<tableCaption confidence="0.99645">
Table 2: Results for motif segmentations
</tableCaption>
<bodyText confidence="0.99932945">
Table 2 shows the performance of the segmen-
tation model with the three proposed learning ap-
proaches described earlier. For a baseline, we
consider a rule-based model that simply learns all
ngram segmentations seen in the training data, and
marks any occurrence of a matching token se-
quence as a motif; without taking neighbouring
context into account. We observe that this model
has a very high precision (since many token se-
quences marked as motifs would recur in simi-
lar contexts, and would thus have the same mo-
tif boundaries). However, the rule-based method
has a very row recall due to lack of generaliza-
tion capabilities. We see that while all three learn-
ing algorithms perform better than the baseline,
the performance of the purely unsupervised sys-
tem is inferior to supervised approaches. This is
not unexpected: the supervision provided to the
model is very weak due to a lack of negative ex-
amples (which leads to spurious motif taggings,
</bodyText>
<equation confidence="0.944909909090909">
� � √ �
p
�P−Q�2
1
1
H(P, Q) = √2
(√pi − √qi)2
√2
tu u v
Xk
i=1
</equation>
<page confidence="0.971966">
640
</page>
<bodyText confidence="0.923876428571429">
While men often (openly or privately) sympathized with Prince Charles when the princess went public
about her rotten marriage, women cheered her on.
The healthcare initiative has become a White elephant for the federal government.
Chirac and Juppe have made a bad situation worse by seeking to meet Maastricht criteria not by
cutting spending, but by raising taxes still further.
Now, say Vatican observers, Pope John Paul II wants to show the world that many church members
did resist the Third Reich and paid the price.
</bodyText>
<tableCaption confidence="0.994305">
Table 3: Examples of output from sentence segmentation model
</tableCaption>
<bodyText confidence="0.996699333333333">
leading to a low precision), as well as no examples
of transitions between adjacent motifs (to learn
transitional weights and penalties). The super-
vised model expectedly outperforms both the rule-
based and the semi-supervised systems. However,
the supervised learning model with subsequent an-
nealing outperforms the supervised model in terms
of both precision and recall; showing the utility of
the semi-supervised method when seeded with a
good initial model, and the additive value of par-
tially labeled data.
Qualitative analysis of motif-segmented sen-
tences shows that our designed feature-set is effec-
tive and helpful in identifying semantically cohe-
sive ngrams. Table 3 provides four examples. The
first example correctly identifies ‘went public’,
while missing out on the potential motif ‘cheered
her on’. In general, these examples illustrate that
the model can identify idiomatic and idiosyncratic
themes as well as commonly recurrent ngrams (in
the second example, the model picks out ‘has be-
come’ which is highly recurrent, but doesn’t have
the semantic cohesiveness of some of the other
motifs). In particular, consider the second exam-
ple, where the model picks ‘white elephant’ as a
motif. In such cases, the disambiguating influence
of context incorporated by the motif is apparent.
Elephant White elephant
tusks expensive
trunk spend
african biggest
white the project
indian very high
baby multibillion dollar
The above table shows some of the top results
for the unary token ‘elephant’ by frequency, and
frequent unary and non-unary motifs for the mo-
tif ‘white elephant’ retrieved by the segmentation
model.
</bodyText>
<subsectionHeader confidence="0.997443">
4.2 Distributional representations
</subsectionHeader>
<bodyText confidence="0.999917956521739">
For evaluating distributional representations for
motifs (in terms of other motifs) learnt by the
framework, we test these representations in two
downstream tasks: sentence polarity classification
and metaphor detection. For sentence polarity, we
consider the Cornell Sentence Polarity corpus by
Pang and Lee (2005), where the task is to classify
the polarity of a sentence as positive or negative.
The data consists of 10662 sentences from movie
reviews that have been annotated as either posi-
tive or negative. For composing the motifs repre-
sentations to get judgments on semantic similarity
of sentences, we use our recent Vector Tree Ker-
nel approach The VTK approach defines a convo-
lutional kernel over graphs defined by the depen-
dency parses of sentences, using a vector repre-
sentation at each graph node that representing a
single lexical token. For our purposes, we mod-
ify the approach to merge the nodes of all tokens
that constitute a motif occurrence, and use the mo-
tif representation as the vector associated with the
node. Table 4 shows results for the sentence polar-
ity task.
</bodyText>
<table confidence="0.999409333333333">
P R F1
DSM 0.56 0.50 0.53
AVM 0.55 0.53 0.54
MVM 0.55 0.49 0.52
VTK 0.65 0.58 0.62
VTK + MotifDSM 0.66 0.60 0.63
</table>
<tableCaption confidence="0.998661">
Table 4: Results for Sentence Polarity detection
</tableCaption>
<bodyText confidence="0.9999256">
For this task, the motif based distributional em-
beddings vastly outperform a conventional distri-
butional model (DSM) based on token distribu-
tions, as well as additive (AVM) and multiplica-
tive (MVM) models of vector compositionality, as
</bodyText>
<page confidence="0.995449">
641
</page>
<bodyText confidence="0.6800325">
proposed by Lapata et al. The model is compet-
itive with the state-of-the-art VTK (Srivastava et
al., 2013) that uses the SENNA neural embeddings
by Collobert et al. (2011).
</bodyText>
<table confidence="0.9995736">
P R F1
CRF 0.74 0.50 0.59
SVM+DSM 0.63 0.80 0.71
VTK+ SENNA 0.67 0.87 0.76
VTK+ MotifDSM 0.70 0.87 0.78
</table>
<tableCaption confidence="0.998176">
Table 5: Results for Metaphor identification
</tableCaption>
<bodyText confidence="0.9999055625">
On the metaphor detection task, we use the
Metaphor dataset (Hovy et al., 2013). The data
consists of sentences with defined phrases, and
the task consists of identifying the linguistic use
in these phrases as metaphorical or literal. For
this task, the motif based model is expected to
perform well as common metaphorical usage is
generally through idiosyncratic MWEs, which the
motif based models is specially geared to capture
through the features of the segmentation model.
For this task, we again use the VTK formalism
for combining vector representations of the indi-
vidual motifs. Table 5 shows that the motif-based
DSM does better than discriminative models such
as CRFs and SVMs, and also slightly improves on
the VTK kernel with distributional embeddings.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999870916666667">
We have presented a new frequency-driven frame-
work for distributional semantics of not only lex-
ical items but also longer cohesive motifs. The
theme of this work is a general paradigm of seek-
ing motifs that are recurrent in common parlance,
are semantically coherent, and are possibly non-
compositional. Such a framework for distribu-
tional models avoids the issue of data sparsity
in learning of representations for larger linguis-
tic structures. The approach depends on drawing
features from frequency statistics, statistical cor-
relations, and linguistic theories; and this work
provides a computational framework to jointly
model recurrence and semantic cohesiveness of
motifs through compositional penalties and affin-
ity scores in a data driven way.
While being deliberately vague in our work-
ing definition of motifs, we have presented simple
efficient formulations to extract such motifs that
uses both annotated as well as partially unanno-
tated data. The qualitative and quantitative analyis
of results from our preliminary motif segmenta-
tion model indicate that such motifs can help to
disambiguate contexts of single tokens, and pro-
vide cleaner, more interpretable representations.
Finally, we obtain motif representations in form
of low-dimensional vector-space embeddings, and
our experimental findings indicate value of the
learnt representations in downstream applications.
We believe that the approach has considerable the-
oretical as well as practical merits, and provides a
simple and clean formulation for modeling phrasal
and sentential semantics.
In particular, we believe that ours is the first
method that can invoke different meaning repre-
sentations for a token depending on textual context
of the sentence. The flexibility of having separate
representations to model different semantic senses
has considerable valuable, as compared with ex-
tant approaches that assign a single representation
to each token, and are hence constrained to con-
flate several semantic senses into a common repre-
sentation. The approach also elegantly deals with
the problematic issue of differential compositional
and non-compositional usage of words. Future
work can focus on a more thorough quantitative
evaluation of the paradigm, as well as extension to
model non-contiguous motifs.
</bodyText>
<sectionHeader confidence="0.996998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9586812">
Colin Bannard. 2007. A measure of syntactic flexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop
on a Broader Perspective on Multiword Expressions,
pages 1–8. Association for Computational Linguis-
tics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193. Association for Computational Linguis-
tics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263–270. Asso-
ciation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Association for Computational Linguistics.
</reference>
<page confidence="0.977939">
642
</page>
<reference confidence="0.999904155963303">
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1034–1046. Asso-
ciation for Computational Linguistics.
James Richard Curran. 2003. From Distributional to
Semantic Similarity. Ph.D. thesis, Institute for Com-
municating and Collaborative Systems School of In-
formatics University of Edinburgh.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A structured distributional semantic
model: Integrating structure with semantics. ACL
2013, page 20.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394–1404. Asso-
ciation for Computational Linguistics.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2010.
Concrete sentence spaces for compositional dis-
tributional models of meaning. arXiv preprint
arXiv:1101.0309.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. Meta4NLP
2013, page 52.
R´emi Lebret and Ronan Lebret. 2013. Word
emdeddings through hellinger pca. arXiv preprint
arXiv:1312.5542.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639–654.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL, pages
236–244.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318–329.
Springer.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL.
Pavel Pecina. 2008. A machine learning approach to
multiword expression extraction. In Proceedings of
the LREC MWE 2008 Workshop, pages 54–57. Cite-
seer.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining
a semantic tagger and a statistical tool for mwe ex-
traction. Computer Speech &amp; Language, 19(4):378–
397.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and
Aline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions.
In Proceedings of the LREC Workshop-Towards
a Shared Task for Multiword Expressions (MWE
2008), pages 50–53.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy.
2013. A walk-based semantically enriched tree
kernel over distributed word representations. In
EMNLP, pages 1411–1416.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. Brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102–107. Association for Computational Lin-
guistics.
Yulia Tsvetkov and Shuly Wintner. 2011. Identifica-
tion of multi-word expressions by combining mul-
tiple linguistic information sources. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 836–845. Association
for Computational Linguistics.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In EMNLP-
CoNLL, pages 1034–1043.
</reference>
<page confidence="0.999171">
643
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981822">
<title confidence="0.996242">Vector space semantics with frequency-driven motifs</title>
<author confidence="0.999855">Shashank Srivastava Eduard Hovy</author>
<affiliation confidence="0.999984">Carnegie Mellon University Carnegie Mellon University</affiliation>
<address confidence="0.999741">Pittsburgh, PA 15217 Pittsburgh, PA 15217</address>
<email confidence="0.998377">ssrivastava@cmu.eduhovy@cmu.edu</email>
<abstract confidence="0.999471708333333">Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items. In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohelineal constituents, or The framework subsumes issues such as differential compositional as well as noncompositional behavior of phrasal consituents, and circumvents some problems of data sparsity by design. We design a segmentation model to optimally partition a sentence into lineal constituents, which can be used to define distributional contexts that are less noisy, semantically more interpretable, and linguistically disambiguated. Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
</authors>
<title>A measure of syntactic flexibility for automatically identifying multiword expressions in corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on a Broader Perspective on Multiword Expressions,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12306" citStr="Bannard (2007)" startWordPosition="1837" endWordPosition="1838">es have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsume most of these ideas. It is worthwhile to point out that the task of motif segmentation is slightly different from MWE identification. Specifically, the onus on recurrent occurrences means that nondecomposibility is not an essential consideration for a word to be considered a m</context>
</contexts>
<marker>Bannard, 2007</marker>
<rawString>Colin Bannard. 2007. A measure of syntactic flexibility for automatically identifying multiword expressions in corpora. In Proceedings of the Workshop on a Broader Perspective on Multiword Expressions, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9488" citStr="Collins and Duffy, 2002" startWordPosition="1410" endWordPosition="1413">and trigrams, methods such as Mitchell and Lapata (2008)and (Socher et al., 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. Finally, the assumption that semantic meanings for sentences could have representations similar to those for smaller individual tokens is in some sense unintuitive, and not supported by linguistic or semantic theories. 2.2 Tree kernels Tree Kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 263–270. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18003" citStr="Collins (2002)" startWordPosition="2737" endWordPosition="2738">res and penalties. We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision. Supervised learning: In the supervised case, optimal state sequences y(k) are fully observed for the training set. For this purpose, we created a dataset of 1000 sentences from the Simple English Wikipedia and the Gigaword Corpus, and manually annotated it with motif boundaries using BRAT (Stenetorp et al., 2012). In this case, learning can follow the online structured perceptron learning procedure by Collins (2002), where weights updates for the k’th training example (x(k), y(k)) are given as: wi ← wi + α(fi(x(k),y(k)) − fi(x(k),y�)) Here y&apos; = Decode(x(k), w) is the optimal Viterbi decoding using the current estimates of the weights. Updates are run for a large number of iterations until the change in objective drops below a threshold, and the learning rate α is adaptively modified as described in Collins et al. Implicitly, the weight learning algorithm can be seen as a gradient descent procedure minimizing the difference between the scores of highest scoring (Viterbi) state sequences, and the label sta</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="33229" citStr="Collobert et al. (2011)" startWordPosition="5156" endWordPosition="5159">e 4 shows results for the sentence polarity task. P R F1 DSM 0.56 0.50 0.53 AVM 0.55 0.53 0.54 MVM 0.55 0.49 0.52 VTK 0.65 0.58 0.62 VTK + MotifDSM 0.66 0.60 0.63 Table 4: Results for Sentence Polarity detection For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as 641 proposed by Lapata et al. The model is competitive with the state-of-the-art VTK (Srivastava et al., 2013) that uses the SENNA neural embeddings by Collobert et al. (2011). P R F1 CRF 0.74 0.50 0.59 SVM+DSM 0.63 0.80 0.71 VTK+ SENNA 0.67 0.87 0.76 VTK+ MotifDSM 0.70 0.87 0.78 Table 5: Results for Metaphor identification On the metaphor detection task, we use the Metaphor dataset (Hovy et al., 2013). The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal. For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the feature</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1034--1046</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9976" citStr="Croce et al. (2011)" startWordPosition="1486" endWordPosition="1489">e gained popularity in the last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between wordnodes very weakly. Figure 1 shows an example of the shortcomings of this general approach. Figure 1: Token</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1034–1046. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Richard Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Institute for Communicating and Collaborative Systems School of Informatics University of Edinburgh.</institution>
<contexts>
<context position="3523" citStr="Curran, 2003" startWordPosition="519" endWordPosition="520">otivates the need for a frequency-driven view of lexical semantics. In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below. Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics (Turney et al., 2010). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al., 2008). However, while conventional DSMs consider colloca634 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634–643, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics With the bad press in wake of the financial crisis, businesses are leaving our shores. crisis: &lt;bad, businesses, financial, leaving, press, shores, wake&gt; financial crisis: &lt;bad press, businesses, in wake of, leaving our shores&gt; Table 1: Meaning representation by conventional DSMs vs notional ideal </context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>James Richard Curran. 2003. From Distributional to Semantic Similarity. Ph.D. thesis, Institute for Communicating and Collaborative Systems School of Informatics University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3421" citStr="Erk, 2007" startWordPosition="506" endWordPosition="507">ervers. Second, the value of learning a very infrequent semantic mapping is likely marginal. This motivates the need for a frequency-driven view of lexical semantics. In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below. Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics (Turney et al., 2010). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al., 2008). However, while conventional DSMs consider colloca634 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634–643, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics With the bad press in wake of the financial crisis, businesses are leaving our shores. crisis: &lt;bad, businesses, financial, leaving, press, shores, wake&gt; financial crisis: &lt;bad press, businesses, i</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kartik Goyal</author>
</authors>
<title>Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard Hovy.</title>
<date>2013</date>
<pages>20</pages>
<marker>Goyal, 2013</marker>
<rawString>Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard Hovy. 2013. A structured distributional semantic model: Integrating structure with semantics. ACL 2013, page 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8984" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="1333" endWordPosition="1336">While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings. While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008)and (Socher et al., 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. Finally, the assumption that semantic meanings for sentences could have representations similar to those for smaller individual tokens is in some sense unintuitive, and not supported by linguistic or semantic theories. 2.2 Tree kernels Tree Kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394–1404. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Stephen Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning. arXiv preprint arXiv:1101.0309.</title>
<date>2010</date>
<contexts>
<context position="7931" citStr="Grefenstette et al. (2010)" startWordPosition="1176" endWordPosition="1179">s. While there is considerable variety in approaches and formulations, existing approaches for phrasal level and sentential semantics can broadly be partitioned into two categories. 2.1 Compositional approaches These have aimed at using semantic representations for individual words to learn semantic representations for larger linguistic structures. These methods implicitly make an assumption of compositionality, and often include explicit computational models of compositionality. Notable among such models are the additive and multiplicative models of composition by Mitchell and Lapata (2008), Grefenstette et al. (2010), Baroni and 635 Zamparelli’s (2010) model that differentially models content and function words for semantic composition, and Goyal et al.’s SDSM model (2013) that incorporates syntactic roles to model semantic composition. Notable among the most effective distributional representations are the recent deep-learning approaches by Socher et al. (2012), that model vector composition through non-linear transformations. While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of e</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2010</marker>
<rawString>Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2010. Concrete sentence spaces for compositional distributional models of meaning. arXiv preprint arXiv:1101.0309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Shashank Srivastava</author>
<author>Sujay Kumar Jauhar</author>
<author>Mrinmaya Sachan</author>
<author>Kartik Goyal</author>
<author>Huiying Li</author>
<author>Whitney Sanders</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying metaphorical word use with tree kernels.</title>
<date>2013</date>
<booktitle>Meta4NLP 2013,</booktitle>
<pages>52</pages>
<contexts>
<context position="33459" citStr="Hovy et al., 2013" startWordPosition="5197" endWordPosition="5200">f based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as 641 proposed by Lapata et al. The model is competitive with the state-of-the-art VTK (Srivastava et al., 2013) that uses the SENNA neural embeddings by Collobert et al. (2011). P R F1 CRF 0.74 0.50 0.59 SVM+DSM 0.63 0.80 0.71 VTK+ SENNA 0.67 0.87 0.76 VTK+ MotifDSM 0.70 0.87 0.78 Table 5: Results for Metaphor identification On the metaphor detection task, we use the Metaphor dataset (Hovy et al., 2013). The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal. For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model. For this task, we again use the VTK formalism for combining vector representations of the individual motifs. Table 5 shows that the motif-based DSM does better than discriminative models such as CRFs a</context>
</contexts>
<marker>Hovy, Srivastava, Jauhar, Sachan, Goyal, Li, Sanders, Hovy, 2013</marker>
<rawString>Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whitney Sanders, and Eduard Hovy. 2013. Identifying metaphorical word use with tree kernels. Meta4NLP 2013, page 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Lebret</author>
</authors>
<title>Word emdeddings through hellinger pca. arXiv preprint arXiv:1312.5542.</title>
<date>2013</date>
<contexts>
<context position="25506" citStr="Lebret and Lebret, 2013" startWordPosition="3916" endWordPosition="3919">onal DSM. With such neighbourhood contexts, the distributional paradigm posits that semantic similarity between a pair of motifs can be given by a sense of ‘distance’ between the two distributions. Most popularly, traditional measures of vector distance 639 such as the cosine similarity, Euclidean distance and City-block distance have been used in several distributional approaches. Additionally, several distance measures between discrete distributions exist in statistical literature, most famously the Kullback Leibler divergence, Bhattacharyya distance and the Hellinger distance. Recent work (Lebret and Lebret, 2013) has shown that the Hellinger distance is an especially effective measure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than neural language modeling approaches, while performing much better than standard PCA, and competitive with the state-of-theart in downstream evaluations. Hence, we use the Hellinger measure between neighbourhood motif distributions in learning representations. The Hellinger distance between two categorical distributions P = (p1...pk) and Q = (q1...qk) is defined as: The Hellinger measure has intuitively desirable pro</context>
</contexts>
<marker>Lebret, Lebret, 2013</marker>
<rawString>R´emi Lebret and Ronan Lebret. 2013. Word emdeddings through hellinger pca. arXiv preprint arXiv:1312.5542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="3477" citStr="McCarthy and Carroll, 2003" startWordPosition="511" endWordPosition="514"> very infrequent semantic mapping is likely marginal. This motivates the need for a frequency-driven view of lexical semantics. In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below. Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics (Turney et al., 2010). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al., 2008). However, while conventional DSMs consider colloca634 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634–643, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics With the bad press in wake of the financial crisis, businesses are leaving our shores. crisis: &lt;bad, businesses, financial, leaving, press, shores, wake&gt; financial crisis: &lt;bad press, businesses, in wake of, leaving our shores&gt; Table 1: Meaning represen</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>Diana McCarthy and John Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="7903" citStr="Mitchell and Lapata (2008)" startWordPosition="1172" endWordPosition="1175">et with only marginal success. While there is considerable variety in approaches and formulations, existing approaches for phrasal level and sentential semantics can broadly be partitioned into two categories. 2.1 Compositional approaches These have aimed at using semantic representations for individual words to learn semantic representations for larger linguistic structures. These methods implicitly make an assumption of compositionality, and often include explicit computational models of compositionality. Notable among such models are the additive and multiplicative models of composition by Mitchell and Lapata (2008), Grefenstette et al. (2010), Baroni and 635 Zamparelli’s (2010) model that differentially models content and function words for semantic composition, and Goyal et al.’s SDSM model (2013) that incorporates syntactic roles to model semantic composition. Notable among the most effective distributional representations are the recent deep-learning approaches by Socher et al. (2012), that model vector composition through non-linear transformations. While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event corefe</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In ACL, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>318--329</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9506" citStr="Moschitti, 2006" startWordPosition="1414" endWordPosition="1415">h as Mitchell and Lapata (2008)and (Socher et al., 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. Finally, the assumption that semantic meanings for sentences could have representations similar to those for smaller individual tokens is in some sense unintuitive, and not supported by linguistic or semantic theories. 2.2 Tree kernels Tree Kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels through the use of</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Machine Learning: ECML 2006, pages 318–329. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="31883" citStr="Pang and Lee (2005)" startWordPosition="4925" endWordPosition="4928">n biggest white the project indian very high baby multibillion dollar The above table shows some of the top results for the unary token ‘elephant’ by frequency, and frequent unary and non-unary motifs for the motif ‘white elephant’ retrieved by the segmentation model. 4.2 Distributional representations For evaluating distributional representations for motifs (in terms of other motifs) learnt by the framework, we test these representations in two downstream tasks: sentence polarity classification and metaphor detection. For sentence polarity, we consider the Cornell Sentence Polarity corpus by Pang and Lee (2005), where the task is to classify the polarity of a sentence as positive or negative. The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative. For composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token. For our purposes, we modify the approach to merge the nodes of a</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>A machine learning approach to multiword expression extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC MWE 2008 Workshop,</booktitle>
<pages>54--57</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="11803" citStr="Pecina, 2008" startWordPosition="1766" endWordPosition="1767">tifs such as ‘water table’ or ‘breaking a fall’ would be helpful in building more effective semantic representations. A significant advantage of a frequency driven view is that it makes the concern of compositionality of recurrent phrases immaterial. If a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired. 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classificati</context>
</contexts>
<marker>Pecina, 2008</marker>
<rawString>Pavel Pecina. 2008. A machine learning approach to multiword expression extraction. In Proceedings of the LREC MWE 2008 Workshop, pages 54–57. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Songlin Piao</author>
<author>Paul Rayson</author>
<author>Dawn Archer</author>
<author>Tony McEnery</author>
</authors>
<title>Comparing and combining a semantic tagger and a statistical tool for mwe extraction.</title>
<date>2005</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>397</pages>
<contexts>
<context position="11881" citStr="Piao et al., 2005" startWordPosition="1775" endWordPosition="1778">ding more effective semantic representations. A significant advantage of a frequency driven view is that it makes the concern of compositionality of recurrent phrases immaterial. If a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired. 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the gener</context>
</contexts>
<marker>Piao, Rayson, Archer, McEnery, 2005</marker>
<rawString>Scott Songlin Piao, Paul Rayson, Dawn Archer, and Tony McEnery. 2005. Comparing and combining a semantic tagger and a statistical tool for mwe extraction. Computer Speech &amp; Language, 19(4):378– 397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Paulo Schreiner</author>
<author>Marco Idiart</author>
<author>Aline Villavicencio</author>
</authors>
<title>An evaluation of methods for the extraction of multiword expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC Workshop-Towards a Shared Task for Multiword Expressions (MWE</booktitle>
<pages>50--53</pages>
<contexts>
<context position="12050" citStr="Ramisch et al. (2008)" startWordPosition="1801" endWordPosition="1804"> immaterial. If a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired. 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsume most of these ideas. </context>
</contexts>
<marker>Ramisch, Schreiner, Idiart, Villavicencio, 2008</marker>
<rawString>Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline Villavicencio. 2008. An evaluation of methods for the extraction of multiword expressions. In Proceedings of the LREC Workshop-Towards a Shared Task for Multiword Expressions (MWE 2008), pages 50–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8283" citStr="Socher et al. (2012)" startWordPosition="1228" endWordPosition="1231">ethods implicitly make an assumption of compositionality, and often include explicit computational models of compositionality. Notable among such models are the additive and multiplicative models of composition by Mitchell and Lapata (2008), Grefenstette et al. (2010), Baroni and 635 Zamparelli’s (2010) model that differentially models content and function words for semantic composition, and Goyal et al.’s SDSM model (2013) that incorporates syntactic roles to model semantic composition. Notable among the most effective distributional representations are the recent deep-learning approaches by Socher et al. (2012), that model vector composition through non-linear transformations. While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings. While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, meth</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shashank Srivastava</author>
<author>Dirk Hovy</author>
<author>Eduard H Hovy</author>
</authors>
<title>A walk-based semantically enriched tree kernel over distributed word representations.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1411--1416</pages>
<contexts>
<context position="10005" citStr="Srivastava et al. (2013)" startWordPosition="1491" endWordPosition="1494">he last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between wordnodes very weakly. Figure 1 shows an example of the shortcomings of this general approach. Figure 1: Tokenwise syntactic and semantic s</context>
<context position="33164" citStr="Srivastava et al., 2013" startWordPosition="5145" endWordPosition="5148"> motif representation as the vector associated with the node. Table 4 shows results for the sentence polarity task. P R F1 DSM 0.56 0.50 0.53 AVM 0.55 0.53 0.54 MVM 0.55 0.49 0.52 VTK 0.65 0.58 0.62 VTK + MotifDSM 0.66 0.60 0.63 Table 4: Results for Sentence Polarity detection For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as 641 proposed by Lapata et al. The model is competitive with the state-of-the-art VTK (Srivastava et al., 2013) that uses the SENNA neural embeddings by Collobert et al. (2011). P R F1 CRF 0.74 0.50 0.59 SVM+DSM 0.63 0.80 0.71 VTK+ SENNA 0.67 0.87 0.76 VTK+ MotifDSM 0.70 0.87 0.78 Table 5: Results for Metaphor identification On the metaphor detection task, we use the Metaphor dataset (Hovy et al., 2013). The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal. For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the moti</context>
</contexts>
<marker>Srivastava, Hovy, Hovy, 2013</marker>
<rawString>Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy. 2013. A walk-based semantically enriched tree kernel over distributed word representations. In EMNLP, pages 1411–1416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
<author>Sampo Pyysalo</author>
<author>Goran Topi´c</author>
<author>Tomoko Ohta</author>
<author>Sophia Ananiadou</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Brat: a web-based tool for nlp-assisted text annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>102--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Stenetorp, Pyysalo, Topi´c, Ohta, Ananiadou, Tsujii, 2012</marker>
<rawString>Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012. Brat: a web-based tool for nlp-assisted text annotation. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 102–107. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Shuly Wintner</author>
</authors>
<title>Identification of multi-word expressions by combining multiple linguistic information sources.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>836--845</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12027" citStr="Tsvetkov and Wintner, 2011" startWordPosition="1797" endWordPosition="1800">ionality of recurrent phrases immaterial. If a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired. 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsum</context>
</contexts>
<marker>Tsvetkov, Wintner, 2011</marker>
<rawString>Yulia Tsvetkov and Shuly Wintner. 2011. Identification of multi-word expressions by combining multiple linguistic information sources. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 836–845. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
<author>Valia Kordoni</author>
<author>Yi Zhang</author>
<author>Marco Idiart</author>
<author>Carlos Ramisch</author>
</authors>
<title>Validation and evaluation of automatically acquired multiword expressions for grammar engineering. In EMNLPCoNLL,</title>
<date>2007</date>
<pages>1034--1043</pages>
<contexts>
<context position="11832" citStr="Villavicencio et al., 2007" startWordPosition="1768" endWordPosition="1771">water table’ or ‘breaking a fall’ would be helpful in building more effective semantic representations. A significant advantage of a frequency driven view is that it makes the concern of compositionality of recurrent phrases immaterial. If a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired. 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phr</context>
</contexts>
<marker>Villavicencio, Kordoni, Zhang, Idiart, Ramisch, 2007</marker>
<rawString>Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco Idiart, and Carlos Ramisch. 2007. Validation and evaluation of automatically acquired multiword expressions for grammar engineering. In EMNLPCoNLL, pages 1034–1043.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>