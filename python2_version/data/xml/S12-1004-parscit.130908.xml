<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000493">
<title confidence="0.9941885">
Towards Building a Multilingual Semantic Network:
Identifying Interlingual Links in Wikipedia
</title>
<author confidence="0.907005">
Bharath Dandala Rada Mihalcea Razvan Bunescu
</author>
<affiliation confidence="0.9982655">
Dept. of Computer Science Dept. of Computer Science School of EECS
University of North Texas University of North Texas Ohio University
</affiliation>
<address confidence="0.842661">
Denton, TX Denton, TX Athens, Ohio
</address>
<email confidence="0.996244">
BharathDandala@my.unt.edu rada@cs.unt.edu bunescu@ohio.edu
</email>
<sectionHeader confidence="0.995568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988019047619">
Wikipedia is a Web based, freely available
multilingual encyclopedia, constructed in a
collaborative effort by thousands of contribu-
tors. Wikipedia articles on the same topic in
different languages are connected via interlin-
gual (or translational) links. These links serve
as an excellent resource for obtaining lexical
translations, or building multilingual dictio-
naries and semantic networks. As these links
are manually built, many links are missing
or simply wrong. This paper describes a su-
pervised learning method for generating new
links and detecting existing incorrect links.
Since there is no dataset available to evaluate
the resulting interlingual links, we create our
own gold standard by sampling translational
links from four language pairs using distance
heuristics. We manually annotate the sampled
translation links and used them to evaluate the
output of our method for automatic link detec-
tion and correction.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999813545454545">
In recent years, Wikipedia has been used as a re-
source of world knowledge in many natural lan-
guage processing applications. A diverse set of
tasks such as text categorization, information ex-
traction, information retrieval, question answering,
word sense disambiguation, semantic relatedness,
and named entity recognition have been shown to
benefit from the semi-structured text of Wikipedia.
Most approaches that use the world knowledge en-
coded in Wikipedia are statistical in nature and
therefore their performance depends significantly
</bodyText>
<page confidence="0.972765">
30
</page>
<bodyText confidence="0.999407454545454">
on the size of Wikipedia. Currently, the English
Wikipedia alone has four million articles. However,
the combined Wikipedias for all other languages
greatly exceed the English Wikipedia in size, yield-
ing a combined total of more than 10 million arti-
cles in more than 280 languages.1 The rich hyper-
link structure of these Wikipedia corpora in different
languages can be very useful in identifying various
relationships between concepts.
Wikipedia articles on the same topic in different
languages are often connected through interlingual
links. These links are the small navigation links
that show up in the “Languages” sidebar in most
Wikipedia articles, and they connect an article with
related articles in other languages. For instance,
the interlingual links for the Wikipedia article about
”Football” connect it to 20 articles in 20 different
languages. In the ideal case, a set of articles con-
nected directly or indirectly via such links would all
describe the same entity or concept. However, these
links are produced either by polyglot editors or by
automatic bots. Editors commonly make mistakes
by linking articles that have conceptual drift, or by
linking to a concept at a different level of granularity.
For instance, if a corresponding article in one of the
languages does not exist, a similar article or a more
general article about the concept is sometimes linked
instead. Various bots also add new interlingual links
or attempt to correct existing ones. The downside of
a bot is that an error in a translational link created
by editors in Wikipedia for one language propagates
to Wikipedias in other languages. Thus, if a bot in-
troduces a wrong link, one may have to search for
</bodyText>
<footnote confidence="0.965088">
1http://en.wikipedia.org/wiki/Wikipedia:Size of Wikipedia
</footnote>
<note confidence="0.9876945">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 30–37,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.996962272727273">
Language Code Articles Redirects Users
English en 4,674,066 4,805,557 16,503,562
French fr 3,298,615 789,408 1,250,266
German de 3,034,238 678,288 1,398,424
Italian it 2,874,747 319,179 731,750
Polish pl 2,598,797 158,956 481,079
Spanish es 2,587,613 504,062 2,162,925
Dutch nl 2,530,250 226,201 446,458
Russian ru 2,300,769 682,402 819,812
Japanese jp 1,737,565 372,909 607,152
Chinese cn 1,199,912 333,436 1,171,148
</table>
<tableCaption confidence="0.977405">
Table 1: Number of articles, redirects, and users for the top nine Wikipedia editions plus Chinese. The total number
of articles also includes the disambiguation pages.
</tableCaption>
<bodyText confidence="0.999496291666667">
the underlying error in a different language version
of Wikipedia.
The contributions of the research described in this
paper are two-fold. First, we describe the construc-
tion of a dataset of interlingual links that are auto-
matically sampled from Wikipedia based on a set of
distance heuristics. This dataset is manually anno-
tated in order to enable the evaluation of methods
for translational link detection. Second, we describe
an automatic model for correcting existing links and
creating new links, with the aim of obtaining a more
stable set of interlingual links. The model’s param-
eters are estimated on the manually labeled dataset
using a supervised machine learning approach.
The remaining of this paper is organized as fol-
lows: Section 2 briefly describes Wikipedia and
the relevant terminology. Section 3 introduces our
method of identifying a candidate set of translational
links based on distance heuristics, while Section 4
introduces the methodology for building a manually
annotated dataset. Section 5 describes the machine
learning experiments for detecting or correcting in-
terlingual links. Finally, we present related work in
Section 6, and concluding remarks in Section 7.
</bodyText>
<sectionHeader confidence="0.99415" genericHeader="introduction">
2 Wikipedia
</sectionHeader>
<bodyText confidence="0.999270138888889">
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
webpage, and this “freedom of contribution” has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential errors
are quickly corrected within the collaborative envi-
ronment) of this online resource.
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or an
event, and consists of a hypertext document with hy-
perlinks to other pages within or outside Wikipedia.
The role of the hyperlinks is to guide the reader to
pages that provide additional information about the
entities or events mentioned in an article. Articles
are organized into categories, which in turn are or-
ganized into category hierarchies. For instance, the
article automobile is included in the category vehi-
cle, which in turn has a parent category named ma-
chine, and so forth.
Each article in Wikipedia is uniquely referenced
by an identifier, consisting of one or more words
separated by spaces or underscores and occasionally
a parenthetical explanation. For example, the article
for bar with the meaning of “counterfor drinks” has
the unique identifier bar (counter).
Wikipedia editions are available for more than
280 languages, with a number of entries vary-
ing from a few pages to three millions articles or
more per language. Table 1 shows the nine largest
Wikipedias (as of March 2012) and the Chinese
Wikipedia, along with the number of articles and ap-
proximate number of contributors.2
The ten languages mentioned above are also the
languages used in our experiments. Note that Chi-
</bodyText>
<footnote confidence="0.992166">
2http://meta.wikimedia.org/wiki/List of Wikipedias
#Grand Total
</footnote>
<page confidence="0.999543">
31
</page>
<sectionHeader confidence="0.568826" genericHeader="method">
Relation Exists Via
</sectionHeader>
<equation confidence="0.757746333333333">
SYMMETRY
en=Ball de=Ball Yes -
en=Hentriacontane it=Entriacontano No -
TRANSITIVITY
en=Deletion (phonology) fr=Amuissement Yes nl=Deletie (taalkunde)
en=Electroplating fr=Galvanoplastie No -
REDIRECTIONS
en=Gun Dog de=Schiesshund Yes de=Jagdhund
en=Ball de=Ball No -
</equation>
<tableCaption confidence="0.952093">
Table 2: Symmetry, transitivity, and redirections in Wikipedia
</tableCaption>
<bodyText confidence="0.999802">
nese is the twelfth largest Wikipedia, but we decided
to include it at the cost of not covering the tenth
largest Wikipedia (Portuguese), which has close
similarities with other languages already covered
(e.g., French, Italian, Spanish).
Relevant for the work described in this paper are
the interlingual links, which explicitly connect arti-
cles in different languages. For instance, the English
article for bar (unit) is connected, among others, to
the Italian article bar (unit´a di misura) and the Pol-
ish article bar (jednostka). On average, about half of
the articles in a Wikipedia version include interlin-
gual links to articles in other languages. The number
of interlingual links per article varies from an aver-
age of five in the English Wikipedia, to ten in the
Spanish Wikipedia, and as many as 23 in the Arabic
Wikipedia.
</bodyText>
<sectionHeader confidence="0.9299175" genericHeader="method">
3 Identifying Interlingual Links in
Wikipedia
</sectionHeader>
<bodyText confidence="0.999978357142857">
The interlingual links connecting Wikipedias in dif-
ferent languages should ideally be symmetric and
transitive. The symmetry property indicates that if
there is an interlingual link Aα —* Aβ between two
articles, one in language α and one in language Q,
then the reverse link Aα +— Aβ should also exist
in Wikipedia. According to the transitivity property,
the presence of two links Aα —* Aβ and Aβ —* Aγ
indicates that the link Aα —* Aγ should also exist
in Wikipedia, where α, Q and γ are three different
languages. While these properties are intuitive, they
are not always satisfied due to Wikipedia’s editorial
policy that accredits editors with the responsibility
of maintaining the articles. Table 2 shows actual
</bodyText>
<table confidence="0.996157857142857">
Total number Newly added
of links links
26,836,572 -
26,836,572 1,277,760
25,763,689 853,658
23,383,535 693,262
21,560,711 548,354
</table>
<tableCaption confidence="0.995787">
Table 3: Number of links identified in Wikipedia, as di-
</tableCaption>
<bodyText confidence="0.852163318181818">
rect, symmetric, or transitional links. The number of
newly added links, not known in the previous set of links,
is also indicated (e.g., DP3/RP3 adds 693,262 new links
not found by direct or symmetric links, or by direct or
reverse paths of length two).
cases in Wikipedia where these properties fail due
to missing interlingual links. The table also shows
examples where the editors link an article from one
language to a redirect page in another language.
In order to generate a normalized set of inter-
lingual links between Wikipedias, we replace all the
redirect pages with the corresponding original arti-
cles, so that each concept in a language is repre-
sented by one unique article. We then identify the
following four types of simple interlingual paths be-
tween articles in different languages:
DL: Direct links Aα —* Aβ between two articles.
RL: Reverse links Aα +— Aβ between two articles.
DPk: Direct, simple paths of length k between two
articles.
RPk: Reverse, simple paths of length k between
two articles.
</bodyText>
<figure confidence="0.9566238">
Link
type
DL
RL
DP2/RP2
DP3/RP3
DP4/RP4
32
Relation Number of paths
DL
en=Ball de=Ball 1
en=Ball it=Palla (sport) 1
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
RL
</figure>
<figureCaption confidence="0.9973785">
Figure 1: A small portion of the multilingual Wikipedia
graph.
</figureCaption>
<bodyText confidence="0.999063">
Figure 1 shows a small portion of the Wikipedia
graph, connecting Wikipedias in four languages:
English, German, Italian, and French. Correspond-
ingly, Table 4 shows a subset of the direct links DL,
reverse links RL, direct translation paths DPk and
reverse translation paths RPk of lengths k = 2, 3, 4
for the graph in the figure.
Using these distance heuristics, we are able to
extract or infer a very large number of interlingual
links. Table 3 shows the number of direct links ex-
tracted from the ten Wikipedias we currently work
with, as well as the number of paths that we add by
enforcing the symmetry and transitivity properties.
</bodyText>
<sectionHeader confidence="0.981525" genericHeader="method">
4 Manual Evaluation of the Interlingual
Links
</sectionHeader>
<bodyText confidence="0.9992288">
The translation links in Wikipedia, whether added
by the Wikipedia editors (direct links), or inferred by
the heuristics described in the previous section, are
not guaranteed for quality. In fact, previous work (de
Melo and Weikum, 2010b) has shown that a large
number of the links created by the Wikipedia users
are incorrect, connecting articles that are not transla-
tions of each other, subsections of articles, or disam-
biguation pages. We have therefore decided to run
a manual annotation study in order to determine the
quality of the interlingual links. The resulting anno-
tation can serve both as a gold standard for evaluat-
ing the quality of predicted links, and as supervision
for a machine learning model that would automati-
cally detect translation links.
</bodyText>
<figure confidence="0.989194693548388">
1
1
0
0
1
2
0
0
1
2
1
0
1
1
0
0
1
0
1
2
0
2
1
0
0
1
0
0
DP2
DP3
DP4
RP2
RP3
RP4
en=Ball de=Ball
en=Ball it=Palla(sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
</figure>
<tableCaption confidence="0.5919385">
Table 4: A subset of the direct links, reverse links, and
inferred direct and reverse paths for the graph in Figure 1
</tableCaption>
<page confidence="0.904383">
33
</page>
<figure confidence="0.9336098">
it=Palla(sport)
en=Ball
de=Ball
fr=Ballon(sport)
fr=Boule(solide)
</figure>
<table confidence="0.9797914">
Language pair 0 1 2 3 4
(English, German) 46 8 29 2 110
(English, Spanish) 22 19 19 13 123
(Italian, French) 30 7 19 7 132
(Spanish, Italian) 21 8 17 13 136
</table>
<tableCaption confidence="0.979251">
Table 6: Number of annotations on a scale of 0-4 for each
pair of languages
</tableCaption>
<bodyText confidence="0.998051416666667">
From the large pool of links directly available in
Wikipedia or inferred automatically through sym-
metry and transitivity, we sampled and then man-
ually annotated 195 pairs of articles for each of
four language pairs: (English, German), (English,
Spanish), (Italian, French), and (Spanish, Italian).
The four language pairs were determined based on
the native or near-native knowledge available in the
group of annotators in our research group. The sam-
pling of the article pairs was done such that it cov-
ers all the potentially interesting cases obtained by
combining the heuristics used to identify interlin-
gual links. The left side of Table 5 shows the com-
bination of heuristics used to select the article pairs.
For each such combination, and for each language
pair, we randomly selected 15 articles. Furthermore,
we added 15 randomly selected pairs for the highest
quality combination (Case 1).
For each language pair, the sampled links were
annotated by one human judge, with the exception of
the (English, Spanish) dataset, which was annotated
by two judges so that we could measure the inter-
annotator agreement. The annotators were asked to
check the articles in each link and annotate the link
on a scale from 0 to 4, as follows:
4: Identical concepts that are perfect translations
of each other.
3: Concepts very close in meaning, which are
good translations of each other, but a better
translation for one of the concepts in the pair
also exists. The annotators are not required to
identify a better translation in Wikipedia, they
only have to use their own knowledge of the
language, e.g. “building” (English) may be a
good translation for “tore” (Spanish), yet a bet-
ter translation is known to exist.
</bodyText>
<listItem confidence="0.656741">
2: Concepts that are closely related but that are not
translations of each other.
1: Concepts that are remotely related and are not
translations of each other.
0: Completely unrelated concepts or links be-
tween an article and a portion of another arti-
cle.
</listItem>
<bodyText confidence="0.999975256410257">
To determine the quality of the annotations,
we ran an inter-annotator study for the (English-
Spanish) language pair. The two annotators had a
Pearson correlation of 70%, which indicates good
agreement. We also calculated their agreement
when grouping the ratings from 0 to 4 in only two
categories: 0, 1, and 2 were mapped to no transla-
tion, whereas 3 and 4 were mapped to translation.
On this coarse scale, the annotators agreed 84% of
the time, with a kappa value of 0.61, which once
again indicate good agreement.
The annotations are summarized in the right side
of Table 5. For each quality rating, the table shows
the number of links annotated with that rating. Note
that this is a summary over the annotations of five
annotators, corresponding to the four language pairs,
as well as an additional annotation for (English,
Spanish).
Not surprisingly, the links that are “supported” by
all the heuristics considered (Case 1) are the links
with the highest quality. These are interlingual links
that are present in Wikipedia and that can also be
inferred through transitive path heuristics. Interest-
ingly, links that are only guaranteed to have a direct
link (DL) and no reverse link (RL) (Case 2) have a
rather low quality, with only 68% of the links being
considered to represent a perfect or a good transla-
tion (score of 3 or 4).
Table 6 summarizes the annotations per language
pair. There appear to be some differences in the
quality of interlingual links extracted or inferred for
different languages, with (Spanish, Italian) being the
pair with the highest quality of links (76% of the
links are either perfect or good translations), while
English to German seems to have the lowest quality
(only 57% of the links are perfect or good). For the
(English, Spanish) pair, we used the average of the
two annotators’ ratings, rounded up to the nearest
integer.
</bodyText>
<page confidence="0.992807">
34
</page>
<figure confidence="0.895776071428571">
Combinations of heuristics to extract or infer interlingual links Link quality on a 0-4 scale
Cases DL RL DP2 RP2 DP3 RP3 DP4 RP4 Samples 0 1 2 3 4
Case 1 y y y y y y y y 30 6 3 6 6 129
Case 2 y n - - - - - - 15 15 3 6 3 48
Case 3 n y - - - - - - 15 13 3 8 4 47
Case 4 n n y y - - - - 15 6 3 16 4 46
Case 5 n n - - y y - - 15 13 9 12 4 28
Case 6 n n - - - - y y 15 15 8 3 8 37
Case 7 n n n n - - - - 15 19 8 11 5 31
Case 8 n n - - n n - - 15 13 8 11 5 32
Case 9 n n - - - - n n 15 25 4 11 2 33
Case 10 y y n n - - - - 15 6 3 4 3 59
Case 11 y y - - n n - - 15 6 2 3 0 64
Case 12 y y - - - - n n 15 3 6 2 4 60
</figure>
<tableCaption confidence="0.9173445">
Table 5: Left side of the table: distance heuristics and number of samples based on each distance heuristic. ‘y’ indicates
that the corresponding path should exist, ‘n’ indicates that the corresponding path should not exist, ‘-’ indicates that
we don’t care whether the corresponding path exists or not. Right side of the table: manual annotations of the quality
of links, on a scale of 0 to 4, with 4 meaning perfect translations.
</tableCaption>
<sectionHeader confidence="0.986615" genericHeader="method">
5 Machine Learning Experiments
</sectionHeader>
<bodyText confidence="0.99997494">
The manual annotations described above are good
indicators of the quality of the interlingual links that
can be extracted and inferred in Wikipedia. But such
manual annotations, because of the human effort in-
volved, do not scale up, and therefore we cannot ap-
ply them on the entire interlingual Wikipedia graph
to determine the links that should be preserved or the
ones that should be removed.
Instead, we experiment with training machine
learning models that would automatically determine
the quality of an interlingual link. As features, we
use the presence or absence of direct or symmet-
ric links, along with the number of inferred paths of
length k = 2, 3, 4, as defined in Section 3. Table 7
shows the feature vectors for the same four pairs of
articles that were used in Table 4. The feature val-
ues are computed based on the sample network of
interlingual links from Figure 1. Each feature vector
is assigned a numerical class, corresponding to the
manual annotation provided by the human judges.
We conduct two experiments, at a fine-grained
and a coarse-grained level. In both experiments, we
use all the annotations for all four language pairs to-
gether (i.e., a total of 780 examples), and perform
evaluations in a ten-fold cross validation scenario.
For the fine-grained experiments, we use all five
numerical classes in a linear regression model.3 We
determine the correctness of the predictions on the
test data by calculating the Pearson correlation with
respect to the gold standard. The resulting corre-
lation was measured at 0.461. For comparison, we
also run an experiment where we only keep the pres-
ence or absence of the direct links as a feature (DL).
In this case, the correlation was measured at 0.418,
which is substantially below the correlation obtained
when using all the features. This indicates that the
interlingual links inferred through our heuristics are
indeed useful.
In the coarse-grained experiments, the quality rat-
ings 0, 1, and 2 are mapped to the no translation
label, while ratings 3 and 4 are mapped to the trans-
lation label. We used the Ada Boost classifier with
decision stumps as the binary classification algo-
rithm. When using the entire feature vectors, the
accuracy is measured at 73.97%, whereas the use
of only the direct links results in an accuracy of
69.35%. Similar to the fine-grained linear regres-
sion experiments, these coarse-grained experiments
further validate the utility of the interlingual links
inferred through the transitive path heuristics.
</bodyText>
<footnote confidence="0.847696">
3We use the Weka machine learning toolkit.
</footnote>
<page confidence="0.987936">
35
</page>
<table confidence="0.902278111111111">
Concept pair DL RL DP2 DP3 DP4 RP2 RP3 RP4 Class
en=Ball de=Ball
en=Ball it=Palla (sport)
en=Ball fr=Boule (solide)
de=Ball fr=Ballon (sport)
1 1 1 1 0 1 1 0 4
1 1 2 0 0 2 0 0 4
0 0 1 1 1 0 0 0 1
0 0 2 1 0 2 1 0 4
</table>
<tableCaption confidence="0.9999975">
Table 7: Examples of feature vectors generated for four interlingual links, corresponding to the concept pairs listed in
Table 4
</tableCaption>
<sectionHeader confidence="0.999841" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999991101694916">
The multilingual nature of Wikipedia has been al-
ready exploited to solve several number of language
processing tasks. A number of projects have used
Wikipedia to build a multilingual semantic knowl-
edge base by using the existing multilingual nature
of Wikipedia. For instance, (Ponzetto and Strube,
2007) derived a large scale taxonomy from the ex-
isting Wikipedia. In related work, (de Melo and
Weikum, 2010a) worked on a similar problem in
which they combined all the existing multilingual
Wikipedias to build a stable, large multilingual tax-
onomy.
The interlingual links have also been used for
cross-lingual information retrieval (Nguyen et al.,
2009) or to generate bilingual parallel corpora (Mo-
hammadi and QasemAghaee, 2010). (Ni et al.,
2011) used multilingual editions of Wikipedia to
mine topics for the task of cross lingual text clas-
sification, while (Hassan and Mihalcea, 2009) used
Wikipedias in different languages to measure cross-
lingual semantic relatedness between concepts and
texts in different languages. (Bharadwaj et al., 2010)
explored the use of the multilingual links to mine
dictionaries for under-resourced languages. They
developed an iterative approach to construct a par-
allel corpus, using the interlingual links, info boxes,
category pages, and abstracts, which they then be
used to extract a bilingual dictionary. (Navigli and
Ponzetto, 2010) explored the connections that can
be drawn between Wikipedia and WordNet. While
no attempts were made to complete the existing link
structure of Wikipedia, the authors made use of ma-
chine translation to enrich the resource.
The two previous works most closely related to
ours are the systems introduced in (Sorg and Cimi-
ano, 2008) and (de Melo and Weikum, 2010a; de
Melo and Weikum, 2010b). (Sorg and Cimiano,
2008) designed a system that predicts new interlin-
gual links by using a classification based approach.
They extract certain types of links from bilingual
Wikipedias, which are then used to create a set of
features for the machine learning system. In follow-
up work, (Erdmann et al., 2008; Erdmann et al.,
2009) used an expanded set of features, which also
accounted for direct links, redirects, and links be-
tween articles in Wikipedia, to identify entries for a
bilingual dictionary. In this line of work, the focus is
mainly on article content analysis, as a way to detect
new potential translations, rather than link analysis
as done in our work.
Finally, (de Melo and Weikum, 2010b) designed
a system that detects errors in the existing interlin-
gual links in Wikipedia. They show that there are a
large number of links that are imprecise or wrong,
and propose the use of a weighted graph to produce
a more consistent set of consistent interlingual links.
Their work is focusing primarily on correcting ex-
isting links in Wikipedia, rather than inferring new
links as we do.
</bodyText>
<sectionHeader confidence="0.999344" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999960642857143">
In this paper, we explored the identification of trans-
lational links in Wikipedia. By using a set of heuris-
tics that extract and infer links between Wikipedias
in different languages, along with a machine learn-
ing algorithm that builds upon these heuristics to
determine the quality of the interlingual links, we
showed that we can both correct existing transla-
tional links in Wikipedia as well as discover new
interlingual links. Additionally, we have also con-
structed a manually annotated dataset of interlingual
links, covering different types of links in four pairs
of languages, which can serve as a gold standard for
evaluating the quality of predicted links, and as su-
pervision for the machine learning model.
</bodyText>
<page confidence="0.992468">
36
</page>
<bodyText confidence="0.999903888888889">
In future work, we plan to experiment with ad-
ditional features to enhance the performance of the
classifier. In particular, we would like to also include
content-based features, such as content overlap and
interlinking.
The collection of interlingual links for the ten
Wikipedias considered in this work, as well as the
manually annotated dataset are publicly available at
http://lit.csci.unt.edu.
</bodyText>
<sectionHeader confidence="0.998279" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997010428571429">
This material is based in part upon work sup-
ported by the National Science Foundation IIS
awards #1018613 and #1018590 and CAREER
award #0747340. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.990012" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.916166559322034">
G.R. Bharadwaj, N. Tandon, and V. Varma. 2010.
An iterative approach to extract dictionaries from
Wikipedia for under-resourced languages. Kharagpur,
India.
G. de Melo and G. Weikum. 2010a. MENTA: induc-
ing multilingual taxonomies from Wikipedia. In Pro-
ceedings of the 19th ACM international conference on
Information and knowledge management, pages 1099–
1108, New York, NY, USA. ACM.
G. de Melo and G. Weikum. 2010b. Untangling the
cross-lingual link structure of Wikipedia. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 844–853, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from Wikipedia. In Proceedings of the 13th In-
ternational Conference on Database Systems for Ad-
vanced Applications.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2009. Improving the extraction of bilingual termi-
nology from Wikipedia. ACM Transactions on Multi-
media Computing, Communications and Applications,
5(4):31:1–31:17.
S. Hassan and R. Mihalcea. 2009. Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Suntec, Sin-
gapore.
M. Mohammadi and N. QasemAghaee. 2010. Build-
ing bilingual parallel corpora based on Wikipedia. In-
ternational Conference on Computer Engineering and
Applications, 2:264–268.
R. Navigli and S. Ponzetto. 2010. Babelnet: Building a
very large multilingual semantic network. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden.
D. Nguyen, A. Overwijk, C. Hauff, D. Trieschnigg,
D. Hiemstra, and F. De Jong. 2009. WikiTrans-
late: query translation for cross-lingual information re-
trieval using only Wikipedia. In Proceedings of the
9th Cross-language evaluation forum conference on
Evaluating systems for multilingual and multimodal
information access, pages 58–65, Berlin, Heidelberg.
Springer-Verlag.
X. Ni, J. Sun, J. Hu, and Z. Chen. 2011. Cross lingual
text classification by mining multilingual topics from
Wikipedia. In Proceedings of the fourth ACM inter-
national conference on Web search and data mining,
pages 375–384, New York, NY, USA. ACM.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
national conference on Artificial intelligence - Volume
2, pages 1440–1445. AAAI Press.
P. Sorg and P. Cimiano. 2008. Enriching the crosslingual
link structure of Wikipedia - a classification-based ap-
proach. In Proceedings of the AAAI 2008 Workshop
on Wikipedia and Artificial Intelligence.
</reference>
<page confidence="0.99961">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.716079">
<title confidence="0.9974945">Towards Building a Multilingual Semantic Network: Identifying Interlingual Links in Wikipedia</title>
<author confidence="0.996001">Bharath Dandala Rada Mihalcea Razvan Bunescu</author>
<affiliation confidence="0.9999685">Dept. of Computer Science Dept. of Computer Science School of EECS University of North Texas University of North Texas Ohio University</affiliation>
<address confidence="0.843704">Denton, TX Denton, TX Athens, Ohio</address>
<email confidence="0.996645">BharathDandala@my.unt.edurada@cs.unt.edubunescu@ohio.edu</email>
<abstract confidence="0.993561909090909">Wikipedia is a Web based, freely available multilingual encyclopedia, constructed in a collaborative effort by thousands of contributors. Wikipedia articles on the same topic in different languages are connected via interlingual (or translational) links. These links serve as an excellent resource for obtaining lexical translations, or building multilingual dictionaries and semantic networks. As these links are manually built, many links are missing or simply wrong. This paper describes a supervised learning method for generating new links and detecting existing incorrect links. Since there is no dataset available to evaluate the resulting interlingual links, we create our own gold standard by sampling translational links from four language pairs using distance heuristics. We manually annotate the sampled translation links and used them to evaluate the output of our method for automatic link detection and correction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G R Bharadwaj</author>
<author>N Tandon</author>
<author>V Varma</author>
</authors>
<title>An iterative approach to extract dictionaries from Wikipedia for under-resourced languages.</title>
<date>2010</date>
<location>Kharagpur, India.</location>
<contexts>
<context position="22236" citStr="Bharadwaj et al., 2010" startWordPosition="3691" endWordPosition="3694">lar problem in which they combined all the existing multilingual Wikipedias to build a stable, large multilingual taxonomy. The interlingual links have also been used for cross-lingual information retrieval (Nguyen et al., 2009) or to generate bilingual parallel corpora (Mohammadi and QasemAghaee, 2010). (Ni et al., 2011) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Navigli and Ponzetto, 2010) explored the connections that can be drawn between Wikipedia and WordNet. While no attempts were made to complete the existing link structure of Wikipedia, the authors made use of machine translation to enrich the resource. The two previous works most closely related to our</context>
</contexts>
<marker>Bharadwaj, Tandon, Varma, 2010</marker>
<rawString>G.R. Bharadwaj, N. Tandon, and V. Varma. 2010. An iterative approach to extract dictionaries from Wikipedia for under-resourced languages. Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G de Melo</author>
<author>G Weikum</author>
</authors>
<title>MENTA: inducing multilingual taxonomies from Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>1099--1108</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>de Melo, Weikum, 2010</marker>
<rawString>G. de Melo and G. Weikum. 2010a. MENTA: inducing multilingual taxonomies from Wikipedia. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1099– 1108, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G de Melo</author>
<author>G Weikum</author>
</authors>
<title>Untangling the cross-lingual link structure of Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>844--853</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>de Melo, Weikum, 2010</marker>
<rawString>G. de Melo and G. Weikum. 2010b. Untangling the cross-lingual link structure of Wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 844–853, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Erdmann</author>
<author>K Nakayama</author>
<author>T Hara</author>
<author>S Nishio</author>
</authors>
<title>An approach for extracting bilingual terminology from Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th International Conference on Database Systems for Advanced Applications.</booktitle>
<contexts>
<context position="23258" citStr="Erdmann et al., 2008" startWordPosition="3856" endWordPosition="3859">ile no attempts were made to complete the existing link structure of Wikipedia, the authors made use of machine translation to enrich the resource. The two previous works most closely related to ours are the systems introduced in (Sorg and Cimiano, 2008) and (de Melo and Weikum, 2010a; de Melo and Weikum, 2010b). (Sorg and Cimiano, 2008) designed a system that predicts new interlingual links by using a classification based approach. They extract certain types of links from bilingual Wikipedias, which are then used to create a set of features for the machine learning system. In followup work, (Erdmann et al., 2008; Erdmann et al., 2009) used an expanded set of features, which also accounted for direct links, redirects, and links between articles in Wikipedia, to identify entries for a bilingual dictionary. In this line of work, the focus is mainly on article content analysis, as a way to detect new potential translations, rather than link analysis as done in our work. Finally, (de Melo and Weikum, 2010b) designed a system that detects errors in the existing interlingual links in Wikipedia. They show that there are a large number of links that are imprecise or wrong, and propose the use of a weighted gr</context>
</contexts>
<marker>Erdmann, Nakayama, Hara, Nishio, 2008</marker>
<rawString>M. Erdmann, K. Nakayama, T. Hara, and S. Nishio. 2008. An approach for extracting bilingual terminology from Wikipedia. In Proceedings of the 13th International Conference on Database Systems for Advanced Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Erdmann</author>
<author>K Nakayama</author>
<author>T Hara</author>
<author>S Nishio</author>
</authors>
<title>Improving the extraction of bilingual terminology from Wikipedia.</title>
<date>2009</date>
<journal>ACM Transactions on Multimedia Computing, Communications and Applications,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="23281" citStr="Erdmann et al., 2009" startWordPosition="3860" endWordPosition="3863">ade to complete the existing link structure of Wikipedia, the authors made use of machine translation to enrich the resource. The two previous works most closely related to ours are the systems introduced in (Sorg and Cimiano, 2008) and (de Melo and Weikum, 2010a; de Melo and Weikum, 2010b). (Sorg and Cimiano, 2008) designed a system that predicts new interlingual links by using a classification based approach. They extract certain types of links from bilingual Wikipedias, which are then used to create a set of features for the machine learning system. In followup work, (Erdmann et al., 2008; Erdmann et al., 2009) used an expanded set of features, which also accounted for direct links, redirects, and links between articles in Wikipedia, to identify entries for a bilingual dictionary. In this line of work, the focus is mainly on article content analysis, as a way to detect new potential translations, rather than link analysis as done in our work. Finally, (de Melo and Weikum, 2010b) designed a system that detects errors in the existing interlingual links in Wikipedia. They show that there are a large number of links that are imprecise or wrong, and propose the use of a weighted graph to produce a more c</context>
</contexts>
<marker>Erdmann, Nakayama, Hara, Nishio, 2009</marker>
<rawString>M. Erdmann, K. Nakayama, T. Hara, and S. Nishio. 2009. Improving the extraction of bilingual terminology from Wikipedia. ACM Transactions on Multimedia Computing, Communications and Applications, 5(4):31:1–31:17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hassan</author>
<author>R Mihalcea</author>
</authors>
<title>Cross-lingual semantic relatedness using encyclopedic knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Suntec,</booktitle>
<contexts>
<context position="22076" citStr="Hassan and Mihalcea, 2009" startWordPosition="3669" endWordPosition="3672">For instance, (Ponzetto and Strube, 2007) derived a large scale taxonomy from the existing Wikipedia. In related work, (de Melo and Weikum, 2010a) worked on a similar problem in which they combined all the existing multilingual Wikipedias to build a stable, large multilingual taxonomy. The interlingual links have also been used for cross-lingual information retrieval (Nguyen et al., 2009) or to generate bilingual parallel corpora (Mohammadi and QasemAghaee, 2010). (Ni et al., 2011) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Navigli and Ponzetto, 2010) explored the connections that can be drawn between Wikipedia and WordNet. While no attempts were made to complete </context>
</contexts>
<marker>Hassan, Mihalcea, 2009</marker>
<rawString>S. Hassan and R. Mihalcea. 2009. Cross-lingual semantic relatedness using encyclopedic knowledge. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohammadi</author>
<author>N QasemAghaee</author>
</authors>
<title>Building bilingual parallel corpora based on</title>
<date>2010</date>
<booktitle>Wikipedia. International Conference on Computer Engineering and Applications,</booktitle>
<pages>2--264</pages>
<contexts>
<context position="21917" citStr="Mohammadi and QasemAghaee, 2010" startWordPosition="3642" endWordPosition="3646">e processing tasks. A number of projects have used Wikipedia to build a multilingual semantic knowledge base by using the existing multilingual nature of Wikipedia. For instance, (Ponzetto and Strube, 2007) derived a large scale taxonomy from the existing Wikipedia. In related work, (de Melo and Weikum, 2010a) worked on a similar problem in which they combined all the existing multilingual Wikipedias to build a stable, large multilingual taxonomy. The interlingual links have also been used for cross-lingual information retrieval (Nguyen et al., 2009) or to generate bilingual parallel corpora (Mohammadi and QasemAghaee, 2010). (Ni et al., 2011) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a biling</context>
</contexts>
<marker>Mohammadi, QasemAghaee, 2010</marker>
<rawString>M. Mohammadi and N. QasemAghaee. 2010. Building bilingual parallel corpora based on Wikipedia. International Conference on Computer Engineering and Applications, 2:264–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>S Ponzetto</author>
</authors>
<title>Babelnet: Building a very large multilingual semantic network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="22561" citStr="Navigli and Ponzetto, 2010" startWordPosition="3739" endWordPosition="3742">1) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Navigli and Ponzetto, 2010) explored the connections that can be drawn between Wikipedia and WordNet. While no attempts were made to complete the existing link structure of Wikipedia, the authors made use of machine translation to enrich the resource. The two previous works most closely related to ours are the systems introduced in (Sorg and Cimiano, 2008) and (de Melo and Weikum, 2010a; de Melo and Weikum, 2010b). (Sorg and Cimiano, 2008) designed a system that predicts new interlingual links by using a classification based approach. They extract certain types of links from bilingual Wikipedias, which are then used to </context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>R. Navigli and S. Ponzetto. 2010. Babelnet: Building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nguyen</author>
<author>A Overwijk</author>
<author>C Hauff</author>
<author>D Trieschnigg</author>
<author>D Hiemstra</author>
<author>F De Jong</author>
</authors>
<title>WikiTranslate: query translation for cross-lingual information retrieval using only Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th Cross-language</booktitle>
<pages>58--65</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Nguyen, Overwijk, Hauff, Trieschnigg, Hiemstra, De Jong, 2009</marker>
<rawString>D. Nguyen, A. Overwijk, C. Hauff, D. Trieschnigg, D. Hiemstra, and F. De Jong. 2009. WikiTranslate: query translation for cross-lingual information retrieval using only Wikipedia. In Proceedings of the 9th Cross-language evaluation forum conference on Evaluating systems for multilingual and multimodal information access, pages 58–65, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ni</author>
<author>J Sun</author>
<author>J Hu</author>
<author>Z Chen</author>
</authors>
<title>Cross lingual text classification by mining multilingual topics from Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="21936" citStr="Ni et al., 2011" startWordPosition="3647" endWordPosition="3650">jects have used Wikipedia to build a multilingual semantic knowledge base by using the existing multilingual nature of Wikipedia. For instance, (Ponzetto and Strube, 2007) derived a large scale taxonomy from the existing Wikipedia. In related work, (de Melo and Weikum, 2010a) worked on a similar problem in which they combined all the existing multilingual Wikipedias to build a stable, large multilingual taxonomy. The interlingual links have also been used for cross-lingual information retrieval (Nguyen et al., 2009) or to generate bilingual parallel corpora (Mohammadi and QasemAghaee, 2010). (Ni et al., 2011) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Na</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2011</marker>
<rawString>X. Ni, J. Sun, J. Hu, and Z. Chen. 2011. Cross lingual text classification by mining multilingual topics from Wikipedia. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 375–384, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd national conference on Artificial intelligence -</booktitle>
<volume>2</volume>
<pages>1440--1445</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="21491" citStr="Ponzetto and Strube, 2007" startWordPosition="3577" endWordPosition="3580">P3 RP4 Class en=Ball de=Ball en=Ball it=Palla (sport) en=Ball fr=Boule (solide) de=Ball fr=Ballon (sport) 1 1 1 1 0 1 1 0 4 1 1 2 0 0 2 0 0 4 0 0 1 1 1 0 0 0 1 0 0 2 1 0 2 1 0 4 Table 7: Examples of feature vectors generated for four interlingual links, corresponding to the concept pairs listed in Table 4 6 Related Work The multilingual nature of Wikipedia has been already exploited to solve several number of language processing tasks. A number of projects have used Wikipedia to build a multilingual semantic knowledge base by using the existing multilingual nature of Wikipedia. For instance, (Ponzetto and Strube, 2007) derived a large scale taxonomy from the existing Wikipedia. In related work, (de Melo and Weikum, 2010a) worked on a similar problem in which they combined all the existing multilingual Wikipedias to build a stable, large multilingual taxonomy. The interlingual links have also been used for cross-lingual information retrieval (Nguyen et al., 2009) or to generate bilingual parallel corpora (Mohammadi and QasemAghaee, 2010). (Ni et al., 2011) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedia</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In Proceedings of the 22nd national conference on Artificial intelligence - Volume 2, pages 1440–1445. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sorg</author>
<author>P Cimiano</author>
</authors>
<title>Enriching the crosslingual link structure of Wikipedia - a classification-based approach.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence.</booktitle>
<contexts>
<context position="22892" citStr="Sorg and Cimiano, 2008" startWordPosition="3793" endWordPosition="3797">gual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Navigli and Ponzetto, 2010) explored the connections that can be drawn between Wikipedia and WordNet. While no attempts were made to complete the existing link structure of Wikipedia, the authors made use of machine translation to enrich the resource. The two previous works most closely related to ours are the systems introduced in (Sorg and Cimiano, 2008) and (de Melo and Weikum, 2010a; de Melo and Weikum, 2010b). (Sorg and Cimiano, 2008) designed a system that predicts new interlingual links by using a classification based approach. They extract certain types of links from bilingual Wikipedias, which are then used to create a set of features for the machine learning system. In followup work, (Erdmann et al., 2008; Erdmann et al., 2009) used an expanded set of features, which also accounted for direct links, redirects, and links between articles in Wikipedia, to identify entries for a bilingual dictionary. In this line of work, the focus is ma</context>
</contexts>
<marker>Sorg, Cimiano, 2008</marker>
<rawString>P. Sorg and P. Cimiano. 2008. Enriching the crosslingual link structure of Wikipedia - a classification-based approach. In Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>