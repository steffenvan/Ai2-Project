<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.083621">
<title confidence="0.982961">
Cohesion and Collocation:
Using Context Vectors in Text Segmentation
</title>
<author confidence="0.995356">
Stefan Kaufmann
</author>
<affiliation confidence="0.985454">
CSLI, Stanford University
</affiliation>
<address confidence="0.914298">
Linguistics Dept., Bldg. 460
Stanford, CA 94305-2150, U.S.A.
</address>
<email confidence="0.973092">
kaufmannOcsli.stanford.edu
</email>
<sectionHeader confidence="0.992949" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999680909090909">
Collocational word similarity is considered a source
of text cohesion that is hard to measure and quan-
tify. The work presented here explores the use of in-
formation from a training corpus in measuring word
similarity and evaluates the method in the text seg-
mentation task. An implementation, the VecTile
system, produces similarity curves over texts using
pre-compiled vector representations of the contex-
tual behavior of words. The performance of this
system is shown to improve over that of the purely
string-based TextTiling algorithm (Hearst, 1997).
</bodyText>
<sectionHeader confidence="0.99346" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999942344827586">
The notion of text cohesion rests on the intuition
that a text is &amp;quot;held together&amp;quot; by a variety of inter-
nal forces. Much of the relevant linguistic literature
is indebted to Halliday and Hasan (1976), where co-
hesion is defined as a network of relationships be-
tween locations in the text, arising from (i) gram-
matical factors (co-reference, use of pro-forms, ellip-
sis and sentential connectives), and (ii) lexical fac-
tors (reiteration and collocation). Subsequent work
has further developed this taxonomy (Hoey, 1991)
and explored its implications in such areas as para-
graphing (Longacre, 1979; Bond and Hayes, 1984;
Stark, 1988), relevance (Sperber and Wilson, 1995)
and discourse structure (Grosz and Sidner, 1986).
The lexical variety of cohesion is semantically de-
fined, invoking a measure of word similarity. But
this is hard to measure objectively, especially in the
case of collocational relationships, which hold be-
tween words primarily because they &amp;quot;regularly co-
occur.&amp;quot; Halliday and Hasan refrained from a deeper
analysis, but hinted at a notion of &amp;quot;degrees of prox-
imity in the lexical system, a function of the prob-
ability with which one tends to co-occur with an-
other.&amp;quot; (p. 290)
The VecTile system presented here is designed
to utilize precisely this kind of lexical relationship,
relying on observations on a large training corpus
to derive a measure of similarity between words and
text passages.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999809235294118">
Previous approaches to calculating cohesion dif-
fer in the kind of lexical relationship they quan-
tify and in the amount of semantic knowledge they
rely on. Topic parsing (Hahn, 1990) utilizes both
grammatical cues and semantic inference based on
pre-coded domain-specific knowledge. More gen-
eral approaches assess word similarity based on the-
sauri (Morris and Hirst, 1991) or dictionary defini-
tions (Kozima, 1994).
Methods that solely use observations of pat-
terns in vocabulary use include vocabulary manage-
ment (Youmans, 1991) and the blocks algorithm im
plemented in the TextTiling system (Hearst, 1997).
The latter is compared below with the system intro-
duced here.
A good recent overview of previous approaches
can be found in Chapters 4 and 5 of (Reynar, 1998).
</bodyText>
<sectionHeader confidence="0.988586" genericHeader="method">
3 The Method
</sectionHeader>
<subsectionHeader confidence="0.998325">
3.1 Context Vectors
</subsectionHeader>
<bodyText confidence="0.999608347826087">
The VecTile system is based on the WordSpace
model of (Schiitze, 1997; Schiitze, 1998). The idea
is to represent words by encoding the environments
in which they typically occur in texts. Such a rep-
resentation can be obtained automatically and often
provides sufficient information to make deep linguis-
tic analysis unnecessary. This has led to promis-
ing results in information retrieval and related ar-
eas (Flournoy et al., 1998a; Flournoy et al., 1998b).
Given a dictionary W and a relatively small set
C of meaningful &amp;quot;content&amp;quot; words, for each pair in
W x C, the number of times is recorded that the
two co-occur within some measure of distance in a
training corpus. This yields a 1C1-dimensional vector
for each w E W. The direction that the vector has in
the resulting 1C1-dimensional space then represents
the collocational behavior of w in the training cor-
pus. In the present implementation, 1W1 = 20,500
and 1C1= 1000. For computational efficiency and to
avoid the high number of zero values in the resulting
matrix, the matrix is reduced to 100 dimensions us-
ing Singular-Value Decomposition (Golub and van
Loan, 1989).
</bodyText>
<page confidence="0.991548">
591
</page>
<figure confidence="0.998286">
cr.
0.
2 3 4
Section Breaks
</figure>
<figureCaption confidence="0.999992">
Figure 1: Example of a VecTile similarity plot
</figureCaption>
<bodyText confidence="0.99939025">
As a measure of similarity in collocational behav-
ior between two words, the cosine between their vec-
tors is computed: Given two n-dimensional vectors
t7,
</bodyText>
<equation confidence="0.415701333333333">
Vi Wi (1)
cos(fl, 65) =
VE7-1 v? E7-1 w?
</equation>
<subsectionHeader confidence="0.999954">
3.2 Comparing Window Vectors •
</subsectionHeader>
<bodyText confidence="0.999970631578947">
In order to represent pieces of text larger than sin-
gle words, the vectors of the constituent words are
added up. This yields new vectors in the same space,
which can again be compared against each other and
word vectors. If the word vectors in two adjacent
portions of text are added up, then the cosine be-
tween the two resulting vectors is a measure of the
lexical similarity between the two portions of text.
The VecTile system uses word vectors based on
co-occurrence counts on a corpus of New York Times
articles. Two adjacent windows (200 words each in
this experiment) move over the input text, and at
pre-determined intervals (every 10 words), the vec-
tors associated with the words in each window are
added up, and the cosine between the resulting win-
dow vectors is assigned to the gap between the win-
dows in the text. High values indicate lexical close-
ness. Troughs in the resulting similarity curve mark
spots with low cohesion.
</bodyText>
<subsectionHeader confidence="0.999539">
3.3 Text Segmentation
</subsectionHeader>
<bodyText confidence="0.9999932">
To evaluate the performance of the system and facil-
itate comparison with other approaches, it was used
in text segmentation. The motivating assumption
behind this test is that cohesion reinforces the topi-
cal unity of subparts of text and lack of it correlates
with their boundaries, hence if a system correctly
predicts segment boundaries, it is indeed measuring
cohesion. For want of a way of observing cohesion
directly, this indirect relationship is commonly used
for purposes of evaluation.
</bodyText>
<sectionHeader confidence="0.996441" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.96640705882353">
The implementation of the text segmenter resem-
bles that of the TextTiling system (Hearst, 1997),
The words from the input are stemmed and asso-
ciated with their context vectors. The similarity
curve over the text, obtained as described above,
is smoothed out by a simple low-pass filter, and low
points are assigned depth scores according to the dif-
ference between their values and those of the sur-
rounding peaks. The mean and standard deviation
of those depth scores are used to calculate a cutoff
below which a trough is judged to be near a sec-
tion break. The nearest paragraph boundary is then
marked as a section break in the output.
An example of a text similarity curve is given in
Figure 1. Paragraph numbers are inside the plot at
the bottom. Speaker judgments by five subjects are
inserted in five rows in the upper half.
</bodyText>
<page confidence="0.996409">
592
</page>
<tableCaption confidence="0.999752">
Table 1: Precision and recall on the text segmentation task
</tableCaption>
<table confidence="0.998466125">
TextTiling VecTile Subjects
Text # Prec Rec Prec Rec Prec Rec
1 60 50 60 50 75 77
2 14 20 100 80 76 76
3 50 50 50 50 72 73
4 25 50 10 25 70 75
5 10 25 40 50 70 74
avg 32 40 52 51 73 75
</table>
<bodyText confidence="0.99644705882353">
The crucial difference between this and the
TextTiling system is that the latter builds win-
dow vectors solely by counting the occurrences of
strings in the windows. Repetition is rewarded by
the present approach, too, as identical . words con-
tribute most to the similarity between the block vec-
tors. However, similarity scores can be high even
in the absence of pure string repetition, as long as
the adjacent windows contain words that co-occur
frequently in the training corpus. Thus what a di-
rect comparison between the systems will show is
whether the addition of collocational information
gleaned from the training corpus sharpens or blunts
the judgment.
For comparison, the TextTiling algorithm was
implemented and run with the same window size
(200) and gap interval (10).
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.983931">
5.1 The Task
</subsectionHeader>
<bodyText confidence="0.985320260869565">
In a pilot study, five subjects were presented with
five texts from a popular-science magazine, all be-
tween 2,000 and 3,400 words, or between 20 and 35
paragraphs, in length. Section headings and any
other clues were removed from the layout. Para-
graph breaks were left in place. Thus the task was
not to find paragraph breaks, but breaks between
multi-paragraph passages that according to the the
subject&apos;s judgment marked topic shifts. All subjects
were native speakers of English.1
1 The instructions read:
&amp;quot;You will be given five magazine articles of roughly equal
length with section breaks removed. Please mark the places
where the topic seems to change (draw a line between para-
graphs). Read at normal speed, do not take much longer than
you normally would. But do feel free to go back and recon-
sider your decisions (even change your markings) as you go
along.
Also, for each section, suggest a headline of a few words that
captures its main content.
If you find it hard to decide between two places, mark both,
giving preference to one and indicating that the other was a
close rival.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.884317">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9999439375">
To obtain an &amp;quot;expert opinion&amp;quot; against which to
compare the algorithms, those paragraph bound-
aries were marked as &amp;quot;correct&amp;quot; section breaks which
at least three out of the five subjects had marked.
(Three out of seven (Litman and Passonneau, 1995;
Hearst, 1997) or 30% (Kozima, 1994) are also some-
times deemed sufficient.) For the two systems as well
as the subjects, precision and recall with respect to
the set of &amp;quot;correct&amp;quot; section breaks were calculated.
The results are listed in Table 1.
The context vectors clearly led to an improved
performance over the counting of pure string repeti-
tions.
The simple assignment of section breaks to the
nearest paragraph boundary may have led to noise
in some cases; moreover, it is not really part of
the task of measuring cohesion. Therefore the texts
were processed again, this time moving the windows
over whole paragraphs at a time, calculating gap-
values at the paragraph gaps. For each paragraph
break, the number of subjects who had marked it
as a section break was taken as an indicator of the
&amp;quot;strength&amp;quot; of the boundary. There was a significant
negative correlation between the values calculated
by both systems and that measure of strength, with
r = —.338(p = .0002) for the VecTile system and
r = —.220(p = .0172) for TextTiling. In othei
words, deep gaps in the similarity measure are asso-
ciated with strong agreement between subjects that
the spot marks a section boundary. Although r2
is low both cases, the VecTile system yields more
significant results.
</bodyText>
<subsectionHeader confidence="0.990235">
5.3 Discussion and Further Work
</subsectionHeader>
<bodyText confidence="0.999721444444444">
The results discussed above need further support
with a larger subject pool, as the level of agree-:
ment among the judges was at the low end of what
can be considered significant. This is shown by
the Kappa coefficients, measured against the expert
opinion and listed in Table 2. The overall average
was .594.
Despite this caveat, the results clearly show that
adding collocational information from the training
</bodyText>
<page confidence="0.999463">
593
</page>
<tableCaption confidence="0.987317">
Table 2: Kappa coefficients
</tableCaption>
<table confidence="0.99959">
Text# 1 2 3 Subject# 5 E
4
1 .775 .629 .596 .444 .642 .617
2 .723 .649 .491 .753 .557 .635
3 .859 .121 .173 .538 .738 _ .486
4 .870 .532 .635 .299 .870 .641
5 .833 .500 .625 .423 .500 .576
All texts .814 .491 .508 481 .675 .594
</table>
<bodyText confidence="0.999822860465116">
corpus improves the prediction of section breaks,
hence, under common assumptions, the measure-
ment of lexical cohesion. It is likely that these en-
couraging results can be further improved. Follow-
ing are a few suggestions of ways to do so.
Some factors work against the context vector
method. For instance, the system currently has no
mechanism to handle words that it has no context
vectors for. Often it is precisely the co-occurrence
of uncommon words not in the training corpus (per-
sonal names, rare terminology etc.) that ties text
together. Such cases pose no challenge to the string-
based system, but the VecTile system cannot utilize
them. The best solution might be a hybrid system
with a backup procedure for unknown words.
Another point to note is how well the much sim-
pler TextTile system compares. Indeed, a close look
at the figures in Table 1 reveals that the better re-
sults of the VecTile system are due in large part to
one of the texts, viz. #2. Considering the additional
effort and resources involved in using context vec-
tors, the modest boost in performance might often
not be worth the effort in practice. This suggests
that pure string repetition is a particularly strong
indicator of similarity, and the vector-based system
might benefit from a mechanism to give those vec-
tors a higher weight than co-occurrences of merely
similar words.
Another potentially important parameter is the
nature of the training corpus. In this case, it con-
sisted mainly of news texts, while the texts in the
experiment were scientific expository texts. A more
homogeneous setting might have further improved
the results.
Finally, the evaluation of results in this task is
complicated by the fact that &amp;quot;near-hits&amp;quot; (cases in
which a section break is off by one paragraph) do
not have any positive effect on the score: This prob-
lem has been dealt with in the Topic Detection and
Tracking (TDT) project by a more flexible score that
becomes gradually worse as the distance between hy-
pothesized and &amp;quot;real&amp;quot; boundaries increases (TDT,
1997a; TDT, 1997b).
</bodyText>
<sectionHeader confidence="0.990098" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997521666666667">
Thanks to Stanley Peters, Yasuhiro Takayama, Hin-
rich Schiitze, David Beaver, Edward Flemming and
three anonymous reviewers for helpful discussion
and comments, to Stanley Peters for office space
and computational infrastructure, and to Raymond
Flournoy for assistance with the vector space.
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999127434782609">
S.J. Bond and J.R. Hayes. 1984. Cues people use
to paragraph text. Research in the Teaching of
English, 18:147-167.
Raymond Flournoy, Ryan Ginstrom, Kenichi Imai,
Stefan Kaufmann, Genichiro Kikui, Stanley Pe-
ters, Hinrich Schiitze, and Yasuhiro Takayama.
1998a. Personalization and users&apos; semantic expec-
tations. ACM SIGIR&apos;98 Workshop on Query In-
put and User Expectations, Melbourne, Australia.
Raymond Flournoy, Hiroshi Masuichi, and Stan-
ley Peters. 1998b. Cross-language information re-
trieval: Some methods and tools. In D. Hiemstra,
F. de Jong, and K. Netter, editors, TWLT 14 Lan-
guage Technology in Multimedia Information Re-
trieval, pages 79-83.
Talmy Givim, editor. 1979. Discourse and Syntax.
Academic Press.
G. H. Golub and C. F. van Loan. 1989. Matrix Com-
putations. Johns Hopkins University Press.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175-204.
Udo Hahn. 1990. Topic parsing: Accounting for text
macro structures in full-text analysis. Information
Processing and Management, 26:135-170.
Michael A.K. Halliday and Rugaiya Hasan. 1976.
Cohesion in English. Longman.
Marti Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Compu-
tational Linguistics, 23(1):33-64.
Michael Hoey. 1991. Patterns of Lexis in Text. Ox-
ford University Press.
Hideki Kozima. 1994. Computing Lexical Cohesion
as a Tool for Text Analysis. Ph.D. thesis, Univer-
sity of Electro-Communications.
Chin-Yew Lin. 1997. Robust Automatic
Topic Identification. Ph.D. thesis, Uni-
versity of Southern California. [Online]
http://www.isi.edurcyl/thesis/thesis.html
[1999, April 24].
Diane J. Litman and Rebecca J. Passonneau. 1995.
Combining multiple knowledge sources for dis-
course segmentation. In Proceedings of the 33rd
ACL, pages 108-115.
L.E. Longacre. 1979. The paragraph as a grammat-
ical unit. In Givon (Givon, 1979), pages 115-134:
</reference>
<page confidence="0.993391">
594
</page>
<bodyText confidence="0.57727825">
Jane Morris and Graeme Hirst. 1991. Lexical co-
hesion computed by thesaural relations as an in-
dication of the structure of text. Computational
Linguistics, 17(1):21-48.
</bodyText>
<reference confidence="0.997127666666667">
Jeffrey C. Reynar. 1998. Topic • Segmenta-
tion: Algorithms and Applications. Ph.D.
thesis, University of Pennsylvania. [Online]
http://www.cis.edurjcreynar/research.html
[1999, April 24].
K. Richmond, A. Smith, and E. Amitay. 1997.
Detecting subject boundaries within text: A
language independent statistical approach. In
Proceedings of The Second Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2).
Hinrich Schiitze. 1997. Ambiguity Resolution in
Language Learning. CSLI.
Hinrich Schiitze. 1998. Automatic word sense
discrimination. Computational Linguistics,
24(1):97-123.
Dan Sperber and Deidre Wilson. 1995. Relevance:
Communication and Cognition. Harvard Univer-
sity Press, 2nd edition.
Heather Stark. 1988. What do paragraph markings
do? Discourse Processes, 11(3):275-304.
1997a. The TDT Pilot Study Corpus Documenta-
tion version 1.3, 10. Distributed by the Linguistic
Data Consortium.
1997b. The Topic Detection and Tracking (TDT) Pi-
lot Study Evaluation Plan, 10. Distributed by the
Linguistic Data Consortium.
Gilbert Youmans. 1991. A new tool for discourse
analysis: The vocabulary-management profile.
Language, 47(4):763-789.
</reference>
<page confidence="0.99865">
595
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9977195">Cohesion and Collocation: Using Context Vectors in Text Segmentation</title>
<author confidence="0.999945">Stefan Kaufmann</author>
<affiliation confidence="0.999598">CSLI, Stanford University</affiliation>
<address confidence="0.9890945">Linguistics Dept., Bldg. 460 Stanford, CA 94305-2150, U.S.A.</address>
<email confidence="0.999783">kaufmannOcsli.stanford.edu</email>
<abstract confidence="0.98530194520548">Collocational word similarity is considered a source of text cohesion that is hard to measure and quantify. The work presented here explores the use of information from a training corpus in measuring word similarity and evaluates the method in the text segtask. An implementation, the system, produces similarity curves over texts using pre-compiled vector representations of the contextual behavior of words. The performance of this system is shown to improve over that of the purely (Hearst, 1997). 1 Background The notion of text cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) presented here is designed to utilize precisely this kind of lexical relationship, relying on observations on a large training corpus to derive a measure of similarity between words and text passages. 2 Related Work Previous approaches to calculating cohesion differ in the kind of lexical relationship they quantify and in the amount of semantic knowledge they on. parsing 1990) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patin vocabulary use include manage- 1991) and the in the (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors is based on the model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas (Flournoy et al., 1998a; Flournoy et al., 1998b). Given a dictionary W and a relatively small set meaningful &amp;quot;content&amp;quot; words, for each pair in x number of times is recorded that the two co-occur within some measure of distance in a training corpus. This yields a 1C1-dimensional vector for each w E W. The direction that the vector has in the resulting 1C1-dimensional space then represents the collocational behavior of w in the training corpus. In the present implementation, 1W1 = 20,500 and 1C1= 1000. For computational efficiency and to avoid the high number of zero values in the resulting matrix, the matrix is reduced to 100 dimensions using Singular-Value Decomposition (Golub and van Loan, 1989). 591 cr. 0. 3 Section Breaks 1: Example of a plot As a measure of similarity in collocational behavior between two words, the cosine between their vectors is computed: Given two n-dimensional vectors t7, 3.2 Comparing Window Vectors • In order to represent pieces of text larger than single words, the vectors of the constituent words are added up. This yields new vectors in the same space, which can again be compared against each other and word vectors. If the word vectors in two adjacent portions of text are added up, then the cosine between the two resulting vectors is a measure of the lexical similarity between the two portions of text. uses word vectors based on co-occurrence counts on a corpus of New York Times articles. Two adjacent windows (200 words each in this experiment) move over the input text, and at pre-determined intervals (every 10 words), the vectors associated with the words in each window are added up, and the cosine between the resulting window vectors is assigned to the gap between the windows in the text. High values indicate lexical closeness. Troughs in the resulting similarity curve mark spots with low cohesion. 3.3 Text Segmentation To evaluate the performance of the system and facilitate comparison with other approaches, it was used in text segmentation. The motivating assumption behind this test is that cohesion reinforces the topical unity of subparts of text and lack of it correlates with their boundaries, hence if a system correctly predicts segment boundaries, it is indeed measuring cohesion. For want of a way of observing cohesion directly, this indirect relationship is commonly used for purposes of evaluation. 4 Implementation The implementation of the text segmenter resemthat of the (Hearst, 1997), The words from the input are stemmed and associated with their context vectors. The similarity curve over the text, obtained as described above, is smoothed out by a simple low-pass filter, and low are assigned scores to the difference between their values and those of the surrounding peaks. The mean and standard deviation of those depth scores are used to calculate a cutoff below which a trough is judged to be near a section break. The nearest paragraph boundary is then marked as a section break in the output. An example of a text similarity curve is given in Figure 1. Paragraph numbers are inside the plot at the bottom. Speaker judgments by five subjects are inserted in five rows in the upper half. 592 Table 1: Precision and recall on the text segmentation task</abstract>
<title confidence="0.940944">TextTiling VecTile Subjects</title>
<author confidence="0.817021">Text Prec Rec Prec Rec Prec Rec</author>
<phone confidence="0.858288">1 60 50 60 50 75 77 2 14 20 100 80 76 76 3 50 50 50 50 72 73 4 25 50 10 25 70 75 5 10 25 40 50 70 74</phone>
<abstract confidence="0.930623073825503">avg 32 40 52 51 73 75 The crucial difference between this and the is that the latter builds window vectors solely by counting the occurrences of strings in the windows. Repetition is rewarded by the present approach, too, as identical . words contribute most to the similarity between the block vectors. However, similarity scores can be high even in the absence of pure string repetition, as long as the adjacent windows contain words that co-occur frequently in the training corpus. Thus what a direct comparison between the systems will show is whether the addition of collocational information gleaned from the training corpus sharpens or blunts the judgment. comparison, the was implemented and run with the same window size (200) and gap interval (10). 5 Evaluation 5.1 The Task pilot study, five subjects were presented with five texts from a popular-science magazine, all between 2,000 and 3,400 words, or between 20 and 35 paragraphs, in length. Section headings and any other clues were removed from the layout. Paragraph breaks were left in place. Thus the task was not to find paragraph breaks, but breaks between multi-paragraph passages that according to the the subject&apos;s judgment marked topic shifts. All subjects native speakers of 1The instructions read: &amp;quot;You will be given five magazine articles of roughly equal length with section breaks removed. Please mark the places where the topic seems to change (draw a line between paragraphs). Read at normal speed, do not take much longer than you normally would. But do feel free to go back and reconsider your decisions (even change your markings) as you go along. Also, for each section, suggest a headline of a few words that captures its main content. If you find it hard to decide between two places, mark both, preference to one and indicating that the other close rival.&amp;quot; 5.2 Results To obtain an &amp;quot;expert opinion&amp;quot; against which to compare the algorithms, those paragraph boundaries were marked as &amp;quot;correct&amp;quot; section breaks which at least three out of the five subjects had marked. (Three out of seven (Litman and Passonneau, 1995; Hearst, 1997) or 30% (Kozima, 1994) are also sometimes deemed sufficient.) For the two systems as well as the subjects, precision and recall with respect to the set of &amp;quot;correct&amp;quot; section breaks were calculated. The results are listed in Table 1. The context vectors clearly led to an improved performance over the counting of pure string repetitions. The simple assignment of section breaks to the nearest paragraph boundary may have led to noise in some cases; moreover, it is not really part of the task of measuring cohesion. Therefore the texts were processed again, this time moving the windows over whole paragraphs at a time, calculating gapvalues at the paragraph gaps. For each paragraph break, the number of subjects who had marked it as a section break was taken as an indicator of the &amp;quot;strength&amp;quot; of the boundary. There was a significant negative correlation between the values calculated by both systems and that measure of strength, with = —.338(p = .0002) for the and = —.220(p = .0172) for othei words, deep gaps in the similarity measure are associated with strong agreement between subjects that spot marks a section boundary. Although low both cases, the yields more significant results. 5.3 Discussion and Further Work The results discussed above need further support with a larger subject pool, as the level of agree-: ment among the judges was at the low end of what can be considered significant. This is shown by measured against the expert opinion and listed in Table 2. The overall average was .594. Despite this caveat, the results clearly show that adding collocational information from the training 593 2: Text# 1 2 3 Subject# 5 E 4 1 .775 .629 .596 .444 .642 .617 2 .723 .649 .491 .753 .557 .635 3 .859 .121 .173 .538 .738 _ .486 4 .870 .532 .635 .299 .870 .641 5 .833 .500 .625 .423 .500 .576 All texts .814 .491 .508 481 .675 .594 the prediction of section breaks, hence, under common assumptions, the measureof lexical cohesion. It is likely encouraging results can be further improved. Following are a few suggestions of ways to do so. Some factors work against the context vector method. For instance, the system currently has no mechanism to handle words that it has no context vectors for. Often it is precisely the co-occurrence of uncommon words not in the training corpus (personal names, rare terminology etc.) that ties text together. Such cases pose no challenge to the stringsystem, but the cannot utilize them. The best solution might be a hybrid system with a backup procedure for unknown words. Another point to note is how well the much simcompares. Indeed, a close look at the figures in Table 1 reveals that the better reof the are due in large part to one of the texts, viz. #2. Considering the additional effort and resources involved in using context vectors, the modest boost in performance might often not be worth the effort in practice. This suggests that pure string repetition is a particularly strong indicator of similarity, and the vector-based system might benefit from a mechanism to give those vectors a higher weight than co-occurrences of merely similar words. Another potentially important parameter is the nature of the training corpus. In this case, it consisted mainly of news texts, while the texts in the experiment were scientific expository texts. A more homogeneous setting might have further improved the results. Finally, the evaluation of results in this task is complicated by the fact that &amp;quot;near-hits&amp;quot; (cases in which a section break is off by one paragraph) do not have any positive effect on the score: This problem has been dealt with in the Topic Detection and Tracking (TDT) project by a more flexible score that becomes gradually worse as the distance between hypothesized and &amp;quot;real&amp;quot; boundaries increases (TDT, 1997a; TDT, 1997b). Acknowledgements Thanks to Stanley Peters, Yasuhiro Takayama, Hinrich Schiitze, David Beaver, Edward Flemming and three anonymous reviewers for helpful discussion and comments, to Stanley Peters for office space and computational infrastructure, and to Raymond Flournoy for assistance with the vector space. References S.J. Bond and J.R. Hayes. 1984. Cues people use paragraph text. in the Teaching of</abstract>
<author confidence="0.729064">Raymond Flournoy</author>
<author confidence="0.729064">Ryan Ginstrom</author>
<author confidence="0.729064">Kenichi Imai</author>
<author confidence="0.729064">Stefan Kaufmann</author>
<author confidence="0.729064">Genichiro Kikui</author>
<author confidence="0.729064">Stanley Pe-</author>
<affiliation confidence="0.468027">ters, Hinrich Schiitze, and Yasuhiro Takayama.</affiliation>
<address confidence="0.30851">1998a. Personalization and users&apos; semantic expec-</address>
<note confidence="0.856023363636364">tations. ACM SIGIR&apos;98 Workshop on Query Input and User Expectations, Melbourne, Australia. Raymond Flournoy, Hiroshi Masuichi, and Stanley Peters. 1998b. Cross-language information re- Some methods and tools. In de Jong, and K. Netter, editors, 14 Language Technology in Multimedia Information Re- 79-83. Givim, editor. 1979. Academic Press. H. Golub and C. F. van Loan. 1989. Com-</note>
<affiliation confidence="0.757542">Hopkins University Press.</affiliation>
<address confidence="0.604496">Barbara J. Grosz and Candace L. Sidner. 1986. At-</address>
<abstract confidence="0.665822666666667">tention, intentions, and the structure of discourse. Linguistics, Udo Hahn. 1990. Topic parsing: Accounting for text structures in full-text analysis. and Management, Michael A.K. Halliday and Rugaiya Hasan. 1976. in English. Marti Hearst. 1997. TextTiling: Segmenting text multi-paragraph subtopic passages. Compu-</abstract>
<affiliation confidence="0.4552">Linguistics,</affiliation>
<address confidence="0.519323">Hoey. 1991. of Lexis in Text. Ox-</address>
<affiliation confidence="0.876003">ford University Press.</affiliation>
<note confidence="0.702884833333333">Kozima. 1994. Lexical Cohesion a Tool for Text Analysis. University of Electro-Communications. Lin. 1997. Automatic Identification. thesis, University of Southern California. [Online]</note>
<web confidence="0.977918">http://www.isi.edurcyl/thesis/thesis.html</web>
<note confidence="0.872882111111111">[1999, April 24]. Diane J. Litman and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for dissegmentation. In of the 33rd 108-115. L.E. Longacre. 1979. The paragraph as a grammatical unit. In Givon (Givon, 1979), pages 115-134: 594 Jane Morris and Graeme Hirst. 1991. Lexical co-</note>
<abstract confidence="0.793527">hesion computed by thesaural relations as an inof the structure of text. C. Reynar. 1998. • Algorithms and Applications.</abstract>
<affiliation confidence="0.543553">thesis, University of Pennsylvania. [Online]</affiliation>
<web confidence="0.986101">http://www.cis.edurjcreynar/research.html</web>
<note confidence="0.848030653846154">[1999, April 24]. K. Richmond, A. Smith, and E. Amitay. 1997. Detecting subject boundaries within text: A language independent statistical approach. In Proceedings of The Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2). Schiitze. 1997. Resolution Learning. Hinrich Schiitze. 1998. Automatic word sense Linguistics, 24(1):97-123. Sperber and Deidre Wilson. 1995. University Press, 2nd edition. Heather Stark. 1988. What do paragraph markings Processes, Study Corpus Documentaversion 1.3, Distributed by the Linguistic Data Consortium. Topic Detection (TDT) Pi- Study Evaluation Plan, Distributed by the Linguistic Data Consortium. Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. 595</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S J Bond</author>
<author>J R Hayes</author>
</authors>
<title>Cues people use to paragraph text.</title>
<date>1984</date>
<booktitle>Research in the Teaching of English,</booktitle>
<pages>18--147</pages>
<contexts>
<context position="1387" citStr="Bond and Hayes, 1984" startWordPosition="207" endWordPosition="210">ground The notion of text cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) The VecTile system presen</context>
</contexts>
<marker>Bond, Hayes, 1984</marker>
<rawString>S.J. Bond and J.R. Hayes. 1984. Cues people use to paragraph text. Research in the Teaching of English, 18:147-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Flournoy</author>
<author>Ryan Ginstrom</author>
<author>Kenichi Imai</author>
<author>Stefan Kaufmann</author>
</authors>
<title>Genichiro Kikui, Stanley Peters, Hinrich Schiitze, and Yasuhiro Takayama.</title>
<date>1998</date>
<booktitle>ACM SIGIR&apos;98 Workshop on Query Input and User Expectations,</booktitle>
<location>Melbourne, Australia.</location>
<contexts>
<context position="3423" citStr="Flournoy et al., 1998" startWordPosition="533" endWordPosition="536"> The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas (Flournoy et al., 1998a; Flournoy et al., 1998b). Given a dictionary W and a relatively small set C of meaningful &amp;quot;content&amp;quot; words, for each pair in W x C, the number of times is recorded that the two co-occur within some measure of distance in a training corpus. This yields a 1C1-dimensional vector for each w E W. The direction that the vector has in the resulting 1C1-dimensional space then represents the collocational behavior of w in the training corpus. In the present implementation, 1W1 = 20,500 and 1C1= 1000. For computational efficiency and to avoid the high number of zero values in the resulting matrix, the </context>
</contexts>
<marker>Flournoy, Ginstrom, Imai, Kaufmann, 1998</marker>
<rawString>Raymond Flournoy, Ryan Ginstrom, Kenichi Imai, Stefan Kaufmann, Genichiro Kikui, Stanley Peters, Hinrich Schiitze, and Yasuhiro Takayama. 1998a. Personalization and users&apos; semantic expectations. ACM SIGIR&apos;98 Workshop on Query Input and User Expectations, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Flournoy</author>
<author>Hiroshi Masuichi</author>
<author>Stanley Peters</author>
</authors>
<title>Cross-language information retrieval: Some methods and tools. In</title>
<date>1998</date>
<booktitle>TWLT 14 Language Technology in Multimedia Information Retrieval,</booktitle>
<pages>79--83</pages>
<editor>D. Hiemstra, F. de Jong, and K. Netter, editors,</editor>
<contexts>
<context position="3423" citStr="Flournoy et al., 1998" startWordPosition="533" endWordPosition="536"> The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas (Flournoy et al., 1998a; Flournoy et al., 1998b). Given a dictionary W and a relatively small set C of meaningful &amp;quot;content&amp;quot; words, for each pair in W x C, the number of times is recorded that the two co-occur within some measure of distance in a training corpus. This yields a 1C1-dimensional vector for each w E W. The direction that the vector has in the resulting 1C1-dimensional space then represents the collocational behavior of w in the training corpus. In the present implementation, 1W1 = 20,500 and 1C1= 1000. For computational efficiency and to avoid the high number of zero values in the resulting matrix, the </context>
</contexts>
<marker>Flournoy, Masuichi, Peters, 1998</marker>
<rawString>Raymond Flournoy, Hiroshi Masuichi, and Stanley Peters. 1998b. Cross-language information retrieval: Some methods and tools. In D. Hiemstra, F. de Jong, and K. Netter, editors, TWLT 14 Language Technology in Multimedia Information Retrieval, pages 79-83.</rawString>
</citation>
<citation valid="true">
<date>1979</date>
<booktitle>Discourse and Syntax.</booktitle>
<editor>Talmy Givim, editor.</editor>
<publisher>Academic Press.</publisher>
<marker>1979</marker>
<rawString>Talmy Givim, editor. 1979. Discourse and Syntax. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Golub</author>
<author>C F van Loan</author>
</authors>
<title>Matrix Computations. Johns Hopkins</title>
<date>1989</date>
<publisher>University Press.</publisher>
<marker>Golub, van Loan, 1989</marker>
<rawString>G. H. Golub and C. F. van Loan. 1989. Matrix Computations. Johns Hopkins University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="1488" citStr="Grosz and Sidner, 1986" startWordPosition="221" endWordPosition="224">ety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) The VecTile system presented here is designed to utilize precisely this kind of lexical relationship, relying on observations </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
</authors>
<title>Topic parsing: Accounting for text macro structures in full-text analysis.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>26--135</pages>
<contexts>
<context position="2378" citStr="Hahn, 1990" startWordPosition="369" endWordPosition="370">an refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) The VecTile system presented here is designed to utilize precisely this kind of lexical relationship, relying on observations on a large training corpus to derive a measure of similarity between words and text passages. 2 Related Work Previous approaches to calculating cohesion differ in the kind of lexical relationship they quantify and in the amount of semantic knowledge they rely on. Topic parsing (Hahn, 1990) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.</context>
</contexts>
<marker>Hahn, 1990</marker>
<rawString>Udo Hahn. 1990. Topic parsing: Accounting for text macro structures in full-text analysis. Information Processing and Management, 26:135-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Rugaiya Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="972" citStr="Halliday and Hasan (1976)" startWordPosition="143" endWordPosition="146">s the use of information from a training corpus in measuring word similarity and evaluates the method in the text segmentation task. An implementation, the VecTile system, produces similarity curves over texts using pre-compiled vector representations of the contextual behavior of words. The performance of this system is shown to improve over that of the purely string-based TextTiling algorithm (Hearst, 1997). 1 Background The notion of text cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of wor</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A.K. Halliday and Rugaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="759" citStr="Hearst, 1997" startWordPosition="108" endWordPosition="109">rd, CA 94305-2150, U.S.A. kaufmannOcsli.stanford.edu Abstract Collocational word similarity is considered a source of text cohesion that is hard to measure and quantify. The work presented here explores the use of information from a training corpus in measuring word similarity and evaluates the method in the text segmentation task. An implementation, the VecTile system, produces similarity curves over texts using pre-compiled vector representations of the contextual behavior of words. The performance of this system is shown to improve over that of the purely string-based TextTiling algorithm (Hearst, 1997). 1 Background The notion of text cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre</context>
<context position="2801" citStr="Hearst, 1997" startWordPosition="431" endWordPosition="432">ted Work Previous approaches to calculating cohesion differ in the kind of lexical relationship they quantify and in the amount of semantic knowledge they rely on. Topic parsing (Hahn, 1990) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas </context>
<context position="5995" citStr="Hearst, 1997" startWordPosition="973" endWordPosition="974">ation To evaluate the performance of the system and facilitate comparison with other approaches, it was used in text segmentation. The motivating assumption behind this test is that cohesion reinforces the topical unity of subparts of text and lack of it correlates with their boundaries, hence if a system correctly predicts segment boundaries, it is indeed measuring cohesion. For want of a way of observing cohesion directly, this indirect relationship is commonly used for purposes of evaluation. 4 Implementation The implementation of the text segmenter resembles that of the TextTiling system (Hearst, 1997), The words from the input are stemmed and associated with their context vectors. The similarity curve over the text, obtained as described above, is smoothed out by a simple low-pass filter, and low points are assigned depth scores according to the difference between their values and those of the surrounding peaks. The mean and standard deviation of those depth scores are used to calculate a cutoff below which a trough is judged to be near a section break. The nearest paragraph boundary is then marked as a section break in the output. An example of a text similarity curve is given in Figure 1</context>
<context position="9149" citStr="Hearst, 1997" startWordPosition="1531" endWordPosition="1532">mally would. But do feel free to go back and reconsider your decisions (even change your markings) as you go along. Also, for each section, suggest a headline of a few words that captures its main content. If you find it hard to decide between two places, mark both, giving preference to one and indicating that the other was a close rival.&amp;quot; 5.2 Results To obtain an &amp;quot;expert opinion&amp;quot; against which to compare the algorithms, those paragraph boundaries were marked as &amp;quot;correct&amp;quot; section breaks which at least three out of the five subjects had marked. (Three out of seven (Litman and Passonneau, 1995; Hearst, 1997) or 30% (Kozima, 1994) are also sometimes deemed sufficient.) For the two systems as well as the subjects, precision and recall with respect to the set of &amp;quot;correct&amp;quot; section breaks were calculated. The results are listed in Table 1. The context vectors clearly led to an improved performance over the counting of pure string repetitions. The simple assignment of section breaks to the nearest paragraph boundary may have led to noise in some cases; moreover, it is not really part of the task of measuring cohesion. Therefore the texts were processed again, this time moving the windows over whole par</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hoey</author>
</authors>
<title>Patterns of Lexis in Text.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1289" citStr="Hoey, 1991" startWordPosition="193" endWordPosition="194"> improve over that of the purely string-based TextTiling algorithm (Hearst, 1997). 1 Background The notion of text cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of </context>
</contexts>
<marker>Hoey, 1991</marker>
<rawString>Michael Hoey. 1991. Patterns of Lexis in Text. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Computing Lexical Cohesion as a Tool for Text Analysis.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Electro-Communications.</institution>
<contexts>
<context position="2609" citStr="Kozima, 1994" startWordPosition="402" endWordPosition="403">is designed to utilize precisely this kind of lexical relationship, relying on observations on a large training corpus to derive a measure of similarity between words and text passages. 2 Related Work Previous approaches to calculating cohesion differ in the kind of lexical relationship they quantify and in the amount of semantic knowledge they rely on. Topic parsing (Hahn, 1990) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can</context>
<context position="9171" citStr="Kozima, 1994" startWordPosition="1535" endWordPosition="1536">el free to go back and reconsider your decisions (even change your markings) as you go along. Also, for each section, suggest a headline of a few words that captures its main content. If you find it hard to decide between two places, mark both, giving preference to one and indicating that the other was a close rival.&amp;quot; 5.2 Results To obtain an &amp;quot;expert opinion&amp;quot; against which to compare the algorithms, those paragraph boundaries were marked as &amp;quot;correct&amp;quot; section breaks which at least three out of the five subjects had marked. (Three out of seven (Litman and Passonneau, 1995; Hearst, 1997) or 30% (Kozima, 1994) are also sometimes deemed sufficient.) For the two systems as well as the subjects, precision and recall with respect to the set of &amp;quot;correct&amp;quot; section breaks were calculated. The results are listed in Table 1. The context vectors clearly led to an improved performance over the counting of pure string repetitions. The simple assignment of section breaks to the nearest paragraph boundary may have led to noise in some cases; moreover, it is not really part of the task of measuring cohesion. Therefore the texts were processed again, this time moving the windows over whole paragraphs at a time, cal</context>
</contexts>
<marker>Kozima, 1994</marker>
<rawString>Hideki Kozima. 1994. Computing Lexical Cohesion as a Tool for Text Analysis. Ph.D. thesis, University of Electro-Communications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Robust Automatic Topic Identification.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<note>[Online] http://www.isi.edurcyl/thesis/thesis.html</note>
<marker>Lin, 1997</marker>
<rawString>Chin-Yew Lin. 1997. Robust Automatic Topic Identification. Ph.D. thesis, University of Southern California. [Online] http://www.isi.edurcyl/thesis/thesis.html [1999, April 24].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd ACL,</booktitle>
<pages>108--115</pages>
<contexts>
<context position="9134" citStr="Litman and Passonneau, 1995" startWordPosition="1527" endWordPosition="1530">take much longer than you normally would. But do feel free to go back and reconsider your decisions (even change your markings) as you go along. Also, for each section, suggest a headline of a few words that captures its main content. If you find it hard to decide between two places, mark both, giving preference to one and indicating that the other was a close rival.&amp;quot; 5.2 Results To obtain an &amp;quot;expert opinion&amp;quot; against which to compare the algorithms, those paragraph boundaries were marked as &amp;quot;correct&amp;quot; section breaks which at least three out of the five subjects had marked. (Three out of seven (Litman and Passonneau, 1995; Hearst, 1997) or 30% (Kozima, 1994) are also sometimes deemed sufficient.) For the two systems as well as the subjects, precision and recall with respect to the set of &amp;quot;correct&amp;quot; section breaks were calculated. The results are listed in Table 1. The context vectors clearly led to an improved performance over the counting of pure string repetitions. The simple assignment of section breaks to the nearest paragraph boundary may have led to noise in some cases; moreover, it is not really part of the task of measuring cohesion. Therefore the texts were processed again, this time moving the windows</context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>Diane J. Litman and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for discourse segmentation. In Proceedings of the 33rd ACL, pages 108-115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Longacre</author>
</authors>
<title>The paragraph as a grammatical unit. In Givon (Givon,</title>
<date>1979</date>
<pages>115--134</pages>
<contexts>
<context position="1365" citStr="Longacre, 1979" startWordPosition="205" endWordPosition="206">t, 1997). 1 Background The notion of text cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) The</context>
</contexts>
<marker>Longacre, 1979</marker>
<rawString>L.E. Longacre. 1979. The paragraph as a grammatical unit. In Givon (Givon, 1979), pages 115-134:</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Topic • Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<note>[Online] http://www.cis.edurjcreynar/research.html</note>
<contexts>
<context position="2961" citStr="Reynar, 1998" startWordPosition="460" endWordPosition="461">on. Topic parsing (Hahn, 1990) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas (Flournoy et al., 1998a; Flournoy et al., 1998b). Given a dictionary W and a relatively small set C of meaningful &amp;quot;content&amp;quot; words, for each pair in W x C, the n</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey C. Reynar. 1998. Topic • Segmentation: Algorithms and Applications. Ph.D. thesis, University of Pennsylvania. [Online] http://www.cis.edurjcreynar/research.html [1999, April 24].</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Richmond</author>
<author>A Smith</author>
<author>E Amitay</author>
</authors>
<title>Detecting subject boundaries within text: A language independent statistical approach.</title>
<date>1997</date>
<booktitle>In Proceedings of The Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2).</booktitle>
<marker>Richmond, Smith, Amitay, 1997</marker>
<rawString>K. Richmond, A. Smith, and E. Amitay. 1997. Detecting subject boundaries within text: A language independent statistical approach. In Proceedings of The Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Ambiguity Resolution in Language Learning.</title>
<date>1997</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="3065" citStr="Schiitze, 1997" startWordPosition="478" endWordPosition="479"> domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas (Flournoy et al., 1998a; Flournoy et al., 1998b). Given a dictionary W and a relatively small set C of meaningful &amp;quot;content&amp;quot; words, for each pair in W x C, the number of times is recorded that the two co-occur within some measure of distance in a training corpus. T</context>
</contexts>
<marker>Schiitze, 1997</marker>
<rawString>Hinrich Schiitze. 1997. Ambiguity Resolution in Language Learning. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="3082" citStr="Schiitze, 1998" startWordPosition="480" endWordPosition="481"> knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary. This has led to promising results in information retrieval and related areas (Flournoy et al., 1998a; Flournoy et al., 1998b). Given a dictionary W and a relatively small set C of meaningful &amp;quot;content&amp;quot; words, for each pair in W x C, the number of times is recorded that the two co-occur within some measure of distance in a training corpus. This yields a 1C1-</context>
</contexts>
<marker>Schiitze, 1998</marker>
<rawString>Hinrich Schiitze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Sperber</author>
<author>Deidre Wilson</author>
</authors>
<title>Relevance: Communication and Cognition.</title>
<date>1995</date>
<publisher>Harvard University Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="1439" citStr="Sperber and Wilson, 1995" startWordPosition="214" endWordPosition="217"> intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) The VecTile system presented here is designed to utilize precisely this kind </context>
</contexts>
<marker>Sperber, Wilson, 1995</marker>
<rawString>Dan Sperber and Deidre Wilson. 1995. Relevance: Communication and Cognition. Harvard University Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Stark</author>
</authors>
<title>What do paragraph markings do?</title>
<date>1988</date>
<booktitle>Discourse Processes,</booktitle>
<pages>11--3</pages>
<contexts>
<context position="1401" citStr="Stark, 1988" startWordPosition="211" endWordPosition="212">ext cohesion rests on the intuition that a text is &amp;quot;held together&amp;quot; by a variety of internal forces. Much of the relevant linguistic literature is indebted to Halliday and Hasan (1976), where cohesion is defined as a network of relationships between locations in the text, arising from (i) grammatical factors (co-reference, use of pro-forms, ellipsis and sentential connectives), and (ii) lexical factors (reiteration and collocation). Subsequent work has further developed this taxonomy (Hoey, 1991) and explored its implications in such areas as paragraphing (Longacre, 1979; Bond and Hayes, 1984; Stark, 1988), relevance (Sperber and Wilson, 1995) and discourse structure (Grosz and Sidner, 1986). The lexical variety of cohesion is semantically defined, invoking a measure of word similarity. But this is hard to measure objectively, especially in the case of collocational relationships, which hold between words primarily because they &amp;quot;regularly cooccur.&amp;quot; Halliday and Hasan refrained from a deeper analysis, but hinted at a notion of &amp;quot;degrees of proximity in the lexical system, a function of the probability with which one tends to co-occur with another.&amp;quot; (p. 290) The VecTile system presented here is de</context>
</contexts>
<marker>Stark, 1988</marker>
<rawString>Heather Stark. 1988. What do paragraph markings do? Discourse Processes, 11(3):275-304.</rawString>
</citation>
<citation valid="true">
<title>The TDT Pilot Study Corpus Documentation version 1.3, 10. Distributed by the Linguistic Data Consortium.</title>
<date>1997</date>
<marker>1997</marker>
<rawString>1997a. The TDT Pilot Study Corpus Documentation version 1.3, 10. Distributed by the Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<title>The Topic Detection and Tracking (TDT) Pilot Study Evaluation Plan, 10. Distributed by the Linguistic Data Consortium.</title>
<date>1997</date>
<marker>1997</marker>
<rawString>1997b. The Topic Detection and Tracking (TDT) Pilot Study Evaluation Plan, 10. Distributed by the Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Youmans</author>
</authors>
<title>A new tool for discourse analysis: The vocabulary-management profile.</title>
<date>1991</date>
<journal>Language,</journal>
<pages>47--4</pages>
<contexts>
<context position="2723" citStr="Youmans, 1991" startWordPosition="419" endWordPosition="420">orpus to derive a measure of similarity between words and text passages. 2 Related Work Previous approaches to calculating cohesion differ in the kind of lexical relationship they quantify and in the amount of semantic knowledge they rely on. Topic parsing (Hahn, 1990) utilizes both grammatical cues and semantic inference based on pre-coded domain-specific knowledge. More general approaches assess word similarity based on thesauri (Morris and Hirst, 1991) or dictionary definitions (Kozima, 1994). Methods that solely use observations of patterns in vocabulary use include vocabulary management (Youmans, 1991) and the blocks algorithm im plemented in the TextTiling system (Hearst, 1997). The latter is compared below with the system introduced here. A good recent overview of previous approaches can be found in Chapters 4 and 5 of (Reynar, 1998). 3 The Method 3.1 Context Vectors The VecTile system is based on the WordSpace model of (Schiitze, 1997; Schiitze, 1998). The idea is to represent words by encoding the environments in which they typically occur in texts. Such a representation can be obtained automatically and often provides sufficient information to make deep linguistic analysis unnecessary.</context>
</contexts>
<marker>Youmans, 1991</marker>
<rawString>Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language, 47(4):763-789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>