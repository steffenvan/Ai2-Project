<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.947157">
Fast and Robust Neural Network Joint Models for Statistical Machine
Translation
</title>
<author confidence="0.9609165">
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John Makhoul
</author>
<affiliation confidence="0.785009">
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
</affiliation>
<email confidence="0.998319">
{jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com
</email>
<sectionHeader confidence="0.997382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862896551724">
Recent work has shown success in us-
ing neural network language models
(NNLMs) as features in MT systems.
Here, we present a novel formulation for
a neural network joint model (NNJM),
which augments the NNLM with a source
context window. Our model is purely lexi-
calized and can be integrated into any MT
decoder. We also present several varia-
tions of the NNJM which provide signif-
icant additive improvements.
Although the model is quite simple, it
yields strong empirical results. On the
NIST OpenMT12 Arabic-English condi-
tion, the NNJM features produce a gain of
+3.0 BLEU on top of a powerful, feature-
rich baseline which already includes a
target-only NNLM. The NNJM features
also produce a gain of +6.3 BLEU on top
of a simpler baseline equivalent to Chi-
ang’s (2007) original Hiero implementa-
tion.
Additionally, we describe two novel tech-
niques for overcoming the historically
high cost of using NNLM-style models
in MT decoding. These techniques speed
up NNJM computation by a factor of
10,000x, making the model as fast as a
standard back-off LM.
</bodyText>
<footnote confidence="0.6727315">
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opin-
ions, and/or findings contained in this article are those of the
author and should not be interpreted as representing the of-
ficial views or policies, either expressed or implied, of the
Defense Advanced Research Projects Agency or the Depart-
ment of Defense.
</footnote>
<sectionHeader confidence="0.998284" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980111111111">
In recent years, neural network models have be-
come increasingly popular in NLP. Initially, these
models were primarily used to create n-gram neu-
ral network language models (NNLMs) for speech
recognition and machine translation (Bengio et al.,
2003; Schwenk, 2010). They have since been ex-
tended to translation modeling, parsing, and many
other NLP tasks.
In this paper we use a basic neural network ar-
chitecture and a lexicalized probability model to
create a powerful MT decoding feature. Specifi-
cally, we introduce a novel formulation for a neu-
ral network joint model (NNJM), which augments
an n-gram target language model with an m-word
source window. Unlike previous approaches to
joint modeling (Le et al., 2012), our feature can be
easily integrated into any statistical machine trans-
lation (SMT) decoder, which leads to substantially
larger improvements than k-best rescoring only.
Additionally, we present several variations of this
model which provide significant additive BLEU
gains.
We also present a novel technique for training
the neural network to be self-normalized, which
avoids the costly step of posteriorizing over the
entire vocabulary in decoding. When used in con-
junction with a pre-computed hidden layer, these
techniques speed up NNJM computation by a fac-
tor of 10,000x, with only a small reduction on MT
accuracy.
Although our model is quite simple, we obtain
strong empirical results. We show primary results
on the NIST OpenMT12 Arabic-English condi-
tion. The NNJM features produce an improvement
of +3.0 BLEU on top of a baseline that is already
better than the 1st place MT12 result and includes
</bodyText>
<page confidence="0.918438">
1370
</page>
<note confidence="0.8293355">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370–1380,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999385636363636">
a powerful NNLM. Additionally, on top of a sim-
pler decoder equivalent to Chiang’s (2007) origi-
nal Hiero implementation, our NNJM features are
able to produce an improvement of +6.3 BLEU –
as much as all of the other features in our strong
baseline system combined.
We also show strong improvements on the
NIST OpenMT12 Chinese-English task, as well as
the DARPA BOLT (Broad Operational Language
Translation) Arabic-English and Chinese-English
conditions.
</bodyText>
<sectionHeader confidence="0.978615" genericHeader="method">
2 Neural Network Joint Model (NNJM)
</sectionHeader>
<bodyText confidence="0.999896285714286">
Formally, our model approximates the probability
of target hypothesis T conditioned on source sen-
tence S. We follow the standard n-gram LM de-
composition of the target, where each target word
ti is conditioned on the previous n − 1 target
words. To make this a joint model, we also condi-
tion on source context vector Si:
</bodyText>
<equation confidence="0.9968345">
P(T|S) ≈ Π|T|
i=1P(ti|ti−1,··· ,ti−n+1,Si)
</equation>
<bodyText confidence="0.9999782">
Intuitively, we want to define Si as the window
that is most relevant to ti. To do this, we first say
that each target word ti is affiliated with exactly
one source word at index ai. Si is then the m-word
source window centered at ai:
</bodyText>
<equation confidence="0.432837">
Si = sai− m−1
2 , ··· , sai, ··· , sai+m−1
2
</equation>
<bodyText confidence="0.9966728">
This notion of affiliation is derived from the
word alignment, but unlike word alignment, each
target word must be affiliated with exactly one
non-NULL source word. The affiliation heuristic
is very simple:
</bodyText>
<listItem confidence="0.974704714285714">
(1) If ti aligns to exactly one source word, ai is
the index of the word it aligns to.
(2) If ti align to multiple source words, ai is the
index of the aligned word in the middle.1
(3) If ti is unaligned, we inherit its affiliation
from the closest aligned word, with prefer-
ence given to the right.2
</listItem>
<bodyText confidence="0.993931166666667">
An example of the NNJM context model for a
Chinese-English parallel sentence is given in Fig-
ure 1.
For all of our experiments we use n = 4 and
m = 11. It is clear that this model is effectively
an (n+m)-gram LM, and a 15-gram LM would be
</bodyText>
<footnote confidence="0.968075333333334">
1We arbitrarily round down.
2We have found that the affiliation heuristic is robust to
small differences, such as left vs. right preference.
</footnote>
<bodyText confidence="0.9995875">
far too sparse for standard probability models such
as Kneser-Ney back-off (Kneser and Ney, 1995)
or Maximum Entropy (Rosenfeld, 1996). Fortu-
nately, neural network language models are able
to elegantly scale up and take advantage of arbi-
trarily large context sizes.
</bodyText>
<subsectionHeader confidence="0.91123">
2.1 Neural Network Architecture
</subsectionHeader>
<bodyText confidence="0.999942814814815">
Our neural network architecture is almost identi-
cal to the original feed-forward NNLM architec-
ture described in Bengio et al. (2003).
The input vector is a 14-word context vector
(3 target words, 11 source words), where each
word is mapped to a 192-dimensional vector us-
ing a shared mapping layer. We use two 512-
dimensional hidden layers with tanh activation
functions. The output layer is a softmax over the
entire output vocabulary.
The input vocabulary contains 16,000 source
words and 16,000 target words, while the out-
put vocabulary contains 32,000 target words. The
vocabulary is selected by frequency-sorting the
words in the parallel training data. Out-of-
vocabulary words are mapped to their POS tag (or
OOV, if POS is not available), and in this case
P(POSi|ti−1, · · · ) is used directly without fur-
ther normalization. Out-of-bounds words are rep-
resented with special tokens &lt;src&gt;, &lt;/src&gt;,
&lt;trg&gt;, &lt;/trg&gt;.
We chose these values for the hidden layer size,
vocabulary size, and source window size because
they seemed to work best on our data sets – larger
sizes did not improve results, while smaller sizes
degraded results. Empirical comparisons are given
in Section 6.5.
</bodyText>
<subsectionHeader confidence="0.98282">
2.2 Neural Network Training
</subsectionHeader>
<bodyText confidence="0.9980966">
The training procedure is identical to that of an
NNLM, except that the parallel corpus is used
instead of a monolingual corpus. Formally, we
seek to maximize the log-likelihood of the train-
ing data:
</bodyText>
<equation confidence="0.9903085">
L =� log(P(xi))
i
</equation>
<bodyText confidence="0.993898428571429">
where xi is the training sample, with one sample
for every target word in the parallel corpus.
Optimization is performed using standard back
propagation with stochastic gradient ascent (Le-
Cun et al., 1998). Weights are randomly initial-
ized in the range of [−0.05,0.05]. We use an ini-
tial learning rate of 10−3 and a minibatch size of
</bodyText>
<page confidence="0.996157">
1371
</page>
<figureCaption confidence="0.971895">
Figure 1: Context vector for target word “the”, using a 3-word target history and a 5-word source window
</figureCaption>
<bodyText confidence="0.95694235">
(i.e., n = 4 and m = 5). Here, “the” inherits its affiliation from “money” because this is the first aligned
word to its right. The number in each box denotes the index of the word in the context vector. This
indexing must be consistent across samples, but the absolute ordering does not affect results.
128.3 At every epoch, which we define as 20,000
minibatches, the likelihood of a validation set is
computed. If this likelihood is worse than the pre-
vious epoch, the learning rate is multiplied by 0.5.
The training is run for 40 epochs. The training
data ranges from 10-30M words, depending on the
condition. We perform a basic weight update with
no L2 regularization or momentum. However, we
have found it beneficial to clip each weight update
to the range of [-0.1, 0.1], to prevent the training
from entering degenerate search spaces (Pascanu
et al., 2012).
Training is performed on a single Tesla K10
GPU, with each epoch (128*20k = 2.6M samples)
taking roughly 1100 seconds to run, resulting in
a total training time of —12 hours. Decoding is
performed on a CPU.
</bodyText>
<equation confidence="0.9961176">
likelihood as:
eU (x) �
log(P(x)) = log Z(x)
= Ur(x) — log(Z(x))
Z(x) =
</equation>
<bodyText confidence="0.999063166666667">
where x is the sample, U is the raw output layer
scores, r is the output layer row corresponding to
the observed target word, and Z(x) is the softmax
normalizer.
If we could guarantee that log(Z(x)) were al-
ways equal to 0 (i.e., Z(x) = 1) then at decode
time we would only have to compute row r of the
output layer instead of the whole matrix. While
we cannot train a neural network with this guaran-
tee, we can explicitly encourage the log-softmax
normalizer to be as close to 0 as possible by aug-
menting our training objective function:
</bodyText>
<equation confidence="0.984742">
Σ|V |
r�=1eU l(x)
</equation>
<subsectionHeader confidence="0.97584">
2.3 Self-Normalized Neural Network EL = [log(P(xi)) — α(log(Z(xi)) — 0)2]
</subsectionHeader>
<bodyText confidence="0.999932714285714">
The computational cost of NNLMs is a significant
issue in decoding, and this cost is dominated by
the output softmax over the entire target vocabu-
lary. Even class-based approaches such as Le et
al. (2012) require a 2-20k shortlist vocabulary, and
are therefore still quite costly.
Here, our goal is to be able to use a fairly
large vocabulary without word classes, and to sim-
ply avoid computing the entire output layer at de-
code time.4 To do this, we present the novel
technique of self-normalization, where the output
layer scores are close to being probabilities with-
out explicitly performing a softmax.
Formally, we define the standard softmax log
</bodyText>
<footnote confidence="0.9993394">
3We do not divide the gradient by the minibatch size. For
those who do, this is equivalent to using an initial learning
rate of 10−3 ∗ 128 ≈ 10−1.
4We are not concerned with speeding up training time, as
we already find GPU training time to be adequate.
</footnote>
<equation confidence="0.969820666666667">
i
E= [log(P(xi)) — α log2(Z(xi))]
i
</equation>
<bodyText confidence="0.9999414375">
In this case, the output layer bias weights are
initialized to log(1/|V |), so that the initial net-
work is self-normalized. At decode time, we sim-
ply use Ur(x) as the feature score, rather than
log(P(x)). For our NNJM architecture, self-
normalization increases the lookup speed during
decoding by a factor of —15x.
Table 1 shows the neural network training re-
sults with various values of the free parameter
α. In all subsequent MT experiments, we use
α = 10−1.
We should note that Vaswani et al. (2013) im-
plements a method called Noise Contrastive Es-
timation (NCE) that is also used to train self-
normalized NNLMs. Although NCE results in
faster training time, it has the downside that there
</bodyText>
<page confidence="0.970458">
1372
</page>
<table confidence="0.9969985">
Arabic BOLT Val
α log(P(x))  |log(Z(x))|
0 −1.82 5.02
10−2 −1.81 1.35
10−1 −1.83 0.68
1 −1.91 0.28
</table>
<tableCaption confidence="0.999171">
Table 1: Comparison of neural network likelihood
</tableCaption>
<bodyText confidence="0.936920166666667">
for various α values. log(P(x)) is the average
log-likelihood on a held-out set.  |log(Z(x)) |is
the mean error in log-likelihood when using Ur(x)
directly instead of the true softmax probability
log(P(x)). Note that α = 0 is equivalent to the
standard neural network objective function.
is no mechanism to control the degree of self-
normalization. By contrast, our α parameter al-
lows us to carefully choose the optimal trade-off
between neural network accuracy and mean self-
normalization error. In future work, we will thor-
oughly compare self-normalization vs. NCE.
</bodyText>
<subsectionHeader confidence="0.999364">
2.4 Pre-Computing the Hidden Layer
</subsectionHeader>
<bodyText confidence="0.999954148148148">
Although self-normalization significantly im-
proves the speed of NNJM lookups, the model
is still several orders of magnitude slower than a
back-off LM. Here, we present a “trick” for pre-
computing the first hidden layer, which further in-
creases the speed of NNJM lookups by a factor of
1,000x.
Note that this technique only results in a signif-
icant speedup for self-normalized, feed-forward,
NNLM-style networks with one hidden layer. We
demonstrate in Section 6.6 that using one hidden
layer instead of two has minimal effect on BLEU.
For the neural network described in Section 2.1,
computing the first hidden layer requires mul-
tiplying a 2689-dimensional input vector5 with
a 2689 × 512 dimensional hidden layer matrix.
However, note that there are only 3 possible posi-
tions for each target word, and 11 for each source
word. Therefore, for every word in the vocabu-
lary, and for each position, we can pre-compute
the dot product between the word embedding and
the first hidden layer. These are computed offline
and stored in a lookup table, which is &lt;500MB in
size.
Computing the first hidden layer now only re-
quires 15 scalar additions for each of the 512
hidden rows – one for each word in the input
</bodyText>
<equation confidence="0.569763">
52689 = 14 words × 192 dimensions + 1 bias
</equation>
<bodyText confidence="0.999687545454545">
vector, plus the bias. This can be reduced to
just 5 scalar additions by pre-summing each 11-
word source window when starting a test sen-
tence. If our neural network has only one hid-
den layer and is self-normalized, the only remain-
ing computation is 512 calls to tanh() and a sin-
gle 513-dimensional dot product for the final out-
put score.6 Thus, only ∼3500 arithmetic opera-
tions are required per n-gram lookup, compared
to ∼2.8M for self-normalized NNJM without pre-
computation, and ∼35M for the standard NNJM.7
</bodyText>
<table confidence="0.9988012">
Neural Network Speed
Condition lookups/sec sec/word
Standard 110 10.9
+ Self-Norm 1500 0.8
+ Pre-Computation 1,430,000 0.0008
</table>
<tableCaption confidence="0.991021">
Table 2: Speed of the neural network computa-
</tableCaption>
<bodyText confidence="0.996840111111111">
tion on a single CPU thread. “lookups/sec” is the
number of unique n-gram probabilities that can be
computed per second. “sec/word” is the amortized
cost of unique NNJM lookups in decoding, per
source word.
Table 2 shows the speed of self-normalization
and pre-computation for the NNJM. The decoding
cost is based on a measurement of ∼1200 unique
NNJM lookups per source word for our Arabic-
English system.8
By combining self-normalization and pre-
computation, we can achieve a speed of 1.4M
lookups/second, which is on par with fast back-
off LM implementations (Tanaka et al., 2013).
We demonstrate in Section 6.6 that using the self-
normalized/pre-computed NNJM results in only
a very small BLEU degradation compared to the
standard NNJM.
</bodyText>
<sectionHeader confidence="0.80797" genericHeader="method">
3 Decoding with the NNJM
</sectionHeader>
<bodyText confidence="0.752138615384616">
Because our NNJM is fundamentally an n-gram
NNLM with additional source context, it can eas-
ily be integrated into any SMT decoder. In this
section, we describe the considerations that must
be taken when integrating the NNJM into a hierar-
chical decoder.
6tanh() is implemented using a lookup table.
73500≈ 5 × 512 + 2 × 513; 2.8M ≈ 2 × 2689 × 512 +
2 × 513; 35M ≈ 2 × 2689 × 512 + 2 × 513 × 32000. For
the sake of a fair comparison, these all use one hidden layer.
A second hidden layer adds 0.5M floating point operations.
8This does not include the cost of duplicate lookups
within the same test sentence, which are cached.
</bodyText>
<page confidence="0.958932">
1373
</page>
<subsectionHeader confidence="0.998107">
3.1 Hierarchical Parsing
</subsectionHeader>
<bodyText confidence="0.9999043">
When performing hierarchical decoding with an
n-gram LM, the leftmost and rightmost n − 1
words from each constituent must be stored in the
state space. Here, we extend the state space to
also include the index of the affiliated source word
for these edge words. This does not noticeably in-
crease the search space. We also train a separate
lower-order n-gram model, which is necessary to
compute estimate scores during hierarchical de-
coding.
</bodyText>
<subsectionHeader confidence="0.999707">
3.2 Affiliation Heuristic
</subsectionHeader>
<bodyText confidence="0.999878583333333">
For aligned target words, the normal affiliation
heuristic can be used, since the word alignment
is available within the rule. For unaligned words,
the normal heuristic can also be used, except when
the word is on the edge of a rule, because then the
target neighbor words are not necessarily known.
In this case, we infer the affiliation from the rule
structure. Specifically, if unaligned target word t
is on the right edge of an arc that covers source
span [si, sj], we simply say that t is affiliated with
source word sj. If t is on the left edge of the arc,
we say it is affiliated with si.
</bodyText>
<sectionHeader confidence="0.994002" genericHeader="method">
4 Model Variations
</sectionHeader>
<bodyText confidence="0.96070075">
Recall that our NNJM feature can be described
with the following probability:
H|T|
i=1P(ti|ti−1, ti−2, ··· , sai, sai−1, sai+1, ··· )
This formulation lends itself to several natural
variations. In particular, we can reverse the trans-
lation direction of the languages, as well as the di-
rection of the language model.
We denote our original formulation as a source-
to-target, left-to-right model (S2T/L2R). We can
train three variations using target-to-source (T2S)
and right-to-left (R2L) models:
</bodyText>
<equation confidence="0.967691222222222">
S2T/R2L
H|T |
i=1P(ti|ti+1, ti+2, ··· , sai, sai−1, sai+1, ··· )
T2S/L2R
S
Hi=1P(si |si−1, si−2, ··· , taz, tai−1, tai+1, ··· )
T2S/R2L
H|S|
i=1P(si|si+1, si+2, ··· , taz, tai−1, tai+1, ··· )
</equation>
<bodyText confidence="0.9999415">
where a0i is the target-to-source affiliation, de-
fined analogously to ai.
The T2S variations cannot be used in decoding
due to the large target context required, and are
thus only used in k-best rescoring. The S2T/R2L
variant could be used in decoding, but we have not
found this beneficial, so we only use it in rescor-
ing.
</bodyText>
<subsectionHeader confidence="0.7624735">
4.1 Neural Network Lexical Translation
Model (NNLTM)
</subsectionHeader>
<bodyText confidence="0.999980142857143">
One issue with the S2T NNJM is that the prob-
ability is computed over every target word, so it
does not explicitly model NULL-aligned source
words. In order to assign a probability to every
source word during decoding, we also train a neu-
ral network lexical translation model (NNLMT).
Here, the input context is the 11-word source
window centered at si, and the output is the tar-
get token tsi which si aligns to. The probabil-
ity is computed over every source word in the in-
put sentence. We treat NULL as a normal target
word, and if a source word aligns to multiple target
words, it is treated as a single concatenated token.
Formally, the probability model is:
</bodyText>
<equation confidence="0.9939935">
S
Hi=1P(tsi  |si, si−1, si+1, ··· )
</equation>
<bodyText confidence="0.996558857142857">
This model is trained and evaluated like our
NNJM. It is easy and computationally inexpensive
to use this model in decoding, since only one neu-
ral network computation must be made for each
source word.
In rescoring, we also use a T2S NNLTM model
computed over every target word:
</bodyText>
<equation confidence="0.9550655">
H||
T 1P(sti  |ti, ti−1, ti+1, ··· )
</equation>
<sectionHeader confidence="0.994898" genericHeader="method">
5 MT System
</sectionHeader>
<bodyText confidence="0.998874">
In this section, we describe the MT system used in
our experiments.
</bodyText>
<subsectionHeader confidence="0.990223">
5.1 MT Decoder
</subsectionHeader>
<bodyText confidence="0.9999665">
We use a state-of-the-art string-to-dependency hi-
erarchical decoder (Shen et al., 2010). Our base-
line decoder contains a large and powerful set of
features, which include:
</bodyText>
<listItem confidence="0.9999622">
• Forward and backward rule probabilities
• 4-gram Kneser-Ney LM
• Dependency LM (Shen et al., 2010)
• Contextual lexical smoothing (Devlin, 2009)
• Length distribution (Shen et al., 2010)
• Trait features (Devlin and Matsoukas, 2012)
• Factored source syntax (Huang et al., 2013)
• 7 sparse feature types, totaling 50k features
(Chiang et al., 2009)
• LM adaptation (Snover et al., 2008)
</listItem>
<page confidence="0.986286">
1374
</page>
<bodyText confidence="0.9963555">
We also perform 1000-best rescoring with the
following features:
</bodyText>
<listItem confidence="0.999422">
• 5-gram Kneser-Ney LM
• Recurrent neural network language model
(RNNLM) (Mikolov et al., 2010)
</listItem>
<bodyText confidence="0.99981425">
Although we consider the RNNLM to be part
of our baseline, we give it special treatment in the
results section because we would expect it to have
the highest overlap with our NNJM.
</bodyText>
<subsectionHeader confidence="0.998221">
5.2 Training and Optimization
</subsectionHeader>
<bodyText confidence="0.999765833333333">
For Arabic word tokenization, we use the MADA-
ARZ tokenizer (Habash et al., 2013) for the BOLT
condition, and the Sakhr9 tokenizer for the NIST
condition. For Chinese tokenization, we use a sim-
ple longest-match-first lexicon-based approach.
For word alignment, we align all of the train-
ing data with both GIZA++ (Och and Ney, 2003)
and NILE (Riesa et al., 2011), and concatenate the
corpora together for rule extraction.
For MT feature weight optimization, we use
iterative k-best optimization with an Expected-
BLEU objective function (Rosti et al., 2010).
</bodyText>
<sectionHeader confidence="0.99825" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999024">
We present MT primary results on Arabic-English
and Chinese-English for the NIST OpenMT12 and
DARPA BOLT conditions. We also present a set
of auxiliary results in order to further analyze our
features.
</bodyText>
<subsectionHeader confidence="0.98345">
6.1 NIST OpenMT12 Results
</subsectionHeader>
<bodyText confidence="0.999671357142857">
Our NIST system is fully compatible with the
OpenMT12 constrained track, which consists of
10M words of high-quality parallel training for
Arabic, and 25M words for Chinese.10 The
Kneser-Ney LM is trained on 5B words of data
from English GigaWord. For test, we use
the “Arabic-To-English Original Progress Test”
(1378 segments) and “Chinese-to-English Orig-
inal Progress Test + OpenMT12 Current Test”
(2190 segments), which consists of a mix of
newswire and web data.11 All test segments have
4 references. Our tuning set contains 5000 seg-
ments, and is a mix of the MT02-05 eval set as
well as held-out parallel training.
</bodyText>
<footnote confidence="0.9984728">
9http://www.sakhr.com
10We also make weak use of 30M-100M words of UN data
+ ISI comparable corpora, but this data provides almost no
benefit.
11http://www.nist.gov/itl/iad/mig/openmt12results.cfm
</footnote>
<table confidence="0.999902095238095">
NIST MT12 Test
Ar-En Ch-En
BLEU BLEU
OpenMT12 - 1st Place 49.5 32.6
OpenMT12 - 2nd Place 47.5 32.2
OpenMT12 - 3rd Place 47.4 30.8
· · · · · · · · ·
OpenMT12 - 9th Place 44.0 27.0
OpenMT12 - 10th Place 41.2 25.7
Baseline (w/o RNNLM) 48.9 33.0
Baseline (w/ RNNLM) 49.8 33.4
+ S2T/L2R NNJM (Dec) 51.2 34.2
+ S2T NNLTM (Dec) 52.0 34.2
+ T2S NNLTM (Resc) 51.9 34.2
+ S2T/R2L NNJM (Resc) 52.2 34.3
+ T2S/L2R NNJM (Resc) 52.3 34.5
+ T2S/R2L NNJM (Resc) 52.8 34.7
“Simple Hier.” Baseline 43.4 30.1
+ S2T/L2R NNJM (Dec) 47.2 31.5
+ S2T NNLTM (Dec) 48.5 31.8
+ Other NNJMs (Resc) 49.7 32.2
</table>
<tableCaption confidence="0.994956">
Table 3: Primary results on Arabic-English and
</tableCaption>
<bodyText confidence="0.981463833333333">
Chinese-English NIST MT12 Test Set. The first
section corresponds to the top and bottom ranked
systems from the evaluation, and are taken from
the NIST website. The second section corresponds
to results on top of our strongest baseline. The
third section corresponds to results on top of a
simpler baseline. Within each section, each row
includes all of the features from previous rows.
BLEU scores are mixed-case.
Results are shown in the second section of Ta-
ble 3. On Arabic-English, the primary S2T/L2R
NNJM gains +1.4 BLEU on top of our baseline,
while the S2T NNLTM gains another +0.8, and
the directional variations gain +0.8 BLEU more.
This leads to a total improvement of +3.0 BLEU
from the NNJM and its variations. Considering
that our baseline is already +0.3 BLEU better than
the 1st place result of MT12 and contains a strong
RNNLM, we consider this to be quite an extraor-
dinary improvement.12
For the Chinese-English condition, there is an
improvement of +0.8 BLEU from the primary
NNJM and +1.3 BLEU overall. Here, the base-
line system is already +0.8 BLEU better than the
</bodyText>
<footnote confidence="0.904370666666667">
12Note that the official 1st place OpenMT12 result was our
own system, so we can assure that these comparisons are ac-
curate.
</footnote>
<page confidence="0.993575">
1375
</page>
<bodyText confidence="0.999898">
best MT12 system. The smaller improvement on
Chinese-English compared to Arabic-English is
consistent with the behavior of our baseline fea-
tures, as we show in the next section.
</bodyText>
<subsectionHeader confidence="0.998154">
6.2 “Simple Hierarchical” NIST Results
</subsectionHeader>
<bodyText confidence="0.999969696969697">
The baseline used in the last section is a highly-
engineered research system, which uses a wide
array of features that were refined over a num-
ber of years, and some of which require linguis-
tic resources. Because of this, the baseline BLEU
scores are much higher than a typical MT system
– especially a real-time, production engine which
must support many language pairs.
Therefore, we also present results using a
simpler version of our decoder which emulates
Chiang’s original Hiero implementation (Chiang,
2007). Specifically, this means that we don’t
use dependency-based rule extraction, and our de-
coder only contains the following MT features: (1)
rule probabilities, (2) n-gram Kneser-Ney LM, (3)
lexical smoothing, (4) target word count, (5) con-
cat rule penalty.
Results are shown in the third section of Table 3.
The “Simple Hierarchical” Arabic-English system
is -6.4 BLEU worse than our strong baseline, and
would have ranked 10th place out of 11 systems
in the evaluation. When the NNJM features are
added to this system, we see an improvement of
+6.3 BLEU, which would have ranked 1st place in
the evaluation.
Effectively, this means that for Arabic-English,
the NNJM features are equivalent to the combined
improvements from the string-to-dependency
model plus all of the features listed in Section 5.1.
For Chinese-English, the “Simple Hierarchical”
system only degrades by -3.2 BLEU compared
to our strongest baseline, and the NNJM features
produce a gain of +2.1 BLEU on top of that.
</bodyText>
<subsectionHeader confidence="0.996249">
6.3 BOLT Web Forum Results
</subsectionHeader>
<bodyText confidence="0.998517266666667">
DARPA BOLT is a major research project with the
goal of improving translation of informal, dialec-
tical Arabic and Chinese into English. The BOLT
domain presented here is “web forum,” which was
crawled from various Chinese and Egyptian Inter-
net forums by LDC. The BOLT parallel training
consists of all of the high-quality NIST training,
plus an additional 3 million words of translated
forum data provided by LDC. The tuning and test
sets consist of roughly 5000 segments each, with
2 references for Arabic and 3 for Chinese.
Results are shown in Table 4. The baseline here
uses the same feature set as the strong NIST sys-
tem. On Arabic, the total gain is +2.6 BLEU,
while on Chinese, the gain is +1.3 BLEU.
</bodyText>
<table confidence="0.999487">
BOLT Test
Ar-En Ch-En
BLEU BLEU
Baseline (w/o RNNLM) 40.2 30.6
Baseline (w/ RNNLM) 41.3 30.9
+ S2T/L2R NNJM (Dec) 42.9 31.9
+ S2T NNLTM (Dec) 43.2 31.9
+ Other NNJMs (Resc) 43.9 32.2
</table>
<tableCaption confidence="0.86122">
Table 4: Primary results on Arabic-English and
Chinese-English BOLT Web Forum. Each row
</tableCaption>
<bodyText confidence="0.654153">
includes the aggregate features from all previous
rows.
</bodyText>
<subsectionHeader confidence="0.998808">
6.4 Effect of k-best Rescoring Only
</subsectionHeader>
<bodyText confidence="0.9997486">
Table 5 shows performance when our S2T/L2R
NNJM is used only in 1000-best rescoring, com-
pared to decoding. The primary purpose of this is
as a comparison to Le et al. (2012), whose model
can only be used in k-best rescoring.
</bodyText>
<table confidence="0.999358375">
BOLT Test
Ar-En
Without With
RNNLM RNNLM
BLEU BLEU
Baseline 40.2 41.3
S2T/L2R NNJM (Resc) 41.7 41.6
S2T/L2R NNJM (Dec) 42.8 42.9
</table>
<tableCaption confidence="0.980973">
Table 5: Comparison of our primary NNJM in de-
coding vs. 1000-best rescoring.
</tableCaption>
<bodyText confidence="0.998545454545455">
We can see that the rescoring-only NNJM per-
forms very well when used on top of a baseline
without an RNNLM (+1.5 BLEU), but the gain on
top of the RNNLM is very small (+0.3 BLEU).
The gain from the decoding NNJM is large in both
cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/
RNNLM). This demonstrates that the full power of
the NNJM can only be harnessed when it is used
in decoding. It is also interesting to see that the
RNNLM is no longer beneficial when the NNJM
is used.
</bodyText>
<page confidence="0.980967">
1376
</page>
<subsectionHeader confidence="0.995208">
6.5 Effect of Neural Network Configuration
</subsectionHeader>
<bodyText confidence="0.999574375">
Table 6 shows results using the S2T/L2R NNJM
with various configurations. We can see that re-
ducing the source window size, layer size, or vo-
cab size will all degrade results. Increasing the
sizes beyond the default NNJM has almost no ef-
fect (102%). Also note that the target-only NNLM
(i.e., Source Window=0) only obtains 33% of the
improvements of the NNJM.
</bodyText>
<table confidence="0.999743882352941">
BOLT Test
Ar-En
BLEU % Gain
“Simple Hier.” Baseline 33.8 -
S2T/L2R NNJM (Dec) 38.4 100%
Source Window=7 38.3 98%
Source Window=5 38.2 96%
Source Window=3 37.8 87%
Source Window=0 35.3 33%
Layers=384x768x768 38.5 102%
Layers=192x512 38.1 93%
Layers=128x128 37.1 72%
Vocab=64,000 38.5 102%
Vocab=16,000 38.1 93%
Vocab=8,000 37.3 83%
Activation=Rectified Lin. 38.5 102%
Activation=Linear 37.3 76%
</table>
<tableCaption confidence="0.989419">
Table 6: Results with different neural net-
</tableCaption>
<bodyText confidence="0.983037777777778">
work architectures. The “default” NNJM in
the second row uses these parameters: SW=11,
L=192x512x512, V=32,000, A=tanh. All mod-
els use a 3-word target history (i.e., 4-gram LM).
“Layers” refers to the size of the word embedding
followed by the hidden layers. “Vocab” refers to
the size of the input and output vocabularies. “%
Gain” is the BLEU gain over the baseline relative
to the default NNJM.
</bodyText>
<subsectionHeader confidence="0.999794">
6.6 Effect of Speedups
</subsectionHeader>
<bodyText confidence="0.986836666666667">
All previous results use a self-normalized neural
network with two hidden layers. In Table 7, we
compare this to using a standard network (with
two hidden layers), as well as a pre-computed neu-
ral network.13 The “Simple Hierarchical” base-
line is used here because it more closely approx-
imates a real-time MT engine. For the sake of
speed, these experiments only use the S2T/L2R
NNJM+S2T NNLTM.
</bodyText>
<footnote confidence="0.919651">
13The difference in score for self-normalized vs. pre-
computed is entirely due to two vs. one hidden layers.
</footnote>
<bodyText confidence="0.999041">
Each result from Table 7 corresponds to a row
in Table 2 of Section 2.4. We can see that go-
ing from the standard model to the pre-computed
model only reduces the BLEU improvement from
+6.4 to +6.1, while increasing the NNJM lookup
speed by a factor of 10,000x.
</bodyText>
<table confidence="0.999506857142857">
BOLT Test
Ar-En
BLEU Gain
“Simple Hier.” Baseline 33.8 -
Standard NNJM 40.2 +6.4
Self-Norm NNJM 40.1 +6.3
Pre-Computed NNJM 39.9 +6.1
</table>
<tableCaption confidence="0.959236">
Table 7: Results for the standard NNs vs. self-
normalized NNs vs. pre-computed NNs.
</tableCaption>
<bodyText confidence="0.999800428571429">
In Table 2 we showed that the cost of unique
lookups for the pre-computed NNJM is only
—0.001 seconds per source word. This does not
include the cost of n-gram creation or cached
lookups, which amount to —0.03 seconds per
source word in our current implementation.14
However, the n-grams created for the NNJM can
be shared with the Kneser-Ney LM, which reduces
the cost of that feature. Thus, the total cost in-
crease of using the NNJM+NNLTM features in
decoding is only —0.01 seconds per source word.
In future work we will provide more detailed
analysis regarding the usability of the NNJM in a
low-latency, high-throughput MT engine.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9999594">
Although there has been a substantial amount of
past work in lexicalized joint models (Marino et
al., 2006; Crego and Yvon, 2010), nearly all of
these papers have used older statistical techniques
such as Kneser-Ney or Maximum Entropy. How-
ever, not only are these techniques intractable to
train with high-order context vectors, they also
lack the neural network’s ability to semantically
generalize (Mikolov et al., 2013) and learn non-
linear relationships.
A number of recent papers have proposed meth-
ods for creating neural network translation/joint
models, but nearly all of these works have ob-
tained much smaller BLEU improvements than
ours. For each related paper, we will briefly con-
</bodyText>
<footnote confidence="0.9791235">
14In our decoder, roughly 95% of NNJM n-gram lookups
within the same sentence are duplicates.
</footnote>
<page confidence="0.993726">
1377
</page>
<bodyText confidence="0.999963035714286">
trast their methodology with our own and summa-
rize their BLEU improvements using scores taken
directly from the cited paper.
Auli et al. (2013) use a fixed continuous-space
source representation, obtained from LDA (Blei
et al., 2003) or a source-only NNLM. Also, their
model is recurrent, so it cannot be used in decod-
ing. They obtain +0.2 BLEU improvement on top
of a target-only NNLM (25.6 vs. 25.8).
Schwenk (2012) predicts an entire target phrase
at a time, rather than a word at a time. He obtains
+0.3 BLEU improvement (24.8 vs. 25.1).
Zou et al. (2013) estimate context-free bilingual
lexical similarity scores, rather than using a large
context. They obtain an +0.5 BLEU improvement
on Chinese-English (30.0 vs. 30.5).
Kalchbrenner and Blunsom (2013) implement
a convolutional recurrent NNJM. They score a
1000-best list using only their model and are able
to achieve the same BLEU as using all 12 standard
MT features (21.8 vs 21.7). However, additive re-
sults are not presented.
The most similar work that we know of is Le et
al. (2012). Le’s basic procedure is to re-order the
source to match the linear order of the target, and
then segment the hypothesis into minimal bilin-
gual phrase pairs. Then, he predicts each target
word given the previous bilingual phrases. How-
ever, Le’s formulation could only be used in k-
best rescoring, since it requires long-distance re-
ordering and a large target context.
Le’s model does obtain an impressive +1.7
BLEU gain on top of a baseline without an NNLM
(25.8 vs. 27.5). However, when compared to
the strongest baseline which includes an NNLM,
Le’s best models (S2T + T2S) only obtain an +0.6
BLEU improvement (26.9 vs. 27.5). This is con-
sistent with our rescoring-only result, which indi-
cates that k-best rescoring is too shallow to take
advantage of the power of a joint model.
Le’s model also uses minimal phrases rather
than being purely lexicalized, which has two main
downsides: (a) a number of complex, hand-crafted
heuristics are required to define phrase boundaries,
which may not transfer well to new languages, (b)
the effective vocabulary size is much larger, which
substantially increases data sparsity issues.
We should note that our best results use six sep-
arate models, whereas all previous work only uses
one or two models. However, we have demon-
strated that we can obtain 50%-80% of the to-
tal improvement with only one model (S2T/L2R
NNJM), and 70%-90% with only two models
(S2T/L2R NNJM + S2T NNLTM). Thus, the one
and two-model conditions still significantly out-
perform any past work.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.9999678">
We have described a novel formulation for a neural
network-based machine translation joint model,
along with several simple variations of this model.
When used as MT decoding features, these models
are able to produce a gain of +3.0 BLEU on top of
a very strong and feature-rich baseline, as well as
a +6.3 BLEU gain on top of a simpler system.
Our model is remarkably simple – it requires no
linguistic resources, no feature engineering, and
only a handful of hyper-parameters. It also has no
reliance on potentially fragile outside algorithms,
such as unsupervised word clustering. We con-
sider the simplicity to be a major advantage. Not
only does this suggest that it will generalize well to
new language pairs and domains, but it also sug-
gests that it will be straightforward for others to
replicate these results.
Overall, we believe that the following factors set
us apart from past work and allowed us to obtain
such significant improvements:
</bodyText>
<listItem confidence="0.596669181818182">
1. The ability to use the NNJM in decoding
rather than rescoring.
2. The use of a large bilingual context vector,
which is provided to the neural network in
“raw” form, rather than as the output of some
other algorithm.
3. The fact that the model is purely lexicalized,
which avoids both data sparsity and imple-
mentation complexity.
4. The large size of the network architecture.
5. The directional variation models.
</listItem>
<bodyText confidence="0.9998872">
One of the biggest goals of this work is to quell
any remaining doubts about the utility of neural
networks in machine translation. We believe that
there are large areas of research yet to be explored.
For example, creating a new type of decoder cen-
tered around a purely lexicalized neural network
model. Our short term ideas include using more
interesting types of context in our input vector
(such as source syntax), or using the NNJM to
model syntactic/semantic structure of the target.
</bodyText>
<page confidence="0.991619">
1378
</page>
<sectionHeader confidence="0.996169" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99985945631068">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In HLT-NAACL, pages 218–226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Josep Maria Crego and Franc¸ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24(2):159–
175.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based hypothesis selection for machine translation.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 528–532, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jacob Devlin. 2009. Lexical features for statistical
machine translation. Master’s thesis, University of
Maryland.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal arabic. In
HLT-NAACL, pages 426–432.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In EMNLP, pages
556–566.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ’12, pages 39–
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yann LeCun, L´eon Bottou, Genevieve B Orr, and
Klaus-Robert M¨uller. 1998. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9–50.
Springer.
Jos´e B Marino, Rafael E Banchs, Josep M Crego, Adri`a
De Gispert, Patrik Lambert, Jos´e AR Fonollosa, and
Marta R Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527–
549.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. On the difficulty of training recurrent neural
networks. arXiv preprint arXiv:1211.5063.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 497–507, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language, 10:187–228.
Antti Rosti, Bing Zhang, Spyros Matsoukas, and
Rich Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In
WMT/MetricsMATR, pages 321–326.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague
Bull. Math. Linguistics, 93:137–146.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071–1080.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649–671,
December.
</reference>
<page confidence="0.889433">
1379
</page>
<reference confidence="0.998322181818182">
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’08, pages 857–866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, and
Mikio Norimatsu. 2013. An efficient language
model using double-array structures.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387–1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393–1398.
</reference>
<page confidence="0.990341">
1380
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441359">
<title confidence="0.9995805">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</title>
<author confidence="0.9587035">Jacob Devlin</author>
<author confidence="0.9587035">Rabih Zbib</author>
<author confidence="0.9587035">Zhongqiang Lamar</author>
<author confidence="0.9587035">Richard Schwartz</author>
<address confidence="0.690664">Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138,</address>
<abstract confidence="0.999852566666667">Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for neural network (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condition, the NNJM features produce a gain of +3.0 BLEU on top of a powerful, featurerich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chiang’s (2007) original Hiero implementation. Additionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.</abstract>
<note confidence="0.910758875">This work was supported by DARPA/I2O Contract No. HR0011-12-C-0014 under the BOLT program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="30509" citStr="Auli et al. (2013)" startWordPosition="5136" endWordPosition="5139">ontext vectors, they also lack the neural network’s ability to semantically generalize (Mikolov et al., 2013) and learn nonlinear relationships. A number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours. For each related paper, we will briefly con14In our decoder, roughly 95% of NNJM n-gram lookups within the same sentence are duplicates. 1377 trast their methodology with our own and summarize their BLEU improvements using scores taken directly from the cited paper. Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM. Also, their model is recurrent, so it cannot be used in decoding. They obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs. 25.8). Schwenk (2012) predicts an entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). Zou et al. (2013) estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5). Kalchbrenner and</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2035" citStr="Bengio et al., 2003" startWordPosition="312" endWordPosition="315">-0014 under the BOLT program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. 1 Introduction In recent years, neural network models have become increasingly popular in NLP. Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger</context>
<context position="6066" citStr="Bengio et al. (2003)" startWordPosition="984" endWordPosition="987">vely an (n+m)-gram LM, and a 15-gram LM would be 1We arbitrarily round down. 2We have found that the affiliation heuristic is robust to small differences, such as left vs. right preference. far too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996). Fortunately, neural network language models are able to elegantly scale up and take advantage of arbitrarily large context sizes. 2.1 Neural Network Architecture Our neural network architecture is almost identical to the original feed-forward NNLM architecture described in Bengio et al. (2003). The input vector is a 14-word context vector (3 target words, 11 source words), where each word is mapped to a 192-dimensional vector using a shared mapping layer. We use two 512- dimensional hidden layers with tanh activation functions. The output layer is a softmax over the entire output vocabulary. The input vocabulary contains 16,000 source words and 16,000 target words, while the output vocabulary contains 32,000 target words. The vocabulary is selected by frequency-sorting the words in the parallel training data. Out-ofvocabulary words are mapped to their POS tag (or OOV, if POS is not</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="30599" citStr="Blei et al., 2003" startWordPosition="5149" endWordPosition="5152">kolov et al., 2013) and learn nonlinear relationships. A number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours. For each related paper, we will briefly con14In our decoder, roughly 95% of NNJM n-gram lookups within the same sentence are duplicates. 1377 trast their methodology with our own and summarize their BLEU improvements using scores taken directly from the cited paper. Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM. Also, their model is recurrent, so it cannot be used in decoding. They obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs. 25.8). Schwenk (2012) predicts an entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). Zou et al. (2013) estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5). Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM. They score a 1000-best list usin</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="19084" citStr="Chiang et al., 2009" startWordPosition="3220" endWordPosition="3223"> ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: • Forward and backward rule probabilities • 4-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we u</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In HLT-NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="23696" citStr="Chiang, 2007" startWordPosition="3990" endWordPosition="3991">ior of our baseline features, as we show in the next section. 6.2 “Simple Hierarchical” NIST Results The baseline used in the last section is a highlyengineered research system, which uses a wide array of features that were refined over a number of years, and some of which require linguistic resources. Because of this, the baseline BLEU scores are much higher than a typical MT system – especially a real-time, production engine which must support many language pairs. Therefore, we also present results using a simpler version of our decoder which emulates Chiang’s original Hiero implementation (Chiang, 2007). Specifically, this means that we don’t use dependency-based rule extraction, and our decoder only contains the following MT features: (1) rule probabilities, (2) n-gram Kneser-Ney LM, (3) lexical smoothing, (4) target word count, (5) concat rule penalty. Results are shown in the third section of Table 3. The “Simple Hierarchical” Arabic-English system is -6.4 BLEU worse than our strong baseline, and would have ranked 10th place out of 11 systems in the evaluation. When the NNJM features are added to this system, we see an improvement of +6.3 BLEU, which would have ranked 1st place in the eva</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Maria Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Factored bilingual n-gram language models for statistical machine translation.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>175</pages>
<contexts>
<context position="29707" citStr="Crego and Yvon, 2010" startWordPosition="5009" endWordPosition="5012"> creation or cached lookups, which amount to —0.03 seconds per source word in our current implementation.14 However, the n-grams created for the NNJM can be shared with the Kneser-Ney LM, which reduces the cost of that feature. Thus, the total cost increase of using the NNJM+NNLTM features in decoding is only —0.01 seconds per source word. In future work we will provide more detailed analysis regarding the usability of the NNJM in a low-latency, high-throughput MT engine. 7 Related Work Although there has been a substantial amount of past work in lexicalized joint models (Marino et al., 2006; Crego and Yvon, 2010), nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy. However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network’s ability to semantically generalize (Mikolov et al., 2013) and learn nonlinear relationships. A number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours. For each related paper, we will briefly con14In our decoder, roughly 95% of NNJM n-</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep Maria Crego and Franc¸ois Yvon. 2010. Factored bilingual n-gram language models for statistical machine translation. Machine Translation, 24(2):159– 175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Spyros Matsoukas</author>
</authors>
<title>Traitbased hypothesis selection for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>528--532</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18968" citStr="Devlin and Matsoukas, 2012" startWordPosition="3200" endWordPosition="3203">source word. In rescoring, we also use a T2S NNLTM model computed over every target word: H|| T 1P(sti |ti, ti−1, ti+1, ··· ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: • Forward and backward rule probabilities • 4-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash e</context>
</contexts>
<marker>Devlin, Matsoukas, 2012</marker>
<rawString>Jacob Devlin and Spyros Matsoukas. 2012. Traitbased hypothesis selection for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 528–532, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
</authors>
<title>Lexical features for statistical machine translation. Master’s thesis,</title>
<date>2009</date>
<institution>University of Maryland.</institution>
<contexts>
<context position="18880" citStr="Devlin, 2009" startWordPosition="3188" endWordPosition="3189">decoding, since only one neural network computation must be made for each source word. In rescoring, we also use a T2S NNLTM model computed over every target word: H|| T 1P(sti |ti, ti−1, ti+1, ··· ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: • Forward and backward rule probabilities • 4-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Traini</context>
</contexts>
<marker>Devlin, 2009</marker>
<rawString>Jacob Devlin. 2009. Lexical features for statistical machine translation. Master’s thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Ramy Eskander</author>
<author>Nadi Tomeh</author>
</authors>
<title>Morphological analysis and disambiguation for dialectal arabic. In</title>
<date>2013</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>426--432</pages>
<contexts>
<context position="19580" citStr="Habash et al., 2013" startWordPosition="3303" endWordPosition="3306">s, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BO</context>
</contexts>
<marker>Habash, Roth, Rambow, Eskander, Tomeh, 2013</marker>
<rawString>Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh. 2013. Morphological analysis and disambiguation for dialectal arabic. In HLT-NAACL, pages 426–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
</authors>
<title>Factored soft source syntactic constraints for hierarchical machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>556--566</pages>
<contexts>
<context position="19014" citStr="Huang et al., 2013" startWordPosition="3208" endWordPosition="3211">el computed over every target word: H|| T 1P(sti |ti, ti−1, ti+1, ··· ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: • Forward and backward rule probabilities • 4-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the S</context>
</contexts>
<marker>Huang, Devlin, Zbib, 2013</marker>
<rawString>Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored soft source syntactic constraints for hierarchical machine translation. In EMNLP, pages 556–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<contexts>
<context position="31124" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="5236" endWordPosition="5239">li et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM. Also, their model is recurrent, so it cannot be used in decoding. They obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs. 25.8). Schwenk (2012) predicts an entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). Zou et al. (2013) estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5). Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM. They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7). However, additive results are not presented. The most similar work that we know of is Le et al. (2012). Le’s basic procedure is to re-order the source to match the linear order of the target, and then segment the hypothesis into minimal bilingual phrase pairs. Then, he predicts each target word given the previous bilingual phrases. However, Le’s formulation could only be used in kbest rescoring, since it require</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5733" citStr="Kneser and Ney, 1995" startWordPosition="933" endWordPosition="936">gned word in the middle.1 (3) If ti is unaligned, we inherit its affiliation from the closest aligned word, with preference given to the right.2 An example of the NNJM context model for a Chinese-English parallel sentence is given in Figure 1. For all of our experiments we use n = 4 and m = 11. It is clear that this model is effectively an (n+m)-gram LM, and a 15-gram LM would be 1We arbitrarily round down. 2We have found that the affiliation heuristic is robust to small differences, such as left vs. right preference. far too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996). Fortunately, neural network language models are able to elegantly scale up and take advantage of arbitrarily large context sizes. 2.1 Neural Network Architecture Our neural network architecture is almost identical to the original feed-forward NNLM architecture described in Bengio et al. (2003). The input vector is a 14-word context vector (3 target words, 11 source words), where each word is mapped to a 192-dimensional vector using a shared mapping layer. We use two 512- dimensional hidden layers with tanh activation functions. The output layer is a softm</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pages 181–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>39--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2505" citStr="Le et al., 2012" startWordPosition="389" endWordPosition="392">ls were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k-best rescoring only. Additionally, we present several variations of this model which provide significant additive BLEU gains. We also present a novel technique for training the neural network to be self-normalized, which avoids the costly step of posteriorizing over the entire vocabulary in decoding. When used in conjunction with a pre-computed hidden layer, these techniques speed up NNJM computation by a factor of 10,000x, with only a small red</context>
<context position="9784" citStr="Le et al. (2012)" startWordPosition="1626" endWordPosition="1629">o 0 (i.e., Z(x) = 1) then at decode time we would only have to compute row r of the output layer instead of the whole matrix. While we cannot train a neural network with this guarantee, we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible by augmenting our training objective function: Σ|V | r�=1eU l(x) 2.3 Self-Normalized Neural Network EL = [log(P(xi)) — α(log(Z(xi)) — 0)2] The computational cost of NNLMs is a significant issue in decoding, and this cost is dominated by the output softmax over the entire target vocabulary. Even class-based approaches such as Le et al. (2012) require a 2-20k shortlist vocabulary, and are therefore still quite costly. Here, our goal is to be able to use a fairly large vocabulary without word classes, and to simply avoid computing the entire output layer at decode time.4 To do this, we present the novel technique of self-normalization, where the output layer scores are close to being probabilities without explicitly performing a softmax. Formally, we define the standard softmax log 3We do not divide the gradient by the minibatch size. For those who do, this is equivalent to using an initial learning rate of 10−3 ∗ 128 ≈ 10−1. 4We ar</context>
<context position="25954" citStr="Le et al. (2012)" startWordPosition="4368" endWordPosition="4371">n Arabic, the total gain is +2.6 BLEU, while on Chinese, the gain is +1.3 BLEU. BOLT Test Ar-En Ch-En BLEU BLEU Baseline (w/o RNNLM) 40.2 30.6 Baseline (w/ RNNLM) 41.3 30.9 + S2T/L2R NNJM (Dec) 42.9 31.9 + S2T NNLTM (Dec) 43.2 31.9 + Other NNJMs (Resc) 43.9 32.2 Table 4: Primary results on Arabic-English and Chinese-English BOLT Web Forum. Each row includes the aggregate features from all previous rows. 6.4 Effect of k-best Rescoring Only Table 5 shows performance when our S2T/L2R NNJM is used only in 1000-best rescoring, compared to decoding. The primary purpose of this is as a comparison to Le et al. (2012), whose model can only be used in k-best rescoring. BOLT Test Ar-En Without With RNNLM RNNLM BLEU BLEU Baseline 40.2 41.3 S2T/L2R NNJM (Resc) 41.7 41.6 S2T/L2R NNJM (Dec) 42.8 42.9 Table 5: Comparison of our primary NNJM in decoding vs. 1000-best rescoring. We can see that the rescoring-only NNJM performs very well when used on top of a baseline without an RNNLM (+1.5 BLEU), but the gain on top of the RNNLM is very small (+0.3 BLEU). The gain from the decoding NNJM is large in both cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/ RNNLM). This demonstrates that the full power of the NNJM can only be ha</context>
<context position="31411" citStr="Le et al. (2012)" startWordPosition="5288" endWordPosition="5291"> entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). Zou et al. (2013) estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5). Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM. They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7). However, additive results are not presented. The most similar work that we know of is Le et al. (2012). Le’s basic procedure is to re-order the source to match the linear order of the target, and then segment the hypothesis into minimal bilingual phrase pairs. Then, he predicts each target word given the previous bilingual phrases. However, Le’s formulation could only be used in kbest rescoring, since it requires long-distance reordering and a large target context. Le’s model does obtain an impressive +1.7 BLEU gain on top of a baseline without an NNLM (25.8 vs. 27.5). However, when compared to the strongest baseline which includes an NNLM, Le’s best models (S2T + T2S) only obtain an +0.6 BLEU</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 39– 48, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>L´eon Bottou</author>
<author>Genevieve B Orr</author>
<author>Klaus-Robert M¨uller</author>
</authors>
<title>Efficient backprop.</title>
<date>1998</date>
<booktitle>In Neural networks: Tricks of the trade,</booktitle>
<pages>9--50</pages>
<publisher>Springer.</publisher>
<marker>LeCun, Bottou, Orr, M¨uller, 1998</marker>
<rawString>Yann LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M¨uller. 1998. Efficient backprop. In Neural networks: Tricks of the trade, pages 9–50. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Marino</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a De Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e AR Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>549</pages>
<marker>Marino, Banchs, Crego, De Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B Marino, Rafael E Banchs, Josep M Crego, Adri`a De Gispert, Patrik Lambert, Jos´e AR Fonollosa, and Marta R Costa-Juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527– 549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="30000" citStr="Mikolov et al., 2013" startWordPosition="5053" endWordPosition="5056"> decoding is only —0.01 seconds per source word. In future work we will provide more detailed analysis regarding the usability of the NNJM in a low-latency, high-throughput MT engine. 7 Related Work Although there has been a substantial amount of past work in lexicalized joint models (Marino et al., 2006; Crego and Yvon, 2010), nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy. However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network’s ability to semantically generalize (Mikolov et al., 2013) and learn nonlinear relationships. A number of recent papers have proposed methods for creating neural network translation/joint models, but nearly all of these works have obtained much smaller BLEU improvements than ours. For each related paper, we will briefly con14In our decoder, roughly 95% of NNJM n-gram lookups within the same sentence are duplicates. 1377 trast their methodology with our own and summarize their BLEU improvements using scores taken directly from the cited paper. Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19830" citStr="Och and Ney, 2003" startWordPosition="3344" endWordPosition="3347"> LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 NIST OpenMT12 Results Our NIST system is fully compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality paralle</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Pascanu</author>
<author>Tomas Mikolov</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063.</title>
<date>2012</date>
<contexts>
<context position="8669" citStr="Pascanu et al., 2012" startWordPosition="1424" endWordPosition="1427">s samples, but the absolute ordering does not affect results. 128.3 At every epoch, which we define as 20,000 minibatches, the likelihood of a validation set is computed. If this likelihood is worse than the previous epoch, the learning rate is multiplied by 0.5. The training is run for 40 epochs. The training data ranges from 10-30M words, depending on the condition. We perform a basic weight update with no L2 regularization or momentum. However, we have found it beneficial to clip each weight update to the range of [-0.1, 0.1], to prevent the training from entering degenerate search spaces (Pascanu et al., 2012). Training is performed on a single Tesla K10 GPU, with each epoch (128*20k = 2.6M samples) taking roughly 1100 seconds to run, resulting in a total training time of —12 hours. Decoding is performed on a CPU. likelihood as: eU (x) � log(P(x)) = log Z(x) = Ur(x) — log(Z(x)) Z(x) = where x is the sample, U is the raw output layer scores, r is the output layer row corresponding to the observed target word, and Z(x) is the softmax normalizer. If we could guarantee that log(Z(x)) were always equal to 0 (i.e., Z(x) = 1) then at decode time we would only have to compute row r of the output layer inst</context>
</contexts>
<marker>Pascanu, Mikolov, Bengio, 2012</marker>
<rawString>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012. On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Ann Irvine</author>
<author>Daniel Marcu</author>
</authors>
<title>Feature-rich language-independent syntax-based alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>497--507</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19860" citStr="Riesa et al., 2011" startWordPosition="3350" endWordPosition="3353">k language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 NIST OpenMT12 Results Our NIST system is fully compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M</context>
</contexts>
<marker>Riesa, Irvine, Marcu, 2011</marker>
<rawString>Jason Riesa, Ann Irvine, and Daniel Marcu. 2011. Feature-rich language-independent syntax-based alignment for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 497–507, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="5770" citStr="Rosenfeld, 1996" startWordPosition="940" endWordPosition="941">ligned, we inherit its affiliation from the closest aligned word, with preference given to the right.2 An example of the NNJM context model for a Chinese-English parallel sentence is given in Figure 1. For all of our experiments we use n = 4 and m = 11. It is clear that this model is effectively an (n+m)-gram LM, and a 15-gram LM would be 1We arbitrarily round down. 2We have found that the affiliation heuristic is robust to small differences, such as left vs. right preference. far too sparse for standard probability models such as Kneser-Ney back-off (Kneser and Ney, 1995) or Maximum Entropy (Rosenfeld, 1996). Fortunately, neural network language models are able to elegantly scale up and take advantage of arbitrarily large context sizes. 2.1 Neural Network Architecture Our neural network architecture is almost identical to the original feed-forward NNLM architecture described in Bengio et al. (2003). The input vector is a 14-word context vector (3 target words, 11 source words), where each word is mapped to a 192-dimensional vector using a shared mapping layer. We use two 512- dimensional hidden layers with tanh activation functions. The output layer is a softmax over the entire output vocabulary.</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Rich Schwartz</author>
</authors>
<title>BBN system description for WMT10 system combination task. In WMT/MetricsMATR,</title>
<date>2010</date>
<pages>321--326</pages>
<contexts>
<context position="20053" citStr="Rosti et al., 2010" startWordPosition="3378" endWordPosition="3381"> the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexicon-based approach. For word alignment, we align all of the training data with both GIZA++ (Och and Ney, 2003) and NILE (Riesa et al., 2011), and concatenate the corpora together for rule extraction. For MT feature weight optimization, we use iterative k-best optimization with an ExpectedBLEU objective function (Rosti et al., 2010). 6 Experimental Results We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features. 6.1 NIST OpenMT12 Results Our NIST system is fully compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese.10 The Kneser-Ney LM is trained on 5B words of data from English GigaWord. For test, we use the “Arabic-To-English Original Progress Test” (1378 segments) and “Chinese-to-Eng</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2010</marker>
<rawString>Antti Rosti, Bing Zhang, Spyros Matsoukas, and Rich Schwartz. 2010. BBN system description for WMT10 system combination task. In WMT/MetricsMATR, pages 321–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous-space language models for statistical machine translation.</title>
<date>2010</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>93--137</pages>
<contexts>
<context position="2051" citStr="Schwenk, 2010" startWordPosition="316" endWordPosition="317">program (Approved for Public Release, Distribution Unlimited). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. 1 Introduction In recent years, neural network models have become increasingly popular in NLP. Initially, these models were primarily used to create n-gram neural network language models (NNLMs) for speech recognition and machine translation (Bengio et al., 2003; Schwenk, 2010). They have since been extended to translation modeling, parsing, and many other NLP tasks. In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature. Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling (Le et al., 2012), our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements th</context>
</contexts>
<marker>Schwenk, 2010</marker>
<rawString>Holger Schwenk. 2010. Continuous-space language models for statistical machine translation. Prague Bull. Math. Linguistics, 93:137–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1071--1080</pages>
<contexts>
<context position="30783" citStr="Schwenk (2012)" startWordPosition="5184" endWordPosition="5185">ave obtained much smaller BLEU improvements than ours. For each related paper, we will briefly con14In our decoder, roughly 95% of NNJM n-gram lookups within the same sentence are duplicates. 1377 trast their methodology with our own and summarize their BLEU improvements using scores taken directly from the cited paper. Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM. Also, their model is recurrent, so it cannot be used in decoding. They obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs. 25.8). Schwenk (2012) predicts an entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). Zou et al. (2013) estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5). Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM. They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7). However, additive results are not presented. The most similar work that we</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLING (Posters), pages 1071–1080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>String-to-dependency statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="18649" citStr="Shen et al., 2010" startWordPosition="3149" endWordPosition="3152">ds, it is treated as a single concatenated token. Formally, the probability model is: S Hi=1P(tsi |si, si−1, si+1, ··· ) This model is trained and evaluated like our NNJM. It is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word. In rescoring, we also use a T2S NNLTM model computed over every target word: H|| T 1P(sti |ti, ti−1, ti+1, ··· ) 5 MT System In this section, we describe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: • Forward and backward rule probabilities • 4-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network langua</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Computational Linguistics, 36(4):649–671, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>857--866</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19122" citStr="Snover et al., 2008" startWordPosition="3227" endWordPosition="3230">cribe the MT system used in our experiments. 5.1 MT Decoder We use a state-of-the-art string-to-dependency hierarchical decoder (Shen et al., 2010). Our baseline decoder contains a large and powerful set of features, which include: • Forward and backward rule probabilities • 4-gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008) 1374 We also perform 1000-best rescoring with the following features: • 5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010) Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM. 5.2 Training and Optimization For Arabic word tokenization, we use the MADAARZ tokenizer (Habash et al., 2013) for the BOLT condition, and the Sakhr9 tokenizer for the NIST condition. For Chinese tokenization, we use a simple longest-match-first lexico</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 857–866, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Tanaka</author>
<author>Yasuhara Toru</author>
<author>Jun-ya Yamamoto</author>
<author>Mikio Norimatsu</author>
</authors>
<title>An efficient language model using double-array structures.</title>
<date>2013</date>
<contexts>
<context position="14471" citStr="Tanaka et al., 2013" startWordPosition="2418" endWordPosition="2421">e 2: Speed of the neural network computation on a single CPU thread. “lookups/sec” is the number of unique n-gram probabilities that can be computed per second. “sec/word” is the amortized cost of unique NNJM lookups in decoding, per source word. Table 2 shows the speed of self-normalization and pre-computation for the NNJM. The decoding cost is based on a measurement of ∼1200 unique NNJM lookups per source word for our ArabicEnglish system.8 By combining self-normalization and precomputation, we can achieve a speed of 1.4M lookups/second, which is on par with fast backoff LM implementations (Tanaka et al., 2013). We demonstrate in Section 6.6 that using the selfnormalized/pre-computed NNJM results in only a very small BLEU degradation compared to the standard NNJM. 3 Decoding with the NNJM Because our NNJM is fundamentally an n-gram NNLM with additional source context, it can easily be integrated into any SMT decoder. In this section, we describe the considerations that must be taken when integrating the NNJM into a hierarchical decoder. 6tanh() is implemented using a lookup table. 73500≈ 5 × 512 + 2 × 513; 2.8M ≈ 2 × 2689 × 512 + 2 × 513; 35M ≈ 2 × 2689 × 512 + 2 × 513 × 32000. For the sake of a fai</context>
</contexts>
<marker>Tanaka, Toru, Yamamoto, Norimatsu, 2013</marker>
<rawString>Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, and Mikio Norimatsu. 2013. An efficient language model using double-array structures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="11022" citStr="Vaswani et al. (2013)" startWordPosition="1843" endWordPosition="1846">ith speeding up training time, as we already find GPU training time to be adequate. i E= [log(P(xi)) — α log2(Z(xi))] i In this case, the output layer bias weights are initialized to log(1/|V |), so that the initial network is self-normalized. At decode time, we simply use Ur(x) as the feature score, rather than log(P(x)). For our NNJM architecture, selfnormalization increases the lookup speed during decoding by a factor of —15x. Table 1 shows the neural network training results with various values of the free parameter α. In all subsequent MT experiments, we use α = 10−1. We should note that Vaswani et al. (2013) implements a method called Noise Contrastive Estimation (NCE) that is also used to train selfnormalized NNLMs. Although NCE results in faster training time, it has the downside that there 1372 Arabic BOLT Val α log(P(x)) |log(Z(x))| 0 −1.82 5.02 10−2 −1.81 1.35 10−1 −1.83 0.68 1 −1.91 0.28 Table 1: Comparison of neural network likelihood for various α values. log(P(x)) is the average log-likelihood on a held-out set. |log(Z(x)) |is the mean error in log-likelihood when using Ur(x) directly instead of the true softmax probability log(P(x)). Note that α = 0 is equivalent to the standard neural </context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="30925" citStr="Zou et al. (2013)" startWordPosition="5209" endWordPosition="5212">ram lookups within the same sentence are duplicates. 1377 trast their methodology with our own and summarize their BLEU improvements using scores taken directly from the cited paper. Auli et al. (2013) use a fixed continuous-space source representation, obtained from LDA (Blei et al., 2003) or a source-only NNLM. Also, their model is recurrent, so it cannot be used in decoding. They obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs. 25.8). Schwenk (2012) predicts an entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). Zou et al. (2013) estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5). Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM. They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7). However, additive results are not presented. The most similar work that we know of is Le et al. (2012). Le’s basic procedure is to re-order the source to match the linear order of the target, and then segment the hyp</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>