<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000097">
<title confidence="0.915193">
The Shared Corpora Working Group Report
</title>
<note confidence="0.73428">
Adam Meyers Nancy Ide Ludovic Denoyer Yusuke Shinyama
New York Vassar College University of Paris New York
University Poughkeepsie, NY Paris, France University
New York, NY ide at cs.vassar.edu ludovic.denoyer New York, NY
meyers at lip6.fr yusuke
at cs.nyu.edu at cs.nyu.edu
</note>
<sectionHeader confidence="0.99366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995765625">
We seek to identify a limited amount of rep-
resentative corpora, suitable for annotation
by the computational linguistics annotation
community. Our hope is that a wide vari-
ety of annotation will be undertaken on the
same corpora, which would facilitate: (1)
the comparison of annotation schemes; (2)
the merging of information represented by
various annotation schemes; (3) the emer-
gence of NLP systems that use informa-
tion in multiple annotation schemes; and (4)
the adoption of various types of best prac-
tice in corpus annotation. Such best prac-
tices would include: (a) clearer demarca-
tion of phenomena being annotated; (b) the
use of particular test corpora to determine
whether a particular annotation task can fea-
sibly achieve good agreement scores; (c)
The use of underlying models for represent-
ing annotation content that facilitate merg-
ing, comparison, and analysis; and (d) To
the extent possible, the use of common an-
notation categories or a mapping among cat-
egories for the same phenomenon used by
different annotation groups.
This study will focus on the problem of
identifying such corpora as well as the suit-
ability of two candidate corpora: the Open
portion of the American National Corpus
(Ide and Macleod, 2001; Ide and Suder-
man, 2004) and the “Controversial” portions
of the WikipediaXML corpus (Denoyer and
</bodyText>
<sectionHeader confidence="0.7208445" genericHeader="keywords">
Gallinari, 2006).
1 Introduction
</sectionHeader>
<bodyText confidence="0.994845333333333">
This working group seeks to identify a limited
amount of representative corpora, suitable for an-
notation by the computational linguistics annotation
community. Our hope is that a wide variety of anno-
tation will be undertaken on the same corpora, which
would facilitate:
</bodyText>
<listItem confidence="0.917650857142857">
1. The comparison of annotation schemes
2. The merging of information represented by var-
ious annotation schemes
3. The emergence of NLP systems that use infor-
mation in multiple annotation schemes; and
4. The adoption of various types of best practice
in corpus annotation, including:
(a) Clearer demarcation of the phenomena be-
ing annotated. Thus if predicate argu-
ment structure annotation adequately han-
dles relative pronouns, a new project that
is annotating coreference is less likely to
include relative pronouns in their annota-
tion; and
(b) The use of particular test corpora to de-
termine whether a particular annotation
task can feasibly achieve good agreement
scores.
(c) The use of underlying models for repre-
senting annotation content that facilitate
merging, comparison, and analysis.
</listItem>
<page confidence="0.977241">
184
</page>
<note confidence="0.7032055">
Proceedings of the Linguistic Annotation Workshop, pages 184–190,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.995947333333333">
(d) To the extent possible, the use of common
annotation categories or a mapping among
categories for the same phenomenon used
by different annotation groups.
In selecting shared corpora, we believe that the
following issues must be taken into consideration:
</bodyText>
<listItem confidence="0.979674176470588">
1. The diversity of genres, lexical items and lin-
guistic phenomena – this will ensure that the
corpora will be useful to many different types
of annotation efforts. Furthermore, systems us-
ing these corpora and annotation as data will
be capable of handling larger and more varied
corpora.
2. The availability of the same or similar corpora
in a wide variety of languages;
3. The availability of corpora in a standard format
that can be easily processed – there should be
mechanisms in place to maintain the availabil-
ity of corpora in this format in the future;
4. The ease in which the corpora can be obtained
by anyone who wants to process or annotate
them – corpora with free licenses or that are in
the public domain are preferred
</listItem>
<bodyText confidence="0.855384473684211">
5. The degree with which the corpora is represen-
tative of text to be processed – this criterion can
be met if the corpora is diverse (1 above) and/or
if more corpora of the same kind is available for
processing.
We have selected the following corpora for con-
sideration:)
1. The OANC: the Open sections of the ANC cor-
pus. These are the sections of the American
National Corpus subject to the opened license,
allowing them to be freely distributed. The full
Open ANC (Version 2.0) contains about 14.5
megawords of American English and covers a
variety of genres as indicated by the full path-
names taken from the ANC distribution (where
a final 1 or 2 indicates which DVD the directory
originates from):
&apos;These corpora can be downloaded from:
http://nlp.cs.nyu.edu/wiki/corpuswg/SharedCorpora
</bodyText>
<listItem confidence="0.94167846875">
• spoken/telephone/switchboard
• written 1/fiction/eggan
• written 1/journal/slate
• written 1/letters/icic
• written 2/non-fiction/OUP
• written 2/technical/biomed
• written 2/travel guides/berlitz1
• written 2/travel guides/berlitz2
• written 1/journal/verbatim
• spoken/face-to-face/charlotte
• written 2/technical/911report
• written 2/technical/plos
• written 2/technical/government
2. The Controversial-Wikipedia-Corpus, a section
of the Wikipedia XML corpus. WikipediaXML
is a corpus derived from Wikipedia, convert-
ing Wikipedia into an XML corpus suitable
for NLP processing. This corpus was selected
from:
• Those articles cited as controversial
according to the November 28, 2006
version of the following Wikipedia page:
http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues
• The talk pages corresponding to these ar-
ticles where Wikipedia users and the com-
munity debate aspects of articles. These
debates may be about content or editorial
considerations.
• Articles in Japanese that are linked to
the English pages (and the associated talk
pages) are also part of our corpus.
</listItem>
<sectionHeader confidence="0.774043" genericHeader="method">
2 American National Corpus
</sectionHeader>
<bodyText confidence="0.997753545454545">
The American National Corpus (ANC) project (Ide
and Macleod, 2001; Ide and Suderman, 2004) has
released over 20 million words of spoken and writ-
ten American English, available from the Linguis-
tic Data Consortium. The ANC 2nd release con-
sists of fiction, non-fiction, newspapers, technical
reports, magazine and journal articles, a substan-
tial amount of spoken data, data from blogs and
other unedited web sources, travel guides, techni-
cal manuals, and other genres. All texts are an-
notated for sentence boundaries; token boundaries,
</bodyText>
<page confidence="0.99467">
185
</page>
<bodyText confidence="0.999907451612903">
lemma, and part of speech produced by two differ-
ent taggers ; and noun and verb chunks. A sub-
corpus of 10 million words reflecting the genre dis-
tribution of the full ANC is currently being hand-
validated for word and sentence boundaries, POS,
and noun and verb chunks. For a complete descrip-
tion of the ANC 2nd release and its contents, see
http://AmericanNationalCorpus.org.
Approximately 65 percent of the ANC data is dis-
tributed under an open license, which allows use and
re-distribution of the data without restriction. The
remainder of the corpus is distributed under a re-
stricted license that disallows re-distribution or use
of the data for commercial purposes for five years
after its release date, unless the user is a member of
the ANC Consortium. After five years, the data in
the restricted portions of the corpus are covered by
the open license.
ANC annotations are distributed as stand-off doc-
uments representing a set of graphs over the primary
data, thus allowing for layering of annotations and
inclusion of multiple annotations of the same type.
Because most existing tools for corpus access and
manipulation do not handle stand-off annotations,
we have developed an easy-to-use tool and user in-
terface to merge the user’s choice of stand-off anno-
tations with the primary data to form a single docu-
ment in any of several XML and non-XML formats,
which is distributed with the corpus. The ANC ar-
chitecture and format is described fully in (Ide and
Suderman, 2006).
</bodyText>
<subsectionHeader confidence="0.980148">
2.1 The ULA Subcorpus
</subsectionHeader>
<bodyText confidence="0.999446142857143">
The Unified Linguistic Annotation (ULA) project
has selected a 40,000 word subcorpus of the Open
ANC for annotation with several different annota-
tion schemes including: the Penn Treebank, Prop-
Bank, NomBank, the Penn Discourse Treebank,
TimeML and Opinion Annotation.2 This initial sub-
corpus can be broken down as follows:
</bodyText>
<listItem confidence="0.899356">
• Spoken Language
– charlotte: 5K words
– switchboard: 5K words
• letters: 10K words
2Other corpora being annotated by the ULA project include
sections of the Brown corpus and LDC parallel corpora.
• Slate (Journal): 5K words
• Travel guides: 5K words
• 911report: 5K words
• OUP books (Kaufman): 5K words
</listItem>
<bodyText confidence="0.999939761904762">
As the ULA project progresses, the participants
intend to expand the corpora annotated to include a
larger subsection of the OANC. They believe that the
diversity of this corpus make it a reasonable testbed
for tuning annotation schemes for diverse modali-
ties. The Travel guides and some of the slate arti-
cles have already been annotated by the FrameNet
project. Thus the inclusion of these documents fur-
thered the goal of producing a multiply annotated
corpus by one additional project.
It is the recommendation of this working group
that: (1) other groups annotate these same subcor-
pora; and (2) other groups choose additional corpora
from the OANC to annotate and publicly announce
which subsections they choose. We would be happy
to put all such subsections on our website for down-
load. The basic idea is to build up a consensus of
what should be mutually annotated, in part, based
on what groups choose to annotate and to try to get
annotation projects to gravitate toward multiply an-
notated, freely available corpora.
</bodyText>
<sectionHeader confidence="0.996039" genericHeader="method">
3 The WikipediaXML Corpus
</sectionHeader>
<subsectionHeader confidence="0.997487">
3.1 Why Wikipedia?
</subsectionHeader>
<bodyText confidence="0.9961516">
The Wikipedia corpus consists of articles in a wide
range of topics written in different genres and
mainly (a) main pages are encyclopedia style arti-
cles; and (b) talk pages are discussions about main
pages they are linked to. The topics of these discus-
sions range from editing contents to disagreements
about content. Although Wikipedia texts are mostly
limited to these two genres, we believe that it is well
suited as training data for natural language process-
ing because:
</bodyText>
<listItem confidence="0.979488">
1. they are lexically diverse (e.g., providing a lot
of lexical information for statistical systems);
2. the textual information is well structured
3. Wikipedia is a large and growing corpus
</listItem>
<page confidence="0.92216">
186
</page>
<listItem confidence="0.494173">
4. the articles are multilingual (cf. section 3.4)
5. and the corpus has various other properties that
many researchers feel would be interesting to
exploit.
</listItem>
<bodyText confidence="0.999923677419355">
To date research in Computational Linguistics us-
ing Wikipedia includes: Automatic derivation of
taxonomy information (Strube and Ponzetto, 2006;
Suchanek et al., 2007; Zesch and Gurevych, 2007;
Ponzetto, 2007); automatic recognition of pairs of
similar sentences in two languages (Adafre and de
Rijke, 2006); corpus mining (R¨udiger Gleim and
Alexander Mehler and Matthias Dehmer, 2007),
Named Entity Recognition (Toral and noz, 2007;
Bunescu and Pasc¸a, 2007) and relation extraction
(Nguyen et al., 2007). In addition several shared
tasks have been set up using Wikipedia as the tar-
get corpus including question answering (cf. (D.
Ahn and V. Jijkoun and G. Mishne and K. M¨uller
and M. de Rijke and S. Schlobach, 2004) and
http://ilps.science.uva.nl/WiQA/); and information
retrieval (Fuhr et al., 2006). Some other interest-
ing properties of Wikipedia that have yet to be ex-
plored to our knowledge include: (1) Most main ar-
ticles have talk pages which discuss them – perhaps
this relation can be exploited by systems which try
to detect discussions about topics, e.g., searches for
discussions about current events topics; (2) There
are various meta tags, many of which are not in-
cluded in the WikipediaXML (see below), but nev-
ertheless are retrievable from the original HTML
files. Some of these may be useful for various ap-
plications. For example, the levels of disputabil-
ity of the content of the main articles is annotated
(cf. http://en.wikipedia.org/wiki/Wikipedia: Tem-
plate messages/Disputes ).
</bodyText>
<subsectionHeader confidence="0.999576">
3.2 Why WikipediaXML?
</subsectionHeader>
<bodyText confidence="0.9996894">
WikipediaXML (Denoyer and Gallinari, 2006) is an
XML version of Wikipedia data, originally designed
for Information Retrieval tasks such as INEX (Fuhr
et al., 2006) and the XML Document Mining Chal-
lenge (Denoyer and P. Gallinari, 2006). Wikipedi-
aXML has become a standard machine readable
form for Wikipedia, suitable for most Computa-
tional Linguistics purposes. It makes it easy to
identify and read in the text portions of the doc-
ument, removing or altering html and wiki code
that is difficult to process in a standard way. The
WikipediaXML standard has (so far) been used to
process Wikipedia documents written in English,
German, French, Dutch, Spanish, Chinese, Arabic
and Japanese.
</bodyText>
<subsectionHeader confidence="0.995825">
3.3 The Controversial Wikipedia Corpus
</subsectionHeader>
<bodyText confidence="0.9985585">
The English Wikipedia corpus is quite large (about
800K articles and growing). Frozen versions of
the corpus are periodically available for download.
We selected a 5 million word subcorpus which
we believed would be good for a wide variety
of annotation schemes. In particular, we chose
articles listed as being controversial (in the En-
glish speaking world) according to the November
28, 2006 version of the following Wikipedia
page: http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues. We believed that
controversial articles would be more likely than
randomly selected articles to: (1) include interesting
discourse phenomena and emotive language; and
(2) have interesting “talk” pages (indeed, some of
Wikipedia pages have no associated talk pages).
</bodyText>
<subsectionHeader confidence="0.982079">
3.4 The Multi-linguality of Wikipedia
</subsectionHeader>
<bodyText confidence="0.99992052173913">
One of the main good points of Wikipedia is the fact
that it is a very large multilingual resource. This
provides several advantages over single-language
corpora, perhaps the clearest such advantage being
the availability of same-genre/same-format text for
many languages. Although, Wikipedia in languages
other than English do not approach 800K articles in
size, there are currently at least 14 languages with
over 100K entries.
It should be clear however, that it is definitely not
a parallel corpus. Although pages are sometimes
translated in their entirety, this is the exception, not
the rule. Pages can be partially translated or summa-
rized into the target language. Individually written
pages can be linked after they are created if it is be-
lieved that they are about the same topic. Also, ini-
tially parallel pages can be edited in both languages,
causing them to diverge. We therefore decided to
do a small small pilot study to attempt to charac-
terize the degree of similarity between English arti-
cles in Wikipedia and articles written in other lan-
guages that have been linked. There are 476 En-
glish Wikipedia articles in the Controversial corpus
</bodyText>
<page confidence="0.995906">
187
</page>
<table confidence="0.929387666666667">
Classification Frequency
Totally Different 2
Same General Topic 3
Overlapping Topics 11
Same Topics 33
Parallel 1
</table>
<bodyText confidence="0.950070235294118">
and 384 associated “talk” pages. There are approxi-
mately 10,000 articles of various languages that are
linked to the English articles. We asked some En-
glish/Japanese bilingual speakers to evaluate the de-
gree of similarity of as many of the the 305 Japanese
articles that were linked to English controversial ar-
ticles. As of this date, 50 articles were evaluated
with the results summarized as table 3.4.3 These
preliminary results suggest the following:
• Languate-linked Wikipedia would usually be
classified as “comparable” corpora as 34 (68%)
of the articles were classified as covering the
same topics or being parallel.
• It may be possible to extract a parallel corpus
for a given pair of languages from Wikipedia.
If the above sample is representative, approxi-
mately 2% of the articles are parallel. (While
the existance of one parallel article does not
provide statistically significant evidence that
2% of Wikipedia is parallel, the article’s ex-
istance is still significant.) Furthermore, addi-
tional parallel sentences may be extracted from
some of the other comparable articles using
techniques along the lines of (Adafre and de Ri-
jke, 2006).
Obviously, a more detailed study would be neces-
sary to gain a more complete understanding of how
language-linked articles are related in Wikipedia.4
Such a study would include characterizations of all
linked articles for several languages. This study
could lead to some practical applications, e.g., (1)
the creation of parallel subcorpora for a number of
languages; (2) the selection of an English monolin-
gual subcorpus consisting of articles, each of which
</bodyText>
<footnote confidence="0.950767333333333">
3According to www.wikipedia.org there are currently over
350K Japanese articles.
4Long Wikipedia articles may be split into multiple articles.
This can result in N to 1, or even N to N, matches between
language-linked articles if a topic is split in one language, but
not in another.
</footnote>
<listItem confidence="0.554692">
is parallel to some article in some other language;
etc.; (3) A compilation of parallel sentences ex-
</listItem>
<bodyText confidence="0.998068590909091">
tracted from comparable articles. While parallel
subcorpora are of maximal utility, finding parallel
sentences could still be extremely useful. (Adafre
and de Rijke, 2006) reports one attempt to automat-
ically select parallel Dutch/English sentences from
language-linked Wikipedia articles with an accuracy
of approximately 45%. Even if higher accuracy can-
not be achieved, this still suggests that it is possible
to create a parallel corpus (of isolated sentences) us-
ing a combination of automatic and manual means.
A human translator would have to go through pro-
posed parallel sentences and eliminate about one
half of them, but would not have to do any man-
ual translation. Selection of corpora for annotation
purposes depends on a number of factors including:
the type of annotation (e.g., a corpus of isolated sen-
tences would not be appropriate for discourse anno-
tation); and possibly an application the annotation
is tuned for (e.g., Machine Translation, Information
Extraction, etc.)
It should be noted that the corpus was chosen for
the controversialness of its articles in the English-
speaking community. It should, however, not be ex-
pected that the same articles will be controversial
in other languages. More generally, the language-
linked Wikipedia articles may have different cultural
contexts depending on the language they are written
in. This is an additional feature that we could test
in a wider study. Furthermore, English pages are
somewhat special because they’re considered as the
common platform and expected to be neutral to any
country. But other lanauages somewhat reflects the
view of each country where the language is spoken.
Indeed, some EN articles are labeled as USA-centric
(cf. http://en.wikipedia.org/wiki/Category:USA-
centric).
Finally, our choice of a corpus based on contro-
versy may have not been the most efficient choice
if our goal had been specifically to find parallel cor-
pora. Just as choosing corpora of articles that are
controversial (in the English-speaking world) may
have helped finding articles interesting to annotate
it is possible that some other choice, e.g., techni-
cal articles, may have helped select articles likely
</bodyText>
<page confidence="0.996885">
188
</page>
<bodyText confidence="0.999368">
to be translated in full5 Thus further study may be
required to choose the right Wikipedia balance for a
set of priorities agreed upon by the annotation com-
munity.
</bodyText>
<sectionHeader confidence="0.984008" genericHeader="method">
4 Legal Issues
</sectionHeader>
<bodyText confidence="0.985879025641026">
The American National Corpus has taken great pains
to establish that the open subset of the corpus is
freely usable by the community. The open license6
makes it clear that these corpora can be used for any
reason and are freely distributable.
In contrast, some aspects of the licensing agree-
ment of corpora derived from Wikipedia are unclear.
Wikipedia is governed by the GNU Free Document
License which includes a provision that “derived
works” are subject to this license as well. While
most academic researchers would be uneffected by
this provision, the effect of this provision is unclear
with respect to commercial products.
Under one view, a machine translation system that
uses a statistical model trained on Wikipedia corpora
is not derived from these corpora. However, on an-
other view it is derived. We contacted Wikipedia
staff by letter asking for clarification on this issue
and received the following response from Michelle
Kinney on behalf of Wikipedia information team:
Wikipedia does not offer legal advice,
and therefore cannot help you decide how
the GNU Free Documentation License
(GFDL) or any other free license applies
to your particular situation. Please con-
tact a local bar association, law society or
similar association of jurists in your legal
jurisdiction to obtain a referral to a com-
petent legal professional.
You may also wish to review the full text
of the GFDL yourself:
http://en.wikipedia.org/wiki/Wikipedia:
Text of the GNU Free Documentation License
5Informally, we observe that linked Japanese/English pairs
of articles about abstract topics (e.g., Adultery, Agnosticsism,
Antisemitism, Capitalism, Censorship, Catholicism) are less
likely to contain parallel sentences than articles about specific
events or people (e.g., Adolf Hitler, Barbara Streisand, The Los
Angeles Riots, etc.)
</bodyText>
<footnote confidence="0.6992975">
6http://projects.ldc.upenn.edu/ANC/ANC SecondRelease
EndUserLicense Open.htm
</footnote>
<bodyText confidence="0.9998025">
While some candidate corpora are completely in
the public domain, e.g., political speeches and very
old documents, many candidate corpora are under
the GFDL or similar “copyleft” licenses. These in-
clude other licenses by the GNU organization and
several Creative Commons licenses. It is simply un-
clear how copyleft licenses should be applied to cor-
pora used as data in computational linguistics and
we believe that this is an important legal question
for the Computational Linguistics community. In
addition to Wikipedia, this issue effects a wide vari-
ety of corpora (e.g., other wiki corpora, some of the
corpora being developed by the American National
Corpus, etc.).
However, getting such legal opinions is expensive
and has to be done carefully. Hypothetically, sup-
pose NYU’s legal department wrote an opinion let-
ter stating that products that were not corpora them-
selves were not to be considered derived works for
purposes of some list of copyleft licensing agree-
ments. Furthermore, let’s suppose that several anno-
tation projects relied on this opinion and produced
millions of dollars worth of annotation for one such
corpus. Large corporations still might not use these
corpora unless their own legal departments agreed
with NYU’s opinion. For the annotation community,
this could mean that certain annotation would only
be used by academics and not by industry, and most
annotation researchers would not be happy with this
outcome. It therefore may be worth some effort
on the part of whole NLP community to seek some
clear determinations on this issue.
</bodyText>
<sectionHeader confidence="0.98437" genericHeader="conclusions">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999755153846154">
The working group selected two freely distributable
corpora for purposes of annotation. Our goal was to
choose texts for annotation by multiple annotation
research groups and describe the process and the pit-
falls involved in selecting those texts. We, further-
more, aimed to establish a protocol for sharing texts,
so that the same texts are annotated with multiple
annotation schemes. This protocol cannot be setup
carte blanche by this group of researchers. Rather,
we believe that our report in combination with the
discussion at the upcoming meeting of the Lingus-
tic Annotation Workshop will provide the jumpstart
necessary for such a protocol to be put in place.
</bodyText>
<page confidence="0.998624">
189
</page>
<sectionHeader confidence="0.995827" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998641736842105">
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-
ing Similar Sentences across Multiple Languages in
Wikipedia. In EACL 2006 Workshop: Wikis and blogs
and other dynamic text source, Trento, Italy.
Razvan Bunescu and Marius Pasc¸a. 2007. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. In Proc. of NAACL/HLT 2007.
D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and
M. de Rijke and S. Schlobach. 2004. Using Wikipedia
at the TREC QA Track. In Proc. TREC 2004.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
L. Denoyer and A. Vercoustre P. Gallinari. 2006. Report
on the XML Mining Track at INEX 2005 and INEX
2006 : Categorization and Clustering of XML Docu-
ments. In Advances in XML Information Retrieval and
Evaluation: Fifthth Workshop of the INitiative for the
Evaluation of XML Retrieval (INEX’06).
N. Fuhr, M. Lalmas, and S. Malik. 2006. Advances in
XML Information Retrieval and Evaluation. In 5th In-
ternational Workshop of the Initiative for the Evalua-
tion of XML Retrieval, INEX 2006.
N. Ide and C. Macleod. 2001. The american national
corpus: A standardized resource of american english.
In Proceedings of Corpus Linguistics 2001, Lancaster,
UK.
N. Ide and K. Suderman. 2004. The american national
corpus first release. In Proceedings of LREC 2004,
pages 1681–1684, Lisbon, Portugal.
N. Ide and K. Suderman. 2006. Integrating linguistic re-
sources: The american national corpus model. In Pro-
ceedings of the 6th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
D. P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Sub-
tree Mining for Relation Extraction from Wikipedia.
In Proc. of NAACL/HLT 2007.
Simone Paolo Ponzetto. 2007. Creating a Knowledge
Base From a Collaboratively Generated Encyclopedia.
In Proc. of NAACL/HLT 2007.
R¨udiger Gleim and Alexander Mehler and Matthias
Dehmer. 2007. Web Corpus Mining by instance of
Wikipedia. In Proc. 2nd Web as Corpus Workshop at
EACL 2006.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia. In Proc.
of AAAI-06, pages 1419–1424.
F. M. Suchanek, G. Kasneci, and G.Weikum. 2007.
YAGO: A core of semantic knowledge. In Proc. of
WWW-07.
Antonio Toral and Rafael Mu noz. 2007. A proposal to
automatically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. In Proc. of
NAACL/HLT 2007.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of
the Wikipedia Category Graph for NLP Applications.
In Proc of NAACL-HLT 2007 Workshop: TextGraphs-
2.
</reference>
<page confidence="0.997783">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.049365">
<title confidence="0.9988">The Shared Corpora Working Group Report</title>
<author confidence="0.79245">Adam Meyers Nancy Ide Ludovic Denoyer Yusuke Shinyama New York Vassar College University of Paris New York</author>
<affiliation confidence="0.98954">University Poughkeepsie, NY Paris, France University</affiliation>
<address confidence="0.913454">New York, NY ide at cs.vassar.edu ludovic.denoyer New York, NY</address>
<note confidence="0.575811">meyers at lip6.fr yusuke</note>
<email confidence="0.648989">atcs.nyu.eduatcs.nyu.edu</email>
<abstract confidence="0.998814172413793">We seek to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: (1) the comparison of annotation schemes; (2) the merging of information represented by various annotation schemes; (3) the emergence of NLP systems that use information in multiple annotation schemes; and (4) the adoption of various types of best practice in corpus annotation. Such best practices would include: (a) clearer demarcation of phenomena being annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open</abstract>
<note confidence="0.5054108">portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sisay Fissaha Adafre</author>
<author>Maarten de Rijke</author>
</authors>
<title>Finding Similar Sentences across Multiple Languages in Wikipedia.</title>
<date>2006</date>
<booktitle>In EACL</booktitle>
<location>Trento, Italy.</location>
<marker>Adafre, de Rijke, 2006</marker>
<rawString>Sisay Fissaha Adafre and Maarten de Rijke. 2006. Finding Similar Sentences across Multiple Languages in Wikipedia. In EACL 2006 Workshop: Wikis and blogs and other dynamic text source, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pasc¸a</author>
</authors>
<title>Using Encyclopedic Knowledge for Named Entity Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLT</booktitle>
<marker>Bunescu, Pasc¸a, 2007</marker>
<rawString>Razvan Bunescu and Marius Pasc¸a. 2007. Using Encyclopedic Knowledge for Named Entity Disambiguation. In Proc. of NAACL/HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ahn</author>
<author>V Jijkoun</author>
<author>G Mishne</author>
<author>K M¨uller</author>
<author>M de Rijke</author>
<author>S Schlobach</author>
</authors>
<title>Using Wikipedia at the TREC QA Track.</title>
<date>2004</date>
<booktitle>In Proc. TREC</booktitle>
<marker>Ahn, Jijkoun, Mishne, M¨uller, de Rijke, Schlobach, 2004</marker>
<rawString>D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach. 2004. Using Wikipedia at the TREC QA Track. In Proc. TREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Denoyer</author>
<author>Patrick Gallinari</author>
</authors>
<title>The Wikipedia XML Corpus.</title>
<date>2006</date>
<publisher>SIGIR Forum.</publisher>
<contexts>
<context position="1666" citStr="Denoyer and Gallinari, 2006" startWordPosition="260" endWordPosition="263"> feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006). 1 Introduction This working group seeks to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: 1. The comparison of annotation schemes 2. The merging of information represented by various annotation schemes 3. The emergence of NLP systems that use information in multiple annotation schemes; and 4. The adoption of various types of best practice in corpus annotation, including: (a) Clearer demarcation of th</context>
<context position="11933" citStr="Denoyer and Gallinari, 2006" startWordPosition="1889" endWordPosition="1892">k pages which discuss them – perhaps this relation can be exploited by systems which try to detect discussions about topics, e.g., searches for discussions about current events topics; (2) There are various meta tags, many of which are not included in the WikipediaXML (see below), but nevertheless are retrievable from the original HTML files. Some of these may be useful for various applications. For example, the levels of disputability of the content of the main articles is annotated (cf. http://en.wikipedia.org/wiki/Wikipedia: Template messages/Disputes ). 3.2 Why WikipediaXML? WikipediaXML (Denoyer and Gallinari, 2006) is an XML version of Wikipedia data, originally designed for Information Retrieval tasks such as INEX (Fuhr et al., 2006) and the XML Document Mining Challenge (Denoyer and P. Gallinari, 2006). WikipediaXML has become a standard machine readable form for Wikipedia, suitable for most Computational Linguistics purposes. It makes it easy to identify and read in the text portions of the document, removing or altering html and wiki code that is difficult to process in a standard way. The WikipediaXML standard has (so far) been used to process Wikipedia documents written in English, German, French,</context>
</contexts>
<marker>Denoyer, Gallinari, 2006</marker>
<rawString>Ludovic Denoyer and Patrick Gallinari. 2006. The Wikipedia XML Corpus. SIGIR Forum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Denoyer</author>
<author>A Vercoustre P Gallinari</author>
</authors>
<title>Report on the XML Mining Track at INEX</title>
<date>2006</date>
<booktitle>In Advances in XML Information Retrieval and Evaluation: Fifthth Workshop of the</booktitle>
<contexts>
<context position="1666" citStr="Denoyer and Gallinari, 2006" startWordPosition="260" endWordPosition="263"> feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006). 1 Introduction This working group seeks to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: 1. The comparison of annotation schemes 2. The merging of information represented by various annotation schemes 3. The emergence of NLP systems that use information in multiple annotation schemes; and 4. The adoption of various types of best practice in corpus annotation, including: (a) Clearer demarcation of th</context>
<context position="11933" citStr="Denoyer and Gallinari, 2006" startWordPosition="1889" endWordPosition="1892">k pages which discuss them – perhaps this relation can be exploited by systems which try to detect discussions about topics, e.g., searches for discussions about current events topics; (2) There are various meta tags, many of which are not included in the WikipediaXML (see below), but nevertheless are retrievable from the original HTML files. Some of these may be useful for various applications. For example, the levels of disputability of the content of the main articles is annotated (cf. http://en.wikipedia.org/wiki/Wikipedia: Template messages/Disputes ). 3.2 Why WikipediaXML? WikipediaXML (Denoyer and Gallinari, 2006) is an XML version of Wikipedia data, originally designed for Information Retrieval tasks such as INEX (Fuhr et al., 2006) and the XML Document Mining Challenge (Denoyer and P. Gallinari, 2006). WikipediaXML has become a standard machine readable form for Wikipedia, suitable for most Computational Linguistics purposes. It makes it easy to identify and read in the text portions of the document, removing or altering html and wiki code that is difficult to process in a standard way. The WikipediaXML standard has (so far) been used to process Wikipedia documents written in English, German, French,</context>
</contexts>
<marker>Denoyer, Gallinari, 2006</marker>
<rawString>L. Denoyer and A. Vercoustre P. Gallinari. 2006. Report on the XML Mining Track at INEX 2005 and INEX 2006 : Categorization and Clustering of XML Documents. In Advances in XML Information Retrieval and Evaluation: Fifthth Workshop of the INitiative for the Evaluation of XML Retrieval (INEX’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
<author>M Lalmas</author>
<author>S Malik</author>
</authors>
<date>2006</date>
<booktitle>Advances in XML Information Retrieval and Evaluation. In 5th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX</booktitle>
<contexts>
<context position="11170" citStr="Fuhr et al., 2006" startWordPosition="1768" endWordPosition="1771"> 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). Some other interesting properties of Wikipedia that have yet to be explored to our knowledge include: (1) Most main articles have talk pages which discuss them – perhaps this relation can be exploited by systems which try to detect discussions about topics, e.g., searches for discussions about current events topics; (2) There are various meta tags, many of which are not included in the WikipediaXML (see below), but nevertheless are retrievable from the original HTML files. Some of these may be useful for various applications. For example, the levels of disputability of the content of the mai</context>
</contexts>
<marker>Fuhr, Lalmas, Malik, 2006</marker>
<rawString>N. Fuhr, M. Lalmas, and S. Malik. 2006. Advances in XML Information Retrieval and Evaluation. In 5th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>C Macleod</author>
</authors>
<title>The american national corpus: A standardized resource of american english.</title>
<date>2001</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<location>Lancaster, UK.</location>
<contexts>
<context position="1551" citStr="Ide and Macleod, 2001" startWordPosition="243" endWordPosition="246">eing annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006). 1 Introduction This working group seeks to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: 1. The comparison of annotation schemes 2. The merging of information represented by various annotation schemes 3. The emergence of NLP systems that use information in multiple annotation schemes; a</context>
<context position="5863" citStr="Ide and Macleod, 2001" startWordPosition="905" endWordPosition="908">rocessing. This corpus was selected from: • Those articles cited as controversial according to the November 28, 2006 version of the following Wikipedia page: http://en.wikipedia.org/wiki/Wikipedia: List of controversial issues • The talk pages corresponding to these articles where Wikipedia users and the community debate aspects of articles. These debates may be about content or editorial considerations. • Articles in Japanese that are linked to the English pages (and the associated talk pages) are also part of our corpus. 2 American National Corpus The American National Corpus (ANC) project (Ide and Macleod, 2001; Ide and Suderman, 2004) has released over 20 million words of spoken and written American English, available from the Linguistic Data Consortium. The ANC 2nd release consists of fiction, non-fiction, newspapers, technical reports, magazine and journal articles, a substantial amount of spoken data, data from blogs and other unedited web sources, travel guides, technical manuals, and other genres. All texts are annotated for sentence boundaries; token boundaries, 185 lemma, and part of speech produced by two different taggers ; and noun and verb chunks. A subcorpus of 10 million words reflecti</context>
</contexts>
<marker>Ide, Macleod, 2001</marker>
<rawString>N. Ide and C. Macleod. 2001. The american national corpus: A standardized resource of american english. In Proceedings of Corpus Linguistics 2001, Lancaster, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>The american national corpus first release.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<pages>1681--1684</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1576" citStr="Ide and Suderman, 2004" startWordPosition="247" endWordPosition="251"> use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006). 1 Introduction This working group seeks to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: 1. The comparison of annotation schemes 2. The merging of information represented by various annotation schemes 3. The emergence of NLP systems that use information in multiple annotation schemes; and 4. The adoption of var</context>
<context position="5888" citStr="Ide and Suderman, 2004" startWordPosition="909" endWordPosition="912">was selected from: • Those articles cited as controversial according to the November 28, 2006 version of the following Wikipedia page: http://en.wikipedia.org/wiki/Wikipedia: List of controversial issues • The talk pages corresponding to these articles where Wikipedia users and the community debate aspects of articles. These debates may be about content or editorial considerations. • Articles in Japanese that are linked to the English pages (and the associated talk pages) are also part of our corpus. 2 American National Corpus The American National Corpus (ANC) project (Ide and Macleod, 2001; Ide and Suderman, 2004) has released over 20 million words of spoken and written American English, available from the Linguistic Data Consortium. The ANC 2nd release consists of fiction, non-fiction, newspapers, technical reports, magazine and journal articles, a substantial amount of spoken data, data from blogs and other unedited web sources, travel guides, technical manuals, and other genres. All texts are annotated for sentence boundaries; token boundaries, 185 lemma, and part of speech produced by two different taggers ; and noun and verb chunks. A subcorpus of 10 million words reflecting the genre distribution</context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>N. Ide and K. Suderman. 2004. The american national corpus first release. In Proceedings of LREC 2004, pages 1681–1684, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>Integrating linguistic resources: The american national corpus model.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="7815" citStr="Ide and Suderman, 2006" startWordPosition="1228" endWordPosition="1231">. ANC annotations are distributed as stand-off documents representing a set of graphs over the primary data, thus allowing for layering of annotations and inclusion of multiple annotations of the same type. Because most existing tools for corpus access and manipulation do not handle stand-off annotations, we have developed an easy-to-use tool and user interface to merge the user’s choice of stand-off annotations with the primary data to form a single document in any of several XML and non-XML formats, which is distributed with the corpus. The ANC architecture and format is described fully in (Ide and Suderman, 2006). 2.1 The ULA Subcorpus The Unified Linguistic Annotation (ULA) project has selected a 40,000 word subcorpus of the Open ANC for annotation with several different annotation schemes including: the Penn Treebank, PropBank, NomBank, the Penn Discourse Treebank, TimeML and Opinion Annotation.2 This initial subcorpus can be broken down as follows: • Spoken Language – charlotte: 5K words – switchboard: 5K words • letters: 10K words 2Other corpora being annotated by the ULA project include sections of the Brown corpus and LDC parallel corpora. • Slate (Journal): 5K words • Travel guides: 5K words • </context>
</contexts>
<marker>Ide, Suderman, 2006</marker>
<rawString>N. Ide and K. Suderman. 2006. Integrating linguistic resources: The american national corpus model. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P T Nguyen</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Subtree Mining for Relation Extraction from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLT</booktitle>
<contexts>
<context position="10871" citStr="Nguyen et al., 2007" startWordPosition="1720" endWordPosition="1723">n 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). Some other interesting properties of Wikipedia that have yet to be explored to our knowledge include: (1) Most main articles have talk pages which discuss them – perhaps this relation can be exploited by systems which try to detect discussions about topics, e.g., searches for discussions about curr</context>
</contexts>
<marker>Nguyen, Matsuo, Ishizuka, 2007</marker>
<rawString>D. P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Subtree Mining for Relation Extraction from Wikipedia. In Proc. of NAACL/HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Creating a Knowledge Base From a Collaboratively Generated Encyclopedia.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLT</booktitle>
<contexts>
<context position="10574" citStr="Ponzetto, 2007" startWordPosition="1678" endWordPosition="1679">aining data for natural language processing because: 1. they are lexically diverse (e.g., providing a lot of lexical information for statistical systems); 2. the textual information is well structured 3. Wikipedia is a large and growing corpus 186 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). So</context>
</contexts>
<marker>Ponzetto, 2007</marker>
<rawString>Simone Paolo Ponzetto. 2007. Creating a Knowledge Base From a Collaboratively Generated Encyclopedia. In Proc. of NAACL/HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R¨udiger Gleim</author>
<author>Alexander Mehler</author>
<author>Matthias Dehmer</author>
</authors>
<title>Web Corpus Mining by instance of Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. 2nd Web as Corpus Workshop at EACL</booktitle>
<marker>Gleim, Mehler, Dehmer, 2007</marker>
<rawString>R¨udiger Gleim and Alexander Mehler and Matthias Dehmer. 2007. Web Corpus Mining by instance of Wikipedia. In Proc. 2nd Web as Corpus Workshop at EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>S P Ponzetto</author>
</authors>
<title>WikiRelate! Computing semantic relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proc. of AAAI-06,</booktitle>
<pages>1419--1424</pages>
<contexts>
<context position="10508" citStr="Strube and Ponzetto, 2006" startWordPosition="1666" endWordPosition="1669"> mostly limited to these two genres, we believe that it is well suited as training data for natural language processing because: 1. they are lexically diverse (e.g., providing a lot of lexical information for statistical systems); 2. the textual information is well structured 3. Wikipedia is a large and growing corpus 186 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.scienc</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>M. Strube and S. P. Ponzetto. 2006. WikiRelate! Computing semantic relatedness using Wikipedia. In Proc. of AAAI-06, pages 1419–1424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>YAGO: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proc. of WWW-07.</booktitle>
<contexts>
<context position="10531" citStr="Suchanek et al., 2007" startWordPosition="1670" endWordPosition="1673">o genres, we believe that it is well suited as training data for natural language processing because: 1. they are lexically diverse (e.g., providing a lot of lexical information for statistical systems); 2. the textual information is well structured 3. Wikipedia is a large and growing corpus 186 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and in</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>F. M. Suchanek, G. Kasneci, and G.Weikum. 2007. YAGO: A core of semantic knowledge. In Proc. of WWW-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Rafael Mu noz</author>
</authors>
<title>A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLT</booktitle>
<contexts>
<context position="10798" citStr="Toral and noz, 2007" startWordPosition="1709" endWordPosition="1712">arge and growing corpus 186 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). Some other interesting properties of Wikipedia that have yet to be explored to our knowledge include: (1) Most main articles have talk pages which discuss them – perhaps this relation can be exploited by systems which try to d</context>
</contexts>
<marker>Toral, noz, 2007</marker>
<rawString>Antonio Toral and Rafael Mu noz. 2007. A proposal to automatically build and maintain gazetteers for Named Entity Recognition by using Wikipedia. In Proc. of NAACL/HLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Analysis of the Wikipedia Category Graph for NLP Applications. In</title>
<date>2007</date>
<booktitle>Proc of NAACL-HLT</booktitle>
<tech>Workshop: TextGraphs2.</tech>
<contexts>
<context position="10557" citStr="Zesch and Gurevych, 2007" startWordPosition="1674" endWordPosition="1677">at it is well suited as training data for natural language processing because: 1. they are lexically diverse (e.g., providing a lot of lexical information for statistical systems); 2. the textual information is well structured 3. Wikipedia is a large and growing corpus 186 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr </context>
</contexts>
<marker>Zesch, Gurevych, 2007</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2007. Analysis of the Wikipedia Category Graph for NLP Applications. In Proc of NAACL-HLT 2007 Workshop: TextGraphs2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>