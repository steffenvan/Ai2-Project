<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001431">
<title confidence="0.960055">
Bielefeld SC: Orthonormal Topic Modelling for Grammar Induction
</title>
<author confidence="0.994885">
John P. McCrae
</author>
<affiliation confidence="0.993102">
CITEC, Bielefeld University
</affiliation>
<address confidence="0.926404">
Inspiration 1
Bielefeld, Germany
</address>
<email confidence="0.885611">
jmccrae@cit-ec.uni-bielefeld.de
</email>
<author confidence="0.965461">
Philipp Cimiano
</author>
<affiliation confidence="0.970869">
CITEC, Bielefeld University
</affiliation>
<address confidence="0.9011145">
Inspiration 1
Bielefeld, Germany
</address>
<email confidence="0.854018">
cimiano@cit-ec.uni-bielefeld.de
</email>
<sectionHeader confidence="0.989424" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903916666666">
In this paper, we consider the application
of topic modelling to the task of induct-
ing grammar rules. In particular, we look
at the use of a recently developed method
called orthonormal explicit topic analysis,
which combines explicit and latent models
of semantics. Although, it remains unclear
how topic model may be applied to the
case of grammar induction, we show that
it is not impossible and that this may allow
the capture of subtle semantic distinctions
that are not captured by other methods.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9739205">
Grammar induction is the task of inducing high-
level rules for application of grammars in spoken
dialogue systems. In practice, we can extract rel-
evant rules and the task of grammar induction re-
duces to finding similar rules between two strings.
As these strings are not necessarily similar in sur-
face form, what we really wish to calculate is
the semantic similarity between these strings. As
such, we could think of applying a semantic anal-
ysis method. As such we attempt to apply topic
modelling, that is methods such as Latent Dirich-
let Allocation (Blei et al., 2003), Latent Seman-
tic Analysis (Deerwester et al., 1990) or Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2007). In particular we build on the recent work
to unify latent and explicit methods by means of
orthonormal explicit topics.
In topic modelling the key choice is the docu-
ment space that will act as the corpus and hence
topic space. The standard choice is to regard all
articles from a background document collection
– Wikipedia articles are a typical choice – as the
</bodyText>
<footnote confidence="0.8254395">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999451642857143">
topic space. However, it is crucial to ensure that
these topics cover the semantic space evenly and
completely. Following McCrae et al. (McCrae et
al., 2013) we remap the semantic space defined by
the topics in such a manner that it is orthonormal.
In this way, each document is mapped to a topic
that is distinct from all other topics.
The structure of the paper is as follows: we de-
scribe our method in three parts, first the method
in section 2, followed by approximation method in
section 3, the normalization methods in section 4
and finally the application to grammar induction
in section 5, we finish with some conclusions in
section 6.
</bodyText>
<sectionHeader confidence="0.934113" genericHeader="method">
2 Orthonormal explicit topic analysis
</sectionHeader>
<bodyText confidence="0.99992175">
ONETA (McCrae et al., 2013, Orthonormal ex-
plicit topic analysis) follows Explicit Semantic
Analysis in the sense that it assumes the avail-
ability of a background document collection B =
{b1, b2, ..., bN} consisting of textual representa-
tions. The mapping into the explicit topic space
is defined by a language-specific function Φ that
maps documents into RN such that the jth value in
the vector is given by some association measure
φj(d) for each background document bj. Typical
choices for this association measure φ are the sum
of the TF-IDF scores or an information retrieval
relevance scoring function such as BM-25 (Sorg
and Cimiano, 2010).
For the case of TF-IDF, the value of the j-th
element of the topic vector is given by:
</bodyText>
<equation confidence="0.9195515">
φj(d) = tf-idf(bj)T −−−→
−−−→ tf-idf(d)
</equation>
<bodyText confidence="0.9906515">
Thus, the mapping function can be represented
as the product of a TF-IDF vector of document d
multiplied by a word-by-document (W x N) TF-
IDF matrix, which we denote as a X:1
</bodyText>
<footnote confidence="0.888726">
1T denotes the matrix transpose as usual
</footnote>
<page confidence="0.951128">
119
</page>
<note confidence="0.6768885">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 119–122,
Dublin, Ireland, August 23-24, 2014.
</note>
<equation confidence="0.9484825">
tf-idf(b1)T
−−−→
tf-idf(d) = XT · −−−→
−−−→ tf-idf(d)
tf-idf(bN)T
−−−→
</equation>
<bodyText confidence="0.999605142857143">
For simplicity, we shall assume from this point
on that all vectors are already converted to a TF-
IDF or similar numeric vector form.
In order to compute the similarity between two
documents di and dj, typically the cosine-function
(or the normalized dot product) between the vec-
tors Φ(di) and Φ(dj) is computed as follows:
</bodyText>
<equation confidence="0.9943334">
Φ(di)TΦ(dj)
sim(di, dj) = cos(Φ(di), Φ(dj)) =
||Φ(di) |Φ(dj)||
sim(di, dj) = cos(XTdi, XTdj) = dT i XXTdj
||XTdi |XTdj||
</equation>
<bodyText confidence="0.998425">
The key challenge with topic modelling is
choosing a good background document collection
B = {b1, ..., bNJ. A simple minimal criterion
for a good background document collection is that
each document in this collection should be maxi-
mally similar to itself and less similar to any other
document:
</bodyText>
<equation confidence="0.989061">
Vi =� j 1 = sim(bj, bj) &gt; sim(bi, bj) &gt; 0
</equation>
<bodyText confidence="0.993765">
As shown in McCrae et al. (2013), this property
is satisfied by the following projection:
</bodyText>
<equation confidence="0.971447">
ΦONETA(d) = (XTX)−1XTd
</equation>
<bodyText confidence="0.990879777777778">
is some W1 such that rows with index greater
than W1 have only zeroes in the columns up to
N1. In other words, we take a subset of N1 doc-
uments and enumerate the words in such a way
that the terms occurring in the first N1 documents
are enumerated 1, ... , W1. Let N2 = N − N1,
W2 = W − W1. The result of this row permuta-
tion does not affect the value of XTX and we can
write the matrix X as:
</bodyText>
<equation confidence="0.910447">
X =(A B)
0 C /)
</equation>
<bodyText confidence="0.978509166666667">
where A is a W1 x N1 matrix representing
term frequencies in the first N1 documents, B is a
W1xN2 matrix containing term frequencies in the
remaining documents for terms that are also found
in the first N1 documents, and C is a W2 x N2
containing the frequency of all terms not found in
the first N1 documents.
Application of the well-known divide-and-
conquer formula (Bernstein, 2005, p. 159) for ma-
trix inversion yields the following easily verifiable
matrix identity, given that we can find C&apos; such that
C&apos;C = I.
</bodyText>
<equation confidence="0.9986655">
( / ( A B /
(ATA)−&apos;AT −(ATA)−&apos;ATBC� = I
0 C&apos; 0 C
(1)
</equation>
<bodyText confidence="0.6827195">
The inverse C&apos; is approximated by the Jacobi
Preconditioner, J, of CTC:
</bodyText>
<figure confidence="0.959621818181818">
C&apos; JCT (2)
⎛
⎜
Φ(d) = ⎝
...
⎞
⎠ ⎟
And hence the similarity between two docu- = ⎛ ||c1||−2 0 ⎞ CT
ments can be calculated as: ⎜ ⎝ ... ⎠ ⎟
0 ||cN2||−2
sim(di, dj) = cos(ΦONETA(di), ΦONETA(dj))
</figure>
<sectionHeader confidence="0.976623" genericHeader="method">
3 Approximations
</sectionHeader>
<bodyText confidence="0.999966818181818">
ONETA relies on the computation of a matrix in-
verse, which has a complexity that, using current
practical algorithms, is approximately cubic and
as such the time spent calculating the inverse can
grow very quickly.
We notice that X is typically very sparse and
moreover some rows of X have significantly fewer
non-zeroes than others (these rows are for terms
with low frequency). Thus, if we take the first N1
columns (documents) in X, it is possible to re-
arrange the rows of X with the result that there
</bodyText>
<sectionHeader confidence="0.998132" genericHeader="method">
4 Normalization
</sectionHeader>
<bodyText confidence="0.999874583333333">
A key factor in the effectiveness of topic-based
methods is the appropriate normalization of the el-
ements of the document matrix X. This is even
more relevant for orthonormal topics as the matrix
inversion procedure can be very sensitive to small
changes in the matrix. In this context, we con-
sider two forms of normalization, term and docu-
ment normalization, which can also be considered
as row/column normalizations of X.
A straightforward approach to normalization is
to normalize each column of X to obtain a matrix
as follows:
</bodyText>
<page confidence="0.868538">
120
</page>
<equation confidence="0.820485">
x1 X0 = ||x1 ||... xN ||xN||
</equation>
<bodyText confidence="0.967861266666667">
If we calculate X0TX0 = Y then we get that the
(i, j)-th element of Y is:
xTi xj
||xi |xj||
Thus, the diagonal of Y consists of ones only and
due to the Cauchy-Schwarz inequality we have
that |yij |&lt; 1, with the result that the matrix Y
is already close to I. Formally, we can use this
to state a bound on ||X0TX0 − I||F, but in prac-
tice it means that the orthonormalizing matrix has
more small or zero values. Previous experiments
have indicated that in general term normalization
such as TF-IDF is not as effective as using the di-
rect term frequency in ONETA, so we do not apply
term normalization.
</bodyText>
<sectionHeader confidence="0.885364" genericHeader="method">
5 Application to grammar induction
</sectionHeader>
<bodyText confidence="0.99979125">
The application to grammar induction is simply
carried out by taking the rules and creating a sin-
gle ground instance. That is if we have a rule of
the form
</bodyText>
<sectionHeader confidence="0.5285" genericHeader="method">
LEAVING FROM &lt;CITY&gt;
</sectionHeader>
<bodyText confidence="0.995829125">
We would replace the instance of &lt;CITY&gt; with
a known terminal for this rule, e.g.,
leaving from Berlin
This reduces the task to that of string simi-
larity which can be processed by means of any
string similarity function, for example such as the
ONETA function described above. As such the
procedure is as follows:
</bodyText>
<listItem confidence="0.9479085">
1. Ground the input grammar rule to an English
string d
2. Ground each candidate matching rule to an
English string di
3. Calculate for each di, the similarity
simONETA(d, di)
4. Add the rule to the grammar class with the
highest similarity
</listItem>
<bodyText confidence="0.999931636363636">
This approach has the obvious drawback that it
removes all information about the valence of the
rule, however the effect of this loss of information
remains unclear.
For application, we used 20,000 Wikipedia ar-
ticles, filtered to contain only those of over 100
words, giving us a corpus of 15.6 million tokens.
We applied ONETA using document normaliza-
tion but no term normalization and the value N1 =
5000. These parameters were chosen based on the
best results in previous experiments.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999982411764706">
The results show that such a naive approach is
not directly applicable to the case of grammar in-
duction, however we believe that it is possible
that the subtle semantic similarities captured by
topic modelling may yet prove useful for gram-
mar induction. However it is clear from the pre-
sented results that the use of a topic model alone
does not suffice to solve this task. We notice that
from the data many of the distinctions rely on
antonyms and stop words, especially distinctions
such as ‘to’/‘from’, which are not captured by a
topic model as topic models generally ignore stop
words, and generally consider antonyms to be in
the same topic, as they frequently occur together
in text. The question of when semantic similarity
such as provided by topic modelling is applicable
remains an open question.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998542526315789">
Dennis S Bernstein. 2005. Matrix mathematics, 2nd
Edition. Princeton University Press Princeton.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, volume 6, page 12.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1732–1740.
</reference>
<equation confidence="0.425934">
yij =
</equation>
<page confidence="0.986247">
121
</page>
<reference confidence="0.9745702">
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natu-
ral Language Processing and Information Systems,
pages 36–48. Springer.
</reference>
<page confidence="0.997452">
122
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.243625">
<title confidence="0.897645">Bielefeld SC: Orthonormal Topic Modelling for Grammar Induction</title>
<author confidence="0.417742">P</author>
<affiliation confidence="0.692603">CITEC, Bielefeld Inspiration</affiliation>
<address confidence="0.962334">Bielefeld, Germany</address>
<email confidence="0.997726">jmccrae@cit-ec.uni-bielefeld.de</email>
<author confidence="0.845846">Philipp</author>
<affiliation confidence="0.898483">CITEC, Bielefeld Inspiration</affiliation>
<address confidence="0.985796">Bielefeld, Germany</address>
<email confidence="0.998726">cimiano@cit-ec.uni-bielefeld.de</email>
<abstract confidence="0.998882615384615">In this paper, we consider the application of topic modelling to the task of inducting grammar rules. In particular, we look at the use of a recently developed method called orthonormal explicit topic analysis, which combines explicit and latent models of semantics. Although, it remains unclear how topic model may be applied to the case of grammar induction, we show that it is not impossible and that this may allow the capture of subtle semantic distinctions that are not captured by other methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dennis S Bernstein</author>
</authors>
<title>Matrix mathematics, 2nd Edition.</title>
<date>2005</date>
<publisher>Princeton University Press Princeton.</publisher>
<contexts>
<context position="5641" citStr="Bernstein, 2005" startWordPosition="952" endWordPosition="953"> way that the terms occurring in the first N1 documents are enumerated 1, ... , W1. Let N2 = N − N1, W2 = W − W1. The result of this row permutation does not affect the value of XTX and we can write the matrix X as: X =(A B) 0 C /) where A is a W1 x N1 matrix representing term frequencies in the first N1 documents, B is a W1xN2 matrix containing term frequencies in the remaining documents for terms that are also found in the first N1 documents, and C is a W2 x N2 containing the frequency of all terms not found in the first N1 documents. Application of the well-known divide-andconquer formula (Bernstein, 2005, p. 159) for matrix inversion yields the following easily verifiable matrix identity, given that we can find C&apos; such that C&apos;C = I. ( / ( A B / (ATA)−&apos;AT −(ATA)−&apos;ATBC� = I 0 C&apos; 0 C (1) The inverse C&apos; is approximated by the Jacobi Preconditioner, J, of CTC: C&apos; JCT (2) ⎛ ⎜ Φ(d) = ⎝ ... ⎞ ⎠ ⎟ And hence the similarity between two docu- = ⎛ ||c1||−2 0 ⎞ CT ments can be calculated as: ⎜ ⎝ ... ⎠ ⎟ 0 ||cN2||−2 sim(di, dj) = cos(ΦONETA(di), ΦONETA(dj)) 3 Approximations ONETA relies on the computation of a matrix inverse, which has a complexity that, using current practical algorithms, is approximately </context>
</contexts>
<marker>Bernstein, 2005</marker>
<rawString>Dennis S Bernstein. 2005. Matrix mathematics, 2nd Edition. Princeton University Press Princeton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1378" citStr="Blei et al., 2003" startWordPosition="210" endWordPosition="213">e not captured by other methods. 1 Introduction Grammar induction is the task of inducing highlevel rules for application of grammars in spoken dialogue systems. In practice, we can extract relevant rules and the task of grammar induction reduces to finding similar rules between two strings. As these strings are not necessarily similar in surface form, what we really wish to calculate is the semantic similarity between these strings. As such, we could think of applying a semantic analysis method. As such we attempt to apply topic modelling, that is methods such as Latent Dirichlet Allocation (Blei et al., 2003), Latent Semantic Analysis (Deerwester et al., 1990) or Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). In particular we build on the recent work to unify latent and explicit methods by means of orthonormal explicit topics. In topic modelling the key choice is the document space that will act as the corpus and hence topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings foot</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JASIS,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1430" citStr="Deerwester et al., 1990" startWordPosition="218" endWordPosition="221">n Grammar induction is the task of inducing highlevel rules for application of grammars in spoken dialogue systems. In practice, we can extract relevant rules and the task of grammar induction reduces to finding similar rules between two strings. As these strings are not necessarily similar in surface form, what we really wish to calculate is the semantic similarity between these strings. As such, we could think of applying a semantic analysis method. As such we attempt to apply topic modelling, that is methods such as Latent Dirichlet Allocation (Blei et al., 2003), Latent Semantic Analysis (Deerwester et al., 1990) or Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). In particular we build on the recent work to unify latent and explicit methods by means of orthonormal explicit topics. In topic modelling the key choice is the document space that will act as the corpus and hence topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: htt</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JASIS, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>6</volume>
<pages>12</pages>
<contexts>
<context position="1495" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="226" endWordPosition="229">les for application of grammars in spoken dialogue systems. In practice, we can extract relevant rules and the task of grammar induction reduces to finding similar rules between two strings. As these strings are not necessarily similar in surface form, what we really wish to calculate is the semantic similarity between these strings. As such, we could think of applying a semantic analysis method. As such we attempt to apply topic modelling, that is methods such as Latent Dirichlet Allocation (Blei et al., 2003), Latent Semantic Analysis (Deerwester et al., 1990) or Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007). In particular we build on the recent work to unify latent and explicit methods by means of orthonormal explicit topics. In topic modelling the key choice is the document space that will act as the corpus and hence topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ topic space. However, it</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, volume 6, page 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John P McCrae</author>
<author>Philipp Cimiano</author>
<author>Roman Klinger</author>
</authors>
<title>Orthonormal explicit topic analysis for crosslingual document matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1732--1740</pages>
<contexts>
<context position="2228" citStr="McCrae et al., 2013" startWordPosition="343" endWordPosition="346"> topics. In topic modelling the key choice is the document space that will act as the corpus and hence topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ topic space. However, it is crucial to ensure that these topics cover the semantic space evenly and completely. Following McCrae et al. (McCrae et al., 2013) we remap the semantic space defined by the topics in such a manner that it is orthonormal. In this way, each document is mapped to a topic that is distinct from all other topics. The structure of the paper is as follows: we describe our method in three parts, first the method in section 2, followed by approximation method in section 3, the normalization methods in section 4 and finally the application to grammar induction in section 5, we finish with some conclusions in section 6. 2 Orthonormal explicit topic analysis ONETA (McCrae et al., 2013, Orthonormal explicit topic analysis) follows Ex</context>
<context position="4768" citStr="McCrae et al. (2013)" startWordPosition="778" endWordPosition="781">nd dj, typically the cosine-function (or the normalized dot product) between the vectors Φ(di) and Φ(dj) is computed as follows: Φ(di)TΦ(dj) sim(di, dj) = cos(Φ(di), Φ(dj)) = ||Φ(di) |Φ(dj)|| sim(di, dj) = cos(XTdi, XTdj) = dT i XXTdj ||XTdi |XTdj|| The key challenge with topic modelling is choosing a good background document collection B = {b1, ..., bNJ. A simple minimal criterion for a good background document collection is that each document in this collection should be maximally similar to itself and less similar to any other document: Vi =� j 1 = sim(bj, bj) &gt; sim(bi, bj) &gt; 0 As shown in McCrae et al. (2013), this property is satisfied by the following projection: ΦONETA(d) = (XTX)−1XTd is some W1 such that rows with index greater than W1 have only zeroes in the columns up to N1. In other words, we take a subset of N1 documents and enumerate the words in such a way that the terms occurring in the first N1 documents are enumerated 1, ... , W1. Let N2 = N − N1, W2 = W − W1. The result of this row permutation does not affect the value of XTX and we can write the matrix X as: X =(A B) 0 C /) where A is a W1 x N1 matrix representing term frequencies in the first N1 documents, B is a W1xN2 matrix conta</context>
</contexts>
<marker>McCrae, Cimiano, Klinger, 2013</marker>
<rawString>John P. McCrae, Philipp Cimiano, and Roman Klinger. 2013. Orthonormal explicit topic analysis for crosslingual document matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1732–1740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Sorg</author>
<author>Philipp Cimiano</author>
</authors>
<title>An experimental comparison of explicit semantic analysis implementations for cross-language retrieval.</title>
<date>2010</date>
<booktitle>In Natural Language Processing and Information Systems,</booktitle>
<pages>36--48</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3398" citStr="Sorg and Cimiano, 2010" startWordPosition="541" endWordPosition="544">2013, Orthonormal explicit topic analysis) follows Explicit Semantic Analysis in the sense that it assumes the availability of a background document collection B = {b1, b2, ..., bN} consisting of textual representations. The mapping into the explicit topic space is defined by a language-specific function Φ that maps documents into RN such that the jth value in the vector is given by some association measure φj(d) for each background document bj. Typical choices for this association measure φ are the sum of the TF-IDF scores or an information retrieval relevance scoring function such as BM-25 (Sorg and Cimiano, 2010). For the case of TF-IDF, the value of the j-th element of the topic vector is given by: φj(d) = tf-idf(bj)T −−−→ −−−→ tf-idf(d) Thus, the mapping function can be represented as the product of a TF-IDF vector of document d multiplied by a word-by-document (W x N) TFIDF matrix, which we denote as a X:1 1T denotes the matrix transpose as usual 119 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 119–122, Dublin, Ireland, August 23-24, 2014. tf-idf(b1)T −−−→ tf-idf(d) = XT · −−−→ −−−→ tf-idf(d) tf-idf(bN)T −−−→ For simplicity, we shall assume from this po</context>
</contexts>
<marker>Sorg, Cimiano, 2010</marker>
<rawString>Philipp Sorg and Philipp Cimiano. 2010. An experimental comparison of explicit semantic analysis implementations for cross-language retrieval. In Natural Language Processing and Information Systems, pages 36–48. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>