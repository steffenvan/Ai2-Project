<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.997372">
Parse Imputation for Dependency Annotations
</title>
<author confidence="0.999948">
Jason Mielens&apos; Liang Sun&apos; Jason Baldridge&apos;
</author>
<affiliation confidence="0.9977955">
&apos;Department of Linguistics &apos;Department of Mechanical Engineering
The University of Texas at Austin The University of Texas at Austin
</affiliation>
<email confidence="0.997399">
{jmielens,jbaldrid}@utexas.edu sally722@utexas.edu
</email>
<sectionHeader confidence="0.994996" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984444444445">
Syntactic annotation is a hard task, but it
can be made easier by allowing annotators
flexibility to leave aspects of a sentence
underspecified. Unfortunately, partial an-
notations are not typically directly usable
for training parsers. We describe a method
for imputing missing dependencies from
sentences that have been partially anno-
tated using the Graph Fragment Language,
such that a standard dependency parser
can then be trained on all annotations. We
show that this strategy improves perfor-
mance over not using partial annotations
for English, Chinese, Portuguese and Kin-
yarwanda, and that performance competi-
tive with state-of-the-art unsupervised and
weakly-supervised parsers can be reached
with just a few hours of annotation.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899603448276">
Linguistically annotated data is produced for
many purposes in many contexts. It typically
requires considerable effort, particularly for lan-
guage documentation efforts in which tooling,
data, and expertise in the language are scarce.
The challenge presented by this scarcity is com-
pounded when doing deeper analysis, such as syn-
tactic structure, which typically requires greater
expertise and existing tooling. In such scenar-
ios, unsupervised approaches are a tempting strat-
egy. While the performance of unsupervised
dependency parsing has improved greatly since
Klein and Manning’s (2004) Dependency Model
with Valence (DMV), state-of-the-art unsuper-
vised parsers still perform well below supervised
approaches (Martins et al., 2010; Spitkovsky et al.,
2012; Blunsom and Cohn, 2010). Additionally,
they typically require large amounts of raw data.
While this is not a problem for some languages,
many of the world’s languages do not have a clean,
digitized corpus available.1 For instance, the ap-
proach of Naseem et al. (2010) is unsupervised in
the sense that it requires no dependency annota-
tions, but it still makes use of the raw version of
the full Penn Treebank. The approach of Mare-
cek et al. (2013) requires extra unlabeled texts to
estimate parameters.
Another strategy is to exploit small amounts of
supervision or knowledge. Naseem et al. (2010)
use a set of universal dependency rules and obtain
substantial gains over unsupervised methods in
many languages. Spitkovsky et al. (2010b; 2011)
use web mark-up and punctuation as additional an-
notations. Alternatively, one could try to obtain
actual dependency annotations cheaply. We use
the Graph Fragment Language (GFL), which was
created with the goal of making annotations eas-
ier for experts and possible for novices (Schneider
et al., 2013; Mordowanec et al., 2014). GFL sup-
ports partial annotations, so annotators can omit
obvious dependencies or skip difficult construc-
tions. The ability to focus on portions of a sen-
tence frees the annotator to target constituents and
dependencies that maximize information that will
be most useful for machine-learned parsers. For
example, Hwa (1999) found higher-level sentence
constituents to be more informative for learning
parsers than lower-level ones.
To support this style of annotation while getting
the benefit from partial annotations, we develop a
two-stage parser learning strategy. The first stage
completes the partial GFL annotations by adapting
a Gibbs tree sampler (Johnson et al., 2007; Sun et
al., 2014). The GFL annotations constrain the tree
sampling space by using both dependencies and
the constituent boundaries they express. The sys-
tem performs missing dependency arc imputation
using Gibbs sampling – we refer to this approach
</bodyText>
<footnote confidence="0.7409315">
1In fact, standardized writing systems have yet to be
adopted for some languages.
</footnote>
<note confidence="0.877763">
1385
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1385–1394,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999974444444444">
as the Gibbs Parse Completer2 (GPC). The sec-
ond stage uses the full dependencies output by the
GPC to train Turbo Parser (Martins et al., 2010),
and evaluation is done with this trained model on
unseen sentences. In simulation experiments for
English, Chinese and Portuguese, we show that
the method gracefully degrades when applied to
training corpora with increasing percentages of the
gold training dependencies removed. We also do
actual GFL annotations for those languages plus
Kinyarwanda, and show that using the GPC to fill
in the missing dependencies after two hours of
annotation enables Turbo Parser to obtain 2-6%
better absolute performance than when it has to
throw incomplete annotations out. Furthermore,
the gains are even greater with less annotation time
and it never hurts to use the GPC—so an annota-
tion project can pursue a partial annotation strat-
egy without undermining the utility of the work
for parser training.
This strategy has the further benefit of needing
only a small number of sentences—in our case,
under 100 sentences annotated in a 2-4 hour win-
dow. Furthermore, it relies on no outside tools or
corpora other than a part-of-speech tagger; a re-
source that can be built with two hours of annota-
tion time (Garrette and Baldridge, 2013).
</bodyText>
<sectionHeader confidence="0.98896" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.99852955">
Data sources We use four languages from three
language families in an effort to both verify the
cross-linguistic applicability of our approach, ac-
counting for variations in linguistic properties, as
well as to attempt to realistically simulate a real-
world, low-resource environment. Our data comes
from English (ENG), Chinese (CHI), Portuguese
(POR), and Kinyarwanda (KIN).
For ENG we use the Penn Treebank (Marcus
et al., 1993), converted into dependencies by the
standard process. Section 23 was used as a test
set, and a random sample of sentences from sec-
tions 02-21 were selected for annotation with GFL
as described below and subsequently used as the
minimal training set. For CHI we use the Chi-
nese Treebank (CTB5) (Xue et al., 2005), also
converted to dependencies. The testing set con-
sisted of files 1-40/900-931, and the sentences pre-
sented for GFL annotation were randomly sam-
pled from files 81-899. The POR data is from
</bodyText>
<footnote confidence="0.9427235">
2The software, instructions, and data are available at
http://www.github.com/jmielens/gpc-acl-2015
</footnote>
<figureCaption confidence="0.993619">
Figure 1: GFL example for Mr. Conlon was ex-
</figureCaption>
<bodyText confidence="0.946590916666666">
ecutive vice president and director of the equity
division.
the CoNLL-X Shared Task on Multilingual De-
pendency Parsing and is derived from the Bosque
portion of the Floresta sint´a(c)tica corpus (Afonso
et al., 2002), using the standard provided splits for
training and testing. The KIN data is a corpus con-
sisting of transcripts of testimonies by survivors
of the Rwandan genocide, provided by the Kigali
Genocide Memorial Center – this data is described
by Garrette and Baldridge (2013).
GFL annotation We use a small number of sen-
tences annotated using the Graph Fragment Lan-
guage (GFL), a simple ASCII markup language
for dependency grammar (Schneider et al., 2013).
Unlike traditional syntactic annotation strategies
requiring trained annotators and great effort, rapid
GFL annotations can be collected from annotators
who have minimal training. Kong et al. (2014)
demonstrate the feasibility of training a depen-
dency parser based on a GFL-annotated corpus of
English tweets.
An example of GFL is shown in Figure 1: (a) is
the GFL markup itself and (b) is a graphical repre-
sentation of the dependencies it encodes. Figure 1
specifies several dependencies: of is a dependent
of director, executive vice president and director
are conjuncts and and is the coordinator. However,
the complete internal structure of the phrase the
equity division remains unspecified, other than di-
vision being marked as the head (via an asterisk).3
Finally, Mr. Conlon in square brackets indicates it
is a multiword expression.
3The graphical representation shows both of these as FE
nodes, for fudge expression, indicating they are grouped to-
gether but otherwise underspecified.
</bodyText>
<equation confidence="0.942047454545455">
1386
CFG Rule EVG distribution Description
S → YH P(root = H) The head of the sentence is H
YH → LHRH - Split-head representation
LH → HL P(STOP|dir = L, head = H, val = 0) H has no left children
LH → L1 P(CONT |dir = L, head = H, val = 0) H has at least one left child
H
L&apos;&apos; H → HL P(STOP|dir = L, head = H, val = 1) H has no more left children
L&apos;H → L1 P(CONT |dir = L, head = H, val = 1) H has other left children
H
L1H → YAL&apos;H P(ArgA|dir = L, head = H, val = 1) A is a left child of H
</equation>
<tableCaption confidence="0.97664725">
Table 1: The CFG-DMV grammar schema from Klein and Manning (2004). Note that in these rules
H and A are parts-of-speech. For brevity, we omit the portion of the grammar that handles the right-
hand arguments since they are symmetric to the left. Valency (val) can take the value 1 (we have made
attachments in the direction (dir) d) or 0 (not).
</tableCaption>
<table confidence="0.99648225">
CHI ENG KIN POR
Sentences Annotated 24 34 69 63
Tokens Annotated 820 798 988 1067
Fully Specified Sentences 4 15 31 20
</table>
<tableCaption confidence="0.999124">
Table 2: Two Hour GFL Annotation Statistics
</tableCaption>
<bodyText confidence="0.999559363636364">
Kong et al. (2014) stipulate that the GFL an-
notations in their corpus must be fully-specified.
They are thus unable to take advantage of such un-
derspecified sentences, and we address that limita-
tion in this paper. From the GFL annotations we
can extract and deduce dependency arcs and con-
straints (see Section 3.2 for full details) in order to
guide the Gibbs sampling process.
Time-bounded annotation As described in
Section 1, a primary goal of this work was to con-
sider the time in which a useful number of depen-
dency tree annotations might be collected, such
as might be required during the initial phase of a
language documentation project or corpus build.
To this end our annotators were operating under a
strict two hour time limit. We also collected two
further hours for English.
The annotators were instructed to annotate as
many sentences as possible in the two hours, and
that they should liberally use underspecification,
especially for particularly difficult sequences in a
given sentence. This was done to facilitate the
availability of partial annotations for experimenta-
tion. All of the annotators had some previous ex-
perience providing GFL annotations, so no train-
ing period was needed. Annotation was done in
30-minute blocks, to provide short breaks for the
annotators and so that learning curves could be
generated. Each language was annotated by a
single annotator. The ENG and CHI annotators
were native speakers of their annotation language,
while the POR and KIN annotators were non-native
though proficient speakers.
The annotators achieved rates of 400-500 to-
kens/hr, whereas we find rates of 150-200 to-
kens/hr more typical when annotators are asked to
fully specify. Requiring full specification also in-
troduces more errors in cases of annotator uncer-
tainty.
Table 2 shows the size of the GFL corpora that
were created. Typically, over 50% of the sentences
were not fully specified—the partial annotations
provided in these are useless to Turbo Parser un-
less the missing dependencies are imputed.
</bodyText>
<sectionHeader confidence="0.995483" genericHeader="method">
3 Gibbs Parse Completer (GPC)
</sectionHeader>
<subsectionHeader confidence="0.999818">
3.1 Gibbs sampler for CFG-DMV Model
</subsectionHeader>
<bodyText confidence="0.999825545454545">
CFG-DMV model The GPC is based on the
DMV model, a generative model for the unsu-
pervised learning of dependency structures (Klein
and Manning, 2004). We denote the input cor-
pus as ω = (ω1, · · · , ωN), where each ωs is a
sentence consisting of words and in a sentence ω,
word ωi has an corresponding part-of-speech tag
τi. We denote the set of all words as Vω and the
set of all parts-of-speech as VT. We use the part-
of-speech sequence as our terminal strings, result-
ing in an unlexicalized grammar. Dependencies
can be formulated as split head bilexical context
free grammars (CFGs) (Eisner and Satta, 1999)
and these bilexical CFGs require that each termi-
nal τi in sentence ω is represented in a split form
by two terminals, with labels marking the left and
right heads (τi,L, τi,R). Henceforth, we denote
w = w0,n as our terminals in the split-form of
sentence ω (e.g., the terminals for the dog walks
are DTL DTR NNL NNR VL VR). Table 1 shows
the grammar rules for the DMV model, from Klein
and Manning (2004).
</bodyText>
<page confidence="0.34416">
1387
</page>
<construct confidence="0.682782555555556">
Require: A is parent node of binary rule; wi,k is a valid
span of terminals and i + 1 &lt; k
function TREESAMPLER(A, i, k)
for i &lt; j &lt; k and pair of child nodes of A:B, C do
P (j, B, C) = θwA→BCc(i,j)c(j,k)·pB,i,j·pC,j,k
pA,i,k
end forSample j*, B*, C* from multinomial distri-
bution for (j, B, C) with probabilities calculated above
return j*, B*, C*
</construct>
<subsectionHeader confidence="0.523187">
end function
</subsectionHeader>
<bodyText confidence="0.965890947368421">
Algorithm 1: Sampling split position and rule to
expand parent node.
Gibbs sampler The split-head representation
encodes dependencies as a CFG. This enables the
use of a Gibbs sampler algorithm for estimating
PCFGs (Johnson et al., 2007; Sun et al., 2014),
and it is straightforward to incorporate constraints
from partial annotations into this sampler. To do
this, we modified the tree-sampling step to incor-
porate constraints derived from GFL annotations
and thereby impute the missing dependencies.
Given a string w = (w1, &apos; &apos; &apos; wn), we define
a span of w as wi,k = (wi+1, &apos; &apos; &apos; , wk), so that
w = w0,n. As introduced in Pereira and Schabes
(1992), a bracketing 13 of w is a finite set of spans
on w satisfying the requirement that no two spans
in a bracketing may overlap unless one span con-
tains the other. For each sentence w = w0,n we
define the auxiliary function for each span wi,j,
</bodyText>
<equation confidence="0.93251525">
0 &lt; i &lt; j &lt; n:
�
1 if span wi,j is valid for 13;
c(i, j) =0 otherwise.
</equation>
<bodyText confidence="0.999935">
Here one span is valid for 13 if it doesn’t cross
any brackets. Section 3.2 describes how to de-
rive bracketing information from GFL annotations
and how to determine if a span wi,j is valid or not.
Note that for parsing a corpus without any annota-
tions and constraints, c(i, j) = 1 for any span, and
the algorithm is equivalent to the Gibbs sampler in
Sun et al. (2014).
There are two parts to the tree-sampling. The
first constructs an inside table as in the Inside-
Outside algorithm for PCFGs and the second se-
lects the tree by recursively sampling productions
from top to bottom. Consider a sentence w, with
sub-spans wi,k = (wi+1, &apos; &apos; &apos; , wk). Given 0w
(modified rule probabilities 0 given constraints of
sentence w, see Section 3.2), we construct the in-
side table with entries pA,i,k for each nonterminal
and each span wi,k: 0 &lt; i &lt; k &lt; n. We introduce
</bodyText>
<equation confidence="0.7179355">
Require: Arcs is the set of all directed arcs extracted from
annotation for sentence w
function RULEPROB-SENT(w, θ, Arcs)
θw = θ
for each directed arc wi &lt; wj do
if i &lt; j then
for nonterminal A =� Lτj do
θwA→β = 0 if β contains Yτi
</equation>
<table confidence="0.891894454545454">
end for
else
for nonterminal A =� Rτj do
θwA→β = 0 if β contains Yτi
end for
end if
end for
return θw
end function
Algorithm 2: Modifying Rule Probabilities for w
to ensure parse tree contains all directed arcs.
</table>
<equation confidence="0.92650675">
c(i, j) into the calculation of inside probabilities:
pA,i,k = c(i, k)&apos;
� � 0wA→BC &apos; pB,i,j &apos; pC,j,k (1)
A→BCER i&lt;j&lt;k
</equation>
<bodyText confidence="0.99575175">
Here, pA,i,k = PGA(wi,k  |0w) is the probability
that terminals i through k were produced by the
non-terminal A, A —* BC E R are possible rules
to expand A. The inside table is computed recur-
sively using Equation 1.
The resulting inside probabilities are then used
to generate trees from the distribution of all valid
trees of the sentence. The tree is generated
from top to bottom recursively with the function
TreeSampler defined in Algorithm 1, which in-
troduces c(i, j) into the sampling function from
Sun et al. (2014).
</bodyText>
<subsectionHeader confidence="0.999177">
3.2 Constraints derived from GFL
</subsectionHeader>
<bodyText confidence="0.9366412">
We exploit one dependency constraint and two
constituency constraints from partial GFL anno-
tations.
Dependency rule Directed arcs are indicated
with angle brackets pointing from the dependent
to its head, e.g. black &gt; cat. Once we have a di-
rected arc annotation, say ωi &gt; ωj, if i &lt; j, which
means word j has a left child, we must have rule
L1τj —* YτiL&apos;τj in our parse tree (similarly if i &gt; j,
we have R1τ. —* R&apos; Yτi in our parse tree), where
</bodyText>
<sectionHeader confidence="0.327513" genericHeader="method">
7 j
</sectionHeader>
<bodyText confidence="0.9923185">
τi,τj are parts-of-speech for ωi and ωj. We en-
force this by modifying the rule probabilities for
sample sentence w to ensure that any sampled tree
contains all specified arcs.
</bodyText>
<figure confidence="0.486188">
1388
</figure>
<figureCaption confidence="0.997282">
Figure 2: Generating brackets for known head
Figure 3: Generating half brackets
</figureCaption>
<bodyText confidence="0.999967827586207">
Brackets GFL allows annotators to group words
with parenthesis, which provides an explicit indi-
cator of constituent brackets. Even when the inter-
nal structure is left underspecified (e.g. (the eq-
uity division*) in Figure 1 (a), the head is usu-
ally marked with *, and we can use this to infer
sub-constituents. Given such a set of parentheses
and the words inside them, we generate brackets
over the split-head representations of their parts-
of-speech, based on possible positions of the head.
Figure 2 shows how to generate brackets for three
situations: the head is the leftmost word, right-
most word, or is in a medial position. For exam-
ple, the first annotation indicates that under is the
head of under the agreement, and the rest of words
are right descendants of under. This leads to the
bracketing shown over the split-heads.
Half brackets We can also derive one-sided half
brackets from dependency arcs by assuming that
dependencies are projective. For example, in Fig-
ure 3, the annotation a &gt; dog specifies that dog
has a left child a, so we know that there is a right
bracket before the right-head of dog. Thus, we can
detect invalid spans using the half brackets; if a
span starts after a and ends after dog, this span is
invalid because it would result in crossing brack-
ets. This half bracketing is a unique advantage
provided by the split-head representation. The de-
tails of this algorithm are shown in Algorithm 3.
</bodyText>
<equation confidence="0.8940245">
Require: Arcs is the set of all directed arcs extracted for
sentence, wa,b is a span to detect
function DETECTINVALIDSPAN(a, b, Arcs)
for each directed arc wi &lt; wj do
</equation>
<construct confidence="0.8728835">
if i &lt; j then
if a &lt; 2i − 1 &lt; b &lt; 2j then
</construct>
<figure confidence="0.5826915">
c(a, b) = 0
end if
else
if 2j − 2 &lt; a &lt; 2i − 1 &lt; b then
c(i, j) = 0
end if
end if
end for
return c(a, b)
end function
</figure>
<figureCaption confidence="0.946524">
Algorithm 3: Detect whether one span is invalid
given all directed arcs.
Figure 4: Process of generating brackets and de-
tecting invalid spans.
</figureCaption>
<bodyText confidence="0.999985611111111">
We use both half bracket and full bracket infor-
mation, B, to determine whether a span is valid.
We set c(i, j) = 0 for all spans over w detected
by Algorithm 3 and violating B. Then, in the sam-
pling scheme, we’ll only sample parse trees that
satisfy these underlying constraints.
Figure 4 shows the resulting blocked out spans
in the chart based on both types of brackets for
the given partial annotation, which is Step 1 of the
process. The black dog is a constituent with dog
marked as its head, so we generate a full bracket
over the terminal string in Step 2. Also, barks has
a right child loudly; this generates a half bracket
before VR. In Step 3, the chart in Figure 4 repre-
sents all spans over terminal symbols. The cells in
black are invalid spans based on the full bracket,
and the hatched cells are invalid spans based on
the half bracket.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.983123">
Experiments There are two points of variation
to consider in empirical evaluations of our ap-
</bodyText>
<page confidence="0.721829">
1389
</page>
<figureCaption confidence="0.997876">
Figure 5: English oracle and degradation results
</figureCaption>
<bodyText confidence="0.99993545945946">
proach. The first is the effectiveness of the GPC
in imputing missing dependencies and the second
is the effectiveness of the GFL annotations them-
selves. Of particular note with respect to the latter
is the reasonable likelihood of divergence between
the annotator and the corpus used for evaluation—
for example, how coordination is handled and
whether subordinate verbs are dependents or heads
of auxiliary verbs. To this end, we perform simu-
lation experiments that remove increasing portions
of gold dependencies from a training corpus to un-
derstand imputation performance and annotation
experiments to evaluate the entire pipeline in a re-
alistically constrained annotation effort.
In that regard, one thing to consider are the part-
of-speech tags used by the unlexicalized GPC.
These do not come for free, so rather than ask
annotators to provide them, the raw sentences to
be annotated were tagged automatically. For En-
glish and Kinyarwanda, we used taggers trained
with resources built in under two hours (Garrette
and Baldridge, 2013), so these results are actually
constrained to the GFL annotation time plus two
hours. Such taggers were not available for Chinese
or Portuguese, so the Stanford tagger (Toutanova
et al., 2003) was used instead.
After imputing missing dependencies, the GPC
outputs fully sentences that are used to train Tur-
boParser (Martins et al., 2010). In all cases,
we compare to a right-branching baseline (RB).
Although comparing to a random baseline is
more typical of imputation experiments, a right-
branching baseline provides a stronger initial com-
parison. For the GFL annotation experiments, we
use two additional baselines. The first is simply
to use the sentences with full annotations and drop
any incomplete ones (GFL-DROP). The second is
</bodyText>
<table confidence="0.999512666666667">
Language ENG CHI POR
RB 25.0 11.6 27.0
GFL-GPC-25 58.7 33.5 60.2
GFL-GPC-50 75.0 46.1 71.4
GFL-GPC-75 77.8 50.1 73.7
Full 81.6 56.2 78.1
</table>
<tableCaption confidence="0.998975">
Table 3: Results with simulated partial annota-
</tableCaption>
<bodyText confidence="0.993558525">
tions, GFL-GPC-X indicates X percent of depen-
dencies were retained.
to make any partial annotations usable by assum-
ing a right-branching completion (GFL-RBC).
Simulated partial annotations Figure 5 shows
the learning curve with respect to number of an-
notated tokens when retaining 100%, 75%, 50%
and 25% of gold-standard training dependencies
and using the GPC to impute the removed ones.
With both 75% and 50% retained, performance
degrades gracefully. It is substantially lower for
25%, but the curve is steeper than the others, indi-
cating it is on track to catch up. Nonetheless, one
recommendation from these results is that it prob-
ably makes sense to start with a small number of
fully annotated sentences and then start mixing in
partially annotated ones.
Table 3 shows the attachment scores obtained
for English, Chinese, and Portuguese with varying
proportions of dependencies removed for the GPC
to impute.4 English and Portuguese hold up well
with 75% and 50% retained, while Chinese drops
more precipitously, and 25% leads to substantial
reductions in performance for all.
Note that these simulations indicate that, given
an equivalent number of total annotated arcs, using
the GPC is more beneficial than requiring annota-
tors to fully specify annotations. Imputing fifty
percent of the dependency arcs from sentences
containing 1000 tokens is typically more effective
by a few points than using the full gold-standard
arcs from sentences containing 500 tokens. Ac-
tually, this simulation is too generous to complete
annotations in that it leaves out consideration of
the time and effort required to obtain those 100%
full gold-standard arcs: it is often a small part of a
sentence that consumes the most effort when full
annotation is required. Additionally, these simula-
tion experiments randomly removed dependencies
while humans tend to annotate higher-level con-
</bodyText>
<note confidence="0.6145785">
4These are based on the same sentences used in the next
section’s GFL annotation experiments for each language.
</note>
<table confidence="0.989071333333333">
1390
Eval Length &lt; 10 &lt; 20 all
GFL-DROP (4hr) 54.5 55.0 52.6
GFL-GPC (4hr) 60.1 61.8 55.1
Blunsom and Cohn, 2010 67.7 – 55.7
Naseem et al., 2010 71.9 50.4 –
</table>
<tableCaption confidence="0.9510995">
Table 4: English results compared to previous un-
supervised and weakly-supervised methods.
</tableCaption>
<bodyText confidence="0.999895465116279">
stituents and leave internal structure (e.g. of noun
phrases) underspecified. Given Hwa’s (1999) find-
ings, we expect non-random partial annotations to
better serve as a basis for imputation.
GFL annotations We conducted three sets of
experiments with GFL annotations, evaluating on
sentences of all lengths, less than 10 words, and
less than 20 words. This was done to determine
the types of sentences that our method works best
on and to compare to previous work that evaluates
on sentences of different lengths.
Table 4 shows how our results on ENG com-
pare to others. Blunsom and Cohn (2010) rep-
resent state-of-the-art unsupervised results for all
lengths, while Naseem et al. (2010) was chosen as
a previous weakly-supervised approach. GFL-GPC
achieves similar results on the ‘all lengths’ crite-
rion as Blunsom and Cohn and substantially out-
performs Naseem et al. on sentences less than 20
words. Our poor performance on short sentences
is slightly surprising, and may result from an un-
even length distribution in the sentences selected
for annotation—we have only 3 training sentences
less than 10 words—as discussed by Spitkovsky et
al. (2010a). To correct this problem, both long and
short sentences should be included to construct a
more representative sample for annotation.
We did not expect GFL-RBC to perform so sim-
ilarly to RB. It is possible that the relatively large
number of under-specified sentences led to the
right-branching quality of GFL-RBC dominating,
rather than the more informative GFL annotations.
The results of the ENG annotation session can
be seen in Figure 6a. GFL-GPC is quite strong even
at thirty minutes, with only seven sentences anno-
tated. GFL-DROP picks up substantially at the end;
this may be in part explained by the fact that the
last block contained many short sentences, which
provide greater marginal benefit to GFL-DROP than
to GFL-GPC.
The learning curves for the other languages can
be seen in Figures 6b-6d, with a summary avail-
able in Table 5. Like ENG, CHI and POR both
</bodyText>
<table confidence="0.99869575">
Language KIN CHI POR
RB 52.6 11.6 27.0
GFL-DROP (2hr) 64.4 36.7 59.8
GFL-GPC (2hr) 64.5 38.8 65.0
</table>
<tableCaption confidence="0.997461">
Table 5: Non-English results summary
</tableCaption>
<bodyText confidence="0.999825711111111">
show clear wins for the GPC strategy. Of particu-
lar note is that the CHI annotations contained many
fewer fully-completed sentences (4) than the ENG
annotations (15). This somewhat addresses the
question raised by the 25% retention simulation
experiments—the GPC method improves results
over dropping partial annotations. The POR results
show a consistent strong win for GPC throughout.
The KIN results in Figure 6c exhibit a pattern
unlike the other languages; specifically, the KIN
data has a very high right-branching baseline (RB
in figures) and responds nearly identically for all
of the more informed methods. Upon investi-
gation, this appears to be an artifact of the data
used in KIN evaluation plus domain adaptation is-
sues. The gold data consists of transcribed natural
speech, whereas the training data consists of sen-
tences extracted from the Kinyarwanda Wikipedia.
All of the learning curves display a large ini-
tial jump after the first round of annotations. This
is encouraging for approaches that use annotated
sentences: just a small number of examples pro-
vide tremendous benefit, regardless of the strategy
employed.
Error analysis The primary errors seen on an
analysis of the GPC-completed sentences varies
somewhat between languages. The ENG data con-
tains many short sentences, consisting often of a
few words and a punctuation mark. Part of the
GFL convention is that the annotator is free to an-
notate punctuation as part of the sentence or in-
stead view it as extra-linguistic and drop the punc-
tuation from the annotation. Often the punctuation
in the ENG data went unannotated, with the result
being that the final parse model is not particularly
good at handling these types of sentences when
encountered in the test set.
Specific constructions like coordination and
possession also suffer a similar issue in that an-
notators (and corpora) varied slightly on how they
were handled. Thus, some languages like CHI
contained many of a particular type of error due
to mismatches in the conventions of the annota-
tor and corpus. Issues like this could have been
avoided by a longer training period prior to anno-
</bodyText>
<page confidence="0.510111">
1391
</page>
<figureCaption confidence="0.973714">
Figure 6: GPC results by annotation time for eval sentences of all lengths.
</figureCaption>
<figure confidence="0.9982885">
(a) English (b) Chinese
(c) Kinyarwanda (d) Portuguese
</figure>
<bodyText confidence="0.999964083333333">
tation, although were this areal annotation project,
there would be no existing corpus to compare to at
first. This brings up a more basic question of eval-
uation - one of usability versus representational
norm matching. It is likely that the GFL anno-
tations (and thus the models trained on them) di-
verge from the gold standard in what amount to
annotation conventions rather than substantive lin-
guistic divergences. To evaluate more fully or
fairly, we would need test sets produced by the
same set of annotators or an external, task-based
evaluation that uses the dependencies as in input.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999995791666667">
We have described a modeling strategy that takes
advantage of a Gibbs sampling algorithm for CFG
parsing plus constraints obtained from partial an-
notations to build dependency parsers. This strat-
egy’s performance improves on that of a parser
built only on the available complete annotations.
In doing so, our approach supports annotation ef-
forts that use GFL to obtain guidance from non-
expert human annotators and allow any annotator
to put in less effort than they would to do complete
annotations.
We find that a remarkably small amount of
supervised data can rival existing unsupervised
methods. While unsupervised methods have been
considered an attractive option for low-resource
parsing, they typically rely on large quantities of
clean, raw sentences. Our method uses less than
one hundred sentences, so in a truly low-resource
scenario, it has the potential to require much less
total effort. For instance, a single native speaker
could easily both generate and annotate the sen-
tences required for our method in a few hours,
while the many thousands of raw sentences needed
for state-of-the-art unsupervised methods could
</bodyText>
<page confidence="0.503422">
1392
</page>
<bodyText confidence="0.999973666666667">
take much longer to assemble if there is no ex-
isting corpus. This also means our method would
be useful for getting in-domain training data for
domain adaptation for parsers.
Finally, our method has the ability to encode
both universal grammar and test-language gram-
mar as a prior. This would be done by replacing
the uniform prior used in this paper with a prior
favoring those grammar rules during the updating-
rule-probabilities phase of the GPC, and would es-
sentially have the effect of weighting those gram-
mar rules.
</bodyText>
<sectionHeader confidence="0.997694" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994816">
This work was supported in part by the U. S. Army
Research Laboratory and the U. S. Army
Research Office under contract/grant number
W911NF-10-1-0533
</bodyText>
<sectionHeader confidence="0.995705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994602890625">
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
“Floresta sint´a(c)tica”: a Treebank for Portuguese.
In Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC),
pages 1698–1703. LREC.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1204–1213. Association for Com-
putational Linguistics.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th an-
nual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages
457–464. Association for Computational Linguis-
tics.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In HLT-NAACL, pages 138–147. Citeseer.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 73–79. Associ-
ation for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 139–146.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 478. Association for Com-
putational Linguistics.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A Smith. 2014. A dependency parser for
tweets. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Doha, Qatar, to appear.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational linguistics, 19(2):313–330.
David Marecek and Milan Straka. 2013. Stop-
probability estimates computed on a large corpus
improve unsupervised dependency parsing. In ACL
(1), pages 281–290.
Andr´e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M´ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approxi-
mate variational inference. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 34–44. Association for
Computational Linguistics.
Michael T. Mordowanec, Nathan Schneider, Chris
Dyer, and Noah A Smith. 2014. Simplified depen-
dency annotations with GFL-Web. ACL 2014, page
121.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1244. Asso-
ciation for Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th annual meeting
on Association for Computational Linguistics, pages
128–135. Association for Computational Linguis-
tics.
Nathan Schneider, Brendan OConnor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. LAW VII &amp; ID,
page 51.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From baby steps to leapfrog: How
less is more in unsupervised dependency parsing. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
751–759. Association for Computational Linguis-
tics.
1393
Valentin I Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1278–1287. Associa-
tion for Computational Linguistics.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011. Punctuation: Making a point in un-
supervised dependency parsing. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning, pages 19–28. Association for
Computational Linguistics.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2012. Three dependency-and-boundary
models for grammar induction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 688–698. Asso-
ciation for Computational Linguistics.
Liang Sun, Jason Mielens, and Jason Baldridge. 2014.
Parsing low-resource languages using Gibbs sam-
pling for PCFGs with latent annotations. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(02):207–238.
</reference>
<page confidence="0.838037">
1394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889417">
<title confidence="0.994782">Parse Imputation for Dependency Annotations</title>
<affiliation confidence="0.9593915">apos;Department of Linguistics &apos;Department of Mechanical Engineering The University of Texas at Austin The University of Texas at Austin</affiliation>
<email confidence="0.99768">sally722@utexas.edu</email>
<abstract confidence="0.998320578947368">Syntactic annotation is a hard task, but it can be made easier by allowing annotators flexibility to leave aspects of a sentence underspecified. Unfortunately, partial annotations are not typically directly usable for training parsers. We describe a method for imputing missing dependencies from sentences that have been partially annotated using the Graph Fragment Language, such that a standard dependency parser can then be trained on all annotations. We show that this strategy improves performance over not using partial annotations for English, Chinese, Portuguese and Kinyarwanda, and that performance competitive with state-of-the-art unsupervised and weakly-supervised parsers can be reached with just a few hours of annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta sint´a(c)tica”: a Treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1698--1703</pages>
<publisher>LREC.</publisher>
<contexts>
<context position="6709" citStr="Afonso et al., 2002" startWordPosition="1027" endWordPosition="1030">ing set. For CHI we use the Chinese Treebank (CTB5) (Xue et al., 2005), also converted to dependencies. The testing set consisted of files 1-40/900-931, and the sentences presented for GFL annotation were randomly sampled from files 81-899. The POR data is from 2The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-2015 Figure 1: GFL example for Mr. Conlon was executive vice president and director of the equity division. the CoNLL-X Shared Task on Multilingual Dependency Parsing and is derived from the Bosque portion of the Floresta sint´a(c)tica corpus (Afonso et al., 2002), using the standard provided splits for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from ann</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. “Floresta sint´a(c)tica”: a Treebank for Portuguese. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1698–1703. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised induction of tree substitution grammars for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1204--1213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1817" citStr="Blunsom and Cohn, 2010" startWordPosition="253" endWordPosition="256">efforts in which tooling, data, and expertise in the language are scarce. The challenge presented by this scarcity is compounded when doing deeper analysis, such as syntactic structure, which typically requires greater expertise and existing tooling. In such scenarios, unsupervised approaches are a tempting strategy. While the performance of unsupervised dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal de</context>
<context position="23234" citStr="Blunsom and Cohn, 2010" startWordPosition="3920" endWordPosition="3923">ally, this simulation is too generous to complete annotations in that it leaves out consideration of the time and effort required to obtain those 100% full gold-standard arcs: it is often a small part of a sentence that consumes the most effort when full annotation is required. Additionally, these simulation experiments randomly removed dependencies while humans tend to annotate higher-level con4These are based on the same sentences used in the next section’s GFL annotation experiments for each language. 1390 Eval Length &lt; 10 &lt; 20 all GFL-DROP (4hr) 54.5 55.0 52.6 GFL-GPC (4hr) 60.1 61.8 55.1 Blunsom and Cohn, 2010 67.7 – 55.7 Naseem et al., 2010 71.9 50.4 – Table 4: English results compared to previous unsupervised and weakly-supervised methods. stituents and leave internal structure (e.g. of noun phrases) underspecified. Given Hwa’s (1999) findings, we expect non-random partial annotations to better serve as a basis for imputation. GFL annotations We conducted three sets of experiments with GFL annotations, evaluating on sentences of all lengths, less than 10 words, and less than 20 words. This was done to determine the types of sentences that our method works best on and to compare to previous work t</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204–1213. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>457--464</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11851" citStr="Eisner and Satta, 1999" startWordPosition="1917" endWordPosition="1920"> CFG-DMV Model CFG-DMV model The GPC is based on the DMV model, a generative model for the unsupervised learning of dependency structures (Klein and Manning, 2004). We denote the input corpus as ω = (ω1, · · · , ωN), where each ωs is a sentence consisting of words and in a sentence ω, word ωi has an corresponding part-of-speech tag τi. We denote the set of all words as Vω and the set of all parts-of-speech as VT. We use the partof-speech sequence as our terminal strings, resulting in an unlexicalized grammar. Dependencies can be formulated as split head bilexical context free grammars (CFGs) (Eisner and Satta, 1999) and these bilexical CFGs require that each terminal τi in sentence ω is represented in a split form by two terminals, with labels marking the left and right heads (τi,L, τi,R). Henceforth, we denote w = w0,n as our terminals in the split-form of sentence ω (e.g., the terminals for the dog walks are DTL DTR NNL NNR VL VR). Table 1 shows the grammar rules for the DMV model, from Klein and Manning (2004). 1387 Require: A is parent node of binary rule; wi,k is a valid span of terminals and i + 1 &lt; k function TREESAMPLER(A, i, k) for i &lt; j &lt; k and pair of child nodes of A:B, C do P (j, B, C) = θwA</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 457–464. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>138--147</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="5410" citStr="Garrette and Baldridge, 2013" startWordPosition="820" endWordPosition="823">e performance than when it has to throw incomplete annotations out. Furthermore, the gains are even greater with less annotation time and it never hurts to use the GPC—so an annotation project can pursue a partial annotation strategy without undermining the utility of the work for parser training. This strategy has the further benefit of needing only a small number of sentences—in our case, under 100 sentences annotated in a 2-4 hour window. Furthermore, it relies on no outside tools or corpora other than a part-of-speech tagger; a resource that can be built with two hours of annotation time (Garrette and Baldridge, 2013). 2 Data Data sources We use four languages from three language families in an effort to both verify the cross-linguistic applicability of our approach, accounting for variations in linguistic properties, as well as to attempt to realistically simulate a realworld, low-resource environment. Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). For ENG we use the Penn Treebank (Marcus et al., 1993), converted into dependencies by the standard process. Section 23 was used as a test set, and a random sample of sentences from sections 02-21 were selected for an</context>
<context position="6981" citStr="Garrette and Baldridge (2013)" startWordPosition="1071" endWordPosition="1074">The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-2015 Figure 1: GFL example for Mr. Conlon was executive vice president and director of the equity division. the CoNLL-X Shared Task on Multilingual Dependency Parsing and is derived from the Bosque portion of the Floresta sint´a(c)tica corpus (Afonso et al., 2002), using the standard provided splits for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from annotators who have minimal training. Kong et al. (2014) demonstrate the feasibility of training a dependency parser based on a GFL-annotated corpus of English tweets. An example of GFL is shown in Figure 1: (a) is the GFL markup itself and (b) is a graphical representation </context>
<context position="20215" citStr="Garrette and Baldridge, 2013" startWordPosition="3439" endWordPosition="3442">rbs. To this end, we perform simulation experiments that remove increasing portions of gold dependencies from a training corpus to understand imputation performance and annotation experiments to evaluate the entire pipeline in a realistically constrained annotation effort. In that regard, one thing to consider are the partof-speech tags used by the unlexicalized GPC. These do not come for free, so rather than ask annotators to provide them, the raw sentences to be annotated were tagged automatically. For English and Kinyarwanda, we used taggers trained with resources built in under two hours (Garrette and Baldridge, 2013), so these results are actually constrained to the GFL annotation time plus two hours. Such taggers were not available for Chinese or Portuguese, so the Stanford tagger (Toutanova et al., 2003) was used instead. After imputing missing dependencies, the GPC outputs fully sentences that are used to train TurboParser (Martins et al., 2010). In all cases, we compare to a right-branching baseline (RB). Although comparing to a random baseline is more typical of imputation experiments, a rightbranching baseline provides a stronger initial comparison. For the GFL annotation experiments, we use two add</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In HLT-NAACL, pages 138–147. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Supervised grammar induction using training data with limited constituent information.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>73--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3187" citStr="Hwa (1999)" startWordPosition="474" endWordPosition="475">tional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The first stage completes the partial GFL annotations by adapting a Gibbs tree sampler (Johnson et al., 2007; Sun et al., 2014). The GFL annotations constrain the tree sampling space by using both dependencies and the constituent boundaries they express. The system performs missing dependency arc imputation using Gibbs sampling – we refer to this appro</context>
</contexts>
<marker>Hwa, 1999</marker>
<rawString>Rebecca Hwa. 1999. Supervised grammar induction using training data with limited constituent information. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 73–79. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="3541" citStr="Johnson et al., 2007" startWordPosition="524" endWordPosition="527">t obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The first stage completes the partial GFL annotations by adapting a Gibbs tree sampler (Johnson et al., 2007; Sun et al., 2014). The GFL annotations constrain the tree sampling space by using both dependencies and the constituent boundaries they express. The system performs missing dependency arc imputation using Gibbs sampling – we refer to this approach 1In fact, standardized writing systems have yet to be adopted for some languages. 1385 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1385–1394, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics </context>
<context position="12860" citStr="Johnson et al., 2007" startWordPosition="2101" endWordPosition="2104">d Manning (2004). 1387 Require: A is parent node of binary rule; wi,k is a valid span of terminals and i + 1 &lt; k function TREESAMPLER(A, i, k) for i &lt; j &lt; k and pair of child nodes of A:B, C do P (j, B, C) = θwA→BCc(i,j)c(j,k)·pB,i,j·pC,j,k pA,i,k end forSample j*, B*, C* from multinomial distribution for (j, B, C) with probabilities calculated above return j*, B*, C* end function Algorithm 1: Sampling split position and rule to expand parent node. Gibbs sampler The split-head representation encodes dependencies as a CFG. This enables the use of a Gibbs sampler algorithm for estimating PCFGs (Johnson et al., 2007; Sun et al., 2014), and it is straightforward to incorporate constraints from partial annotations into this sampler. To do this, we modified the tree-sampling step to incorporate constraints derived from GFL annotations and thereby impute the missing dependencies. Given a string w = (w1, &apos; &apos; &apos; wn), we define a span of w as wi,k = (wi+1, &apos; &apos; &apos; , wk), so that w = w0,n. As introduced in Pereira and Schabes (1992), a bracketing 13 of w is a finite set of spans on w satisfying the requirement that no two spans in a bracketing may overlap unless one span contains the other. For each sentence w = w0</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>478</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8711" citStr="Klein and Manning (2004)" startWordPosition="1385" endWordPosition="1388">n, indicating they are grouped together but otherwise underspecified. 1386 CFG Rule EVG distribution Description S → YH P(root = H) The head of the sentence is H YH → LHRH - Split-head representation LH → HL P(STOP|dir = L, head = H, val = 0) H has no left children LH → L1 P(CONT |dir = L, head = H, val = 0) H has at least one left child H L&apos;&apos; H → HL P(STOP|dir = L, head = H, val = 1) H has no more left children L&apos;H → L1 P(CONT |dir = L, head = H, val = 1) H has other left children H L1H → YAL&apos;H P(ArgA|dir = L, head = H, val = 1) A is a left child of H Table 1: The CFG-DMV grammar schema from Klein and Manning (2004). Note that in these rules H and A are parts-of-speech. For brevity, we omit the portion of the grammar that handles the righthand arguments since they are symmetric to the left. Valency (val) can take the value 1 (we have made attachments in the direction (dir) d) or 0 (not). CHI ENG KIN POR Sentences Annotated 24 34 69 63 Tokens Annotated 820 798 988 1067 Fully Specified Sentences 4 15 31 20 Table 2: Two Hour GFL Annotation Statistics Kong et al. (2014) stipulate that the GFL annotations in their corpus must be fully-specified. They are thus unable to take advantage of such underspecified se</context>
<context position="11391" citStr="Klein and Manning, 2004" startWordPosition="1831" endWordPosition="1834">s of 150-200 tokens/hr more typical when annotators are asked to fully specify. Requiring full specification also introduces more errors in cases of annotator uncertainty. Table 2 shows the size of the GFL corpora that were created. Typically, over 50% of the sentences were not fully specified—the partial annotations provided in these are useless to Turbo Parser unless the missing dependencies are imputed. 3 Gibbs Parse Completer (GPC) 3.1 Gibbs sampler for CFG-DMV Model CFG-DMV model The GPC is based on the DMV model, a generative model for the unsupervised learning of dependency structures (Klein and Manning, 2004). We denote the input corpus as ω = (ω1, · · · , ωN), where each ωs is a sentence consisting of words and in a sentence ω, word ωi has an corresponding part-of-speech tag τi. We denote the set of all words as Vω and the set of all parts-of-speech as VT. We use the partof-speech sequence as our terminal strings, resulting in an unlexicalized grammar. Dependencies can be formulated as split head bilexical context free grammars (CFGs) (Eisner and Satta, 1999) and these bilexical CFGs require that each terminal τi in sentence ω is represented in a split form by two terminals, with labels marking t</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Doha, Qatar,</location>
<note>to appear.</note>
<contexts>
<context position="7362" citStr="Kong et al. (2014)" startWordPosition="1128" endWordPosition="1131">s for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from annotators who have minimal training. Kong et al. (2014) demonstrate the feasibility of training a dependency parser based on a GFL-annotated corpus of English tweets. An example of GFL is shown in Figure 1: (a) is the GFL markup itself and (b) is a graphical representation of the dependencies it encodes. Figure 1 specifies several dependencies: of is a dependent of director, executive vice president and director are conjuncts and and is the coordinator. However, the complete internal structure of the phrase the equity division remains unspecified, other than division being marked as the head (via an asterisk).3 Finally, Mr. Conlon in square bracke</context>
<context position="9170" citStr="Kong et al. (2014)" startWordPosition="1470" endWordPosition="1473">s other left children H L1H → YAL&apos;H P(ArgA|dir = L, head = H, val = 1) A is a left child of H Table 1: The CFG-DMV grammar schema from Klein and Manning (2004). Note that in these rules H and A are parts-of-speech. For brevity, we omit the portion of the grammar that handles the righthand arguments since they are symmetric to the left. Valency (val) can take the value 1 (we have made attachments in the direction (dir) d) or 0 (not). CHI ENG KIN POR Sentences Annotated 24 34 69 63 Tokens Annotated 820 798 988 1067 Fully Specified Sentences 4 15 31 20 Table 2: Two Hour GFL Annotation Statistics Kong et al. (2014) stipulate that the GFL annotations in their corpus must be fully-specified. They are thus unable to take advantage of such underspecified sentences, and we address that limitation in this paper. From the GFL annotations we can extract and deduce dependency arcs and constraints (see Section 3.2 for full details) in order to guide the Gibbs sampling process. Time-bounded annotation As described in Section 1, a primary goal of this work was to consider the time in which a useful number of dependency tree annotations might be collected, such as might be required during the initial phase of a lang</context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A Smith. 2014. A dependency parser for tweets. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="5847" citStr="Marcus et al., 1993" startWordPosition="888" endWordPosition="891">Furthermore, it relies on no outside tools or corpora other than a part-of-speech tagger; a resource that can be built with two hours of annotation time (Garrette and Baldridge, 2013). 2 Data Data sources We use four languages from three language families in an effort to both verify the cross-linguistic applicability of our approach, accounting for variations in linguistic properties, as well as to attempt to realistically simulate a realworld, low-resource environment. Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). For ENG we use the Penn Treebank (Marcus et al., 1993), converted into dependencies by the standard process. Section 23 was used as a test set, and a random sample of sentences from sections 02-21 were selected for annotation with GFL as described below and subsequently used as the minimal training set. For CHI we use the Chinese Treebank (CTB5) (Xue et al., 2005), also converted to dependencies. The testing set consisted of files 1-40/900-931, and the sentences presented for GFL annotation were randomly sampled from files 81-899. The POR data is from 2The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-20</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Marecek</author>
<author>Milan Straka</author>
</authors>
<title>Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>281--290</pages>
<marker>Marecek, Straka, 2013</marker>
<rawString>David Marecek and Milan Straka. 2013. Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing. In ACL (1), pages 281–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro MQ Aguiar</author>
<author>M´ario AT Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>34--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1767" citStr="Martins et al., 2010" startWordPosition="245" endWordPosition="248">ffort, particularly for language documentation efforts in which tooling, data, and expertise in the language are scarce. The challenge presented by this scarcity is compounded when doing deeper analysis, such as syntactic structure, which typically requires greater expertise and existing tooling. In such scenarios, unsupervised approaches are a tempting strategy. While the performance of unsupervised dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowled</context>
<context position="4284" citStr="Martins et al., 2010" startWordPosition="637" endWordPosition="640">boundaries they express. The system performs missing dependency arc imputation using Gibbs sampling – we refer to this approach 1In fact, standardized writing systems have yet to be adopted for some languages. 1385 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1385–1394, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics as the Gibbs Parse Completer2 (GPC). The second stage uses the full dependencies output by the GPC to train Turbo Parser (Martins et al., 2010), and evaluation is done with this trained model on unseen sentences. In simulation experiments for English, Chinese and Portuguese, we show that the method gracefully degrades when applied to training corpora with increasing percentages of the gold training dependencies removed. We also do actual GFL annotations for those languages plus Kinyarwanda, and show that using the GPC to fill in the missing dependencies after two hours of annotation enables Turbo Parser to obtain 2-6% better absolute performance than when it has to throw incomplete annotations out. Furthermore, the gains are even gre</context>
<context position="20553" citStr="Martins et al., 2010" startWordPosition="3493" endWordPosition="3496"> the unlexicalized GPC. These do not come for free, so rather than ask annotators to provide them, the raw sentences to be annotated were tagged automatically. For English and Kinyarwanda, we used taggers trained with resources built in under two hours (Garrette and Baldridge, 2013), so these results are actually constrained to the GFL annotation time plus two hours. Such taggers were not available for Chinese or Portuguese, so the Stanford tagger (Toutanova et al., 2003) was used instead. After imputing missing dependencies, the GPC outputs fully sentences that are used to train TurboParser (Martins et al., 2010). In all cases, we compare to a right-branching baseline (RB). Although comparing to a random baseline is more typical of imputation experiments, a rightbranching baseline provides a stronger initial comparison. For the GFL annotation experiments, we use two additional baselines. The first is simply to use the sentences with full annotations and drop any incomplete ones (GFL-DROP). The second is Language ENG CHI POR RB 25.0 11.6 27.0 GFL-GPC-25 58.7 33.5 60.2 GFL-GPC-50 75.0 46.1 71.4 GFL-GPC-75 77.8 50.1 73.7 Full 81.6 56.2 78.1 Table 3: Results with simulated partial annotations, GFL-GPC-X i</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and M´ario AT Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael T Mordowanec</author>
<author>Nathan Schneider</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<date>2014</date>
<booktitle>Simplified dependency annotations with GFL-Web. ACL 2014,</booktitle>
<pages>121</pages>
<contexts>
<context position="2864" citStr="Mordowanec et al., 2014" startWordPosition="423" endWordPosition="426">) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The first stage completes the p</context>
</contexts>
<marker>Mordowanec, Schneider, Dyer, Smith, 2014</marker>
<rawString>Michael T. Mordowanec, Nathan Schneider, Chris Dyer, and Noah A Smith. 2014. Simplified dependency annotations with GFL-Web. ACL 2014, page 121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1234--1244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2061" citStr="Naseem et al. (2010)" startWordPosition="294" endWordPosition="297">ng. In such scenarios, unsupervised approaches are a tempting strategy. While the performance of unsupervised dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotat</context>
<context position="23266" citStr="Naseem et al., 2010" startWordPosition="3927" endWordPosition="3930">us to complete annotations in that it leaves out consideration of the time and effort required to obtain those 100% full gold-standard arcs: it is often a small part of a sentence that consumes the most effort when full annotation is required. Additionally, these simulation experiments randomly removed dependencies while humans tend to annotate higher-level con4These are based on the same sentences used in the next section’s GFL annotation experiments for each language. 1390 Eval Length &lt; 10 &lt; 20 all GFL-DROP (4hr) 54.5 55.0 52.6 GFL-GPC (4hr) 60.1 61.8 55.1 Blunsom and Cohn, 2010 67.7 – 55.7 Naseem et al., 2010 71.9 50.4 – Table 4: English results compared to previous unsupervised and weakly-supervised methods. stituents and leave internal structure (e.g. of noun phrases) underspecified. Given Hwa’s (1999) findings, we expect non-random partial annotations to better serve as a basis for imputation. GFL annotations We conducted three sets of experiments with GFL annotations, evaluating on sentences of all lengths, less than 10 words, and less than 20 words. This was done to determine the types of sentences that our method works best on and to compare to previous work that evaluates on sentences of di</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234–1244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13274" citStr="Pereira and Schabes (1992)" startWordPosition="2177" endWordPosition="2180">lit position and rule to expand parent node. Gibbs sampler The split-head representation encodes dependencies as a CFG. This enables the use of a Gibbs sampler algorithm for estimating PCFGs (Johnson et al., 2007; Sun et al., 2014), and it is straightforward to incorporate constraints from partial annotations into this sampler. To do this, we modified the tree-sampling step to incorporate constraints derived from GFL annotations and thereby impute the missing dependencies. Given a string w = (w1, &apos; &apos; &apos; wn), we define a span of w as wi,k = (wi+1, &apos; &apos; &apos; , wk), so that w = w0,n. As introduced in Pereira and Schabes (1992), a bracketing 13 of w is a finite set of spans on w satisfying the requirement that no two spans in a bracketing may overlap unless one span contains the other. For each sentence w = w0,n we define the auxiliary function for each span wi,j, 0 &lt; i &lt; j &lt; n: � 1 if span wi,j is valid for 13; c(i, j) =0 otherwise. Here one span is valid for 13 if it doesn’t cross any brackets. Section 3.2 describes how to derive bracketing information from GFL annotations and how to determine if a span wi,j is valid or not. Note that for parsing a corpus without any annotations and constraints, c(i, j) = 1 for an</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th annual meeting on Association for Computational Linguistics, pages 128–135. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Brendan OConnor</author>
<author>Naomi Saphra</author>
<author>David Bamman</author>
<author>Manaal Faruqui</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
<author>Jason Baldridge</author>
</authors>
<title>A framework for (under) specifying dependency syntax without overloading annotators.</title>
<date>2013</date>
<journal>LAW VII &amp; ID,</journal>
<pages>51</pages>
<contexts>
<context position="2838" citStr="Schneider et al., 2013" startWordPosition="419" endWordPosition="422"> of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The f</context>
<context position="7162" citStr="Schneider et al., 2013" startWordPosition="1101" endWordPosition="1104">ity division. the CoNLL-X Shared Task on Multilingual Dependency Parsing and is derived from the Bosque portion of the Floresta sint´a(c)tica corpus (Afonso et al., 2002), using the standard provided splits for training and testing. The KIN data is a corpus consisting of transcripts of testimonies by survivors of the Rwandan genocide, provided by the Kigali Genocide Memorial Center – this data is described by Garrette and Baldridge (2013). GFL annotation We use a small number of sentences annotated using the Graph Fragment Language (GFL), a simple ASCII markup language for dependency grammar (Schneider et al., 2013). Unlike traditional syntactic annotation strategies requiring trained annotators and great effort, rapid GFL annotations can be collected from annotators who have minimal training. Kong et al. (2014) demonstrate the feasibility of training a dependency parser based on a GFL-annotated corpus of English tweets. An example of GFL is shown in Figure 1: (a) is the GFL markup itself and (b) is a graphical representation of the dependencies it encodes. Figure 1 specifies several dependencies: of is a dependent of director, executive vice president and director are conjuncts and and is the coordinato</context>
</contexts>
<marker>Schneider, OConnor, Saphra, Bamman, Faruqui, Smith, Dyer, Baldridge, 2013</marker>
<rawString>Nathan Schneider, Brendan OConnor, Naomi Saphra, David Bamman, Manaal Faruqui, Noah A Smith, Chris Dyer, and Jason Baldridge. 2013. A framework for (under) specifying dependency syntax without overloading annotators. LAW VII &amp; ID, page 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: How less is more in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>751--759</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2529" citStr="Spitkovsky et al. (2010" startWordPosition="371" endWordPosition="374">blem for some languages, many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most u</context>
<context position="24516" citStr="Spitkovsky et al. (2010" startWordPosition="4126" endWordPosition="4129">ows how our results on ENG compare to others. Blunsom and Cohn (2010) represent state-of-the-art unsupervised results for all lengths, while Naseem et al. (2010) was chosen as a previous weakly-supervised approach. GFL-GPC achieves similar results on the ‘all lengths’ criterion as Blunsom and Cohn and substantially outperforms Naseem et al. on sentences less than 20 words. Our poor performance on short sentences is slightly surprising, and may result from an uneven length distribution in the sentences selected for annotation—we have only 3 training sentences less than 10 words—as discussed by Spitkovsky et al. (2010a). To correct this problem, both long and short sentences should be included to construct a more representative sample for annotation. We did not expect GFL-RBC to perform so similarly to RB. It is possible that the relatively large number of under-specified sentences led to the right-branching quality of GFL-RBC dominating, rather than the more informative GFL annotations. The results of the ENG annotation session can be seen in Figure 6a. GFL-GPC is quite strong even at thirty minutes, with only seven sentences annotated. GFL-DROP picks up substantially at the end; this may be in part expla</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010a. From baby steps to leapfrog: How less is more in unsupervised dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 751–759. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Daniel Jurafsky</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1278--1287</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2529" citStr="Spitkovsky et al. (2010" startWordPosition="371" endWordPosition="374">blem for some languages, many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) use a set of universal dependency rules and obtain substantial gains over unsupervised methods in many languages. Spitkovsky et al. (2010b; 2011) use web mark-up and punctuation as additional annotations. Alternatively, one could try to obtain actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations easier for experts and possible for novices (Schneider et al., 2013; Mordowanec et al., 2014). GFL supports partial annotations, so annotators can omit obvious dependencies or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most u</context>
<context position="24516" citStr="Spitkovsky et al. (2010" startWordPosition="4126" endWordPosition="4129">ows how our results on ENG compare to others. Blunsom and Cohn (2010) represent state-of-the-art unsupervised results for all lengths, while Naseem et al. (2010) was chosen as a previous weakly-supervised approach. GFL-GPC achieves similar results on the ‘all lengths’ criterion as Blunsom and Cohn and substantially outperforms Naseem et al. on sentences less than 20 words. Our poor performance on short sentences is slightly surprising, and may result from an uneven length distribution in the sentences selected for annotation—we have only 3 training sentences less than 10 words—as discussed by Spitkovsky et al. (2010a). To correct this problem, both long and short sentences should be included to construct a more representative sample for annotation. We did not expect GFL-RBC to perform so similarly to RB. It is possible that the relatively large number of under-specified sentences led to the right-branching quality of GFL-RBC dominating, rather than the more informative GFL annotations. The results of the ENG annotation session can be seen in Figure 6a. GFL-GPC is quite strong even at thirty minutes, with only seven sentences annotated. GFL-DROP picks up substantially at the end; this may be in part expla</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>Valentin I Spitkovsky, Daniel Jurafsky, and Hiyan Alshawi. 2010b. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1278–1287. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Punctuation: Making a point in unsupervised dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>pages</pages>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011. Punctuation: Making a point in unsupervised dependency parsing. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 19–28. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Three dependency-and-boundary models for grammar induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>688--698</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1792" citStr="Spitkovsky et al., 2012" startWordPosition="249" endWordPosition="252">r language documentation efforts in which tooling, data, and expertise in the language are scarce. The challenge presented by this scarcity is compounded when doing deeper analysis, such as syntactic structure, which typically requires greater expertise and existing tooling. In such scenarios, unsupervised approaches are a tempting strategy. While the performance of unsupervised dependency parsing has improved greatly since Klein and Manning’s (2004) Dependency Model with Valence (DMV), state-of-the-art unsupervised parsers still perform well below supervised approaches (Martins et al., 2010; Spitkovsky et al., 2012; Blunsom and Cohn, 2010). Additionally, they typically require large amounts of raw data. While this is not a problem for some languages, many of the world’s languages do not have a clean, digitized corpus available.1 For instance, the approach of Naseem et al. (2010) is unsupervised in the sense that it requires no dependency annotations, but it still makes use of the raw version of the full Penn Treebank. The approach of Marecek et al. (2013) requires extra unlabeled texts to estimate parameters. Another strategy is to exploit small amounts of supervision or knowledge. Naseem et al. (2010) </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2012</marker>
<rawString>Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2012. Three dependency-and-boundary models for grammar induction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 688–698. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Sun</author>
<author>Jason Mielens</author>
<author>Jason Baldridge</author>
</authors>
<title>Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3560" citStr="Sun et al., 2014" startWordPosition="528" endWordPosition="531"> or skip difficult constructions. The ability to focus on portions of a sentence frees the annotator to target constituents and dependencies that maximize information that will be most useful for machine-learned parsers. For example, Hwa (1999) found higher-level sentence constituents to be more informative for learning parsers than lower-level ones. To support this style of annotation while getting the benefit from partial annotations, we develop a two-stage parser learning strategy. The first stage completes the partial GFL annotations by adapting a Gibbs tree sampler (Johnson et al., 2007; Sun et al., 2014). The GFL annotations constrain the tree sampling space by using both dependencies and the constituent boundaries they express. The system performs missing dependency arc imputation using Gibbs sampling – we refer to this approach 1In fact, standardized writing systems have yet to be adopted for some languages. 1385 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1385–1394, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics as the Gibbs Parse </context>
<context position="12879" citStr="Sun et al., 2014" startWordPosition="2105" endWordPosition="2108"> Require: A is parent node of binary rule; wi,k is a valid span of terminals and i + 1 &lt; k function TREESAMPLER(A, i, k) for i &lt; j &lt; k and pair of child nodes of A:B, C do P (j, B, C) = θwA→BCc(i,j)c(j,k)·pB,i,j·pC,j,k pA,i,k end forSample j*, B*, C* from multinomial distribution for (j, B, C) with probabilities calculated above return j*, B*, C* end function Algorithm 1: Sampling split position and rule to expand parent node. Gibbs sampler The split-head representation encodes dependencies as a CFG. This enables the use of a Gibbs sampler algorithm for estimating PCFGs (Johnson et al., 2007; Sun et al., 2014), and it is straightforward to incorporate constraints from partial annotations into this sampler. To do this, we modified the tree-sampling step to incorporate constraints derived from GFL annotations and thereby impute the missing dependencies. Given a string w = (w1, &apos; &apos; &apos; wn), we define a span of w as wi,k = (wi+1, &apos; &apos; &apos; , wk), so that w = w0,n. As introduced in Pereira and Schabes (1992), a bracketing 13 of w is a finite set of spans on w satisfying the requirement that no two spans in a bracketing may overlap unless one span contains the other. For each sentence w = w0,n we define the au</context>
<context position="15526" citStr="Sun et al. (2014)" startWordPosition="2605" endWordPosition="2608">lation of inside probabilities: pA,i,k = c(i, k)&apos; � � 0wA→BC &apos; pB,i,j &apos; pC,j,k (1) A→BCER i&lt;j&lt;k Here, pA,i,k = PGA(wi,k |0w) is the probability that terminals i through k were produced by the non-terminal A, A —* BC E R are possible rules to expand A. The inside table is computed recursively using Equation 1. The resulting inside probabilities are then used to generate trees from the distribution of all valid trees of the sentence. The tree is generated from top to bottom recursively with the function TreeSampler defined in Algorithm 1, which introduces c(i, j) into the sampling function from Sun et al. (2014). 3.2 Constraints derived from GFL We exploit one dependency constraint and two constituency constraints from partial GFL annotations. Dependency rule Directed arcs are indicated with angle brackets pointing from the dependent to its head, e.g. black &gt; cat. Once we have a directed arc annotation, say ωi &gt; ωj, if i &lt; j, which means word j has a left child, we must have rule L1τj —* YτiL&apos;τj in our parse tree (similarly if i &gt; j, we have R1τ. —* R&apos; Yτi in our parse tree), where 7 j τi,τj are parts-of-speech for ωi and ωj. We enforce this by modifying the rule probabilities for sample sentence w t</context>
</contexts>
<marker>Sun, Mielens, Baldridge, 2014</marker>
<rawString>Liang Sun, Jason Mielens, and Jason Baldridge. 2014. Parsing low-resource languages using Gibbs sampling for PCFGs with latent annotations. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20408" citStr="Toutanova et al., 2003" startWordPosition="3470" endWordPosition="3473">uate the entire pipeline in a realistically constrained annotation effort. In that regard, one thing to consider are the partof-speech tags used by the unlexicalized GPC. These do not come for free, so rather than ask annotators to provide them, the raw sentences to be annotated were tagged automatically. For English and Kinyarwanda, we used taggers trained with resources built in under two hours (Garrette and Baldridge, 2013), so these results are actually constrained to the GFL annotation time plus two hours. Such taggers were not available for Chinese or Portuguese, so the Stanford tagger (Toutanova et al., 2003) was used instead. After imputing missing dependencies, the GPC outputs fully sentences that are used to train TurboParser (Martins et al., 2010). In all cases, we compare to a right-branching baseline (RB). Although comparing to a random baseline is more typical of imputation experiments, a rightbranching baseline provides a stronger initial comparison. For the GFL annotation experiments, we use two additional baselines. The first is simply to use the sentences with full annotations and drop any incomplete ones (GFL-DROP). The second is Language ENG CHI POR RB 25.0 11.6 27.0 GFL-GPC-25 58.7 3</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Marta Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering,</title>
<date>2005</date>
<pages>11--02</pages>
<contexts>
<context position="6159" citStr="Xue et al., 2005" startWordPosition="943" endWordPosition="946">ity of our approach, accounting for variations in linguistic properties, as well as to attempt to realistically simulate a realworld, low-resource environment. Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). For ENG we use the Penn Treebank (Marcus et al., 1993), converted into dependencies by the standard process. Section 23 was used as a test set, and a random sample of sentences from sections 02-21 were selected for annotation with GFL as described below and subsequently used as the minimal training set. For CHI we use the Chinese Treebank (CTB5) (Xue et al., 2005), also converted to dependencies. The testing set consisted of files 1-40/900-931, and the sentences presented for GFL annotation were randomly sampled from files 81-899. The POR data is from 2The software, instructions, and data are available at http://www.github.com/jmielens/gpc-acl-2015 Figure 1: GFL example for Mr. Conlon was executive vice president and director of the equity division. the CoNLL-X Shared Task on Multilingual Dependency Parsing and is derived from the Bosque portion of the Floresta sint´a(c)tica corpus (Afonso et al., 2002), using the standard provided splits for training </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering, 11(02):207–238.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>