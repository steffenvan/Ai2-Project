<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000395">
<title confidence="0.988462">
Modeling Filled Pauses in Medical Dictations
</title>
<author confidence="0.979233">
Sergey V. Pakhomov
</author>
<affiliation confidence="0.999442">
University of Minnesota
</affiliation>
<address confidence="0.7183755">
190 Klaeber Court
320-16th Ave. S.E. Minneapolis, MN 55455
</address>
<email confidence="0.997082">
palch0002@tc.umn.edu
</email>
<sectionHeader confidence="0.993837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912125">
Filled pauses are characteristic of
spontaneous speech and can present
considerable problems for speech
recognition by being often recognized as
short words. An um can be recognized as
thumb or arm if the recognizer&apos;s language
model does not adequately represent FP&apos;s.
Recognition of quasi-spontaneous speech
(medical dictation) is subject to this problem
as well. Results from medical dictations by
21 family practice physicians show that
using an FP model trained on the corpus
populated with FP&apos;s produces overall better
results than a model trained on a corpus that
excluded FP&apos;s or a corpus that had random
FP&apos;s.
</bodyText>
<sectionHeader confidence="0.961469" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.9999495">
Filled pauses (FP&apos;s), false starts, repetitions,
fragments, etc. are characteristic of
spontaneous speech and can present
considerable problems for speech
recognition. FP&apos;s are often recognized as
short words of similar phonetic quality. For
example, an um can be recognized as thumb
or arm if the recognizer&apos;s language model
does not adequately represent FP&apos;s.
Recognition of quasi-spontaneous speech
(medical dictation) is subject to this problem
as well. The FP problem becomes
especially pertinent where the corpora used
to build language models are compiled from
text with no FP&apos;s. Shriberg (1996) has
shown that representing FP&apos;s in a language
model helps decrease the model&apos;s
perplexity. She finds that when a FP occurs
at a major phrase or discourse boundary, the
FP itself is the best predictor of the
following lexical material; conversely, in a
non-boundary context, FP&apos;s are predictable
from the preceding words. Shriberg (1994)
shows that the rate of disfluencies grows
exponentially with the length of the
sentence, and that FP&apos;s occur more often in
the initial position (see also Swerts (1996)).
This paper presents a method of using
bigram probabilities for extracting FP
distribution from a corpus of hand-
transcribed data. The resulting bigram
model is used to populate another training
corpus that originally had no FP&apos;s. Results
from medical dictations by 21 family
practice physicians show that using an FP
model trained on the corpus populated with
FP&apos;s produces overall better results than a
model trained on a corpus that excluded
FP&apos;s or a corpus that had random FP&apos;s.
Recognition accuracy improves
proportionately to the frequency of FP&apos;s in
the speech.
</bodyText>
<sectionHeader confidence="0.946701" genericHeader="method">
1. Filled Pauses
</sectionHeader>
<bodyText confidence="0.999827909090909">
FP&apos;s are not random events, but have a
systematic distribution and well-defined
functions in discourse. (Shriberg and
Stolcke 1996, Shriberg 1994, Swerts 1996,
Macalay and Osgood 1959, Cook 1970,
Cook and Lalljee 1970, Christenfeld, et al.
1991) Cook and Lalljee (1970) make an
interesting proposal that FP&apos;s may have
something to do with the listener&apos;s
perception of disfluent speech. They
suggest that speech may be more
</bodyText>
<page confidence="0.998442">
619
</page>
<bodyText confidence="0.9999302">
comprehensible when it contains filler
material during hesitations by preserving
continuity and that a FP may serve as a
signal to draw the listeners attention to the
next utterance in order for the listener not to
lose the onset of the following utterance.
Perhaps, from the point of view of
perception, FP&apos;s are not disfluent events at
all. This proposal bears directly on the
domain of medical dictations, since many
doctors who use old voice operated
equipment train themselves to use FP&apos;s
instead of silent pauses, so that the recorder
wouldn&apos;t cut off the beginning of the post
pause utterance.
</bodyText>
<sectionHeader confidence="0.945616" genericHeader="method">
2. Quasi-spontaneous speech
</sectionHeader>
<bodyText confidence="0.968996933333333">
Family practice medical dictations tend to be
pre-planned and follow an established
SOAP format: (Subjective (informal
observations), Objective (examination),
Assessment (diagnosis) and Plan (treatment
plan)). Despite that, doctors vary greatly in
how frequently they use FP&apos;s, which agrees
with Cook and Lalljee&apos;s (1970) fmdings of
no correlation between FP use and the mode
of discourse. Audience awareness may also
play a role in variability. My observations
provide multiple examples where the
doctors address the transcriptionists directly
by making editing comments and thanking
them.
</bodyText>
<sectionHeader confidence="0.7391975" genericHeader="method">
3. Training Corpora and FP
Model
</sectionHeader>
<bodyText confidence="0.976722833333333">
This study used three base and two derived
corpora. Base corpora represent three
different sets of dictations described in
section 3.1. Derived corpora are variations
on the base corpora conditioned in several
different ways described in section 3.2.
</bodyText>
<subsectionHeader confidence="0.997761">
3.1 Base
</subsectionHeader>
<listItem confidence="0.86650775">
• Balanced FP training corpus (BFP-
CORPUS) that has 75, 887 words of
word-by-word transcription data evenly
distributed between 16 talkers. This
</listItem>
<bodyText confidence="0.988631">
corpus was used to build a BIGRAM-
FP-LM which controls the process of
populating a no-FP corpus with artificial
FP&apos;s.
</bodyText>
<listItem confidence="0.997900909090909">
• Unbalanced FP training corpus (UFP-
CORPUS) of approximately 500,000
words of all available word-by-word
transcription data from approximately
20 talkers. This corpus was used only to
calculate average frequency of FP use
among all available talkers.
• Finished transcriptions corpus (FT-
CORPUS) of 12,978,707 words
contains all available dictations and no
FP&apos;s. It represents over 200 talkers of
</listItem>
<bodyText confidence="0.598459166666667">
mixed gender and professional status.
The corpus contains no FP&apos;s or any
other types of disfluencies such as
repetitions, repairs and false starts. The
language in this corpus is also edited for
grammar.
</bodyText>
<subsectionHeader confidence="0.838072">
3.2 Derived
</subsectionHeader>
<listItem confidence="0.951909142857143">
• CONTROLLED-FP-CORPUS is a
version of the finished transcriptions
corpus populated stochastically with
2,665,000 FP&apos;s based on the BIGRAM-
FP-LM.
• RANDOM-FP-CORPUS-1 (normal
density) is another version of the
</listItem>
<bodyText confidence="0.979056916666666">
finished transcriptions corpus populated
with 916,114 FP&apos;s where the insertion
point was selected at random in the
range between 0 and 29. The random
function is based on the average
frequency of FPs in the unbalanced
UFP-CORPUS where an FP occurs on
the average after every 15th word.
Another RANDOM-FP-CORPUS-2
(high density) was used to approximate
the frequency of FP&apos;s in the
CONTROLLED-FP-CORPUS.
</bodyText>
<page confidence="0.994864">
620
</page>
<sectionHeader confidence="0.982725" genericHeader="method">
4. Models
</sectionHeader>
<bodyText confidence="0.99997675">
The language modeling process in this study
was conducted in two stages. First, a bigram
model containing bigram probabilities of
FP&apos;s in the balanced BFP-COPRUS was
built followed by four different trigram
language models, some of which used
corpora generated with the BIGRAM-FP-
LM built during the first stage.
</bodyText>
<subsectionHeader confidence="0.994165">
4.1 Bigram FP model
</subsectionHeader>
<bodyText confidence="0.9997415">
This model contains the distribution of FP&apos;s
obtained by using the following formulas:
</bodyText>
<equation confidence="0.9500935">
P(FPiwi_i) =
---- CH) w+1 Cw+i
</equation>
<bodyText confidence="0.997631">
Thus, each word in a corpus to be populated
with FP&apos;s becomes a potential landing site
for a FP and does or does not receive one
based on the probability found in the
BIGRAM-FP-LM.
</bodyText>
<subsectionHeader confidence="0.965651">
4.2 Trigram models
</subsectionHeader>
<bodyText confidence="0.99753925">
The following trigram models were built
using ECRL&apos;s Transcriber language
modeling tools (Valtchev, et al. 1998). Both
bigram and trigram cutoffs were set to 3.
</bodyText>
<listItem confidence="0.947510352941176">
• NOFP-LM was built using the FT-
CORPUS with no FP&apos;s.
• ALLFP-LM was built entirely on
CONTROLLED-FP-CORPUS.
• ADAPTFP-LM was built by
interpolating ALLFP-LM and NOFP-
LM at 90/10 ratio. Here 90 % of the
resulting ADAPTFP-LM represents the
CONTROLLED-FP-CORPUS and 10%
represents FT-CORPUS.
• RANDOMFP-LM-1 (normal density)
was built entirely on the RANDOM-FP-
CORPUS-1.
• RANDOMFP-LM-2 (high density) was
built entirely on the RANDOM-FP-
CORPUS-2
5. Testing Data
</listItem>
<bodyText confidence="0.996719538461538">
Testing data comes from 21 talkers selected
at random and represents 3 (1-3 min)
dictations for each talker. The talkers are a
random mix of male and female medical
doctors and practitioners who vary greatly in
their use of FP&apos;s. Some use literally no FP&apos;s
(but long silences instead), others use FP&apos;s
almost every other word. Based on the
frequency of FP use, the talkers were
roughly split into a high FP user and low FP
user groups. The relevance of such division
will become apparent during the discussion
of test results.
</bodyText>
<sectionHeader confidence="0.995602" genericHeader="method">
6. Adaptation
</sectionHeader>
<bodyText confidence="0.9998873">
Test results for ALLFP-LM (63.01% avg.
word accuracy) suggest that the model over
represents FP&apos;s. The recognition accuracy
for this model is 4.21 points higher than that
of the NOFP-LM (58.8% avg. word
accuracy) but lower than that of both the
RANDOMFP-LM-1 (67.99% avg. word
accuracy) by about 5% and RANDOMFP-
LM-2 (65.87% avg. word accuracy) by
about 7%. One way of decreasing the FP
representation is to correct the BIGRAM-
FP-LM, which proves to be computationally
expensive because of having to rebuild the
large training corpus with each change in
BIGRAM-FP-LM. Another method is to
build a NOFP-LM and an ALLFP-LM once
and experiment with their relative weights
through adaptation. I chose the second
method because ECRL Transcriber toolkit
provides an adaptation tool that achieves the
goals of the first method much faster. The
results show that introducing a NOFP-LM
into the equation improves recognition. The
difference in recognition accuracy between
the ALLFP-LM and ADAPTFP-LM is on
average 4.9% across all talkers in
ADAPTFP-LM&apos;s favor. Separating the
talkers into high FP user group and low FP
user group raises ADAPTFP-LM&apos;s gain to
6.2% for high FP users and lowers it to 3.3%
</bodyText>
<page confidence="0.997227">
621
</page>
<bodyText confidence="0.967318666666667">
for low FP users. This shows that
adaptation to no-FP data is, counter-
intuitively more beneficial for high FP users.
</bodyText>
<sectionHeader confidence="0.927132" genericHeader="evaluation">
7. Results and discussion
</sectionHeader>
<bodyText confidence="0.98840025">
Although a perplexity test provides a good
theoretical measure of a language model, it
is not always accurate in predicting the
model&apos;s performance in a recognizer (Chen
1998); therefore, both perplexity and
recognition accuracy were used in this
study. Both were calculated using ECRL&apos;s
LM Transcriber tools.
</bodyText>
<subsectionHeader confidence="0.989806">
7.1 Perplexity
</subsectionHeader>
<bodyText confidence="0.954512428571428">
Perplexity tests were conducted with
ECRL&apos;s LPlex tool based on the same text
corpus (BFP-CORPUS) that was used to
build the BIGRAM-FP-LM. Three
conditions were used. Condition A used the
whole corpus. Condition B used a subset of
the corpus that contained high frequency FP
users (FPs/Words ratio above 1.0).
Condition C used the remaining subset
containing data from lower frequency FP
users (FPs/Words ratio below 1.0). Table 1
summarizes the results of perplexity tests at
3-gram level for the models under the three
conditions.
</bodyText>
<table confidence="0.997942777777778">
Model Condition A• (all) Condition B (high) Condition C (low
, Lplex 00V rate Lplex 00V rate Lplex 00V rate
N (%) N
NOFP-LM 617.59 6.35 1618.35 6.08 287.46 6.06
ADAPTFP-LM 132.74 6.35 120.69 6.08 131.70 6.06
,
RANDOMFP-LM-1 138.02 6.35 140.47 6.08 125.79 6.06
RANDOMFP-LM-2 156.09 6.35 152.16 6.08 145.47 6.06
ALLFP-LM 980.67 6.35 964.48 6.08 916.53 6.06
</table>
<tableCaption confidence="0.999682">
Table 1. Perplexity measurements
</tableCaption>
<bodyText confidence="0.999697666666667">
The perplexity measures in Condition A show
over 400 point difference between ADAPTFP-
LM and NOFP-LM language models. The
363,08 increase in perplexity for ALLFP-LM
model corroborates the results discussed in
Section 6. Another interesting result is
contained in the hi blighted fields of Table 1.
ADAPTFP-LM based on CONTROLLED-FP-
CORPUS has lower perplexity in general.
When tested on conditions B and C, ADAPTFP-
LM does better on frequent FP users, whereas
RANDOMFP-LM-1 does better on infrequent
FP users, which is consistent with the
recognition accuracy results for the two models
(see Table 2).
</bodyText>
<subsectionHeader confidence="0.999251">
7.2 Recognition accuracy
</subsectionHeader>
<bodyText confidence="0.5387165">
Recognition accuracy was obtained with
ECRL&apos;s HEResults tool and is summarized in
</bodyText>
<tableCaption confidence="0.992668">
Table 2.
</tableCaption>
<table confidence="0.999833666666667">
Language Model . AvgWordAet (High FP . AvgWOrdAce (Low FP user)
,
NOFP-LM 51 AO % 67.76%
RANDOMFP-LM- SnOnnal density) 66.57 % 71.46 %
RANDOMFP-LM-2 (high density) 63.35 % 69.23 %
ADAPTFP-LM 67.14 % 71.24%
</table>
<tableCaption confidence="0.999918">
Table 2. Recognition accuracy tests for LM&apos;s.
</tableCaption>
<bodyText confidence="0.684113166666667">
The results in Table 2 demonstrate two
things. First, a FP model performs better
than a clean model that has no FP
representation. Second, a FP model based on
populating a no-FP training corpus with
FP&apos;s whose distribution was derived from a
</bodyText>
<page confidence="0.99622">
622
</page>
<bodyText confidence="0.999804740740741">
small sample of speech data performs better
than the one populated with FP&apos;s at random
based solely on the frequency of FP&apos;s. The
results also show that ADAPTFP-LM
performs slightly better than RANDOMFP-
LM-1 on high FP users. The gain becomes
more pronounced towards the higher end of
the FP use continuum. For example, the
scores for the top four high FP users are
62.07% with RANDOMFP-LM-1 and
63.51% with ADAPTFP-LM. This
difference cannot be attributed to the fact
that RANDOMFP-LM-1 contains fewer
FP&apos;s than ADAPTFP-LM. The word
accuracy rates for RANDOMFP-LM-2
indicate that frequency of FP&apos;s in the
training corpus is not responsible for the
difference in performance between the
RANDOM-FP-LM-1 and the ADAPTFP-
LM. The frequency is roughly the same for
both RANDOMFP-CORPUS-2 and
CONTROLLED-FP-CORPUS, but
RANDOMFP-LM-2 scores are lower than
those of RANDOMFP-LM-1, which allows
in absence of further evidence to attribute
the difference in scores to the pattern of FP
distribution, not their frequency.
</bodyText>
<sectionHeader confidence="0.67875" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.985397333333333">
Based on the results so far, several
conclusions about FP modeling can be
made:
</bodyText>
<listItem confidence="0.9741422">
1. Representing FP&apos;s in the training data
improves both the language model&apos;s
perplexity and recognition accuracy.
2. It is not absolutely necessary to have a
corpus that contains naturally occurring
</listItem>
<bodyText confidence="0.968654882352941">
FP&apos;s for successful recognition. FP
distribution can be extrapolated from a
relatively small corpus containing
naturally occurring FP&apos;s to a larger
clean corpus. This becomes vital in
situations where the language model has
to be built from &amp;quot;clean&amp;quot; text such as
finished transcriptions, newspaper
articles, web documents, etc.
3. If one is hard-pressed for hand
transcribed data with natural FP&apos;s, a
random population can be used with
relatively good results.
4. FP&apos;s are quite common to both quasi-
spontaneous monologue and
spontaneous dialogue (medical
dictation).
</bodyText>
<subsectionHeader confidence="0.6411">
Research in progress
</subsectionHeader>
<bodyText confidence="0.999018">
The present study leaves a number of issues
to be investigated further:
</bodyText>
<listItem confidence="0.953298">
1. The results for RANDOMFP-LM-1
are very close to those of
ADAPTFP-LM. A statistical test is
needed in order to determine if the
difference is significant.
2. A systematic study of the syntactic as
</listItem>
<bodyText confidence="0.96809740625">
well as discursive contexts in which
FP&apos;s are used in medical dictations.
This will involve tagging a corpus of
literal transcriptions for various kinds of
syntactic and discourse boundaries such
as clause, phrase and theme/rheme
boundaries. The results of the analysis
of the tagged corpus may lead to
investigating which lexical items may be
helpful in identifying syntactic and
discourse boundaries. Although FP&apos;s
may not always be lexically
conditioned, lexical information may be
useful in modeling FP&apos;s that occur at
discourse boundaries due to co-
occurrence of such boundaries and
certain lexical items.
3. The present study roughly categorizes
talkers according to the frequency of
FP&apos;s in their speech into high FP users
and low FP users. A more finely tuned
categorization of talkers in respect to FP
use as well as its usefulness remain to be
investigated.
4. Another area of investigation will focus
on the SOAP structure of medical
dictations. I plan to look at relative
frequency of FP use in the four parts of
a medical dictation. Informal
observation of data collected so far
indicates that FP use is more frequent
and different from other parts during the
</bodyText>
<page confidence="0.99775">
623
</page>
<bodyText confidence="0.99990125">
Subjective part of a dictation. This is
when the doctor uses fewer frozen
expressions and the discourse is closest
to a natural conversation.
</bodyText>
<sectionHeader confidence="0.992381" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.96496">
I would like to thank Joan Bachenko and
Michael Shonwetter, at Linguistic
Technologies, Inc. and Bruce Downing at
the University of Minnesota for helpful
discussions and comments.
</bodyText>
<sectionHeader confidence="0.996375" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922403846154">
Chen, S., Beeferman, Rosenfeld, R. (1998).
&amp;quot;Evaluation metrics for language models,&amp;quot; In
DARPA Broadcast News Transcription and
Understanding Workshop.
Christenfeld, N, Schachter, S and Bilous, F.
(1991). &amp;quot;Filled Pauses and Gestures: It&apos;s not
coincidence,&amp;quot; Journal of Psycholinguistic
Research, Vol. 20(1).
Cook, M. (1977). &amp;quot;The incidence of filled pauses
in relation to part of speech,&amp;quot; Language and
Speech, Vol. 14, pp.135-139.
Cook, M. and Lalljee, M. (1970). &amp;quot;The
interpretation of pauses by the listener,&amp;quot; Brit.
J. Soc. Clin. Psy. Vol. 9, pp. 375-376.
Cook, M., Smith, J, and Lalljee, M (1977).
&amp;quot;Filled pauses and syntactic complexity,&amp;quot;
Language and Speech, Vol. 17, pp.11-16.
Valtchev, V. Kershaw, D. and Odell, J. 1998.
The truetalk transcriber book. Entropic
Cambridge Research Laboratory, Cambridge,
England.
Heeman, P.A. and Loken-Kim, K. and Allen,
J.F. (1996). &amp;quot;Combining the detection and
correlation of speech repairs,&amp;quot; In Proc.,
ICSLP.
Lalljee, M and Cook, M. (1974). &amp;quot;Filled pauses
and floor holding: The final test?&amp;quot;
Semiotica, Vol. 12, pp.219-225.
Maclay, H, and Osgood, C. (1959). &amp;quot;Hesitation
phenomena in spontaneous speech,&amp;quot; Word,
Vol.15, pp.19-44.
Shriberg, E. E. (1994). Preliminaries to a theory
of speech disfluencies. Ph.D. thesis,
University of California at Berkely.
Shriberg, E.E. and Stolcke, A. (1996). &amp;quot;Word
predictability after hesitations: A corpus-
based study,&amp;quot; In Proc. ICSLP.
Shriberg, E.E. (1996). &amp;quot;Disfluencies in
Switchboard,&amp;quot; In Proc. ICSLP.
Shriberg, E.E. Bates, R. and Stolcke, A. (1997).
&amp;quot;A prosody-only decision-tree model for
disfluency detection&amp;quot; In Proc.
EUROSPEECH.
Siu, M. and Ostendorf, M. (1996). &amp;quot;Modeling
disfluencies in conversational speech,&amp;quot; Proc.
ICSLP.
Stolcke, A and Shriberg, E. (1996). &amp;quot;Statistical
language modeling for speech disfluencies,&amp;quot;
In Proc. ICASSP.
Swerts, M, Wichmann, A and Beun, R. (1996).
&amp;quot;Filled pauses as markers of discourse
structure,&amp;quot; Proc. ICSLP.
</reference>
<page confidence="0.998647">
624
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780346">
<title confidence="0.99999">Modeling Filled Pauses in Medical Dictations</title>
<author confidence="0.999919">Sergey V Pakhomov</author>
<affiliation confidence="0.999959">University of Minnesota</affiliation>
<address confidence="0.925833">190 Klaeber Court Ave. S.E. Minneapolis, MN 55455</address>
<email confidence="0.995305">palch0002@tc.umn.edu</email>
<abstract confidence="0.995076941176471">Filled pauses are characteristic of spontaneous speech and can present considerable problems for speech recognition by being often recognized as words. An be recognized as the recognizer&apos;s language model does not adequately represent FP&apos;s. Recognition of quasi-spontaneous speech (medical dictation) is subject to this problem as well. Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP&apos;s produces overall better results than a model trained on a corpus that excluded FP&apos;s or a corpus that had random FP&apos;s.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>Rosenfeld Beeferman</author>
<author>R</author>
</authors>
<title>Evaluation metrics for language models,&amp;quot;</title>
<date>1998</date>
<booktitle>In DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<marker>Chen, Beeferman, R, 1998</marker>
<rawString>Chen, S., Beeferman, Rosenfeld, R. (1998). &amp;quot;Evaluation metrics for language models,&amp;quot; In DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Christenfeld</author>
<author>S Schachter</author>
<author>F Bilous</author>
</authors>
<title>Filled Pauses and Gestures: It&apos;s not coincidence,&amp;quot;</title>
<date>1991</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="2749" citStr="Christenfeld, et al. 1991" startWordPosition="419" endWordPosition="422"> originally had no FP&apos;s. Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP&apos;s produces overall better results than a model trained on a corpus that excluded FP&apos;s or a corpus that had random FP&apos;s. Recognition accuracy improves proportionately to the frequency of FP&apos;s in the speech. 1. Filled Pauses FP&apos;s are not random events, but have a systematic distribution and well-defined functions in discourse. (Shriberg and Stolcke 1996, Shriberg 1994, Swerts 1996, Macalay and Osgood 1959, Cook 1970, Cook and Lalljee 1970, Christenfeld, et al. 1991) Cook and Lalljee (1970) make an interesting proposal that FP&apos;s may have something to do with the listener&apos;s perception of disfluent speech. They suggest that speech may be more 619 comprehensible when it contains filler material during hesitations by preserving continuity and that a FP may serve as a signal to draw the listeners attention to the next utterance in order for the listener not to lose the onset of the following utterance. Perhaps, from the point of view of perception, FP&apos;s are not disfluent events at all. This proposal bears directly on the domain of medical dictations, since man</context>
</contexts>
<marker>Christenfeld, Schachter, Bilous, 1991</marker>
<rawString>Christenfeld, N, Schachter, S and Bilous, F. (1991). &amp;quot;Filled Pauses and Gestures: It&apos;s not coincidence,&amp;quot; Journal of Psycholinguistic Research, Vol. 20(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cook</author>
</authors>
<title>The incidence of filled pauses in relation to part of speech,&amp;quot;</title>
<date>1977</date>
<journal>Language and Speech,</journal>
<volume>14</volume>
<pages>135--139</pages>
<marker>Cook, 1977</marker>
<rawString>Cook, M. (1977). &amp;quot;The incidence of filled pauses in relation to part of speech,&amp;quot; Language and Speech, Vol. 14, pp.135-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cook</author>
<author>M Lalljee</author>
</authors>
<title>The interpretation of pauses by the listener,&amp;quot;</title>
<date>1970</date>
<journal>Brit. J. Soc. Clin. Psy.</journal>
<volume>9</volume>
<pages>375--376</pages>
<contexts>
<context position="2721" citStr="Cook and Lalljee 1970" startWordPosition="415" endWordPosition="418">er training corpus that originally had no FP&apos;s. Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP&apos;s produces overall better results than a model trained on a corpus that excluded FP&apos;s or a corpus that had random FP&apos;s. Recognition accuracy improves proportionately to the frequency of FP&apos;s in the speech. 1. Filled Pauses FP&apos;s are not random events, but have a systematic distribution and well-defined functions in discourse. (Shriberg and Stolcke 1996, Shriberg 1994, Swerts 1996, Macalay and Osgood 1959, Cook 1970, Cook and Lalljee 1970, Christenfeld, et al. 1991) Cook and Lalljee (1970) make an interesting proposal that FP&apos;s may have something to do with the listener&apos;s perception of disfluent speech. They suggest that speech may be more 619 comprehensible when it contains filler material during hesitations by preserving continuity and that a FP may serve as a signal to draw the listeners attention to the next utterance in order for the listener not to lose the onset of the following utterance. Perhaps, from the point of view of perception, FP&apos;s are not disfluent events at all. This proposal bears directly on the domain of m</context>
</contexts>
<marker>Cook, Lalljee, 1970</marker>
<rawString>Cook, M. and Lalljee, M. (1970). &amp;quot;The interpretation of pauses by the listener,&amp;quot; Brit. J. Soc. Clin. Psy. Vol. 9, pp. 375-376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cook</author>
<author>J Smith</author>
<author>M Lalljee</author>
</authors>
<title>Filled pauses and syntactic complexity,&amp;quot;</title>
<date>1977</date>
<journal>Language and Speech,</journal>
<volume>17</volume>
<pages>11--16</pages>
<marker>Cook, Smith, Lalljee, 1977</marker>
<rawString>Cook, M., Smith, J, and Lalljee, M (1977). &amp;quot;Filled pauses and syntactic complexity,&amp;quot; Language and Speech, Vol. 17, pp.11-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kershaw Valtchev</author>
<author>D</author>
<author>J Odell</author>
</authors>
<title>The truetalk transcriber book. Entropic Cambridge Research Laboratory,</title>
<date>1998</date>
<location>Cambridge, England.</location>
<contexts>
<context position="6705" citStr="Valtchev, et al. 1998" startWordPosition="1037" endWordPosition="1040">e balanced BFP-COPRUS was built followed by four different trigram language models, some of which used corpora generated with the BIGRAM-FPLM built during the first stage. 4.1 Bigram FP model This model contains the distribution of FP&apos;s obtained by using the following formulas: P(FPiwi_i) = ---- CH) w+1 Cw+i Thus, each word in a corpus to be populated with FP&apos;s becomes a potential landing site for a FP and does or does not receive one based on the probability found in the BIGRAM-FP-LM. 4.2 Trigram models The following trigram models were built using ECRL&apos;s Transcriber language modeling tools (Valtchev, et al. 1998). Both bigram and trigram cutoffs were set to 3. • NOFP-LM was built using the FTCORPUS with no FP&apos;s. • ALLFP-LM was built entirely on CONTROLLED-FP-CORPUS. • ADAPTFP-LM was built by interpolating ALLFP-LM and NOFPLM at 90/10 ratio. Here 90 % of the resulting ADAPTFP-LM represents the CONTROLLED-FP-CORPUS and 10% represents FT-CORPUS. • RANDOMFP-LM-1 (normal density) was built entirely on the RANDOM-FPCORPUS-1. • RANDOMFP-LM-2 (high density) was built entirely on the RANDOM-FPCORPUS-2 5. Testing Data Testing data comes from 21 talkers selected at random and represents 3 (1-3 min) dictations fo</context>
</contexts>
<marker>Valtchev, D, Odell, 1998</marker>
<rawString>Valtchev, V. Kershaw, D. and Odell, J. 1998. The truetalk transcriber book. Entropic Cambridge Research Laboratory, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>K Loken-Kim</author>
<author>J F Allen</author>
</authors>
<title>Combining the detection and correlation of speech repairs,&amp;quot;</title>
<date>1996</date>
<booktitle>In Proc., ICSLP.</booktitle>
<marker>Heeman, Loken-Kim, Allen, 1996</marker>
<rawString>Heeman, P.A. and Loken-Kim, K. and Allen, J.F. (1996). &amp;quot;Combining the detection and correlation of speech repairs,&amp;quot; In Proc., ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lalljee</author>
<author>M Cook</author>
</authors>
<title>Filled pauses and floor holding: The final test?&amp;quot;</title>
<date>1974</date>
<journal>Semiotica,</journal>
<volume>12</volume>
<pages>219--225</pages>
<marker>Lalljee, Cook, 1974</marker>
<rawString>Lalljee, M and Cook, M. (1974). &amp;quot;Filled pauses and floor holding: The final test?&amp;quot; Semiotica, Vol. 12, pp.219-225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maclay</author>
<author>C Osgood</author>
</authors>
<title>Hesitation phenomena in spontaneous speech,&amp;quot;</title>
<date>1959</date>
<journal>Word,</journal>
<volume>15</volume>
<pages>19--44</pages>
<marker>Maclay, Osgood, 1959</marker>
<rawString>Maclay, H, and Osgood, C. (1959). &amp;quot;Hesitation phenomena in spontaneous speech,&amp;quot; Word, Vol.15, pp.19-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Shriberg</author>
</authors>
<title>Preliminaries to a theory of speech disfluencies.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkely.</institution>
<contexts>
<context position="1748" citStr="Shriberg (1994)" startWordPosition="262" endWordPosition="263"> does not adequately represent FP&apos;s. Recognition of quasi-spontaneous speech (medical dictation) is subject to this problem as well. The FP problem becomes especially pertinent where the corpora used to build language models are compiled from text with no FP&apos;s. Shriberg (1996) has shown that representing FP&apos;s in a language model helps decrease the model&apos;s perplexity. She finds that when a FP occurs at a major phrase or discourse boundary, the FP itself is the best predictor of the following lexical material; conversely, in a non-boundary context, FP&apos;s are predictable from the preceding words. Shriberg (1994) shows that the rate of disfluencies grows exponentially with the length of the sentence, and that FP&apos;s occur more often in the initial position (see also Swerts (1996)). This paper presents a method of using bigram probabilities for extracting FP distribution from a corpus of handtranscribed data. The resulting bigram model is used to populate another training corpus that originally had no FP&apos;s. Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP&apos;s produces overall better results than a model trained on a corpus </context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Shriberg, E. E. (1994). Preliminaries to a theory of speech disfluencies. Ph.D. thesis, University of California at Berkely.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Word predictability after hesitations: A corpusbased study,&amp;quot;</title>
<date>1996</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="2634" citStr="Shriberg and Stolcke 1996" startWordPosition="401" endWordPosition="404">from a corpus of handtranscribed data. The resulting bigram model is used to populate another training corpus that originally had no FP&apos;s. Results from medical dictations by 21 family practice physicians show that using an FP model trained on the corpus populated with FP&apos;s produces overall better results than a model trained on a corpus that excluded FP&apos;s or a corpus that had random FP&apos;s. Recognition accuracy improves proportionately to the frequency of FP&apos;s in the speech. 1. Filled Pauses FP&apos;s are not random events, but have a systematic distribution and well-defined functions in discourse. (Shriberg and Stolcke 1996, Shriberg 1994, Swerts 1996, Macalay and Osgood 1959, Cook 1970, Cook and Lalljee 1970, Christenfeld, et al. 1991) Cook and Lalljee (1970) make an interesting proposal that FP&apos;s may have something to do with the listener&apos;s perception of disfluent speech. They suggest that speech may be more 619 comprehensible when it contains filler material during hesitations by preserving continuity and that a FP may serve as a signal to draw the listeners attention to the next utterance in order for the listener not to lose the onset of the following utterance. Perhaps, from the point of view of perception</context>
</contexts>
<marker>Shriberg, Stolcke, 1996</marker>
<rawString>Shriberg, E.E. and Stolcke, A. (1996). &amp;quot;Word predictability after hesitations: A corpusbased study,&amp;quot; In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Shriberg</author>
</authors>
<title>Disfluencies in Switchboard,&amp;quot; In</title>
<date>1996</date>
<booktitle>Proc. ICSLP.</booktitle>
<contexts>
<context position="1410" citStr="Shriberg (1996)" startWordPosition="208" endWordPosition="209">Introduction Filled pauses (FP&apos;s), false starts, repetitions, fragments, etc. are characteristic of spontaneous speech and can present considerable problems for speech recognition. FP&apos;s are often recognized as short words of similar phonetic quality. For example, an um can be recognized as thumb or arm if the recognizer&apos;s language model does not adequately represent FP&apos;s. Recognition of quasi-spontaneous speech (medical dictation) is subject to this problem as well. The FP problem becomes especially pertinent where the corpora used to build language models are compiled from text with no FP&apos;s. Shriberg (1996) has shown that representing FP&apos;s in a language model helps decrease the model&apos;s perplexity. She finds that when a FP occurs at a major phrase or discourse boundary, the FP itself is the best predictor of the following lexical material; conversely, in a non-boundary context, FP&apos;s are predictable from the preceding words. Shriberg (1994) shows that the rate of disfluencies grows exponentially with the length of the sentence, and that FP&apos;s occur more often in the initial position (see also Swerts (1996)). This paper presents a method of using bigram probabilities for extracting FP distribution f</context>
</contexts>
<marker>Shriberg, 1996</marker>
<rawString>Shriberg, E.E. (1996). &amp;quot;Disfluencies in Switchboard,&amp;quot; In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Bates Shriberg</author>
<author>R</author>
<author>A Stolcke</author>
</authors>
<title>A prosody-only decision-tree model for disfluency detection&amp;quot; In</title>
<date>1997</date>
<booktitle>Proc. EUROSPEECH.</booktitle>
<marker>Shriberg, R, Stolcke, 1997</marker>
<rawString>Shriberg, E.E. Bates, R. and Stolcke, A. (1997). &amp;quot;A prosody-only decision-tree model for disfluency detection&amp;quot; In Proc. EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siu</author>
<author>M Ostendorf</author>
</authors>
<title>Modeling disfluencies in conversational speech,&amp;quot;</title>
<date>1996</date>
<booktitle>Proc. ICSLP.</booktitle>
<marker>Siu, Ostendorf, 1996</marker>
<rawString>Siu, M. and Ostendorf, M. (1996). &amp;quot;Modeling disfluencies in conversational speech,&amp;quot; Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Statistical language modeling for speech disfluencies,&amp;quot; In</title>
<date>1996</date>
<booktitle>Proc. ICASSP.</booktitle>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Stolcke, A and Shriberg, E. (1996). &amp;quot;Statistical language modeling for speech disfluencies,&amp;quot; In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swerts</author>
<author>A Wichmann</author>
<author>R Beun</author>
</authors>
<title>Filled pauses as markers of discourse structure,&amp;quot;</title>
<date>1996</date>
<booktitle>Proc. ICSLP.</booktitle>
<marker>Swerts, Wichmann, Beun, 1996</marker>
<rawString>Swerts, M, Wichmann, A and Beun, R. (1996). &amp;quot;Filled pauses as markers of discourse structure,&amp;quot; Proc. ICSLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>