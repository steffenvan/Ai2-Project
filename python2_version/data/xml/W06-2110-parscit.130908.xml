<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9983305">
Automatic Identification of English Verb Particle Constructions
using Linguistic Features
</title>
<author confidence="0.994379">
Su Nam Kim and Timothy Baldwin
</author>
<affiliation confidence="0.9996925">
Department of Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
</affiliation>
<email confidence="0.994853">
{snkim,tim}@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.997352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999814615384615">
This paper presents a method for identify-
ing token instances of verb particle con-
structions (VPCs) automatically, based on
the output of the RASP parser. The pro-
posed method pools together instances of
VPCs and verb-PPs from the parser out-
put and uses the sentential context of each
such instance to differentiate VPCs from
verb-PPs. We show our technique to per-
form at an F-score of 97.4% at identifying
VPCs in Wall Street Journal and Brown
Corpus data taken from the Penn Tree-
bank.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999861279411765">
Multiword expressions (hereafter MWEs) are
lexical items that can be decomposed into multi-
ple simplex words and display lexical, syntactic
and/or semantic idiosyncracies (Sag et al., 2002;
Calzolari et al., 2002). In the case of English,
MWEs are conventionally categorised syntactico-
semantically into classes such as compound nom-
inals (e.g. New York, apple juice, GM car), verb
particle constructions (e.g. hand in, battle on),
non-decomposable idioms (e.g. a piece of cake,
kick the bucket) and light-verb constructions (e.g.
make a mistake). MWE research has focussed
largely on their implications in language under-
standing, fluency and robustness (Pearce, 2001;
Sag et al., 2002; Copestake and Lascarides, 1997;
Bannard et al., 2003; McCarthy et al., 2003; Wid-
dows and Dorow, 2005). In this paper, our goal
is to identify individual token instances of En-
glish verb particle constructions (VPCs hereafter)
in running text.
For the purposes of this paper, we follow Bald-
win (2005) in adopting the simplifying assump-
tion that VPCs: (a) consist of a head verb and a
unique prepositional particle (e.g. hand in, walk
off); and (b) are either transitive (e.g. hand in, put
on) or intransitive (e.g. battle on). A defining char-
acteristic of transitive VPCs is that they can gen-
erally occur with either joined (e.g. He put on the
sweater) or split (e.g. He put the sweater on) word
order. In the case that the object is pronominal,
however, the VPC must occur in split word order
(c.f. *He handed in it) (Huddleston and Pullum,
2002; Villavicencio, 2003).
The semantics of the VPC can either derive
transparently from the semantics of the head verb
and particle (e.g. walk off) or be significantly re-
moved from the semantics of the head verb and/or
particle (e.g. lookup); analogously, the selectional
preferences of VPCs can mirror those of their head
verbs or alternatively diverge markedly. The syn-
tax of the VPC can also coincide with that of the
head verb (e.g. walk off) or alternatively diverge
(e.g. lift off).
In the following, we review relevant past
research on VPCs, focusing on the extrac-
tion/identification of VPCs and the prediction of
the compositionality/productivity of VPCs.
There is a modest body of research on the iden-
tification and extraction of VPCs. Note that in
the case of VPC identification we seek to detect
individual VPC token instances in corpus data,
whereas in the case of VPC extraction we seek
to arrive at an inventory of VPC types/lexical
items based on analysis of token instances in cor-
pus data. Li et al. (2003) identify English VPCs
(or “phrasal verbs” in their parlance) using hand-
coded regular expressions. Baldwin and Villavi-
cencio (2002) extract a simple list of VPCs from
corpus data, while Baldwin (2005) extracts VPCs
with valence information under the umbrella of
deep lexical acquisition.1 The method of Baldwin
(2005) is aimed at VPC extraction and takes into
account only the syntactic features of verbs. In this
paper, our interest is in VPC identification, and we
make use of deeper semantic information.
In Fraser (1976) and Villavicencio (2006) it is
argued that the semantic properties of verbs can
determine the likelihood of their occurrence with
</bodyText>
<footnote confidence="0.998514">
1The learning of lexical items in a form that can be fed
directly into a deep grammar or other richly-annotated lexical
resource
</footnote>
<note confidence="0.9968195">
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65–72,
Trento, Italy, April 2006. c�2006 Association for Computational Linguistics
</note>
<page confidence="0.999488">
65
</page>
<bodyText confidence="0.999444470588235">
particles. Bannard et al. (2003) and McCarthy et
al. (2003) investigate methods for estimating the
compositionality of VPCs based largely on dis-
tributional similarity of the head verb and VPC.
O’Hara and Wiebe (2003) propose a method for
disambiguating the verb sense of verb-PPs. While
our interest is in VPC identification—a fundamen-
tally syntactic task—we draw on the shallow se-
mantic processing employed in these methods in
modelling the semantics of VPCs relative to their
base verbs.
The contribution of this paper is to combine
syntactic and semantic features in the task of VPC
identification. The basic intuition behind the pro-
posed method is that the selectional preferences of
VPCs over predefined argument positions,2 should
provide insight into whether a verb and preposi-
tion in a given sentential context combine to form
a VPC (e.g. Kim handed in the paper) or alter-
natively constitute a verb-PP (e.g. Kim walked in
the room). That is, we seek to identify individual
preposition token instances as intransitive preposi-
tions (i.e. prepositional particles) or transitive par-
ticles based on analysis of the governing verb.
The remainder of the paper is structured as fol-
lows. Section 2 outlines the linguistic features of
verbs and their co-occuring nouns. Section 3 pro-
vides a detailed description of our technique. Sec-
tion 4 describes the data properties and the identi-
fication method. Section 5 contains detailed evalu-
ation of the proposed method. Section 6 discusses
the effectiveness of our approach. Finally, Sec-
tion 7 summarizes the paper and outlines future
work.
</bodyText>
<listItem confidence="0.924377">
(1) put= place
EX: Put the book on the table.
ARGS: bookOBJ = book, publication, object
ANALYSIS: verb-PP
(2) put on = wear
EX: Put on the sweater .
ARGS: sweaterOBJ = garment, clothing
ANALYSIS: verb particle construction
</listItem>
<bodyText confidence="0.997905222222222">
While put on is generally used in the context of
wearing something, it usually occurs with clothing-
type nouns such as sweater and coat, whereas the
simplex put has less sharply defined selectional re-
strictions and can occur with any noun. In terms
of the word senses of the head nouns of the ob-
ject NPs, the VPC put on will tend to co-occur
with objects which have the semantics of clothes
or garment. On the other hand, the simplex verb
put in isolation tends to be used with objects with
the semantics of object and prepositional phrases
containing NPs with the semantics of place.
Also, as observed above, the valence of a VPC
can differ from that of the head verb. (3) and (4)
illustrate two different senses of take off with in-
transitive and transitive syntax, respectively. Note
that take cannot occur as a simplex intransitive
verb.
</bodyText>
<listItem confidence="0.547406">
(3) take off = lift off
</listItem>
<bodyText confidence="0.91032">
EX: The airplane takes off.
</bodyText>
<sectionHeader confidence="0.976605" genericHeader="method">
2 Linguistic Features
</sectionHeader>
<bodyText confidence="0.989007066666667">
When verbs co-occur with particles to form VPCs,
their meaning can be significantly different from
the semantics of the head verb in isolation. Ac-
cording to Baldwin et al. (2003), divergences in
VPC and head verb semantics are often reflected
in differing selectional preferences, as manifested
in patterns of noun co-occurrence. In one example
cited in the paper, the cosine similarity between
cut and cut out, based on word co-occurrence vec-
tors, was found to be greater than that between cut
and cut off, mirroring the intuitive compositional-
ity of these VPCs.
(1) and (2) illustrate the difference in the selec-
tional preferences of the verb put in isolation as
compared with the VPC put on.3
</bodyText>
<footnote confidence="0.987419666666667">
2Focusing exclusively on the subject and object argument
positions.
3All sense definitions are derived from WordNet 2.1.
</footnote>
<listItem confidence="0.478585714285714">
ARGS: airplaneSUBJ = airplane, aeroplane
ANALYSIS: verb particle construction
(4) take off = remove
EX: They take off the cape .
ARGS: theySUBJ = person, individual
capeOBJ = garment, clothing
ANALYSIS: verb particle construction
</listItem>
<bodyText confidence="0.99950325">
Note that in (3), take off = lift off co-occurs with
a subject of the class airplane, aeroplane. In (4), on
the other hand, take off = remove and the corre-
sponding object noun is of class garment or cloth-
ing. From the above, we can see that head nouns
in the subject and object argument positions can
be used to distinguish VPCs from simplex verbs
with prepositional phrases (i.e. verb-PPs).
</bodyText>
<page confidence="0.993601">
66
</page>
<sectionHeader confidence="0.997942" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999975461538462">
Our goal is to distinguish VPCs from verb-PPs in
corpus data, i.e. to take individual inputs such as
Kim handed the paper in today and tag each as
either a VPC or a verb-PP. Our basic approach is
to parse each sentence with RASP (Briscoe and
Carroll, 2002) to obtain a first-gloss estimate of
the VPC and verb-PP token instances, and also
identify the head nouns of the arguments of each
VPC and simplex verb. For the head noun of each
subject and object, as identified by RASP, we use
WordNet 2.1 (Fellbaum, 1998) to obtain the word
sense. Finally we build a supervised classifier us-
ing TiMBL 5.1 (Daelemans et al., 2004).
</bodyText>
<subsectionHeader confidence="0.998804">
3.1 Method
</subsectionHeader>
<bodyText confidence="0.9874484">
Compared to the method proposed by Baldwin
(2005), our approach (a) tackles the task of VPC
identification rather than VPC extraction, and (b)
uses both syntactic and semantic features, employ-
ing the WordNet 2.1 senses of the subject and/or
object(s) of the verb. In the sentence He put the
coat on the table, e.g., to distinguish the VPC put
on from the verb put occurring with the preposi-
tional phrase on the table, we identify the senses
of the head nouns of the subject and object(s) of
the verb put (i.e. he and coat, respectively).
First, we parse all sentences in the given corpus
using RASP, and identify verbs and prepositions
in the RASP output. This is a simple process of
checking the POS tags in the most-probable parse,
and for both particles (tagged RP) and transitive
prepositions (tagged II) reading off the governing
verb from the dependency tuple output (see Sec-
tion 3.2 for details). We also retrieved the head
nouns of the subject and object(s) of each head
verb directly from the dependency tuples. Using
WordNet 2.1, we then obtain the word sense of the
head nouns.
The VPCs or verb-PPs are represented with cor-
responding information as given below:
P(type|v,p, wsSUBJ, wsDOBJ, wsIOBJ)
where type denotes either a VPC or verb-PP, v is
the head verb, p is the preposition, and ws* is the
word sense of the subject, direct object or indirect
object.
Once all the data was gathered, we separated it
into test and training data. We then used TiMBL
5.1 to learn a classifier from the training data,
which was then run and evaluated over the test
data. See Section 5 for full details of the results.
</bodyText>
<figureCaption confidence="0.883913666666667">
Figure 1 depicts the complete process used to
distinguish VPCs from verb-PPs.
Figure 1: System Architecture
</figureCaption>
<subsectionHeader confidence="0.9395315">
3.2 On the use of RASP, WordNet and
TiMBL
</subsectionHeader>
<bodyText confidence="0.99995546875">
RASP is used to identify the syntactic structure
of each sentence, including the head nouns of ar-
guments and first-gloss determination of whether
a given preposition is incorporated in a VPC or
verb-PP. The RASP output contains dependency
tuples derived from the most probable parse, each
of which includes a label identifying the nature
of the dependency (e.g. SUBJ, DOBJ), the head
word of the modifying constituent, and the head of
the modified constituent. In addition, each word
is tagged with a POS tag from which it is possi-
ble to determine the valence of any prepositions.
McCarthy et al. (2003) evaluate the precision of
RASP at identifying VPCs to be 87.6% and the re-
call to be 49.4%. However the paper does not eval-
uate the parser’s ability to distinguish sentences
containing VPCs and sentences with verb-PPs.
To better understand the baseline performance
of RASP, we counted the number of false-positive
examples tagged with RP and false-negative ex-
amples tagged with II, relative to gold-standard
data. See Section 5 for details.
We use WordNet to obtain the first-sense word
sense of the head nouns of subject and object
phrases, according to the default word sense rank-
ing provided within WordNet. McCarthy et al.
(2004) found that 54% of word tokens are used
with their first (or default) sense. With the per-
formance of current word sense disambiguation
(WSD) systems hovering around 60-70%, a sim-
ple first-sense WSD system has room for improve-
ment, but is sufficient for our immediate purposes
</bodyText>
<figure confidence="0.997469526315789">
Verbs
Particles
Subjects
Objects
TiMBL Classifier
Preprocessing
v+p with Semantics
e.g.
take_off := [..
put_on := [..
look_after := [..
WordNet
Word
Senses
RASP
parser
raw
corpus
text
</figure>
<page confidence="0.994234">
67
</page>
<bodyText confidence="0.9970634">
in this paper.
To evaluate our approach, we built a super-
vised classifier using the TiMBL 5.1 memory-
based learner and training data extracted from the
Brown and WSJ corpora.
</bodyText>
<sectionHeader confidence="0.988568" genericHeader="method">
4 Data Collection
</sectionHeader>
<bodyText confidence="0.999959666666667">
We evaluated out method by running RASP over
Brown Corpus and Wall Street Journal, as con-
tained in the Penn Treebank (Marcus et al., 1993).
</bodyText>
<subsectionHeader confidence="0.986436">
4.1 Data Classification
</subsectionHeader>
<bodyText confidence="0.999935638888889">
The data we consider is sentences containing
prepositions tagged as either RP or II. Based on
the output of RASP, we divide the data into four
groups:
Group A contains the verb–preposition token
instances tagged tagged exclusively as VPCs (i.e.
the preposition is never tagged as II in combi-
nation with the given head verb). Group B con-
tains the verb–preposition token instances iden-
tified as VPCs by RASP where there were also
instances of that same combination identified as
verb-PPs. Group C contains the verb–preposition
token instances identified as verb-PPs by RASP
where there were also instances of that same com-
bination identified as VPCs. Finally, group D
contains the verb-preposition combinations which
were tagged exclusively as verb-PPs by RASP.
We focus particularly on disambiguating verb-
preposition token instances falling into groups B
and C, where RASP has identified an ambiguity
for that particular combination. We do not further
classify token instances in group D, on the grounds
that (a) for high-frequency verb–preposition com-
binations, RASP was unable to find a single in-
stance warranting a VPC analysis, suggesting it
had high confidence in its ability to correctly iden-
tify instances of this lexical type, and (b) for low-
frequency verb–preposition combinations where
the confidence of there definitively no being a
VPC usage is low, the token sample is too small
to disambiguate effectively and the overall impact
would be negligible even if we tried. We do, how-
ever, return to considered data in group D in com-
puting the precision and recall of RASP.
Naturally, the output of RASP parser is not
error-free, i.e. VPCs may be parsed as verb-PPs
</bodyText>
<table confidence="0.9991214">
FPR FNR Agreement
Group A 4.08% — 95.24%
Group B 3.96% — 99.61%
Group C — 10.15% 93.27%
Group D — 3.4% 99.20%
</table>
<tableCaption confidence="0.875105666666667">
Table 1: False positive rate (FPR), false negative
rate (FNR) and inter-annotator agreement across
the four groups of token instances
</tableCaption>
<table confidence="0.999966">
f &gt; 1 f &gt; 5
VPC V-PP VPC V-PP
Group A 5,223 0 3,787 0
Group B 1,312 0 1,108 0
Group C 0 995 0 217
Total 6,535 995 4,895 217
</table>
<tableCaption confidence="0.949829333333333">
Table 2: The number of VPC and verb-PP token
instances occurring in groups A, B and C at vary-
ing frequency cut-offs
</tableCaption>
<bodyText confidence="0.998793764705883">
and vice versa. In particular, other than the re-
ported results of McCarthy et al. (2003) targeting
VPCs vs. all other analyses, we had no a priori
sense of RASP’s ability to distinguish VPCs and
verb-PPs. Therefore, we manually checked the
false-positive and false-negative rates in all four
groups and obtained the performance of parser
with respect to VPCs. The verb-PPs in group A
and B are false-positives while the VPCs in group
C and D are false-negatives (we consider the VPCs
to be positive examples).
To calculate the number of incorrect examples,
two human annotators independently checked
each verb–preposition instance. Table 1 details the
rate of false-positives and false-negative examples
in each data group, as well as the inter-annotator
agreement (calculated over the entire group).
</bodyText>
<subsectionHeader confidence="0.979066">
4.2 Collection
</subsectionHeader>
<bodyText confidence="0.9999754">
We combined together the 6,535 (putative) VPCs
and 995 (putative) verb-PPs from groups A, B and
C, as identified by RASP over the corpus data. Ta-
ble 2 shows the number of VPCs in groups A and
B and the number of verb-PPs in group C. The
first number is the number of examples occuring
at least once and the second number that of exam-
ples occurring five or more times.
From the sentences containing VPCs and verb-
PPs, we retrieved a total of 8,165 nouns, including
</bodyText>
<figure confidence="0.993772">
RP tagged data RP &amp; II tagged data II tagged data
Group B
Group A
Group C
Group D
</figure>
<page confidence="0.984262">
68
</page>
<table confidence="0.997318375">
Type Groups A&amp;B Group C
common noun 7,116 1,239
personal pronoun 629 79
demonstrative pronoun 127 1
proper noun 156 18
who 94 6
which 32 0
No sense (what) 11 0
</table>
<tableCaption confidence="0.9889675">
Table 3: Breakdown of subject and object head
nouns in group A&amp;B, and group C
</tableCaption>
<bodyText confidence="0.994334466666667">
pronouns (e.g. I, he, she), proper nouns (e.g. CITI,
Canada, Ford) and demonstrative pronouns (e.g.
one, some, this), which occurred as the head noun
of a subject or object of a VPC in group A or B.
We similarly retrieved 1,343 nouns for verb-PPs in
group C. Table 3 shows the distribution of different
noun types in these two sets.
We found that about 10% of the nouns are pro-
nouns (personal or demonstrative), proper nouns
or WH words. For pronouns, we manually re-
solved the antecedent and took this as the head
noun. When which is used as a relative pronoun,
we identified if it was coindexed with an argument
position of a VPC or verb-PP, and if so, manually
identified the antecedent, as illustrated in (5).
</bodyText>
<listItem confidence="0.956397">
(5) EX: Tom likes the books which he sold off.
ARGS: heSUBJ = person
whichOBJ = book
</listItem>
<bodyText confidence="0.998861">
With what, on the other hand, we were gener-
ally not able to identify an antecedent, in which
case the argument position was left without a word
sense (we come back to this in Section 6).
</bodyText>
<listItem confidence="0.8229375">
(6) Tom didn’t look up what to do.
What went on?
</listItem>
<bodyText confidence="0.932906">
We also replaced all proper nouns with cor-
responding common noun hypernyms based on
manual disambiguation, as the coverage of proper
nouns in WordNet is (intentionally) poor. The fol-
lowing are examples of proper nouns and their
common noun hypernyms:
Proper noun Common noun hypernym
</bodyText>
<footnote confidence="0.4976855">
CITI bank
Canada country
Ford company
Smith human
</footnote>
<figureCaption confidence="0.999696">
Figure 2: Senses of apple and orange
</figureCaption>
<bodyText confidence="0.998899333333333">
When we retrieved the first word sense of nouns
from WordNet, we selected the first sense and the
associated hypernyms (up to) three levels up the
WordNet hierarchy. This is intended as a crude
form of smoothing for closely-related word senses
which occur in the same basic region of the Word-
Net hierarchy. As an illustration of this process,
in Figure 2, apple and orange are used as edi-
ble fruit, fruit or food, and the semantic overlap is
picked up on by the fact that edible fruit is a hy-
pernym of both apple and orange. On the other
hand, food is the fourth hypernym for orange so it
is ignored by our method. However, because we
use the four senses, the common senses of nouns
are extracted properly. This approach works rea-
sonably well for retrieving common word senses
of nouns which are in the immediate vicinity of
each other in the WordNet hierarchy, as was the
case with apple and orange. In terms of feature
representation, we generate an individual instance
for each noun sense generated based on the above
method, and in the case that we have multiple ar-
guments for a given VPC or verb-PP (e.g. both a
subject and a direct object), we generate an indi-
vidual instance for the cross product of all sense
combinations between the arguments.
We use 80% of the data for training and 20%
for testing. The following is the total number of
training instances, before and after performing hy-
pernym expansion:
</bodyText>
<figure confidence="0.922057346153846">
Training Instances
Before expansion After expansion
Group A 5,223 24,602
Group B 1,312 4,158
Group C 995 5,985
Sense 1
apple
edible fruit(1st)
produce, green goods, ...
food(3rd)
...
fruit(2nd)
reproductive structure
...
pome, false fruit
fruit
reproductive structure
fruit(3rd)
..
Sense 1
orange
citrus, citrus fruit, citrous fruit
edible fruit(2nd)
produce, green goods, ...
food(4th)
...
</figure>
<page confidence="0.866662">
69
</page>
<bodyText confidence="0.335628">
Group Frequency of VPCs Size
</bodyText>
<equation confidence="0.983042875">
B (f≥1) test:272
(f≥5) train:1,040
BA (f≥1 &amp; f≥1) test:1,327
(f≥5 &amp; f≥5) train:4,163
BC (f≥1 &amp; f≥1) test:498
(f≥5 &amp; f≥1) train:1,809
BAC (f≥1 &amp; f≥1 &amp; f≥1) test:1,598
(f≥5 &amp; f≥5 &amp; f≥1) train:5,932
</equation>
<tableCaption confidence="0.955934">
Table 4: Data set sizes at different frequency cut-
offs
</tableCaption>
<sectionHeader confidence="0.998495" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.984728620689655">
We selected 20% of the test data from different
combinations of the four groups and over the two
frequency thresholds, leading to a total of 8 test
data sets. The first data set contains examples from
group B only, the second set is from groups B and
A, the third set is from groups B and C, and the
fourth set is from groups B, A and C. Addition-
ally, each data set is divided into: (1) f &gt; 1, i.e.
verb–preposition combinations occurring at least
once, and (2) f &gt; 5, i.e. verb–preposition com-
binations occurring at least five times (hereafter,
f &gt; 1 is labelled f≥1 and f &gt; 5 is labelled f≥5).
In the group C data, there are 217 verb-PPs with
f≥5, which is slightly more than 20% of the data
so we use verb-PPs with f≥1 for experiments in-
stead of verb-PP with f≥5. The first and second
data sets do not contain negative examples while
the third and fourth data sets contain both positive
and negative examples. As a result, the precision
for the first two data sets is 1.0.
Table 5 shows the precision, recall and F-score
of our method over each data set, relative to the
identification of VPCs only. A,B,C are groups and
f# is the frequency of examples.
Table 6 compares the performance of VPC iden-
tification and verb-PP identification.
Table 7 indicates the result using four word
senses (i.e. with hypernym expansion) and only
one word sense (i.e. the first sense only).
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997593">
The performance of RASP as shown in Tables 5
and 6 is based on human judgement. Note that
we only consider the ability of the parser to distin-
guish sentences with prepositions as either VPCs
or verb-PPs (i.e. we judge the parse to be correct if
the preposition is classified correctly, irrespective
of whether there are other errors in the output).
</bodyText>
<table confidence="0.9997864">
Data Freq P R F
RASP f≥1 .959 .955 .957
B f≥1 1.0 .819 .901
f≥5 1.0 .919 .957
BA f≥1 f≥1 1.0 .959 .979
f≥5 f≥5 1.0 .962 .980
BC f≥1 f≥1 .809 .845 .827
f≥5 f≥1 .836 .922 .877
BAC f≥1 f≥1 f≥1 .962 .962 .962
f≥5 f≥5 f≥1 .964 .983 .974
</table>
<tableCaption confidence="0.805491333333333">
Table 5: Results for VPC identification only (P =
precision, R = recall, F = F-score)
Data Freq Type P R F
</tableCaption>
<equation confidence="0.965988">
BC f≥1 f≥1 P+V
f≥5 f≥1 P+V
BAC f≥1 f≥1 P+V
f≥5 f≥1 P+V
</equation>
<bodyText confidence="0.968421333333333">
Table 6: Results for VPC (=V) and verb-PP (=P)
identification (P = precision, R = recall, F = F-
score)
Also, we ignore the ambiguity between particles
and adverbs, which is the principal reason for our
evaluation being much higher than that reported
by McCarthy et al. (2003). In Table 5, the preci-
sion (P) and recall (R) for VPCs are computed as
follows:
</bodyText>
<table confidence="0.442103">
Data Correctly Tagged as VPCs
P_
Data Retrieved as VPCs
Data Correctly Tagged as VPCs
R _
All VPCs in Data Set
</table>
<bodyText confidence="0.999750133333333">
The performance of RASP in Table 6 shows
how well it distinguishes between VPCs and verb-
PPs for ambiguous verb–preposition combina-
tions. Since Table 6 shows the comparative per-
formance of our method between VPCs and verb-
PPs, the performance of RASP with examples
which are misrecognized as each other should be
the guideline. Note, the baseline RASP accuracy,
based on assigning the majority class to instances
in each of groups A, B and C, is 83.04%.
In Table 5, the performance over high-
frequency data identified from groups B, A and
C is the highest (F-score = .974). In general, we
would expect the data set containing the high fre-
quency and both positive and negative examples
</bodyText>
<table confidence="0.97766625">
.8068 .8033 .8051
.8653 .8529 .8591
.8660 .8660 .8660
.9272 .8836 .9054
70
Types
Error Rate Reduction (%)
Freq Type # P R F
f&gt;_1 V 4WS .962 .962 .962
1WS .958 .969 .963
f&gt;_1 P 4WS .769 .769 .769
1WS .800 .743 .770
f&gt;_5 V 4WS .964 .983 .974
1WS .950 .973 .962
f&gt;_5 P 4WS .889 .783 .832
1WS .813 .614 .749
</table>
<tableCaption confidence="0.997991">
Table 7: Results with hypernym expansion (4WS)
</tableCaption>
<bodyText confidence="0.999700975609756">
and only the first sense (1WS), in terms of preci-
sion (P), recall (R) and F-score (F)
to give us the best performance at VPC identifi-
cation. We achieved a slightly better result than
the 95.8%-97.5% performance reported by Li et
al. (2003). However, considering that Li et al.
(2003) need considerable time and human labour
to generate hand-coded rules, our method has ad-
vantages in terms of both raw performance and
labour efficiency.
Combining the results for Table 5 and Table 6,
we see that our method performs better for VPC
identification than verb-PP identification. Since
we do not take into account the data from group
D with our method, the performance of verb-PP
identification is low compared to that for RASP,
which in turn leads to a decrement in the overall
performance.
Since we ignored the data from group D con-
taining unambiguous verb-PPs, the number of pos-
itive training instances for verb-PP identification
was relatively small. As for the different number
of word senses in Table 7, we conclude that the
more word senses the better the performance, par-
ticularly for higher-frequency data items.
In order to get a clearer sense of the impact of
selectional preferences on the results, we investi-
gated the relative performance over VPCs of vary-
ing semantic compositionality, based on 117 VPCs
(f&gt;_1) attested in the data set of McCarthy et al.
(2003). According to our hypothesis from above,
we would expect VPCs with low composition-
ality to have markedly different selectional pref-
erences to the corresponding simplex verb, and
VPCs with high compositionality to have similar
selectional preferences to the simplex verb. In
terms of the performance of our method, therefore,
we would expect the degree of compositionality
to be inversely proportional to the system perfor-
mance. We test this hypothesis in Figure 3, where
we calculate the error rate reduction (in F-score)
</bodyText>
<figure confidence="0.323022">
Compositionality
</figure>
<figureCaption confidence="0.942613">
Figure 3: Error rate reduction for VPCs of varying
compositionality
</figureCaption>
<bodyText confidence="0.991133962962963">
for the proposed method relative to the majority-
class baseline, at various degrees of composition-
ality. McCarthy et al. (2003) provides compo-
sitionality judgements from three human judges,
which we take the average of and bin into 11 cate-
gories (with 0 = non-compositional and 10 = fully
compositional). In Figure 3, we plot both the er-
ror rate reduction in each bin (both the raw num-
bers and a smoothed curve), and also the number
of attested VPC types found in each bin. From
the graph, we see our hypothesis born out that,
with perfect performance over non-compositional
VPCs and near-baseline performance over fully
compositional VPCs. Combining this result with
the overall results from above, we conclude that
our method is highly successful at distinguishing
non-compositional VPCs from verb-PPs, and fur-
ther that there is a direct correlation between the
degree of compositionality and the similarity of
the selectional preferences of VPCs and their verb
counterparts.
Several factors are considered to have influ-
enced performance. Some data instances are miss-
ing head nouns which would assist us in determin-
ing the semantics of the verb–preposition combi-
nation. Particular examples of this are imperative
and abbreviated sentences:
</bodyText>
<listItem confidence="0.6746655">
(7) a. Come in.
b. (How is your cold?) Broiled out.
</listItem>
<bodyText confidence="0.9785965">
Another confounding factor is the lack of word
sense data, particularly in WH questions:
</bodyText>
<listItem confidence="0.8508835">
(8) a. What do I hand in?
b. You can add up anything.
</listItem>
<figure confidence="0.999656615384615">
0 1 2 3 4 5 6 7 8 9 10
100
80
60
40
20
0
100
80
60
40
20
0
</figure>
<page confidence="0.991875">
71
</page>
<sectionHeader confidence="0.998815" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999979444444444">
In this paper, we have proposed a method for iden-
tifying VPCs automatically from raw corpus data.
We first used the RASP parser to identify VPC
and verb-PP candidates. Then, we used analysis of
the head nouns of the arguments of the head verbs
to model selectional preferences, and in doing so,
distinguish between VPCs and verb-PPs. Using
TiMBL 5.1, we built a classifier which achieved
an F-score of 97.4% at identifying frequent VPC
examples. We also investigated the comparative
performance of RASP at VPC identification.
The principal drawback of our method is that it
relies on the performance of RASP and we assume
a pronoun resolution oracle to access the word
senses of pronouns. Since the performance of such
systems is improving, however, we consider our
approach to be a promising, stable method of iden-
tifying VPCs.
</bodyText>
<sectionHeader confidence="0.996843" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.941308">
This material is based upon work supported in part by the
Australian Research Council under Discovery Grant No.
DP0663879 and NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation. We would
like to thank the three anonymous reviewers for their valu-
able input on this research.
</bodyText>
<sectionHeader confidence="0.998036" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862144444445">
Timothy Baldwin and Aline Villavicencio. 2002. Extract-
ing the unextractable: A case study on verb-particles. In
Proc. of the 6th Conference on Natural Language Learn-
ing (CoNLL-2002), pages 98–104, Taipei, Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Do-
minic Widdows. 2003. An empirical model of multiword
expression decomposability. In Proc. of the ACL-2003
Workshop on Multiword Expressions: Analysis, Acquisi-
tion and Treatment, pages 89–96, Sapporo, Japan.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expressions,
19(4):398–414.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of the ACL-2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treatment,
pages 65–72, Sapporo, Japan.
Ted Briscoe and John Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of the 3rd Interna-
tional Conference on Language Resources and Evaluation
(LREC 2002), pages 1499–1504, Las Palmas, Canary Is-
lands.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod, and
Antonio Zampolli. 2002. Towards best practice for mul-
tiword expressions in computational lexicons. In Proc. of
the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 1934–40, Las Pal-
mas, Canary Islands.
Ann Copestake and Alex Lascarides. 1997. Integrating sym-
bolic and statistical representations: The lexicon pragmat-
ics interface. In Proc. of the 35th Annual Meeting of the
ACL and 8th Conference of the EACL (ACL-EACL’97),
pages 136–43, Madrid, Spain.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. ILK Technical Re-
port 04-02.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
B. Fraser. 1976. The Verb-Particle Combination in English.
The Hague: Mouton.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, and Ro-
hini K. Srihari. 2003. An expert lexicon approach to iden-
tifying English phrasal verbs. In Proc. of the 41st Annual
Meeting of the ACL, pages 513–20, Sapporo, Japan.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313–30.
Diana McCarthy, Bill Keller, and John Carroll. 2003. De-
tecting a continuum of compositionality in phrasal verbs.
In Proc. of the ACL-2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, Sapporo,
Japan.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proc. of the 42nd Annual Meeting of the ACL, pages
280–7, Barcelona, Spain.
Tom O’Hara and Janyce Wiebe. 2003. Preposition semantic
classification via Treebank and FrameNet. In Proc. of the
7th Conference on Natural Language Learning (CoNLL-
2003), pages 79–86, Edmonton, Canada.
Darren Pearce. 2001. Synonymy in collocation extraction.
In Proceedings of the NAACL 2001 Workshop on WordNet
and Other Lexical Resources: Applications, Extensions
and Customizations, Pittsburgh, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Proc. of the 3rd Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 1–15, Mex-
ico City, Mexico.
Aline Villavicencio. 2003. Verb-particle constructions and
lexical resources. In Proc. of the ACL-2003 Workshop on
Multiword Expressions: Analysis, Acquisition and Treat-
ment, pages 57–64, Sapporo, Japan.
Aline Villavicencio. 2006. Verb-particle constructions in the
world wide web. In Patrick Saint-Dizier, editor, Compu-
tational Linguistics Dimensions of Syntax and Semantics
of Prepositions. Springer, Dordrecht, Netherlands.
Dominic Widdows and Beate Dorow. 2005. Automatic ex-
traction of idioms using graph analysis and asymmetric
lexicosyntactic patterns. In Proc. of the ACL-SIGLEX
2005 Workshop on Deep Lexical Acquisition, pages 48–
56, Ann Arbor, USA.
</reference>
<page confidence="0.998725">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.229982">
<title confidence="0.999482">Automatic Identification of English Verb Particle using Linguistic Features</title>
<author confidence="0.999703">Nam Kim</author>
<affiliation confidence="0.999818">Department of Computer Science and Software</affiliation>
<address confidence="0.567043">University of Melbourne, Victoria 3010</address>
<email confidence="0.990364">snkim@csse.unimelb.edu.au</email>
<email confidence="0.990364">tim@csse.unimelb.edu.au</email>
<abstract confidence="0.920230214285714">This paper presents a method for identifying token instances of verb particle constructions (VPCs) automatically, based on the output of the RASP parser. The proposed method pools together instances of VPCs and verb-PPs from the parser output and uses the sentential context of each such instance to differentiate VPCs from verb-PPs. We show our technique to perform at an F-score of 97.4% at identifying VPCs in Wall Street Journal and Brown Corpus data taken from the Penn Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Aline Villavicencio</author>
</authors>
<title>Extracting the unextractable: A case study on verb-particles.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>98--104</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3446" citStr="Baldwin and Villavicencio (2002)" startWordPosition="552" endWordPosition="556">vant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1The learning of lexical items in a form that can be fed directly into a dee</context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>Timothy Baldwin and Aline Villavicencio. 2002. Extracting the unextractable: A case study on verb-particles. In Proc. of the 6th Conference on Natural Language Learning (CoNLL-2002), pages 98–104, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>89--96</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7151" citStr="Baldwin et al. (2003)" startWordPosition="1163" endWordPosition="1166"> with objects with the semantics of object and prepositional phrases containing NPs with the semantics of place. Also, as observed above, the valence of a VPC can differ from that of the head verb. (3) and (4) illustrate two different senses of take off with intransitive and transitive syntax, respectively. Note that take cannot occur as a simplex intransitive verb. (3) take off = lift off EX: The airplane takes off. 2 Linguistic Features When verbs co-occur with particles to form VPCs, their meaning can be significantly different from the semantics of the head verb in isolation. According to Baldwin et al. (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. In one example cited in the paper, the cosine similarity between cut and cut out, based on word co-occurrence vectors, was found to be greater than that between cut and cut off, mirroring the intuitive compositionality of these VPCs. (1) and (2) illustrate the difference in the selectional preferences of the verb put in isolation as compared with the VPC put on.3 2Focusing exclusively on the subject and object argument positions. 3All sense def</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 89–96, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>The deep lexical acquisition of English verb-particle constructions.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="1749" citStr="Baldwin (2005)" startWordPosition="266" endWordPosition="268">car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order. In the case that the object is pronominal, however, the VPC must occur in split word order (c.f. *He handed in it) (Huddleston and Pullum, 2002; Villavicencio, 2003). The semantics of the VPC can e</context>
<context position="3515" citStr="Baldwin (2005)" startWordPosition="567" endWordPosition="568">rediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1The learning of lexical items in a form that can be fed directly into a deep grammar or other richly-annotated lexical resource Proceedings of t</context>
<context position="9110" citStr="Baldwin (2005)" startWordPosition="1499" endWordPosition="1500">take individual inputs such as Kim handed the paper in today and tag each as either a VPC or a verb-PP. Our basic approach is to parse each sentence with RASP (Briscoe and Carroll, 2002) to obtain a first-gloss estimate of the VPC and verb-PP token instances, and also identify the head nouns of the arguments of each VPC and simplex verb. For the head noun of each subject and object, as identified by RASP, we use WordNet 2.1 (Fellbaum, 1998) to obtain the word sense. Finally we build a supervised classifier using TiMBL 5.1 (Daelemans et al., 2004). 3.1 Method Compared to the method proposed by Baldwin (2005), our approach (a) tackles the task of VPC identification rather than VPC extraction, and (b) uses both syntactic and semantic features, employing the WordNet 2.1 senses of the subject and/or object(s) of the verb. In the sentence He put the coat on the table, e.g., to distinguish the VPC put on from the verb put occurring with the prepositional phrase on the table, we identify the senses of the head nouns of the subject and object(s) of the verb put (i.e. he and coat, respectively). First, we parse all sentences in the given corpus using RASP, and identify verbs and prepositions in the RASP o</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005. The deep lexical acquisition of English verb-particle constructions. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):398–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Timothy Baldwin</author>
<author>Alex Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verbparticles.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>65--72</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1503" citStr="Bannard et al., 2003" startWordPosition="223" endWordPosition="226">lay lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) o</context>
<context position="4285" citStr="Bannard et al. (2003)" startWordPosition="684" endWordPosition="687">takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1The learning of lexical items in a form that can be fed directly into a deep grammar or other richly-annotated lexical resource Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65–72, Trento, Italy, April 2006. c�2006 Association for Computational Linguistics 65 particles. Bannard et al. (2003) and McCarthy et al. (2003) investigate methods for estimating the compositionality of VPCs based largely on distributional similarity of the head verb and VPC. O’Hara and Wiebe (2003) propose a method for disambiguating the verb sense of verb-PPs. While our interest is in VPC identification—a fundamentally syntactic task—we draw on the shallow semantic processing employed in these methods in modelling the semantics of VPCs relative to their base verbs. The contribution of this paper is to combine syntactic and semantic features in the task of VPC identification. The basic intuition behind the</context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003. A statistical approach to the semantics of verbparticles. In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 65–72, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="8682" citStr="Briscoe and Carroll, 2002" startWordPosition="1422" endWordPosition="1425">f co-occurs with a subject of the class airplane, aeroplane. In (4), on the other hand, take off = remove and the corresponding object noun is of class garment or clothing. From the above, we can see that head nouns in the subject and object argument positions can be used to distinguish VPCs from simplex verbs with prepositional phrases (i.e. verb-PPs). 66 3 Approach Our goal is to distinguish VPCs from verb-PPs in corpus data, i.e. to take individual inputs such as Kim handed the paper in today and tag each as either a VPC or a verb-PP. Our basic approach is to parse each sentence with RASP (Briscoe and Carroll, 2002) to obtain a first-gloss estimate of the VPC and verb-PP token instances, and also identify the head nouns of the arguments of each VPC and simplex verb. For the head noun of each subject and object, as identified by RASP, we use WordNet 2.1 (Fellbaum, 1998) to obtain the word sense. Finally we build a supervised classifier using TiMBL 5.1 (Daelemans et al., 2004). 3.1 Method Compared to the method proposed by Baldwin (2005), our approach (a) tackles the task of VPC identification rather than VPC extraction, and (b) uses both syntactic and semantic features, employing the WordNet 2.1 senses of</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proc. of the 3rd International Conference on Language Resources and Evaluation (LREC 2002), pages 1499–1504, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicoletta Calzolari</author>
<author>Charles Fillmore</author>
<author>Ralph Grishman</author>
<author>Nancy Ide</author>
<author>Alessandro Lenci</author>
<author>Catherine MacLeod</author>
<author>Antonio Zampolli</author>
</authors>
<title>Towards best practice for multiword expressions in computational lexicons.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1934--40</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="979" citStr="Calzolari et al., 2002" startWordPosition="144" endWordPosition="147">ns (VPCs) automatically, based on the output of the RASP parser. The proposed method pools together instances of VPCs and verb-PPs from the parser output and uses the sentential context of each such instance to differentiate VPCs from verb-PPs. We show our technique to perform at an F-score of 97.4% at identifying VPCs in Wall Street Journal and Brown Corpus data taken from the Penn Treebank. 1 Introduction Multiword expressions (hereafter MWEs) are lexical items that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal i</context>
</contexts>
<marker>Calzolari, Fillmore, Grishman, Ide, Lenci, MacLeod, Zampolli, 2002</marker>
<rawString>Nicoletta Calzolari, Charles Fillmore, Ralph Grishman, Nancy Ide, Alessandro Lenci, Catherine MacLeod, and Antonio Zampolli. 2002. Towards best practice for multiword expressions in computational lexicons. In Proc. of the 3rd International Conference on Language Resources and Evaluation (LREC 2002), pages 1934–40, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Alex Lascarides</author>
</authors>
<title>Integrating symbolic and statistical representations: The lexicon pragmatics interface.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the ACL and 8th Conference of the EACL (ACL-EACL’97),</booktitle>
<pages>136--43</pages>
<location>Madrid,</location>
<contexts>
<context position="1481" citStr="Copestake and Lascarides, 1997" startWordPosition="219" endWordPosition="222"> multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He</context>
</contexts>
<marker>Copestake, Lascarides, 1997</marker>
<rawString>Ann Copestake and Alex Lascarides. 1997. Integrating symbolic and statistical representations: The lexicon pragmatics interface. In Proc. of the 35th Annual Meeting of the ACL and 8th Conference of the EACL (ACL-EACL’97), pages 136–43, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide.</title>
<date>2004</date>
<tech>ILK Technical Report 04-02.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2004. TiMBL: Tilburg Memory Based Learner, version 5.1, Reference Guide. ILK Technical Report 04-02.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fraser</author>
</authors>
<title>The Verb-Particle Combination in English.</title>
<date>1976</date>
<publisher>The Hague: Mouton.</publisher>
<contexts>
<context position="3839" citStr="Fraser (1976)" startWordPosition="619" endWordPosition="620">types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1The learning of lexical items in a form that can be fed directly into a deep grammar or other richly-annotated lexical resource Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65–72, Trento, Italy, April 2006. c�2006 Association for Computational Linguistics 65 particles. Bannard et al. (2003) and McCarthy et al. (2003) investigate methods for estimating the compositionality of VPCs based largely on distributional similarity of the head verb an</context>
</contexts>
<marker>Fraser, 1976</marker>
<rawString>B. Fraser. 1976. The Verb-Particle Combination in English. The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney Huddleston</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="2295" citStr="Huddleston and Pullum, 2002" startWordPosition="363" endWordPosition="366">eafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order. In the case that the object is pronominal, however, the VPC must occur in split word order (c.f. *He handed in it) (Huddleston and Pullum, 2002; Villavicencio, 2003). The semantics of the VPC can either derive transparently from the semantics of the head verb and particle (e.g. walk off) or be significantly removed from the semantics of the head verb and/or particle (e.g. lookup); analogously, the selectional preferences of VPCs can mirror those of their head verbs or alternatively diverge markedly. The syntax of the VPC can also coincide with that of the head verb (e.g. walk off) or alternatively diverge (e.g. lift off). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney Huddleston and Geoffrey K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Xiuhong Zhang</author>
<author>Cheng Niu</author>
<author>Yuankai Jiang</author>
<author>Rohini K Srihari</author>
</authors>
<title>An expert lexicon approach to identifying English phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the ACL,</booktitle>
<pages>513--20</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3315" citStr="Li et al. (2003)" startWordPosition="534" endWordPosition="537">th that of the head verb (e.g. walk off) or alternatively diverge (e.g. lift off). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs</context>
<context position="24018" citStr="Li et al. (2003)" startWordPosition="4130" endWordPosition="4133">ive and negative examples .8068 .8033 .8051 .8653 .8529 .8591 .8660 .8660 .8660 .9272 .8836 .9054 70 Types Error Rate Reduction (%) Freq Type # P R F f&gt;_1 V 4WS .962 .962 .962 1WS .958 .969 .963 f&gt;_1 P 4WS .769 .769 .769 1WS .800 .743 .770 f&gt;_5 V 4WS .964 .983 .974 1WS .950 .973 .962 f&gt;_5 P 4WS .889 .783 .832 1WS .813 .614 .749 Table 7: Results with hypernym expansion (4WS) and only the first sense (1WS), in terms of precision (P), recall (R) and F-score (F) to give us the best performance at VPC identification. We achieved a slightly better result than the 95.8%-97.5% performance reported by Li et al. (2003). However, considering that Li et al. (2003) need considerable time and human labour to generate hand-coded rules, our method has advantages in terms of both raw performance and labour efficiency. Combining the results for Table 5 and Table 6, we see that our method performs better for VPC identification than verb-PP identification. Since we do not take into account the data from group D with our method, the performance of verb-PP identification is low compared to that for RASP, which in turn leads to a decrement in the overall performance. Since we ignored the data from group D containing una</context>
</contexts>
<marker>Li, Zhang, Niu, Jiang, Srihari, 2003</marker>
<rawString>Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, and Rohini K. Srihari. 2003. An expert lexicon approach to identifying English phrasal verbs. In Proc. of the 41st Annual Meeting of the ACL, pages 513–20, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12857" citStr="Marcus et al., 1993" startWordPosition="2135" endWordPosition="2138">a simple first-sense WSD system has room for improvement, but is sufficient for our immediate purposes Verbs Particles Subjects Objects TiMBL Classifier Preprocessing v+p with Semantics e.g. take_off := [.. put_on := [.. look_after := [.. WordNet Word Senses RASP parser raw corpus text 67 in this paper. To evaluate our approach, we built a supervised classifier using the TiMBL 5.1 memorybased learner and training data extracted from the Brown and WSJ corpora. 4 Data Collection We evaluated out method by running RASP over Brown Corpus and Wall Street Journal, as contained in the Penn Treebank (Marcus et al., 1993). 4.1 Data Classification The data we consider is sentences containing prepositions tagged as either RP or II. Based on the output of RASP, we divide the data into four groups: Group A contains the verb–preposition token instances tagged tagged exclusively as VPCs (i.e. the preposition is never tagged as II in combination with the given head verb). Group B contains the verb–preposition token instances identified as VPCs by RASP where there were also instances of that same combination identified as verb-PPs. Group C contains the verb–preposition token instances identified as verb-PPs by RASP wh</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1526" citStr="McCarthy et al., 2003" startWordPosition="227" endWordPosition="230"> and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put th</context>
<context position="4312" citStr="McCarthy et al. (2003)" startWordPosition="689" endWordPosition="692">e syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1The learning of lexical items in a form that can be fed directly into a deep grammar or other richly-annotated lexical resource Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65–72, Trento, Italy, April 2006. c�2006 Association for Computational Linguistics 65 particles. Bannard et al. (2003) and McCarthy et al. (2003) investigate methods for estimating the compositionality of VPCs based largely on distributional similarity of the head verb and VPC. O’Hara and Wiebe (2003) propose a method for disambiguating the verb sense of verb-PPs. While our interest is in VPC identification—a fundamentally syntactic task—we draw on the shallow semantic processing employed in these methods in modelling the semantics of VPCs relative to their base verbs. The contribution of this paper is to combine syntactic and semantic features in the task of VPC identification. The basic intuition behind the proposed method is that th</context>
<context position="11431" citStr="McCarthy et al. (2003)" startWordPosition="1898" endWordPosition="1901"> WordNet and TiMBL RASP is used to identify the syntactic structure of each sentence, including the head nouns of arguments and first-gloss determination of whether a given preposition is incorporated in a VPC or verb-PP. The RASP output contains dependency tuples derived from the most probable parse, each of which includes a label identifying the nature of the dependency (e.g. SUBJ, DOBJ), the head word of the modifying constituent, and the head of the modified constituent. In addition, each word is tagged with a POS tag from which it is possible to determine the valence of any prepositions. McCarthy et al. (2003) evaluate the precision of RASP at identifying VPCs to be 87.6% and the recall to be 49.4%. However the paper does not evaluate the parser’s ability to distinguish sentences containing VPCs and sentences with verb-PPs. To better understand the baseline performance of RASP, we counted the number of false-positive examples tagged with RP and false-negative examples tagged with II, relative to gold-standard data. See Section 5 for details. We use WordNet to obtain the first-sense word sense of the head nouns of subject and object phrases, according to the default word sense ranking provided withi</context>
<context position="15127" citStr="McCarthy et al. (2003)" startWordPosition="2524" endWordPosition="2527">t error-free, i.e. VPCs may be parsed as verb-PPs FPR FNR Agreement Group A 4.08% — 95.24% Group B 3.96% — 99.61% Group C — 10.15% 93.27% Group D — 3.4% 99.20% Table 1: False positive rate (FPR), false negative rate (FNR) and inter-annotator agreement across the four groups of token instances f &gt; 1 f &gt; 5 VPC V-PP VPC V-PP Group A 5,223 0 3,787 0 Group B 1,312 0 1,108 0 Group C 0 995 0 217 Total 6,535 995 4,895 217 Table 2: The number of VPC and verb-PP token instances occurring in groups A, B and C at varying frequency cut-offs and vice versa. In particular, other than the reported results of McCarthy et al. (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP’s ability to distinguish VPCs and verb-PPs. Therefore, we manually checked the false-positive and false-negative rates in all four groups and obtained the performance of parser with respect to VPCs. The verb-PPs in group A and B are false-positives while the VPCs in group C and D are false-negatives (we consider the VPCs to be positive examples). To calculate the number of incorrect examples, two human annotators independently checked each verb–preposition instance. Table 1 details the rate of false-positives and false-ne</context>
<context position="22554" citStr="McCarthy et al. (2003)" startWordPosition="3860" endWordPosition="3863">1.0 .819 .901 f≥5 1.0 .919 .957 BA f≥1 f≥1 1.0 .959 .979 f≥5 f≥5 1.0 .962 .980 BC f≥1 f≥1 .809 .845 .827 f≥5 f≥1 .836 .922 .877 BAC f≥1 f≥1 f≥1 .962 .962 .962 f≥5 f≥5 f≥1 .964 .983 .974 Table 5: Results for VPC identification only (P = precision, R = recall, F = F-score) Data Freq Type P R F BC f≥1 f≥1 P+V f≥5 f≥1 P+V BAC f≥1 f≥1 P+V f≥5 f≥1 P+V Table 6: Results for VPC (=V) and verb-PP (=P) identification (P = precision, R = recall, F = Fscore) Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al. (2003). In Table 5, the precision (P) and recall (R) for VPCs are computed as follows: Data Correctly Tagged as VPCs P_ Data Retrieved as VPCs Data Correctly Tagged as VPCs R _ All VPCs in Data Set The performance of RASP in Table 6 shows how well it distinguishes between VPCs and verbPPs for ambiguous verb–preposition combinations. Since Table 6 shows the comparative performance of our method between VPCs and verbPPs, the performance of RASP with examples which are misrecognized as each other should be the guideline. Note, the baseline RASP accuracy, based on assigning the majority class to instanc</context>
<context position="25148" citStr="McCarthy et al. (2003)" startWordPosition="4318" endWordPosition="4321"> decrement in the overall performance. Since we ignored the data from group D containing unambiguous verb-PPs, the number of positive training instances for verb-PP identification was relatively small. As for the different number of word senses in Table 7, we conclude that the more word senses the better the performance, particularly for higher-frequency data items. In order to get a clearer sense of the impact of selectional preferences on the results, we investigated the relative performance over VPCs of varying semantic compositionality, based on 117 VPCs (f&gt;_1) attested in the data set of McCarthy et al. (2003). According to our hypothesis from above, we would expect VPCs with low compositionality to have markedly different selectional preferences to the corresponding simplex verb, and VPCs with high compositionality to have similar selectional preferences to the simplex verb. In terms of the performance of our method, therefore, we would expect the degree of compositionality to be inversely proportional to the system performance. We test this hypothesis in Figure 3, where we calculate the error rate reduction (in F-score) Compositionality Figure 3: Error rate reduction for VPCs of varying compositi</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>280--7</pages>
<location>Barcelona,</location>
<contexts>
<context position="12064" citStr="McCarthy et al. (2004)" startWordPosition="2002" endWordPosition="2005">he precision of RASP at identifying VPCs to be 87.6% and the recall to be 49.4%. However the paper does not evaluate the parser’s ability to distinguish sentences containing VPCs and sentences with verb-PPs. To better understand the baseline performance of RASP, we counted the number of false-positive examples tagged with RP and false-negative examples tagged with II, relative to gold-standard data. See Section 5 for details. We use WordNet to obtain the first-sense word sense of the head nouns of subject and object phrases, according to the default word sense ranking provided within WordNet. McCarthy et al. (2004) found that 54% of word tokens are used with their first (or default) sense. With the performance of current word sense disambiguation (WSD) systems hovering around 60-70%, a simple first-sense WSD system has room for improvement, but is sufficient for our immediate purposes Verbs Particles Subjects Objects TiMBL Classifier Preprocessing v+p with Semantics e.g. take_off := [.. put_on := [.. look_after := [.. WordNet Word Senses RASP parser raw corpus text 67 in this paper. To evaluate our approach, we built a supervised classifier using the TiMBL 5.1 memorybased learner and training data extra</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proc. of the 42nd Annual Meeting of the ACL, pages 280–7, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Preposition semantic classification via Treebank and FrameNet.</title>
<date>2003</date>
<booktitle>In Proc. of the 7th Conference on Natural Language Learning (CoNLL2003),</booktitle>
<pages>79--86</pages>
<location>Edmonton, Canada.</location>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>Tom O’Hara and Janyce Wiebe. 2003. Preposition semantic classification via Treebank and FrameNet. In Proc. of the 7th Conference on Natural Language Learning (CoNLL2003), pages 79–86, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Pearce</author>
</authors>
<title>Synonymy in collocation extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="1431" citStr="Pearce, 2001" startWordPosition="213" endWordPosition="214">tems that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that th</context>
</contexts>
<marker>Pearce, 2001</marker>
<rawString>Darren Pearce. 2001. Synonymy in collocation extraction. In Proceedings of the NAACL 2001 Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002),</booktitle>
<pages>1--15</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="954" citStr="Sag et al., 2002" startWordPosition="140" endWordPosition="143">rticle constructions (VPCs) automatically, based on the output of the RASP parser. The proposed method pools together instances of VPCs and verb-PPs from the parser output and uses the sentential context of each such instance to differentiate VPCs from verb-PPs. We show our technique to perform at an F-score of 97.4% at identifying VPCs in Wall Street Journal and Brown Corpus data taken from the Penn Treebank. 1 Introduction Multiword expressions (hereafter MWEs) are lexical items that can be decomposed into multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). </context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In Proc. of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002), pages 1–15, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>Verb-particle constructions and lexical resources.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>57--64</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2317" citStr="Villavicencio, 2003" startWordPosition="367" endWordPosition="368">the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order. In the case that the object is pronominal, however, the VPC must occur in split word order (c.f. *He handed in it) (Huddleston and Pullum, 2002; Villavicencio, 2003). The semantics of the VPC can either derive transparently from the semantics of the head verb and particle (e.g. walk off) or be significantly removed from the semantics of the head verb and/or particle (e.g. lookup); analogously, the selectional preferences of VPCs can mirror those of their head verbs or alternatively diverge markedly. The syntax of the VPC can also coincide with that of the head verb (e.g. walk off) or alternatively diverge (e.g. lift off). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the</context>
</contexts>
<marker>Villavicencio, 2003</marker>
<rawString>Aline Villavicencio. 2003. Verb-particle constructions and lexical resources. In Proc. of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 57–64, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>Verb-particle constructions in the world wide web.</title>
<date>2006</date>
<booktitle>Computational Linguistics Dimensions of Syntax and Semantics of Prepositions.</booktitle>
<editor>In Patrick Saint-Dizier, editor,</editor>
<publisher>Springer,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="3864" citStr="Villavicencio (2006)" startWordPosition="622" endWordPosition="623">s based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1The learning of lexical items in a form that can be fed directly into a deep grammar or other richly-annotated lexical resource Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65–72, Trento, Italy, April 2006. c�2006 Association for Computational Linguistics 65 particles. Bannard et al. (2003) and McCarthy et al. (2003) investigate methods for estimating the compositionality of VPCs based largely on distributional similarity of the head verb and VPC. O’Hara and Wiebe (</context>
</contexts>
<marker>Villavicencio, 2006</marker>
<rawString>Aline Villavicencio. 2006. Verb-particle constructions in the world wide web. In Patrick Saint-Dizier, editor, Computational Linguistics Dimensions of Syntax and Semantics of Prepositions. Springer, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>Automatic extraction of idioms using graph analysis and asymmetric lexicosyntactic patterns.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition,</booktitle>
<pages>48--56</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1552" citStr="Widdows and Dorow, 2005" startWordPosition="231" endWordPosition="235">ncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order. </context>
</contexts>
<marker>Widdows, Dorow, 2005</marker>
<rawString>Dominic Widdows and Beate Dorow. 2005. Automatic extraction of idioms using graph analysis and asymmetric lexicosyntactic patterns. In Proc. of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition, pages 48– 56, Ann Arbor, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>