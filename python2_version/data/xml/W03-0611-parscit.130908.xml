<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015820">
<title confidence="0.967117">
Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data
Corpus
</title>
<author confidence="0.992076">
Ehud Reiter
</author>
<affiliation confidence="0.9986645">
Department of Computing Science
University of Aberdeen
</affiliation>
<email confidence="0.994594">
ereiter@csd.abdn.ac.uk
</email>
<author confidence="0.992057">
Somayajulu Sripada
</author>
<affiliation confidence="0.998732">
Department of Computing Science
University of Aberdeen
</affiliation>
<email confidence="0.99683">
ssripada@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.995626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976357142857">
We present an empirical corpus study of the
meaning and usage of time phrases in weather
forecasts; this is based on a novel corpus anal-
ysis technique where we align phrases from
the forecast text with data extracted from a nu-
merical weather simulation. Previous papers
have summarised this analysis and discussed
the substantial variations we discovered among
individual writers, which was perhaps our most
surprising finding. In this paper we describe
our analysis procedure and results in consid-
erably more detail, and also discuss our cur-
rent work on using parallel text-data corpora to
learn the meanings of other types of words.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999135">
NLP systems that interact with the world often need mod-
els of what words mean in terms of the non-linguistic
world. In this paper, we describe how we have deter-
mined the meaning of time phrases in weather forecasts
by analysing a parallel corpus of (A) manually-written
weather forecast texts and (B) the numerical data (from a
weather simulation) that the human forecasters examined
when writing the textual forecasts. The analysis proce-
dure first aligns (associates) text fragments with data seg-
ments, and then infers the meaning of each time phrase by
statistically analysing the time of data segments that are
aligned to textual phrases that contain this time phrase.
This is broadly similar in concept to the use of parallel
multilingual corpora in machine translation (Brown et al.,
1990), except that our parallel corpus consists of texts and
underlying numeric data, not texts and their translations.
In other words, we are trying to learn what words mean
in terms of non-linguistic data, not the best translations
of words in another language.
Probably the biggest surprise in our analysis was the
substantial variation we saw between individuals. For
example, by evening apparently meant 1800 to some peo-
ple, but 0000 to others. Although the possibility of such
variation in individual idiolects has been acknowledged
in the past (for example, (Nunberg,1978; Parikh, 1994)),
it seems to be ignored by most recent work on lexical se-
mantics.
We have published other papers that have summarised
our key findings, notably variation between individu-
als (Reiter and Sripada, 2002a; Reiter and Sripada,
2002b); and also described the corpus itself (Sripada et
al., 2003b). The purpose of this paper is to describe
our analysis procedure (including alignment) and results
in detail, and to also discuss our current work on using
parallel text-data corpora to learn the meanings of other
types of words.
</bodyText>
<sectionHeader confidence="0.966403" genericHeader="method">
2 Previous Research
</sectionHeader>
<bodyText confidence="0.999873956521739">
Linguists and lexicographers have used a number of dif-
ferent techniques to determine the meanings of words.
These include asking native-speaker informants to judge
the acceptability and oddness of test sentences (Cruse,
1986); defining word senses via lexicographic analysis
of citations and corpora (Landau, 1984); and asking sub-
jects to respond to ‘fill in the blank’ questions (Cassidy
and Hall, 1996). These techniques have focused purely
on texts, and have not analysed how texts and words re-
late to non-linguistic representations of the meanings of
a text, which is our focus.
Psychologists interested in categorisation have done
formal experiments to determine which objects human
subjects consider to be in a mental category (Rosch,
1978; Malt et al., 1999). If we assume that the mean-
ing of a word is one or more mental categories, then this
research has shed considerable light on what words mean.
However, like all psychological research, it has examined
language usage in an artificial experimental context, not
naturally occurring language.
In the NLP community, models of word meanings are
typically either entered by a user or developer (for exam-
ple in Microsoft’s English Query natural-language inter-
</bodyText>
<figure confidence="0.6658424">
day hour wind dir wind speed
25-10-00 0 SSW 12
25-10-00 3 SSE 11
25-10-00 6 ESE 18
25-10-00 9 ESE 16
25-10-00 12 E 15
25-10-00 15 ENE 15
25-10-00 18 ENE 18
25-10-00 21 NNE 20
26-10-00 0 NNW 26
</figure>
<tableCaption confidence="0.995404">
Table 1: Wind (at 10m) extract from 24-Oct-00 data file
</tableCaption>
<bodyText confidence="0.999794545454545">
face) or derived from a hand-built knowledge base (eg,
(Reiter, 1991)). There is growing interest in trying to
learn word meanings from parallel text-data corpora, for
example (Siskind, 2001; Barzilay and Lee, 2002; Roy,
2002). We believe our work is unusual because we are
using naturally occurring texts and data. Siskind (2001),
in contrast, used data which was explicitly created for his
experiments; Barzilay and Lee (2002) used texts which
subjects had written for a previous experiment; and Roy
(2002) used both data and texts that were created for his
experiments.
</bodyText>
<sectionHeader confidence="0.949222" genericHeader="method">
3 SumTime Project and Corpora
</sectionHeader>
<bodyText confidence="0.999973878048781">
The SUMTIME project is investigating better technology
for building software systems that automatically gener-
ate textual summaries of time-series data. One of the
domains SUMTIME is working in is weather forecasts,
and in this domain we acquired a corpus of 1119 weather
forecasts (for off-shore oil rigs) written by five profes-
sional meteorologists (Sripada et al., 2002; Sripada et al.,
2003b). The reports were primarily based on the output
of a numerical weather simulation, and our corpus con-
tains this information as well as the forecast texts. Each
forecast is roughly 400 words long, giving a total corpus
size of about 400,000 words. The forecasts are split into
an initial section which gives an overview of the weather,
and then additional sections which give detailed forecasts
for different periods of time. Figure 1 shows an example
extract from a forecast text; this is the detailed descrip-
tion of predicted weather on 25 Oct 2000, from a forecast
issued at 3AM on 24 Oct 2000.
Much of our analysis has focused on statements de-
scribing predicted wind speed and direction at 10 meters
altitude during the first 72 hours after the forecast was is-
sued. In other words, the WIND(10M) field from the de-
tailed weather descriptions up to 3 days after the forecast
was issued. One reason for focusing on wind statements
is that they are based fairly directly on two fields from
the data files, predicted wind direction and speed; the re-
lationship between some of the other statements (such as
weather) and the data files is more complex. The pre-
dicted wind (at 10m) speed and direction on 25 Oct 2000,
from the 24 Oct 2000 data file, is shown in Table 1. This
is the primary information that the meteorologists looked
at when writing the wind statement in Figure 1, although
they also have access to other information sources, such
as satellite weather photographs.
Each forecast contains 3 such wind statements, with an
average length of approximately 10 words; hence there
are about 30,000 words in our wind-statement subcorpus.
This of course is very small compared to many text-only
corpora such as the British National Corpus (BNC), but
we believe that our weather forecast corpus is one of the
largest parallel text-data corpora in existence.
</bodyText>
<sectionHeader confidence="0.954228" genericHeader="method">
4 Analysis Procedure for Time Phrases
</sectionHeader>
<bodyText confidence="0.999555125">
One of SUMTIME’s research goals is to learn the meaning
of time phrases; in other words, what a forecaster meant
when he used a time phrase such as by evening or af-
ter midnight. We also wished to learn which time phrase
should be included in a computer-generated weather fore-
cast text to indicate a time; for example, which time
phrase should be used to indicate a change in the weather
at 1200. Note that it is rare for weather forecasts to ex-
plicitly mention numerical times such as 1200, and also
that although there are standard terminologies for some
meteorological phenomena such as cloud cover and pre-
cipitation, we are not aware of any standard terminologies
for the use of time phrases in weather forecasts.
We performed this analysis as follows. First we ex-
tracted the wind at 10 meters statements for the next 72
hours from all forecasts in our corpus, and parsed these
texts with a simple parser tuned to the linguistic structure
of these texts. The parser essentially broke sentences up
into individual phrases, and then recorded the speed, di-
rection, and time phrase mentioned in each such phrase,
along with other information (such as verb) which was
not used in the analysis described here. For example the
WIND (10M) statement from Figure 1 was broken up by
the parser into four wind phrases:
</bodyText>
<listItem confidence="0.98483475">
1. SSW 12-16
(speed:12-16, direction:SSW, timephrase: none)
2. BACKING ESE 16-20 IN THE MORNING,
(speed:16-20, direction:ESE, timephrase: IN THE
MORNING)
3. BACKING NE EARLY AFTERNOON
(speed:(16-20), direction:NE, timephrase: EARLY
AFTERNOON)
4. THEN NNW 24-28 LATE EVENING
(speed:24-28, direction:NNW, timephrase: LATE
EVENING)
FORECAST 00-24 GMT, WEDNESDAY, 25-Oct 2000
WIND(10M): SSW 12-16 BACKING ESE 16-20 IN THE MORNING, BACKING
NE EARLY AFTERNOON THEN NNW 24-28 LATE EVENING
(50M): SSW 15-20 BACKING ESE 20-25 IN THE MORNING, BACKING
NE EARLY AFTERNOON THEN NNW 30-35 LATE EVENING
SIG WAVE: 2.0-2.5 RISING 3.0-3.5 BY AFTERNOON
MAX WAVE: 3.0-4.0 RISING 5.0-5.5 BY AFTERNOON
WEATHER: RAIN SOON, CLEARING TO SHOWERS IN THE EVENING
VIS: GOOD BECOMING MODERATE IN RAIN
</listItem>
<figureCaption confidence="0.993626">
Figure 1: Extract from 5-day forecast issued on 24-Oct-00
</figureCaption>
<bodyText confidence="0.9997877375">
If a wind phrase did not specify speed or direction, the
parser assumed that this was unchanged from the pre-
vious wind phrase; such elision is common in weather
forecast texts. Thus, for example, the speed recorded for
BACKING NE EARLY AFTERNOON is 16-20, which
is the speed from the previous phrase (BACKING ESE
16-20 IN THE MORNING). Our parser successfully
parsed 3225 of the 3357 WIND(10M) statements; 132
(4%) of the statements could not be parsed. The parser
produced 8198 wind phrases in total.
From these 8198 wind phrases we selected those
phrases which (a) included a time phrase, (b) did not
use a qualifier such as mainly or occasionally, (c) did not
specify that wind speed or direction was variable, (d) for
which we had the corresponding data files, and (e) for
which we knew the forecast author. There were 3654
such phrases. The majority (4014 phrases) of the elim-
inated phrases did not specify a time phrase, such as the
first phrase (SSW 12-16) in the above example.
We next associated each wind phrase with an entry in
the corresponding data file. In other words, we aligned
the textual wind phrases with the numeric data file en-
tries. As in other uses of parallel corpora, good alignment
is essential in order for the results to be meaningful (Och
and Ney, 2000).
To associate data file entries with wind phrases, we
first searched the data file for entries which matched the
wind phrase. An entry matched if its speed was within the
range defined in the phrase, and if its direction was within
12 degrees of the direction mentioned in the phrase. In
343 cases, no data file entry matched the wind phrase. We
believe that such cases were mostly due to (a) forecasters
not literally reporting the data file, but instead adjusting
what they said based on their meteorological expertise
and on information not available to the numerical weather
simulation (such as satellite weather images); (b) fore-
casters reporting a simultaneous change in wind speed
and direction, when in fact speed and direction changed
at different times (this may be due to forecasters trying to
write texts quickly, so that they can use the most up-to-
date data (Reiter et al., 2003)); and (c) forecaster errors.
For example, the third phrase in our example, BACK-
ING NE EARLY AFTERNOON, does not match any of
the data file entries shown in Table 1. This could be be-
cause the forecaster decided that the numerical forecast
was underestimating the speed at which the wind was
shifting, and hence he believed that the wind would be
NE at 12 or 15, even though the data file predicted E and
ENE for these times. It could also be that the forecaster
made a mistake, and perhaps was intending to write ENE
but wrote NE instead because he was writing under time
pressure. In any case, wind phrases which did not match
any data file entries were dropped from our analysis.
Out of the 3311 matched wind phrases, 1434 (43%)
were unambiguous and only matched one data file en-
try. For example, the fourth wind phrase in our example,
THEN NNW 24-28 LATE EVENING, matches only one
data file entry, the one for 0000 on 26 Oct 2000.
1877 (57%) of the matched wind phrases were ambigu-
ous and matched more than one data file entry. Typically
this happened when the wind was changing slowly and
hence two or more adjacent data file entries matched the
wind phrase. In such cases we checked if one data file
entry had a speed which was was closer than the other
data files entries to the middle of the speed range in the
textual wind phrase. This heuristic produced a preferred
match for 1105 (33%) of the matched wind phrases, and
left 772 (23%) phrases as ambiguous and unmatched.
For example, the second wind phrase in our example,
BACKING ESE 16-20 IN THE MORNING, matches two
data file entries: 0600 (direction ESE, speed 18) and 0900
(direction ESE, speed 16). The midpoint of the 16-20
speed range reported in the forecast is 18, so our speed
heuristic matches this wind phrase to the 0600 data file
entry, since its speed is closer to the speed range midpoint
(indeed, it matches the midpoint).
We evaluated our alignment process by applying it to
the subset of wind phrases which used a time phrase
which we thought had a clear and unambiguous inter-
pretation, namely an absolute time (such as by 0600), by
</bodyText>
<table confidence="0.996856727272727">
time F1 F2 F3 F4 F5 total
0000 2 9 80 5 14 110
0300 1 1
0600 1 1
0900 0
1200 1 1
1500 2 1 1 2 6
1800 30 5 2 27 13 77
2100 13 6 8 2 11 40
total 37 22 91 34 42 236
Significance of differences: p 0.001
</table>
<tableCaption confidence="0.898962">
Table 2: Usage of by evening, by forecaster (mode in
bold)
</tableCaption>
<table confidence="0.997293909090909">
time F1 F2 F3 F4 F5 total
0000 2 1 3
0300 1 1
0600 1 1
0900 3 1 7 2 13
1200 23 71 86 11 191
1500 7 1 9 5 2 24
1800 2 2 1 5
2100 1 1
total 34 1 85 103 16 239
Significance of differences: p 0.1
</table>
<tableCaption confidence="0.883145">
Table 3: Usage of by midday, by forecaster (mode in
bold)
</tableCaption>
<bodyText confidence="0.999893625">
midday (which means 1200), by midnight (which means
0000), and by end ofperiod (which means either 0000 or
0600, depending on the forecast section). The matching
process was fairly accurate; in 86% of cases it produced
the expected meaning (such as 0000 for by midnight).
Clearly we would benefit from better matching and
alignment techniques, and we wonder if perhaps some of
the alignment techniques used for parallel multi-lingual
corpora (Och and Ney, 2000) could be adapted to help
align our text-data corpora. This is a topic we plan to
investigate in future research.
This matching/alignment procedure is different in de-
tail from the preliminary analysis procedure reported in
(Reiter and Sripada, 2002b). The procedure used in our
earlier paper aligned fewer phrases (1382 vs. 2539) and
had a higher error rate (22% vs. 14%), so it was inferior.
</bodyText>
<sectionHeader confidence="0.999951" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99886525">
We examined the association between time phrase and
time in the 2539 aligned (wind phrase, data file entry)
pairs. In this analysis, we regarded time phrases as dif-
ferent if they involved different head nouns (for example,
</bodyText>
<table confidence="0.992174454545455">
time F1 F2 F3 F4 F5 total
0000 215 9 15 239
0300 1 1
0600 0
0900 0
1200 1 1
1500 0
1800 0
2100 3 3 2 8
total 0 0 219 12 18 249
Significance of differences p 0.001(ANOVA: p = 0.06)
</table>
<tableCaption confidence="0.9146255">
Table 4: Usage of by late evening, by forecaster (mode in
bold)
</tableCaption>
<bodyText confidence="0.999914774647888">
by evening and by afternoon), different prepositions (for
example, midday and by midday) and/or different adjec-
tives (for example, by afternoon and by late afternoon).
However, we ignored determiners (for example, by this
evening was regarded as the same phrase as by evening).
Tables 2, 3, and 4 gives details of the usage of the three
most common non-contextual time phrases: by evening,
by midday, and by late evening. This tables also shows
the statistical significance of differences between fore-
casters, calculated with a a chi-square test (which treats
time as a categorical variable). As some colleagues
have expressed an interest in a one-way ANOVA analysis
(which compares mean time), we show this as well where
it gives a substantially different value from the chi-square
analysis. This data suggests that
by evening means different things to different peo-
ple; for example, F1 and F4 primarily use this phrase
to mean 1800, while F3 primarily uses this phrase to
mean 0000.
by midday was used in a very similar way by all fore-
casters (ignoring F2, who only used the term once).
by late evening was used by all forecasters (who
used this term) primarily to mean 0000. However,
the usages of the different forecasters was still sig-
nificantly different. This reflects a difference in the
distribution of usage; in particular, F3 almost always
(98% of cases) used this phrase to mean 0000, while
F4 and F5 used this phrase to mean 0000 in about
80% of cases.
These patterns are replicated across the corpus: some
phrases (such as by midday and by morning) are used in
the same way by all forecasters; some phrases (such as
by evening and by late morning) are used in very differ-
ent ways by the forecasters; and some phrases (such as by
late evening and by midnight) have the same core mean-
ing (eg, 0000) but different distributions around the core.
We have, incidentally, looked for seasonal variations in
meaning (for example, by evening meaning one thing in
the winter and another in the summer), but we have found
no evidence of such variation.
Roy (2002) has also noted variation in the meanings
that individuals assign to words, in his parallel text-data
study of object descriptions. For example, one object
might be described as having the colour pink by one sub-
ject, but other subjects might have problems identifying
the object when it was described as pink, because they did
not consider it to have this colour.
Table 5 presents the most common time-phrase used
by each forecaster for each time, including context-
dependent phrases such as later. This highlights ma-
jor ‘stylistic’ differences between forecasters in terms of
which time phrases they prefer to use. For example, F1
and F2 make heavy use of contextual time phrases such
as later and soon, while F5 (and to a lesser extent F4)
seem to prefer to avoid such terms. It is also interest-
ing that contextual time phrases are especially commonly
used to refer to the time 0300. We wonder if this could
reflect a ‘lexical gap’ in English; there are no commonly
used time phrases in English for times around 0300, and
perhaps this encourages the forecasters to use contextual
time phrases to refer to 0300.
Table 6 presents the most common (mode) meaning
of non-contextual time phrases, for each forecaster. Per-
haps not surprisingly, the greatest variability occurs when
a time phrase denoting a time period (morning, afternoon,
or evening) occurs without being modified by an adjec-
tive (early, mid, or late). The data also suggests that the
forecasters may disagree about the meaning of morning,
with F4 in particular considering morning to be the period
0300-0900, while F5 considers morning to be the period
0600-1200.
</bodyText>
<sectionHeader confidence="0.979104" genericHeader="evaluation">
6 Current and Future Work
</sectionHeader>
<subsectionHeader confidence="0.989749">
6.1 Verb Choice
</subsectionHeader>
<bodyText confidence="0.978788238095238">
We would like to use our corpus to learn choice rules
for verbs which are near-synonyms (Edmonds and Hirst,
2002). We are currently attempting to learn rules which
predict which of three possible verbs – decreasing, eas-
ing, and falling – are used when the wind speed de-
creases.
We have conducted two experiments. The first was a
semantic analysis, where we attempted to learn a choice
rule based on features extracted from the numerical data.
To do this, we used our aligned corpus to extract seman-
tic features which we thought could be relevant to this
decision (such as the amount by which the wind speed
has decreased), and then analysed this with Ripper (Co-
hen, 1995). This gave the rules shown in Figure 2; these
again show substantial variation between individual fore-
verb F1 F2 F3 F4 F5 total
decreasing 0 0 3 2 0 5
easing 1 19 0 0 0 20
falling 4 0 61 0 0 65
Table 7: Choice of wind decrease verb when subsequent
word is variable, by forecaster (mode in bold)
casters. These rules are mildly effective; 10-fold cross
validation error is 25%, compared to a baseline error rate
of 33% from always choosing the most common verb
(easing). These rules suggest that at least for some fore-
casters, decreasing suggests a larger change in the wind
speed than easing; this is the sort of near-synonym con-
notational difference that we expected to find. More sur-
prisingly (at least to us), the presence of forecast date in
some of the rules suggests that forecasters change how
they write over time. Perhaps in retrospect this should
not have been a surprise, because we have also observed
changes over time in how people write in a previous
project (Reiter et al., 2000).
We also analysed collocation effects, that is whether
we could predict verb choice based purely on the words
immediately preceding and following the verb (and hence
ignoring the numerical prediction data). This was done
on the complete corpus (not just verbs that were part of
successfully aligned phrases). It is difficult to directly
compare the collocation analysis with the semantic one
due to differences in the corpora texts used, but in gen-
eral terms the reduction in baseline error rate seems com-
parable to the semantic analysis. Some of the collocation
effects were both strong and forecaster-dependent. For
example, Table 7 shows the choice of wind decrease verb
when the word following the verb was variable (indicat-
ing wind direction was variable). In this context, fore-
casters F1 and F3 usually used falling; F2 always used
easing; and F4 always used decreasing (F5 never used
variable in his forecasts). Similar individual differences
were observed in other collocations. For example, when
the word preceding the verb was gradually, F3, F4, and
F5 preferred decreasing, but F2 always used easing (F1
never used gradually in his forecasts).
In summary, it seems that the choice between the near
synonyms decreasing, easing, and falling depends on
semantics: how much the actual wind speed has
changed;
collocation: immediately preceding and following
words in the sentence;
author: which forecaster wrote this particular text;
date: when the text was written.
</bodyText>
<table confidence="0.972458545454546">
time F1 F2 F3 F4 F5 all
0000 later later by late evening by midnight in evening later
0300 later soon soon soon tonight soon
0600 later overnight soon by morning later in period later
0900 soon soon soon by midday in morning soon
1200 by midday soon by midday by midday in morning by midday
1500 by soon by mid by mid early by mid
afternoon afternoon afternoon afternoon afternoon
1800 by evening by evening by late afternoon by evening by evening by evening
2100 later later by evening later in evening/ later/
by evening by evening
bold font means this phrases was at least twice as common as the second-most common term.
X/Y means X and Y were equally common
Table 5: Most common time-phrases for each time, by forecaster
Forecaster rule
F1 never 10 knots AND time interval 15 hours
F2 never 9 knots OR forecast date 2-November-2001
Choose decreasing if speed change 30 March 2001
F3 speed change
F4 forecast date
F5
Otherwise choose easing
</table>
<figureCaption confidence="0.99441">
Figure 2: Verb choice rule based on data features
</figureCaption>
<bodyText confidence="0.999972833333333">
All of these factors are important, and in particular the
kind of semantic differences investigated by (Edmonds
and Hirst, 2002) are only one factor among many, and do
not dominate the choice decision. We plan to continue
working on this and other analyses of near-synonyms,
and obtain a better idea of how these factors interact.
</bodyText>
<subsectionHeader confidence="0.999373">
6.2 Other corpora
</subsectionHeader>
<bodyText confidence="0.999956896551724">
In addition to the weather corpus, the SUMTIME project
also has access to a parallel text-data corpus of doc-
tors describing signal data from a neonatal intensive care
unit (Alberdi et al., 2001). We would like to analyse
this corpus to determine the meanings of words such as
steady and oscillations. However, a preliminary analy-
sis has suggested to us that we cannot conduct such an
analysis until we remove non-physiological events from
the data (Sripada et al., 2003a). For example, a doctor
may describe a signal as steady even when it contains a
large spike, if the doctor believes the spike is due to a
non-physiological event (such as a sensor falling off the
baby and then being replaced by a nurse). Hence non-
physiological events (known in this domain as ‘artifacts’)
must be removed from the data before it is possible to
analyse word meaning. We are currently working on ar-
tifact removal, and once this is complete we will start our
analysis of word meanings.
SUMTIME is also working on generating textual sum-
maries of gas turbine sensor data (Yu et al., 2003). Un-
fortunately in this domain, as in many other NLG appli-
cations (Reiter et al., 2003), there is no existing corpus
of manually written texts describing the input data. We
have explicitly asked two experts to write descriptions
of 38 signal fragments. This very small corpus showed
that once again there were major differences between in-
dividuals (Reiter and Sripada, 2002a), but the corpus is
too small to allow meaningful statistical analysis of word
meanings.
</bodyText>
<sectionHeader confidence="0.998484" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999671615384615">
To conclude, we believe that parallel text-data corpora
are a valuable resource for investigating lexical seman-
tic and pragmatic issues, and can help shed valuable light
on the fundamental question of how words relate to the
non-linguistic world. We have described in detail how we
have used such a corpus to analyse the meaning and usage
of time phrases in weather forecasts, and also sketched
our current work on other analyses of text-data corpora.
We hope that other researchers interested in semantics
and pragmatics will find our techniques interesting, and
consider whether they might be useful in exploring other
semantic and pragmatic questions about word meaning
usage.
</bodyText>
<table confidence="0.999109085714286">
phrase F1 F2 F3 F4 F5 combined
after midnight 0600 0600
afternoon * 1630 1630
around midday * * * 1200
by 0600 0600 0600
by afternoon 1500 1200 1200 1200 1200
by early afternoon * * * 1330 1330
by early evening 1800 * 1800
by early morning 0300 0300
by evening 1800 0000 0000 1800 0000 0000
by late afternoon 1800 * 1800
by late evening 0000 0000 0000 0000
by late morning * 0900 1200 1030
by mid afternoon 1500 1500 * 1500
by mid evening * 2100 * 2100
by mid morning * * * 0900
by midday 1200 * 1200 1200 1200 1200
by midnight 0000 0000 0000 0000
by morning 0600 0600 * 0600
during afternoon * * 1500
during evening 0000 0000 0000 0000
during morning * 1030 * 0900 0900
early afternoon * * * 1500 1500
early evening * 1800 1800
evening 1612 2100 0000 0000
from midday 1200 1200
in afternoon * * 1800 1800
in evening 0000 0000 0000
in morning 1200 1200
late evening * 0000 0000 0000
later in evening 0000 0000
later in night 0600 0600
mid morning * * * 0900
overnight 0600 0600 * 0600
tonight * 0000 0000
</table>
<tableCaption confidence="0.686158">
* means phrase was used fewer than five times by this forecaster.
Phrases used less than 5 times by all forecasters combined are omitted.
Contextual phrases (such as later) are also omitted.
If 2 or more times are equally common, their average is shown.
Table 6: Most common (mode) usage of time phrases, by forecaster
</tableCaption>
<sectionHeader confidence="0.989394" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999944571428572">
Our thanks to the many individuals who have discussed
this work with us, of which there are too many to list
here. Special thanks to our industrial collaborators at
WNI/Oceanroutes, without whom this work would have
been impossible. This work was supported by the UK En-
gineering and Physical Sciences Research Council (EP-
SRC), under grant GR/M76881.
</bodyText>
<sectionHeader confidence="0.998307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999065337078652">
Eugenio Alberdi, Julie-Clare Becher, Ken Gilhooly, Jim
Hunter, Robert Logie, Andy Lyon, Neil McIntosh, and
Jan Reiss. 2001. Expertise and the interpretation of
computerized physiological data: implications for the
design of computerized monitoring in neonatal inten-
sive care. International Journal of Human-Computer
Studies, 55:191–216.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2002).
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79–85.
Frederick Cassidy and Joan Hall, editors. 1996. Dictio-
nary ofAmerican Regional English. Belknap.
William Cohen. 1995. Fast effective rule induction.
In Proc. 12th International Conference on Machine
Learning, pages 115–123. Morgan Kaufmann.
D. Cruse. 1986. Lexical Semantics. Cambridge Univer-
sity Press.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Linguis-
tics, pages 105–144.
Sidney Landau. 1984. Dictionaries: the art and craft of
lexicography. Scribner.
Barbara Malt, Steven Sloman, Silvia Gennari, Meiyi Shi,
and Yuan Wang. 1999. Knowing versus naming:
Similarity and the linguistic categorization of artifacts.
Journal ofMemory and Language, 40:230–262.
Geoffrey Nunberg. 1978. The Pragmatics of Reference.
University of Indiana Linguistics Club, Bloomington,
Indiana.
Franz Och and Herman Ney. 2000. A comparison of
alignment models for statistical machine translation.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
1086–1090.
Rohit Parikh. 1994. Vagueness and utility: The seman-
tics of common nouns. Linguistics and Philosophy,
17:521–535.
Ehud Reiter and Somayajulu Sripada. 2002a. Human
variation and lexical choice. Computational Linguis-
tics, 28:545–553.
Ehud Reiter and Somayajulu Sripada. 2002b. Should
corpora texts be gold standards for NLG? In Proceed-
ings of the Second International Conference on Natu-
ral Language Generation, pages 97–104.
Ehud Reiter, Roma Robertson, and Liesl Osman. 2000.
Knowledge acquisition for natural language genera-
tion. In Proceedings of the First International Con-
ference on Natural Language Generation, pages 217–
215.
Ehud Reiter, Somayajulu Sripada, and Roma Robertson.
2003. Acquiring correct knowledge for natural lan-
guage generation. Journal ofArtificial Intelligence Re-
search. Forthcoming.
Ehud Reiter. 1991. A new model of lexical choice for
nouns. Computational Intelligence, 7(4):240–251.
Eleanor Rosch. 1978. Principles of categorization. In
E. Rosch and B. Lloyd, editors, Cognition and Catego-
rization, pages 27–48. Lawrence Erlbaum, Hillsdale,
NJ.
Deb Roy. 2002. Learning visually grounded words and
syntax for a scene description task. Computer Speech
and Language, 16:353–385.
Jeffrey Siskind. 2001. Grounding the lexical semantics
of verbs in visual perspection using force dynamics
and event logic. Journal of Artificial Intelligence Re-
search, 15:31–90.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2002. SUMTIME-METEO: Parallel corpus of
naturally occurring forecast texts and weather data.
Technical Report AUCS/TR0201, Computing Science
Dept, Univ of Aberdeen, Aberdeen AB24 3UE, UK.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003a. Exploiting a parallel text-data corpus.
In Proceedings of Corpus Linguistics 2003. UCREL,
Lancaster University, UK.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003b. Summarising neonatal time-series data. In
Proceedings of EACL-2003. Forthcoming.
Jin Yu, Ehud Reiter, Jim Hunter, and Somayajulu Sri-
pada. 2003. Sumtime-turbine: A knowledge-based
system to communicate gas turbine time-series data.
In Proceedings ofIEA/AIE-2003.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.393939">
<title confidence="0.9798745">Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data Corpus</title>
<author confidence="0.805847">Ehud</author>
<affiliation confidence="0.998773">Department of Computing University of</affiliation>
<email confidence="0.989311">ereiter@csd.abdn.ac.uk</email>
<author confidence="0.510232">Somayajulu</author>
<affiliation confidence="0.999071">Department of Computing University of</affiliation>
<email confidence="0.98949">ssripada@csd.abdn.ac.uk</email>
<abstract confidence="0.999529733333333">We present an empirical corpus study of the meaning and usage of time phrases in weather forecasts; this is based on a novel corpus analysis technique where we align phrases from the forecast text with data extracted from a numerical weather simulation. Previous papers have summarised this analysis and discussed the substantial variations we discovered among individual writers, which was perhaps our most surprising finding. In this paper we describe our analysis procedure and results in considerably more detail, and also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugenio Alberdi</author>
<author>Julie-Clare Becher</author>
<author>Ken Gilhooly</author>
<author>Jim Hunter</author>
<author>Robert Logie</author>
<author>Andy Lyon</author>
<author>Neil McIntosh</author>
<author>Jan Reiss</author>
</authors>
<title>Expertise and the interpretation of computerized physiological data: implications for the design of computerized monitoring in neonatal intensive care.</title>
<date>2001</date>
<journal>International Journal of Human-Computer Studies,</journal>
<pages>55--191</pages>
<contexts>
<context position="23892" citStr="Alberdi et al., 2001" startWordPosition="4073" endWordPosition="4076"> F5 Otherwise choose easing Figure 2: Verb choice rule based on data features All of these factors are important, and in particular the kind of semantic differences investigated by (Edmonds and Hirst, 2002) are only one factor among many, and do not dominate the choice decision. We plan to continue working on this and other analyses of near-synonyms, and obtain a better idea of how these factors interact. 6.2 Other corpora In addition to the weather corpus, the SUMTIME project also has access to a parallel text-data corpus of doctors describing signal data from a neonatal intensive care unit (Alberdi et al., 2001). We would like to analyse this corpus to determine the meanings of words such as steady and oscillations. However, a preliminary analysis has suggested to us that we cannot conduct such an analysis until we remove non-physiological events from the data (Sripada et al., 2003a). For example, a doctor may describe a signal as steady even when it contains a large spike, if the doctor believes the spike is due to a non-physiological event (such as a sensor falling off the baby and then being replaced by a nurse). Hence nonphysiological events (known in this domain as ‘artifacts’) must be removed f</context>
</contexts>
<marker>Alberdi, Becher, Gilhooly, Hunter, Logie, Lyon, McIntosh, Reiss, 2001</marker>
<rawString>Eugenio Alberdi, Julie-Clare Becher, Ken Gilhooly, Jim Hunter, Robert Logie, Andy Lyon, Neil McIntosh, and Jan Reiss. 2001. Expertise and the interpretation of computerized physiological data: implications for the design of computerized monitoring in neonatal intensive care. International Journal of Human-Computer Studies, 55:191–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple sequence alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP2002).</booktitle>
<contexts>
<context position="4531" citStr="Barzilay and Lee, 2002" startWordPosition="720" endWordPosition="723">the NLP community, models of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday hour wind dir wind speed 25-10-00 0 SSW 12 25-10-00 3 SSE 11 25-10-00 6 ESE 18 25-10-00 9 ESE 16 25-10-00 12 E 15 25-10-00 15 ENE 15 25-10-00 18 ENE 18 25-10-00 21 NNE 20 26-10-00 0 NNW 26 Table 1: Wind (at 10m) extract from 24-Oct-00 data file face) or derived from a hand-built knowledge base (eg, (Reiter, 1991)). There is growing interest in trying to learn word meanings from parallel text-data corpora, for example (Siskind, 2001; Barzilay and Lee, 2002; Roy, 2002). We believe our work is unusual because we are using naturally occurring texts and data. Siskind (2001), in contrast, used data which was explicitly created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains SUMTIME is working in is weather forecas</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple sequence alignment. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1721" citStr="Brown et al., 1990" startWordPosition="265" endWordPosition="268">ng of time phrases in weather forecasts by analysing a parallel corpus of (A) manually-written weather forecast texts and (B) the numerical data (from a weather simulation) that the human forecasters examined when writing the textual forecasts. The analysis procedure first aligns (associates) text fragments with data segments, and then infers the meaning of each time phrase by statistically analysing the time of data segments that are aligned to textual phrases that contain this time phrase. This is broadly similar in concept to the use of parallel multilingual corpora in machine translation (Brown et al., 1990), except that our parallel corpus consists of texts and underlying numeric data, not texts and their translations. In other words, we are trying to learn what words mean in terms of non-linguistic data, not the best translations of words in another language. Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg,1978; Parikh, 1994)), it seems </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Cassidy</author>
<author>Joan Hall</author>
<author>editors</author>
</authors>
<date>1996</date>
<booktitle>Dictionary ofAmerican Regional English.</booktitle>
<publisher>Belknap.</publisher>
<marker>Cassidy, Hall, editors, 1996</marker>
<rawString>Frederick Cassidy and Joan Hall, editors. 1996. Dictionary ofAmerican Regional English. Belknap.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. 12th International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="19885" citStr="Cohen, 1995" startWordPosition="3392" endWordPosition="3394">erbs which are near-synonyms (Edmonds and Hirst, 2002). We are currently attempting to learn rules which predict which of three possible verbs – decreasing, easing, and falling – are used when the wind speed decreases. We have conducted two experiments. The first was a semantic analysis, where we attempted to learn a choice rule based on features extracted from the numerical data. To do this, we used our aligned corpus to extract semantic features which we thought could be relevant to this decision (such as the amount by which the wind speed has decreased), and then analysed this with Ripper (Cohen, 1995). This gave the rules shown in Figure 2; these again show substantial variation between individual foreverb F1 F2 F3 F4 F5 total decreasing 0 0 3 2 0 5 easing 1 19 0 0 0 20 falling 4 0 61 0 0 65 Table 7: Choice of wind decrease verb when subsequent word is variable, by forecaster (mode in bold) casters. These rules are mildly effective; 10-fold cross validation error is 25%, compared to a baseline error rate of 33% from always choosing the most common verb (easing). These rules suggest that at least for some forecasters, decreasing suggests a larger change in the wind speed than easing; this i</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William Cohen. 1995. Fast effective rule induction. In Proc. 12th International Conference on Machine Learning, pages 115–123. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3077" citStr="Cruse, 1986" startWordPosition="480" endWordPosition="481">between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexicographic analysis of citations and corpora (Landau, 1984); and asking subjects to respond to ‘fill in the blank’ questions (Cassidy and Hall, 1996). These techniques have focused purely on texts, and have not analysed how texts and words relate to non-linguistic representations of the meanings of a text, which is our focus. Psychologists interested in categorisation have done formal experiments to determine which objects human subjects consider to be in a mental category (Rosch, 1978; Malt et al., 1999). If we assume that the meaning of a word is one or more men</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>D. Cruse. 1986. Lexical Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Graeme Hirst</author>
</authors>
<title>Nearsynonymy and lexical choice. Computational Linguistics,</title>
<date>2002</date>
<pages>105--144</pages>
<contexts>
<context position="19327" citStr="Edmonds and Hirst, 2002" startWordPosition="3293" endWordPosition="3296">g of non-contextual time phrases, for each forecaster. Perhaps not surprisingly, the greatest variability occurs when a time phrase denoting a time period (morning, afternoon, or evening) occurs without being modified by an adjective (early, mid, or late). The data also suggests that the forecasters may disagree about the meaning of morning, with F4 in particular considering morning to be the period 0300-0900, while F5 considers morning to be the period 0600-1200. 6 Current and Future Work 6.1 Verb Choice We would like to use our corpus to learn choice rules for verbs which are near-synonyms (Edmonds and Hirst, 2002). We are currently attempting to learn rules which predict which of three possible verbs – decreasing, easing, and falling – are used when the wind speed decreases. We have conducted two experiments. The first was a semantic analysis, where we attempted to learn a choice rule based on features extracted from the numerical data. To do this, we used our aligned corpus to extract semantic features which we thought could be relevant to this decision (such as the amount by which the wind speed has decreased), and then analysed this with Ripper (Cohen, 1995). This gave the rules shown in Figure 2; t</context>
<context position="23477" citStr="Edmonds and Hirst, 2002" startWordPosition="4002" endWordPosition="4005">ening/ later/ by evening by evening bold font means this phrases was at least twice as common as the second-most common term. X/Y means X and Y were equally common Table 5: Most common time-phrases for each time, by forecaster Forecaster rule F1 never 10 knots AND time interval 15 hours F2 never 9 knots OR forecast date 2-November-2001 Choose decreasing if speed change 30 March 2001 F3 speed change F4 forecast date F5 Otherwise choose easing Figure 2: Verb choice rule based on data features All of these factors are important, and in particular the kind of semantic differences investigated by (Edmonds and Hirst, 2002) are only one factor among many, and do not dominate the choice decision. We plan to continue working on this and other analyses of near-synonyms, and obtain a better idea of how these factors interact. 6.2 Other corpora In addition to the weather corpus, the SUMTIME project also has access to a parallel text-data corpus of doctors describing signal data from a neonatal intensive care unit (Alberdi et al., 2001). We would like to analyse this corpus to determine the meanings of words such as steady and oscillations. However, a preliminary analysis has suggested to us that we cannot conduct suc</context>
</contexts>
<marker>Edmonds, Hirst, 2002</marker>
<rawString>Philip Edmonds and Graeme Hirst. 2002. Nearsynonymy and lexical choice. Computational Linguistics, pages 105–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Landau</author>
</authors>
<title>Dictionaries: the art and craft of lexicography.</title>
<date>1984</date>
<publisher>Scribner.</publisher>
<contexts>
<context position="3166" citStr="Landau, 1984" startWordPosition="492" endWordPosition="493">cribed the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexicographic analysis of citations and corpora (Landau, 1984); and asking subjects to respond to ‘fill in the blank’ questions (Cassidy and Hall, 1996). These techniques have focused purely on texts, and have not analysed how texts and words relate to non-linguistic representations of the meanings of a text, which is our focus. Psychologists interested in categorisation have done formal experiments to determine which objects human subjects consider to be in a mental category (Rosch, 1978; Malt et al., 1999). If we assume that the meaning of a word is one or more mental categories, then this research has shed considerable light on what words mean. Howeve</context>
</contexts>
<marker>Landau, 1984</marker>
<rawString>Sidney Landau. 1984. Dictionaries: the art and craft of lexicography. Scribner.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Malt</author>
<author>Steven Sloman</author>
<author>Silvia Gennari</author>
<author>Meiyi Shi</author>
<author>Yuan Wang</author>
</authors>
<title>Knowing versus naming: Similarity and the linguistic categorization of artifacts.</title>
<date>1999</date>
<journal>Journal ofMemory and Language,</journal>
<pages>40--230</pages>
<contexts>
<context position="3617" citStr="Malt et al., 1999" startWordPosition="563" endWordPosition="566">ormants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexicographic analysis of citations and corpora (Landau, 1984); and asking subjects to respond to ‘fill in the blank’ questions (Cassidy and Hall, 1996). These techniques have focused purely on texts, and have not analysed how texts and words relate to non-linguistic representations of the meanings of a text, which is our focus. Psychologists interested in categorisation have done formal experiments to determine which objects human subjects consider to be in a mental category (Rosch, 1978; Malt et al., 1999). If we assume that the meaning of a word is one or more mental categories, then this research has shed considerable light on what words mean. However, like all psychological research, it has examined language usage in an artificial experimental context, not naturally occurring language. In the NLP community, models of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday hour wind dir wind speed 25-10-00 0 SSW 12 25-10-00 3 SSE 11 25-10-00 6 ESE 18 25-10-00 9 ESE 16 25-10-00 12 E 15 25-10-00 15 ENE 15 25-10-00 18 </context>
</contexts>
<marker>Malt, Sloman, Gennari, Shi, Wang, 1999</marker>
<rawString>Barbara Malt, Steven Sloman, Silvia Gennari, Meiyi Shi, and Yuan Wang. 1999. Knowing versus naming: Similarity and the linguistic categorization of artifacts. Journal ofMemory and Language, 40:230–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>The Pragmatics of Reference.</title>
<date>1978</date>
<institution>University of Indiana Linguistics Club,</institution>
<location>Bloomington, Indiana.</location>
<marker>Nunberg, 1978</marker>
<rawString>Geoffrey Nunberg. 1978. The Pragmatics of Reference. University of Indiana Linguistics Club, Bloomington, Indiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Herman Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000),</booktitle>
<pages>1086--1090</pages>
<contexts>
<context position="10636" citStr="Och and Ney, 2000" startWordPosition="1740" endWordPosition="1743">ecify that wind speed or direction was variable, (d) for which we had the corresponding data files, and (e) for which we knew the forecast author. There were 3654 such phrases. The majority (4014 phrases) of the eliminated phrases did not specify a time phrase, such as the first phrase (SSW 12-16) in the above example. We next associated each wind phrase with an entry in the corresponding data file. In other words, we aligned the textual wind phrases with the numeric data file entries. As in other uses of parallel corpora, good alignment is essential in order for the results to be meaningful (Och and Ney, 2000). To associate data file entries with wind phrases, we first searched the data file for entries which matched the wind phrase. An entry matched if its speed was within the range defined in the phrase, and if its direction was within 12 degrees of the direction mentioned in the phrase. In 343 cases, no data file entry matched the wind phrase. We believe that such cases were mostly due to (a) forecasters not literally reporting the data file, but instead adjusting what they said based on their meteorological expertise and on information not available to the numerical weather simulation (such as </context>
<context position="14610" citStr="Och and Ney, 2000" startWordPosition="2480" endWordPosition="2483">500 7 1 9 5 2 24 1800 2 2 1 5 2100 1 1 total 34 1 85 103 16 239 Significance of differences: p 0.1 Table 3: Usage of by midday, by forecaster (mode in bold) midday (which means 1200), by midnight (which means 0000), and by end ofperiod (which means either 0000 or 0600, depending on the forecast section). The matching process was fairly accurate; in 86% of cases it produced the expected meaning (such as 0000 for by midnight). Clearly we would benefit from better matching and alignment techniques, and we wonder if perhaps some of the alignment techniques used for parallel multi-lingual corpora (Och and Ney, 2000) could be adapted to help align our text-data corpora. This is a topic we plan to investigate in future research. This matching/alignment procedure is different in detail from the preliminary analysis procedure reported in (Reiter and Sripada, 2002b). The procedure used in our earlier paper aligned fewer phrases (1382 vs. 2539) and had a higher error rate (22% vs. 14%), so it was inferior. 5 Results We examined the association between time phrase and time in the 2539 aligned (wind phrase, data file entry) pairs. In this analysis, we regarded time phrases as different if they involved different</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Och and Herman Ney. 2000. A comparison of alignment models for statistical machine translation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000), pages 1086–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Parikh</author>
</authors>
<title>Vagueness and utility: The semantics of common nouns. Linguistics and Philosophy,</title>
<date>1994</date>
<pages>17--521</pages>
<contexts>
<context position="2309" citStr="Parikh, 1994" startWordPosition="359" endWordPosition="360">tion (Brown et al., 1990), except that our parallel corpus consists of texts and underlying numeric data, not texts and their translations. In other words, we are trying to learn what words mean in terms of non-linguistic data, not the best translations of words in another language. Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg,1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key findings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of differ</context>
</contexts>
<marker>Parikh, 1994</marker>
<rawString>Rohit Parikh. 1994. Vagueness and utility: The semantics of common nouns. Linguistics and Philosophy, 17:521–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Human variation and lexical choice.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--545</pages>
<contexts>
<context position="2510" citStr="Reiter and Sripada, 2002" startWordPosition="390" endWordPosition="393">mean in terms of non-linguistic data, not the best translations of words in another language. Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg,1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key findings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexico</context>
<context position="14858" citStr="Reiter and Sripada, 2002" startWordPosition="2519" endWordPosition="2522"> means either 0000 or 0600, depending on the forecast section). The matching process was fairly accurate; in 86% of cases it produced the expected meaning (such as 0000 for by midnight). Clearly we would benefit from better matching and alignment techniques, and we wonder if perhaps some of the alignment techniques used for parallel multi-lingual corpora (Och and Ney, 2000) could be adapted to help align our text-data corpora. This is a topic we plan to investigate in future research. This matching/alignment procedure is different in detail from the preliminary analysis procedure reported in (Reiter and Sripada, 2002b). The procedure used in our earlier paper aligned fewer phrases (1382 vs. 2539) and had a higher error rate (22% vs. 14%), so it was inferior. 5 Results We examined the association between time phrase and time in the 2539 aligned (wind phrase, data file entry) pairs. In this analysis, we regarded time phrases as different if they involved different head nouns (for example, time F1 F2 F3 F4 F5 total 0000 215 9 15 239 0300 1 1 0600 0 0900 0 1200 1 1 1500 0 1800 0 2100 3 3 2 8 total 0 0 219 12 18 249 Significance of differences p 0.001(ANOVA: p = 0.06) Table 4: Usage of by late evening, by fore</context>
<context position="25142" citStr="Reiter and Sripada, 2002" startWordPosition="4287" endWordPosition="4290">possible to analyse word meaning. We are currently working on artifact removal, and once this is complete we will start our analysis of word meanings. SUMTIME is also working on generating textual summaries of gas turbine sensor data (Yu et al., 2003). Unfortunately in this domain, as in many other NLG applications (Reiter et al., 2003), there is no existing corpus of manually written texts describing the input data. We have explicitly asked two experts to write descriptions of 38 signal fragments. This very small corpus showed that once again there were major differences between individuals (Reiter and Sripada, 2002a), but the corpus is too small to allow meaningful statistical analysis of word meanings. 7 Conclusion To conclude, we believe that parallel text-data corpora are a valuable resource for investigating lexical semantic and pragmatic issues, and can help shed valuable light on the fundamental question of how words relate to the non-linguistic world. We have described in detail how we have used such a corpus to analyse the meaning and usage of time phrases in weather forecasts, and also sketched our current work on other analyses of text-data corpora. We hope that other researchers interested in</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2002a. Human variation and lexical choice. Computational Linguistics, 28:545–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Should corpora texts be gold standards for NLG?</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Natural Language Generation,</booktitle>
<pages>97--104</pages>
<contexts>
<context position="2510" citStr="Reiter and Sripada, 2002" startWordPosition="390" endWordPosition="393">mean in terms of non-linguistic data, not the best translations of words in another language. Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg,1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key findings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexico</context>
<context position="14858" citStr="Reiter and Sripada, 2002" startWordPosition="2519" endWordPosition="2522"> means either 0000 or 0600, depending on the forecast section). The matching process was fairly accurate; in 86% of cases it produced the expected meaning (such as 0000 for by midnight). Clearly we would benefit from better matching and alignment techniques, and we wonder if perhaps some of the alignment techniques used for parallel multi-lingual corpora (Och and Ney, 2000) could be adapted to help align our text-data corpora. This is a topic we plan to investigate in future research. This matching/alignment procedure is different in detail from the preliminary analysis procedure reported in (Reiter and Sripada, 2002b). The procedure used in our earlier paper aligned fewer phrases (1382 vs. 2539) and had a higher error rate (22% vs. 14%), so it was inferior. 5 Results We examined the association between time phrase and time in the 2539 aligned (wind phrase, data file entry) pairs. In this analysis, we regarded time phrases as different if they involved different head nouns (for example, time F1 F2 F3 F4 F5 total 0000 215 9 15 239 0300 1 1 0600 0 0900 0 1200 1 1 1500 0 1800 0 2100 3 3 2 8 total 0 0 219 12 18 249 Significance of differences p 0.001(ANOVA: p = 0.06) Table 4: Usage of by late evening, by fore</context>
<context position="25142" citStr="Reiter and Sripada, 2002" startWordPosition="4287" endWordPosition="4290">possible to analyse word meaning. We are currently working on artifact removal, and once this is complete we will start our analysis of word meanings. SUMTIME is also working on generating textual summaries of gas turbine sensor data (Yu et al., 2003). Unfortunately in this domain, as in many other NLG applications (Reiter et al., 2003), there is no existing corpus of manually written texts describing the input data. We have explicitly asked two experts to write descriptions of 38 signal fragments. This very small corpus showed that once again there were major differences between individuals (Reiter and Sripada, 2002a), but the corpus is too small to allow meaningful statistical analysis of word meanings. 7 Conclusion To conclude, we believe that parallel text-data corpora are a valuable resource for investigating lexical semantic and pragmatic issues, and can help shed valuable light on the fundamental question of how words relate to the non-linguistic world. We have described in detail how we have used such a corpus to analyse the meaning and usage of time phrases in weather forecasts, and also sketched our current work on other analyses of text-data corpora. We hope that other researchers interested in</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Ehud Reiter and Somayajulu Sripada. 2002b. Should corpora texts be gold standards for NLG? In Proceedings of the Second International Conference on Natural Language Generation, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Roma Robertson</author>
<author>Liesl Osman</author>
</authors>
<title>Knowledge acquisition for natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Conference on Natural Language Generation,</booktitle>
<pages>217--215</pages>
<contexts>
<context position="20880" citStr="Reiter et al., 2000" startWordPosition="3571" endWordPosition="3574">%, compared to a baseline error rate of 33% from always choosing the most common verb (easing). These rules suggest that at least for some forecasters, decreasing suggests a larger change in the wind speed than easing; this is the sort of near-synonym connotational difference that we expected to find. More surprisingly (at least to us), the presence of forecast date in some of the rules suggests that forecasters change how they write over time. Perhaps in retrospect this should not have been a surprise, because we have also observed changes over time in how people write in a previous project (Reiter et al., 2000). We also analysed collocation effects, that is whether we could predict verb choice based purely on the words immediately preceding and following the verb (and hence ignoring the numerical prediction data). This was done on the complete corpus (not just verbs that were part of successfully aligned phrases). It is difficult to directly compare the collocation analysis with the semantic one due to differences in the corpora texts used, but in general terms the reduction in baseline error rate seems comparable to the semantic analysis. Some of the collocation effects were both strong and forecas</context>
</contexts>
<marker>Reiter, Robertson, Osman, 2000</marker>
<rawString>Ehud Reiter, Roma Robertson, and Liesl Osman. 2000. Knowledge acquisition for natural language generation. In Proceedings of the First International Conference on Natural Language Generation, pages 217– 215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu Sripada</author>
<author>Roma Robertson</author>
</authors>
<title>Acquiring correct knowledge for natural language generation.</title>
<date>2003</date>
<journal>Journal ofArtificial Intelligence Research. Forthcoming.</journal>
<contexts>
<context position="11529" citStr="Reiter et al., 2003" startWordPosition="1891" endWordPosition="1894"> the phrase. In 343 cases, no data file entry matched the wind phrase. We believe that such cases were mostly due to (a) forecasters not literally reporting the data file, but instead adjusting what they said based on their meteorological expertise and on information not available to the numerical weather simulation (such as satellite weather images); (b) forecasters reporting a simultaneous change in wind speed and direction, when in fact speed and direction changed at different times (this may be due to forecasters trying to write texts quickly, so that they can use the most up-todate data (Reiter et al., 2003)); and (c) forecaster errors. For example, the third phrase in our example, BACKING NE EARLY AFTERNOON, does not match any of the data file entries shown in Table 1. This could be because the forecaster decided that the numerical forecast was underestimating the speed at which the wind was shifting, and hence he believed that the wind would be NE at 12 or 15, even though the data file predicted E and ENE for these times. It could also be that the forecaster made a mistake, and perhaps was intending to write ENE but wrote NE instead because he was writing under time pressure. In any case, wind </context>
<context position="24856" citStr="Reiter et al., 2003" startWordPosition="4242" endWordPosition="4245">t contains a large spike, if the doctor believes the spike is due to a non-physiological event (such as a sensor falling off the baby and then being replaced by a nurse). Hence nonphysiological events (known in this domain as ‘artifacts’) must be removed from the data before it is possible to analyse word meaning. We are currently working on artifact removal, and once this is complete we will start our analysis of word meanings. SUMTIME is also working on generating textual summaries of gas turbine sensor data (Yu et al., 2003). Unfortunately in this domain, as in many other NLG applications (Reiter et al., 2003), there is no existing corpus of manually written texts describing the input data. We have explicitly asked two experts to write descriptions of 38 signal fragments. This very small corpus showed that once again there were major differences between individuals (Reiter and Sripada, 2002a), but the corpus is too small to allow meaningful statistical analysis of word meanings. 7 Conclusion To conclude, we believe that parallel text-data corpora are a valuable resource for investigating lexical semantic and pragmatic issues, and can help shed valuable light on the fundamental question of how words</context>
</contexts>
<marker>Reiter, Sripada, Robertson, 2003</marker>
<rawString>Ehud Reiter, Somayajulu Sripada, and Roma Robertson. 2003. Acquiring correct knowledge for natural language generation. Journal ofArtificial Intelligence Research. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>A new model of lexical choice for nouns.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="4386" citStr="Reiter, 1991" startWordPosition="700" endWordPosition="701"> all psychological research, it has examined language usage in an artificial experimental context, not naturally occurring language. In the NLP community, models of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday hour wind dir wind speed 25-10-00 0 SSW 12 25-10-00 3 SSE 11 25-10-00 6 ESE 18 25-10-00 9 ESE 16 25-10-00 12 E 15 25-10-00 15 ENE 15 25-10-00 18 ENE 18 25-10-00 21 NNE 20 26-10-00 0 NNW 26 Table 1: Wind (at 10m) extract from 24-Oct-00 data file face) or derived from a hand-built knowledge base (eg, (Reiter, 1991)). There is growing interest in trying to learn word meanings from parallel text-data corpora, for example (Siskind, 2001; Barzilay and Lee, 2002; Roy, 2002). We believe our work is unusual because we are using naturally occurring texts and data. Siskind (2001), in contrast, used data which was explicitly created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for buildin</context>
</contexts>
<marker>Reiter, 1991</marker>
<rawString>Ehud Reiter. 1991. A new model of lexical choice for nouns. Computational Intelligence, 7(4):240–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Rosch</author>
</authors>
<title>Principles of categorization.</title>
<date>1978</date>
<booktitle>Cognition and Categorization,</booktitle>
<pages>27--48</pages>
<editor>In E. Rosch and B. Lloyd, editors,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="3597" citStr="Rosch, 1978" startWordPosition="561" endWordPosition="562">e-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexicographic analysis of citations and corpora (Landau, 1984); and asking subjects to respond to ‘fill in the blank’ questions (Cassidy and Hall, 1996). These techniques have focused purely on texts, and have not analysed how texts and words relate to non-linguistic representations of the meanings of a text, which is our focus. Psychologists interested in categorisation have done formal experiments to determine which objects human subjects consider to be in a mental category (Rosch, 1978; Malt et al., 1999). If we assume that the meaning of a word is one or more mental categories, then this research has shed considerable light on what words mean. However, like all psychological research, it has examined language usage in an artificial experimental context, not naturally occurring language. In the NLP community, models of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday hour wind dir wind speed 25-10-00 0 SSW 12 25-10-00 3 SSE 11 25-10-00 6 ESE 18 25-10-00 9 ESE 16 25-10-00 12 E 15 25-10-00 15</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>Eleanor Rosch. 1978. Principles of categorization. In E. Rosch and B. Lloyd, editors, Cognition and Categorization, pages 27–48. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb Roy</author>
</authors>
<title>Learning visually grounded words and syntax for a scene description task. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--353</pages>
<contexts>
<context position="4543" citStr="Roy, 2002" startWordPosition="724" endWordPosition="725">s of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday hour wind dir wind speed 25-10-00 0 SSW 12 25-10-00 3 SSE 11 25-10-00 6 ESE 18 25-10-00 9 ESE 16 25-10-00 12 E 15 25-10-00 15 ENE 15 25-10-00 18 ENE 18 25-10-00 21 NNE 20 26-10-00 0 NNW 26 Table 1: Wind (at 10m) extract from 24-Oct-00 data file face) or derived from a hand-built knowledge base (eg, (Reiter, 1991)). There is growing interest in trying to learn word meanings from parallel text-data corpora, for example (Siskind, 2001; Barzilay and Lee, 2002; Roy, 2002). We believe our work is unusual because we are using naturally occurring texts and data. Siskind (2001), in contrast, used data which was explicitly created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains SUMTIME is working in is weather forecasts, and in t</context>
<context position="17545" citStr="Roy (2002)" startWordPosition="2996" endWordPosition="2997"> of cases. These patterns are replicated across the corpus: some phrases (such as by midday and by morning) are used in the same way by all forecasters; some phrases (such as by evening and by late morning) are used in very different ways by the forecasters; and some phrases (such as by late evening and by midnight) have the same core meaning (eg, 0000) but different distributions around the core. We have, incidentally, looked for seasonal variations in meaning (for example, by evening meaning one thing in the winter and another in the summer), but we have found no evidence of such variation. Roy (2002) has also noted variation in the meanings that individuals assign to words, in his parallel text-data study of object descriptions. For example, one object might be described as having the colour pink by one subject, but other subjects might have problems identifying the object when it was described as pink, because they did not consider it to have this colour. Table 5 presents the most common time-phrase used by each forecaster for each time, including contextdependent phrases such as later. This highlights major ‘stylistic’ differences between forecasters in terms of which time phrases they </context>
</contexts>
<marker>Roy, 2002</marker>
<rawString>Deb Roy. 2002. Learning visually grounded words and syntax for a scene description task. Computer Speech and Language, 16:353–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Siskind</author>
</authors>
<title>Grounding the lexical semantics of verbs in visual perspection using force dynamics and event logic.</title>
<date>2001</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>15--31</pages>
<contexts>
<context position="4507" citStr="Siskind, 2001" startWordPosition="718" endWordPosition="719">g language. In the NLP community, models of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday hour wind dir wind speed 25-10-00 0 SSW 12 25-10-00 3 SSE 11 25-10-00 6 ESE 18 25-10-00 9 ESE 16 25-10-00 12 E 15 25-10-00 15 ENE 15 25-10-00 18 ENE 18 25-10-00 21 NNE 20 26-10-00 0 NNW 26 Table 1: Wind (at 10m) extract from 24-Oct-00 data file face) or derived from a hand-built knowledge base (eg, (Reiter, 1991)). There is growing interest in trying to learn word meanings from parallel text-data corpora, for example (Siskind, 2001; Barzilay and Lee, 2002; Roy, 2002). We believe our work is unusual because we are using naturally occurring texts and data. Siskind (2001), in contrast, used data which was explicitly created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains SUMTIME is worki</context>
</contexts>
<marker>Siskind, 2001</marker>
<rawString>Jeffrey Siskind. 2001. Grounding the lexical semantics of verbs in visual perspection using force dynamics and event logic. Journal of Artificial Intelligence Research, 15:31–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>SUMTIME-METEO: Parallel corpus of naturally occurring forecast texts and weather data.</title>
<date>2002</date>
<tech>Technical Report AUCS/TR0201,</tech>
<institution>Computing Science Dept, Univ of Aberdeen,</institution>
<location>Aberdeen AB24 3UE, UK.</location>
<contexts>
<context position="5291" citStr="Sripada et al., 2002" startWordPosition="840" endWordPosition="843"> which was explicitly created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains SUMTIME is working in is weather forecasts, and in this domain we acquired a corpus of 1119 weather forecasts (for off-shore oil rigs) written by five professional meteorologists (Sripada et al., 2002; Sripada et al., 2003b). The reports were primarily based on the output of a numerical weather simulation, and our corpus contains this information as well as the forecast texts. Each forecast is roughly 400 words long, giving a total corpus size of about 400,000 words. The forecasts are split into an initial section which gives an overview of the weather, and then additional sections which give detailed forecasts for different periods of time. Figure 1 shows an example extract from a forecast text; this is the detailed description of predicted weather on 25 Oct 2000, from a forecast issued a</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2002</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2002. SUMTIME-METEO: Parallel corpus of naturally occurring forecast texts and weather data. Technical Report AUCS/TR0201, Computing Science Dept, Univ of Aberdeen, Aberdeen AB24 3UE, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Exploiting a parallel text-data corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<publisher>UCREL, Lancaster University, UK.</publisher>
<contexts>
<context position="2599" citStr="Sripada et al., 2003" startWordPosition="404" endWordPosition="407"> Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg,1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key findings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexicographic analysis of citations and corpora (Landau, 1984); and asking subjects to respond </context>
<context position="5313" citStr="Sripada et al., 2003" startWordPosition="844" endWordPosition="847">created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains SUMTIME is working in is weather forecasts, and in this domain we acquired a corpus of 1119 weather forecasts (for off-shore oil rigs) written by five professional meteorologists (Sripada et al., 2002; Sripada et al., 2003b). The reports were primarily based on the output of a numerical weather simulation, and our corpus contains this information as well as the forecast texts. Each forecast is roughly 400 words long, giving a total corpus size of about 400,000 words. The forecasts are split into an initial section which gives an overview of the weather, and then additional sections which give detailed forecasts for different periods of time. Figure 1 shows an example extract from a forecast text; this is the detailed description of predicted weather on 25 Oct 2000, from a forecast issued at 3AM on 24 Oct 2000. </context>
<context position="24167" citStr="Sripada et al., 2003" startWordPosition="4119" endWordPosition="4122">on. We plan to continue working on this and other analyses of near-synonyms, and obtain a better idea of how these factors interact. 6.2 Other corpora In addition to the weather corpus, the SUMTIME project also has access to a parallel text-data corpus of doctors describing signal data from a neonatal intensive care unit (Alberdi et al., 2001). We would like to analyse this corpus to determine the meanings of words such as steady and oscillations. However, a preliminary analysis has suggested to us that we cannot conduct such an analysis until we remove non-physiological events from the data (Sripada et al., 2003a). For example, a doctor may describe a signal as steady even when it contains a large spike, if the doctor believes the spike is due to a non-physiological event (such as a sensor falling off the baby and then being replaced by a nurse). Hence nonphysiological events (known in this domain as ‘artifacts’) must be removed from the data before it is possible to analyse word meaning. We are currently working on artifact removal, and once this is complete we will start our analysis of word meanings. SUMTIME is also working on generating textual summaries of gas turbine sensor data (Yu et al., 200</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2003</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003a. Exploiting a parallel text-data corpus. In Proceedings of Corpus Linguistics 2003. UCREL, Lancaster University, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Summarising neonatal time-series data.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL-2003. Forthcoming.</booktitle>
<contexts>
<context position="2599" citStr="Sripada et al., 2003" startWordPosition="404" endWordPosition="407"> Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg,1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key findings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defining word senses via lexicographic analysis of citations and corpora (Landau, 1984); and asking subjects to respond </context>
<context position="5313" citStr="Sripada et al., 2003" startWordPosition="844" endWordPosition="847">created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The SUMTIME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains SUMTIME is working in is weather forecasts, and in this domain we acquired a corpus of 1119 weather forecasts (for off-shore oil rigs) written by five professional meteorologists (Sripada et al., 2002; Sripada et al., 2003b). The reports were primarily based on the output of a numerical weather simulation, and our corpus contains this information as well as the forecast texts. Each forecast is roughly 400 words long, giving a total corpus size of about 400,000 words. The forecasts are split into an initial section which gives an overview of the weather, and then additional sections which give detailed forecasts for different periods of time. Figure 1 shows an example extract from a forecast text; this is the detailed description of predicted weather on 25 Oct 2000, from a forecast issued at 3AM on 24 Oct 2000. </context>
<context position="24167" citStr="Sripada et al., 2003" startWordPosition="4119" endWordPosition="4122">on. We plan to continue working on this and other analyses of near-synonyms, and obtain a better idea of how these factors interact. 6.2 Other corpora In addition to the weather corpus, the SUMTIME project also has access to a parallel text-data corpus of doctors describing signal data from a neonatal intensive care unit (Alberdi et al., 2001). We would like to analyse this corpus to determine the meanings of words such as steady and oscillations. However, a preliminary analysis has suggested to us that we cannot conduct such an analysis until we remove non-physiological events from the data (Sripada et al., 2003a). For example, a doctor may describe a signal as steady even when it contains a large spike, if the doctor believes the spike is due to a non-physiological event (such as a sensor falling off the baby and then being replaced by a nurse). Hence nonphysiological events (known in this domain as ‘artifacts’) must be removed from the data before it is possible to analyse word meaning. We are currently working on artifact removal, and once this is complete we will start our analysis of word meanings. SUMTIME is also working on generating textual summaries of gas turbine sensor data (Yu et al., 200</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2003</marker>
<rawString>Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003b. Summarising neonatal time-series data. In Proceedings of EACL-2003. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Yu</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Somayajulu Sripada</author>
</authors>
<title>Sumtime-turbine: A knowledge-based system to communicate gas turbine time-series data.</title>
<date>2003</date>
<booktitle>In Proceedings ofIEA/AIE-2003.</booktitle>
<contexts>
<context position="24769" citStr="Yu et al., 2003" startWordPosition="4226" endWordPosition="4229">a et al., 2003a). For example, a doctor may describe a signal as steady even when it contains a large spike, if the doctor believes the spike is due to a non-physiological event (such as a sensor falling off the baby and then being replaced by a nurse). Hence nonphysiological events (known in this domain as ‘artifacts’) must be removed from the data before it is possible to analyse word meaning. We are currently working on artifact removal, and once this is complete we will start our analysis of word meanings. SUMTIME is also working on generating textual summaries of gas turbine sensor data (Yu et al., 2003). Unfortunately in this domain, as in many other NLG applications (Reiter et al., 2003), there is no existing corpus of manually written texts describing the input data. We have explicitly asked two experts to write descriptions of 38 signal fragments. This very small corpus showed that once again there were major differences between individuals (Reiter and Sripada, 2002a), but the corpus is too small to allow meaningful statistical analysis of word meanings. 7 Conclusion To conclude, we believe that parallel text-data corpora are a valuable resource for investigating lexical semantic and prag</context>
</contexts>
<marker>Yu, Reiter, Hunter, Sripada, 2003</marker>
<rawString>Jin Yu, Ehud Reiter, Jim Hunter, and Somayajulu Sripada. 2003. Sumtime-turbine: A knowledge-based system to communicate gas turbine time-series data. In Proceedings ofIEA/AIE-2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>