<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9977115">
Fast Context-Free Parsing Requires Fast Boolean Matrix
Multiplication
</title>
<author confidence="0.99719">
Lillian Lee
</author>
<affiliation confidence="0.975778">
Division of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.98994">
33 Oxford Street
Cambridge, MA 012138
</address>
<email confidence="0.999606">
llee@eecs.harvard.edu
</email>
<sectionHeader confidence="0.994321" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888666666667">
Valiant showed that Boolean matrix
multiplication (BMM) can be used for
CFG parsing. We prove a dual re-
sult: CFG parsers running in time
0(IGI Iw13-6) on a grammar G and a
string w can be used to multiply m x m
Boolean matrices in time 0(m3-E/3).
In the process we also provide a formal
definition of parsing motivated by an
informal notion due to Lang. Our re-
sult establishes one of the first limita-
tions on general CFG parsing: a fast,
practical CFG parser would yield a
fast, practical BMM algorithm, which
is not believed to exist.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997903358490566">
The context-free grammar (CFG) formalism
was developed during the birth of the field of
computational linguistics. The standard meth-
ods for CFG parsing are the CKY algorithm
(Kasami, 1965; Younger, 1967) and Earley&apos;s al-
gorithm (Earley, 1970), both of which have a
worst-case running time of 0(gN3) for a CFG
(in Chomsky normal form) of size g and a string
of length N. Graham et al. (1980) give a vari-
ant of Earley&apos;s algorithm which runs in time
0(g N3/ log N). Valiant&apos;s parsing method is the
asymptotically fastest known (Valiant, 1975).
It uses Boolean matrix multiplication (BMM)
to speed up the dynamic programming in the
CKY algorithm: its worst-case running time is
0(gM(N)), where M(m) is the time it takes to
multiply two m x m Boolean matrices together.
The standard method for multiplying ma-
trices takes time 0(m3). There exist matrix
multiplication algorithms with time complexity
0(m3-5); for instance, Strassen&apos;s has a worst-
case running time of 0(m281) (Strassen, 1969),
and the fastest currently known has a worst-case
running time of 0(m2376) (Coppersmith and
Winograd, 1990). Unfortunately, the constants
involved are so large that these fast algorithms
(with the possible exception of Strassen&apos;s) can-
not be used in practice. As matrix multi-
plication is a very well-studied problem (see
Strassen&apos;s historical account (Strassen, 1990,
section 10)), it is highly unlikely that simple,
practical fast matrix multiplication algorithms
exist. Since the best BMM algorithms all rely
on general matrix multiplication&apos;, it is widely
believed that there are no practical 0(m3-6)
BMM algorithms.
One might therefore hope to find a way
to speed up CFG parsing without relying on
matrix multiplication. However, we show in
this paper that fast CFG parsing requires
fast Boolean matrix multiplication in a precise
sense: any parser running in time 0(g N3&apos;)
that represents parse data in a retrieval-efficient
way can be converted with little computational
overhead into a 0(m3-€/3) BMM algorithm.
Since it is very improbable that practical fast
matrix multiplication algorithms exist, we thus
establish one of the first nontrivial limitations
on practical CFG parsing.
The &amp;quot;four Russians&amp;quot; algorithm (Arlazarov et al.,
1970), the fastest BMM algorithm that does not sim-
ply use ordinary matrix multiplication, has worst-case
running time 0(m3/ log m).
</bodyText>
<page confidence="0.995205">
9
</page>
<bodyText confidence="0.999744285714286">
Our technique, adapted from that used by
Satta (1994) for tree-adjoining grammar (TAG)
parsing, is to show that BMM can be efficiently
reduced to CFG parsing. Satta&apos;s result does not
apply to CFG parsing, since it explicitly relies
on the properties of TAGs that allow them to
generate non-context-free languages.
</bodyText>
<sectionHeader confidence="0.991631" genericHeader="introduction">
2 Definitions
</sectionHeader>
<bodyText confidence="0.999359833333333">
A Boolean matrix is a matrix with entries from
the set {0, 1}. A Boolean matrix multiplication
algorithm takes as input two in x in Boolean ma-
trices A and B and returns their Boolean prod-
uct A x B, which is the m x m Boolean matrix
C whose entries C23 are defined by
</bodyText>
<equation confidence="0.9584675">
= V (aik A bki).
k=1
</equation>
<bodyText confidence="0.970337263157895">
That is, cjj = 1 if and only if there exists a
number k, 1 &lt; k &lt; m, such that aik = bki = 1.
We use the usual definition of a context-free
grammar (CFG) as a 4-tuple G = (E, V, R, S),
where E is the set of terminals, V is the set
of nonterminals, R is the set of productions,
and S E V is the start symbol. Given a string
w = w1w2 • • wAr over E*, where each wi is an
element of E, we use the notation w.1 to denote
the substring wiwi+i • • wi-itvj•
We will be concerned with the notion of
c-derivations, which are substring derivations
that are consistent with a derivation of an entire
string. Intuitively, A w3i is a c-derivation if
it is consistent with at least one parse of w.
Definition 1 Let G = (E, V, R, 8) be a CFG,
and let w = wiw2•• • w, wi E E. A nontermi-
nal A E V c-derives (consistently derives) w3i if
and only if the following conditions hold:
</bodyText>
<listItem confidence="0.9997255">
• A = w, and
• S = wt-1
</listItem>
<bodyText confidence="0.991725">
(These conditions together imply that S w.)
We would like our results to apply to all
&amp;quot;practical&amp;quot; parsers, but what does it mean for
a parser to be practical? First, we would like
to be able to retrieve constituent information
for all possible parses of a string (after all,
the recovery of structural information is what
distinguishes parsing algorithms from recogni-
tion algorithms); such information is very use-
ful for applications like natural language under-
standing, where multiple interpretations for a
sentence may result from different constituent
structures. Therefore, practical parsers should
keep track of c-derivations. Secondly, a parser
should create an output structure from which
information about constituents can be retrieved
in an efficient way — Satta (1994) points out an
observation of Lang to the effect that one can
consider the input string itself to be a retrieval-
inefficient representation of parse information.
In short, we require practical parsers to output
a representation of the parse forest for a string
that allows efficient retrieval of parse informa-
tion. Lang in fact argues that parsing means
exactly the production of a shared forest struc-
ture &amp;quot;from which any specific parse can be ex-
tracted in time linear with the size of the ex-
tracted parse tree&amp;quot; (Lang, 1994, pg. 487), and
Satta (1994) makes this assumption as well.
These notions lead us to equate practical
parsers with the class of c-parsers, which keep
track of c-derivations and may also calculate
general substring derivations as well.
</bodyText>
<construct confidence="0.730793">
Definition 2 A c-parser is an algorithm that
takes a CFG grammar G = (E, V, R, S) and
string w E E* as input and produces output
</construct>
<bodyText confidence="0.7272845">
.FG,111 acts as an oracle about parse in-
formation, as follows:
</bodyText>
<listItem confidence="0.999928">
• If A c-derives w, then YG,w(A,i, =
&amp;quot;yes&amp;quot;.
• If A 0* w (which implies that A does not
c-derive wO, then .FG,w(A,i, j) = &amp;quot;no&amp;quot;.
• YG,to answers queries in constant time.
</listItem>
<bodyText confidence="0.995153666666667">
Note that the answer .FG,ID gives can be arbi-
trary if A = w-1 but A does not c-derive w.
The constant-time constraint encodes the no-
tion that information extraction is efficient; ob-
serve that this is a stronger condition than that
called for by Lang.
</bodyText>
<page confidence="0.992655">
10
</page>
<bodyText confidence="0.999990764705882">
We define c-parsers in this way to make the
class of c-parsers as broad as possible. If we
had changed the first condition to &amp;quot;If A derives
...&amp;quot;, then Earley parsers would be excluded,
since they do not keep track of all substring
derivations. If we had written the second con-
dition as &amp;quot;If A does not c-derive w, then ... &amp;quot;,
then CKY parsers would not be c-parsers, since
they keep track of all substring derivations, not
just c-derivations. So as it stands, the class of
c-parsers includes tabular parsers (e.g. CKY),
where ..FG,. is the table of substring deriva-
tions, and Earley-type parsers, where .FG,„, is
the chart. Indeed, it includes all of the parsing
algorithms mentioned in the introduction, and
can be thought of as a formalization of Lang&apos;s
informal definition of parsing.
</bodyText>
<sectionHeader confidence="0.98604" genericHeader="method">
3 The reduction
</sectionHeader>
<bodyText confidence="0.99692922">
We will reduce BMM to c-parsing, thus prov-
ing that any c-parsing algorithm can be used
as a Boolean matrix multiplication algorithm.
Our method, adapted from that of Satta (1994)
(who considered the problem of parsing with
tree-adjoining grammars), is to encode informa-
tion about Boolean matrices into a CFG. Thus,
given two Boolean matrices, we need to produce
a string and a grammar such that parsing the
string with respect to the grammar yields out-
put from which information about the product
of the two matrices can be easily retrieved.
We can sketch the behavior of the grammar
as follows. Suppose entries aik in A and bk3 in
B are both 1. Assume we have some way to
break up array indices into two parts so that
i can be reconstructed from i1 and i2, j can
be reconstructed from j1 and j2, and k can be
reconstructed from k1 and k2. (We will describe
a way to do this later.) Then, we will have
the following derivation (for a quantity 8 to be
defined later) :
The key thing to observe is that C,1,31 generates
two nonterminals whose &amp;quot;inner&amp;quot; indices match,
and that these two nonterminals generate sub-
strings that lie exactly next to each other. The
&amp;quot;inner&amp;quot; indices constitute a check on kl, and the
substring adjacency constitutes a check on k2.
Let A and B be two Boolean matrices, each
of size m x m, and let C be their Boolean matrix
product, C = A x B. In the rest of this section,
we consider A, B, C, and m to be fixed. Set
n = [m1/31, and set 6 = n + 2. We will be
constructing a string of length 38; we choose 5
slightly larger than n in order to avoid having
epsilon-productions in our grammar.
Recall that aij is non-zero if and only if we
can find a non-zero aik and a non-zero such
that k = i. In essence, we need simply check
for the equality of indices k and T. We will
break matrix indices into two parts: our gram-
mar will check whether the first parts of k and
Tc are equal, and our string will check whether
the second parts are also equal, as we sketched
above. Encoding the indices ensures that the
grammar is of as small a size as possible, which
will be important for our time bound results.
Our index encoding function is as follows. Let
i be a matrix index, 1 &lt; i &lt; m. Then we define
the function f (i) = f2(i)) by
</bodyText>
<equation confidence="0.5859605">
= Li/n] (0 &lt; f1(i) &lt; n2), and
12(i) = (i mod n) + 2 (2 f2(i) n + 1).
</equation>
<bodyText confidence="0.999940882352941">
Since Ii and 12 are essentially the quotient and
remainder of integer division of i by n, we can
retrieve i from (MO, 12(i)). We will use the
notational shorthand of using subscripts instead
of the functions fi and f2, that is, we write
and i2 for f1(i) and f2(i).
It is now our job to create a CFG G =
(E, V, R, S) and a string w that encode infor-
mation about A and B and express constraints
about their product C. Our plan is to include
a set of nonterminals {Cp,q : 1 &lt;p, q &lt; n2} in
V so that cij = 1 if and only if Ci, c-derives
wi22+25. In section 3.1, we describe a version
of G and prove it has this c-derivation property.
Then, in section 3.2 we explain that G can easily
be converted to Chomsky normal form in such
a way as to preserve c-derivations.
</bodyText>
<equation confidence="0.8023415">
Wi2 • • • wk2+5 wk2+6+1 • • wi2+26 •
derived by derived by Bki
</equation>
<page confidence="0.987302">
11
</page>
<bodyText confidence="0.997037555555556">
We choose the set of terminals to be E =
{wt : 1 &lt; t &lt; 3n + 6}, and choose the string
to be parsed to be w = wiw2 • • • W3n+6.
We consider w to be made up of three
parts, x, y, and z, each of size 6: w =
w1w2 • • • Wn-1-2 Wn-I-3 • &amp;quot; W2n+4 W2n-1-5 &apos; • &apos; W3n+6.
Observe that for any i, 1 &lt; i &lt; m, wi, lies
within x, wi2+6 lies within y, and Wi2+26 lies
within z, since
</bodyText>
<equation confidence="0.999537333333333">
i2 E [2,n + 1],
i2+8E [n + 4, 2n + 3], and
i2 + 25 E [2n + 6, 3n + 5].
</equation>
<subsectionHeader confidence="0.998295">
3.1 The grammar
</subsectionHeader>
<bodyText confidence="0.999882">
Now we begin building the grammar G =
(E, V, R, S). We start with the nonterminals
V = {S} and the production set R = 0. We
add nonterminal W to V for generating arbi-
trary non-empty substrings of w; thus we need
the productions
</bodyText>
<equation confidence="0.723013">
(W-rules) W weWiwt, 1 &lt; &lt; 3m +6.
</equation>
<bodyText confidence="0.9576025">
Next we encode the entries of the input matrices
A and B in our grammar. We include sets of
</bodyText>
<equation confidence="0.53009675">
non-terminals {Ap,q : 1 &lt; p,q &lt; n2} and {Bp,q :
1 &lt; p, q &lt; n2}. Then, for every non-zero entry
a23 in A, we add the production
(A-rules) A1,3 wi, W w3, +6 .
</equation>
<bodyText confidence="0.903211">
For every non-zero entry b13 in B, we add the
production
</bodyText>
<equation confidence="0.608702">
(B-rules) Wi,±i±oW W32+26.
</equation>
<bodyText confidence="0.980169333333333">
We need to represent entries of C, so we cre-
ate nonterminals {Cpa : 1 &lt; p, q &lt; n2} and pro-
ductions
(C-rules)Cp,qAp,rBr,q, 1 &lt; p, q,r &lt; n2.
Finally, we complete the construction with
productions for the start symbol S:
</bodyText>
<equation confidence="0.887342">
(S-rules) S WCp,qW, 1 &lt; p,q &lt; n2.
</equation>
<bodyText confidence="0.99755">
We now prove the following result about the
grammar and string we have just described.
</bodyText>
<equation confidence="0.23750425">
Theorem 1 For 1 &lt; i,j &lt; m, the entry cij
in C is non-zero if and only if Ci1,31 c-derives
-22
o.
</equation>
<bodyText confidence="0.816633">
Proof. Fix i and j.
Let us prove the &amp;quot;only if&amp;quot; direction first.
Thus, suppose Ci) = 1. Then there exists a k
such that aik = bk3 = 1. Figure 1 sketches how
</bodyText>
<equation confidence="0.601754">
c-derives Wji:+26.
Claim 1 C1,1wg+26
</equation>
<bodyText confidence="0.997908666666667">
The production C1,1 --&gt;Aj1,k1Bk1,1 is one of
the C-rules in our grammar. Since aik = 1,
A1 ,k1 —+ wi,Wwk2+6 is one of our A-rules, and
since bki = 1, Bkiji wk2+1+6WWj2+26 is
one of our B-rules. Finally, since i2 + 1 &lt; (k2 +
6) - 1 and (k2 + 1 + 6) +1 &lt; (j2 + 25) - 1,
we have W wik22++15-1 and W wic22++225±-61,
since both substrings are of length at least one.
Therefore,
</bodyText>
<figure confidence="0.471508571428571">
wi2 WWk2+6 Wk2+1-1-6WWj2+25
derived by Ail,ki derived by Bki
j2+28
Wi2
and Claim 1 follows.
Claim 2 S w 212 - w332n++265 ±
1.
</figure>
<bodyText confidence="0.956359928571429">
This claim is essentially trivial, since by
the definition of the S-rules, we know that
S .WC„,j,W . We need only show that nei-
ther 4-1 nor W3n+6 is the empty string (and
hence can be derived by W); since 1 &lt; i2 - 1
and j2 + 26 + 1 &lt; 3n + 6, the claim holds.
Claims .1 and 2 together prove that C,1,31 c-
derives w3,22+26, as required.2
Next we prove the &amp;quot;if&amp;quot; direction. Sup-
pose Ci1,31 c-derives V.7102+26, which by definition
means C21,31 =* tV3222+25. Then there must be
a derivation resulting from the application of a
C-rule as follows:
Ail ,k&apos;Bici,i1 4:+25
</bodyText>
<footnote confidence="0.80858">
2This proof would have been simpler if we had al-
lowed W to derive the empty string. However, we avoid
epsilon-productions in order to facilitate the conversion
to Chomsky normal form, discussed later.
</footnote>
<page confidence="0.996274">
12
</page>
<figureCaption confidence="0.9798225">
Figure 1: Schematic of the derivation process when ath = bk3 = 1. The substrings derived by Aii,ki
and Bkiji lie right next to each other.
</figureCaption>
<bodyText confidence="0.9963745">
for some k&apos;. It must be the case that for some
t, Aii,k, =&apos;4.2 and B 74+2+125. But
then we must have the productions Azi,ki
wi,Wwe and Bic, ,31 wt+iWw32+26 with e
k&amp;quot; + 6 for some k&amp;quot;. But we can only have such
productions if there exists a number k such that
k1 = k&apos;, k2 = k&amp;quot;, aik = 1, and bki = 1; and this
implies that cii = 1. •
Examination of the proof reveals that we have
also shown the following two corollaries.
</bodyText>
<equation confidence="0.34760225">
Corollary 1 For 1 .&lt; i,j &lt; in, c = 1 if and
only if Ci1,31 W:+2°.
Corollary 2 S = w if and only if C is not
the all-zeroes matrix.
</equation>
<bodyText confidence="0.999170888888889">
Let us now calculate the size of G. V consists
of 0((n2)2) = 0(m4/3) nonterminals. R con-
tains 0(n) W-rules and 0((n2)2) = 0(m4/3)
S-rules. There are at most m2 A-rules, since
we have an A-rule for each non-zero entry in A;
similarly, there are at most m2 B-rules. And
lastly, there are (n2)3 = 0(m2) C-rules. There-
fore, our grammar is of size 0(m2); since G en-
codes matrices A and B, it is of optimal size.
</bodyText>
<subsectionHeader confidence="0.999939">
3.2 Chomsky normal form
</subsectionHeader>
<bodyText confidence="0.999991111111111">
We would like our results to be true for the
largest class of parsers possible. Since some
parsers require the input grammar to be in
Chomsky normal form (CNF), we therefore wish
to construct a CNF version G&apos; of G. However,
in order to preserve time bounds, we desire that
0(G11) = 0(1G1), and we also require that The-
orem 1 holds for G&apos; as well as G.
The standard algorithm for converting CFGs
to CNF can yield a quadratic blow-up in the
size of the grammar and thus is clearly un-
satisfactory for our purposes. However, since
G contains no epsilon-productions or unit pro-
ductions, it is easy to see that we can convert
G simply by introducing a small (0(n)) num-
ber of nonterminals without changing any c-
derivations for the Cmq. Thus, from now on we
will simply assume that G is in CNF.
</bodyText>
<subsectionHeader confidence="0.999257">
3.3 Time bounds
</subsectionHeader>
<bodyText confidence="0.999909666666667">
We are now in a position to prove our relation
between time bounds for Boolean matrix multi-
plication and time bounds for CFG parsing.
</bodyText>
<page confidence="0.997635">
13
</page>
<figure confidence="0.472574">
Theorem 2 Any c-parser P with running time
0(T(g)t(N)) on grammars of size g and
strings of length N can be converted into
a BMM algorithm Mp that runs in time
0 (max(m2 ,T (m2 )t(mi 3 ))). In particular, if P
takes time 0(g N3-6), then Mp runs in time
0(m3-613).
</figure>
<figureCaption confidence="0.372213">
Proof. Mp acts as follows. Given two Boolean
</figureCaption>
<bodyText confidence="0.999645256410256">
m x m matrices A and B, it constructs G and
w as described above. It feeds G and w to P,
which outputs FG,W. To compute the prod-
uct matrix C, Mp queries for each i and j,
1 &lt; i,j &lt; m, whether C1,1 derives wii.r26
(we do not need to ask whether Ci,o, c-derives
4:1-25 because of corollary 1), setting cii appro-
priately. By definition of c-parsers, each such
query takes constant time. Let us compute the
running time of Mp. It takes 0(m2) time to
read the input matrices. Since G is of size
0(m2) and w = 0(m113), it takes 0(m2) time
to build the input to P, which then computes
.7&amp;quot;G,„, in time 0(T(m2)t(m1/3)). Retrieving C
takes 0(m2). So the total time spent by Mp is
0(max(m2,T(m2)i(mi./3))), as was claimed.
In the case where T(g) = g and t(N) = N3-6,
Mp has a running time of 0(m2 (mil3)3-6)
0(m2-1-1-6/3) = 0(m3-e/3). •
The case in which P takes time linear in the
grammar size is of the most interest, since in
natural language processing applications, the
grammar tends to be far larger than the strings
to be parsed. Observe that theorem 2 trans-
lates the running time of the standard CFG
parsers, 0(gN3), into the running time of the
standard BMM algorithm, 0(m3). Also, a c-
parser with running time 0(gN2.43) would yield
a matrix multiplication algorithm rivalling that
of Strassen&apos;s, and a c-parser with running time
better than 0(gN1.12) could be converted into
a BMM method faster than Coppersmith and
Winograd. As per the discussion above, even if
such parsers exist, they would in all likelihood
not be very practical. Finally, we note that if
a lower bound on BMM of the form 1/(m3&apos;)
were found, then we would have an immediate
lower bound of 12(N3-3&apos;) on c-parsers running
in time linear in g.
</bodyText>
<sectionHeader confidence="0.987637" genericHeader="method">
4 Related results and conclusion
</sectionHeader>
<bodyText confidence="0.999849032258064">
We have shown that fast practical CFG parsing
algorithms yield fast practical BMM algorithms.
Given that fast practical BMM algorithms are
unlikely to exist, we have established a limita-
tion on practical CFG parsing.
Valiant (personal communication) notes that
there is a reduction of m x m Boolean matrix
multiplication checking to context-free recog-
nition of strings of length m2; this reduc-
tion is alluded to in a footnote of a paper
by Harrison and Havel (1974). However, this
reduction converts a parser running in time
0(1w11.5) to a BMM checking algorithm run-
ning in time 0(m3) (the running time of the
standard multiplication method), whereas our
result says that sub-cubic practical parsers are
quite unlikely; thus, our result is quite a bit
stronger.
Seiferas (1986) gives a simple proof of
anC2(N2 ) lower bound (originally due to
log N
Gallaire (1969)) for the problem of on-line lin-
ear CFL recognition by multitape Turing ma-
chines. However, his results concern on-line
recognition, which is a harder problem than
parsing, and so do not apply to the general off-
line parsing case.
Finally, we recall Valiant&apos;s reduction of
CFG parsing to boolean matrix multiplication
(Valiant, 1975); it is rather pleasing to have the
reduction cycle completed.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="conclusions">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999380166666667">
I thank Joshua Goodman, Rebecca Hwa, Jon
Kleinberg, and Stuart Shieber for many helpful
comments and conversations. Thanks to Les
Valiant for pointing out the &amp;quot;folklore&amp;quot; reduc-
tion. This material is based upon work sup-
ported in part by the National Science Foun-
dation under Grant No. IRI-9350192. I also
gratefully acknowledge partial support from
an NSF Graduate Fellowship and an AT&amp;T
GRPW/ALFP grant. Finally, thanks to Gior-
gio Satta, who mailed me a preprint of his
BMM/TAG paper several years ago.
</bodyText>
<page confidence="0.998646">
14
</page>
<sectionHeader confidence="0.995899" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873795918367">
Arlazarov, V. L., E. A. Dinic, M. A. Kronrod, and
I. A. Faradiev. 1970. On economical construc-
tion of the transitive closure of an oriented graph.
Soviet Math. Dokl., 11:1209-1210. English trans-
lation of the Russian article in Dokl. Akad. Nauk
SSSR 194 (1970).
Coppersmith, Don and Shmuel Winograd. 1990.
Matrix multiplication via arithmetic progression.
Journal of Symbolic Computation, 9(3):251-280.
Special Issue on Computational Algebraic Com-
plexity.
Earley, Jay. 1970. An efficient context-free pars-
ing algorithm. Communications of the ACM,
13(2):94-102.
Gallaire, Herve. 1969. Recognition time of context-
free languages by on-line turing machines. Infor-
mation and Control, 15(3):288-295, September.
Graham, Susan L., Michael A. Harrison, and Wal-
ter L. Ruzzo. 1980. An improved context-free
recognizer. ACM Transactions on Programming
Languages and Systems, 2(3):415-462.
Harrison, Michael and Ivan Havel. 1974. On the
parsing of deterministic languages. Journal of the
ACM, 21(4):525-548, October.
Kasami, Tadao. 1965. An efficient recognition and
syntax algorithm for context-free languages. Sci-
entific Report AFCRL-65-758, Air Force Cam-
bridge Research Lab, Bedford, MA.
Lang, Bernard. 1994. Recognition can be
harder than parsing. Computational Intelligence,
10(4):486-494, November.
Satta, Giorgio. 1994. Tree-adjoining grammar pars-
ing and boolean matrix multiplication. Computa-
tional Linguistics, 20(2):173-191, June.
Seiferas, Joel. 1986. A simplified lower bound
for context-free-language recognition. Informa-
tion and Control, 69:255-260.
Strassen, Volker. 1969. Gaussian elimination is not
optimal. Numerische Mathematik, 14(3):354-356.
Strassen, Volker. 1990. Algebraic complexity the-
ory. In Jan van Leeuwen, editor, Handbook of
Theoretical Computer Science, volume A. Elsevier
Science Publishers, chapter 11, pages 633-672.
Valiant, Leslie G. 1975. General context-free recog-
nition in less than cubic time. Journal of Com-
puter and System Sciences, 10:308-315.
Younger, Daniel H. 1967. Recognition and parsing
of context-free languages in time n3. Information
and Control, 10(2):189-208.
</reference>
<page confidence="0.997925">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956836">
<title confidence="0.9997915">Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication</title>
<author confidence="0.999902">Lillian Lee</author>
<affiliation confidence="0.999448">Division of Engineering and Applied Sciences Harvard University</affiliation>
<address confidence="0.999275">33 Oxford Street Cambridge, MA 012138</address>
<email confidence="0.999875">llee@eecs.harvard.edu</email>
<abstract confidence="0.9973665">Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing. We prove a dual result: CFG parsers running in time on a grammar a string w can be used to multiply m x m matrices in time In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V L Arlazarov</author>
<author>E A Dinic</author>
<author>M A Kronrod</author>
<author>I A Faradiev</author>
</authors>
<title>On economical construction of the transitive closure of an oriented graph.</title>
<date>1970</date>
<journal>Soviet Math. Dokl.,</journal>
<booktitle>English translation of the Russian article in Dokl. Akad. Nauk SSSR 194</booktitle>
<pages>11--1209</pages>
<contexts>
<context position="2993" citStr="Arlazarov et al., 1970" startWordPosition="472" endWordPosition="475">ne might therefore hope to find a way to speed up CFG parsing without relying on matrix multiplication. However, we show in this paper that fast CFG parsing requires fast Boolean matrix multiplication in a precise sense: any parser running in time 0(g N3&apos;) that represents parse data in a retrieval-efficient way can be converted with little computational overhead into a 0(m3-€/3) BMM algorithm. Since it is very improbable that practical fast matrix multiplication algorithms exist, we thus establish one of the first nontrivial limitations on practical CFG parsing. The &amp;quot;four Russians&amp;quot; algorithm (Arlazarov et al., 1970), the fastest BMM algorithm that does not simply use ordinary matrix multiplication, has worst-case running time 0(m3/ log m). 9 Our technique, adapted from that used by Satta (1994) for tree-adjoining grammar (TAG) parsing, is to show that BMM can be efficiently reduced to CFG parsing. Satta&apos;s result does not apply to CFG parsing, since it explicitly relies on the properties of TAGs that allow them to generate non-context-free languages. 2 Definitions A Boolean matrix is a matrix with entries from the set {0, 1}. A Boolean matrix multiplication algorithm takes as input two in x in Boolean mat</context>
</contexts>
<marker>Arlazarov, Dinic, Kronrod, Faradiev, 1970</marker>
<rawString>Arlazarov, V. L., E. A. Dinic, M. A. Kronrod, and I. A. Faradiev. 1970. On economical construction of the transitive closure of an oriented graph. Soviet Math. Dokl., 11:1209-1210. English translation of the Russian article in Dokl. Akad. Nauk SSSR 194 (1970).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Coppersmith</author>
<author>Shmuel Winograd</author>
</authors>
<title>Matrix multiplication via arithmetic progression.</title>
<date>1990</date>
<journal>Journal of Symbolic Computation,</journal>
<booktitle>Special Issue on Computational Algebraic Complexity.</booktitle>
<pages>9--3</pages>
<contexts>
<context position="1857" citStr="Coppersmith and Winograd, 1990" startWordPosition="300" endWordPosition="303">. Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time 0(m3). There exist matrix multiplication algorithms with time complexity 0(m3-5); for instance, Strassen&apos;s has a worstcase running time of 0(m281) (Strassen, 1969), and the fastest currently known has a worst-case running time of 0(m2376) (Coppersmith and Winograd, 1990). Unfortunately, the constants involved are so large that these fast algorithms (with the possible exception of Strassen&apos;s) cannot be used in practice. As matrix multiplication is a very well-studied problem (see Strassen&apos;s historical account (Strassen, 1990, section 10)), it is highly unlikely that simple, practical fast matrix multiplication algorithms exist. Since the best BMM algorithms all rely on general matrix multiplication&apos;, it is widely believed that there are no practical 0(m3-6) BMM algorithms. One might therefore hope to find a way to speed up CFG parsing without relying on matrix</context>
</contexts>
<marker>Coppersmith, Winograd, 1990</marker>
<rawString>Coppersmith, Don and Shmuel Winograd. 1990. Matrix multiplication via arithmetic progression. Journal of Symbolic Computation, 9(3):251-280. Special Issue on Computational Algebraic Complexity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>13--2</pages>
<contexts>
<context position="1007" citStr="Earley, 1970" startWordPosition="161" endWordPosition="162">w can be used to multiply m x m Boolean matrices in time 0(m3-E/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 Introduction The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley&apos;s algorithm (Earley, 1970), both of which have a worst-case running time of 0(gN3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time 0(g N3/ log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time 0(m3). There exist ma</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herve Gallaire</author>
</authors>
<title>Recognition time of contextfree languages by on-line turing machines.</title>
<date>1969</date>
<journal>Information and Control,</journal>
<pages>15--3</pages>
<contexts>
<context position="18634" citStr="Gallaire (1969)" startWordPosition="3462" endWordPosition="3463"> notes that there is a reduction of m x m Boolean matrix multiplication checking to context-free recognition of strings of length m2; this reduction is alluded to in a footnote of a paper by Harrison and Havel (1974). However, this reduction converts a parser running in time 0(1w11.5) to a BMM checking algorithm running in time 0(m3) (the running time of the standard multiplication method), whereas our result says that sub-cubic practical parsers are quite unlikely; thus, our result is quite a bit stronger. Seiferas (1986) gives a simple proof of anC2(N2 ) lower bound (originally due to log N Gallaire (1969)) for the problem of on-line linear CFL recognition by multitape Turing machines. However, his results concern on-line recognition, which is a harder problem than parsing, and so do not apply to the general offline parsing case. Finally, we recall Valiant&apos;s reduction of CFG parsing to boolean matrix multiplication (Valiant, 1975); it is rather pleasing to have the reduction cycle completed. 5 Acknowledgments I thank Joshua Goodman, Rebecca Hwa, Jon Kleinberg, and Stuart Shieber for many helpful comments and conversations. Thanks to Les Valiant for pointing out the &amp;quot;folklore&amp;quot; reduction. This ma</context>
</contexts>
<marker>Gallaire, 1969</marker>
<rawString>Gallaire, Herve. 1969. Recognition time of contextfree languages by on-line turing machines. Information and Control, 15(3):288-295, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Graham</author>
<author>Michael A Harrison</author>
<author>Walter L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<pages>2--3</pages>
<contexts>
<context position="1155" citStr="Graham et al. (1980)" startWordPosition="189" endWordPosition="192">y an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 Introduction The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley&apos;s algorithm (Earley, 1970), both of which have a worst-case running time of 0(gN3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time 0(g N3/ log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time 0(m3). There exist matrix multiplication algorithms with time complexity 0(m3-5); for instance, Strassen&apos;s has a worstcase running time of 0(m281) (Strassen, 1969), and </context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Graham, Susan L., Michael A. Harrison, and Walter L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2(3):415-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Harrison</author>
<author>Ivan Havel</author>
</authors>
<title>On the parsing of deterministic languages.</title>
<date>1974</date>
<journal>Journal of the ACM,</journal>
<pages>21--4</pages>
<contexts>
<context position="18235" citStr="Harrison and Havel (1974)" startWordPosition="3395" endWordPosition="3398">he form 1/(m3&apos;) were found, then we would have an immediate lower bound of 12(N3-3&apos;) on c-parsers running in time linear in g. 4 Related results and conclusion We have shown that fast practical CFG parsing algorithms yield fast practical BMM algorithms. Given that fast practical BMM algorithms are unlikely to exist, we have established a limitation on practical CFG parsing. Valiant (personal communication) notes that there is a reduction of m x m Boolean matrix multiplication checking to context-free recognition of strings of length m2; this reduction is alluded to in a footnote of a paper by Harrison and Havel (1974). However, this reduction converts a parser running in time 0(1w11.5) to a BMM checking algorithm running in time 0(m3) (the running time of the standard multiplication method), whereas our result says that sub-cubic practical parsers are quite unlikely; thus, our result is quite a bit stronger. Seiferas (1986) gives a simple proof of anC2(N2 ) lower bound (originally due to log N Gallaire (1969)) for the problem of on-line linear CFL recognition by multitape Turing machines. However, his results concern on-line recognition, which is a harder problem than parsing, and so do not apply to the ge</context>
</contexts>
<marker>Harrison, Havel, 1974</marker>
<rawString>Harrison, Michael and Ivan Havel. 1974. On the parsing of deterministic languages. Journal of the ACM, 21(4):525-548, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages. Scientific Report AFCRL-65-758, Air Force Cambridge Research Lab,</title>
<date>1965</date>
<location>Bedford, MA.</location>
<contexts>
<context position="953" citStr="Kasami, 1965" startWordPosition="153" endWordPosition="154">ng in time 0(IGI Iw13-6) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time 0(m3-E/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 Introduction The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley&apos;s algorithm (Earley, 1970), both of which have a worst-case running time of 0(gN3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time 0(g N3/ log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, Tadao. 1965. An efficient recognition and syntax algorithm for context-free languages. Scientific Report AFCRL-65-758, Air Force Cambridge Research Lab, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Recognition can be harder than parsing.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<pages>10--4</pages>
<contexts>
<context position="5924" citStr="Lang, 1994" startWordPosition="1010" endWordPosition="1011">which information about constituents can be retrieved in an efficient way — Satta (1994) points out an observation of Lang to the effect that one can consider the input string itself to be a retrievalinefficient representation of parse information. In short, we require practical parsers to output a representation of the parse forest for a string that allows efficient retrieval of parse information. Lang in fact argues that parsing means exactly the production of a shared forest structure &amp;quot;from which any specific parse can be extracted in time linear with the size of the extracted parse tree&amp;quot; (Lang, 1994, pg. 487), and Satta (1994) makes this assumption as well. These notions lead us to equate practical parsers with the class of c-parsers, which keep track of c-derivations and may also calculate general substring derivations as well. Definition 2 A c-parser is an algorithm that takes a CFG grammar G = (E, V, R, S) and string w E E* as input and produces output .FG,111 acts as an oracle about parse information, as follows: • If A c-derives w, then YG,w(A,i, = &amp;quot;yes&amp;quot;. • If A 0* w (which implies that A does not c-derive wO, then .FG,w(A,i, j) = &amp;quot;no&amp;quot;. • YG,to answers queries in constant time. Note</context>
</contexts>
<marker>Lang, 1994</marker>
<rawString>Lang, Bernard. 1994. Recognition can be harder than parsing. Computational Intelligence, 10(4):486-494, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
</authors>
<title>Tree-adjoining grammar parsing and boolean matrix multiplication.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="3175" citStr="Satta (1994)" startWordPosition="504" endWordPosition="505">ication in a precise sense: any parser running in time 0(g N3&apos;) that represents parse data in a retrieval-efficient way can be converted with little computational overhead into a 0(m3-€/3) BMM algorithm. Since it is very improbable that practical fast matrix multiplication algorithms exist, we thus establish one of the first nontrivial limitations on practical CFG parsing. The &amp;quot;four Russians&amp;quot; algorithm (Arlazarov et al., 1970), the fastest BMM algorithm that does not simply use ordinary matrix multiplication, has worst-case running time 0(m3/ log m). 9 Our technique, adapted from that used by Satta (1994) for tree-adjoining grammar (TAG) parsing, is to show that BMM can be efficiently reduced to CFG parsing. Satta&apos;s result does not apply to CFG parsing, since it explicitly relies on the properties of TAGs that allow them to generate non-context-free languages. 2 Definitions A Boolean matrix is a matrix with entries from the set {0, 1}. A Boolean matrix multiplication algorithm takes as input two in x in Boolean matrices A and B and returns their Boolean product A x B, which is the m x m Boolean matrix C whose entries C23 are defined by = V (aik A bki). k=1 That is, cjj = 1 if and only if there</context>
<context position="5402" citStr="Satta (1994)" startWordPosition="920" endWordPosition="921">we would like to be able to retrieve constituent information for all possible parses of a string (after all, the recovery of structural information is what distinguishes parsing algorithms from recognition algorithms); such information is very useful for applications like natural language understanding, where multiple interpretations for a sentence may result from different constituent structures. Therefore, practical parsers should keep track of c-derivations. Secondly, a parser should create an output structure from which information about constituents can be retrieved in an efficient way — Satta (1994) points out an observation of Lang to the effect that one can consider the input string itself to be a retrievalinefficient representation of parse information. In short, we require practical parsers to output a representation of the parse forest for a string that allows efficient retrieval of parse information. Lang in fact argues that parsing means exactly the production of a shared forest structure &amp;quot;from which any specific parse can be extracted in time linear with the size of the extracted parse tree&amp;quot; (Lang, 1994, pg. 487), and Satta (1994) makes this assumption as well. These notions lead</context>
<context position="7755" citStr="Satta (1994)" startWordPosition="1336" endWordPosition="1337">ce they keep track of all substring derivations, not just c-derivations. So as it stands, the class of c-parsers includes tabular parsers (e.g. CKY), where ..FG,. is the table of substring derivations, and Earley-type parsers, where .FG,„, is the chart. Indeed, it includes all of the parsing algorithms mentioned in the introduction, and can be thought of as a formalization of Lang&apos;s informal definition of parsing. 3 The reduction We will reduce BMM to c-parsing, thus proving that any c-parsing algorithm can be used as a Boolean matrix multiplication algorithm. Our method, adapted from that of Satta (1994) (who considered the problem of parsing with tree-adjoining grammars), is to encode information about Boolean matrices into a CFG. Thus, given two Boolean matrices, we need to produce a string and a grammar such that parsing the string with respect to the grammar yields output from which information about the product of the two matrices can be easily retrieved. We can sketch the behavior of the grammar as follows. Suppose entries aik in A and bk3 in B are both 1. Assume we have some way to break up array indices into two parts so that i can be reconstructed from i1 and i2, j can be reconstruct</context>
</contexts>
<marker>Satta, 1994</marker>
<rawString>Satta, Giorgio. 1994. Tree-adjoining grammar parsing and boolean matrix multiplication. Computational Linguistics, 20(2):173-191, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Seiferas</author>
</authors>
<title>A simplified lower bound for context-free-language recognition.</title>
<date>1986</date>
<journal>Information and Control,</journal>
<pages>69--255</pages>
<contexts>
<context position="18547" citStr="Seiferas (1986)" startWordPosition="3446" endWordPosition="3447">ave established a limitation on practical CFG parsing. Valiant (personal communication) notes that there is a reduction of m x m Boolean matrix multiplication checking to context-free recognition of strings of length m2; this reduction is alluded to in a footnote of a paper by Harrison and Havel (1974). However, this reduction converts a parser running in time 0(1w11.5) to a BMM checking algorithm running in time 0(m3) (the running time of the standard multiplication method), whereas our result says that sub-cubic practical parsers are quite unlikely; thus, our result is quite a bit stronger. Seiferas (1986) gives a simple proof of anC2(N2 ) lower bound (originally due to log N Gallaire (1969)) for the problem of on-line linear CFL recognition by multitape Turing machines. However, his results concern on-line recognition, which is a harder problem than parsing, and so do not apply to the general offline parsing case. Finally, we recall Valiant&apos;s reduction of CFG parsing to boolean matrix multiplication (Valiant, 1975); it is rather pleasing to have the reduction cycle completed. 5 Acknowledgments I thank Joshua Goodman, Rebecca Hwa, Jon Kleinberg, and Stuart Shieber for many helpful comments and </context>
</contexts>
<marker>Seiferas, 1986</marker>
<rawString>Seiferas, Joel. 1986. A simplified lower bound for context-free-language recognition. Information and Control, 69:255-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volker Strassen</author>
</authors>
<title>Gaussian elimination is not optimal.</title>
<date>1969</date>
<journal>Numerische Mathematik,</journal>
<pages>14--3</pages>
<contexts>
<context position="1749" citStr="Strassen, 1969" startWordPosition="286" endWordPosition="287"> Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time 0(g N3/ log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time 0(m3). There exist matrix multiplication algorithms with time complexity 0(m3-5); for instance, Strassen&apos;s has a worstcase running time of 0(m281) (Strassen, 1969), and the fastest currently known has a worst-case running time of 0(m2376) (Coppersmith and Winograd, 1990). Unfortunately, the constants involved are so large that these fast algorithms (with the possible exception of Strassen&apos;s) cannot be used in practice. As matrix multiplication is a very well-studied problem (see Strassen&apos;s historical account (Strassen, 1990, section 10)), it is highly unlikely that simple, practical fast matrix multiplication algorithms exist. Since the best BMM algorithms all rely on general matrix multiplication&apos;, it is widely believed that there are no practical 0(m3</context>
</contexts>
<marker>Strassen, 1969</marker>
<rawString>Strassen, Volker. 1969. Gaussian elimination is not optimal. Numerische Mathematik, 14(3):354-356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volker Strassen</author>
</authors>
<title>Algebraic complexity theory.</title>
<date>1990</date>
<booktitle>Handbook of Theoretical Computer Science, volume A. Elsevier Science Publishers, chapter 11,</booktitle>
<pages>633--672</pages>
<editor>In Jan van Leeuwen, editor,</editor>
<contexts>
<context position="2115" citStr="Strassen, 1990" startWordPosition="340" endWordPosition="341">m Boolean matrices together. The standard method for multiplying matrices takes time 0(m3). There exist matrix multiplication algorithms with time complexity 0(m3-5); for instance, Strassen&apos;s has a worstcase running time of 0(m281) (Strassen, 1969), and the fastest currently known has a worst-case running time of 0(m2376) (Coppersmith and Winograd, 1990). Unfortunately, the constants involved are so large that these fast algorithms (with the possible exception of Strassen&apos;s) cannot be used in practice. As matrix multiplication is a very well-studied problem (see Strassen&apos;s historical account (Strassen, 1990, section 10)), it is highly unlikely that simple, practical fast matrix multiplication algorithms exist. Since the best BMM algorithms all rely on general matrix multiplication&apos;, it is widely believed that there are no practical 0(m3-6) BMM algorithms. One might therefore hope to find a way to speed up CFG parsing without relying on matrix multiplication. However, we show in this paper that fast CFG parsing requires fast Boolean matrix multiplication in a precise sense: any parser running in time 0(g N3&apos;) that represents parse data in a retrieval-efficient way can be converted with little com</context>
</contexts>
<marker>Strassen, 1990</marker>
<rawString>Strassen, Volker. 1990. Algebraic complexity theory. In Jan van Leeuwen, editor, Handbook of Theoretical Computer Science, volume A. Elsevier Science Publishers, chapter 11, pages 633-672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>General context-free recognition in less than cubic time.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>10--308</pages>
<contexts>
<context position="1304" citStr="Valiant, 1975" startWordPosition="216" endWordPosition="217">st, practical BMM algorithm, which is not believed to exist. 1 Introduction The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley&apos;s algorithm (Earley, 1970), both of which have a worst-case running time of 0(gN3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time 0(g N3/ log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying matrices takes time 0(m3). There exist matrix multiplication algorithms with time complexity 0(m3-5); for instance, Strassen&apos;s has a worstcase running time of 0(m281) (Strassen, 1969), and the fastest currently known has a worst-case running time of 0(m2376) (Coppersmith and Winograd, 1990). Unfortunately, the constants involved are so </context>
</contexts>
<marker>Valiant, 1975</marker>
<rawString>Valiant, Leslie G. 1975. General context-free recognition in less than cubic time. Journal of Computer and System Sciences, 10:308-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="969" citStr="Younger, 1967" startWordPosition="155" endWordPosition="156">GI Iw13-6) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time 0(m3-E/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 1 Introduction The context-free grammar (CFG) formalism was developed during the birth of the field of computational linguistics. The standard methods for CFG parsing are the CKY algorithm (Kasami, 1965; Younger, 1967) and Earley&apos;s algorithm (Earley, 1970), both of which have a worst-case running time of 0(gN3) for a CFG (in Chomsky normal form) of size g and a string of length N. Graham et al. (1980) give a variant of Earley&apos;s algorithm which runs in time 0(g N3/ log N). Valiant&apos;s parsing method is the asymptotically fastest known (Valiant, 1975). It uses Boolean matrix multiplication (BMM) to speed up the dynamic programming in the CKY algorithm: its worst-case running time is 0(gM(N)), where M(m) is the time it takes to multiply two m x m Boolean matrices together. The standard method for multiplying mat</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, Daniel H. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>