<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000266">
<note confidence="0.996126">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<title confidence="0.99739">
Adapting a Semantic Question Answering System to the Web
</title>
<author confidence="0.99353">
Sven Hartrumpf
</author>
<affiliation confidence="0.989179">
Intelligent Information and Communication Systems (IICS)
University of Hagen (FernUniversit¨at in Hagen)
</affiliation>
<address confidence="0.85559">
58084 Hagen, Germany
</address>
<email confidence="0.830839">
Sven.Hartrumpf@fernuni-hagen.de
</email>
<sectionHeader confidence="0.985778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999995333333333">
This paper describes how a question an-
swering (QA) system developed for small-
sized document collections of several mil-
lion sentences was modified in order to
work with a monolingual subset of the
web. The basic QA system relies on com-
plete sentence parsing, inferences, and se-
mantic representation matching. The ex-
tensions and modifications needed for use-
ful and quick answers from web docu-
ments are discussed. The main extension
is a two-level approach that first accesses a
web search engine and downloads some of
its document hits and then works similar
to the basic QA system. Most modifica-
tions are restrictions like a maximal num-
ber of documents and a maximal length
of investigated document parts; they en-
sure acceptable answer times. The result-
ing web QA system is evaluated on the
German test collection from QA@CLEF
2004. Several parameter settings and
strategies for accessing the web search en-
gine are investigated. The main results are:
precision-oriented extensions and exper-
imentally derived parameter settings are
needed to achieve similar performance on
the web as on small-sized document col-
lections that show higher homogeneity and
quality of the contained texts; adapting a
semantic QA system to the web is feasible,
but answering a question is still expensive
in terms of bandwidth and CPU time.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941333333334">
There are question answering (QA) systems in-
tended for small-sized document collections (tex-
tual QA systems) and QA systems aiming at
the web (web(-based) QA systems). In this pa-
per, a system of the former type (InSicht, see
(Hartrumpf, 2005b)) is transformed into one of
the latter type (called InSicht-W3 for short). In
Sect. 2, the textual QA system for German is pre-
sented. The extensions and modifications required
to get it working with the web as a virtual doc-
ument collection are described and discussed in
Sect. 3. The resulting system is evaluated on a
well-known test collection and compared to the
basic QA system (Sect. 4). After the conclusion,
some directions for further research are indicated.
</bodyText>
<sectionHeader confidence="0.882238" genericHeader="method">
2 The Basic QA System
</sectionHeader>
<bodyText confidence="0.998281793103448">
The semantic&apos; QA system that was turned into a
web QA system is InSicht. It relies on complete
sentence parsing, inferences, and semantic repre-
sentation matching and comprises six main steps.
In the document processing step, all docu-
ments from a given collection are transformed into
a standard XML format (CES, corpus encoding
standard, see http://www.cs.vassar.edu/CES/) with
word, sentence, and paragraph borders marked
up by XML elements w, s, and p, respectively.
Then, all preprocessed documents are parsed by
WOCADI (Hartrumpf, 2003) yielding a syntactic
dependency structure and, more importantly, a se-
mantic representation, a semantic network of the
MultiNet formalism (Helbig, 2006) for each doc-
ument sentence. The parser can produce intersen-
tential coreference links for a document.
In the second step (query processing), the user’s
question is parsed by WOCADI. Determining the
sentence type (here, often a subtype of question)
is especially important because it controls some
parts of two later steps: query expansion and an-
swer generation.
Next comes query expansion: Equivalent and
similar semantic networks are derived from the
original query network by means of lexico-
&apos;Semantic in the sense that formal semantic representa-
tions of documents and questions are automatically produced
by a parser and form the system center.
</bodyText>
<page confidence="0.997259">
61
</page>
<note confidence="0.98432">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.999140046511628">
semantic relations from HaGenLex (Hagen Ger-
man Lexicon, (Hartrumpf et al., 2003)) and a lex-
ical database (GermaNet), equivalence rules, and
inferential rules like entailments for situations (ap-
plied in backward chaining). The result is a set
of disjunctively connected semantic networks that
try to cover many possible kinds of representations
of sentences possibly containing an explicit or im-
plicit answer to the user’s question.
In the fourth step (semantic network matching),
all document sentences matching at least one of
the semantic networks from query expansion are
collected. A two-level approach is chosen for effi-
ciency reasons. First, an index of concepts (disam-
biguated words with IDs from the lexicon) is con-
sulted with the relevant concepts from the query
networks. Second, the retrieved documents are
compared sentence network by sentence network
to find a match with a query network.
Answer generation is next: Natural language
(NL) generation rules are applied to semantic net-
works that match a query network in order to gen-
erate an NL answer string from the deep seman-
tic representations. The sentence type and the se-
mantic network control the selection of generation
rules. The rules also act as a filter for uninforma-
tive or bad answers. The results are tuples of gen-
erated answer string, numerical score, supporting
document ID, and supporting sentence ID.
To deal with different answer candidates, an
answer selection step is required. It realizes a
quite simple but successful strategy that combines
a preference for more frequent answers and a pref-
erence for more elaborate answers. The best an-
swers (by default only the single best answer) and
the supporting sentences are presented to the user
that posed the question.
The first step, document processing, is run off-
line in InSicht; it is run online in InSicht-W3 to
avoid unacceptable parsing costs before the sys-
tem can be used. The remaining five steps will be
left mostly unchanged for InSicht-W3, in parts just
differently parameterized.
</bodyText>
<sectionHeader confidence="0.98987" genericHeader="method">
3 QA System Extensions for the Web
</sectionHeader>
<subsectionHeader confidence="0.998688">
3.1 A Naive Approach to Web-based QA
</subsectionHeader>
<bodyText confidence="0.998510333333333">
The simplest approach to turning InSicht into a
web QA system would be to collect German web
pages and work with the resulting document col-
lection as described in Sect. 2. However, a deep
semantic analyzer will need several years to parse
all web pages in German (even if excluding the
pages from the much larger deep web). The fol-
lowing formula provides a rough estimate2 of the
CPU years needed:
</bodyText>
<equation confidence="0.9085962">
#documents · #sent per document[sent]
parser speed[sent/h]
500,000,000·320[sent]
4000[sent/h]
= 40,000,000h ≈ 4,566a
</equation>
<bodyText confidence="0.999862">
This long time indicates that the naive approach
is currently not an option. Therefore a multi-level
approach was investigated. In this paper, only a
two-level approach is discussed, which has been
tried in shallow QA systems.
</bodyText>
<subsectionHeader confidence="0.999211">
3.2 A Two-Level Approach to Web-based QA
</subsectionHeader>
<bodyText confidence="0.999846043478261">
As in other applications, a web search engine can
be used (as a first level) to preselect possibly rel-
evant documents for the task at hand (here, an-
swering a question posed by a user). In InSicht-
W3, the web is accessed when similar and equiva-
lent semantic networks have been generated dur-
ing query expansion. Clearly, one cannot di-
rectly use this semantic representation for retriev-
ing document URLs from any service out there on
the web—at least till the arrival of a web anno-
tated with formal NL semantics. One must trans-
form the semantic networks to an adequate level;
in many web search engines, this is the level of
search terms connected in a Boolean formula us-
ing and and or.
To derive a search engine query from a seman-
tic network, all lexical concepts from the seman-
tic network are collected. For example, question
qa04 068 from QA@CLEF 2004 in example (1)
leads to the semantic network depicted in Fig. 1
(and related semantic networks; for a QA@CLEF
2004 question, 4.8 additional semantic networks
on average).
</bodyText>
<footnote confidence="0.830451666666667">
(1) Wo sitzt Hugo Lacour hinter Gittern?
Where sits Hugo Lacour behind bars?
‘Where is Hugo Lacour imprisoned?’
</footnote>
<bodyText confidence="0.949163">
The lexical concepts are roughly speaking those
concepts (represented as nodes in graphical form)
whose names are not of the form c1, c2, etc. The
lexical concepts are named after the lemma of the
</bodyText>
<footnote confidence="0.921165666666667">
2The average number of sentences per document has been
determined from a sample of 23,800 preprocessed web docu-
ments in InSicht-W3.
</footnote>
<equation confidence="0.960087">
t =
</equation>
<page confidence="0.989132">
62
</page>
<note confidence="0.928875">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<figureCaption confidence="0.9801215">
Figure 1: Semantic network for CLEF question qa04 068: Wo sitzt Hugo Lacour hinter Gittern? (‘Where
is Hugo Lacour imprisoned?’). Some edges are shown folded below the node name.
</figureCaption>
<figure confidence="0.998258101694915">
lacour.0fe �� c28na c37?wh-questionl
SUB nachname.1.1 SUB underspecified-location.0
⎡ ⎤ [GENER sp]
GENER sp s A
⎢ QUANT one ⎥
⎣ ⎦
CARD 1
ETYPE 0
s VAL c
c �� LOC
ATTRc
s
c26d
SUB mensch.1.1
�� c2st
SUBS sitzen.1.1
TEMP present.0
[GENER sp]
s SCAR c
s
LOC
V
s
c27na
SUB vorname.1.1
⎡ ⎤
GENER sp
⎢ QUANT one ⎥
⎣ ⎦
CARD 1
ETYPE 0
c
VAL
V
hugo.0fe
��
c ATTR c
s
�� c36l
⎡ ⎤
FACT real
⎢ QUANT mult ⎥
⎣REFER indet ⎦
ETYPE 1
s *HINTER c
⎡ FACT real ⎤
⎢ GENER sp ⎥
⎢ ⎢QUANT one ⎥ ⎥
⎢ ⎢REFER det ⎥ ⎥
⎢ ⎣CARD 1 ⎦ ⎥
ETYPE 0
VARIA con
c32d
PRED gitter.1.1
⎡ ⎤
FACT real
⎢ QUANT mult ⎥
⎣REFER indet ⎦
ETYPE 1
</figure>
<bodyText confidence="0.999699384615385">
corresponding word plus a numerical homograph
identifier and a numerical reading identifier.
As current web search engines provide no (or
only rough) lemmatization for German, one must
go one step further away from the semantic net-
work by generating full forms for each word be-
longing to a lexical concept. (Others have tried
a similar approach; for example, Neumann and
Sacaleanu (2005) construct IR queries which can
be easily adapted to different IR engines.) In the
example, the node sitzen.1.1 (to sit) leads to 26
different full forms (shown below in example (2)).
These forms can be used as search terms; as alter-
natives, they are connected disjunctively. Thus for
each semantic network from query expansion, a
conjunction of disjunctions of word forms is con-
structed. Word forms can belong to words not
occurring in the question because query expan-
sion can introduce new concepts (and thereby new
words), e.g. by inferential rules. The Boolean
formulae for several semantic networks are con-
nected disjunctively and the resulting formula is
simplified by applying logical equivalences. What
one can pass to a web search engine as a prese-
lection query for question (1) is shown in prefix
notation in example (2):
</bodyText>
<figure confidence="0.335955">
(2) (or
(and
(or Gitter Gittern Gitters)
(or Hugo Hugos)
(or Lacour Lacours)
</figure>
<construct confidence="0.9269624">
(or gesessen gesessene gesessenem gesesse-
nen gesessener gesessenes sa3 sa3en sa3est
sa3et sa3t sitze sitzen sitzend sitzende sitzen-
dem sitzenden sitzender sitzendes sitzest sitzet
sitzt s¨a3e s¨a3en s¨a3est s¨a3et))
</construct>
<bodyText confidence="0.985755416666667">
. . . ;query derived from second query network
)
If a search engine does not offer Boolean operators
(or limits Boolean queries to some hundred char-
acters), one can transform the Boolean query into
DNF and issue for each disjunct a search engine
query that requires all its terms (for example, by
prepending a plus sign to every term).
There are several problems related to break-
ing down a semantic network representation into
a Boolean formula over word forms. For example,
lexical concepts that do not stem from a surface
form in the question should be excluded from the
query. An example is the concept nachname.1.1
(‘last name’) which the named entity subparser of
WOCADI installs in the semantic network when
constructing the semantic representation of a hu-
man being whose last name is specified in the text
(see Fig. 1).
The generation of a preselection query de-
scribed above indicates that there is a fundamental
level mismatch between the first level (web search
engine) and the second level (an NL understand-
ing system like the QA system InSicht): words
</bodyText>
<page confidence="0.997391">
63
</page>
<note confidence="0.92648">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.999957304347826">
(plus proximity information3) vs. concepts within
(possibly normalized) semantic representations of
sentences (or texts). This level mismatch deteri-
orates the precision of the first level and thereby
the recall of the second level. The latter might be
surprising. The precision of the first level (viewed
in isolation) is not problematic because the second
level achieves the same precision no matter how
low the precision of the first level is. But precision
does matter because the limited number of docu-
ments that the first level can return and that the
second level can efficiently process might lead to
not seeing relevant documents (i.e. lower recall) or
at least to seeing them much later (which can im-
ply unacceptable answer times). Or to put it differ-
ently: Very low precision in the first level can lead
to lower recall of the whole multi-level system if
there are any retrieval set limits for the first level.
At the end of the first level, the retrieved web
documents are simplified by a script that uses the
text dump function of the lynx web browser to ex-
tract the textual content. The resulting document
representations can be processed by InSicht.
</bodyText>
<subsectionHeader confidence="0.995436">
3.3 Deficiencies of Web Search Engines
</subsectionHeader>
<bodyText confidence="0.9999374">
Web search engines typically restrict the length
of queries, e.g. to 1000 bytes, 1500 bytes, or 10
terms. But the rich morphology of languages like
German can lead to long and complex preselection
queries so that one needs to reduce preselection
queries as far as possible (e.g. by dropping rare
word forms). An additional strategy is to split the
query into several independent subqueries.
The QA system is currently based on match-
ing semantic representations with the granularity
of a sentence only. Thus one would achieve much
better precision if the web search engine allowed
a same-sentence operator or at least a proximity
operator (often named NEAR) of say 50 words
which ignores linguistic units like sentences and
paragraphs but suffices for our goals. Although
some web search engines once had some kind of
proximity operator, currently none of the search
engines with large indices and Boolean queries
seems to offer this facility.
</bodyText>
<subsectionHeader confidence="0.964381">
3.4 Pragmatic Limits for Web-based QA
</subsectionHeader>
<bodyText confidence="0.8333085">
Pragmatic limits are needed to ensure acceptable
answer times when dealing with the vast number
</bodyText>
<footnote confidence="0.8834145">
3At least some years ago and hopefully in the future again,
see Sect. 3.3.
</footnote>
<bodyText confidence="0.999551441860465">
of web documents. The following constraining pa-
rameters are implemented in InSicht-W3:
number of documents d (BT-R) 4 The number
of documents retrieved from a search engine
is limited by InSicht-W3. A separate query
is sent to the search engine for each top-level
disjunct, which corresponds to one semantic
network. For each of the q subqueries, d/q
documents are retrieved, at most.
Many search engines employ a maximal
number of hits h. (A popular choice is cur-
rently 1000.) Due to other filtering con-
straints (like snippet testing as defined be-
low), the limit h can inhibit the performance
of InSicht-W3 even if h is greater than d.
document format (BT-R) Only documents en-
coded as HTML or plain text are covered.
Other formats are omitted by adding a format
constraint to the preselection query, by inves-
tigating the URL, and by interpreting the re-
sult of the UNIX program file.
document language (BPT-R) Only documents
that are mainly written in German are treated.
A language identification module is available
in InSicht-W3, but the selection of document
language in the web search engine was
precise enough.
document length l (T-R) If a document is longer
than l bytes it is shortened to the first l bytes
(current value: 1,000,000 bytes).
document URL (BPT-R) The URL of a docu-
ment must belong to certain top-level do-
mains (.at, .ch, .de, .info, .net, .org) and must
not be on a blacklist. Both conditions aim at
higher quality pages. The blacklist is short
and should be replaced by one that is collab-
oratively maintained on the web.
snippet testing (BPT-R) Many search engines
return a text snippet for each search hit that
typically contains all word forms from the
search engine query. Sometimes the snip-
pet does not fulfill the preselection query. A
stronger constraint is that a snippet passage
</bodyText>
<footnote confidence="0.823418333333333">
4Trade-offs (and goals) are indicated in parentheses, be-
tween bandwidth (B), precision (P), (answer) time (T), on the
one hand, and recall (R), on the other hand.
</footnote>
<page confidence="0.997413">
64
</page>
<note confidence="0.929804">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.977553763157895">
without ellipsis (... ) must fulfill the preselec-
tion query. This realizes an imperfect same-
sentence or similar proximity operator.
word form selection (BT-R) To shorten prese-
lection queries for highly inflecting word
categories (e.g. German verbs), InSicht-W3
omits full forms that are less likely to be con-
tained in answering document sentences (e.g.
verb forms that can only be imperative or first
and second person). For adjectives, only the
forms matching the degree (positive, compar-
ative, and superlative in German) used in the
question are included. The most restrictive
parameter setting is to choose only one word
form per content word, e.g. the word form oc-
curring in the question.
These constraints can be checked before the sec-
ond level starts.
The following parameters are applicable on the
second level:
number of parsed sentences s (T-R) For each
document, at most s sentences are parsed
(current value: 100).
sentence language (PT-R) As many web pages
contain passages in several languages, it is
sometimes important to discard sentences
that seem to be in a different language than
the document itself. If the percentage of un-
known words in a sentences reaches a thresh-
old, the sentence is considered to be written
in a foreign language
sentence selection (T) Only a sentence whose set
of word forms fulfills the preselection query
sent to the web search engine is parsed by
WOCADI. This realizes a same-sentence op-
erator on the second level. Of course, this
operator would be more helpful in the web
search engine (see Sect. 3.3).
</bodyText>
<subsectionHeader confidence="0.936657">
3.5 Parameter Settings of the QA System
</subsectionHeader>
<bodyText confidence="0.999978928571429">
In general, the QA system must be parameterized
differently when going from a high quality, low-
redundancy, small-sized collection (like archives
of some newspapers) to a mixed quality, high-
redundancy, large-sized collection (like a consid-
erable subset of the web). For example, the varia-
tion of concepts (like synonyms, hyponyms, and
hyperonyms) is less important on the web than
in smaller collections due to its large redundancy
and increased chance to find a formulation whose
semantic representation contains exactly the con-
cepts from the question. This is helpful because
full concept variations lead for some questions to
very long preselection queries.
</bodyText>
<subsectionHeader confidence="0.930607">
3.6 Language-specific Problems
</subsectionHeader>
<bodyText confidence="0.9993972">
InSicht-W3 has to deal with several problems in
the German language and German web pages,
which might be less problematic for English or
some other languages. On the one hand, the rich
morphology requires to generate word forms for
content words derived from query networks; on
the other hand, to avoid very long preselection
queries the generation must be limited (e.g. with
the word form selection parameter).
The morphological phenomenon of separable
prefix verbs in German requires special treatment.
Certain word forms of such verbs must appear as
one orthographic word or two orthographic words
(often separated by many intervening words). The
choice depends on the word order of the sentence
or clause that contains the verb. InSicht-W3’s pre-
selection queries contain both variants, e.g. for the
word form absitzt the following formula is gener-
ated: (or absitzt (and ab sitzt)).
In German web pages, two orthography sys-
tems can be found, the old one and the new one
introduced in 1998. As both systems are often
mixed in pages (and question sets), the NL pars-
ing must be flexible and exploit the links between
old spellings, new spellings, and spelling vari-
ants. The parser normalizes all words and concept
names to the old orthography. For preselection
queries, word forms for all orthography systems
must be generated.
Not only the orthography system might change
in a web page, also the language itself might
switch (often without correct markup), e.g. be-
tween English and German. Therefore the sen-
tence language parameter from Sect. 3.4 was in-
troduced. Such effects also occur for other lan-
guages.
German texts contain important characters that
are not covered by the ASCII character set. (The
most frequent ones are ¨a, ¨o, ¨u, ¨A, ¨O, ¨U, andß.) Un-
fortunately, different character encodings are used
for German web pages, sometimes with a mis-
match of declared (or implied) encoding and ac-
tually used encoding. This problem has been suc-
cessfully delegated to the lynx web browser (see
end of Sect. 3.2).
</bodyText>
<page confidence="0.998988">
65
</page>
<note confidence="0.972665">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<tableCaption confidence="0.9546975">
Table 1: Evaluation results for the German questions from QA@CLEF 2004. InSicht used all 200
questions, InSicht-W3 only 170 questions (see text).
</tableCaption>
<bodyText confidence="0.4112244">
setup answers (%) K1
system document collection non-empty empty
right inexact wrong right wrong
InSicht QA@CLEF document collection 30.5 2.5 0.0 10.0 57.0 0.28
InSicht-W3 web (as virtual document collection) 22.9 2.9 2.4 0.0 71.8 0.18
</bodyText>
<sectionHeader confidence="0.586625" genericHeader="method">
4 Evaluation of the Web QA System
</sectionHeader>
<bodyText confidence="0.985371611111111">
The resulting web QA system is evaluated on an
established set of test questions: QA@CLEF 2004
(Magnini et al., 2005). Some questions in this
set have an important implicit temporal context.
For example, the correct answer to questions like
qa04 090 in example (3) critically depends on the
time this present tense question is uttered.
(3) Wer ist Pr¨asident von UNICE?
Who is president of UNICE?
In QA@CLEF, a supported correct answer to this
question can be the name of a UNICE presi-
dent only from a certain time period because
the QA@CLEF document collection consists of
newspaper and newswire articles from 1994 and
1995 (and because there are no documents about
the history of UNICE). On the web, there are ad-
ditional correct answers.5 Questions with implicit
time restriction (30 cases) are excluded from the
evaluation so that 170 questions remain for eval-
uation in InSicht-W3. Alternatives would be to
refine these questions by making the temporal re-
striction explicit or to extend the gold standard by
answers that are to be considered correct if work-
ing on the web.
Table 1 contains evaluation results for InSicht-
W3: the percentages of right, inexact, and wrong
answers (separately for non-empty answers and
empty answers) and the K1-measure (see (Her-
rera et al., 2005) for a definition). For compar-
ison, the results of the textual QA system InSicht
on the QA@CLEF document collection are shown
in the first row. The percentages for non-empty an-
swers differ for right answers and wrong answers.
In both aspects, InSicht-W3 is worse than InSicht.
The main reason for these changes is that the struc-
ture and textual content of documents on the web
</bodyText>
<footnote confidence="0.910617666666667">
5Or just one correct answer, which differs from the one
in QA@CLEF, when one interprets the present tense in the
question as referring to today.
</footnote>
<bodyText confidence="0.999773780487805">
are much more diverse than in the QA@CLEF col-
lection. For example, InSicht-W3 regarded some
sequences of words from unrelated link anchor
texts as a sentence, which led to some wrong an-
swers especially for definition questions. Also
for empty answers, InSicht-W3 performs worse
than InSicht. This is in part due to the too opti-
mistic assumption during answer assessment that
all German questions from QA@CLEF 2004 have
a correct answer on the web (therefore the col-
umn labelled right empty answers contains 0.0
for InSicht-W3). However, InSicht-W3 found 12
right answers that InSicht missed. So there is a
potential for a fruitful system combination.
The impact of some interesting parameters was
evaluated by varying parameter settings (Table 2).
The query network quality qmin (column 2) is a
value between 0 (worst) and 1 (best) that mea-
sures how far away a derived query network is
from the original semantic network of the ques-
tion. For example, omitting information from the
semantic network for the question (like the first
name of a person if also the last name is specified),
leads to a reduction of the initial quality value of
1. The value in column 2 indicates at what thresh-
old variant query networks are ignored. Column
3 corresponds to the parameter of word form se-
lection (see Sect. 3.4). The two runs with d = 300
and qmin = 0.8 show that morphological genera-
tion pays off, but the effect is smaller than in other
document collections. This is probably due to the
fact that the large and redundant web often con-
tains answers with the exact word forms from the
question. Another interesting aspect found in Ta-
ble 2 is that even with d = 500 results still im-
prove; it remains to be seen at what number of
documents performance stays stable (or maybe de-
grades). The impact of a lower quality threshold
qmin was not significant. This might change if one
operates with a larger parameter d and more infer-
ential rules.
</bodyText>
<page confidence="0.97406">
66
</page>
<note confidence="0.968746">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<tableCaption confidence="0.8457585">
Table 2: Influence of parameters on InSicht-W3 results. Parameter d is the maximal number of doc-
uments used from the search engine results (see Sect. 3.4); qmin is the minimal query network quality.
</tableCaption>
<bodyText confidence="0.9775406">
parameter setting results
d qmin morph. #docs. #sent. %docs. right non- inexact wrong K1
generat. from first matching with empty answ. (%) non-
level pre. query matching answ. (%) empty
per quest. per quest. sent. answ. (%)
</bodyText>
<table confidence="0.9985985">
100 0.9 frequent 18.1 30.5 59.7 18.8 4.1 1.8 0.13
200 0.9 frequent 34.4 59.0 60.4 20.6 3.5 2.9 0.14
300 0.9 frequent 45.5 76.9 60.2 22.4 3.5 2.4 0.16
300 0.8 frequent 48.4 84.2 61.7 22.4 3.5 2.4 0.16
300 0.8 none 57.3 91.3 61.5 19.4 3.5 3.5 0.12
500 0.8 frequent 68.9 119.8 62.2 22.9 2.9 2.4 0.18
</table>
<bodyText confidence="0.992299956521739">
Error analysis and classification is difficult as
soon as one steps to the vast web. One could start
with a classification of error reasons for wrong
empty answers. The error class that can be most
easily determined is that the search engine re-
turned no results for the preselection query. 40
of the 170 questions (23.5%) belong to this class.
This might surprise users that believe that they
can find nearly every bit of information on the
web. But there are areas where this assumption is
wrong: many QA@CLEF questions relate to very
specific events of the year 1994 or 1995. Twelve
years ago, the web publishing rate was much lower
than today; and even if the relevant pieces of in-
formation were on the web at that time (or in the
following years), they might have been moved to
archives or removed in recent years. Other er-
ror reasons are very difficult and labor-intensive
to separate:
1. Result pages from the first level do not con-
tain an answer, but if one requests more docu-
ments (e.g. by raising the parameter d) result
pages with an answer will be found.
2. Result pages from the first level contain an
answer, but one or more components of the
textual QA system cause that the answer
is not found. Subclasses could be defined
by adapting the hierarchy that Hartrumpf
(2005a) applied to evaluate InSicht.
On average, the 40th search engine result is
the first that contains a right answer (in the best
InSicht-W3 run).6 Although this result is for the
system and not for a real user (they differ in their
strengths and weaknesses in NL understanding), it
indicates that human users of web search engines
might find the correct answer quite late if at all.
This is a strong argument for more advanced, sec-
ond level tools like a semantic QA system: How
many users will read through 40 search engine re-
sults and (possibly) 40 underlying web pages?
The answer time of InSicht-W3 currently
ranges between 15 and 1200 seconds. The doc-
ument download time was excluded because it de-
pended on too many external factors (caches, in-
tranet effects, parallel vs. sequential downloads)
to be measured consistently and reliably.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.904555444444444">
There are few QA systems for German. The sys-
tem described by Neumann and Xu (2003) works
on German web pages. Its general approach dif-
fers from InSicht because it relies on shallow, but
robust methods, while InSicht builds on sentence
parsing and semantic representations. In this re-
spect, InSicht resembles the (English) textual QA
system presented by Harabagiu et al. (2001). In
contrast to InSicht, this system applies a theorem
prover and a large knowledge base to validate can-
didate answers. An interesting combination of
web and textual QA is presented by Vicedo et al.
(2005): English and Spanish web documents are
used to enhance textual QA in Spanish.
6This high number is in part an artifact of the preselection
query generation. In a more thorough analysis, human users
could be given the NL question and be asked to formulate a
corresponding search engine query.
</bodyText>
<page confidence="0.997209">
67
</page>
<note confidence="0.941604">
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
</note>
<bodyText confidence="0.9999252">
One of the first web QA systems for English
was Mulder (Kwok et al., 2001). Mulder parses
only the text snippets returned by the search en-
gine, while InSicht-W3 parses the underlying web
pages because the text snippets often have omis-
sions (‘...’) so that full parsing becomes problem-
atic or impossible. InSicht-W3’s approach needs
more time, especially if the web pages are not in
the local cache that InSicht-W3 maintains in order
to reduce its bandwidth requirements.
</bodyText>
<sectionHeader confidence="0.99312" genericHeader="conclusions">
6 Conclusion and Perspectives
</sectionHeader>
<bodyText confidence="0.99999325">
In this paper, an existent textual QA system was
extended and modified to work successfully on
the German web as a virtual document collection.
The main results are: precision-oriented exten-
sions and experimentally derived parameter set-
tings are needed to achieve similar performance
on the vast web as on small-sized document col-
lections that show higher homogeneity and quality
of the contained texts; taking a semantic QA sys-
tem to the web is feasible as demonstrated in this
paper, but answering a question is still expensive
in terms of bandwidth and CPU time.
There are several interesting directions for fu-
ture work. The first level of InSicht-W3 can be im-
proved by finding a better suited search engine or
by building and running a new one in a distributed
manner. Ideally, it should support arbitrarily com-
plex Boolean queries, parameterized proximity
operators like NEAR/N (N ∈ {1,2,...,100}), or
even linguistically informed operators like same-
sentence and same-paragraph.
The second level (the textual QA system) can
be improved by acquiring more inferential knowl-
edge to allow better query expansion. The match-
ing approach can be extended from the unit sen-
tence to a larger linguistic unit like paragraph, text,
and even text collection. Distributed architectures
and algorithms can reduce answer times.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936966666667">
Sanda Harabagiu, Dan Moldovan, Marius Pas¸ca, Rada
Mihalcea, Mihai Surdeanu, R˘azvan Bunescu, Rox-
ana Girju, Vasile Rus, and Paul Mor˘arescu. 2001.
The role of lexico-semantic feedback in open-
domain textual question-answering. In Proceedings
of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL-2001), pages 274–
281, Toulouse, France.
Sven Hartrumpf, Hermann Helbig, and Rainer Oss-
wald. 2003. The semantically based computer lex-
icon HaGenLex – Structure and technological en-
vironment. Traitement automatique des langues,
44(2):81–105.
Sven Hartrumpf. 2003. Hybrid Disambiguation in
Natural Language Analysis. Der Andere Verlag, Os-
nabr¨uck, Germany.
Sven Hartrumpf. 2005a. Question answering using
sentence parsing and semantic network matching. In
(Peters et al., 2005), pages 512–521.
Sven Hartrumpf. 2005b. University of Hagen
at QA@CLEF 2005: Extending knowledge and
deepening linguistic processing for question an-
swering. In Carol Peters, editor, Results of the
CLEF 2005 Cross-Language System Evaluation
Campaign, Working Notes for the CLEF 2005 Work-
shop. Centromedia, Wien, Austria.
Hermann Helbig. 2006. Knowledge Representation
and the Semantics of Natural Language. Springer,
Berlin.
Jes´us Herrera, Anselmo Pe˜nas, and Felisa Verdejo.
2005. Question answering pilot task at CLEF 2004.
In (Peters et al., 2005), pages 581–590.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling question answering to the web. ACM Trans-
actions on Information Systems, 19(3):242–262.
Bernardo Magnini, Alessandro Vallin, Christelle Ay-
ache, Gregor Erbach, Anselmo Pe˜nas, Maarten
de Rijke, Paulo Rocha, Kiril Simov, and Richard
Sutcliffe. 2005. Overview of the CLEF 2004 mul-
tilingual question answering track. In (Peters et al.,
2005), pages 371–391.
G¨unter Neumann and Bogdan Sacaleanu. 2005.
Experiments on robust NL question interpretation
and multi-layered document annotation for a cross-
language question/answering system. In (Peters et
al., 2005), pages 411–422.
G¨unter Neumann and Feiyu Xu. 2003. Mining an-
swers in German web pages. In Proceedings of the
International Conference on Web Intelligence (WI-
2003), Halifax, Canada.
Carol Peters, Paul Clough, Julio Gonzalo, Gareth J. F.
Jones, Michael Kluck, and Bernardo Magnini, ed-
itors. 2005. Multilingual Information Access for
Text, Speech and Images: 5th Workshop of the
Cross-Language Evaluation Forum, CLEF 2004,
volume 3491 of LNCS. Springer, Berlin.
Jos´e L. Vicedo, Maximiliano Saiz, Rub´en Izquierdo,
and Fernando Llopis. 2005. Does English help
question answering in Spanish? In (Peters et al.,
2005), pages 552–556.
</reference>
<page confidence="0.999447">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652357">
<note confidence="0.698887">EACL 2006 Workshop on Multilingual Question Answering - MLQA06</note>
<title confidence="0.993849">Adapting a Semantic Question Answering System to the Web</title>
<author confidence="0.98616">Sven</author>
<affiliation confidence="0.996937">Intelligent Information and Communication Systems University of Hagen (FernUniversit¨at in</affiliation>
<address confidence="0.999429">58084 Hagen,</address>
<email confidence="0.975127">Sven.Hartrumpf@fernuni-hagen.de</email>
<abstract confidence="0.999393882352941">This paper describes how a question answering (QA) system developed for smallsized document collections of several million sentences was modified in order to work with a monolingual subset of the web. The basic QA system relies on complete sentence parsing, inferences, and semantic representation matching. The extensions and modifications needed for useful and quick answers from web documents are discussed. The main extension is a two-level approach that first accesses a web search engine and downloads some of its document hits and then works similar to the basic QA system. Most modifications are restrictions like a maximal number of documents and a maximal length of investigated document parts; they ensure acceptable answer times. The resulting web QA system is evaluated on the German test collection from QA@CLEF 2004. Several parameter settings and strategies for accessing the web search engine are investigated. The main results are: precision-oriented extensions and experimentally derived parameter settings are needed to achieve similar performance on the web as on small-sized document collections that show higher homogeneity and quality of the contained texts; adapting a semantic QA system to the web is feasible, but answering a question is still expensive in terms of bandwidth and CPU time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pas¸ca</author>
<author>Rada Mihalcea</author>
<author>Mihai Surdeanu</author>
<author>R˘azvan Bunescu</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
<author>Paul Mor˘arescu</author>
</authors>
<title>The role of lexico-semantic feedback in opendomain textual question-answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001),</booktitle>
<pages>274--281</pages>
<location>Toulouse, France.</location>
<marker>Harabagiu, Moldovan, Pas¸ca, Mihalcea, Surdeanu, Bunescu, Girju, Rus, Mor˘arescu, 2001</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pas¸ca, Rada Mihalcea, Mihai Surdeanu, R˘azvan Bunescu, Roxana Girju, Vasile Rus, and Paul Mor˘arescu. 2001. The role of lexico-semantic feedback in opendomain textual question-answering. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001), pages 274– 281, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
<author>Hermann Helbig</author>
<author>Rainer Osswald</author>
</authors>
<title>The semantically based computer lexicon</title>
<date>2003</date>
<booktitle>HaGenLex – Structure and technological environment. Traitement automatique des langues,</booktitle>
<pages>44--2</pages>
<contexts>
<context position="3852" citStr="Hartrumpf et al., 2003" startWordPosition="599" endWordPosition="602"> by WOCADI. Determining the sentence type (here, often a subtype of question) is especially important because it controls some parts of two later steps: query expansion and answer generation. Next comes query expansion: Equivalent and similar semantic networks are derived from the original query network by means of lexico&apos;Semantic in the sense that formal semantic representations of documents and questions are automatically produced by a parser and form the system center. 61 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 semantic relations from HaGenLex (Hagen German Lexicon, (Hartrumpf et al., 2003)) and a lexical database (GermaNet), equivalence rules, and inferential rules like entailments for situations (applied in backward chaining). The result is a set of disjunctively connected semantic networks that try to cover many possible kinds of representations of sentences possibly containing an explicit or implicit answer to the user’s question. In the fourth step (semantic network matching), all document sentences matching at least one of the semantic networks from query expansion are collected. A two-level approach is chosen for efficiency reasons. First, an index of concepts (disambigua</context>
</contexts>
<marker>Hartrumpf, Helbig, Osswald, 2003</marker>
<rawString>Sven Hartrumpf, Hermann Helbig, and Rainer Osswald. 2003. The semantically based computer lexicon HaGenLex – Structure and technological environment. Traitement automatique des langues, 44(2):81–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
</authors>
<title>Hybrid Disambiguation in Natural Language Analysis. Der Andere Verlag,</title>
<date>2003</date>
<location>Osnabr¨uck, Germany.</location>
<contexts>
<context position="2907" citStr="Hartrumpf, 2003" startWordPosition="457" endWordPosition="458">, some directions for further research are indicated. 2 The Basic QA System The semantic&apos; QA system that was turned into a web QA system is InSicht. It relies on complete sentence parsing, inferences, and semantic representation matching and comprises six main steps. In the document processing step, all documents from a given collection are transformed into a standard XML format (CES, corpus encoding standard, see http://www.cs.vassar.edu/CES/) with word, sentence, and paragraph borders marked up by XML elements w, s, and p, respectively. Then, all preprocessed documents are parsed by WOCADI (Hartrumpf, 2003) yielding a syntactic dependency structure and, more importantly, a semantic representation, a semantic network of the MultiNet formalism (Helbig, 2006) for each document sentence. The parser can produce intersentential coreference links for a document. In the second step (query processing), the user’s question is parsed by WOCADI. Determining the sentence type (here, often a subtype of question) is especially important because it controls some parts of two later steps: query expansion and answer generation. Next comes query expansion: Equivalent and similar semantic networks are derived from </context>
</contexts>
<marker>Hartrumpf, 2003</marker>
<rawString>Sven Hartrumpf. 2003. Hybrid Disambiguation in Natural Language Analysis. Der Andere Verlag, Osnabr¨uck, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
</authors>
<title>Question answering using sentence parsing and semantic network matching.</title>
<date>2005</date>
<journal>In (Peters</journal>
<pages>512--521</pages>
<contexts>
<context position="1875" citStr="Hartrumpf, 2005" startWordPosition="292" endWordPosition="293">n-oriented extensions and experimentally derived parameter settings are needed to achieve similar performance on the web as on small-sized document collections that show higher homogeneity and quality of the contained texts; adapting a semantic QA system to the web is feasible, but answering a question is still expensive in terms of bandwidth and CPU time. 1 Introduction There are question answering (QA) systems intended for small-sized document collections (textual QA systems) and QA systems aiming at the web (web(-based) QA systems). In this paper, a system of the former type (InSicht, see (Hartrumpf, 2005b)) is transformed into one of the latter type (called InSicht-W3 for short). In Sect. 2, the textual QA system for German is presented. The extensions and modifications required to get it working with the web as a virtual document collection are described and discussed in Sect. 3. The resulting system is evaluated on a well-known test collection and compared to the basic QA system (Sect. 4). After the conclusion, some directions for further research are indicated. 2 The Basic QA System The semantic&apos; QA system that was turned into a web QA system is InSicht. It relies on complete sentence pars</context>
<context position="26569" citStr="Hartrumpf (2005" startWordPosition="4436" endWordPosition="4437">nt pieces of information were on the web at that time (or in the following years), they might have been moved to archives or removed in recent years. Other error reasons are very difficult and labor-intensive to separate: 1. Result pages from the first level do not contain an answer, but if one requests more documents (e.g. by raising the parameter d) result pages with an answer will be found. 2. Result pages from the first level contain an answer, but one or more components of the textual QA system cause that the answer is not found. Subclasses could be defined by adapting the hierarchy that Hartrumpf (2005a) applied to evaluate InSicht. On average, the 40th search engine result is the first that contains a right answer (in the best InSicht-W3 run).6 Although this result is for the system and not for a real user (they differ in their strengths and weaknesses in NL understanding), it indicates that human users of web search engines might find the correct answer quite late if at all. This is a strong argument for more advanced, second level tools like a semantic QA system: How many users will read through 40 search engine results and (possibly) 40 underlying web pages? The answer time of InSicht-W</context>
</contexts>
<marker>Hartrumpf, 2005</marker>
<rawString>Sven Hartrumpf. 2005a. Question answering using sentence parsing and semantic network matching. In (Peters et al., 2005), pages 512–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
</authors>
<title>of Hagen at QA@CLEF 2005: Extending knowledge and deepening linguistic processing for question answering.</title>
<date>2005</date>
<booktitle>Results of the CLEF 2005 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2005 Workshop. Centromedia,</booktitle>
<editor>In Carol Peters, editor,</editor>
<publisher>University</publisher>
<location>Wien, Austria.</location>
<contexts>
<context position="1875" citStr="Hartrumpf, 2005" startWordPosition="292" endWordPosition="293">n-oriented extensions and experimentally derived parameter settings are needed to achieve similar performance on the web as on small-sized document collections that show higher homogeneity and quality of the contained texts; adapting a semantic QA system to the web is feasible, but answering a question is still expensive in terms of bandwidth and CPU time. 1 Introduction There are question answering (QA) systems intended for small-sized document collections (textual QA systems) and QA systems aiming at the web (web(-based) QA systems). In this paper, a system of the former type (InSicht, see (Hartrumpf, 2005b)) is transformed into one of the latter type (called InSicht-W3 for short). In Sect. 2, the textual QA system for German is presented. The extensions and modifications required to get it working with the web as a virtual document collection are described and discussed in Sect. 3. The resulting system is evaluated on a well-known test collection and compared to the basic QA system (Sect. 4). After the conclusion, some directions for further research are indicated. 2 The Basic QA System The semantic&apos; QA system that was turned into a web QA system is InSicht. It relies on complete sentence pars</context>
<context position="26569" citStr="Hartrumpf (2005" startWordPosition="4436" endWordPosition="4437">nt pieces of information were on the web at that time (or in the following years), they might have been moved to archives or removed in recent years. Other error reasons are very difficult and labor-intensive to separate: 1. Result pages from the first level do not contain an answer, but if one requests more documents (e.g. by raising the parameter d) result pages with an answer will be found. 2. Result pages from the first level contain an answer, but one or more components of the textual QA system cause that the answer is not found. Subclasses could be defined by adapting the hierarchy that Hartrumpf (2005a) applied to evaluate InSicht. On average, the 40th search engine result is the first that contains a right answer (in the best InSicht-W3 run).6 Although this result is for the system and not for a real user (they differ in their strengths and weaknesses in NL understanding), it indicates that human users of web search engines might find the correct answer quite late if at all. This is a strong argument for more advanced, second level tools like a semantic QA system: How many users will read through 40 search engine results and (possibly) 40 underlying web pages? The answer time of InSicht-W</context>
</contexts>
<marker>Hartrumpf, 2005</marker>
<rawString>Sven Hartrumpf. 2005b. University of Hagen at QA@CLEF 2005: Extending knowledge and deepening linguistic processing for question answering. In Carol Peters, editor, Results of the CLEF 2005 Cross-Language System Evaluation Campaign, Working Notes for the CLEF 2005 Workshop. Centromedia, Wien, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Helbig</author>
</authors>
<title>Knowledge Representation and the Semantics of Natural Language.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3059" citStr="Helbig, 2006" startWordPosition="478" endWordPosition="479">ies on complete sentence parsing, inferences, and semantic representation matching and comprises six main steps. In the document processing step, all documents from a given collection are transformed into a standard XML format (CES, corpus encoding standard, see http://www.cs.vassar.edu/CES/) with word, sentence, and paragraph borders marked up by XML elements w, s, and p, respectively. Then, all preprocessed documents are parsed by WOCADI (Hartrumpf, 2003) yielding a syntactic dependency structure and, more importantly, a semantic representation, a semantic network of the MultiNet formalism (Helbig, 2006) for each document sentence. The parser can produce intersentential coreference links for a document. In the second step (query processing), the user’s question is parsed by WOCADI. Determining the sentence type (here, often a subtype of question) is especially important because it controls some parts of two later steps: query expansion and answer generation. Next comes query expansion: Equivalent and similar semantic networks are derived from the original query network by means of lexico&apos;Semantic in the sense that formal semantic representations of documents and questions are automatically pr</context>
</contexts>
<marker>Helbig, 2006</marker>
<rawString>Hermann Helbig. 2006. Knowledge Representation and the Semantics of Natural Language. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Herrera</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>Question answering pilot task at CLEF</title>
<date>2005</date>
<booktitle>In (Peters et</booktitle>
<pages>581--590</pages>
<marker>Herrera, Pe˜nas, Verdejo, 2005</marker>
<rawString>Jes´us Herrera, Anselmo Pe˜nas, and Felisa Verdejo. 2005. Question answering pilot task at CLEF 2004. In (Peters et al., 2005), pages 581–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cody Kwok</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling question answering to the web.</title>
<date>2001</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="28440" citStr="Kwok et al., 2001" startWordPosition="4750" endWordPosition="4753">cht, this system applies a theorem prover and a large knowledge base to validate candidate answers. An interesting combination of web and textual QA is presented by Vicedo et al. (2005): English and Spanish web documents are used to enhance textual QA in Spanish. 6This high number is in part an artifact of the preselection query generation. In a more thorough analysis, human users could be given the NL question and be asked to formulate a corresponding search engine query. 67 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 One of the first web QA systems for English was Mulder (Kwok et al., 2001). Mulder parses only the text snippets returned by the search engine, while InSicht-W3 parses the underlying web pages because the text snippets often have omissions (‘...’) so that full parsing becomes problematic or impossible. InSicht-W3’s approach needs more time, especially if the web pages are not in the local cache that InSicht-W3 maintains in order to reduce its bandwidth requirements. 6 Conclusion and Perspectives In this paper, an existent textual QA system was extended and modified to work successfully on the German web as a virtual document collection. The main results are: precisi</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001. Scaling question answering to the web. ACM Transactions on Information Systems, 19(3):242–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Alessandro Vallin</author>
</authors>
<title>Christelle Ayache, Gregor Erbach, Anselmo Pe˜nas, Maarten de Rijke, Paulo Rocha, Kiril Simov,</title>
<date>2005</date>
<journal>Overview of the CLEF</journal>
<pages>371--391</pages>
<location>and</location>
<marker>Magnini, Vallin, 2005</marker>
<rawString>Bernardo Magnini, Alessandro Vallin, Christelle Ayache, Gregor Erbach, Anselmo Pe˜nas, Maarten de Rijke, Paulo Rocha, Kiril Simov, and Richard Sutcliffe. 2005. Overview of the CLEF 2004 multilingual question answering track. In (Peters et al., 2005), pages 371–391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unter Neumann</author>
<author>Bogdan Sacaleanu</author>
</authors>
<title>Experiments on robust NL question interpretation and multi-layered document annotation for a crosslanguage question/answering system.</title>
<date>2005</date>
<booktitle>In (Peters</booktitle>
<pages>411--422</pages>
<contexts>
<context position="9343" citStr="Neumann and Sacaleanu (2005)" startWordPosition="1548" endWordPosition="1551"> s �� c36l ⎡ ⎤ FACT real ⎢ QUANT mult ⎥ ⎣REFER indet ⎦ ETYPE 1 s *HINTER c ⎡ FACT real ⎤ ⎢ GENER sp ⎥ ⎢ ⎢QUANT one ⎥ ⎥ ⎢ ⎢REFER det ⎥ ⎥ ⎢ ⎣CARD 1 ⎦ ⎥ ETYPE 0 VARIA con c32d PRED gitter.1.1 ⎡ ⎤ FACT real ⎢ QUANT mult ⎥ ⎣REFER indet ⎦ ETYPE 1 corresponding word plus a numerical homograph identifier and a numerical reading identifier. As current web search engines provide no (or only rough) lemmatization for German, one must go one step further away from the semantic network by generating full forms for each word belonging to a lexical concept. (Others have tried a similar approach; for example, Neumann and Sacaleanu (2005) construct IR queries which can be easily adapted to different IR engines.) In the example, the node sitzen.1.1 (to sit) leads to 26 different full forms (shown below in example (2)). These forms can be used as search terms; as alternatives, they are connected disjunctively. Thus for each semantic network from query expansion, a conjunction of disjunctions of word forms is constructed. Word forms can belong to words not occurring in the question because query expansion can introduce new concepts (and thereby new words), e.g. by inferential rules. The Boolean formulae for several semantic netwo</context>
</contexts>
<marker>Neumann, Sacaleanu, 2005</marker>
<rawString>G¨unter Neumann and Bogdan Sacaleanu. 2005. Experiments on robust NL question interpretation and multi-layered document annotation for a crosslanguage question/answering system. In (Peters et al., 2005), pages 411–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unter Neumann</author>
<author>Feiyu Xu</author>
</authors>
<title>Mining answers in German web pages.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Web Intelligence (WI2003),</booktitle>
<location>Halifax, Canada.</location>
<contexts>
<context position="27507" citStr="Neumann and Xu (2003)" startWordPosition="4595" endWordPosition="4598">earch engines might find the correct answer quite late if at all. This is a strong argument for more advanced, second level tools like a semantic QA system: How many users will read through 40 search engine results and (possibly) 40 underlying web pages? The answer time of InSicht-W3 currently ranges between 15 and 1200 seconds. The document download time was excluded because it depended on too many external factors (caches, intranet effects, parallel vs. sequential downloads) to be measured consistently and reliably. 5 Related Work There are few QA systems for German. The system described by Neumann and Xu (2003) works on German web pages. Its general approach differs from InSicht because it relies on shallow, but robust methods, while InSicht builds on sentence parsing and semantic representations. In this respect, InSicht resembles the (English) textual QA system presented by Harabagiu et al. (2001). In contrast to InSicht, this system applies a theorem prover and a large knowledge base to validate candidate answers. An interesting combination of web and textual QA is presented by Vicedo et al. (2005): English and Spanish web documents are used to enhance textual QA in Spanish. 6This high number is </context>
</contexts>
<marker>Neumann, Xu, 2003</marker>
<rawString>G¨unter Neumann and Feiyu Xu. 2003. Mining answers in German web pages. In Proceedings of the International Conference on Web Intelligence (WI2003), Halifax, Canada.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004,</booktitle>
<volume>3491</volume>
<editor>Carol Peters, Paul Clough, Julio Gonzalo, Gareth J. F. Jones, Michael Kluck, and Bernardo Magnini, editors.</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="9343" citStr="(2005)" startWordPosition="1551" endWordPosition="1551">al ⎢ QUANT mult ⎥ ⎣REFER indet ⎦ ETYPE 1 s *HINTER c ⎡ FACT real ⎤ ⎢ GENER sp ⎥ ⎢ ⎢QUANT one ⎥ ⎥ ⎢ ⎢REFER det ⎥ ⎥ ⎢ ⎣CARD 1 ⎦ ⎥ ETYPE 0 VARIA con c32d PRED gitter.1.1 ⎡ ⎤ FACT real ⎢ QUANT mult ⎥ ⎣REFER indet ⎦ ETYPE 1 corresponding word plus a numerical homograph identifier and a numerical reading identifier. As current web search engines provide no (or only rough) lemmatization for German, one must go one step further away from the semantic network by generating full forms for each word belonging to a lexical concept. (Others have tried a similar approach; for example, Neumann and Sacaleanu (2005) construct IR queries which can be easily adapted to different IR engines.) In the example, the node sitzen.1.1 (to sit) leads to 26 different full forms (shown below in example (2)). These forms can be used as search terms; as alternatives, they are connected disjunctively. Thus for each semantic network from query expansion, a conjunction of disjunctions of word forms is constructed. Word forms can belong to words not occurring in the question because query expansion can introduce new concepts (and thereby new words), e.g. by inferential rules. The Boolean formulae for several semantic netwo</context>
<context position="28007" citStr="(2005)" startWordPosition="4679" endWordPosition="4679">eliably. 5 Related Work There are few QA systems for German. The system described by Neumann and Xu (2003) works on German web pages. Its general approach differs from InSicht because it relies on shallow, but robust methods, while InSicht builds on sentence parsing and semantic representations. In this respect, InSicht resembles the (English) textual QA system presented by Harabagiu et al. (2001). In contrast to InSicht, this system applies a theorem prover and a large knowledge base to validate candidate answers. An interesting combination of web and textual QA is presented by Vicedo et al. (2005): English and Spanish web documents are used to enhance textual QA in Spanish. 6This high number is in part an artifact of the preselection query generation. In a more thorough analysis, human users could be given the NL question and be asked to formulate a corresponding search engine query. 67 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 One of the first web QA systems for English was Mulder (Kwok et al., 2001). Mulder parses only the text snippets returned by the search engine, while InSicht-W3 parses the underlying web pages because the text snippets often have omissions (</context>
</contexts>
<marker>2005</marker>
<rawString>Carol Peters, Paul Clough, Julio Gonzalo, Gareth J. F. Jones, Michael Kluck, and Bernardo Magnini, editors. 2005. Multilingual Information Access for Text, Speech and Images: 5th Workshop of the Cross-Language Evaluation Forum, CLEF 2004, volume 3491 of LNCS. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e L Vicedo</author>
<author>Maximiliano Saiz</author>
<author>Rub´en Izquierdo</author>
<author>Fernando Llopis</author>
</authors>
<title>Does English help question answering in Spanish? In (Peters et al.,</title>
<date>2005</date>
<pages>552--556</pages>
<contexts>
<context position="28007" citStr="Vicedo et al. (2005)" startWordPosition="4676" endWordPosition="4679">istently and reliably. 5 Related Work There are few QA systems for German. The system described by Neumann and Xu (2003) works on German web pages. Its general approach differs from InSicht because it relies on shallow, but robust methods, while InSicht builds on sentence parsing and semantic representations. In this respect, InSicht resembles the (English) textual QA system presented by Harabagiu et al. (2001). In contrast to InSicht, this system applies a theorem prover and a large knowledge base to validate candidate answers. An interesting combination of web and textual QA is presented by Vicedo et al. (2005): English and Spanish web documents are used to enhance textual QA in Spanish. 6This high number is in part an artifact of the preselection query generation. In a more thorough analysis, human users could be given the NL question and be asked to formulate a corresponding search engine query. 67 EACL 2006 Workshop on Multilingual Question Answering - MLQA06 One of the first web QA systems for English was Mulder (Kwok et al., 2001). Mulder parses only the text snippets returned by the search engine, while InSicht-W3 parses the underlying web pages because the text snippets often have omissions (</context>
</contexts>
<marker>Vicedo, Saiz, Izquierdo, Llopis, 2005</marker>
<rawString>Jos´e L. Vicedo, Maximiliano Saiz, Rub´en Izquierdo, and Fernando Llopis. 2005. Does English help question answering in Spanish? In (Peters et al., 2005), pages 552–556.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>