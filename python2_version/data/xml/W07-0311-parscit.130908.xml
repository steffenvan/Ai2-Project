<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056239">
<title confidence="0.9959145">
Enhancing commercial grammar-based applications using robust
approaches to speech understanding
</title>
<author confidence="0.901505">
Matthieu H´ebert
</author>
<affiliation confidence="0.728118">
Network ASR R+D, Nuance Communications
</affiliation>
<address confidence="0.886931">
1500, Universit´e, Suite 935, Montr´eal, Qu´ebec, H3A 3T2, Canada
</address>
<email confidence="0.994813">
hebert@nuance.com
</email>
<sectionHeader confidence="0.99556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984333333333">
This paper presents a series of measure-
ments of the accuracy of speech under-
standing when grammar-based or robust
approaches are used. The robust ap-
proaches considered here are based on sta-
tistical language models (SLMs) with the
interpretation being carried out by phrase-
spotting or robust parsing methods. We
propose a simple process to leverage ex-
isting grammars and logged utterances
to upgrade grammar-based applications to
become more robust to out-of-coverage
inputs. All experiments herein are run
on data collected from deployed directed
dialog applications and show that SLM-
based techniques outperform grammar-
based ones without requiring any change
in the application logic.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987244444445">
The bulk of the literature on spoken dialog systems
is based on the simple architecture in which the
input speech is processed by a statistical language
model-based recognizer (SLM-based recognizer) to
produce a word string. This word string is further
processed by a robust parser (Ward, 1990) or call
router (Gorin et al, 1997) to be converted in a se-
mantic interpretation. However, it is striking to see
that a large portion of deployed commercial appli-
cations do not follow this architecture and approach
the recognition/interpretation problem by relying on
hand-crafted rules (context-free grammars - CFGs).
The apparent reasons for this are the up-front cost
and additional delays of collecting domain-specific
utterances to properly train the SLM (not to men-
tion semantic tagging needed to train the call router)
(Hemphill et al, 1990; Knight et al, 2001; Gorin et
al, 1997). Choosing to use a grammar-based ap-
proach also makes the application predictable and
relatively easy to design. On the other hand, these
applications are usually very rigid: the users are al-
lowed only a finite set of ways to input their requests
and, by way of consequences, these applications suf-
fer from high out-of-grammar (OOG) rates or out-
of-coverage rates.
A few studies have been published compar-
ing grammar-based and SLM-based approaches to
speech understanding. In (Knight et al, 2001),
a comparison of grammar-based and robust ap-
proaches is presented for a user-initiative home au-
tomation application. The authors concluded that
it was relatively easy to use the corpus collected
during the course of the application development to
train a SLM which would perform better on out-
of-coverage utterances, while degrading the accu-
racy on in-coverage utterances. They also reported
that the SLM-based system showed slightly lower
word error rate but higher semantic error rate for
the users who know the application’s coverage. In
(Rayner et al, 2005), a rigorous test protocol is pre-
sented to compare grammar-based and robust ap-
proaches in the context of a medical translation sys-
tem. The paper highlights the difficulties to con-
struct a clean experimental set-up. Efforts are spent
to control the training set of both approaches to
</bodyText>
<page confidence="0.933974">
76
</page>
<note confidence="0.770946">
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 76–83,
NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999846139534884">
have them align. The training sets are defined as
the set of data available to build each system: for a
grammar-based system, it might be a series of sam-
ple dialogs. (ten Bosch, 2005) presents experiments
comparing grammar-based and SLM-based systems
for naive users and an expert user. They conclude
that the SLM-based system is most effective in re-
ducing the error rate for naive users. Recently (see
(Balakrishna et al, 2006)), a process was presented
to automatically build SLMs from a wide variety
of sources (in-service data, thesaurus, WordNet and
world-wide web). Results on data from commer-
cial speech applications presented therein echo ear-
lier results (Knight et al, 2001) while reducing the
effort to build interpretation rules.
Most of the above studies are not based on data
collected on deployed applications. One of the con-
clusions from previous work, based on the measured
fact that in-coverage accuracy of the grammar-based
systems was far better than the SLM one, was that
as people get more experience with the applications,
they will naturally learn its coverage and gravitate
towards it. While this can be an acceptable option
for some types of applications (when the user pop-
ulation tends to be experienced or captive), it cer-
tainly is not a possibility for large-scale commercial
applications that are targeted at the general public. A
few examples of such applications are public transit
schedules and fares information, self-help applica-
tions for utilities, banks, telecommunications busi-
ness, and etc. Steering application design and re-
search based on in-coverage accuracy is not suitable
for these types of applications because a large frac-
tion of the users are naives and tend to use more nat-
ural and unconstrained speech inputs.
This paper exploits techniques known since the
90’s (SLM with robust parsing, (Ward, 1990)) and
applies them to build robust speech understanding
into existing large scale directed dialog grammar-
based applications. This practical application of
(Ward, 1990; Knight et al, 2001; Rayner et al, 2005;
ten Bosch, 2005) is cast as an upgrade problem
which must obey the following constraints.
</bodyText>
<listItem confidence="0.993185428571429">
1. No change in the application logic and to the
voice user interface (VUI)
2. Roughly similar CPU consumption
3. Leverage existing grammars
4. Leverage existing transcribed utterances
5. Simple process that requires little manual inter-
vention
</listItem>
<bodyText confidence="0.949597666666667">
The first constraint dictates that, for each context,
the interpretation engines (from the current and up-
graded systems) must return the same semantics (i.e.
same set of slots).
The rest of this paper is organized as follows. The
next Section describes the applications from which
the data was collected, the experimental set-up and
the accuracy measures used. Section 3 describes
how the semantic truth is generated. The main re-
sults of the upgrade from grammar-based to SLM-
based recognition are presented in Section 4. The
target audience for this paper is composed of appli-
cation developers and researchers that are interested
in the robust information extraction from directed
dialog speech applications targeted at the general
public.
2 Applications, corpus and experimental
set-up
</bodyText>
<subsectionHeader confidence="0.997442">
2.1 Application descriptions
</subsectionHeader>
<bodyText confidence="0.999899761904762">
As mentioned earlier, the data for this study was col-
lected on deployed commercial directed dialog ap-
plications. AppA is a self-help application in the in-
ternet service provider domain, while AppB is also
a self-help application in the public transportation
domain. Both applications are grammar-based di-
rected dialogs and receive a daily average of 50k
calls. We will concentrate on a subset of contexts
(dialog states) for each application as described in
Table 1. The mainmenu grammars (each application
has its own mainmenu grammar) contain high-level
targets for the rest of the application and are active
once the initial prompt has been played. The com-
mand grammar contains universal commands like
“help”, “agent”, etc. The origin and destination
grammars contain a list of — 2500 cities and states
with the proper prefixes to discriminate origin and
destination. num type passenger accepts up to nine
passengers of types adults, children, seniors, etc.
Finally time is self explanatory. For each applica-
tion, the prompt directs the user to provide a specific
</bodyText>
<page confidence="0.998036">
77
</page>
<table confidence="0.999518916666667">
Context Description Active grammars Training Testing
sentences utts
AppA MainMenu Main menu mainmenu and 5000 5431
for the application commands (350) (642)
AppB MainMenu Main menu mainmenu and 5000 4039
for the application commands (19) (987)
AppB Origin Origin of travel origin, destination 5000 8818
and commands (20486) (529)
AppB Passenger Number and type num type passenger 1500 2312
of passenger and commands (32332) (66)
AppB Time Time of departure time and commands 1000 1149
(4102) (55)
</table>
<tableCaption confidence="0.998267">
Table 1: Description of studied contexts for each application. Note that the AppB Origin context contains a
</tableCaption>
<bodyText confidence="0.990190833333334">
destination grammar: this is due to the fact that the same set of grammars was used in the AppB Destination
context (not studied here). “Training” contains the number of training sentences drawn from the corpus and
used to train the SLMs. As mentioned in Sec. 2.3, in the case of word SLMs, we also use sentences that are
covered by the grammars in each context as backoffs (see Sec. 2). The number of unique sentences covered
by the grammars is in parenthesis in the “Training” column. The “Testing” column contains the number of
utterances in the test set. The number of those utterances that contain no speech (noise) is in parenthesis.
piece of information (directed dialog). Each gram-
mar fills a single slot with that information. The in-
formation contained in the utterance “two adults and
one child” (AppB Passenger context) would be col-
lapsed to fill the num type passenger slot with the
value “Adult2 Child1”. From the application point
of view, each context can fill only a very limited set
of slots. To keep results as synthesized as possible,
unless otherwise stated, the results from all studied
contexts will be presented per application: as such
results from all contexts in AppB will be pooled to-
gether.
</bodyText>
<subsectionHeader confidence="0.999311">
2.2 Corpus description
</subsectionHeader>
<bodyText confidence="0.9999741">
Table 1 presents the details of the corpus that we
have used for this study. As mentioned above the en-
tire corpora used for this study is drawn from com-
mercially deployed systems that are used by the gen-
eral public. The user population reflects realistic
usage (expert vs naive), noise conditions, handsets,
etc. The training utterances do not contain noise ut-
terances and is used primarily for SLM training (no
acoustic adaptation of the recognition models is per-
formed).
</bodyText>
<subsectionHeader confidence="0.998818">
2.3 Experimental set-up description
</subsectionHeader>
<bodyText confidence="0.99997912">
The baseline system is the grammar-based system;
the recognizer uses, on a per-context basis, the gram-
mars listed in Table 1 in parallel. The SLM systems
studied all used the same interpretation engine: ro-
bust parsing with the grammars listed in Table 1 as
rules to fill slots. Note that this allows the applica-
tion logic to stay unchanged since the set of potential
slots returned within any given context is the same as
for the grammar-based systems (see first constraint
in Sec. 1). Adhering to this experimental set-up also
guarantees that improvements measured in the lab
will have a direct impact on the raw accuracy of the
deployed application.
We have considered two different SLM-based
systems in this study: standard SLM (wordSLM)
and class-based SLM (classSLM) (Jelinek, 1990;
Gillett and Ward, 1998). In the classSLM systems,
the classes are defined as the rules of the interpre-
tation engine (i.e. the grammars active for each
context as defined in Table 1). The SLMs are all
trained on a per-context basis (Xu and Rudnicky,
2000; Goel and Gopinath, 2006) as bi-grams with
Witten-Bell discounting. To insure that the word-
SLM system covered all sentences that the grammar-
based system does, we augmented the training set of
</bodyText>
<page confidence="0.982565">
78
</page>
<figure confidence="0.969761866666667">
CA-in/FA-total
0 0.05 0.1 0.15 0.2
FA-total
Recall/Precision
0 0.2 0.4 0.6 0.8 1
Recall
CA-in
0.8
0.6
0.4
0.2
0
1
grammar-based - automatic
grammar-based - human
wordSLM - automatic
wordSLM - human
Precision
0.98
0.96
0.94
0.92
0.88
0.86
0.9
1
grammar-based - automatic
grammar-based - human
wordSLM - automatic
wordSLM - human
</figure>
<figureCaption confidence="0.9993495">
Figure 1: ROC curves for AppA MainMenu with the automatic or human-generated truth. In each the
grammar-based and SLM-based systems are compared.
</figureCaption>
<bodyText confidence="0.9165025">
the wordSLM (see Table 1) with the list of sentences
that are covered by the baseline grammar-based sys-
tem. This acts as a backoff in case a word or bi-
gram is not found in the training set (not to be con-
fused with bi-gram to uni-gram backoffs found in
standard SLM training). This is particularly helpful
when a little amount of data is available for training
the wordSLM (see Sec. 4.3).
</bodyText>
<subsectionHeader confidence="0.99223">
2.4 Accuracy measures
</subsectionHeader>
<bodyText confidence="0.9999825">
Throughout this paper, we will use two sets of mea-
sures. This is motivated by the fact that applica-
tion developers are familiar with the concepts of cor-
rect/false acceptance at the utterance level. For in-
formation extraction (slot filling) from utterances,
these concepts are restrictive because an utterance
can be partly correct or wrong. In this case we pre-
fer a more relevant measure from the information re-
trieval field: precision and recall on a per-slot basis.
We use the following definitions.
</bodyText>
<listItem confidence="0.997425444444444">
• CA-in = #utts that had ALL slots correct (slot
name and value) / #utts that are in-coverage
(i.e. truth has at least a slot filled)
• FA-total = #utts that had at least one erroneous
slot (slot name or value) / total #utts
• Precision = #slot correct slots (slot name and
value) / #slots returned by system
• Recall = #slot correct slots (slot name and
value) / #slots potential slots (in truth)
</listItem>
<bodyText confidence="0.999773272727273">
Since applications use confidence extensively to
guide the course of dialogue, it is of limited interest
to study forced-choice accuracy (accuracy with no
rejection). Hence, we will present receiver operat-
ing characteristic (ROC) curves. The slot confidence
measure is based on redundancy of a slot/value pair
across the NBest list. For CA-in and FA-total, the
confidence is the average confidence of all slots
present in the utterance. Note that in the case where
each utterance only fills a single slot, CA-in = Re-
call.
</bodyText>
<sectionHeader confidence="0.998915" genericHeader="introduction">
3 Truth
</sectionHeader>
<bodyText confidence="0.999936142857143">
Due to the large amount of data processed (see Table
1), semantic tagging by a human may not be avail-
able for all contexts (orthographic transcriptions are
available however). We need to resort to a more au-
tomatic way of generating the truth files while main-
taining a strong confidence in our measurements. To
this end, we need to ensure that any automatic way
of generating the truth will not bias the results to-
wards any of the systems.
The automatic truth can be generated by simply
using the robust parser (see Sec. 2.3) on the or-
thographic transcriptions which are fairly cheap to
acquire. This will generate a semantic interpreta-
tion for those utterances that contain fragments that
</bodyText>
<page confidence="0.993136">
79
</page>
<bodyText confidence="0.999924875">
parse rules defined by the interpretation engine. The
human-generated truth is the result of semantically
tagging all utterances that didn’t yield a full parse
by one of the rules for the relevant context.
Figure 1 presents the ROC curves of human and
automatic truth generation for the grammar-based
and wordSLM systems. We can see that human se-
mantic tagging increases the accuracy substantially,
but this increase doesn’t seem to favor one system
over the other. We are thus led to believe that in our
case (very few well defined non-overlapping classes)
the automatic truth generation is sufficient. This
would not be the case, for example if for a given con-
text a time grammar and number were active classes.
Then, an utterance like “seven” might lead to an er-
roneous slot being automatically filled while a hu-
man tagger (who would have access to the entire di-
alog) would tag it correctly.
In our experiments, we will use the hu-
man semantically tagged truth when available
(AppA MainMenu and AppB Origin). We have
checked that the conclusions of this paper are not
altered in any way if the automatic semantically
tagged truth had been used for these two contexts.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="related work">
4 Results and analysis
</sectionHeader>
<subsectionHeader confidence="0.988629">
4.1 Out-of-coverage analysis
</subsectionHeader>
<table confidence="0.997513571428571">
Context (#utts) grammar- SLM-based
based
AppA MainMenu 1252 1086
AppB MainMenu 1287 1169
AppB Origin 1617 1161
AppB Passenger 492 414
AppB Time 327 309
</table>
<tableCaption confidence="0.9439645">
Table 2: Number of utterances out-of-coverage for
each context.
</tableCaption>
<bodyText confidence="0.999831333333333">
Coverage is a function of the interpretation en-
gine. We can readily analyze the effect of going
from a grammar-based interpretation engine (gram-
mars in Table 1 are in parallel) to the robust ap-
proach (rules from grammars in Table 1 are used
in robust parsing). This is simply done by running
the interpretation engine on the orthographic tran-
scriptions. As expected, the coverage increased. Ta-
ble 2 shows the number of utterances that didn’t
fire any rule for each of the interpretation engines.
These include noise utterances as described in Table
1. If we remove the noise utterances, going from
the grammar-based interpretation to an SLM-based
one reduces the out-of-coverage by 31%. This result
is interesting because the data was collected from
directed-dialog applications which should be heav-
ily guiding the users to the grammar-based system’s
coverage.
</bodyText>
<subsectionHeader confidence="0.96216">
4.2 Results with recognizer
</subsectionHeader>
<bodyText confidence="0.999937702702703">
The main results of this paper are found in Fig-
ure 2. It presents for grammar-based, wordSLM
and classSLM systems the four measurements men-
tioned in Sec.2.4 for AppA and AppB. We have
managed, with proper Viterbi beam settings, to keep
in the increase in CPU (grammar-based system —*
SLM-based system) between 0% and 24% relative.
We can see that the wordSLM is outperforming the
classSLM. The SLM-based systems outperform the
grammar-based systems substantially (— 30 — 50%
error rate reduction on most of the confidence do-
main). The only exception to this is the classSLM
in AppA: we will come back to this in Sec. 4.4.
This can be interpreted as a different conclusion than
those of (Knight et al, 2001; ten Bosch, 2005). The
discrepancy can be tied to the fact that the data we
are studying comes from a live deployment targeted
to the general public. In this case, we can make
the hypothesis that a large fraction of the popula-
tion is composed of naive users. As mentioned in
(ten Bosch, 2005), SLM-based systems perform bet-
ter than grammar-based ones on that cross-section of
the user population.
One might argue that the comparison between the
grammar-based and wordSLM systems is unfair be-
cause the wordSLM intrinsically records the a priori
probability that a user says a specific phrase while
the grammar-based system studied here didn’t ben-
efit from this information. In Sec. 4.4, we will ad-
dress this and show that a priori has a negligible ef-
fect in this context.
Note that these impressive results are surprisingly
easy to achieve. A simple process could be as fol-
lows. An application is developed using grammar-
based paradigm. After a limited deployment or pilot
with real users, a wordSLM is built from transcribed
(orthographic) data from the field. Then the recog-
</bodyText>
<page confidence="0.972508">
80
</page>
<figure confidence="0.998979875">
CA-in/FA-total
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
FA-total
Recall/Precision
0 0.2 0.4 0.6 0.8 1
Recall
CA-in
0.8
0.6
0.4
0.2
0
1
grammar-based (73ms)
wordSLM (74ms)
classSLM (108ms)
Precision
0.95
0.85
0.75
0.9
0.8
0.7
1
grammar-based (73ms)
wordSLM (74ms)
classSLM (108ms)
AppA
CA-in/FA-total
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35
FA-total
Recall/Precision
0 0.2 0.4 0.6 0.8 1
Recall
CA-in
0.8
0.6
0.4
0.2
0
1
grammar-based (94ms)
wordSLM (117ms)
classSLM (113ms)
Precision
0.95
0.85
0.75
0.9
0.8
0.7
1
grammar-based (94ms)
wordSLM (117ms)
classSLM (113ms)
AppB
</figure>
<figureCaption confidence="0.9618075">
Figure 2: ROC curves for AppA (top) and AppB (bottom). In parenthesis is the average time for the
recognition and interpretation.
</figureCaption>
<bodyText confidence="0.991439333333333">
nition and interpretation engines are upgraded. The
grammars built in the early stages of development
can largely be re-used as interpretation rules.
</bodyText>
<subsectionHeader confidence="0.999811">
4.3 Amount of training data for SLM training
</subsectionHeader>
<bodyText confidence="0.999913285714286">
For the remaining Sections, we will use precision
and recall for simplicity. We will discuss an ex-
treme case where only a subset of 250 sentences
from the standard training set is used to train the
SLM. We have run experiments with two contexts:
AppA MainMenu and AppB Origin. These con-
texts are useful because a) we have the human-
generated truth and b) they represent extremes in the
complexity of grammars (see Section 2). On one
hand, the grammars for AppA MainMenu can cover
a total of 350 unique sentences while AppB Origin
can cover over 20k. As the amount of training
data for the SLMs is reduced from 5000 down to
250 sentences, the accuracy for AppA MainMenu
is only perceptibly degraded for the wordSLM and
classSLM systems on the entire confidence domain
(not shown here). On the other hand, in the case
of the more complex grammar (class), it is a dif-
ferent story which highlights a second regime. For
AppB Origin, the precision and recall curve is pre-
sented on Figure 3. In the case of classSLM (left),
</bodyText>
<page confidence="0.993736">
81
</page>
<figure confidence="0.999012551724138">
Recall/Precision
0 0.2 0.4 0.6 0.8 1
Recall
Recall/Precision
0 0.2 0.4 0.6 0.8 1
Recall
Precision
0.95
0.85
0.75
0.9
0.8
0.7
1
grammar-based
classSLM - 5000
classSLM - 250
Precision
0.95
0.85
0.75
0.9
0.8
0.7
1
grammar-based
wordSLM - 5000
wordSLM - 250
wordSLM - 250 - no backoff
</figure>
<figureCaption confidence="0.996504">
Figure 3: Precision and recall for the AppB Origin context as the amount of training data for the SLMs is
reduced. On the left, classSLM systems are presented; on the right it is the wordSLM.
</figureCaption>
<bodyText confidence="0.999323272727273">
even with very little training data, the accuracy is
far better than the grammar-based system and only
slightly degraded by reducing the size of the training
set. In the case of wordSLM (right), we can still see
that the accuracy is better than the grammar-based
system (refer to “wordSLM - 250” on the graph),
but the reduction of training data has a much more
visible effect. If we remove the sentences that were
drawn from the grammar-based system’s coverage
(backoff - see Sec. 2.3), we can see that the drop in
accuracy is even more dramatic.
</bodyText>
<subsectionHeader confidence="0.999455">
4.4 Coverage of interpretation rules and priors
</subsectionHeader>
<bodyText confidence="0.999963022222222">
As seen in Sec. 4.2, the classSLM results for AppA
are disappointing. They, however, shed some light
on two caveats of the robust approach described
here. The first caveat is the coverage of the interpre-
tation rules. As described in Sec. 2, the SLM-based
systems’ training sets and interpretation rules (gram-
mars from Table 1) were built in isolation. This can
have a dramatic effect: after error analysis of the
classSLM system’s results, we noticed a large frac-
tion of errors for which the recognized string was a
close (semantically identical) variant of a rule in the
interpretation engine (“cancellations” vs “cancella-
tion”). In response, we implemented a simple tool
to increase the coverage of the grammars (and hence
the coverage of the interpretation rules) using the list
of words seen in the training set. The criteria for se-
lection is based on common stem with a word in the
grammar.
The second caveat is based on fact that the
classSLM suffers from a lack of prior information
once the decoding process enters a specific class
since the grammars (class) do not contain priors.
The wordSLM benefits from the full prior informa-
tion all along the search. We have solved this by
training a small wordSLM within each grammar
(class): for each grammar, the training set for the
small wordSLM is composed of the set of fragments
from all utterances in the main training set that fire
that specific rule. Note that this represents a way
to have the grammar-based and SLM-based systems
share a common training set (Rayner et al, 2005).
In Figure 4, we show the effect of increasing the
coverage and adding priors in the grammars. The
first conclusion comes in comparing the grammar-
based results with and without increased coverage
(enhanced+priors in figure) and priors. We see that
the ROC curves are one on top of the other. The only
differences are: a) at low confidence where the en-
hanced+priors version shows better precision, and
b) the CPU consumption is greatly reduced (73ms
—* 52ms). When the enhanced+priors version of
the grammars (for classes and interpretation rules)
is used in the context of the classSLM system, we
can see that there is a huge improvement in the accu-
racy: this shows the importance of keeping the SLM
</bodyText>
<page confidence="0.984831">
82
</page>
<figure confidence="0.997393333333333">
Recall/Precision
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.834422333333333">
Figure 4: ROC curves for AppA showing the ef-
fect of increasing the grammar coverage and adding
prior information in the grammars.
</figureCaption>
<bodyText confidence="0.993516666666667">
and interpretation rules in-sync. The final classSLM
ROC curve (Figure 4) is now comparable with its
wordSLM counter-part (Figure 2 upper right graph).
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99996395">
We have demonstrated in this paper that grammar-
based systems for commercially deployed directed
dialog applications targeted at the general public
can be improved substantially by using SLMs with
robust parsing. This conclusion is different than
(Rayner et al, 2005) and can be attributed to that fact
that the general public is likely composed of a large
portion of naive users. We have sketched a very sim-
ple process to upgrade an application from using a
grammar-based approach to a robust approach when
in-service data and interpretation rules (grammars)
are available. We have also shown that only a very
small amount of data is necessary to train the SLMs
(Knight et al, 2001). Class-based SLMs should be
favored in the case where the amount of training
data is low while word-based SLMs should be used
when enough training data is available. In the case
of non-overlapping classes, we have demonstrated
the soundness of automatically generated semantic
truth.
</bodyText>
<sectionHeader confidence="0.999072" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999880333333333">
The author would like to acknowledge the helpful
discussions with M. Fanty, R. Tremblay, R. Lacou-
ture and K. Govindarajan during this project.
</bodyText>
<sectionHeader confidence="0.996429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999395157894737">
W. Ward. 1990. The CMU Air Travel Information Ser-
vice: Understanding spontaneous speech. Proc. of the
Speech and Natural Language Workshop, Hidden Val-
ley PA, pp. 127–129.
A.L. Gorin, B.A. Parker, R.M. Sachs and J.G. Wilpon.
1997. How may I help you?. Speech Communica-
tions, 23(1):113–127.
C. Hemphill, J. Godfrey and G. Doddington. 1990. The
ATIS spoken language systems and pilot corpus. Proc.
of the Speech and Natural Language Workshop, Hid-
den Valley PA, pp. 96–101.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. Proc. ofEuroSpeech.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kanzaki
and Y. Nakao. 2005. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. Proc. of EuroSpeech.
L. ten Bosch. 2005. Improving out-of-coverage lan-
guage modelling in a multimodal dialogue system us-
ing small training sets. Proc. of EuroSpeech.
M. Balakrishna, C. Cerovic, D. Moldovan and E. Cave.
2006. Automatic generation of statistical language
models for interactive voice response applications.
Proc. ofICSLP.
J. Gillett and W. Ward. 1998. A language model com-
bining tri-grams and stochastic context-free grammars.
Proc. ofICSLP.
F. Jelinek. 1990. Readings in speech recognition, Edited
by A. Waibel and K.-F. Lee , pp. 450-506. Morgan
Kaufmann, Los Altos.
W. Xu and A. Rudnicky. 2000. Language modeling for
dialog system. Proc. ofICSLP.
V. Goel and R. Gopinath. 2006. On designing context
sensitive language models for spoken dialog systems.
Proc. ofICSLP.
</reference>
<figure confidence="0.987773833333333">
Precision
0.95
0.85
0.75
0.9
0.8
0.7
1
gram.-based (73ms)
gram.-based - enhanced+priors (52ms)
classSLM (108ms)
classSLM - enhanced+priors (79ms)
</figure>
<page confidence="0.982808">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.213991">
<title confidence="0.95351">Enhancing commercial grammar-based applications using approaches to speech understanding</title>
<author confidence="0.5896645">Matthieu Network ASR RD</author>
<author confidence="0.5896645">Nuance</author>
<address confidence="0.697547">1500, Universit´e, Suite 935, Montr´eal, Qu´ebec, H3A 3T2,</address>
<email confidence="0.999537">hebert@nuance.com</email>
<abstract confidence="0.999632368421053">This paper presents a series of measurements of the accuracy of speech understanding when grammar-based or robust approaches are used. The robust approaches considered here are based on statistical language models (SLMs) with the interpretation being carried out by phrasespotting or robust parsing methods. We propose a simple process to leverage exgrammars utterances to upgrade grammar-based applications to become more robust to out-of-coverage inputs. All experiments herein are run on data collected from deployed directed dialog applications and show that SLMbased techniques outperform grammarbased ones without requiring any change in the application logic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Ward</author>
</authors>
<title>The CMU Air Travel Information Service: Understanding spontaneous speech.</title>
<date>1990</date>
<booktitle>Proc. of the Speech and Natural Language Workshop,</booktitle>
<pages>127--129</pages>
<location>Hidden Valley PA,</location>
<contexts>
<context position="1239" citStr="Ward, 1990" startWordPosition="179" endWordPosition="180">nces to upgrade grammar-based applications to become more robust to out-of-coverage inputs. All experiments herein are run on data collected from deployed directed dialog applications and show that SLMbased techniques outperform grammarbased ones without requiring any change in the application logic. 1 Introduction The bulk of the literature on spoken dialog systems is based on the simple architecture in which the input speech is processed by a statistical language model-based recognizer (SLM-based recognizer) to produce a word string. This word string is further processed by a robust parser (Ward, 1990) or call router (Gorin et al, 1997) to be converted in a semantic interpretation. However, it is striking to see that a large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars - CFGs). The apparent reasons for this are the up-front cost and additional delays of collecting domain-specific utterances to properly train the SLM (not to mention semantic tagging needed to train the call router) (Hemphill et al, 1990; Knight et al, 2001; Gorin et al, 1997). Choosing to</context>
<context position="5213" citStr="Ward, 1990" startWordPosition="811" endWordPosition="812">ve), it certainly is not a possibility for large-scale commercial applications that are targeted at the general public. A few examples of such applications are public transit schedules and fares information, self-help applications for utilities, banks, telecommunications business, and etc. Steering application design and research based on in-coverage accuracy is not suitable for these types of applications because a large fraction of the users are naives and tend to use more natural and unconstrained speech inputs. This paper exploits techniques known since the 90’s (SLM with robust parsing, (Ward, 1990)) and applies them to build robust speech understanding into existing large scale directed dialog grammarbased applications. This practical application of (Ward, 1990; Knight et al, 2001; Rayner et al, 2005; ten Bosch, 2005) is cast as an upgrade problem which must obey the following constraints. 1. No change in the application logic and to the voice user interface (VUI) 2. Roughly similar CPU consumption 3. Leverage existing grammars 4. Leverage existing transcribed utterances 5. Simple process that requires little manual intervention The first constraint dictates that, for each context, the </context>
</contexts>
<marker>Ward, 1990</marker>
<rawString>W. Ward. 1990. The CMU Air Travel Information Service: Understanding spontaneous speech. Proc. of the Speech and Natural Language Workshop, Hidden Valley PA, pp. 127–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>B A Parker</author>
<author>R M Sachs</author>
<author>J G Wilpon</author>
</authors>
<title>How may I help you?.</title>
<date>1997</date>
<journal>Speech Communications,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1274" citStr="Gorin et al, 1997" startWordPosition="184" endWordPosition="187">d applications to become more robust to out-of-coverage inputs. All experiments herein are run on data collected from deployed directed dialog applications and show that SLMbased techniques outperform grammarbased ones without requiring any change in the application logic. 1 Introduction The bulk of the literature on spoken dialog systems is based on the simple architecture in which the input speech is processed by a statistical language model-based recognizer (SLM-based recognizer) to produce a word string. This word string is further processed by a robust parser (Ward, 1990) or call router (Gorin et al, 1997) to be converted in a semantic interpretation. However, it is striking to see that a large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars - CFGs). The apparent reasons for this are the up-front cost and additional delays of collecting domain-specific utterances to properly train the SLM (not to mention semantic tagging needed to train the call router) (Hemphill et al, 1990; Knight et al, 2001; Gorin et al, 1997). Choosing to use a grammar-based approach also </context>
</contexts>
<marker>Gorin, Parker, Sachs, Wilpon, 1997</marker>
<rawString>A.L. Gorin, B.A. Parker, R.M. Sachs and J.G. Wilpon. 1997. How may I help you?. Speech Communications, 23(1):113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hemphill</author>
<author>J Godfrey</author>
<author>G Doddington</author>
</authors>
<title>The ATIS spoken language systems and pilot corpus.</title>
<date>1990</date>
<booktitle>Proc. of the Speech and Natural Language Workshop,</booktitle>
<pages>96--101</pages>
<location>Hidden Valley PA,</location>
<contexts>
<context position="1786" citStr="Hemphill et al, 1990" startWordPosition="263" endWordPosition="266">ng. This word string is further processed by a robust parser (Ward, 1990) or call router (Gorin et al, 1997) to be converted in a semantic interpretation. However, it is striking to see that a large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars - CFGs). The apparent reasons for this are the up-front cost and additional delays of collecting domain-specific utterances to properly train the SLM (not to mention semantic tagging needed to train the call router) (Hemphill et al, 1990; Knight et al, 2001; Gorin et al, 1997). Choosing to use a grammar-based approach also makes the application predictable and relatively easy to design. On the other hand, these applications are usually very rigid: the users are allowed only a finite set of ways to input their requests and, by way of consequences, these applications suffer from high out-of-grammar (OOG) rates or outof-coverage rates. A few studies have been published comparing grammar-based and SLM-based approaches to speech understanding. In (Knight et al, 2001), a comparison of grammar-based and robust approaches is presente</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>C. Hemphill, J. Godfrey and G. Doddington. 1990. The ATIS spoken language systems and pilot corpus. Proc. of the Speech and Natural Language Workshop, Hidden Valley PA, pp. 96–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Knight</author>
<author>G Gorrell</author>
<author>M Rayner</author>
<author>D Milward</author>
<author>R Koeling</author>
<author>I Lewin</author>
</authors>
<title>Comparing grammar-based and robust approaches to speech understanding: a case study.</title>
<date>2001</date>
<booktitle>Proc. ofEuroSpeech.</booktitle>
<contexts>
<context position="1806" citStr="Knight et al, 2001" startWordPosition="267" endWordPosition="270">s further processed by a robust parser (Ward, 1990) or call router (Gorin et al, 1997) to be converted in a semantic interpretation. However, it is striking to see that a large portion of deployed commercial applications do not follow this architecture and approach the recognition/interpretation problem by relying on hand-crafted rules (context-free grammars - CFGs). The apparent reasons for this are the up-front cost and additional delays of collecting domain-specific utterances to properly train the SLM (not to mention semantic tagging needed to train the call router) (Hemphill et al, 1990; Knight et al, 2001; Gorin et al, 1997). Choosing to use a grammar-based approach also makes the application predictable and relatively easy to design. On the other hand, these applications are usually very rigid: the users are allowed only a finite set of ways to input their requests and, by way of consequences, these applications suffer from high out-of-grammar (OOG) rates or outof-coverage rates. A few studies have been published comparing grammar-based and SLM-based approaches to speech understanding. In (Knight et al, 2001), a comparison of grammar-based and robust approaches is presented for a user-initiat</context>
<context position="4047" citStr="Knight et al, 2001" startWordPosition="622" endWordPosition="625">a available to build each system: for a grammar-based system, it might be a series of sample dialogs. (ten Bosch, 2005) presents experiments comparing grammar-based and SLM-based systems for naive users and an expert user. They conclude that the SLM-based system is most effective in reducing the error rate for naive users. Recently (see (Balakrishna et al, 2006)), a process was presented to automatically build SLMs from a wide variety of sources (in-service data, thesaurus, WordNet and world-wide web). Results on data from commercial speech applications presented therein echo earlier results (Knight et al, 2001) while reducing the effort to build interpretation rules. Most of the above studies are not based on data collected on deployed applications. One of the conclusions from previous work, based on the measured fact that in-coverage accuracy of the grammar-based systems was far better than the SLM one, was that as people get more experience with the applications, they will naturally learn its coverage and gravitate towards it. While this can be an acceptable option for some types of applications (when the user population tends to be experienced or captive), it certainly is not a possibility for la</context>
<context position="5399" citStr="Knight et al, 2001" startWordPosition="836" endWordPosition="839">dules and fares information, self-help applications for utilities, banks, telecommunications business, and etc. Steering application design and research based on in-coverage accuracy is not suitable for these types of applications because a large fraction of the users are naives and tend to use more natural and unconstrained speech inputs. This paper exploits techniques known since the 90’s (SLM with robust parsing, (Ward, 1990)) and applies them to build robust speech understanding into existing large scale directed dialog grammarbased applications. This practical application of (Ward, 1990; Knight et al, 2001; Rayner et al, 2005; ten Bosch, 2005) is cast as an upgrade problem which must obey the following constraints. 1. No change in the application logic and to the voice user interface (VUI) 2. Roughly similar CPU consumption 3. Leverage existing grammars 4. Leverage existing transcribed utterances 5. Simple process that requires little manual intervention The first constraint dictates that, for each context, the interpretation engines (from the current and upgraded systems) must return the same semantics (i.e. same set of slots). The rest of this paper is organized as follows. The next Section d</context>
<context position="17271" citStr="Knight et al, 2001" startWordPosition="2803" endWordPosition="2806">rdSLM and classSLM systems the four measurements mentioned in Sec.2.4 for AppA and AppB. We have managed, with proper Viterbi beam settings, to keep in the increase in CPU (grammar-based system —* SLM-based system) between 0% and 24% relative. We can see that the wordSLM is outperforming the classSLM. The SLM-based systems outperform the grammar-based systems substantially (— 30 — 50% error rate reduction on most of the confidence domain). The only exception to this is the classSLM in AppA: we will come back to this in Sec. 4.4. This can be interpreted as a different conclusion than those of (Knight et al, 2001; ten Bosch, 2005). The discrepancy can be tied to the fact that the data we are studying comes from a live deployment targeted to the general public. In this case, we can make the hypothesis that a large fraction of the population is composed of naive users. As mentioned in (ten Bosch, 2005), SLM-based systems perform better than grammar-based ones on that cross-section of the user population. One might argue that the comparison between the grammar-based and wordSLM systems is unfair because the wordSLM intrinsically records the a priori probability that a user says a specific phrase while th</context>
</contexts>
<marker>Knight, Gorrell, Rayner, Milward, Koeling, Lewin, 2001</marker>
<rawString>S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koeling and I. Lewin. 2001. Comparing grammar-based and robust approaches to speech understanding: a case study. Proc. ofEuroSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>P Bouillon</author>
<author>N Chatzichrisafis</author>
<author>B A Hockey</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>Y Nakao</author>
</authors>
<title>A methodology for comparing grammar-based and robust approaches to speech understanding.</title>
<date>2005</date>
<booktitle>Proc. of EuroSpeech.</booktitle>
<contexts>
<context position="2882" citStr="Rayner et al, 2005" startWordPosition="439" endWordPosition="442">pproaches to speech understanding. In (Knight et al, 2001), a comparison of grammar-based and robust approaches is presented for a user-initiative home automation application. The authors concluded that it was relatively easy to use the corpus collected during the course of the application development to train a SLM which would perform better on outof-coverage utterances, while degrading the accuracy on in-coverage utterances. They also reported that the SLM-based system showed slightly lower word error rate but higher semantic error rate for the users who know the application’s coverage. In (Rayner et al, 2005), a rigorous test protocol is presented to compare grammar-based and robust approaches in the context of a medical translation system. The paper highlights the difficulties to construct a clean experimental set-up. Efforts are spent to control the training set of both approaches to 76 Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 76–83, NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics have them align. The training sets are defined as the set of data available to build each system: for a grammar-based </context>
<context position="5419" citStr="Rayner et al, 2005" startWordPosition="840" endWordPosition="843">rmation, self-help applications for utilities, banks, telecommunications business, and etc. Steering application design and research based on in-coverage accuracy is not suitable for these types of applications because a large fraction of the users are naives and tend to use more natural and unconstrained speech inputs. This paper exploits techniques known since the 90’s (SLM with robust parsing, (Ward, 1990)) and applies them to build robust speech understanding into existing large scale directed dialog grammarbased applications. This practical application of (Ward, 1990; Knight et al, 2001; Rayner et al, 2005; ten Bosch, 2005) is cast as an upgrade problem which must obey the following constraints. 1. No change in the application logic and to the voice user interface (VUI) 2. Roughly similar CPU consumption 3. Leverage existing grammars 4. Leverage existing transcribed utterances 5. Simple process that requires little manual intervention The first constraint dictates that, for each context, the interpretation engines (from the current and upgraded systems) must return the same semantics (i.e. same set of slots). The rest of this paper is organized as follows. The next Section describes the applica</context>
<context position="22858" citStr="Rayner et al, 2005" startWordPosition="3764" endWordPosition="3767">sed on fact that the classSLM suffers from a lack of prior information once the decoding process enters a specific class since the grammars (class) do not contain priors. The wordSLM benefits from the full prior information all along the search. We have solved this by training a small wordSLM within each grammar (class): for each grammar, the training set for the small wordSLM is composed of the set of fragments from all utterances in the main training set that fire that specific rule. Note that this represents a way to have the grammar-based and SLM-based systems share a common training set (Rayner et al, 2005). In Figure 4, we show the effect of increasing the coverage and adding priors in the grammars. The first conclusion comes in comparing the grammarbased results with and without increased coverage (enhanced+priors in figure) and priors. We see that the ROC curves are one on top of the other. The only differences are: a) at low confidence where the enhanced+priors version shows better precision, and b) the CPU consumption is greatly reduced (73ms —* 52ms). When the enhanced+priors version of the grammars (for classes and interpretation rules) is used in the context of the classSLM system, we ca</context>
<context position="24167" citStr="Rayner et al, 2005" startWordPosition="3979" endWordPosition="3982">ng the SLM 82 Recall/Precision 0 0.2 0.4 0.6 0.8 1 Recall Figure 4: ROC curves for AppA showing the effect of increasing the grammar coverage and adding prior information in the grammars. and interpretation rules in-sync. The final classSLM ROC curve (Figure 4) is now comparable with its wordSLM counter-part (Figure 2 upper right graph). 5 Conclusion We have demonstrated in this paper that grammarbased systems for commercially deployed directed dialog applications targeted at the general public can be improved substantially by using SLMs with robust parsing. This conclusion is different than (Rayner et al, 2005) and can be attributed to that fact that the general public is likely composed of a large portion of naive users. We have sketched a very simple process to upgrade an application from using a grammar-based approach to a robust approach when in-service data and interpretation rules (grammars) are available. We have also shown that only a very small amount of data is necessary to train the SLMs (Knight et al, 2001). Class-based SLMs should be favored in the case where the amount of training data is low while word-based SLMs should be used when enough training data is available. In the case of no</context>
</contexts>
<marker>Rayner, Bouillon, Chatzichrisafis, Hockey, Santaholma, Starlander, Isahara, Kanzaki, Nakao, 2005</marker>
<rawString>M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey, M. Santaholma, M. Starlander, H. Isahara, K. Kanzaki and Y. Nakao. 2005. A methodology for comparing grammar-based and robust approaches to speech understanding. Proc. of EuroSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L ten Bosch</author>
</authors>
<title>Improving out-of-coverage language modelling in a multimodal dialogue system using small training sets.</title>
<date>2005</date>
<booktitle>Proc. of EuroSpeech.</booktitle>
<contexts>
<context position="3547" citStr="Bosch, 2005" startWordPosition="547" endWordPosition="548">ammar-based and robust approaches in the context of a medical translation system. The paper highlights the difficulties to construct a clean experimental set-up. Efforts are spent to control the training set of both approaches to 76 Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 76–83, NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics have them align. The training sets are defined as the set of data available to build each system: for a grammar-based system, it might be a series of sample dialogs. (ten Bosch, 2005) presents experiments comparing grammar-based and SLM-based systems for naive users and an expert user. They conclude that the SLM-based system is most effective in reducing the error rate for naive users. Recently (see (Balakrishna et al, 2006)), a process was presented to automatically build SLMs from a wide variety of sources (in-service data, thesaurus, WordNet and world-wide web). Results on data from commercial speech applications presented therein echo earlier results (Knight et al, 2001) while reducing the effort to build interpretation rules. Most of the above studies are not based on</context>
<context position="5437" citStr="Bosch, 2005" startWordPosition="845" endWordPosition="846">cations for utilities, banks, telecommunications business, and etc. Steering application design and research based on in-coverage accuracy is not suitable for these types of applications because a large fraction of the users are naives and tend to use more natural and unconstrained speech inputs. This paper exploits techniques known since the 90’s (SLM with robust parsing, (Ward, 1990)) and applies them to build robust speech understanding into existing large scale directed dialog grammarbased applications. This practical application of (Ward, 1990; Knight et al, 2001; Rayner et al, 2005; ten Bosch, 2005) is cast as an upgrade problem which must obey the following constraints. 1. No change in the application logic and to the voice user interface (VUI) 2. Roughly similar CPU consumption 3. Leverage existing grammars 4. Leverage existing transcribed utterances 5. Simple process that requires little manual intervention The first constraint dictates that, for each context, the interpretation engines (from the current and upgraded systems) must return the same semantics (i.e. same set of slots). The rest of this paper is organized as follows. The next Section describes the applications from which t</context>
<context position="17289" citStr="Bosch, 2005" startWordPosition="2808" endWordPosition="2809">ms the four measurements mentioned in Sec.2.4 for AppA and AppB. We have managed, with proper Viterbi beam settings, to keep in the increase in CPU (grammar-based system —* SLM-based system) between 0% and 24% relative. We can see that the wordSLM is outperforming the classSLM. The SLM-based systems outperform the grammar-based systems substantially (— 30 — 50% error rate reduction on most of the confidence domain). The only exception to this is the classSLM in AppA: we will come back to this in Sec. 4.4. This can be interpreted as a different conclusion than those of (Knight et al, 2001; ten Bosch, 2005). The discrepancy can be tied to the fact that the data we are studying comes from a live deployment targeted to the general public. In this case, we can make the hypothesis that a large fraction of the population is composed of naive users. As mentioned in (ten Bosch, 2005), SLM-based systems perform better than grammar-based ones on that cross-section of the user population. One might argue that the comparison between the grammar-based and wordSLM systems is unfair because the wordSLM intrinsically records the a priori probability that a user says a specific phrase while the grammar-based sy</context>
</contexts>
<marker>Bosch, 2005</marker>
<rawString>L. ten Bosch. 2005. Improving out-of-coverage language modelling in a multimodal dialogue system using small training sets. Proc. of EuroSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Balakrishna</author>
<author>C Cerovic</author>
<author>D Moldovan</author>
<author>E Cave</author>
</authors>
<title>Automatic generation of statistical language models for interactive voice response applications.</title>
<date>2006</date>
<booktitle>Proc. ofICSLP.</booktitle>
<contexts>
<context position="3792" citStr="Balakrishna et al, 2006" startWordPosition="583" endWordPosition="586">Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 76–83, NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics have them align. The training sets are defined as the set of data available to build each system: for a grammar-based system, it might be a series of sample dialogs. (ten Bosch, 2005) presents experiments comparing grammar-based and SLM-based systems for naive users and an expert user. They conclude that the SLM-based system is most effective in reducing the error rate for naive users. Recently (see (Balakrishna et al, 2006)), a process was presented to automatically build SLMs from a wide variety of sources (in-service data, thesaurus, WordNet and world-wide web). Results on data from commercial speech applications presented therein echo earlier results (Knight et al, 2001) while reducing the effort to build interpretation rules. Most of the above studies are not based on data collected on deployed applications. One of the conclusions from previous work, based on the measured fact that in-coverage accuracy of the grammar-based systems was far better than the SLM one, was that as people get more experience with t</context>
</contexts>
<marker>Balakrishna, Cerovic, Moldovan, Cave, 2006</marker>
<rawString>M. Balakrishna, C. Cerovic, D. Moldovan and E. Cave. 2006. Automatic generation of statistical language models for interactive voice response applications. Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillett</author>
<author>W Ward</author>
</authors>
<title>A language model combining tri-grams and stochastic context-free grammars.</title>
<date>1998</date>
<booktitle>Proc. ofICSLP.</booktitle>
<contexts>
<context position="10807" citStr="Gillett and Ward, 1998" startWordPosition="1710" endWordPosition="1713">tation engine: robust parsing with the grammars listed in Table 1 as rules to fill slots. Note that this allows the application logic to stay unchanged since the set of potential slots returned within any given context is the same as for the grammar-based systems (see first constraint in Sec. 1). Adhering to this experimental set-up also guarantees that improvements measured in the lab will have a direct impact on the raw accuracy of the deployed application. We have considered two different SLM-based systems in this study: standard SLM (wordSLM) and class-based SLM (classSLM) (Jelinek, 1990; Gillett and Ward, 1998). In the classSLM systems, the classes are defined as the rules of the interpretation engine (i.e. the grammars active for each context as defined in Table 1). The SLMs are all trained on a per-context basis (Xu and Rudnicky, 2000; Goel and Gopinath, 2006) as bi-grams with Witten-Bell discounting. To insure that the wordSLM system covered all sentences that the grammarbased system does, we augmented the training set of 78 CA-in/FA-total 0 0.05 0.1 0.15 0.2 FA-total Recall/Precision 0 0.2 0.4 0.6 0.8 1 Recall CA-in 0.8 0.6 0.4 0.2 0 1 grammar-based - automatic grammar-based - human wordSLM - au</context>
</contexts>
<marker>Gillett, Ward, 1998</marker>
<rawString>J. Gillett and W. Ward. 1998. A language model combining tri-grams and stochastic context-free grammars. Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Readings in speech recognition,</title>
<date>1990</date>
<journal>Edited</journal>
<pages>450--506</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>Los Altos.</location>
<contexts>
<context position="10782" citStr="Jelinek, 1990" startWordPosition="1708" endWordPosition="1709">e same interpretation engine: robust parsing with the grammars listed in Table 1 as rules to fill slots. Note that this allows the application logic to stay unchanged since the set of potential slots returned within any given context is the same as for the grammar-based systems (see first constraint in Sec. 1). Adhering to this experimental set-up also guarantees that improvements measured in the lab will have a direct impact on the raw accuracy of the deployed application. We have considered two different SLM-based systems in this study: standard SLM (wordSLM) and class-based SLM (classSLM) (Jelinek, 1990; Gillett and Ward, 1998). In the classSLM systems, the classes are defined as the rules of the interpretation engine (i.e. the grammars active for each context as defined in Table 1). The SLMs are all trained on a per-context basis (Xu and Rudnicky, 2000; Goel and Gopinath, 2006) as bi-grams with Witten-Bell discounting. To insure that the wordSLM system covered all sentences that the grammarbased system does, we augmented the training set of 78 CA-in/FA-total 0 0.05 0.1 0.15 0.2 FA-total Recall/Precision 0 0.2 0.4 0.6 0.8 1 Recall CA-in 0.8 0.6 0.4 0.2 0 1 grammar-based - automatic grammar-b</context>
</contexts>
<marker>Jelinek, 1990</marker>
<rawString>F. Jelinek. 1990. Readings in speech recognition, Edited by A. Waibel and K.-F. Lee , pp. 450-506. Morgan Kaufmann, Los Altos.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>A Rudnicky</author>
</authors>
<title>Language modeling for dialog system.</title>
<date>2000</date>
<booktitle>Proc. ofICSLP.</booktitle>
<contexts>
<context position="11037" citStr="Xu and Rudnicky, 2000" startWordPosition="1751" endWordPosition="1754">s for the grammar-based systems (see first constraint in Sec. 1). Adhering to this experimental set-up also guarantees that improvements measured in the lab will have a direct impact on the raw accuracy of the deployed application. We have considered two different SLM-based systems in this study: standard SLM (wordSLM) and class-based SLM (classSLM) (Jelinek, 1990; Gillett and Ward, 1998). In the classSLM systems, the classes are defined as the rules of the interpretation engine (i.e. the grammars active for each context as defined in Table 1). The SLMs are all trained on a per-context basis (Xu and Rudnicky, 2000; Goel and Gopinath, 2006) as bi-grams with Witten-Bell discounting. To insure that the wordSLM system covered all sentences that the grammarbased system does, we augmented the training set of 78 CA-in/FA-total 0 0.05 0.1 0.15 0.2 FA-total Recall/Precision 0 0.2 0.4 0.6 0.8 1 Recall CA-in 0.8 0.6 0.4 0.2 0 1 grammar-based - automatic grammar-based - human wordSLM - automatic wordSLM - human Precision 0.98 0.96 0.94 0.92 0.88 0.86 0.9 1 grammar-based - automatic grammar-based - human wordSLM - automatic wordSLM - human Figure 1: ROC curves for AppA MainMenu with the automatic or human-generated</context>
</contexts>
<marker>Xu, Rudnicky, 2000</marker>
<rawString>W. Xu and A. Rudnicky. 2000. Language modeling for dialog system. Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>R Gopinath</author>
</authors>
<title>On designing context sensitive language models for spoken dialog systems.</title>
<date>2006</date>
<booktitle>Proc. ofICSLP.</booktitle>
<contexts>
<context position="11063" citStr="Goel and Gopinath, 2006" startWordPosition="1755" endWordPosition="1758"> systems (see first constraint in Sec. 1). Adhering to this experimental set-up also guarantees that improvements measured in the lab will have a direct impact on the raw accuracy of the deployed application. We have considered two different SLM-based systems in this study: standard SLM (wordSLM) and class-based SLM (classSLM) (Jelinek, 1990; Gillett and Ward, 1998). In the classSLM systems, the classes are defined as the rules of the interpretation engine (i.e. the grammars active for each context as defined in Table 1). The SLMs are all trained on a per-context basis (Xu and Rudnicky, 2000; Goel and Gopinath, 2006) as bi-grams with Witten-Bell discounting. To insure that the wordSLM system covered all sentences that the grammarbased system does, we augmented the training set of 78 CA-in/FA-total 0 0.05 0.1 0.15 0.2 FA-total Recall/Precision 0 0.2 0.4 0.6 0.8 1 Recall CA-in 0.8 0.6 0.4 0.2 0 1 grammar-based - automatic grammar-based - human wordSLM - automatic wordSLM - human Precision 0.98 0.96 0.94 0.92 0.88 0.86 0.9 1 grammar-based - automatic grammar-based - human wordSLM - automatic wordSLM - human Figure 1: ROC curves for AppA MainMenu with the automatic or human-generated truth. In each the gramma</context>
</contexts>
<marker>Goel, Gopinath, 2006</marker>
<rawString>V. Goel and R. Gopinath. 2006. On designing context sensitive language models for spoken dialog systems. Proc. ofICSLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>