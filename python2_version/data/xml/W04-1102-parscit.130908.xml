<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.79547">
A Preliminary Study on Probabilistic Models for Chinese Abbreviations
</title>
<author confidence="0.993206">
Jing-Shin Chang
</author>
<affiliation confidence="0.9544395">
Department of Computer Science &amp;
Information Engineering
National Chi-Nan University
Puli, Nantou, Taiwan, ROC.
</affiliation>
<email confidence="0.995732">
jshin@csie.ncnu.edu.tw
</email>
<author confidence="0.983826">
Yu-Tso Lai
</author>
<affiliation confidence="0.9543235">
Department of Computer Science &amp;
Information Engineering
National Chi-Nan University
Puli, Nantou, Taiwan, ROC.
</affiliation>
<email confidence="0.995001">
s0321521@ncnu.edu.tw
</email>
<sectionHeader confidence="0.99469" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881964285714">
Chinese abbreviations are widely used in
the modern Chinese texts. They are a
special form of unknown words, including
many named entities. This results in
difficulty for correct Chinese processing.
In this study, the Chinese abbreviation
problem is regarded as an error recovery
problem in which the suspect root words
are the “errors” to be recovered from a set
of candidates. Such a problem is mapped
to an HMM-based generation model for
both abbreviation identification and root
word recovery, and is integrated as part of
a unified word segmentation model when
the input extends to a complete sentence.
Two major experiments are conducted to
test the abbreviation models. In the first
experiment, an attempt is made to guess
the abbreviations of the root words. An
accuracy rate of 72% is observed. In
contrast, a second experiment is
conducted to guess the root words from
abbreviations. Some submodels could
achieve as high as 51% accuracy with the
simple HMM-based model. Some
quantitative observations against heuristic
abbreviation knowledge about Chinese
are also observed.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995696">
The modern Chinese language is a highly
abbreviated one due to the mixed uses of ancient
single character words as well as modern
multi-character words and compound words. The
abbreviated form and root form are used
interchangeably everywhere in the current Chinese
articles. Some news articles may contain about
20% of sentences that have suspect abbreviated
words in them (Lai 2003). Since abbreviations
cannot be enumerated in a dictionary, it forms a
special class of unknown words, many of which
originate from named entities. Many other open
class words are also abbreviatable. This particular
class thus introduces complication for Chinese
language processing, including the fundamental
word segmentation process (Chiang 1992, Lin
1993, Chang 1997) and many word-based
applications. For instance, a keyword-based
information retrieval system may requires the two
forms, such as “ A -1� ” and “ A &amp; _1� A ”
(“legislators”), in order not to miss any relevant
documents. The Chinese word segmentation
process is also significantly degraded by the
existence of unknown words (Chiang 1992),
including unknown abbreviations.
There are many heuristics for Chinese
abbreviations. Such heuristics, however, can easily
break (Sproat 2002). Currently, only some
quantitative approaches (Huang 1994a, 94b) are
available in predicting the presentation of an
abbreviation. Since such formulations regard the
word segmentation process and abbreviation
identification as two independent processes, they
probably cannot optimize the identification
process jointly with the word segmentation process,
and thus may lose the useful contextual
information. Some class-based segmentation
models (Sun 2002, Gao 2003) well integrate the
identification of some regular non-lexicalized units
(such as named entities). However, the
abbreviation process can be applied to almost all
word forms (or classes of words). Therefore, this
particular word formation process may have to be
handled as a separate layer in the segmentation
process.
To resolve the Chinese abbreviation problems
and integrate its identification into the word
segmentation process, this study proposes to
regard the abbreviation problem in the word
segmentation process as an “error recovery”
problem in which the suspect root words are the
“errors” to be recovered from a set of candidates
according to some generation probability criteria.
This idea implies that an HMM-based model for
identifying Chinese abbreviations could be
effective in either identifying the existence of an
abbreviation or the recovery of the root words
from an abbreviation. We therefore start with a
unified word segmentation model so that both
processes can be handled at the same time, and
when the input is reduced to a single abbreviated
word, the model can be equally useful for
recovering its root.
As a side effect of using HMM-based
formulation, we expect that a large abbreviation
dictionary could be derived from a large corpus or
from web documents through the training process
of the unified word segmentation model
automatically.
Section 2 will show our HMM models and the
three abbreviation problems correspond to the
three basic HMM problems. Section 3 will show
the experiment setup. Section 4 will examine the
experiments to guess abbreviations from root or
vice versa.
</bodyText>
<sectionHeader confidence="0.954984" genericHeader="method">
2 Chinese Abbreviation Models
</sectionHeader>
<subsectionHeader confidence="0.916793">
2.1 An Error Recovery Paradigm
</subsectionHeader>
<bodyText confidence="0.999970470588235">
To resolve the abbreviation problems, first note
that the most common action one would take when
encountering an abbreviation is to find its
candidate roots (probably from a large
abbreviation dictionary if available or from an
ordinary dictionary with some educated guesses),
and then identify the most probable one. This
process is identical to the operation of many
spelling correction models, which generate the
candidate corrections according to a reversed word
formation process, then justify the best candidate.
Such an analogy indicates that we may use an
HMM model (Rabiner 1993), which is good at
finding the best unseen state sequence, for error
recovery. There will be a direct map between the
two paradigms if we regard the observed input
character sequence as our “observation sequence”,
and regard the unseen word candidates as the
underlying “state sequence”.
Given these mappings, we will be able to use
many standard processing approaches for HMM
when we have to answer some interesting
questions (including root word recovery). Among
all interesting questions for an HMM, we have
three basic questions to ask the model (Rabiner
1993), namely the output probability of an output
sequence, the best underlying state sequence and
the best parameters given a training corpus.
If we can ask the HMM for abbreviation the
same questions, then we will also be able to
answer the question on (1) what is the likelihood
that a string is an abbreviation, (2) what are the
best underlying root words for an input character
string that contains abbreviations, and (3) how to
estimate the model parameters automatically given
a corpus.
The first question is related to the problem of
generating an appropriate abbreviation from a root
word; the second question is linked to finding the
best underlying roots from an abbreviated string,
and the third question have a direct link to the
construction of an abbreviation dictionary
automatically from a corpus. For now we will not
explore this third question, but leave it to a
research that would be launched in the near future.
The most interesting question to ask is, of
course, the second question in the Chinese
tokenization process. Therefore, we will start with
a unified word segmentation model, which has the
capability to handle abbreviation problem jointly
with the word segmentation process.
</bodyText>
<subsectionHeader confidence="0.9973875">
2.2 HMM-Q2: Unified Word Segmentation
Model for Abbreviation Recovery
</subsectionHeader>
<bodyText confidence="0.998499">
To integrate the abbreviation process into the word
segmentation model, firstly we can regard the
segmentation model as finding the best underlying
</bodyText>
<equation confidence="0.9499518">
words w1m ≡ w1, •, w (which include only
m
base/root forms), given the surface string of
characters c1n ≡ c1 , , c (which may contain
n
</equation>
<bodyText confidence="0.998463">
abbreviated forms of compound words.) The
segmentation process is then equivalent to finding
the best word sequence w *
</bodyText>
<equation confidence="0.975622928571429">
� such that:
P 4vm |cn
1 1
P2n  |wm HP w )
1 1 1
w w
m m
1 1
:
PA  |wi 113P w4  |wi−1 )
w w
m m
1 1
:
</equation>
<bodyText confidence="0.9639431875">
Equation 1. Unified Word Segmentation
Model for Abbreviation Recovery
where c�i refers to the surface form of wi, which
could be in an abbreviated or non-abbreviated (or
any transformed) form of wi . The last equality
assumes that the generation of an abbreviation is
independent of context, and the language model is
a word-based bigram model. Such assumptions
can be adapted to different submodels for word
segmentation (Chiang 1992) as appropriate.
Furthermore, in many cases, the underlying word
wi will be a compound word consisting of other
constituent words wij (e.g., “aA )QATRA”). And,
the probability P( c i  |wi )
� is not always 1 or 0,
since the constituents may be abbreviated
</bodyText>
<equation confidence="0.987132476190476">
w�*
max
= arg
⇒cn1
w w
m m
1 1
:
max
= arg
⇒ c n 1
max
= arg
⇒cn1
∏
i
=1
,m
wi
&apos;
⇒c i
</equation>
<bodyText confidence="0.9999615">
differently in different context, making the
mapping of the compound ambiguous. For
instance, some people may prefer to abbreviate ‘_T_
������’ (Industrial Technology Research
Institute; ITRI) into ‘z4ffr’ (IRI) while other
may prefer an abbreviation of ‘���’ (ITI).
Notice that, this equation is equivalent to the
formulation for an HMM (Hidden Markov Model)
(Rabiner 1993) to find the best “state” sequence
given the observation symbols. The parameters
</bodyText>
<equation confidence="0.9404555">
P (wi  |wi−1 ) and P ( c i  |w i )
�
</equation>
<bodyText confidence="0.936957666666667">
represent the
transition probability and the (word-wise) output
probability of an HMM, respectively; and, the
</bodyText>
<equation confidence="0.839505666666667">
formulations for ( )
P w1 and ( )
m P c 1 n  |w1 m are
</equation>
<bodyText confidence="0.999576">
the respective “language model” of the Chinese
language and the “generation model” for the
abbreviated words (i.e., the “abbreviation model”
in the current context). The “state” sequence in the
word segmentation case is characterized by the
root forms w1m ≡ w1 , � , w , or the hidden words;
</bodyText>
<equation confidence="0.593845">
m
and, the “observation symbols” are characterized
here by c; ≡ c1 , • • • , cn ≡ J1, • • • , cm , where the
surface form ci ≡ cb i
e ( i ) is a chunk of characters
( )
</equation>
<bodyText confidence="0.999651352941176">
beginning at the b(i)-th character and ending at the
e(i)-th character.
Such an analogy with an HMM enables us to
estimate the model parameters using an
unsupervised training method that is directly
ported from the forward-backward or
Baum-Welch re-estimation formula (Rabiner 1993)
or a generic EM algorithm (Dempster 1977).
Note also that, while the above formulation is
intended for finding root words in a sentence, with
the help of contextual words, we can also apply the
same formulation to a single abbreviated word
(likely to have a compound word as its root in
many cases) to find the most likely constituent
words, without the help of surrounding words, but
with the help of contextual constraints among its
constituents.
</bodyText>
<subsectionHeader confidence="0.583518">
2.2.1. Language Model
</subsectionHeader>
<bodyText confidence="0.999597666666667">
The word transition probability P (wi  |wi −1 )
used in the language model is used to provide
contextual constraints among root words. It may
not be reliably estimated when the language has a
large vocabulary and when the training corpus is
small. To resolve this problem, we can back-off
the bigram word transition probability to a
unigram word probability using Katz’s method
(Katz 1987) for rare bigrams. We can, of course,
use other smoothing methods to acquire reliable
parameters. The smoothing issues, however, are
not the main focus of this preliminary study.
</bodyText>
<subsubsectionHeader confidence="0.964143">
2.2.2 Generation Model for Abbreviations
</subsubsectionHeader>
<bodyText confidence="0.9570083">
In the perfect case where all words are
lexicalized, rendering all surface forms identical to
their “root” forms and all words are known to the
�
system dictionary, we will have P ( c i  |wi )= 1,
∀i = 1, m, and Equation 1 is no more than a word
bigram model for word segmentation (Chiang
1992). In the presence of unknown words (e.g.,
abbreviations being one of such entities), however,
we can no longer ignore the generation probability
</bodyText>
<equation confidence="0.685387">
PCSi  |wi E
</equation>
<bodyText confidence="0.96964225">
For example, if c�i is ‘p~’ then wi could
be the compound word ‘p� �A’ (Taiwan
University) or ‘p� kiff-9’ (Taiwan Major
League). In this case, the parameters in P(,kA|p
</bodyText>
<equation confidence="0.9961025">
4) x P(p|p4) x P(,k|k^) and P(,k |p4)
x P(p|4,4) x P(�. |-9) will indicate how
</equation>
<bodyText confidence="0.986486833333333">
likely ‘p~’ is an abbreviation, and which of the
above two compounds is the root form of the
abbreviation. Therefore, we need a method for
estimating the probabilities between the
abbreviations and their root forms (many of which
are compound words with other constituents).
</bodyText>
<subsectionHeader confidence="0.999657">
2.3 Applying Abbreviation Models
</subsectionHeader>
<bodyText confidence="0.8890710625">
There are two problems to use the unified model
which takes abbreviated words into account. First
of all, since the word lattice is constructed from all
possible w1m ≡ w1 , �, w , how can we construct it
m
without really knowing the candidate base forms
of c�i in advance? We don’t really want to
randomly combine all possible root forms, which
is not affordable in computational cost. Therefore,
we have to make some smarter choices. Second,
how to compute the abbreviation
(output/generation) probability P ( c i  |w i )
�
once
the lattice is constructed with candidate root
words?
</bodyText>
<subsectionHeader confidence="0.705494">
2.3.1 Candidate Root Word Generation
</subsectionHeader>
<bodyText confidence="0.994952869565218">
The first problem can be resolved if we choose
some highly probable constituents w that would
generate each individual characters cij in c�i
independently, and allow such Top-N candidates
to form part of the complete word lattice. That is,
for each individual character cij , we choose its
Top-N candidates according to: P (cij  |w i)P (w .)
The probability P (cij  |w )here represents the
character-wise generation probability of a single
character from its corresponding root word. Notice
that, after we apply the word segmentation model
Equation 1 to the word lattice, some of the above
candidates may be preferred and others be
discarded, by consulting the neighboring words
and their transition probabilities. This makes the
abbreviation model jointly optimized in the word
segmentation process, instead of being optimized
independent of context.
abbreviation model is to introduce the length and
the bit pattern for abbreviation operations as
additional features into the abbreviation model. If
this is the case, we will have the following
augmented abbreviation model.
</bodyText>
<equation confidence="0.950682">
P I  |w C11 P(c1m , bit, m  |r1n, n)
≡ P c r P bit n P m n
(  |) (  |) (  |)
m n × ×
1 1
</equation>
<table confidence="0.944909857142857">
�
�
���
where m: length of surface characters.
� n: length of root word character
�
s.
�� bit: bit pattern of abbreviation
: surface characters.
: root word characters.
m
c1
n
r 1
</table>
<subsubsectionHeader confidence="0.693834">
2.3.2 Abbreviation Probability
</subsubsectionHeader>
<bodyText confidence="0.998279">
The second problem can be resolved using the
following equation if wi can be segmented into
</bodyText>
<equation confidence="0.988570263157895">
wi ≡ wi1 , �, wiL , each constituent corresponding
�
to a character in c i ≡ cb i
e ( i )
( ) :
P [1�i|wi [fl] P [ b(i|wil )
P ( c  |w P w w
) (
⋅  |)
b i j
( ) 1
+ − ij ij i j
, − 1
j L
= 1,
( ) ( )
i b i
− + ≡
1 L i( )
</equation>
<figureCaption confidence="0.474809">
Equation 2. Abbreviation Probability.
</figureCaption>
<bodyText confidence="0.999792333333333">
In other words, we use the transition
probability between constituent words and the
character-wise generation probabilities of
individual characters from a constituent word to
estimate the global generation probability of the
abbreviated form.
</bodyText>
<subsectionHeader confidence="0.580903">
2.3.3 Simplified Abbreviation Models
</subsectionHeader>
<bodyText confidence="0.99998925">
It is sometimes simply not efficient to save all
pairs of root compounds and their respective
abbreviations in an abbreviation dictionary.
Therefore, it is desirable to simplify the
abbreviation probability by using some simpler
features for Chinese abbreviation words. For
instance, it is known that many 4-character
compound words will be abbreviated as
2-character abbreviations (such as the case for the
&lt;aA)CAT, a)C&gt; pair.) It was also known
heuristically that many such 4-character words are
abbreviated by reserving the first and the third
characters, which can be represented by a ‘1010’
bit pattern, where a ‘1’ means to reserve the
respective character and a ‘0’ means to delete it.
Therefore, a reasonable simplification for the
</bodyText>
<equation confidence="0.5087905">
notation of Pr
w
</equation>
<bodyText confidence="0.959226666666667">
which could means a pair
of &lt;abbreviation, root&gt; or be evaluated as the
product of the per-character generation
probabilities and the sub-constituent transition
probabilities as outlined in Equation 2. This term
can of course be ignored from the above
augmented abbreviation model so that only very
simple length and position features are used for
abbreviation han
</bodyText>
<equation confidence="0.7946084">
c 1 m  |r1n plays the same role as the older
(
(c�|
, )
dling.
</equation>
<sectionHeader confidence="0.557613666666667" genericHeader="method">
3 Data and Parameter Estimation
Equation 3. Abbreviation Probability using
Abbreviation Pattern an
</sectionHeader>
<subsectionHeader confidence="0.399904">
d Length Features.
</subsectionHeader>
<bodyText confidence="0.993977733333333">
All these three terms can be combined freely to
produce as many as 7 sub-models for the
abbreviation model. Note, the first term
Pr
An abbreviation dictionary containing the
word-abbreviation pairs is required to test the
proposed models. Unfortunately, a large Chinese
abbreviation dictionary is not available. Therefore,
we have to collect some of the generic
abbreviations, and make others manually from
some named entity lists. Almost half of our
collection comes from the Ministry of Education
of the ROC.
(In a
future plan, a large abbreviation dictionary will be
built automatically by using the proposed models.)
Eventually, we got 1547 root-abbreviation pairs.
Among them, 1235 pairs are considered simple
and 312 pairs are
in the sense that they
violate some model assumptions. For instance, we
required that a root in a compound word be
mapped to at least one character in its abbreviation
(not to a null string), and we also assume that the
word cannot be mapped to a character that is not
part of the word. (For example, AB can be
abbreviated as A or B but not C.) Some tough
words will actually map substrings to null stri
(http://www.edu.tw/clc/dict/).
“tough”
</bodyText>
<equation confidence="0.90249675">
ngs;
= ∏
L e
=
</equation>
<bodyText confidence="0.991088365853659">
others may be recursively abbreviated; and yet
others may change the word order (as in
abbreviating “Z—OMURO” as “~~~”
instead of “—~1”.). As a result, the tough pairs
will not be handled correctly with current models.
To simplify the task, only the 1235 simple pairs
are tested for evaluation. They are further divided
randomly into a training set of 986 pairs (80%)
and a test set of 249 pairs (20%). Since the corpus
size is not large, the compound words are also
manually segmented into their constituents in
order to know the true alignments between each
character of the abbreviation with its root form in
the compound word. Admittedly, such an
extremely small training set causes serious data
sparseness problem during training. Therefore, the
evaluated performance in this preliminary report
will be highly underestimated.
The parameters are estimated in the
unsupervised mode using a standard EM algorithm
or the re-estimation method as conventional HMM
models would do (Rabiner 1993). In addition, the
manually segmented dictionary also allows us to
estimate the model parameters in the supervised
mode.
The unsupervised training will automatically
align each character in the abbreviations to its root
form in the full words. It is observed that 65.5% of
the training set dictionary pairs will be aligned
correctly. Other pairs are aligned partially correct.
Note that parameters P(m  |n Eand P(bit  |n o
can be estimated using maximum likelihood
estimation by directly consulting the abbreviation
dictionary since they are only related to word
length and character position. It is interesting, in
the first place, to check these types of parameters
quantitatively to see if they reveal some
abbreviation heuristics recognized by native
Chinese speakers. The high frequency patterns,
which are much more frequent than the ones
ranked in lower places, are listed in Table 1 and
</bodyText>
<tableCaption confidence="0.976455">
Table 2.
</tableCaption>
<table confidence="0.997975">
P(m|n) Score Examples
P(1|2) 1.00 (~|W~), (~|~f)
P(2|3) 0.67 (��|���), (~ |~
)
P(2|4) 0.95 (WE|R�ffI E),
(In)IQ|1rA )IQIi)
P(3|5) 0.73 (LpfffR|LP5�fRfRR),
(�z*|WUtz Tq*)
P(3|6) 0.70 ( Z, ZfgIi,*,,),
(9f4)IQ|9 f4!�Ii)
P(3|7) 0.76 (&amp;quot;#$|&amp;quot;%#&amp;&apos;7,$),
(rPf4R|rP(f4WRfR�)
</table>
<tableCaption confidence="0.9662705">
Table 1. High Frequency Abbreviation
Patterns [by lengths]
</tableCaption>
<table confidence="0.999938222222222">
P(bit|n) Score Examples
P(10|2) 0.87 ()|)*),(+|+*)
P(101|3) 0.44 (, |,-�),
(./|.0/)
P(1010|4) 0.56 (1~|12~3),
(4~|45~~)
P(10101|5) 0.66 (678|697:8),
(;&lt;=|;&gt;&lt;?=)
P(101001|6) 0.51 (@_T @IiZfgIi,*r.),
(=,*,,|Z71_T_���)
P(1010001|7) 0.55 (*�A|*B��CD
A),
(���|�(����
tic)
P(10101010|8) 0.21 (Er-PEF|EGr-P*E
GFH),
(E�EI|EG�JE
GIK)
</table>
<tableCaption confidence="0.968469">
Table 2. High Frequency Abbreviation Patterns
[by P(bit|n)]
</tableCaption>
<bodyText confidence="0.9900685">
Table 1 shows how word lengths will change
during the abbreviation process, and Table 2
shows which characters will be deleted from the
root of a particular length. The tables
quantitatively support some general heuristics for
native Chinese speaker. For instance, most words
will be abbreviated by deleting about half the
characters in the words, as shown in Table 1. The
data also shows that the first character in a
two-character word will be retained in most cases,
and the first and the third characters in a
4-character word will be retained in 56% of the
cases. However, the tables also shows that around
50% of the cases cannot be uniquely determined
simply by consulting the word length for its
abbreviated form. This does suggest the necessity
of an abbreviation model for resolving this kind of
unknown words and named entities.
</bodyText>
<sectionHeader confidence="0.991856" genericHeader="evaluation">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.9999545">
The unified model can be applied to a whole
sentence which contains abbreviations during
word segmentation. When the input is reduced to a
single abbreviated word (or compound), it can also
be applied to recover the underlying root
constituent words (without consulting contextual
words). In this paper, we will only focus on the
abbreviation word recovery problems.
Two major experiments are conducted. The
first experiment is to guess the most likely
abbreviation form for a word using various feature
combinations; the second is to guess the root word
from an abbreviation. The following sections will
give more details.
</bodyText>
<subsectionHeader confidence="0.999771">
4.1 Guessing Abbreviations from Roots
</subsectionHeader>
<bodyText confidence="0.999981333333333">
The main task of this experiment is to guess the
most probable abbreviation forms for the
unabbreviated words in a word list. The
abbreviation forms of a word can be enumerated
by arbitrarily retaining some characters of this root
word and deleting others. For example, the word
“MR,09” has six possible abbreviated forms: “fel”,
“W”, “�”, “��”, “��” and “��”. In general,
if we have a root word of length L, there could be
</bodyText>
<equation confidence="0.480694">
2 −2
</equation>
<bodyText confidence="0.935855714285714">
L possible abbreviations for this root word
(excluding the word itself and the null string).
The best possible abbreviation form *
c� for an
input word wi can be determined as the one with
�
the highest generation probability P ( c i  |wi ), i.e.,
</bodyText>
<equation confidence="0.903552">
c� = *arg max P [1�i  |w ). The generation
ai
</equation>
<bodyText confidence="0.9999114">
probability for a candidate c�i , in turn, can be
estimated by summing up all probabilities of
alignments between each character cij in c�i
and the suspect constituent words wij in wi . In
other words, we have
</bodyText>
<equation confidence="0.974772">
P (ei |wi [D]∑P CA |wi )
A ∈ all alignments
PA [�i |wi )
</equation>
<bodyText confidence="0.999701432432432">
where PA (c i  |wi )
� is the generation probability for a
known alignment A, which can be estimated as in
Equation 2.
For simplicity, we assume that each character
in c�i will be mapped to a substring wij in wi.
In other words, we assume that the mapping
between the constituents is 1-1, and no 1-0 or 0-1
mapping is possible. (In future works, such a
constraint could be removed.) Also, we will
assume that wij should at least contain the
character that is aligned to it. (This is not always
true for Chinese abbreviations. For example, “�
�” can be abbreviated with its ancient location
name “r4”, which does not appear anywhere in its
root.)
There is also a normalization issue in
computing the probability of a particular alignment.
In general, a shorter string may be preferred as the
best abbreviation simply because it multiplies less
probability factors when estimating the alignment
probability. To reduce this effect, we intentionally
scale down, by a normalization factor, the
generation probabilities for those alignments that
map a complete word into a single character. In
fact, there are only about 10% of such alignments,
and many of which are mapping a two-character
word into a single character (which can be
compensated by the large Pr(1|2) factor in the
model. This simple normalization approach
actually improves the test set performance greatly.
The following table shows the test set
performance for using different features in the
abbreviation probability as given in Equation 3.
(The training set performance ranges from 94% to
98%, which suggests a good fit to the training
data.)
</bodyText>
<table confidence="0.998497666666667">
Feature Unsupervised Supervised
P(c|w)xP(wi|wi-1) 1 1 1 1 1 1 1 1
P(bit|n) 1 1 0 0 1 1 0 0
P(m|n) 1 0 1 0 1 0 1 0
Accuracy 68 68 61 60 72 70 61 58
Rate(%)
</table>
<tableCaption confidence="0.954418">
Table 3. Test Set Performance for
</tableCaption>
<bodyText confidence="0.9793444">
Abbreviation Generation with Combined
Features.
Each column shows the test set performance for
a submodel, which is identified by the features
used for estimating the probability. The label ‘1’
(or ‘0’) indicates that the feature at the first
column is used (or unused) in the submodel. For
instance, the submodel of the second column
(‘111’) uses all the features, including the word
transition probability, word-to-abbreviation
probability, probability for mapping n character
word to a particular abbreviation bit pattern
P(bit|n), and the probability for mapping
n-character words into m-character abbreviations.
It is seen that supervised training acquires a
little better performance than its unsupervised (EM)
counterpart. Although not shown in this table, it is
observed that the word transition probability and
word-to-abbreviation probability in general should
be used to get better performance. The table also
shows that the other two features based on
character positions and word lengths provide
additional help. In particular, P(bit|n) seems to be
more helpful than P(m|n) since it contains detailed
information for retaining characters at particular
positions.
The best performance is about 72% when
supervised training is used and all the three types
of features are used for estimating the abbreviation
probability.
</bodyText>
<figure confidence="0.563685333333333">
≡
∑
A
</figure>
<subsectionHeader confidence="0.996719">
4.2 Guessing Roots from Abbreviations
</subsectionHeader>
<bodyText confidence="0.9999627">
In this experiment, we are given an
abbreviation list; the goal is to guess the best root
words of the abbreviations in the list. The
parameters used here are acquired from human
tagged alignments in a supervised manner.
To find the best root candidates of an
abbreviated compound word, we need to find the
candidate root words for each input character first.
The candidate root words can be found from the
training set whose generation probability
</bodyText>
<equation confidence="0.830055">
P( c i  |wi )
</equation>
<bodyText confidence="0.987917777777778">
� is non-zero. The Top-N candidates can
then be picked up as described earlier.
For instance, if we want to find the root words
of the abbreviation “��”, and the probabilities
P(A|A{M) and P(-I�|-IM) are non-zero, then we
have the chance to recover the abbreviation “A
-&amp;” back to the correct compound word “AAM�
A” , which consists of the candidate root words
“AA~” and “-IM” for the input characters “~”
and “-&amp;” respectively.
Unfortunately, the limited abbreviation
dictionary we have is highly sparse. Among the
249 abbreviations in the test set, only 144 (58%) of
them have their candidate root words available in
the training set. The other 105 abbreviations (42%)
cannot be recovered since each of them has at least
one character whose candidate cannot be
discovered from the training set. For this reason,
we will limit ourselves to the performance of the
“trainable” test set consisting of the 144
abbreviations, in order to factor out the sparseness
problem pertaining to the training corpus.
Under such a restricted environment, we have
tested various submodels to see how different
language models and simple smoothing affect the
results of this error recovery process. The results
are summarized in the following table:
</bodyText>
<table confidence="0.999467875">
LM SM? Top-N TR(%) TS(%) Best?
bigram No all 90.9 35
2 69.6 43 2
1 46.0 45 1
unigram No all 44.2 44
bigram Yes all 90.7 51 1
2 69.6 51 1
1 46.0 45
</table>
<tableCaption confidence="0.999244">
Table 4. Abbreviation Recovery Performance.
</tableCaption>
<reference confidence="0.4650035">
Notations: LM: Language Model, SM?: Apply
Smoothing?, Top-N: maximum number of Top-N
candidate root words for each character, TR:
Training Set Accuracy Rate, TS: Test Set
Accuracy, Best?: Best TS Performance among all
N’s? (1 = yes, 2= rank 2)
</reference>
<bodyText confidence="0.994261375">
The bigram language model uses P(wi  |wi −1 )
in the unified HMM model while the unigram
model uses P(wi )instead. Both of them use
maximum likelihood estimation over the manually
tagged abbreviation-root pairs when smoothing is
not applied. When smoothing is applied, the
smoothed bigram probability is acquired by
linearly interpolating the unigram and bigram
probabilities with an equal weight (0.5). The above
table indicates that using the less complicated
unigram model generally improve the test set
performance significantly (from 35% to 44%). If
the model parameters are smoothed, the
improvement is even greater. Such results can be
well expected in the current environment where
the training data is very sparse.
Overall, the best test set performance is about
51% when using a smoothed bigram language
model; and this can be achieved by using at most 2
Top-N candidate root words while constructing the
underlying word lattice. This suggests that we
don’t really need to wildly enumerate all possible
candidate root words for each input character with
this model.
</bodyText>
<sectionHeader confidence="0.982177" genericHeader="conclusions">
5. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999950606060606">
Chinese abbreviations, a special form of
unknown words and named entities, are widely
seen in the modern Chinese texts. This results in
difficulty for correct Chinese processing. In this
preliminary study, the Chinese abbreviation
problem is modeled as an error recovery problem
in which the suspect root words are to be
recovered from a set of candidates. An
HMM-based model is thus used for Chinese in
either abbreviation identification, or in the
recovery of the root words from an abbreviation.
By extending a simple abbreviation string into a
whole text involving abbreviations, it can also be
applied to the Chinese word segmentation for
identifying abbreviations in a text, or for
bootstrapping an abbreviation dictionary from a
text corpus.
With the proposed model, the abbreviated
forms can be guessed from root words at about
72% correction. The recovery of the root words
from abbreviations is conducted at about 51%
accuracy rate. Although further improvement is
possible, the preliminary results are encouraging.
In the near future, bootstrapping a large
abbreviation dictionary from web text by applying
the proposed models is planned. This should
partially resolve the data sparseness problems.
Such models will also be integrated into a Chinese
word segmentation model to partially resolve the
unknown word and named entity identification
problems in the tokenization process. It is expected
that more applications will rely on such models for
Chinese processing.
</bodyText>
<sectionHeader confidence="0.994282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786450980393">
Chang, Jing-Shin and Keh-Yih Su, 1997. “An
Unsupervised Iterative Method for Chinese
New Lexicon Extraction”, International Journal
of Computational Linguistics and Chinese
Language Processing (CLCLP), 2(2): 97-148.
Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin
and Keh-Yih Su, 1992. “Statistical Models for
Word Segmentation and Unknown Word
Resolution,” Proceedings of ROCLING-V,
pages 123-146, Taipei, Taiwan, ROC.
Dempster, A. P., N. M. Laird, and D. B. Rubin,
1977. “Maximum Likelihood from Incomplete
Data via the EM Algorithm”, Journal of the
Royal Statistical Society, 39 (b): 1-38.
Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003.
“Improved Source-Channel Models for Chinese
Word Segmentation,” Proc. ACL 2003, pages
272-279.
Huang, Chu-Ren, Kathleen Ahrens, and Keh-Jiann
Chen, 1994a. “A data-driven approach to
psychological reality of the mental lexicon: Two
studies on Chinese corpus linguistics.” In
Language and its Psychobiological Bases,
Taipei.
Huang, Chu-Ren, Wei-Mei Hong, and Keh-Jiann
Chen, 1994b. “Suoxie: An information based
lexical rule of abbreviation.” In Proceedings of
the Second Pacific Asia Conference on Formal
and Computational Linguistics II, pages 49–52,
Japan.
Katz, Slava M., 1987. “Estimation of Probabilities
from Sparse Data for the Language Model
Component of a Speech Recognizer,” IEEE
Trans. ASSP-35 (3).
Lai, Yu-Tso, 2003. A Probabilistic Model for
Chinese Abbreviations, Master Thesis, National
Chi-Nan University, ROC.
Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su,
1993. “A Preliminary Study on Unknown Word
Problem in Chinese Word Segmentation,”
Proceedings of ROCLING VI, pages 119-142.
Rabiner, L., and B.-H., Juang, 1993. Fundamentals
of Speech Recognition, Prentice-Hall.
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou
and Chang-Ning Huang, 2002. “Chinese named
entity identification using class-based language
model,” Proc. of COLING 2002, Taipei, ROC.
Sproat, Richard, 2002. “Corpus-Based Methods in
Chinese Morphology”, Pre-conference
Tutorials, COLING-2002, Taipei, Taiwan,
ROC.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579817">
<title confidence="0.999567">A Preliminary Study on Probabilistic Models for Chinese Abbreviations</title>
<author confidence="0.972865">Jing-Shin</author>
<affiliation confidence="0.996178333333333">Department of Computer Science Information National Chi-Nan</affiliation>
<address confidence="0.985191">Puli, Nantou, Taiwan, ROC.</address>
<email confidence="0.974805">jshin@csie.ncnu.edu.tw</email>
<author confidence="0.77586">Yu-Tso</author>
<affiliation confidence="0.990129">Department of Computer Science Information National Chi-Nan</affiliation>
<address confidence="0.90659">Puli, Nantou, Taiwan, ROC.</address>
<email confidence="0.87043">s0321521@ncnu.edu.tw</email>
<abstract confidence="0.999514137931034">Chinese abbreviations are widely used in the modern Chinese texts. They are a form of including This results in difficulty for correct Chinese processing. In this study, the Chinese abbreviation is regarded as an recovery problem in which the suspect root words are the “errors” to be recovered from a set of candidates. Such a problem is mapped to an HMM-based generation model for both abbreviation identification and root word recovery, and is integrated as part of a unified word segmentation model when the input extends to a complete sentence. Two major experiments are conducted to test the abbreviation models. In the first experiment, an attempt is made to guess the abbreviations of the root words. An accuracy rate of 72% is observed. In contrast, a second experiment is conducted to guess the root words from abbreviations. Some submodels could achieve as high as 51% accuracy with the simple HMM-based model. Some quantitative observations against heuristic abbreviation knowledge about Chinese are also observed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Notations LM Language Model</author>
</authors>
<title>SM?: Apply Smoothing?, Top-N: maximum number of Top-N candidate root words for each character, TR: Training Set Accuracy Rate, TS: Test Set Accuracy,</title>
<booktitle>TS Performance among all N’s? (1 = yes,</booktitle>
<volume>2</volume>
<location>Best?: Best</location>
<marker>Model, </marker>
<rawString>Notations: LM: Language Model, SM?: Apply Smoothing?, Top-N: maximum number of Top-N candidate root words for each character, TR: Training Set Accuracy Rate, TS: Test Set Accuracy, Best?: Best TS Performance among all N’s? (1 = yes, 2= rank 2)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing-Shin Chang</author>
<author>Keh-Yih Su</author>
</authors>
<title>An Unsupervised Iterative Method for Chinese New Lexicon Extraction”,</title>
<date>1997</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing (CLCLP),</journal>
<volume>2</volume>
<issue>2</issue>
<pages>97--148</pages>
<marker>Chang, Su, 1997</marker>
<rawString>Chang, Jing-Shin and Keh-Yih Su, 1997. “An Unsupervised Iterative Method for Chinese New Lexicon Extraction”, International Journal of Computational Linguistics and Chinese Language Processing (CLCLP), 2(2): 97-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tung-Hui Chiang</author>
<author>Jing-Shin Chang</author>
<author>Ming-Yu Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Statistical Models for Word Segmentation and Unknown Word Resolution,”</title>
<date>1992</date>
<booktitle>Proceedings of ROCLING-V,</booktitle>
<pages>123--146</pages>
<location>Taipei, Taiwan, ROC.</location>
<marker>Chiang, Chang, Lin, Su, 1992</marker>
<rawString>Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin and Keh-Yih Su, 1992. “Statistical Models for Word Segmentation and Unknown Word Resolution,” Proceedings of ROCLING-V, pages 123-146, Taipei, Taiwan, ROC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm”,</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P., N. M. Laird, and D. B. Rubin, 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm”, Journal of the Royal Statistical Society, 39 (b): 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
</authors>
<title>Chang-Ning Huang,</title>
<date>2003</date>
<booktitle>Proc. ACL</booktitle>
<pages>272--279</pages>
<marker>Gao, Li, 2003</marker>
<rawString>Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003. “Improved Source-Channel Models for Chinese Word Segmentation,” Proc. ACL 2003, pages 272-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Kathleen Ahrens</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>A data-driven approach to psychological reality of the mental lexicon: Two studies on Chinese corpus linguistics.” In Language and its Psychobiological Bases,</title>
<date>1994</date>
<location>Taipei.</location>
<marker>Huang, Ahrens, Chen, 1994</marker>
<rawString>Huang, Chu-Ren, Kathleen Ahrens, and Keh-Jiann Chen, 1994a. “A data-driven approach to psychological reality of the mental lexicon: Two studies on Chinese corpus linguistics.” In Language and its Psychobiological Bases, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Wei-Mei Hong</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>Suoxie: An information based lexical rule of abbreviation.”</title>
<date>1994</date>
<booktitle>In Proceedings of the Second Pacific Asia Conference on Formal and Computational Linguistics II,</booktitle>
<pages>49--52</pages>
<marker>Huang, Hong, Chen, 1994</marker>
<rawString>Huang, Chu-Ren, Wei-Mei Hong, and Keh-Jiann Chen, 1994b. “Suoxie: An information based lexical rule of abbreviation.” In Proceedings of the Second Pacific Asia Conference on Formal and Computational Linguistics II, pages 49–52, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer,”</title>
<date>1987</date>
<journal>IEEE Trans.</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="10811" citStr="Katz 1987" startWordPosition="1757" endWordPosition="1758">o have a compound word as its root in many cases) to find the most likely constituent words, without the help of surrounding words, but with the help of contextual constraints among its constituents. 2.2.1. Language Model The word transition probability P (wi |wi −1 ) used in the language model is used to provide contextual constraints among root words. It may not be reliably estimated when the language has a large vocabulary and when the training corpus is small. To resolve this problem, we can back-off the bigram word transition probability to a unigram word probability using Katz’s method (Katz 1987) for rare bigrams. We can, of course, use other smoothing methods to acquire reliable parameters. The smoothing issues, however, are not the main focus of this preliminary study. 2.2.2 Generation Model for Abbreviations In the perfect case where all words are lexicalized, rendering all surface forms identical to their “root” forms and all words are known to the � system dictionary, we will have P ( c i |wi )= 1, ∀i = 1, m, and Equation 1 is no more than a word bigram model for word segmentation (Chiang 1992). In the presence of unknown words (e.g., abbreviations being one of such entities), ho</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M., 1987. “Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer,” IEEE Trans. ASSP-35 (3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Tso Lai</author>
</authors>
<title>A Probabilistic Model for Chinese Abbreviations, Master Thesis,</title>
<date>2003</date>
<institution>National Chi-Nan University, ROC.</institution>
<contexts>
<context position="1858" citStr="Lai 2003" startWordPosition="276" endWordPosition="277">rom abbreviations. Some submodels could achieve as high as 51% accuracy with the simple HMM-based model. Some quantitative observations against heuristic abbreviation knowledge about Chinese are also observed. 1 Introduction The modern Chinese language is a highly abbreviated one due to the mixed uses of ancient single character words as well as modern multi-character words and compound words. The abbreviated form and root form are used interchangeably everywhere in the current Chinese articles. Some news articles may contain about 20% of sentences that have suspect abbreviated words in them (Lai 2003). Since abbreviations cannot be enumerated in a dictionary, it forms a special class of unknown words, many of which originate from named entities. Many other open class words are also abbreviatable. This particular class thus introduces complication for Chinese language processing, including the fundamental word segmentation process (Chiang 1992, Lin 1993, Chang 1997) and many word-based applications. For instance, a keyword-based information retrieval system may requires the two forms, such as “ A -1� ” and “ A &amp; _1� A ” (“legislators”), in order not to miss any relevant documents. The Chine</context>
</contexts>
<marker>Lai, 2003</marker>
<rawString>Lai, Yu-Tso, 2003. A Probabilistic Model for Chinese Abbreviations, Master Thesis, National Chi-Nan University, ROC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Yu Lin</author>
<author>Tung-Hui Chiang</author>
<author>Keh-Yih Su</author>
</authors>
<date>1993</date>
<booktitle>A Preliminary Study on Unknown Word Problem in Chinese Word Segmentation,” Proceedings of ROCLING VI,</booktitle>
<pages>119--142</pages>
<marker>Lin, Chiang, Su, 1993</marker>
<rawString>Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su, 1993. “A Preliminary Study on Unknown Word Problem in Chinese Word Segmentation,” Proceedings of ROCLING VI, pages 119-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
<author>B-H Juang</author>
</authors>
<title>Fundamentals of Speech Recognition,</title>
<date>1993</date>
<publisher>Prentice-Hall.</publisher>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Rabiner, L., and B.-H., Juang, 1993. Fundamentals of Speech Recognition, Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Sun</author>
<author>Jianfeng Gao</author>
<author>Lei Zhang</author>
</authors>
<title>Ming Zhou and Chang-Ning Huang,</title>
<date>2002</date>
<booktitle>Proc. of COLING 2002,</booktitle>
<location>Taipei, ROC.</location>
<marker>Sun, Gao, Zhang, 2002</marker>
<rawString>Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou and Chang-Ning Huang, 2002. “Chinese named entity identification using class-based language model,” Proc. of COLING 2002, Taipei, ROC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<date>2002</date>
<booktitle>Corpus-Based Methods in Chinese Morphology”, Pre-conference Tutorials, COLING-2002,</booktitle>
<location>Taipei, Taiwan, ROC.</location>
<contexts>
<context position="2709" citStr="Sproat 2002" startWordPosition="402" endWordPosition="403">omplication for Chinese language processing, including the fundamental word segmentation process (Chiang 1992, Lin 1993, Chang 1997) and many word-based applications. For instance, a keyword-based information retrieval system may requires the two forms, such as “ A -1� ” and “ A &amp; _1� A ” (“legislators”), in order not to miss any relevant documents. The Chinese word segmentation process is also significantly degraded by the existence of unknown words (Chiang 1992), including unknown abbreviations. There are many heuristics for Chinese abbreviations. Such heuristics, however, can easily break (Sproat 2002). Currently, only some quantitative approaches (Huang 1994a, 94b) are available in predicting the presentation of an abbreviation. Since such formulations regard the word segmentation process and abbreviation identification as two independent processes, they probably cannot optimize the identification process jointly with the word segmentation process, and thus may lose the useful contextual information. Some class-based segmentation models (Sun 2002, Gao 2003) well integrate the identification of some regular non-lexicalized units (such as named entities). However, the abbreviation process ca</context>
</contexts>
<marker>Sproat, 2002</marker>
<rawString>Sproat, Richard, 2002. “Corpus-Based Methods in Chinese Morphology”, Pre-conference Tutorials, COLING-2002, Taipei, Taiwan, ROC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>