<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005673">
<title confidence="0.973878">
A general scheme for broad-coverage multimodal annotation
</title>
<author confidence="0.563192">
Philippe Blache
</author>
<note confidence="0.715453666666667">
Laboratoire Parole et Langage
CNRS &amp; Aix-Marseille Universit´es
blache@lpl-aix.fr
</note>
<sectionHeader confidence="0.981346" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999578217391304">
We present in this paper a formal and
computational scheme in the perspective
of broad-coverage multimodal annotation.
We propose in particular to introduce
the notion of annotation hypergraphs in
which primary and secondary data are rep-
resented by means of the same structure.
This paper addresses the question of resources
and corpora for natural human-human interaction,
in other words broad-coverage annotation of natu-
ral data. In this kind of study, most of domains
have do be taken into consideration: prosody,
pragmatics, syntax, gestures, etc. All these dif-
ferent domains interact in order to build an un-
derstandable message. We need then large mul-
timodal annotated corpora of real data, precisely
annotated for all domains. Building this kind of
resource is a relatively new, but very active re-
search domain, illustrated by the number of work-
shops (cf. (Martin, 2008)), international initia-
tives, such as MUMIN (Allwood, 2005), anno-
tation tools such as NITE NXT (Carletta, 2003),
Anvil (Kipp, 2001), etc.
</bodyText>
<sectionHeader confidence="0.931125" genericHeader="method">
1 A characterization of primary data
</sectionHeader>
<bodyText confidence="0.985214041666667">
Different types of primary data constitute the basis
of an annotation: speech signal, video input, word
strings, images, etc. But other kinds of primary
data also can be used, for example in the perspec-
tive of semantic annotations such as concepts, ref-
erences, types, etc. Such data are considered to be
atomic in the sense that they are not built on top
of lower level data. When looking more closely at
these kinds of data, several characteristics can be
identified:
- Location: primary data is usually localized with
respect to a timeline or a position: gestures can
be localized into the video signal, phonemes into
the speech one, words into the string or objects
into a scene or a context. Two different kinds of
localisation are used: temporal and spatial. In the
first case, a data is situated by means of a time
interval whereas spatial data are localised in terms
of relative or absolute positions.
- Realization: primary data usually refer to con-
crete (or physical) objects: phonemes, gestures,
referential elements into a scene, etc. However,
other kinds of primary data can be abstract such
as concepts, ideas, emotions, etc.
</bodyText>
<listItem confidence="0.793631166666667">
- Medium: The W3C recommendation EMMA
(Extensible Multi-Modal Annotations) proposes to
distinguish different medium: acoustic, tactile and
visual. This classification is only relevant for data
corresponding to concrete objects.
- Production: the study of information structure
</listItem>
<bodyText confidence="0.966579625">
shows the necessity to take into account accessi-
bility of the objects: some data are directly acces-
sible from the signal or the discourse, they have an
existence or have already been mentioned. In this
case, they are said to be “produced”. For example,
gestures, sounds, physical objects fall in this cate-
gory. On the other hand, other kinds of data are de-
duced from the context, typically the abstract ones.
They are considered as “accessible”.
In the remaining of the paper, we propose the
following definition:
Primary data: atomic objects that cannot be de-
composed. They represent possible constituent on
top of which higher level objects can be built. Pri-
mary data does not require any interpretation to
be identified, they are of direct access.
This primary data typology is given in fig-
ure (1). It shows a repartition between concrete
vs. abstract objects. Concrete objects are usu-
ally those taken into account in corpus annotation.
As a consequence, annotation usually focuses on
speech and gestures, which narrows down the set
of data to those with a temporal localization. How-
ever, other kinds of data cannot be situated in the
</bodyText>
<page confidence="0.975958">
174
</page>
<note confidence="0.975687">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 174–177,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.9289009">
Phonemes Words Gestures Discourse referents Synsets Physical objects
Produced + + + +/- - +
Accessible - - - +/- + -
Concrete + + + +/- - +
Abstract - - - +/- - +
Temporal + + + +/- - -
Spatial - - +/- +/- - +
Acoustic + +/- - - - -
Visual - - + +/- - +
Tactile - - +/- +/- - +
</figure>
<figureCaption confidence="0.999553">
Figure 1: Primary data description
</figureCaption>
<bodyText confidence="0.991940857142857">
timeline (e.g. objects in the environment of the
scene) nor spatially (e.g. abstract data).
We need to propose a more general approach
of data indexing that has to distinguish on the
one hand between temporal and spatial localiza-
tion and on the other hand between data that can
be located and data that cannot.
</bodyText>
<sectionHeader confidence="0.817348" genericHeader="method">
2 Graph representation: nodes and
edges semantics
</sectionHeader>
<bodyText confidence="0.999955269230769">
One of the most popular linguistic annotation rep-
resentation is annotation graphs (Bird, 2001) in
which nodes are positions whereas edges bear lin-
guistic information. This representation is elabo-
rated on the basis of a temporal anchoring, even
though it is also possible to represent other kinds
of anchoring. Several generic annotation format
has been proposed on top of this representation,
such as LAF and its extension GrAF (cf. (Ide,
2007)). In these approaches, edges to their turn
can be interpreted as nodes in order to build higher
level information. One can consider the result as
an hypergraph, in which nodes can be subgraphs.
In order to explore farther this direction, we pro-
pose a more general interpretation for nodes that
are not only positions in the input: nodes are com-
plex objects that can be referred at different lev-
els of the representation, they encode all annota-
tions. In order to obtain an homogeneous repre-
sentations, the two node types used in hypergraphs
(nodes and hypernodes) share the same informa-
tion structure which relies on the following points:
- Index: using an index renders possible to repre-
sent any kind of graphs, not only trees. They give
to nodes the possibility of encoding any kind of
information.
</bodyText>
<listItem confidence="0.947162857142857">
- Domain: prosody, semantics, syntax, gesture,
pragmatics, etc. It is important to indicate as pre-
cisely as possible this information, eventually by
means of sub-domains
- Location: annotations generally have a spatial or
a temporal situation. This information is optional.
- Features: nodes have to bear specific linguistic
</listItem>
<bodyText confidence="0.661341833333333">
indications, describing its properties.
Hypernodes bear, on top of this information,
the specification of the subgraph represented by
its constituents and their relations. We propose to
add another kind of information in the hypernode
structure:
</bodyText>
<listItem confidence="0.943249333333333">
• Relations: secondary data are built on top
of primary one. They can be represented by
means of a set of properties (constituency,
</listItem>
<bodyText confidence="0.956181428571429">
linearity, coreference, etc.) implemented as
edges plus the basic characteristics of a node.
A secondary data is then graph with a label,
these two elements composing an hypernode.
The distinction between node and hypernodes
makes it possible to give a homogeneous repre-
sentation of primary and secondary data.
</bodyText>
<sectionHeader confidence="0.9895685" genericHeader="method">
3 An XML representation of annotation
hypergraphs
</sectionHeader>
<bodyText confidence="0.999934">
We propose in this section an XML encoding of
the scheme presented above.
</bodyText>
<subsectionHeader confidence="0.997358">
3.1 Atomic nodes
</subsectionHeader>
<bodyText confidence="0.9999534375">
The first example of the figure (2) illustrates the
representation of a phoneme. The node is indexed,
making its reference possible in higher level struc-
tures. Its label corresponds to the tag that would be
indicated in the annotation. Other elements com-
plete the description: the linguistic domain (speci-
fied by the attributes type and sub-type), the speci-
fication of the medium, the object localization (by
means of anchors). In this example, a phoneme
being part of the acoustic signal, the anchor is tem-
poral and use an explicit timeline reference.
The same kind of representation can be given
for transcription tokens (see node n21 in figure
(2)). The value of the node is the orthographic
form. It is potentially aligned on the signal, and
then represented with a temporal anchoring. Such
</bodyText>
<page confidence="0.996525">
175
</page>
<figureCaption confidence="0.988279">
Figure 2: XML encoding of atomic nodes
</figureCaption>
<figure confidence="0.997025653846154">
&lt;node ID=&amp;quot;n1&amp;quot; label=&amp;quot;u&amp;quot;&gt;
&lt;domain type=&amp;quot;phonetics&amp;quot; subtype=&amp;quot;phoneme&amp;quot;
medium=&amp;quot;acoustic&amp;quot;/&gt;
&lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;285&amp;quot; end=&amp;quot;312&amp;quot;/&gt;
&lt;/node&gt;
&lt;node ID=&amp;quot;n21&amp;quot; label=&amp;quot;book&amp;quot;&gt;
&lt;domain type=&amp;quot;transcription&amp;quot; subtype=&amp;quot;token&amp;quot;/&gt;
&lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;242&amp;quot; end=&amp;quot;422&amp;quot;/&gt;
&lt;/node&gt;
&lt;node ID=&amp;quot;n24&amp;quot; label=&amp;quot;N&amp;quot;&gt;
&lt;domain type=&amp;quot; morphosyntax&amp;quot; subtype=&amp;quot;word&amp;quot;/&gt;
&lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;242&amp;quot; end=&amp;quot;422&amp;quot;/&gt;
&lt;features ms=&amp;quot;ncms---&amp;quot;/&gt;
&lt;/node&gt;
&lt;node ID=&amp;quot;n3&amp;quot; label=&amp;quot;deictic&amp;quot;&gt;
&lt;domain type=&amp;quot;gestures&amp;quot; subtype=&amp;quot;hand&amp;quot;/&gt;
&lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;200&amp;quot; end=&amp;quot;422&amp;quot;/&gt;
&lt;features hand=&amp;quot;right&amp;quot; deictic type=&amp;quot;space&amp;quot;
object=&amp;quot;ref object&amp;quot;/&gt;
&lt;/node&gt;
&lt;node ID=&amp;quot;n4&amp;quot; label=&amp;quot;discourse-referent&amp;quot;&gt;
&lt;domain type=&amp;quot;semantics&amp;quot; subtype=&amp;quot;discourse universe&amp;quot;
medium=&amp;quot;visual&amp;quot;/&gt;
&lt;anchoring type=&amp;quot;spatial&amp;quot; x=&amp;quot;242&amp;quot; y=&amp;quot;422&amp;quot; z=&amp;quot;312&amp;quot;/&gt;
&lt;features isa=&amp;quot;book&amp;quot; color=&amp;quot;red&amp;quot; /&gt;
&lt;/node&gt;
</figure>
<bodyText confidence="0.999411035714286">
anchoring makes it possible to align the ortho-
graphic transcription with the phonetic one. In the
case of written texts, temporal bounds would be
replaced by the positions in the texts, which could
be interpreted as an implicit temporal anchoring.
The next example presented in node n24 illus-
trates the representation of part-of-speech nodes.
The domain in this case is morphosyntax, its sub-
type is “word”. In this case too, the anchoring is
temporal, with same bounds as the corresponding
token. In this node, a feature element is added,
bearing the morpho-syntactic description.
The atomic node described in node n3 repre-
sents another physical object: a deictic gesture. Its
domain is gesture and its subtype, as proposed for
example in the MUMIN scheme (see (Allwood,
2005)) is the part of the body. The anchoring is
also temporal and we can observe in this exam-
ple a synchronization of the gesture with the token
“book”.
The last example (node n4) presents an atomic
node describing a physical object present in the
scene (a book on a shelf of a library). It belongs to
the semantics domain as a discourse referent and is
anchored spatially by its spatial coordinates. One
can note that anchoring can be absolute (as in the
examples presented here) or relative (situating the
object with respect to other ones).
</bodyText>
<subsectionHeader confidence="0.998501">
3.2 Relations
</subsectionHeader>
<bodyText confidence="0.999581666666667">
Relations are represented in the same way as
nodes. They are of different types, such as con-
stituency, linearity, syntactic dependency, seman-
tic specification, etc. and correspond to a certain
domain. The example r1 in figure (3) illustrates a
specification relation between a noun (node
n21, described above) and its determiner (node
n20). Non-oriented binary relations also occur,
for example cooccurrency. Relations can be ex-
pressed in order to represent a set of objects. The
next example (relation r2) presents the case of
three constituents of an higher-level object (the
complete description of which being given in the
next section).
Finally, the alignment between objects is speci-
fied by two different values: strict when they have
exactly the same temporal or spatial marks; fuzzy
otherwize.
</bodyText>
<subsectionHeader confidence="0.997066">
3.3 Hypernodes
</subsectionHeader>
<bodyText confidence="0.999994533333333">
Hypernodes encode subgraphs with the possibility
of being themselves considered as nodes. Their
structure completes the atomic node with a set of
relations. Hypernodes encode different kinds of
objects such as phrases, constructions, referential
expressions, etc. The first example represents a
NP. The node is indexed, bears a tag, a domain, an
anchoring and features. The set of relations spec-
ifies two types of information. First, the NP node
has three constituents: n20 (for example a deter-
miner), n22 (for example an adjective) and n24
(the noun described in the previous section). The
alignment is said to be strict which means that the
right border of the first element and the left border
of the last one have to be the same. The resulting
structure is an hypernode describing the different
characteristics of the NP by means of features and
relations.
The second example illustrates the case of a ref-
erential expression. Let’s imagine the situation
where a person points out at a book on a shelf,
saying “The book will fall down”. In terms of in-
formation structure, the use of a definite NP is pos-
sible because the referent is accessible from the
physical context: the alignment of the NP (n50)
and the deictic gesture (n3, see previous section)
makes the coreference possible. This construc-
tion results in a discourse referent bringing to-
gether all the properties of the physical object (n3)
and that of the object described in the discourse
</bodyText>
<page confidence="0.996216">
176
</page>
<table confidence="0.966369260869565">
&lt;relation id=&amp;quot;r1&amp;quot; label=&amp;quot;specification&amp;quot;&gt; &lt;relation id=&amp;quot;r2&amp;quot; label=&amp;quot;constituency&amp;quot;&gt;
&lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;oriented rel&amp;quot;/&gt; &lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;set rel&amp;quot;/&gt;
&lt;edge from=&amp;quot;n20&amp;quot; to=&amp;quot;n24&amp;quot;&gt; &lt;node list&gt;
&lt;/relation&gt; &lt;node id=&amp;quot;n20&amp;quot;/&gt; &lt;node id=&amp;quot;n22&amp;quot;/&gt; &lt;node id=&amp;quot;n24&amp;quot;/&gt;
&lt;/node list&gt;
&lt;alignment type=&amp;quot;strict&amp;quot;/&gt;
&lt;/relation&gt;
Figure 3: XML encoding of relations
&lt;node ID=&amp;quot;n50&amp;quot; label=&amp;quot;NP&amp;quot;&gt; &lt;node ID=&amp;quot;n51&amp;quot; label=&amp;quot;ref expression&amp;quot;&gt;
&lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;phrase&amp;quot;/&gt; &lt;domain type=&amp;quot;semantics&amp;quot; subtype=&amp;quot;discourse referent&amp;quot;/&gt;
&lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;200&amp;quot; end=&amp;quot;422&amp;quot;/&gt; &lt;features referent=&amp;quot;book’&amp;quot; color=&amp;quot;red&amp;quot; /&gt;
&lt;features cat=&amp;quot;NP&amp;quot; agr=&amp;quot;ms&amp;quot; sem type=&amp;quot;ref&amp;quot;/&gt; &lt;relations&gt;
&lt;relations&gt; &lt;relation id=&amp;quot;r3&amp;quot; type=&amp;quot;constituency&amp;quot;&gt;
&lt;relation id=&amp;quot;r1&amp;quot; type=&amp;quot;constituency&amp;quot;&gt; &lt;domain type=&amp;quot;semantics&amp;quot; type=&amp;quot;set rel&amp;quot;/&gt;
&lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;set rel&amp;quot;/&gt; &lt;node list&gt;
&lt;node list&gt; &lt;node id=&amp;quot;n50&amp;quot;/&gt; &lt;node id=&amp;quot;n3&amp;quot;/&gt; &lt;node id=&amp;quot;n4&amp;quot;/&gt;
&lt;node id=&amp;quot;n20&amp;quot;/&gt; &lt;node id=&amp;quot;n22&amp;quot;/&gt; &lt;node id=&amp;quot;n24&amp;quot;/&gt; &lt;/node list&gt;
&lt;/node list&gt; &lt;alignment type=&amp;quot;fuzzy&amp;quot;/&gt;
&lt;alignment type=&amp;quot;strict&amp;quot;/&gt; &lt;/relation&gt;
&lt;/relation&gt; &lt;relation id=&amp;quot;r4&amp;quot; type=&amp;quot;pointing&amp;quot;&gt;
&lt;relation id=&amp;quot;r2&amp;quot; type=&amp;quot;specification&amp;quot;&gt; &lt;domain type=&amp;quot;gesture&amp;quot; type=&amp;quot;oriented rel&amp;quot;/&gt;
&lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;oriented rel&amp;quot;/&gt; &lt;edge from=&amp;quot;n3&amp;quot; to=&amp;quot;n4&amp;quot;&gt;
&lt;edge from=&amp;quot;n20&amp;quot; to=&amp;quot;n24&amp;quot;&gt; &lt;alignment type=&amp;quot;strict&amp;quot;/&gt;
</table>
<figure confidence="0.946775">
&lt;/relation&gt; &lt;/relation&gt;
&lt;/relations&gt; &lt;/relations&gt;
&lt;/node&gt; &lt;/node&gt;
</figure>
<figureCaption confidence="0.999918">
Figure 4: XML encoding of hypernodes
</figureCaption>
<bodyText confidence="0.999902571428572">
(n50). In this expression, the alignment between
the objects is fuzzy, which is the normal situation
when different modalities interact. The second re-
lation describes the pointing action, implementing
the coreference between the noun phrase and the
physical object. This representation indicates the
three nodes as constituents.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999975892857143">
Understanding the mechanisms of natural interac-
tion requires to explain how the different modal-
ities interact. We need for this to acquire multi-
modal data and to annotate them as precisely as
possible for all modalities. Such resources have
to be large enough both for theoretical and com-
putational reasons: we need to cover as broadly
as possible the different phenomena and give the
possibility to use machine learning techniques in
order to produce a new generation of multimodal
annotation tools. However, neither such resource,
and a fortiori such tools, already exist. One reason,
besides the cost of the annotation task itself which
is still mainly manual for multimodal information,
is the lack of a general and homogeneous anno-
tation scheme capable of representing all kinds of
information, whatever its origin.
We have presented in this paper the basis of
such a scheme, proposing the notion of annota-
tion hypergraphs in which primary as well as sec-
ondary data are represented by means of the same
node structure. This homogeneous representation
is made possible thanks to a generic description
of primary data, identifying four types of basic in-
formation (index, domain, location, features). We
have shown that this scheme can be directly repre-
sented in XML, resulting in a generic multimodal
coding scheme.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999450869565218">
Allwood J., L. Cerrato, L. Dybkjaer, &amp; al. (2005) “The
MUMIN Multimodal Coding Scheme”, NorFA
yearbook
Bird S., M. Liberman (2001) “A formal framework
for linguistic annotation” Speech Communication,
Elsevier
Carletta, J., J. Kilgour, and T. O’Donnell (2003) “The
NITE Object Model Library for Handling Structured
Linguistic Annotation on Multimodal Data Sets” in
procs of the EACL Workshop on Language Technol-
ogy and the Semantic Web
Ide N. &amp; K. Suderman (2007) “GrAF: A Graph-based
Format for Linguistic Annotations”, in proceed-
ings of the Linguistic Annotation Workshop at the
ACL’07 (LAW-07)
Kipp M. (2001) “Anvil-a generic annotation tool for
multimodal dialogue” in procs of 7th European
Conference on Speech Communication and Tech-
nology
Martin, J.-C., Paggio, P., Kipp, M., Heylen, D. (2008)
Proceedings of the Workshop on Multimodal Cor-
pora : From Models of Natural Interaction to Sys-
tems and Applications (LREC’2008)
</reference>
<page confidence="0.997705">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001917">
<title confidence="0.776699333333334">A general scheme for broad-coverage multimodal annotation Philippe Laboratoire Parole et</title>
<author confidence="0.511807">CNRS</author>
<author confidence="0.511807">Aix-Marseille</author>
<email confidence="0.960136">blache@lpl-aix.fr</email>
<abstract confidence="0.989460328445748">We present in this paper a formal and computational scheme in the perspective of broad-coverage multimodal annotation. We propose in particular to introduce notion of hypergraphs which primary and secondary data are represented by means of the same structure. This paper addresses the question of resources and corpora for natural human-human interaction, in other words broad-coverage annotation of natural data. In this kind of study, most of domains have do be taken into consideration: prosody, pragmatics, syntax, gestures, etc. All these different domains interact in order to build an understandable message. We need then large multimodal annotated corpora of real data, precisely annotated for all domains. Building this kind of resource is a relatively new, but very active research domain, illustrated by the number of workshops (cf. (Martin, 2008)), international initiatives, such as MUMIN (Allwood, 2005), annotation tools such as NITE NXT (Carletta, 2003), Anvil (Kipp, 2001), etc. 1 A characterization of primary data Different types of primary data constitute the basis of an annotation: speech signal, video input, word strings, images, etc. But other kinds of primary data also can be used, for example in the perspective of semantic annotations such as concepts, references, types, etc. Such data are considered to be atomic in the sense that they are not built on top of lower level data. When looking more closely at these kinds of data, several characteristics can be identified: primary data is usually localized with respect to a timeline or a position: gestures can be localized into the video signal, phonemes into the speech one, words into the string or objects into a scene or a context. Two different kinds of are used: In the first case, a data is situated by means of a time interval whereas spatial data are localised in terms of relative or absolute positions. primary data usually refer to conphysical) objects: phonemes, gestures, referential elements into a scene, etc. However, kinds of primary data can be as concepts, ideas, emotions, etc. The W3C recommendation EMMA Multi-Modal proposes to different medium: tactile This classification is only relevant for data corresponding to concrete objects. the study of information structure shows the necessity to take into account accessibility of the objects: some data are directly accessible from the signal or the discourse, they have an existence or have already been mentioned. In this they are said to be For example, gestures, sounds, physical objects fall in this category. On the other hand, other kinds of data are deduced from the context, typically the abstract ones. are considered as In the remaining of the paper, we propose the following definition: atomic objects that cannot be decomposed. They represent possible constituent on top of which higher level objects can be built. Primary data does not require any interpretation to be identified, they are of direct access. This primary data typology is given in fig- (1). It shows a repartition between Concrete objects are usually those taken into account in corpus annotation. As a consequence, annotation usually focuses on speech and gestures, which narrows down the set of data to those with a temporal localization. However, other kinds of data cannot be situated in the 174 of the Third Linguistic Annotation Workshop, ACL-IJCNLP pages Singapore, 6-7 August 2009. ACL and AFNLP Phonemes Words Gestures Discourse referents Synsets Physical objects Produced + + + +/- - + Accessible - - - +/- + - Concrete + + + +/- - + Abstract - - - +/- - + Temporal + + + +/- - - Spatial - - +/- +/- - + Acoustic + +/- - - - - Visual - - + +/- - + Tactile - - +/- +/- - + 1: data description timeline (e.g. objects in the environment of the scene) nor spatially (e.g. abstract data). We need to propose a more general approach of data indexing that has to distinguish on the one hand between temporal and spatial localization and on the other hand between data that can be located and data that cannot. 2 Graph representation: nodes and edges semantics One of the most popular linguistic annotation repis graphs 2001) in which nodes are positions whereas edges bear linguistic information. This representation is elaborated on the basis of a temporal anchoring, even though it is also possible to represent other kinds of anchoring. Several generic annotation format has been proposed on top of this representation, such as LAF and its extension GrAF (cf. (Ide, 2007)). In these approaches, edges to their turn can be interpreted as nodes in order to build higher level information. One can consider the result as an hypergraph, in which nodes can be subgraphs. In order to explore farther this direction, we propose a more general interpretation for nodes that are not only positions in the input: nodes are complex objects that can be referred at different levels of the representation, they encode all annotations. In order to obtain an homogeneous representations, the two node types used in hypergraphs share the same information structure which relies on the following points: using an index renders possible to represent any kind of graphs, not only trees. They give to nodes the possibility of encoding any kind of information. prosody, semantics, syntax, gesture, pragmatics, etc. It is important to indicate as precisely as possible this information, eventually by means of sub-domains annotations generally have a spatial or a temporal situation. This information is optional. nodes have to bear specific linguistic indications, describing its properties. Hypernodes bear, on top of this information, the specification of the subgraph represented by its constituents and their relations. We propose to add another kind of information in the hypernode structure: • secondary data are built on top of primary one. They can be represented by means of a set of properties (constituency, linearity, coreference, etc.) implemented as edges plus the basic characteristics of a node. A secondary data is then graph with a label, these two elements composing an hypernode. The distinction between node and hypernodes makes it possible to give a homogeneous representation of primary and secondary data. 3 An XML representation of annotation hypergraphs We propose in this section an XML encoding of the scheme presented above. 3.1 Atomic nodes The first example of the figure (2) illustrates the of a The node is indexed, making its reference possible in higher level structures. Its label corresponds to the tag that would be indicated in the annotation. Other elements complete the description: the linguistic domain (specified by the attributes type and sub-type), the specification of the medium, the object localization (by means of anchors). In this example, a phoneme being part of the acoustic signal, the anchor is temporal and use an explicit timeline reference. The same kind of representation can be given transcription tokens (see node figure (2)). The value of the node is the orthographic form. It is potentially aligned on the signal, and then represented with a temporal anchoring. Such 175 2: encoding of atomic nodes &lt;node ID=&amp;quot;n1&amp;quot; label=&amp;quot;u&amp;quot;&gt; &lt;domain type=&amp;quot;phonetics&amp;quot; subtype=&amp;quot;phoneme&amp;quot; medium=&amp;quot;acoustic&amp;quot;/&gt; &lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;285&amp;quot; end=&amp;quot;312&amp;quot;/&gt; &lt;/node&gt; &lt;node ID=&amp;quot;n21&amp;quot; label=&amp;quot;book&amp;quot;&gt; &lt;domain type=&amp;quot;transcription&amp;quot; subtype=&amp;quot;token&amp;quot;/&gt; &lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;242&amp;quot; end=&amp;quot;422&amp;quot;/&gt; &lt;/node&gt; &lt;node ID=&amp;quot;n24&amp;quot; label=&amp;quot;N&amp;quot;&gt; &lt;domain type=&amp;quot; morphosyntax&amp;quot; subtype=&amp;quot;word&amp;quot;/&gt; &lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;242&amp;quot; end=&amp;quot;422&amp;quot;/&gt; &lt;features ms=&amp;quot;ncms---&amp;quot;/&gt; &lt;/node&gt; &lt;node ID=&amp;quot;n3&amp;quot; label=&amp;quot;deictic&amp;quot;&gt; &lt;domain type=&amp;quot;gestures&amp;quot; subtype=&amp;quot;hand&amp;quot;/&gt; &lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;200&amp;quot; end=&amp;quot;422&amp;quot;/&gt; &lt;features hand=&amp;quot;right&amp;quot; deictic type=&amp;quot;space&amp;quot; object=&amp;quot;ref object&amp;quot;/&gt; &lt;/node&gt; &lt;node ID=&amp;quot;n4&amp;quot; label=&amp;quot;discourse-referent&amp;quot;&gt; &lt;domain type=&amp;quot;semantics&amp;quot; subtype=&amp;quot;discourse universe&amp;quot; medium=&amp;quot;visual&amp;quot;/&gt; &lt;anchoring type=&amp;quot;spatial&amp;quot; x=&amp;quot;242&amp;quot; y=&amp;quot;422&amp;quot; z=&amp;quot;312&amp;quot;/&gt; &lt;features isa=&amp;quot;book&amp;quot; color=&amp;quot;red&amp;quot; /&gt; &lt;/node&gt; anchoring makes it possible to align the orthographic transcription with the phonetic one. In the case of written texts, temporal bounds would be replaced by the positions in the texts, which could be interpreted as an implicit temporal anchoring. next example presented in node illustrates the representation of part-of-speech nodes. domain in this case is its subis In this case too, the anchoring is temporal, with same bounds as the corresponding token. In this node, a feature element is added, bearing the morpho-syntactic description. atomic node described in node repreanother physical object: a Its domain is gesture and its subtype, as proposed for example in the MUMIN scheme (see (Allwood, 2005)) is the part of the body. The anchoring is also temporal and we can observe in this example a synchronization of the gesture with the token last example (node presents an atomic node describing a physical object present in the scene (a book on a shelf of a library). It belongs to the semantics domain as a discourse referent and is anchored spatially by its spatial coordinates. One note that anchoring can be in the presented here) or the object with respect to other ones). 3.2 Relations Relations are represented in the same way as nodes. They are of different types, such as constituency, linearity, syntactic dependency, semantic specification, etc. and correspond to a certain The example figure (3) illustrates a between a noun (node described above) and its determiner (node Non-oriented binary relations also occur, for example cooccurrency. Relations can be expressed in order to represent a set of objects. The example (relation presents the case of three constituents of an higher-level object (the complete description of which being given in the next section). Finally, the alignment between objects is speciby two different values: they have the same temporal or spatial marks; otherwize. 3.3 Hypernodes Hypernodes encode subgraphs with the possibility of being themselves considered as nodes. Their structure completes the atomic node with a set of relations. Hypernodes encode different kinds of objects such as phrases, constructions, referential expressions, etc. The first example represents a The node is indexed, bears a tag, a domain, an anchoring and features. The set of relations spectwo types of information. First, the three constituents: example a deterexample an adjective) and (the noun described in the previous section). The is said to be means that the right border of the first element and the left border of the last one have to be the same. The resulting structure is an hypernode describing the different of the means of features and relations. The second example illustrates the case of a referential expression. Let’s imagine the situation where a person points out at a book on a shelf, book will fall In terms of instructure, the use of a definite possible because the referent is accessible from the context: the alignment of the the deictic gesture see previous section) makes the coreference possible. This construction results in a discourse referent bringing toall the properties of the physical object and that of the object described in the discourse 176 &lt;relation id=&amp;quot;r1&amp;quot; label=&amp;quot;specification&amp;quot;&gt; &lt;relation id=&amp;quot;r2&amp;quot; label=&amp;quot;constituency&amp;quot;&gt; &lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;oriented rel&amp;quot;/&gt; &lt;edge from=&amp;quot;n20&amp;quot; to=&amp;quot;n24&amp;quot;&gt; &lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;set rel&amp;quot;/&gt; &lt;/relation&gt; &lt;node list&gt; &lt;node id=&amp;quot;n20&amp;quot;/&gt; &lt;node id=&amp;quot;n22&amp;quot;/&gt; &lt;node id=&amp;quot;n24&amp;quot;/&gt; &lt;/node list&gt; &lt;alignment type=&amp;quot;strict&amp;quot;/&gt; &lt;/relation&gt; 3: encoding of relations &lt;node ID=&amp;quot;n50&amp;quot; label=&amp;quot;NP&amp;quot;&gt; &lt;node ID=&amp;quot;n51&amp;quot; label=&amp;quot;ref expression&amp;quot;&gt; &lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;phrase&amp;quot;/&gt; &lt;domain type=&amp;quot;semantics&amp;quot; subtype=&amp;quot;discourse referent&amp;quot;/&gt; &lt;anchor type=&amp;quot;temporal&amp;quot; start=&amp;quot;200&amp;quot; end=&amp;quot;422&amp;quot;/&gt; &lt;features referent=&amp;quot;book’&amp;quot; color=&amp;quot;red&amp;quot; /&gt; &lt;features cat=&amp;quot;NP&amp;quot; agr=&amp;quot;ms&amp;quot; sem type=&amp;quot;ref&amp;quot;/&gt; &lt;relations&gt; &lt;relations&gt; &lt;relation id=&amp;quot;r3&amp;quot; type=&amp;quot;constituency&amp;quot;&gt; &lt;relation id=&amp;quot;r1&amp;quot; type=&amp;quot;constituency&amp;quot;&gt; &lt;domain type=&amp;quot;semantics&amp;quot; type=&amp;quot;set rel&amp;quot;/&gt; &lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;set rel&amp;quot;/&gt; &lt;node list&gt; &lt;node list&gt; &lt;node id=&amp;quot;n50&amp;quot;/&gt; &lt;node id=&amp;quot;n3&amp;quot;/&gt; &lt;node id=&amp;quot;n4&amp;quot;/&gt; &lt;node id=&amp;quot;n20&amp;quot;/&gt; &lt;node id=&amp;quot;n22&amp;quot;/&gt; &lt;node id=&amp;quot;n24&amp;quot;/&gt; &lt;/node list&gt; &lt;/node list&gt; &lt;alignment type=&amp;quot;fuzzy&amp;quot;/&gt; &lt;alignment type=&amp;quot;strict&amp;quot;/&gt; &lt;/relation&gt; &lt;/relation&gt; &lt;relation id=&amp;quot;r4&amp;quot; type=&amp;quot;pointing&amp;quot;&gt; &lt;relation id=&amp;quot;r2&amp;quot; type=&amp;quot;specification&amp;quot;&gt; &lt;domain type=&amp;quot;gesture&amp;quot; type=&amp;quot;oriented rel&amp;quot;/&gt; &lt;domain type=&amp;quot;syntax&amp;quot; subtype=&amp;quot;oriented rel&amp;quot;/&gt; &lt;edge from=&amp;quot;n3&amp;quot; to=&amp;quot;n4&amp;quot;&gt; &lt;edge from=&amp;quot;n20&amp;quot; to=&amp;quot;n24&amp;quot;&gt; &lt;alignment type=&amp;quot;strict&amp;quot;/&gt; &lt;/relation&gt; &lt;/relation&gt; &lt;/relations&gt; &lt;/relations&gt; &lt;/node&gt; &lt;/node&gt; 4: encoding of hypernodes In this expression, the alignment between the objects is fuzzy, which is the normal situation when different modalities interact. The second relation describes the pointing action, implementing the coreference between the noun phrase and the physical object. This representation indicates the three nodes as constituents. 4 Conclusion Understanding the mechanisms of natural interaction requires to explain how the different modalities interact. We need for this to acquire multimodal data and to annotate them as precisely as possible for all modalities. Such resources have to be large enough both for theoretical and computational reasons: we need to cover as broadly as possible the different phenomena and give the possibility to use machine learning techniques in order to produce a new generation of multimodal annotation tools. However, neither such resource, and a fortiori such tools, already exist. One reason, besides the cost of the annotation task itself which is still mainly manual for multimodal information, is the lack of a general and homogeneous annotation scheme capable of representing all kinds of information, whatever its origin. We have presented in this paper the basis of a scheme, proposing the notion of annotahypergraphs which primary as well as secondary data are represented by means of the same node structure. This homogeneous representation is made possible thanks to a generic description of primary data, identifying four types of basic information (index, domain, location, features). We have shown that this scheme can be directly represented in XML, resulting in a generic multimodal coding scheme.</abstract>
<note confidence="0.50936925">References Allwood J., L. Cerrato, L. Dybkjaer, &amp; al. (2005) “The Multimodal Coding Scheme”, yearbook Bird S., M. Liberman (2001) “A formal framework linguistic annotation” Elsevier Carletta, J., J. Kilgour, and T. O’Donnell (2003) “The</note>
<title confidence="0.651003">NITE Object Model Library for Handling Structured Linguistic Annotation on Multimodal Data Sets” in procs of the EACL Workshop on Language Technology and the Semantic Web</title>
<author confidence="0.299812">N Ide</author>
<author confidence="0.299812">K Suderman “GrAF A Graph-based</author>
<note confidence="0.86483175">Format for Linguistic Annotations”, in proceedof the Annotation Workshop at the ACL’07 (LAW-07) Kipp M. (2001) “Anvil-a generic annotation tool for multimodal dialogue” in procs of 7th European Conference on Speech Communication and Technology Martin, J.-C., Paggio, P., Kipp, M., Heylen, D. (2008) the Workshop on Multimodal Corpora : From Models of Natural Interaction to Sysand Applications 177</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allwood</author>
<author>L Cerrato</author>
<author>L Dybkjaer</author>
<author>al</author>
</authors>
<date>2005</date>
<journal>The MUMIN Multimodal Coding Scheme”, NorFA yearbook</journal>
<marker>Allwood, Cerrato, Dybkjaer, al, 2005</marker>
<rawString>Allwood J., L. Cerrato, L. Dybkjaer, &amp; al. (2005) “The MUMIN Multimodal Coding Scheme”, NorFA yearbook</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>M Liberman</author>
</authors>
<title>A formal framework for linguistic annotation” Speech Communication,</title>
<date>2001</date>
<publisher>Elsevier</publisher>
<marker>Bird, Liberman, 2001</marker>
<rawString>Bird S., M. Liberman (2001) “A formal framework for linguistic annotation” Speech Communication, Elsevier</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>J Kilgour</author>
<author>T O’Donnell</author>
</authors>
<title>The NITE Object Model Library for Handling Structured Linguistic Annotation</title>
<date>2003</date>
<booktitle>on Multimodal Data Sets” in procs of the EACL Workshop on Language Technology and the Semantic Web</booktitle>
<marker>Carletta, Kilgour, O’Donnell, 2003</marker>
<rawString>Carletta, J., J. Kilgour, and T. O’Donnell (2003) “The NITE Object Model Library for Handling Structured Linguistic Annotation on Multimodal Data Sets” in procs of the EACL Workshop on Language Technology and the Semantic Web</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>GrAF: A Graph-based Format for Linguistic Annotations”,</title>
<date>2007</date>
<booktitle>in proceedings of the Linguistic Annotation Workshop at the ACL’07 (LAW-07)</booktitle>
<marker>Ide, Suderman, 2007</marker>
<rawString>Ide N. &amp; K. Suderman (2007) “GrAF: A Graph-based Format for Linguistic Annotations”, in proceedings of the Linguistic Annotation Workshop at the ACL’07 (LAW-07)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Anvil-a generic annotation tool for multimodal dialogue” in procs of</title>
<date>2001</date>
<booktitle>7th European Conference on Speech Communication and Technology</booktitle>
<contexts>
<context position="1172" citStr="Kipp, 2001" startWordPosition="176" endWordPosition="177">overage annotation of natural data. In this kind of study, most of domains have do be taken into consideration: prosody, pragmatics, syntax, gestures, etc. All these different domains interact in order to build an understandable message. We need then large multimodal annotated corpora of real data, precisely annotated for all domains. Building this kind of resource is a relatively new, but very active research domain, illustrated by the number of workshops (cf. (Martin, 2008)), international initiatives, such as MUMIN (Allwood, 2005), annotation tools such as NITE NXT (Carletta, 2003), Anvil (Kipp, 2001), etc. 1 A characterization of primary data Different types of primary data constitute the basis of an annotation: speech signal, video input, word strings, images, etc. But other kinds of primary data also can be used, for example in the perspective of semantic annotations such as concepts, references, types, etc. Such data are considered to be atomic in the sense that they are not built on top of lower level data. When looking more closely at these kinds of data, several characteristics can be identified: - Location: primary data is usually localized with respect to a timeline or a position:</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Kipp M. (2001) “Anvil-a generic annotation tool for multimodal dialogue” in procs of 7th European Conference on Speech Communication and Technology</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Martin</author>
<author>P Paggio</author>
<author>M Kipp</author>
<author>D Heylen</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the Workshop on Multimodal Corpora : From Models of Natural Interaction to Systems and Applications (LREC’2008)</booktitle>
<marker>Martin, Paggio, Kipp, Heylen, 2008</marker>
<rawString>Martin, J.-C., Paggio, P., Kipp, M., Heylen, D. (2008) Proceedings of the Workshop on Multimodal Corpora : From Models of Natural Interaction to Systems and Applications (LREC’2008)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>