<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029032">
<title confidence="0.9928475">
The CIPS-SIGHAN CLP 2010
Chinese Word Segmentation Bakeoff
</title>
<author confidence="0.995697">
Hongmei Zhao and Qun Liu
</author>
<affiliation confidence="0.9946935">
Key Laboratory of Intelligent Information Processing,
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<email confidence="0.993395">
{zhaohongmei,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.993767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794473684211">
The CIPS-SIGHAN CLP 2010 Chinese
Word Segmentation Bakeoff was held
in the summer of 2010 to evaluate the
current state of the art in word
segmentation. It focused on the cross-
domain performance of Chinese word
segmentation algorithms. Eighteen
groups submitted 128 results over two
tracks (open training and closed
training), four domains (literature,
computer science, medicine and finance)
and two subtasks (simplified Chinese
and traditional Chinese). We found that
compared with the previous Chinese
word segmentation bakeoffs, the
performance of cross-domain Chinese
word segmentation is not much lower,
and the out-of-vocabulary recall is
improved.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998485257142857">
Chinese is written without inter-word spaces, so
finding word-boundaries is an essential first
step in many natural language processing tasks
ranging from part of speech tagging to parsing,
reference resolution and machine translation.
SIGHAN, the Special Interest Group for
Chinese Language Processing of the
Association for Computational Linguistics,
successfully conducted four prior word
segmentation bakeoffs, in 2003 (Sproat and
Emerson, 2003), 2005 (Emerson, 2005), 2006
(Levow, 2006) and 2007 (Jin and Chen, 2007),
and the bakeoff 2007 was jointly organized with
the Chinese Information Processing Society of
China (CIPS). These evaluations established
benchmarks for word segmentation with which
researchers evaluate their segmentation system.
After years of intensive researches, Chinese
word segmentation has achieved a quite high
precision, though the out-of-vocabulary
problem is still a continuing challenge.
However, the performance of segmentation is
not so satisfying for out-of-domain text.
The CIPS-SIGHAN CLP 2010 Chinese
Word Segmentation Bakeoff continues the
ongoing series of the SIGHAN Chinese Word
Segmentation Bakeoff. It was organized by
Institute of Computing Technology, Chinese
Academy of Sciences (abbreviated as ICT
below). It focused on the cross-domain
performance of Chinese word segmentation
algorithms. And the bakeoff results will be
reported in conjunction with the First CIPS-
SIGHAN joint conference on Chinese
Language Processing, Beijing, China.
</bodyText>
<sectionHeader confidence="0.803608" genericHeader="method">
2 Details of the Evaluation
</sectionHeader>
<subsectionHeader confidence="0.71293">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999825666666667">
There are two kinds of corpora in the evaluation,
with one using the simplified Chinese
characters and another using the traditional
Chinese characters. For the simplified Chinese
corpora, the test corpora, reference corpora, and
the unlabeled training corpora were provided by
ICT, and the labeled training corpus (1 month
data of The People&apos;s Daily in 1998) was
provided by Peking University. For the
traditional Chinese corpora, all the training, test
and reference corpora were provided by the
Hongkong City University.
There are four domains in this evaluation.
Before the releasing of the test data, two of
them (literature and computer science, we
abbreviate “computer science” to “computer”
below) are known to the participants (we
provided the corresponding unlabeled training
</bodyText>
<table confidence="0.961673666666667">
corpora Characters Tokens Word Types TTR OOV Rate
Simplified Test Literature 50,637 35,736 6,364 0.18 0.069
Chinese
Computer 53,382 35,319 4,150 0.12 0.152
Medicine 50,969 31,490 5,076 0.16 0.11
Finance 53,253 33,028 4,918 0.15 0.087
Training Labeled 1,820,456 1,109,947 55,303 0.05
Unlabeled-L 100,352
Unlabeled-C 103,764
Traditional Test Literature 54,357 36,378 8,141 0.22 0.094
Chinese
Computer 67,321 43,499 6,197 0.14 0.094
Medicine 68,090 43,458 6,510 0.15 0.075
Finance 74,461 47,144 6,652 0.14 0.068
Training Labeled 1,863,298 1,146,988 63,588 0.06
Unlabeled-L 105,653
Unlabeled-C 109,303
Table1. Overall corpus statistics
Site ID Site Name Contact Simplified Traditional
Chinese Chinese
S1 College of Computer and Information Engineering, Jiangde Yu ◆◇ ◆◇
Anyang Normal University, Henan province, China
S2 Institute of Intelligent Information Processing, Beijing Wenjie Su ◆
Information Science &amp; Technology University
S3 Beijing Institute of Technology Hhapi g ◇
S4 Center for Language Information Processing Institute, Zhiyong ◇
Beijing Language and Culture University Luo
S5 Beijing University of Posts and Telecommunications Yuan ◆ ◇
S6 Dalian University of Technology Huiwei ◆◇
Zhou
S7 Fudan University Xipeng ◆ ◆
Qiu
S8 Shenzhen Graduate School Harbin Institute of Jianping ◇ ◇
Technology Shen
S9 Language Technologies Institute, Carnegie Mellon Qin Gao ◆◇
University
S10 National Central University, Taiwan Yu-Chieh ◆ ◆
Wu
S11 Natural Language Processing Lab, Northeastern Huizhen ◆
University, China Wang
S12 National Laboratory of Pattern Recognition, Institute of Kun Wang ◆
Automation, Chinese Academy of Science
S13 Institute of Computer Science and Technology, Peking Liang ◆
University Zong
S14 Institute of Computational Linguistics, Peking University Mairgup ◆
S15 Queensland University of Technology Eric Tang ◆◇ ◆◇
S16 Institute of Information Science, Academia Sinica, Cheng-◆ ◆
Taiwan Lung Sung
S17 Natural Language Processing Lab, Suzhou University Junhui Li ◆
S18 Anhui Speech and language Technology Engineering Zhigang ◆◇
Research Center Chen
</table>
<tableCaption confidence="0.999888">
Table 2. Participating groups (◆=closed track, ◇=open track, there are four domains on every track)
</tableCaption>
<bodyText confidence="0.999844775">
corpora for each during the training phrase),
and another two domains (medicine and finance)
are unknown to the participants (without any in-
domain training corpora). All corpora are UTF-
8 encoded. Details on each corpus are provided
in Table 1. We introduce a type-token ratio
(TTR) to indicate the vocabulary diversity in
each corpus.
During the process of building the reference
corpora for the simplified Chinese word
segmentation subtask, we manually check the
automatically segmented results of the test data
against the standard provided in “The
Specification for the Basic Processing of
Contemporary Chinese Corpus from Peking
University”. In this process, we refer to the
labeled training data frequently with a view to
keep the annotation consistency between these
two kinds of corpora. Furthermore, we made a
comparison test which compared the
segmentation of the same character strings
present in both corpora automatically, and
corrected the inconsistent cases. However, in
the labeled training corpus, there are minor
incorrect segmentation cases against the
standard from Peking University, such as “赢
家 ” (yin2 jia1, with the meaning of “the
winner”, this word should be regarded as a
word according to the above-mentioned
standard), and there are also a few interior
inconsistent cases in this corpus, such as “患
有&amp;quot; and “患 有&amp;quot; (huan4 you3, with the
meaning of “suffer from”). Whenever the
segmentation of the reference corpora was
different from the above-mentioned incorrect or
inconsistent segmentation in the training corpus,
we followed the standard from Peking
University. All the evaluation corpora can be
accessible from the Chinese Linguistic Data
Consortium at: http://www.chineseldc.org.
</bodyText>
<subsectionHeader confidence="0.999347">
2.2 Rules and Procedures
</subsectionHeader>
<bodyText confidence="0.99991175">
This bakeoff followed a strict set of guidelines
and a rigid timetable. The detailed instructions
for the bakeoff can be found at
http://www.cipsc.org.cn/clp2010/cfpa.htm. The
training material of simplified Chinese word
segmentation was available starting April 1, the
training material of traditional Chinese word
segmentation was available April 23, testing
material was available June 9, and the results
had to be returned to the organizer by email by
June 11 no later than 18:00 Beijing time.
The participating groups (“sites”) of CIPS-
SIGHAN CLP 2010 Bakeoff registered by
email. There are two subtasks in this evaluation:
word segmentation for simplified Chinese text
and word segmentation for traditional Chinese
text. The participating sites were required to
declare which subtask they would participate in.
The open and closed tracks were defined as
follows:
</bodyText>
<listItem confidence="0.925571529411765">
• For the closed training evaluation,
participants can only use data provided by
the organizer to train their systems.
Specifically, the following data resources
and software tools are not permitted to be
used in the training:
1. Unspecified corpus;
2. Unspecified dictionary, word list or
character list: include the dictionaries of
named entity, character lists for specific
type of Chinese named entities, idiom
dictionaries, semantic lexicons, etc.;
3. Human-encoded rule bases;
4. Unspecified software tools, include
word segmenters, part-of-speech
taggers, or parsers which are trained
using unspecified data resources.
</listItem>
<bodyText confidence="0.99945">
The character type information to
distinguish the following four character
types can be used in training: Chinese
characters, English letters, digits and
punctuations.
</bodyText>
<listItem confidence="0.74900325">
• In the Open training evaluation,
participants can use any language
resources, including the training data
provided by the organizer.
</listItem>
<bodyText confidence="0.999749466666667">
Participants were asked to submit their data
using specific naming conventions, and from
the result file name we can see in which track
the result was run, as well as other necessary
information. Of course, the results on both
tracks are welcomed.
Scoring was done automatically using a
combination of Perl and shell scripts. The
scripts (Sproat and Emerson, 2003, 2005) used
for scoring can be downloaded from
http://www.sighan.org/bakeoff2005/. The
bakeoff organizer provided an on-line scoring
system to all the participants who had submitted
their bakeoff results for their follow-up
experiments.
</bodyText>
<subsectionHeader confidence="0.999532">
2.3 Participating sites
</subsectionHeader>
<bodyText confidence="0.999977818181818">
Eighteen sites submitted results and a technical
report. Mainland China had the greatest number
with 14, followed by Taiwan (2), the United
States (1) and Australia (1). A summary of
participating groups and the tracks for which
they submitted results can be found in Table 2
on the preceding page. There are more sites
who had registered for the bakeoff. However,
several of them withdrew due to technical
difficulties or other problems. Altogether 128
runs were submitted for scoring.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="method">
3 Results
</sectionHeader>
<subsectionHeader confidence="0.99922">
3.1 Baseline and topline experiments
</subsectionHeader>
<bodyText confidence="0.999921875">
Following previous bakeoffs, to provide a
basis for comparison, we computed baseline
and topline scores for each of the corpora.
When computing a baseline, we compiled a
dictionary of all the words in the labeled
training corpus, and then we used this
dictionary with a simple left-to-right maximal
match algorithm to segment the test corpus. The
results of this experiment are shown in Table 3.
We expect systems to do at least as well as the
baseline. The topline employed the same
procedure, but instead used the dictionary of all
the words in the test corpus. These results are
presented in Table 4. We expect systems to
generally underperform this topline, because no
one could exactly know the set of words that
occur in the test corpus.
In these and subsequent tables, we list the
word count for the test corpus, test recall (R),
test precision (P), balanced F score (where F =
2PR/(P+R)), the out-of-vocabulary (OOV) rate
on the test corpus, the recall on OOV words
(Roov), and the recall on in-vocabulary words
(Riv).
</bodyText>
<subsectionHeader confidence="0.998298">
3.2 Raw scores
</subsectionHeader>
<bodyText confidence="0.999918">
All the results are presented in Tables 5-20.
Column headings are as above, except for “Cr”
and “Cp” for which see Section 3.3. All tables
are sorted by F score.
</bodyText>
<subsectionHeader confidence="0.998307">
3.3 Statistical significance of the results
</subsectionHeader>
<bodyText confidence="0.99748572">
Following previous bakeoffs, let us assume that
the recall rates represent the probability p that a
word will be successfully identified, and let us
further assume the binomial distribution is
appropriate for this experiment. Given the
Central Limit Theorem for Bernouli trials —
e.g. (Grinstead and Snell, 1997), then the 95%
confidence interval is given as ±2 p(1− p) / n ,
where n is the number of trials (words). The
recall-based confidences (±2 p(1− p) / n ) are
given as “Cr” in Tables 5-20. Similarly, we can
assume the precision rates represent the
probability that a character string that has been
identified as a word is really a word. And the
precision-based confidences are given as “Cp”
in the tables. They can be interpreted as follows:
To decide whether two systems are significantly
different (at the 95% confidential level), one
just has to compute whether their confidence
intervals overlap. If at least one of the “Cr” and
“Cp” are different, we can treat these two
systems as significantly different (at the 95%
confidential level). Using this criterion all
systems in this bakeoff are significantly
different from each other.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.921719">
4.1 Comparison between open and closed
tracks
</subsectionHeader>
<bodyText confidence="0.999969571428571">
In this bakeoff, there are 8 systems that ran on
both closed and open tracks, which result in 32
pairs of scores for F measure and OOV recall
respectively. Table 21 shows the results of these
systems. We can see that their scores of F
measures on open track don’t have advantage
over their counterparts on the closed track: only
14 scores (in 32 scores) on open track are
higher than their counterparts on the closed
track. This is different from the previous
bakeoffs. But for OOV recall, the case is
different. There are 23 scores (in 32 scores) on
open track are higher than their counterparts on
the closed track.
</bodyText>
<sectionHeader confidence="0.8249055" genericHeader="method">
4.2 Improved OOV recall over the prior
bakeoffs
</sectionHeader>
<bodyText confidence="0.999884210526316">
From all the results, we can see that the widest
variation among systems lies in the OOV recall
rate. And dealing with unknown words is still
the most difficult problem of Chinese word
segmentation.
However, while comparing the top OOV
recall rates of this bakeoff with those of the
prior four bakeoffs, we found the OOV recall
rates of this bakeoff achieved an obvious
improvement. Table 22 shows the comparisons.
We managed to find four pairs of test corpora
with similar OOV rates for comparisons. In the
comparisons, most top OOV recall rates of
bakeoff 2010 are much higher than their
counterparts of prior bakeoffs. An exception
comes from the open track of medicine domain
for traditional CWS subtask, and because only 3
systems submitted results, this comparison
seems less meaningful.
</bodyText>
<subsectionHeader confidence="0.995727">
4.3 Performance under different domains
</subsectionHeader>
<bodyText confidence="0.999976689655173">
We listed the top performance by F measure on
every track, domain and subtask on Table 23.
Generally we think that cross-domain word
segmentation will lead to a lower performance
than in-domain word segmentation. In this
bakeoff, it seems that the best performance of
cross-domain word segmentation is at almost
the same level of that of the prior bakeoffs. We
know that the performance of different test set
is incomparable. However, the performance in
the out-of-domain text is somewhat surprising
to us. We guess one reason may be the usage of
domain adaptive technology, another reason
may be the new technologies used by the
participants. We hope to see the exact reasons
in the technological reports of participants in
the coming conference.
We provided unlabeled data to two domains.
However, we did not see significant difference
on the performance of closed test between these
domains and other domains. Some participants
pointed out that it is because the size of the
unlabeled data is rather small.
And we found that among four domains,
the performance (by the value of F measure and
OOV recall, with scores in bold in the table) on
finance is always the best or very close to the
best. Perhaps this is because the OOV rate on
finance test corpus is rather low.
</bodyText>
<table confidence="0.9989435">
Corpus Word Count R P F OOV Roov Riv
Simplifed L 35736 0.917 0.862 0.889 0.069 0.156 0.973
Chinese
C 35319 0.856 0.632 0.727 0.152 0.163 0.98
M 31490 0.886 0.774 0.826 0.11 0.123 0.981
F 33028 0.914 0.803 0.855 0.087 0.233 0.979
Tradition L 36378 0.863 0.788 0.824 0.094 0.041 0.948
al
Chinese
C 43499 0.873 0.701 0.778 0.094 0.01 0.963
M 43458 0.886 0.81 0.846 0.075 0.027 0.955
F 47144 0.888 0.826 0.855 0.068 0.006 0.952
</table>
<tableCaption confidence="0.957523">
Table 3. Baseline scores: Results for maximum match with training vocabulary (L=literature, C=computer,
M=medicine, F=finance)
</tableCaption>
<table confidence="0.99956125">
Corpus Word Count R P F OOV Roov Riv
Simplifed L 35736 0.986 0.99 0.988 0.069 0.996 0.985
Chinese
C 35319 0.991 0.993 0.992 0.152 0.99 0.991
M 31490 0.989 0.991 0.99 0.11 0.98 0.99
F 33028 0.994 0.995 0.994 0.087 0.995 0.994
Tradition L 36378 0.981 0.988 0.985 0.094 0.998 0.979
al
Chinese
C 43499 0.988 0.991 0.99 0.094 0.996 0.987
M 43458 0.984 0.989 0.986 0.075 0.992 0.983
F 47144 0.981 0.986 0.984 0.068 0.997 0.98
</table>
<tableCaption confidence="0.9840885">
Table 4. Topline scores: Results for maximum match with testing vocabulary (L=literature, C=computer,
M=medicine, F=finance)
</tableCaption>
<table confidence="0.9999503125">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S5 35736 0.945 ±0.00241 0.946 ±0.00239 0.946 0.069 0.816 0.954
S6 35736 0.94 ±0.00251 0.942 ±0.00247 0.941 0.069 0.649 0.961
S12 35736 0.937 ±0.00257 0.937 ±0.00257 0.937 0.069 0.652 0.958
S10 35736 0.936 ±0.00259 0.932 ±0.00266 0.934 0.069 0.564 0.964
S11 35736 0.931 ±0.00268 0.936 ±0.00259 0.934 0.069 0.648 0.952
S18 35736 0.932 ±0.00266 0.935 ±0.00261 0.933 0.069 0.654 0.953
S14 35736 0.925 ±0.00279 0.931 ±0.00268 0.928 0.069 0.667 0.944
S9 35736 0.92 ±0.00287 0.925 ±0.00279 0.923 0.069 0.625 0.942
S7 35736 0.915 ±0.00295 0.925 ±0.00279 0.92 0.069 0.577 0.94
S13 35736 0.916 ±0.00293 0.922 ±0.00284 0.919 0.069 0.613 0.939
S16 35736 0.917 ±0.00292 0.921 ±0.00285 0.919 0.069 0.699 0.933
S1 35736 0.908 ±0.00306 0.918 ±0.00290 0.913 0.069 0.556 0.935
S17 35736 0.909 ±0.00304 0.903 ±0.00313 0.906 0.069 0.707 0.924
S15 35736 0.907 ±0.00307 0.862 ±0.00365 0.884 0.069 0.206 0.959
S2 35736 0.695 ±0.00487 0.744 ±0.00462 0.719 0.069 0.381 0.719
</table>
<tableCaption confidence="0.998043">
Table 5. Simplified Chinese: Literature -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.9998684">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S6 35736 0.958 ±0.00212 0.953 ±0.00224 0.955 0.069 0.655 0.981
S3 35736 0.965 ±0.00194 0.94 ±0.00251 0.952 0.069 0.814 0.976
S18 35736 0.942 ±0.00247 0.943 ±0.00245 0.942 0.069 0.702 0.959
S9 35736 0.939 ±0.00253 0.943 ±0.00245 0.941 0.069 0.699 0.957
S1 35736 0.908 ±0.00306 0.916 ±0.00293 0.912 0.069 0.535 0.936
S5 35736 0.893 ±0.00327 0.918 ±0.00290 0.905 0.069 0.803 0.899
S4 35736 0.897 ±0.00322 0.907 ±0.00307 0.902 0.069 0.688 0.913
S15 35736 0.869 ±0.00357 0.873 ±0.00352 0.871 0.069 0.657 0.885
S8 35736 0.836 ±0.00392 0.841 ±0.00387 0.838 0.069 0.609 0.853
</table>
<tableCaption confidence="0.869611">
Table 6. Simplified Chinese: Literature --Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.98619125">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S6 35319 0.953 ±0.00225 0.95 ±0.00232 0.951 0.152 0.827 0.975
S11 35319 0.948 ±0.00236 0.945 ±0.00243 0.947 0.152 0.853 0.965
S12 35319 0.941 ±0.00251 0.94 ±0.00253 0.94 0.152 0.757 0.974
S9 35319 0.938 ±0.00257 0.936 ±0.00260 0.937 0.152 0.805 0.962
S13 35319 0.939 ±0.00255 0.934 ±0.00264 0.937 0.152 0.81 0.962
S18 35319 0.935 ±0.00262 0.934 ±0.00264 0.935 0.152 0.792 0.961
S5 35319 0.946 ±0.00241 0.914 ±0.00298 0.93 0.152 0.808 0.971
S14 35319 0.941 ±0.00251 0.916 ±0.00295 0.928 0.152 0.796 0.967
S7 35319 0.934 ±0.00264 0.919 ±0.00290 0.926 0.152 0.739 0.969
S10 35319 0.915 ±0.00297 0.915 ±0.00297 0.915 0.152 0.594 0.972
S17 35319 0.921 ±0.00287 0.9 ±0.00319 0.91 0.152 0.748 0.952
S1 35319 0.89 ±0.00333 0.908 ±0.00308 0.899 0.152 0.592 0.943
S15 35319 0.876 ±0.00351 0.844 ±0.00386 0.86 0.152 0.457 0.951
S16 35319 0.876 ±0.00351 0.799 ±0.00426 0.836 0.152 0.456 0.952
S2 35319 0.713 ±0.00481 0.641 ±0.00511 0.675 0.152 0.257 0.795
</table>
<tableCaption confidence="0.999028">
Table 7. Simplified Chinese: Computer -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.999805">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S9 35319 0.95 ±0.00232 0.95 ±0.00232 0.95 0.152 0.82 0.973
S18 35319 0.948 ±0.00236 0.946 ±0.00241 0.947 0.152 0.812 0.973
S6 35319 0.948 ±0.00236 0.929 ±0.00273 0.939 0.152 0.735 0.986
S3 35319 0.951 ±0.00230 0.926 ±0.00279 0.938 0.152 0.775 0.982
S8 35319 0.951 ±0.00230 0.915 ±0.00297 0.932 0.152 0.77 0.983
S5 35319 0.918 ±0.00292 0.896 ±0.00325 0.907 0.152 0.771 0.945
S1 35319 0.893 ±0.00329 0.908 ±0.00308 0.9 0.152 0.607 0.944
S4 35319 0.892 ±0.00330 0.88 ±0.00346 0.886 0.152 0.791 0.91
S15 35319 0.859 ±0.00370 0.878 ±0.00348 0.868 0.152 0.668 0.893
</table>
<tableCaption confidence="0.996965">
Table 8. Simplified Chinese: Computer -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.9997050625">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S6 31490 0.942 ±0.00263 0.936 ±0.00276 0.939 0.11 0.75 0.965
S18 31490 0.937 ±0.00274 0.934 ±0.00280 0.936 0.11 0.761 0.959
S5 31490 0.94 ±0.00268 0.928 ±0.00291 0.934 0.11 0.761 0.962
S7 31490 0.927 ±0.00293 0.924 ±0.00299 0.925 0.11 0.714 0.953
S10 31490 0.933 ±0.00282 0.915 ±0.00314 0.924 0.11 0.642 0.969
S11 31490 0.924 ±0.00299 0.922 ±0.00302 0.923 0.11 0.756 0.944
S12 31490 0.93 ±0.00288 0.917 ±0.00311 0.923 0.11 0.674 0.961
S14 31490 0.928 ±0.00291 0.918 ±0.00309 0.923 0.11 0.73 0.953
S9 31490 0.923 ±0.00300 0.917 ±0.00311 0.92 0.11 0.729 0.947
S13 31490 0.917 ±0.00311 0.911 ±0.00321 0.914 0.11 0.699 0.944
S1 31490 0.902 ±0.00335 0.907 ±0.00327 0.904 0.11 0.633 0.935
S16 31490 0.9 ±0.00338 0.896 ±0.00344 0.898 0.11 0.596 0.937
S17 31490 0.894 ±0.00347 0.873 ±0.00375 0.884 0.11 0.647 0.925
S15 31490 0.885 ±0.00360 0.804 ±0.00447 0.842 0.11 0.218 0.967
S2 31490 0.735 ±0.00497 0.74 ±0.00494 0.738 0.11 0.378 0.779
</table>
<tableCaption confidence="0.997026">
Table 9. Simplified Chinese: Medicine -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.9998222">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S9 31490 0.94 ±0.00268 0.936 ±0.00276 0.938 0.11 0.768 0.962
S18 31490 0.941 ±0.00266 0.935 ±0.00278 0.938 0.11 0.787 0.96
S6 31490 0.951 ±0.00243 0.92 ±0.00306 0.935 0.11 0.67 0.986
S3 31490 0.953 ±0.00239 0.913 ±0.00318 0.933 0.11 0.704 0.984
S5 31490 0.917 ±0.00311 0.907 ±0.00327 0.912 0.11 0.704 0.943
S4 31490 0.91 ±0.00323 0.901 ±0.00337 0.906 0.11 0.725 0.933
S1 31490 0.904 ±0.00332 0.906 ±0.00329 0.905 0.11 0.635 0.937
S15 31490 0.865 ±0.00385 0.846 ±0.00407 0.855 0.11 0.559 0.903
S8 31490 0.839 ±0.00414 0.832 ±0.00421 0.836 0.11 0.618 0.866
</table>
<tableCaption confidence="0.998716">
Table 10. Simplified Chinese: Medicine -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.999957625">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S6 33028 0.959 ±0.00218 0.96 ±0.00216 0.959 0.087 0.827 0.972
S12 33028 0.957 ±0.00223 0.956 ±0.00226 0.957 0.087 0.813 0.971
S9 33028 0.956 ±0.00226 0.955 ±0.00228 0.956 0.087 0.857 0.965
S11 33028 0.953 ±0.00233 0.956 ±0.00226 0.955 0.087 0.871 0.961
S18 33028 0.955 ±0.00228 0.956 ±0.00226 0.955 0.087 0.848 0.965
S5 33028 0.956 ±0.00226 0.952 ±0.00235 0.954 0.087 0.849 0.966
S10 33028 0.945 ±0.00251 0.941 ±0.00259 0.943 0.087 0.666 0.972
S7 33028 0.94 ±0.00261 0.942 ±0.00257 0.941 0.087 0.719 0.961
S13 33028 0.943 ±0.00255 0.94 ±0.00261 0.941 0.087 0.773 0.959
S14 33028 0.948 ±0.00244 0.928 ±0.00284 0.937 0.087 0.761 0.965
S1 33028 0.925 ±0.00290 0.938 ±0.00265 0.931 0.087 0.664 0.95
S17 33028 0.935 ±0.00271 0.915 ±0.00307 0.925 0.087 0.736 0.954
S16 33028 0.91 ±0.00315 0.906 ±0.00321 0.908 0.087 0.562 0.943
S15 33028 0.904 ±0.00324 0.865 ±0.00376 0.884 0.087 0.321 0.96
S2 33028 0.736 ±0.00485 0.752 ±0.00475 0.744 0.087 0.23 0.784
</table>
<tableCaption confidence="0.996745">
Table 11. Simplified Chinese: Finance -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.9999161">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S9 33028 0.96 ±0.00216 0.96 ±0.00216 0.96 0.087 0.847 0.971
S6 33028 0.964 ±0.00205 0.95 ±0.00240 0.957 0.087 0.763 0.983
S18 33028 0.948 ±0.00244 0.955 ±0.00228 0.951 0.087 0.853 0.957
S3 33028 0.963 ±0.00208 0.938 ±0.00265 0.95 0.087 0.758 0.982
S1 33028 0.925 ±0.00290 0.937 ±0.00267 0.931 0.087 0.669 0.95
S5 33028 0.928 ±0.00284 0.934 ±0.00273 0.931 0.087 0.808 0.939
S8 33028 0.893 ±0.00340 0.896 ±0.00336 0.894 0.087 0.796 0.902
S4 33028 0.885 ±0.00351 0.893 ±0.00340 0.889 0.087 0.757 0.897
S15 33028 0.853 ±0.00390 0.85 ±0.00393 0.851 0.087 0.438 0.893
</table>
<tableCaption confidence="0.99923">
Table 12. Simplified Chinese: Finance -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.999843166666667">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S10 36378 0.942 ±0.00245 0.942 ±0.00245 0.942 0.094 0.788 0.958
S1 36378 0.888 ±0.00331 0.905 ±0.00307 0.896 0.094 0.728 0.904
S7 36378 0.869 ±0.00354 0.91 ±0.00300 0.889 0.094 0.698 0.887
S16 36378 0.871 ±0.00351 0.891 ±0.00327 0.881 0.094 0.67 0.891
S15 36378 0.864 ±0.00359 0.789 ±0.00428 0.825 0.094 0.105 0.943
</table>
<tableCaption confidence="0.991932">
Table 13. Traditional Chinese: Literature -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.99875525">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S1 36378 0.905 ±0.00307 0.9 ±0.00315 0.902 0.094 0.775 0.918
S8 36378 0.868 ±0.00355 0.802 ±0.00418 0.834 0.094 0.503 0.905
S15 36378 0.804 ±0.00416 0.722 ±0.00470 0.761 0.094 0.234 0.863
</table>
<tableCaption confidence="0.995979">
Table 14. Traditional Chinese: Literature -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.996507">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S10 43499 0.948 ±0.00213 0.957 ±0.00195 0.952 0.094 0.666 0.977
S7 43499 0.933 ±0.00240 0.949 ±0.00211 0.941 0.094 0.791 0.948
S1 43499 0.908 ±0.00277 0.931 ±0.00243 0.919 0.094 0.684 0.931
S16 43499 0.913 ±0.00270 0.917 ±0.00265 0.915 0.094 0.663 0.939
S15 43499 0.868 ±0.00325 0.85 ±0.00342 0.859 0.094 0.316 0.926
</table>
<tableCaption confidence="0.990038">
Table 15. Traditional Chinese: Computer -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.9995795">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S1 43499 0.911 ±0.00273 0.924 ±0.00254 0.918 0.094 0.698 0.933
S8 43499 0.875 ±0.00317 0.829 ±0.00361 0.851 0.094 0.594 0.904
S15 43499 0.789 ±0.00391 0.736 ±0.00423 0.761 0.094 0.35 0.834
</table>
<tableCaption confidence="0.990939">
Table 16. Traditional Chinese: Computer -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.999621666666667">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S10 43458 0.953 ±0.00203 0.957 ±0.00195 0.955 0.075 0.798 0.966
S7 43458 0.908 ±0.00277 0.932 ±0.00242 0.92 0.075 0.771 0.919
S1 43458 0.905 ±0.00281 0.924 ±0.00254 0.914 0.075 0.725 0.919
S16 43458 0.9 ±0.00288 0.915 ±0.00268 0.908 0.075 0.668 0.919
S15 43458 0.871 ±0.00322 0.815 ±0.00373 0.842 0.075 0.115 0.932
</table>
<tableCaption confidence="0.987566">
Table 17. Traditional Chinese: Medicine -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.996383">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S1 43458 0.903 ±0.00284 0.903 ±0.00284 0.903 0.075 0.729 0.917
S8 43458 0.879 ±0.00313 0.814 ±0.00373 0.846 0.075 0.48 0.912
S15 43458 0.811 ±0.00376 0.74 ±0.00421 0.774 0.075 0.254 0.856
</table>
<tableCaption confidence="0.991942">
Table 18. Traditional Chinese: Medicine -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.992508166666667">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S10 47144 0.964 ±0.00172 0.962 ±0.00176 0.963 0.068 0.812 0.975
S7 47144 0.925 ±0.00243 0.939 ±0.00220 0.932 0.068 0.793 0.935
S16 47144 0.922 ±0.00247 0.929 ±0.00237 0.925 0.068 0.732 0.935
S1 47144 0.891 ±0.00287 0.912 ±0.00261 0.901 0.068 0.676 0.907
S15 47144 0.875 ±0.00305 0.834 ±0.00343 0.854 0.068 0.169 0.926
</table>
<tableCaption confidence="0.993264">
Table 19. Traditional Chinese: Finance -- Closed (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.979137">
Site ID Word Count R Cr P Cp F OOV Roov Riv
S1 47144 0.903 ±0.00273 0.916 ±0.00256 0.91 0.068 0.721 0.916
S8 47144 0.832 ±0.00344 0.76 ±0.00393 0.794 0.068 0.356 0.866
S15 47144 0.811 ±0.00361 0.753 ±0.00397 0.781 0.068 0.235 0.853
</table>
<tableCaption confidence="0.993819">
Table 20. Traditional Chinese: Finance -- Open (italics indicate performance below baseline)
</tableCaption>
<table confidence="0.99995319047619">
Subtask Site Track Literature Computer Medicine Finance
ID
F Roov F Roov F Roov F Roov
Simplified S1 ◆ 0.913 0.556 0.899 0.592 0.904 0.633 0.931 0.664
Chinese
◇ 0.912 0.535 0.9 0.607 0.905 0.635 0.931 0.669
S5 ◆ 0.946 0.816 0.93 0.808 0.934 0.761 0.954 0.849
◇ 0.905 0.803 0.907 0.771 0.912 0.704 0.931 0.808
S6 ◆ 0.941 0.649 0.951 0.827 0.939 0.75 0.959 0.827
◇ 0.955 0.655 0.939 0.735 0.935 0.67 0.957 0.763
S9 ◆ 0.923 0.625 0.937 0.805 0.92 0.729 0.956 0.857
◇ 0.941 0.699 0.95 0.82 0.938 0.768 0.96 0.847
S15 ◆ 0.884 0.206 0.86 0.457 0.842 0.218 0.884 0.321
◇ 0.871 0.657 0.868 0.668 0.855 0.559 0.851 0.438
S18 ◆ 0.933 0.654 0.935 0.792 0.936 0.761 0.955 0.848
◇ 0.942 0.702 0.947 0.812 0.938 0.787 0.951 0.853
Traditional S1 ◆ 0.896 0.728 0.919 0.684 0.914 0.725 0.901 0.676
Chinese
◇ 0.902 0.775 0.918 0.698 0.903 0.729 0.91 0.721
S15 ◆ 0.825 0.105 0.859 0.316 0.842 0.115 0.854 0.169
◇ 0.761 0.234 0.761 0.35 0.774 0.254 0.781 0.235
</table>
<tableCaption confidence="0.996482">
Table 21. Comparison: closed track vs. open track (◆=closed track, ◇=open track)
</tableCaption>
<table confidence="0.999223466666667">
bakeoff corpus characters OOV word closed track open track
rate count
Roov F Roov F
2007 CKIP traditional 0.074 90678 0.740 0.947 0.780 0.956
Chinese
2010 medicine 0.075 43458 0.798 0.955 0.729 0.903
2006 UPUC simplified 0.088 155K 0.707 0.933 0.768 0.944
Chinese
2010 finance 0.087 33028 0.871 0.955 0.853 0.951
2005 CityU traditional 0.074 41K 0.736 0.941 0.806 0.962
Chinese
2010 medicine 0.075 43458 0.798 0.955 0.729 0.903
2003 PK simplified 0.069 17K 0.763 0.940 0.799 0.959
Chinese
2010 literature 0.069 35736 0.816 0.946 0.814 0.952
</table>
<tableCaption confidence="0.973758">
Table 22. Comparisons of top OOV recall rates of different bakeoffs on the test corpora with similar OOV
rates (2003, 2005, 2006 and 2007 represent the SIGHAN bakeoff 2003, 2005, 2006 and 2007 respectively, and
2010 represents the CIPS-SIGHAN CLP 2010 bakeoff)
</tableCaption>
<table confidence="0.999910545454545">
Closed Track Open Track
OOV ID R P F Roov Riv ID R P F Roov Riv
S L 0.069 S5 0.945 0.946 0.946 0.816 0.954 S6 0.958 0.953 0.955 0.655 0.981
C 0.152 S6 0.953 0.95 0.951 0.827 0.975 S9 0.95 0.95 0.95 0.82 0.973
M 0.11 S6 0.942 0.936 0.939 0.75 0.965 S9 0.94 0.936 0.938 0.768 0.962
S18 0.941 0.935 0.938 0.787 0.96
F 0.087 S6 0.959 0.96 0.959 0.827 0.972 S9 0.96 0.96 0.96 0.847 0.971
T L 0.094 S10 0.942 0.942 0.942 0.788 0.958 S1 0.905 0.9 0.902 0.775 0.918
C 0.094 S10 0.948 0.957 0.952 0.666 0.977 S1 0.911 0.924 0.918 0.698 0.933
M 0.075 S10 0.953 0.957 0.955 0.798 0.966 S1 0.903 0.903 0.903 0.729 0.917
F 0.068 S10 0.964 0.962 0.963 0.812 0.975 S1 0.903 0.916 0.91 0.721 0.916
</table>
<tableCaption confidence="0.933896">
Table 23. Top performance on every subtask, domain, and track (S=simplified Chinese test, T=traditional
Chinese test, L=literature, C=computer, M=medicine, F=finance)
</tableCaption>
<sectionHeader confidence="0.993436" genericHeader="conclusions">
5 Conclusions &amp; Future Directions
</sectionHeader>
<bodyText confidence="0.999966194444445">
The CIPS-SIGHAN CLP 2010 Chinese Word
Segmentation Bakeoff successfully brought
together a collection of 18 strong research
groups to assess the progress of this
fundamental research in Chinese language
processing.
There is clearly no single best system. And
the participating sites S1, S10, S9, S6, S5 and
S18 have all achieved respectable scores on
different track runs of this bakeoff. An
improvement on the OOV recall over the prior
bakeoffs has been observed.
It is the first time to apply word
segmentation bakeoff on four domains. It’s also
the first time to use unlabeled training corpora
in the bakeoff to test the unsupervised or semi-
supervised learning ability of the segmentation
system. Unsupervised or Semi-supervised
learning needs to incorporate large amounts of
unlabeled data. We design the evaluation with
two unknown domains without any in-domain
training corpora, compared with two known
domains each with an in-domain unlabeled
training corpus. Although no significant
difference has been found, it’s still worth it. The
size of our unlabeled training corpora was too
small in this bakeoff, and we hope to improve
this in next evaluation.
The word segmentation is a necessary pre-
processing phase for the downstream processing
tasks. In future evaluations, we hope to see the
integration of word segmentation task with a
higher level task such as machine translation,
with a view to exactly evaluate the impact of
improvements in word segmentation on broader
downstream applications.
</bodyText>
<sectionHeader confidence="0.982918" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999390875">
This work is supported by the National Natural
Science Foundation of China (Grant No.
90920004). We gratefully acknowledge the
generous assistance of the organizations listed
below who provided the data and the Chinese
word segmentation standard for this bakeoff;
without their support, it could not have taken
place:
</bodyText>
<listItem confidence="0.902993333333333">
• City University of Hong Kong
• Institute for Computational Linguistics,
Beijing University, Beijing, China
</listItem>
<bodyText confidence="0.999414666666667">
We thank Le Sun for his organization of the
First CIPS-SIGHAN Joint Conference on
Chinese Language Processing of which this
bakeoff is part. We thank Lili Zhao for her
segmentation-inconsistency-checking program
and other preparation works for this bakeoff,
and thank Guanghui Luo and Siyang Cao for
the online scoring system they set up and
maintained. We thank Yajuan Lü for her helpful
suggestions on this paper. Finally we thank all
the participants for their interest and hard work
in making this bakeoff a success.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999723">
Charles Grinstead and J. Laurie Snell. 1997.
Introduction to Probability. American
Mathematical Society, Providence, RI.
Gina-Anne Levow. 2006. The third
international Chinese language processing
bakeoff: Word segmentation and named
entity recognition. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language
Processing, pages 108–117, Sydney,
Australia, July. Association for
Computational Linguistics.
Guangjin Jin and Xiao Chen. 2007. The Fourth
International Chinese Language Processing
Bakeoff: Chinese Word Segmentation,
Named Entity Recognition and Chinese POS
Tagging. In the Sixth SIGHAN Workshop
on Chinese Language Processing, pages 69-
81, Hyderabad, India.
Richard Sproat and Thomas Emerson. 2003.
The first international Chinese word
segmentation bakeoff. In The Second
SIGHAN Workshop on Chinese Language
Processing, pages 133–143, Sapporo, Japan.
Thomas Emerson. 2005. The second
international Chinese word segmentation
bakeoff. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language
Processing, pages 123–133, Jeju Island,
Korea.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549978">
<title confidence="0.8725295">The CIPS-SIGHAN CLP Chinese Word Segmentation Bakeoff</title>
<author confidence="0.845794">Hongmei Zhao</author>
<author confidence="0.845794">Qun Liu</author>
<affiliation confidence="0.9862215">Key Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of Sciences</affiliation>
<email confidence="0.834128">zhaohongmei@ict.ac.cn</email>
<email confidence="0.834128">liuqun@ict.ac.cn</email>
<abstract confidence="0.99646">The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff was held in the summer of 2010 to evaluate the current state of the art in word segmentation. It focused on the crossdomain performance of Chinese word segmentation algorithms. Eighteen groups submitted 128 results over two tracks (open training and closed training), four domains (literature, computer science, medicine and finance) and two subtasks (simplified Chinese and traditional Chinese). We found that compared with the previous Chinese word segmentation bakeoffs, the performance of cross-domain Chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charles Grinstead</author>
<author>J Laurie Snell</author>
</authors>
<title>Introduction to Probability.</title>
<date>1997</date>
<publisher>American Mathematical Society,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="11657" citStr="Grinstead and Snell, 1997" startWordPosition="1735" endWordPosition="1738">he test corpus, the recall on OOV words (Roov), and the recall on in-vocabulary words (Riv). 3.2 Raw scores All the results are presented in Tables 5-20. Column headings are as above, except for “Cr” and “Cp” for which see Section 3.3. All tables are sorted by F score. 3.3 Statistical significance of the results Following previous bakeoffs, let us assume that the recall rates represent the probability p that a word will be successfully identified, and let us further assume the binomial distribution is appropriate for this experiment. Given the Central Limit Theorem for Bernouli trials — e.g. (Grinstead and Snell, 1997), then the 95% confidence interval is given as ±2 p(1− p) / n , where n is the number of trials (words). The recall-based confidences (±2 p(1− p) / n ) are given as “Cr” in Tables 5-20. Similarly, we can assume the precision rates represent the probability that a character string that has been identified as a word is really a word. And the precision-based confidences are given as “Cp” in the tables. They can be interpreted as follows: To decide whether two systems are significantly different (at the 95% confidential level), one just has to compute whether their confidence intervals overlap. If</context>
</contexts>
<marker>Grinstead, Snell, 1997</marker>
<rawString>Charles Grinstead and J. Laurie Snell. 1997. Introduction to Probability. American Mathematical Society, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
</authors>
<title>The third international Chinese language processing bakeoff: Word segmentation and named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>108--117</pages>
<location>Sydney,</location>
<contexts>
<context position="1403" citStr="Levow, 2006" startWordPosition="191" endWordPosition="192">f cross-domain Chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved. 1 Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, successfully conducted four prior word segmentation bakeoffs, in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006) and 2007 (Jin and Chen, 2007), and the bakeoff 2007 was jointly organized with the Chinese Information Processing Society of China (CIPS). These evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system. After years of intensive researches, Chinese word segmentation has achieved a quite high precision, though the out-of-vocabulary problem is still a continuing challenge. However, the performance of segmentation is not so satisfying for out-of-domain text. The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff continues the ongoing s</context>
</contexts>
<marker>Levow, 2006</marker>
<rawString>Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108–117, Sydney,</rawString>
</citation>
<citation valid="false">
<authors>
<author>July Australia</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>Australia, </marker>
<rawString>Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangjin Jin</author>
<author>Xiao Chen</author>
</authors>
<title>The Fourth International Chinese Language Processing Bakeoff: Chinese Word Segmentation, Named Entity Recognition and Chinese POS Tagging.</title>
<date>2007</date>
<booktitle>In the Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>69--81</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1433" citStr="Jin and Chen, 2007" startWordPosition="195" endWordPosition="198">word segmentation is not much lower, and the out-of-vocabulary recall is improved. 1 Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, successfully conducted four prior word segmentation bakeoffs, in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006) and 2007 (Jin and Chen, 2007), and the bakeoff 2007 was jointly organized with the Chinese Information Processing Society of China (CIPS). These evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system. After years of intensive researches, Chinese word segmentation has achieved a quite high precision, though the out-of-vocabulary problem is still a continuing challenge. However, the performance of segmentation is not so satisfying for out-of-domain text. The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff continues the ongoing series of the SIGHAN Chinese Wo</context>
</contexts>
<marker>Jin, Chen, 2007</marker>
<rawString>Guangjin Jin and Xiao Chen. 2007. The Fourth International Chinese Language Processing Bakeoff: Chinese Word Segmentation, Named Entity Recognition and Chinese POS Tagging. In the Sixth SIGHAN Workshop on Chinese Language Processing, pages 69-81, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In The Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1361" citStr="Sproat and Emerson, 2003" startWordPosition="183" endWordPosition="186">s Chinese word segmentation bakeoffs, the performance of cross-domain Chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved. 1 Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, successfully conducted four prior word segmentation bakeoffs, in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006) and 2007 (Jin and Chen, 2007), and the bakeoff 2007 was jointly organized with the Chinese Information Processing Society of China (CIPS). These evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system. After years of intensive researches, Chinese word segmentation has achieved a quite high precision, though the out-of-vocabulary problem is still a continuing challenge. However, the performance of segmentation is not so satisfying for out-of-domain text. The CIPS-SIGHAN CLP 2010 Chinese Word Se</context>
<context position="9301" citStr="Sproat and Emerson, 2003" startWordPosition="1353" endWordPosition="1356">ish the following four character types can be used in training: Chinese characters, English letters, digits and punctuations. • In the Open training evaluation, participants can use any language resources, including the training data provided by the organizer. Participants were asked to submit their data using specific naming conventions, and from the result file name we can see in which track the result was run, as well as other necessary information. Of course, the results on both tracks are welcomed. Scoring was done automatically using a combination of Perl and shell scripts. The scripts (Sproat and Emerson, 2003, 2005) used for scoring can be downloaded from http://www.sighan.org/bakeoff2005/. The bakeoff organizer provided an on-line scoring system to all the participants who had submitted their bakeoff results for their follow-up experiments. 2.3 Participating sites Eighteen sites submitted results and a technical report. Mainland China had the greatest number with 14, followed by Taiwan (2), the United States (1) and Australia (1). A summary of participating groups and the tracks for which they submitted results can be found in Table 2 on the preceding page. There are more sites who had registered</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international Chinese word segmentation bakeoff. In The Second SIGHAN Workshop on Chinese Language Processing, pages 133–143, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international Chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1383" citStr="Emerson, 2005" startWordPosition="188" endWordPosition="189">ffs, the performance of cross-domain Chinese word segmentation is not much lower, and the out-of-vocabulary recall is improved. 1 Introduction Chinese is written without inter-word spaces, so finding word-boundaries is an essential first step in many natural language processing tasks ranging from part of speech tagging to parsing, reference resolution and machine translation. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, successfully conducted four prior word segmentation bakeoffs, in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006) and 2007 (Jin and Chen, 2007), and the bakeoff 2007 was jointly organized with the Chinese Information Processing Society of China (CIPS). These evaluations established benchmarks for word segmentation with which researchers evaluate their segmentation system. After years of intensive researches, Chinese word segmentation has achieved a quite high precision, though the out-of-vocabulary problem is still a continuing challenge. However, the performance of segmentation is not so satisfying for out-of-domain text. The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff con</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 123–133, Jeju Island, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>