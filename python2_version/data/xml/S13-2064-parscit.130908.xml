<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030010">
<title confidence="0.909685">
USNA: A Dual-Classifier Approach to Contextual Sentiment Analysis
</title>
<author confidence="0.849446">
Ganesh Harihara and Eugene Yang and Nathanael Chambers
</author>
<affiliation confidence="0.599989">
United States Naval Academy
Annapolis, MD 21401, USA
</affiliation>
<email confidence="0.998651">
nchamber@usna.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999290684210526">
This paper describes a dual-classifier ap-
proach to contextual sentiment analysis at the
SemEval-2013 Task 2. Contextual analysis of
polarity focuses on a word or phrase, rather
than the broader task of identifying the senti-
ment of an entire text. The Task 2 definition
includes target word spans that range in size
from a single word to entire sentences. How-
ever, the context of a single word is depen-
dent on the word’s surrounding syntax, while a
phrase contains most of the polarity within it-
self. We thus describe separate treatment with
two independent classifiers, outperforming the
accuracy of a single classifier. Our system
ranked 6th out of 19 teams on SMS message
classification, and 8th of 23 on twitter data.
We also show a surprising result that a very
small amount of word context is needed for
high-performance polarity extraction.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998986466666667">
A variety of approaches to sentiment analysis have
been proposed in the literature. Early work sought to
identify the general sentiment of entire documents,
but a recent shift to social media has provided a large
quantity of publicly available data, and private orga-
nizations are increasingly interested in how a pop-
ulation “feels” toward its products. Identifying the
polarity of language toward a particular topic, how-
ever, no longer requires identifying the sentiment of
an entire text, but rather the contextual sentiment
surrounding a target phrase.
Identifying the polarity of text toward a phrase is
significantly different from a sentence’s overall po-
larity, as seen in this example from the SemEval-
2013 Task 2 (Wilson et al., 2013) training set:
</bodyText>
<construct confidence="0.7203955">
I had a severe nosebleed last night. I think
my iPad caused it as I was browsing for a
few hours on it. Anyhow, its stopped, which
is good.
</construct>
<bodyText confidence="0.999961518518519">
An ideal sentiment classifier would classify this
text as overall positive (the nosebleed stopped!), but
this short snippet actually contains three types of po-
larity (positive, negative, and neutral). The middle
sentence about the iPad is not positive, but neutral.
The word ‘nosebleed’ has a very negative polarity
in this context, and the phrase ‘its stopped’ is posi-
tive. Someone interested in specific health concerns,
such as nosebleeds, needs a contextual classifier to
identify the desired polarity in this context.
This example also illustrates how phrases of dif-
ferent sizes require unique handling. Single token
phrases, such as ‘nosebleed’, are highly dependent
on the surrounding context for its polarity. How-
ever, the polarity of the middle iPad sentence is con-
tained within the phrase itself. The surrounding con-
text is not as important. This paper thus proposes
a dual-classifier that trains two separate classifiers,
one for single words, and another for phrases. We
empirically show that unique features apply to both,
and both benefit from independent training. In fact,
we show a surprising result that a very small win-
dow size is needed for the context of single word
phrases. Our system performs well on the SemEval
task, placing 8th of 23 systems on twitter text. It also
shows strong generalization to SMS text messages,
placing 6th of 19.
</bodyText>
<page confidence="0.952238">
390
</page>
<bodyText confidence="0.805385">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 390–394, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.986445" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999762568627451">
Sentiment analysis is a large field applicable to
many genres. This paper focuses on social media
(microblogs) and contextual polarity, so we only
address the closest work in those areas. For a
broader perspective, several survey papers are avail-
able (Pang and Lee, 2008; Tang et al., 2009; Liu and
Zhang, 2012; Tsytsarau and Palpanas, 2012).
Microblogs serve as a quick way to measure a
large population’s mood and opinion. Many differ-
ent sources have been used. O’Connor et al. (2010)
used Twitter data to compute a ratio of positive and
negative words to measure consumer confidence and
presidential approval. Kramer (2010) counted lex-
icon words on Facebook for a general ’happiness’
measure, and Thelwall (2011) built a general senti-
ment model on MySpace user comments. These are
general sentiment algorithms.
Specific work on microblogs has focused on find-
ing noisy training data with distant supervision.
Many of these algorithms use emoticons as seman-
tic indicators of polarity. For instance, a tweet that
contains a sad face likely contains a negative polar-
ity (Read, 2005; Go et al., 2009; Bifet and Frank,
2010; Pak and Paroubek, 2010; Davidov et al., 2010;
Kouloumpis et al., 2011). In a similar vein, hash-
tags can also serve as noisy labels (Davidov et al.,
2010; Kouloumpis et al., 2011). Most work on dis-
tant supervision relies on a variety of syntactic and
word-based features (Marchetti-Bowick and Cham-
bers, 2012). We adopt many of these features.
Supervised learning for contextual sentiment
analysis has not been thoroughly investigated. La-
beled data for specific words or queries is expensive
to generate, so Jiang et al. (2011) is one of the few
approaches with labeled training data. Earlier work
on product reviews sought the sentiment toward par-
ticular product features. These systems used rule
based approaches based on parts of speech and other
surface features (Nasukawa and Yi, 2003; Hu and
Liu, 2004; Ding and Liu, 2007).
Finally, topic identification in microblogs is also
related. The first approaches are somewhat simple,
selecting single keywords (e.g., “Obama”) to rep-
resent the topic (e.g., “US President”), and retrieve
tweets that contain the word (O’Connor et al., 2010;
Tumasjan et al., 2010; Tan et al., 2011). These sys-
tems then classify the polarity of the entire tweet,
and ignore the question of polarity toward the partic-
ular topic. This paper focuses on the particular key-
word or phrase, and identifies the sentiment toward
that phrase, not the overall sentiment of the text.
</bodyText>
<sectionHeader confidence="0.996727" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.944421166666667">
This paper uses three polarity classes: positive, neg-
ative, and neutral. We developed all algorithms on
the ‘Task A’ corpora provided by SemEval-2013
Task 2 (Wilson et al., 2013). Both training and de-
velopment sets were provided, and an unseen test
set was ultimately used to evaluate the final systems.
The number of tweets in each set are shown here:
positive negative neutral
training 5348 2817 422
development 648 430 57
test (tweet) 2734 1541 160
test (sms) 1071 1104 159
</bodyText>
<sectionHeader confidence="0.987327" genericHeader="method">
4 Contextual Sentiment Analysis
</sectionHeader>
<bodyText confidence="0.993633611111111">
Contextual sentiment analysis focuses on the dispo-
sition of a certain word or groups of words. Most
data-driven approaches rely on a labeled corpus to
drive the learning process, and this paper is no dif-
ferent. However, we propose a novel approach to
contextual analysis that differentiates between sin-
gle words and phrases.
The semantics of a single word in context from
that of a phrase are fundamentally different. Since
one word will have multiple contexts and is heavily
influenced by the surrounding words, more consid-
eration is given to adjacent words. A phrase often
carries its own semantics, so has less variability in
its meaning based on its context. Context is still im-
portant, but we propose separate classifiers in order
to learn weights unique to tokens and phrases. The
following describes the two unique feature sets. We
trained a Maximum Entropy classifier for each set.
</bodyText>
<subsectionHeader confidence="0.988149">
4.1 Text Pre-Processing
</subsectionHeader>
<bodyText confidence="0.9998414">
All text is lowercased, and twitter usernames (e.g.,
@user) and URLs are replaced with placeholder to-
kens. The text is then split on whitespace. We also
prepend the occurrence of token “not” to the subse-
quent token, merging the two (e.g., “not happy” be-
</bodyText>
<page confidence="0.994081">
391
</page>
<bodyText confidence="0.999824">
comes “not-happy”). We also found that removing
prefix and affix punctuation from each token, and
storing the punctuation for later use in punctuation
features boosts performance. These cleaned tokens
are the input to the features described below.
</bodyText>
<subsectionHeader confidence="0.986731">
4.2 Single Word Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.9998258">
Assigning polarity to a single word mainly requires
features that accurately capture the surrounding con-
text. In fact, many single words do no carry any po-
larity in isolation, but solely require context. Take
the following two examples:
</bodyText>
<construct confidence="0.958538">
Justin LOVE YA so excited for the concert in
october MEXICO LOVES YOU
Im not getting on twitter tomorrow because all
my TL will consist of is a bunch of girls talking
about Justin Beiber
</construct>
<bodyText confidence="0.965328807692308">
In these examples, Justin is the name of a singer
who does not carry an initial polarity. The first tweet
is clearly positive toward him, while the second is
not. Our single-token classifier used the following
set of features to capture these different contexts:
Target Token: The first features are the unigram
and bigram ending with the target token. We attach a
unique string to each to distinguish it from the text’s
other n-grams. We also include a feature for any
punctuation that was attached to the end of the token
(e.g., ’Justin!’ generates ’!’ as a feature).
Target Patterns: This feature generalizes the n-
grams that include the target word. It replaces the
target word with a variable in an effort to capture
general patterns that indicate sentiment. For in-
stance, using the first tweet above, we add the tri-
gram ‘&lt;s&gt; LOVE’ and two bigrams, ‘&lt;s&gt; ’
and ‘ LOVE’.
Unigrams, Bigrams, Trigrams: We include all
other n-grams in the text within a window of size
n from the target token.
Dictionary Matching: We have two binary fea-
tures, postivemood and negativemood, that indicate
if any word in the text appears in a sentiment lex-
icon’s positive or negative list. We use Bing Liu’s
Opinion Lexicon1.
</bodyText>
<footnote confidence="0.9236815">
1http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html\#lexicon
</footnote>
<bodyText confidence="0.994410857142857">
Punctuation Features: We included a binary fea-
ture for the presence or absence of exclamation
marks anywhere in the text. Further, we generate
a feature for punctuation at the end of the text.
Emoticons: We included two binary features for the
presence or absence of a smiley face and sad face
emoticon.
</bodyText>
<subsectionHeader confidence="0.998408">
4.3 Phrasal Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.990207928571429">
We adopted several single word features for use in
phrases, including punctuation, dictionary match-
ing, and emoticons. However, since phrasal analy-
sis is often less dependent on context and more de-
pendent on the phrase itself, we altered the n-gram
features to be unique to the phrase. The following
features are solely used for target phrases, not single
words:
Unigrams, Bigrams, Trigrams: We include all n-
grams in the target phrase only. This differs from
the single token features that included n-grams from
a surrounding window.
Phrasal Punctuation: If the target phrase ends with
any type of punctuation, we include it as a feature.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999835">
Initial model design and feature tuning was con-
ducted on the SemEval-2013 Task 2 training set for
training, and its dev set for evaluation. We split the
data into two parts: tweets with single word targets,
and tweets with target phrases. We trained two Max-
Ent classifiers using the Stanford JavaNLP toolkit2.
Each datum in the test set is labeled using the appro-
priate classifier based on the target phrase’s length.
The first experiments are ablation over the fea-
tures described in Section 4, separately improving
the single token and phrasal classifiers. Results are
reported in Table 1 using simple accuracy on the de-
velopment set. We initially do not split off punc-
tuation, and use only unigram features for phrases.
The window size is initally infinite (i.e., the entire
text is used for n-grams). Bigrams and trigrams hurt
performance and are not shown. Reducing the win-
dow size to a single token (ignore the entire tweet)
increased performance by 1.2%, and stripping punc-
tuation off tokens by another 1.9%. The perfor-
</bodyText>
<footnote confidence="0.95035">
2http://nlp.stanford.edu/software/index.shtml
</footnote>
<page confidence="0.991969">
392
</page>
<table confidence="0.990376321428571">
Twitter Dataset
F1 Score
Top System (1st) 88.9
This Paper (8th) 81.3
Majority Baseline (20th) 61.6
Bottom System (24th) 34.7
SMS Dataset
F1 Score
Top System (1st) 88.4
This Paper (6th) 79.8
Majority Baseline (19th) 47.3
Min System (20th) 36.4
d SMS Data.
Single Token Features
Just Unigrams 70.5
+ Target Token Patterns 70.4
+ Sentiment Lexicon 71.5
+ Target Token N-Grams 73.3
+ EOS punctuation 73.2
+ Emoticons 73.3
Set Window Size = 1 74.5
Strip punctuation off tokens 76.4
Phrasal Features
Just Unigrams 76.4
+ Emoticons 76.3
+ EOS punctuation 76.6
+ Exclamation Marks 76.5
+ Sentiment Lexicon 77.7
</table>
<tableCaption confidence="0.8278014">
Table 1: Feature ablation in order. Single token features
begin with unigrams only, holding phrasal features con-
stant at unigrams only. The phrasal table picks up where
the single token table finishes. Each row uses all features
added in previous rows.
</tableCaption>
<table confidence="0.736500666666667">
Dual-Classifier Comparison
Single Classifier 76.6%
Dual-Classifier 77.7%
</table>
<tableCaption confidence="0.9970965">
Table 2: Performance increase from splitting into two
classifiers. Accuracy reported on the development set.
</tableCaption>
<bodyText confidence="0.999942428571429">
mance increase with phrasal features is 1.3% abso-
lute, whereas token features contributed 5.9%.
After choosing the optimum set of features based
on ablation, we then retrained the classifiers on both
the training and development sets as one large train-
ing corpus. The SemEval-2013 Task 2 competition
included two datasets for testing: tweets and SMS
messages. Official results for both are given in Ta-
ble 3 using the F1 measure.
Finally, we compare our dual-classifier to a single
standard classifier. We use the same features used
in Table 1, train on the training set, and report accu-
racy on the development set. See Table 2. Our dual
classifier improves relative accuracy by 1.4%.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.995161333333333">
One of the main surprises from our experiments was
that a large portion of text could be ignored with-
out hurting classification performance. We reduced
</bodyText>
<tableCaption confidence="0.992147">
Table 3: Performance on Twitter an
</tableCaption>
<bodyText confidence="0.999269228571429">
the window size in which n-grams are extracted to
size one, and performance actually increases 1.2%.
At least for single word target phrases, including n-
grams of the entire tweet/sms is not helpful. We
only used n-gram patterns that included the token
and its two immediate neighbors. A nice side ben-
efit is that the classifier contains fewer features, and
trains faster as a result.
The decision to use two separate classifiers helped
performance, improving by 1.4% relative accuracy
on the development set. The decision was moti-
vated by the observation that the polarity of a token
is dependent on its surrounding context, but a longer
phrase is dependent more on its internal syntax. This
allowed us to make finer-grained feature decisions,
and the feature ablation experiments suggest our ob-
servation to be true. Better feature weights are ulti-
mately learned for the unique tasks.
Finally, the feature ablation experiments revealed
a few key takeaways for feature engineering: bi-
grams and trigrams hurt classification, using a win-
dow size is better than the entire text, and punctu-
ation should always be split off tokens. Further, a
sentiment lexicon reliably improves both token and
phrasal classification.
Opportunities for future work on contextual anal-
ysis exist in further analysis of the feature window
size. Why
more context help token classifi-
cation? Do n-grams simply lack the deeper seman-
tics needed, or are these supervised algorithms still
suffering from sparse training data? Better sentence
an
doesn’t
d phrase detection may be a fruitful focus.
</bodyText>
<page confidence="0.993364">
393
</page>
<sectionHeader confidence="0.989999" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999256180722892">
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Lecture
Notes in Computer Science, volume 6332, pages 1–15.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING
2010).
Xiaowen Ding and Bing Liu. 2007. The utility of lin-
guistic rules in opinion mining. In Proceedings of
SIGIR-2007, pages 23–27.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the Association for Com-
putational Linguistics (ACL-2011).
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media.
Adam D. I. Kramer. 2010. An unobtrusive behavioral
model of ‘gross national happiness’. In Proceedings of
the 28th International Conference on Human Factors
in Computing Systems (CHI 2010).
Bing Liu and Lei Zhang. 2012. A survey of opinion min-
ing and sentiment analysis. Mining Text Data, pages
415–463.
Micol Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant supervi-
sion: Political forecasting with twitter. In Proceedings
of the 13th Conference of the European Chapter of the
Association for Computational Linguistics.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In Proceedings of K-CAP.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of the AAAI
Conference on Weblogs and Social Media.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
On Language Resources and Evaluation (LREC).
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval.
Jonathon Read. 2005. Using emoticons to reduce depen-
dency in machine learning techniques for sentiment
classification. In Proceedings of the ACL Student Re-
search Workshop (ACL-2005).
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings
of the 17th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining.
H. Tang, S. Tan, and X. Cheng. 2009. A survey on senti-
ment detection of reviews. Expert Systems with Appli-
cations.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406–418.
M. Tsytsarau and T. Palpanas. 2012. Survey on mining
subjective data on the web. Data Mining and Knowl-
edge Discovery Journal, 24(3):478–514.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Election forecasts
with twitter: How 140 characters reflect the political
landscape. Social Science Computer Review.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
</reference>
<page confidence="0.998985">
394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932713">
<title confidence="0.999899">USNA: A Dual-Classifier Approach to Contextual Sentiment Analysis</title>
<author confidence="0.998462">Harihara Yang</author>
<affiliation confidence="0.986611">United States Naval</affiliation>
<address confidence="0.961424">Annapolis, MD 21401,</address>
<email confidence="0.99967">nchamber@usna.edu</email>
<abstract confidence="0.99900435">This paper describes a dual-classifier approach to contextual sentiment analysis at the SemEval-2013 Task 2. Contextual analysis of polarity focuses on a word or phrase, rather than the broader task of identifying the sentiment of an entire text. The Task 2 definition includes target word spans that range in size from a single word to entire sentences. However, the context of a single word is dependent on the word’s surrounding syntax, while a phrase contains most of the polarity within itself. We thus describe separate treatment with two independent classifiers, outperforming the accuracy of a single classifier. Our system ranked 6th out of 19 teams on SMS message classification, and 8th of 23 on twitter data. We also show a surprising result that a very small amount of word context is needed for high-performance polarity extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<volume>6332</volume>
<pages>1--15</pages>
<contexts>
<context position="4712" citStr="Bifet and Frank, 2010" startWordPosition="751" endWordPosition="754"> data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Lecture Notes in Computer Science, volume 6332, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="4758" citStr="Davidov et al., 2010" startWordPosition="759" endWordPosition="762">ve words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward p</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
</authors>
<title>The utility of linguistic rules in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR-2007,</booktitle>
<pages>23--27</pages>
<contexts>
<context position="5540" citStr="Ding and Liu, 2007" startWordPosition="887" endWordPosition="890">ision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). These systems then classify the polarity of the entire tweet, and ignore the question of polarity toward the particular topic. This paper focuses on the particular keyword or phrase, and identifies the sentiment toward that phrase, not the overall sentiment of the text. 3 Dataset This paper uses t</context>
</contexts>
<marker>Ding, Liu, 2007</marker>
<rawString>Xiaowen Ding and Bing Liu. 2007. The utility of linguistic rules in opinion mining. In Proceedings of SIGIR-2007, pages 23–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report.</tech>
<contexts>
<context position="4689" citStr="Go et al., 2009" startWordPosition="747" endWordPosition="750">010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled trainin</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="5519" citStr="Hu and Liu, 2004" startWordPosition="883" endWordPosition="886"> on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). These systems then classify the polarity of the entire tweet, and ignore the question of polarity toward the particular topic. This paper focuses on the particular keyword or phrase, and identifies the sentiment toward that phrase, not the overall sentiment of the text. 3 Data</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL-2011).</booktitle>
<contexts>
<context position="5239" citStr="Jiang et al. (2011)" startWordPosition="837" endWordPosition="840">face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of the Association for Computational Linguistics (ACL-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4784" citStr="Kouloumpis et al., 2011" startWordPosition="763" endWordPosition="766">nsumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg! In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam D I Kramer</author>
</authors>
<title>An unobtrusive behavioral model of ‘gross national happiness’.</title>
<date>2010</date>
<booktitle>In Proceedings of the 28th International Conference on Human Factors in Computing Systems (CHI</booktitle>
<contexts>
<context position="4218" citStr="Kramer (2010)" startWordPosition="672" endWordPosition="673">lysis is a large field applicable to many genres. This paper focuses on social media (microblogs) and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Microblogs serve as a quick way to measure a large population’s mood and opinion. Many different sources have been used. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can </context>
</contexts>
<marker>Kramer, 2010</marker>
<rawString>Adam D. I. Kramer. 2010. An unobtrusive behavioral model of ‘gross national happiness’. In Proceedings of the 28th International Conference on Human Factors in Computing Systems (CHI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Lei Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis. Mining Text Data,</title>
<date>2012</date>
<pages>415--463</pages>
<contexts>
<context position="3902" citStr="Liu and Zhang, 2012" startWordPosition="620" endWordPosition="623">SMS text messages, placing 6th of 19. 390 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 390–394, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2 Previous Work Sentiment analysis is a large field applicable to many genres. This paper focuses on social media (microblogs) and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Microblogs serve as a quick way to measure a large population’s mood and opinion. Many different sources have been used. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision.</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. Mining Text Data, pages 415–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micol Marchetti-Bowick</author>
<author>Nathanael Chambers</author>
</authors>
<title>Learning for microblogs with distant supervision: Political forecasting with twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5021" citStr="Marchetti-Bowick and Chambers, 2012" startWordPosition="802" endWordPosition="806">entiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approach</context>
</contexts>
<marker>Marchetti-Bowick, Chambers, 2012</marker>
<rawString>Micol Marchetti-Bowick and Nathanael Chambers. 2012. Learning for microblogs with distant supervision: Political forecasting with twitter. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Nasukawa</author>
<author>Jeonghee Yi</author>
</authors>
<title>Sentiment analysis: capturing favorability using natural language processing.</title>
<date>2003</date>
<booktitle>In Proceedings of K-CAP.</booktitle>
<contexts>
<context position="5501" citStr="Nasukawa and Yi, 2003" startWordPosition="879" endWordPosition="882">t al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). These systems then classify the polarity of the entire tweet, and ignore the question of polarity toward the particular topic. This paper focuses on the particular keyword or phrase, and identifies the sentiment toward that phrase, not the overall sentiment o</context>
</contexts>
<marker>Nasukawa, Yi, 2003</marker>
<rawString>Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: capturing favorability using natural language processing. In Proceedings of K-CAP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference On Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="4736" citStr="Pak and Paroubek, 2010" startWordPosition="755" endWordPosition="758">o of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought </context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference On Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</title>
<date>2008</date>
<contexts>
<context position="3862" citStr="Pang and Lee, 2008" startWordPosition="612" endWordPosition="615">It also shows strong generalization to SMS text messages, placing 6th of 19. 390 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 390–394, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2 Previous Work Sentiment analysis is a large field applicable to many genres. This paper focuses on social media (microblogs) and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Microblogs serve as a quick way to measure a large population’s mood and opinion. Many different sources have been used. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using emoticons to reduce dependency in machine learning techniques for sentiment classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop (ACL-2005).</booktitle>
<contexts>
<context position="4672" citStr="Read, 2005" startWordPosition="745" endWordPosition="746">or et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches wit</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon Read. 2005. Using emoticons to reduce dependency in machine learning techniques for sentiment classification. In Proceedings of the ACL Student Research Workshop (ACL-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Jie Tang</author>
<author>Long Jiang</author>
<author>Ming Zhou</author>
<author>Ping Li</author>
</authors>
<title>User-level sentiment analysis incorporating social networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="5840" citStr="Tan et al., 2011" startWordPosition="933" endWordPosition="936">ng et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). These systems then classify the polarity of the entire tweet, and ignore the question of polarity toward the particular topic. This paper focuses on the particular keyword or phrase, and identifies the sentiment toward that phrase, not the overall sentiment of the text. 3 Dataset This paper uses three polarity classes: positive, negative, and neutral. We developed all algorithms on the ‘Task A’ corpora provided by SemEval-2013 Task 2 (Wilson et al., 2013). Both training and development sets were provided, and an unseen test set was ultimately used to evaluate the final systems. The number of</context>
</contexts>
<marker>Tan, Lee, Tang, Jiang, Zhou, Li, 2011</marker>
<rawString>Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li. 2011. User-level sentiment analysis incorporating social networks. In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tang</author>
<author>S Tan</author>
<author>X Cheng</author>
</authors>
<title>A survey on sentiment detection of reviews. Expert Systems with Applications.</title>
<date>2009</date>
<contexts>
<context position="3881" citStr="Tang et al., 2009" startWordPosition="616" endWordPosition="619"> generalization to SMS text messages, placing 6th of 19. 390 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 390–394, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2 Previous Work Sentiment analysis is a large field applicable to many genres. This paper focuses on social media (microblogs) and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Microblogs serve as a quick way to measure a large population’s mood and opinion. Many different sources have been used. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with</context>
</contexts>
<marker>Tang, Tan, Cheng, 2009</marker>
<rawString>H. Tang, S. Tan, and X. Cheng. 2009. A survey on sentiment detection of reviews. Expert Systems with Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment in twitter events.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>2</issue>
<marker>Thelwall, Buckley, Paltoglou, 2011</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2011. Sentiment in twitter events. Journal of the American Society for Information Science and Technology, 62(2):406–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tsytsarau</author>
<author>T Palpanas</author>
</authors>
<title>Survey on mining subjective data on the web.</title>
<date>2012</date>
<journal>Data Mining and Knowledge Discovery Journal,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="3933" citStr="Tsytsarau and Palpanas, 2012" startWordPosition="624" endWordPosition="627">acing 6th of 19. 390 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 390–394, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics 2 Previous Work Sentiment analysis is a large field applicable to many genres. This paper focuses on social media (microblogs) and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Microblogs serve as a quick way to measure a large population’s mood and opinion. Many different sources have been used. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use e</context>
</contexts>
<marker>Tsytsarau, Palpanas, 2012</marker>
<rawString>M. Tsytsarau and T. Palpanas. 2012. Survey on mining subjective data on the web. Data Mining and Knowledge Discovery Journal, 24(3):478–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Election forecasts with twitter: How 140 characters reflect the political landscape.</title>
<date>2010</date>
<journal>Social Science Computer Review.</journal>
<contexts>
<context position="5821" citStr="Tumasjan et al., 2010" startWordPosition="929" endWordPosition="932">ive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). These systems then classify the polarity of the entire tweet, and ignore the question of polarity toward the particular topic. This paper focuses on the particular keyword or phrase, and identifies the sentiment toward that phrase, not the overall sentiment of the text. 3 Dataset This paper uses three polarity classes: positive, negative, and neutral. We developed all algorithms on the ‘Task A’ corpora provided by SemEval-2013 Task 2 (Wilson et al., 2013). Both training and development sets were provided, and an unseen test set was ultimately used to evaluate the final sys</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Election forecasts with twitter: How 140 characters reflect the political landscape. Social Science Computer Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1803" citStr="Wilson et al., 2013" startWordPosition="283" endWordPosition="286">neral sentiment of entire documents, but a recent shift to social media has provided a large quantity of publicly available data, and private organizations are increasingly interested in how a population “feels” toward its products. Identifying the polarity of language toward a particular topic, however, no longer requires identifying the sentiment of an entire text, but rather the contextual sentiment surrounding a target phrase. Identifying the polarity of text toward a phrase is significantly different from a sentence’s overall polarity, as seen in this example from the SemEval2013 Task 2 (Wilson et al., 2013) training set: I had a severe nosebleed last night. I think my iPad caused it as I was browsing for a few hours on it. Anyhow, its stopped, which is good. An ideal sentiment classifier would classify this text as overall positive (the nosebleed stopped!), but this short snippet actually contains three types of polarity (positive, negative, and neutral). The middle sentence about the iPad is not positive, but neutral. The word ‘nosebleed’ has a very negative polarity in this context, and the phrase ‘its stopped’ is positive. Someone interested in specific health concerns, such as nosebleeds, ne</context>
<context position="6301" citStr="Wilson et al., 2013" startWordPosition="1010" endWordPosition="1013">ama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). These systems then classify the polarity of the entire tweet, and ignore the question of polarity toward the particular topic. This paper focuses on the particular keyword or phrase, and identifies the sentiment toward that phrase, not the overall sentiment of the text. 3 Dataset This paper uses three polarity classes: positive, negative, and neutral. We developed all algorithms on the ‘Task A’ corpora provided by SemEval-2013 Task 2 (Wilson et al., 2013). Both training and development sets were provided, and an unseen test set was ultimately used to evaluate the final systems. The number of tweets in each set are shown here: positive negative neutral training 5348 2817 422 development 648 430 57 test (tweet) 2734 1541 160 test (sms) 1071 1104 159 4 Contextual Sentiment Analysis Contextual sentiment analysis focuses on the disposition of a certain word or groups of words. Most data-driven approaches rely on a labeled corpus to drive the learning process, and this paper is no different. However, we propose a novel approach to contextual analysi</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>