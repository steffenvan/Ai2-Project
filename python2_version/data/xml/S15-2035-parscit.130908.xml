<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037066">
<title confidence="0.950187">
HITSZ-ICRC: Exploiting Classification Approach for Answer Selection
in Community Question Answering
</title>
<author confidence="0.9917865">
Yongshuai Hou, Cong Tan, Xiaolong Wang
Yaoyun Zhang, Jun Xu and Qingcai Chen
</author>
<affiliation confidence="0.9871555">
Key Laboratory of Network Oriented Intelligent Computation
Department of Computer Science and Technology
Harbin Institute of Technology Shenzhen Graduate School
HIT Campus, The University Town of Shenzhen, Shenzhen, 518055, China
</affiliation>
<email confidence="0.976147">
{yongshuai.hou, viptancong}@gmail.com, wangxl@insun.hit.edu.cn
{xiaoni5122, hit.xujun,qingcai.chen}@gmail.com
</email>
<sectionHeader confidence="0.998535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999573411764706">
This paper describes the participation of the
HITSZ-ICRC team on the Answer Selection
Challenge in SemEval-2015. Our team parti-
cipated in English subtask A, English subtask
B and Arabic task. Two approaches, ensemble
learning and hierarchical classification were
proposed for answer selection in each task.
Bag-of-words features, lexical features and
non-textual features were employed. For the
Arabic task, features were extracted from both
Arabic data and English data that translated
from the Arabic data. Evaluation demonstrat-
ed that the proposed methods were effective,
achieving a macro-averaged F1 of 56.41%
(rank 2nd) in English subtask A, 53.60 % (rank
3rd) in English subtask B and 67.70% (rank 3rd)
in Arabic task, respectively.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864341463415">
In recent years, community question answering
(CQA) systems are becoming more and more pop-
ular on the Internet. By using CQA system, a user
can post his/her question on CQA portal and re-
ceive answers from other users. All users can post
questions and answers on CQA portal freely. Al-
though it makes CQA users to get answers easily,
the answer quality evaluation becomes a challenge
for questions with multiple answers. To reduce the
inconvenient in going through plenty of candidate
answers, it makes sense to evaluate the quality of
answers and select high-quality answers automati-
cally for CQA systems. As a consequently, the task
of answer quality evaluation and answer selection
in CQA have attracted more and more attention in
recent years (Arai and Handayani, 2013; Shah and
Pomerantz, 2010; Agichtein et al., 2008).
The Answer Selection in CQA challenge was
opened as one new task in SemEval-2015: SemEv-
al-2015 Task 3 (Màrquez et al., 2015). It created a
venue and provided annotated datasets for re-
searchers to compare their methods for answer se-
lection in CQA. This challenge consisted of
Subtask A and Subtask B. Subtask A required par-
ticipant system to classify answers as relevant, po-
tentially useful and bad for each question. Subtask
B required participant system to decide whether
the answer to a YES_IO question should be Yes,
Io or Unsure based on the answer list. Subtask A
was offered for two languages: English and Arabic.
Data for the two languages was in different data set
format. In remainder of this paper, Subtask A in
English is abbreviated to English subtask A, Sub-
task A in Arabic is abbreviated to Arabic task and
Subtask B in English is abbreviated to English sub-
task B.
HITSZ-ICRC team participated in English sub-
task A, English subtask B and Arabic task. This
paper describes the ensemble learning method and
hierarchical classification method proposed for
each subtask in SemEval-2015 Task 3.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="method">
2 Methods for Answer Classification
</sectionHeader>
<bodyText confidence="0.998559666666667">
Different classification methods were tried by pre-
vious researchers for answer evaluation, prediction
and selection in CQA. Jeon et al. (2006) designed a
framework using non-textual features, most of
which were user profile features, to predict the
document quality and tried the framework on CQA.
</bodyText>
<page confidence="0.977914">
196
</page>
<note confidence="0.77007">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 196–202,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999850222222222">
Shah and Pomerantz (2010) used text, user infor-
mation and answer rank features to evaluate and
predict answer quality. Arai and Handayani (2013)
tried non-textual features mainly include no-
content features of text to train models to predict
answer quality in CQA. For SemEval-2015 Task 3,
we proposed ensemble learning method and hierar-
chical classification method to classify answers for
each task.
</bodyText>
<subsectionHeader confidence="0.988274">
2.1 English subtask A
</subsectionHeader>
<bodyText confidence="0.98979132">
English subtask A required participant system to
classify each answer of test questions as definitely
relevant (good), potentially useful (potential) or
bad (bad, dialog, non-English and other).
Features employed to train classifiers for Eng-
lish subtask A include:
Word length features: length of the max length
word, average word length.
Word number features: word number, capital
word number, polite word number, word “yes”
number, word “no” number, word “thank” num-
ber.
Punctuation features: question mark number,
exclamation mark number.
Sentence features: average sentence length,
sentence number.
Part-of-speech features: noun word number
and ratio, verb word number and ratio, pronoun
word number and ratio, WH word number and ra-
tio.
Name entity feature: number of name entity.
Content tag features: number of web link and
number of image link contained in content.
The 7 groups features in the upper list were ex-
tracted separately on questions and answers.
Answer position in Answer list: whether the
answer is first, whether the answer is last.
User id features: whether user id of answer is
the question user id, whether the user id of pre-
vious answer is question user id, whether the user
id of next answer is question user id.
Answer and question correlative features:
number and ratio of same n-gram terms between
answer and question, cosine similarity between
answer body and question body, KL distance be-
tween answer body and question body.
Class tag features: QCATEGORY tag of ques-
tion, QTYPE tag of question.
Frequent n-gram term features: frequent uni-
gram terms, bigram terms and trigram terms.
Two methods were proposed to classify answers
for English subtask A: (1) two-level hierarchical
classification: classifying answers as
good_potential and bad_dialog in the first level;
classifying good_potential answers as good and
potential, classifying bad_dialog answers as bad
and dialog separately in the second level; (2) en-
semble learning: training and choosing top N best
classifiers based on cross validation on training
data, then using the N classifiers to vote final result.
</bodyText>
<subsectionHeader confidence="0.996747">
2.2 English subtask B
</subsectionHeader>
<bodyText confidence="0.999964433333333">
The English subtask B required participant system
to give “Yes”, “1o” or “Unsure” answer directly to
a YES_1O question based on its candidate answers.
Evidence to answer YES_1O question is the
yes/no opinion of each good answer in answer list.
YES_1O question answering can be split into three
steps: first, finding out good answers from candi-
date answers; second, classifying each good an-
swer into yes, no or unsure based on its opinion;
third, summarizing final answer for YES_1O ques-
tion according to opinions of all good answers.
Given a YES_1O question, recognizing good
answers can be achieved with the classifiers
trained in English subtask A; final answer is pre-
dicted based on the comparison between the num-
ber of yes class answers and the number of no class
answers in answer list of the question. So the re-
maining task for YES_1O question answering is
good answer classification according to the opinion.
Two methods were proposed for answer opinion
classification: (1) piping the best performance clas-
sifier for answer selection and the best classifier
for answer opinion classification; (2) classifying
answers of YES_1O question into 5 classes with
single classifier: yes, no, unsure, bad and dialogue.
Feature extraction for English subtask A was
same as English subtask A. Features employed
were selected according to gain ratio. We proposed
ensemble learning method for the answer classifi-
cation in English subtask B.
</bodyText>
<subsectionHeader confidence="0.999028">
2.3 Arabic task
</subsectionHeader>
<bodyText confidence="0.998637714285714">
Dataset for Arabic task is in Arabic. The task re-
quired participant system to classify answers of
question into definitely relevant (direct), potential-
ly useful (related) and bad (irrelevant).
Features extracted for Arabic task are similar to
English subtask A. But some features were not ex-
tracted for Arabic task, such as “answer position”
</bodyText>
<page confidence="0.990459">
197
</page>
<bodyText confidence="0.999282451612903">
was ineffective for Arabic task; “WH word num-
ber” cannot be extracted on Arabic data. To get
more effective features, the dataset for Arabic task
was translated to English by Google Translate1,
and feature extraction was done on both original
Arabic data and English data translated from origi-
nal Arabic data.
Features extracted for answer classification in
Arabic task include:
Word length features: length of the max length
word, average word length.
Word number feature: number of words.
Punctuation features: question mark number,
exclamation mark number.
Sentence features: average sentence length,
sentence number.
The features in the upper list were extracted
separately on answers and questions.
Answer and question correlative features:
number and ratio of same n-gram terms, cosine
similarity between answer and question body, KL
distance between answer and question body.
Name entity feature: number of name entity in
answer.
Frequent n-gram term features: frequent uni-
gram, bigram terms and trigram terms in Arabic
data and English data.
Features were extracted only on translated Eng-
lish data in the following 2 groups:
Word number features in English: all capital
word number, polite word number, word “yes”
number, word “no” number.
Part-of-speech features: noun word number
and ratio, verb word number and ratio, pronoun
word number and ratio, WH word number and ra-
tio.
Methods proposed for Arabic task include: (1)
two-level hierarchical classification method: classi-
fying answers as irrelevant and not irrelevant in
the first level and classifying not irrelevant an-
swers as direct and related in the second level; (2)
ensemble learning method: training and choosing
top N best classifiers and using the results of those
classifiers to vote final result.
For English task, CQA-QL corpus (Màrquez et
al., 2015) was provided. This corpus was gotten
from the Qatar Living Forum2 and was filtered and
annotated manually. Questions in the corpus were
labeled into GENERAL and YES_NO class in
QTYPE dimension, and yes, no, unsure and Not
Applicable class in QGOLD_YN dimension. An-
swers were labeled into Good, Potential, Bad, Di-
alogue, Not English and Other class in CGOLD
dimension, and Yes, No, Unsure and Not Applica-
ble class in CGOLD_YN dimension.
For Arabic task, Fatwa corpus (Màrquez et al.,
2015) was provided, which was manually
processed and annotated on source data from the
Fatwa website3. Answers in this corpus were la-
beled into direct, related, and irrelevant class. The
irrelevant class answers for each question were
random selected from answers of other questions.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
4 Results Evaluation
</sectionHeader>
<bodyText confidence="0.999918">
Some toolkits were employed to extract features
and train classifiers. NLTK (Bird et al., 2009) was
used to extract features, include part-of-speech of
question and answer, frequent n-gram terms, co-
sine similarity and so on. WEKA (Hall et al., 2009)
toolkit was used to do feature selection and clas-
sifier training and choosing. LIBSVM (Chang and
Lin, 2011) and LIBLINEAR (Fan et al., 2008)
were used to train SVM classifier. Scikit-learn
toolkit (Pedregosa et al., 2011) was used to train
classifiers.
We submitted 3 formal results for each subtask
including English subtask A, English subtask B
and Arabic task following task result submission
requests: 1 primary result as team official result, 2
contrastive results to compare effects of different
methods.
</bodyText>
<subsectionHeader confidence="0.950313">
4.1 Measures
</subsectionHeader>
<bodyText confidence="0.998123">
The official metric to evaluate results is the macro-
averaged F1-score (Màrquez et al., 2015), which is
calculated as:
</bodyText>
<figure confidence="0.991855538461538">
NumC

1i
F
i

1
1
3 Data Sets
(1)
macro F

NumC
</figure>
<bodyText confidence="0.888711">
Data sets used for classifiers training includes the
training and development data provided. No exter-
nal data was used for classifiers training.
</bodyText>
<footnote confidence="0.920617">
1 http://translate.google.com
</footnote>
<bodyText confidence="0.582932">
where NumC is the number of class in test set, F1i
is the F1 value for class i in test set. F1 value is
calculated as:
</bodyText>
<footnote confidence="0.9993885">
2 http://www.qatarliving.com/forum
3 http://fatwa.islamweb.net
</footnote>
<page confidence="0.945379">
198
</page>
<equation confidence="0.972908333333333">
F 1 2xPxR (2)
P R

</equation>
<bodyText confidence="0.9991318">
where P and R is the precision and recall of test
results for a class in test set.
The total accuracy for test result is used as sec-
ondary metric for results comparison, which is cal-
culated as:
</bodyText>
<equation confidence="0.68156075">
totalRighNum
Accuracy (3)

totalTestCaseNum
</equation>
<subsectionHeader confidence="0.953069">
4.2 Results of English subtask A
</subsectionHeader>
<bodyText confidence="0.999777588235294">
Official evaluation on English subtask A was dif-
ferent to other task. In CQA-QL corpus, all an-
swers were labeled in fine-grained labels which
include 6 classes: good, bad, potential, dialogue,
“not English” and other. But in official evaluation,
the macro-F1 score was calculated based on the
coarse-grained labels which include 3 classes:
good, bad, potential. The class dialogue, “not Eng-
lish” and other were merged with class bad.
We considered English subtask A as a 5-class
(good, potential, bad, dialogue, and “not English”)
classification problem. The answers in “not Eng-
lish” class were firstly recognized by toolkit Lan-
guage Detection (Shuyo, 2010). Other answers
were classified with methods we proposed.
The evaluation results for English subtask A
submissions are shown in table 1.
</bodyText>
<table confidence="0.99923225">
Submission Macro F1 Accuracy
primary 56.41 68.67
contrastive1 56.44 69.43
contrastive2 55.22 67.91
</table>
<tableCaption confidence="0.999943">
Table 1. Macro F1 and accuracy of English subtask A.
</tableCaption>
<bodyText confidence="0.999911368421053">
The primary submission was gotten by two-level
hierarchical classification method: in the first level,
answers were classified into good_potential and
bad_dialogue. In the second level, good_potential
answers and bad_dialogue answers were classified
separately: good_potential answers were classified
into good and potential, bad_dialogue answers
were classified into bad and dialogue. The classifi-
ers used here were SVM which were trained using
toolkit LIBLINEAR.
In contrastive1 submission, two-level hierar-
chical classification method was used, and a spe-
cial ensemble learning method was designed for
potential answers classifying. The potential class
answers were classified using ensemble learning
method in the first level. The other 3 classes an-
swers were classified in the second level. The en-
semble learning method for potential answers clas-
sification using 5 binary classifiers: 3 good-
potential classifiers trained using different training
data; 1 bad-potential classifier and 1 dialogue-
potential classifier. The training data for good-
potential classifiers was gotten by random splitting
good answers into 3 parts. Classifiers used for the
contrastive1 submission were SVM trained with
toolkit LIBLINEAR.
Steps for getting the contrastive2 submission
were similar to the primary submission. The differ-
ence was that the first level classifier was trained
using Random Forest algorithm (Breiman, 2001).
The training data good-potential classifier was re-
sampled to balance the instance distribution be-
tween good and potential class.
Features employed for English subtask A in-
cludes 4044 features: the top 4000 frequent n-gram
terms and the top 44 maximum gain ratio features
of all the features described in section 2.1 except
the “Frequent n-gram term features”.
</bodyText>
<subsectionHeader confidence="0.996428">
4.3 Results of English subtask B
</subsectionHeader>
<bodyText confidence="0.999235">
Three submissions were submitted for English sub-
task B including primary submission, contrastive1
submission and contrastive2 submission. The eval-
uation results are presented in table 2.
</bodyText>
<table confidence="0.995716">
Submission Macro F1 Accuracy
primary 53.60 64.00
contrastive1 42.50 60.00
contrastive2 42.40 60.00
</table>
<tableCaption confidence="0.999875">
Table 2. Macro F1 and accuracy of English subtask B.
</tableCaption>
<bodyText confidence="0.999968375">
For the primary submission, answers in YES_NO
question answer list were classified into 5 classes.
Steps to classify answers in CGOLD_YN dimen-
sion were: first, a rule based method was used to
classify answers; second, ensemble learning me-
thod was used to classify the answers that cannot
be classified by rule based method. Classifiers used
in ensemble learning method include: SMO (se-
quential minimal optimization algorithm for SVM)
(Keerthi et al., 2001), Random Forest, DMNBtext
(Discriminative Multinomial Naïve Bayes) (Su et
al., 2008), Logistic Regression (Le Cessie and Van
Houwelingen, 1992) and RBFNetwork (norma-
lized Gaussian radial basis function network).
Those classifiers were the top 5 best of all classifi-
ers have been tried based on 10 folds cross valida-
</bodyText>
<page confidence="0.997839">
199
</page>
<bodyText confidence="0.999876740740741">
tion on training data. Features employed for the
primary submission include 187 features, which
were the top 187 maximum gain ratio features of
the 4400 features used in English task A.
The contrastive1 submission and contrastive2
submission were based on the good answers in
English subtask A primary submission. Only good
answers of YES_NO question in subtask A primary
submission were classified in CGOLD_YN dimen-
sion. Good answers of YES_NO question were
classified into: yes, no and unsure.
For the contrastive1 submission, good class an-
swers were classified with ensemble learning me-
thod. Classifiers used for the ensemble learning
method included the top 5 best classifiers for an-
swer classification in CGOLD_YN dimension:
SMO, Random Forest, DMNBtext, Logistic Re-
gression and LMT (logistic model tree) (Sumner et
al., 2005).
For the contrastive2 submission, only classifier
LMT, which was the best classifier of all classifi-
ers tried based on 10 folds cross validation results
on training data, was used to classify good answers.
Features employed for the contrastive1 and con-
trastive2 submission include 110 features, which
were the top 110 maximum gain ratio features of
the 4400 features used in English task A.
</bodyText>
<subsectionHeader confidence="0.997102">
4.4 Results of Arabic task
</subsectionHeader>
<bodyText confidence="0.993316111111111">
Answers were classified into 3 classes in Arabic
task: direct, related, and irrelevant. Evaluation
results for Arabic task are presented in table 3.
The primary submission was gotten by ensemble
learning method using 3 classifiers. The classifiers
were top 3 classifiers chosen based on 10 folds
cross validation results on training data: SMO,
REPTree (decision/regression tree) and J48graft
(grafted C4.5 decision tree) (Webb, 1999).
</bodyText>
<table confidence="0.99977675">
Submission Macro F1 Accuracy
primary 67.70 74.53
contrastive1 68.36 73.93
contrastive2 67.98 73.23
</table>
<tableCaption confidence="0.999866">
Table 3. Macro F1 and accuracy of Arabic task.
</tableCaption>
<bodyText confidence="0.9996656">
The contrastive1 submission was gotten by two-
level hierarchical classification method: in the first
level, answers were classified into irrelevant and
not irrelevant; in the second level, not irrelevant
answers were classified into direct and related. All
classifiers were trained using SMO algorithm.
The contrastive2 submission was gotten only by
SMO classifier. The SMO classifier was trained as
multi-class classifier to classify answers into direct,
related and irrelevant.
Features employed for Arabic task include 5049
features: the top 5000 frequent n-gram terms and
the top 49 maximum gain ratio features of all the
features described in section 2.3 except “Frequent
n-gram term features”.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999635333333333">
In English subtask A, performance of the submis-
sion contrastive1, the hierarchical classification
method result, was better than other submissions.
The performance of hierarchical classification me-
thod was also better than other submission in Arab-
ic task. This shows that the hierarchical
classification method is effective for answer selec-
tion task.
The performances on different class varied from
each other remarkable for English subtask A and
Arabic task as shown in table 4. It is difficult to
distinguish the potentially useful class answers for
all classification methods that have been tried.
Analysis on feature extraction showed that, most
features were extracted to judge whether the an-
swer was good or bad, but few features were ex-
tracted to judge whether the answer was potentially
useful.
</bodyText>
<table confidence="0.999787533333333">
Submission Class P R F1
English Good 78.02 79.74 78.87
subtask A
contrastive1
Bad 80.6 66.01 72.58
Pot. 14.04 24.55 17.86
English Yes 80 80 80
subtask B
primary
No 28.57 50 36.36
Unsure 66.67 33.33 44.44
Arabic task direct 62.4 74.88 68.08
contrastive1
Irrel. 85.14 83.33 84.23
related 57.07 49.1 52.78
</table>
<tableCaption confidence="0.8197865">
Table 4. Detailed evaluation results (P, R and F1) of the
best performance result for each task.
</tableCaption>
<bodyText confidence="0.99978325">
In English subtask B, performance on primary
submission, which was result of one-step classifi-
cation method on all answers of YES_NO question,
was much better than other submissions which
were results of two-step classification method. The
results showed that cascade error of piping clas-
sifiers for answer classification in CGOLD and
answer classification in CGOLD_YN had great im-
</bodyText>
<page confidence="0.977339">
200
</page>
<bodyText confidence="0.999650375">
pact on final answer accuracy for YES 1O ques-
tion. The one-step classification methodcan avoid
the cascade error for Yes_1O questions answering.
We compared performance of SVM classifier
using bag-of-word features, non-bag-of-word fea-
tures and all features for English subtask A, sub-
task B and Arabic task on macro-F1 scores. The
results are shown in table 5.
</bodyText>
<table confidence="0.99855775">
Task bow non_bow bow+non_bow
Subtask A 0.39 0.48 0.50
Subtask B 0.42 0.64 0.68
Arabic Task 0.36 0.35 0.42
</table>
<tableCaption confidence="0.9954015">
Table 5. Macro F1 of SVM classifier using bag-of-word
features, non-bag-of-word features and all features.
</tableCaption>
<bodyText confidence="0.99985164">
Feature set bag-of-words (bow) includes Fre-
quent n-gram term features described in section
2.1 and 2.3. Feature set non-bag-of-words
(non_bow) includes other features described in
section 2.1 and 2.3 which were specially designed
for answer selection task. Set bow+non_bow in-
cludes all features in set bow and non_bow.
The performance of the classifier using
bow+non_bow features is better than using the
other two sets features in isolation, which means
bow set features and non_bow set features are ef-
fective to improve performance of answer classifi-
er if used both. The contribution of different sets is
different on different tasks. Performance of
non_bow (44 features for English data and 49 fea-
tures for Arabic data) is better than bow (4000 for
English and 5000 for Arabic) on Answer Selection
task. It shows the features specially extracted for
answer selection are more effective. But perfor-
mance of non_bow (22 features) is worse than bow
(165 features) on YES_1O questions answering.
The reason is that the non_bow features are not
designed for opinion recognition. It shows that de-
signing special features for opinion recognition for
task B is necessary.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999944909090909">
In this paper, we presented multi-classifier ensem-
ble method and hierarchical classification method
proposed for each subtask in SemEval-2015 Task 3.
Experimental results demonstrated that the pro-
posed classification methods were effective in both
English and Arabic subtasks.
In the next stage, syntax feature and deep se-
mantic feature will be exploited to further improve
the performance of our approaches. Besides, more
effective features for potential answers classifica-
tion will also be explored.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988643545454546">
The authors thank Daniel Cer and all the anonym-
ous reviewers for their insightful comments for this
paper.
This work was supported in part by the National
Natural Science Foundation of China (61272383,
61173075 and 61203378), the Strategic Emerging
Industry Development Special Funds of Shenzhen
(ZDSY20120613125401420 and
JCYJ20120613151940045) and the Key Basic Re-
search Foundation of Shenzhen
(JC201005260118A).
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999564081081081">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology,
2(3):1-27.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluating
and Predicting Answer Quality in Community QA. In
Proceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, 411-418, Geneva, Switzerland, 19-
23 July.
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
High-quality Content in Social Media. In Proceed-
ings of the 2008 International Conference on Web
Search and Data Mining, 183-194, Palo Alto, Cali-
fornia, USA, 11-12 February.
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
Vincent Dubourg, Jake Vanderplas, Alexandre Pas-
sos, David Cournapeau, Matthieu Brucher, Matthieu
Perrot, and Édouard Duchesnay. 2011. Scikit-learn:
Machine Learning in Python. The Journal of Ma-
chine Learning Research, 12:2825-2830.
Geoffrey I Webb. 1999. Decision tree grafting from the
all-tests-but-one partition. In Proceedings of the Six-
teenth International Joint Conference on Artificial
Intelligence, 2:702-707, San Francisco, California,
USA.
Jiang Su, Harry Zhang, Charles X Ling, and Stan Mat-
win. 2008. Discriminative parameter learning for
Bayesian networks. In Proceedings of the 25th inter-
national conference on Machine learning, 1016-1023,
Helsinki, Finland.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A Framework to Predict the Quality of
Answers with Non-textual Features. In Proceedings
</reference>
<page confidence="0.97007">
201
</page>
<reference confidence="0.999620128205128">
of the 29th Annual International ACM SIGIR Confe-
rence on Research and Development in Information
Retrieval, 228-235, Seattle, Washington, USA, 6-11
August.
Kohei Arai and Anik Nur Handayani. 2013. Predicting
quality of answer in collaborative Q/A community.
Society and culture, 2(3):21-25.
Leo Breiman. 2001. Random forests. Machine learning,
45(1):5-32.
Lluís Màrquez, James Glass, Walid Magdy, Alessandro
Moschitti, Preslav Nakov, and Bilal Randeree. 2015.
SemEval-2015 Task 3: Answer Selection in Commu-
nity Question Answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation
(SemEval 2015), Denver, Colorado, USA.
Marc Sumner, Eibe Frank, and Mark Hall. 2005. Speed-
ing up logistic model tree induction. In Knowledge
Discovery in Databases: PKDD 2005, 3721:675-683.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
ACM SIGKDD Explorations Newsletter, 11(1):10-18.
Nakatani Shuyo. 2010. Language Detection Library for
Java.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. The Journal
of Machine Learning Research, 9:1871-1874.
Sathiya Sathiya Keerthi, Shirish Krishnaj Shevade, Chi-
ru Bhattacharyya, and K. R. K. Murthy. 2001. Im-
provements to Platt’s SMO Algorithm for SVM
Classifier Design. Neural Computation, 13(3):637-
649.
Saskia Le Cessie and Johannes C Van Houwelingen.
1992. Ridge estimators in logistic regression. Applied
statistics, 191-201.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python. O’Reilly
Media, Inc. .
</reference>
<page confidence="0.998323">
202
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367991">
<title confidence="0.9994925">HITSZ-ICRC: Exploiting Classification Approach for Answer Selection in Community Question Answering</title>
<author confidence="0.865305">Yongshuai Hou</author>
<author confidence="0.865305">Cong Tan</author>
<author confidence="0.865305">Xiaolong Yaoyun Zhang</author>
<author confidence="0.865305">Jun Xu</author>
<author confidence="0.865305">Qingcai</author>
<affiliation confidence="0.84139">Key Laboratory of Network Oriented Intelligent Department of Computer Science and Harbin Institute of Technology Shenzhen Graduate</affiliation>
<address confidence="0.97316">HIT Campus, The University Town of Shenzhen, Shenzhen, 518055,</address>
<email confidence="0.951402">yongshuai.hou@gmail.com</email>
<email confidence="0.951402">viptancong}@gmail.com@gmail.com</email>
<email confidence="0.951402">{xiaoni5122@gmail.com</email>
<email confidence="0.951402">hit.xujun@gmail.com</email>
<email confidence="0.951402">qingcai.chen@gmail.com</email>
<abstract confidence="0.996681722222222">This paper describes the participation of the HITSZ-ICRC team on the Answer Selection Challenge in SemEval-2015. Our team participated in English subtask A, English subtask B and Arabic task. Two approaches, ensemble learning and hierarchical classification were proposed for answer selection in each task. Bag-of-words features, lexical features and non-textual features were employed. For the Arabic task, features were extracted from both Arabic data and English data that translated from the Arabic data. Evaluation demonstrated that the proposed methods were effective, achieving a macro-averaged F1 of 56.41% in English subtask A, 53.60 % (rank in English subtask B and 67.70% (rank in Arabic task, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--3</pages>
<contexts>
<context position="11042" citStr="Chang and Lin, 2011" startWordPosition="1718" endWordPosition="1721">cessed and annotated on source data from the Fatwa website3. Answers in this corpus were labeled into direct, related, and irrelevant class. The irrelevant class answers for each question were random selected from answers of other questions. 4 Results Evaluation Some toolkits were employed to extract features and train classifiers. NLTK (Bird et al., 2009) was used to extract features, include part-of-speech of question and answer, frequent n-gram terms, cosine similarity and so on. WEKA (Hall et al., 2009) toolkit was used to do feature selection and classifier training and choosing. LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) were used to train SVM classifier. Scikit-learn toolkit (Pedregosa et al., 2011) was used to train classifiers. We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official result, 2 contrastive results to compare effects of different methods. 4.1 Measures The official metric to evaluate results is the macroaveraged F1-score (Màrquez et al., 2015), which is calculated as: NumC  1i F i  1 1 3 Data Sets (1) macro F  NumC Data sets used</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chirag Shah</author>
<author>Jefferey Pomerantz</author>
</authors>
<title>Evaluating and Predicting Answer Quality in Community QA.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>411--418</pages>
<location>Geneva,</location>
<contexts>
<context position="2077" citStr="Shah and Pomerantz, 2010" startWordPosition="300" endWordPosition="303"> portal and receive answers from other users. All users can post questions and answers on CQA portal freely. Although it makes CQA users to get answers easily, the answer quality evaluation becomes a challenge for questions with multiple answers. To reduce the inconvenient in going through plenty of candidate answers, it makes sense to evaluate the quality of answers and select high-quality answers automatically for CQA systems. As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years (Arai and Handayani, 2013; Shah and Pomerantz, 2010; Agichtein et al., 2008). The Answer Selection in CQA challenge was opened as one new task in SemEval-2015: SemEval-2015 Task 3 (Màrquez et al., 2015). It created a venue and provided annotated datasets for researchers to compare their methods for answer selection in CQA. This challenge consisted of Subtask A and Subtask B. Subtask A required participant system to classify answers as relevant, potentially useful and bad for each question. Subtask B required participant system to decide whether the answer to a YES_IO question should be Yes, Io or Unsure based on the answer list. Subtask A was </context>
<context position="3754" citStr="Shah and Pomerantz (2010)" startWordPosition="569" endWordPosition="572">chical classification method proposed for each subtask in SemEval-2015 Task 3. 2 Methods for Answer Classification Different classification methods were tried by previous researchers for answer evaluation, prediction and selection in CQA. Jeon et al. (2006) designed a framework using non-textual features, most of which were user profile features, to predict the document quality and tried the framework on CQA. 196 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 196–202, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Shah and Pomerantz (2010) used text, user information and answer rank features to evaluate and predict answer quality. Arai and Handayani (2013) tried non-textual features mainly include nocontent features of text to train models to predict answer quality in CQA. For SemEval-2015 Task 3, we proposed ensemble learning method and hierarchical classification method to classify answers for each task. 2.1 English subtask A English subtask A required participant system to classify each answer of test questions as definitely relevant (good), potentially useful (potential) or bad (bad, dialog, non-English and other). Features</context>
</contexts>
<marker>Shah, Pomerantz, 2010</marker>
<rawString>Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and Predicting Answer Quality in Community QA. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 411-418, Geneva, Switzerland, 19-23 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Gilad Mishne</author>
</authors>
<title>Finding High-quality Content in Social Media.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining,</booktitle>
<pages>183--194</pages>
<location>Palo Alto, California, USA,</location>
<contexts>
<context position="2102" citStr="Agichtein et al., 2008" startWordPosition="304" endWordPosition="307">s from other users. All users can post questions and answers on CQA portal freely. Although it makes CQA users to get answers easily, the answer quality evaluation becomes a challenge for questions with multiple answers. To reduce the inconvenient in going through plenty of candidate answers, it makes sense to evaluate the quality of answers and select high-quality answers automatically for CQA systems. As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years (Arai and Handayani, 2013; Shah and Pomerantz, 2010; Agichtein et al., 2008). The Answer Selection in CQA challenge was opened as one new task in SemEval-2015: SemEval-2015 Task 3 (Màrquez et al., 2015). It created a venue and provided annotated datasets for researchers to compare their methods for answer selection in CQA. This challenge consisted of Subtask A and Subtask B. Subtask A required participant system to classify answers as relevant, potentially useful and bad for each question. Subtask B required participant system to decide whether the answer to a YES_IO question should be Yes, Io or Unsure based on the answer list. Subtask A was offered for two languages</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding High-quality Content in Social Media. In Proceedings of the 2008 International Conference on Web Search and Data Mining, 183-194, Palo Alto, California, USA, 11-12 February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gaël Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<contexts>
<context position="11156" citStr="Pedregosa et al., 2011" startWordPosition="1736" endWordPosition="1739">elated, and irrelevant class. The irrelevant class answers for each question were random selected from answers of other questions. 4 Results Evaluation Some toolkits were employed to extract features and train classifiers. NLTK (Bird et al., 2009) was used to extract features, include part-of-speech of question and answer, frequent n-gram terms, cosine similarity and so on. WEKA (Hall et al., 2009) toolkit was used to do feature selection and classifier training and choosing. LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) were used to train SVM classifier. Scikit-learn toolkit (Pedregosa et al., 2011) was used to train classifiers. We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official result, 2 contrastive results to compare effects of different methods. 4.1 Measures The official metric to evaluate results is the macroaveraged F1-score (Màrquez et al., 2015), which is calculated as: NumC  1i F i  1 1 3 Data Sets (1) macro F  NumC Data sets used for classifiers training includes the training and development data provided. No external data was used for class</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. The Journal of Machine Learning Research, 12:2825-2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey I Webb</author>
</authors>
<title>Decision tree grafting from the all-tests-but-one partition.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2--702</pages>
<location>San Francisco, California, USA.</location>
<contexts>
<context position="17829" citStr="Webb, 1999" startWordPosition="2771" endWordPosition="2772">ive1 and contrastive2 submission include 110 features, which were the top 110 maximum gain ratio features of the 4400 features used in English task A. 4.4 Results of Arabic task Answers were classified into 3 classes in Arabic task: direct, related, and irrelevant. Evaluation results for Arabic task are presented in table 3. The primary submission was gotten by ensemble learning method using 3 classifiers. The classifiers were top 3 classifiers chosen based on 10 folds cross validation results on training data: SMO, REPTree (decision/regression tree) and J48graft (grafted C4.5 decision tree) (Webb, 1999). Submission Macro F1 Accuracy primary 67.70 74.53 contrastive1 68.36 73.93 contrastive2 67.98 73.23 Table 3. Macro F1 and accuracy of Arabic task. The contrastive1 submission was gotten by twolevel hierarchical classification method: in the first level, answers were classified into irrelevant and not irrelevant; in the second level, not irrelevant answers were classified into direct and related. All classifiers were trained using SMO algorithm. The contrastive2 submission was gotten only by SMO classifier. The SMO classifier was trained as multi-class classifier to classify answers into direc</context>
</contexts>
<marker>Webb, 1999</marker>
<rawString>Geoffrey I Webb. 1999. Decision tree grafting from the all-tests-but-one partition. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, 2:702-707, San Francisco, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Su</author>
<author>Harry Zhang</author>
<author>Charles X Ling</author>
<author>Stan Matwin</author>
</authors>
<title>Discriminative parameter learning for Bayesian networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>1016--1023</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="15911" citStr="Su et al., 2008" startWordPosition="2470" endWordPosition="2473">ntrastive2 42.40 60.00 Table 2. Macro F1 and accuracy of English subtask B. For the primary submission, answers in YES_NO question answer list were classified into 5 classes. Steps to classify answers in CGOLD_YN dimension were: first, a rule based method was used to classify answers; second, ensemble learning method was used to classify the answers that cannot be classified by rule based method. Classifiers used in ensemble learning method include: SMO (sequential minimal optimization algorithm for SVM) (Keerthi et al., 2001), Random Forest, DMNBtext (Discriminative Multinomial Naïve Bayes) (Su et al., 2008), Logistic Regression (Le Cessie and Van Houwelingen, 1992) and RBFNetwork (normalized Gaussian radial basis function network). Those classifiers were the top 5 best of all classifiers have been tried based on 10 folds cross valida199 tion on training data. Features employed for the primary submission include 187 features, which were the top 187 maximum gain ratio features of the 4400 features used in English task A. The contrastive1 submission and contrastive2 submission were based on the good answers in English subtask A primary submission. Only good answers of YES_NO question in subtask A p</context>
</contexts>
<marker>Su, Zhang, Ling, Matwin, 2008</marker>
<rawString>Jiang Su, Harry Zhang, Charles X Ling, and Stan Matwin. 2008. Discriminative parameter learning for Bayesian networks. In Proceedings of the 25th international conference on Machine learning, 1016-1023, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A Framework to Predict the Quality of Answers with Non-textual Features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>228--235</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3386" citStr="Jeon et al. (2006)" startWordPosition="517" endWordPosition="520">data set format. In remainder of this paper, Subtask A in English is abbreviated to English subtask A, Subtask A in Arabic is abbreviated to Arabic task and Subtask B in English is abbreviated to English subtask B. HITSZ-ICRC team participated in English subtask A, English subtask B and Arabic task. This paper describes the ensemble learning method and hierarchical classification method proposed for each subtask in SemEval-2015 Task 3. 2 Methods for Answer Classification Different classification methods were tried by previous researchers for answer evaluation, prediction and selection in CQA. Jeon et al. (2006) designed a framework using non-textual features, most of which were user profile features, to predict the document quality and tried the framework on CQA. 196 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 196–202, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Shah and Pomerantz (2010) used text, user information and answer rank features to evaluate and predict answer quality. Arai and Handayani (2013) tried non-textual features mainly include nocontent features of text to train models to predict answer quality i</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A Framework to Predict the Quality of Answers with Non-textual Features. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 228-235, Seattle, Washington, USA, 6-11 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kohei Arai</author>
<author>Anik Nur Handayani</author>
</authors>
<title>Predicting quality of answer in collaborative Q/A community. Society and culture,</title>
<date>2013</date>
<pages>2--3</pages>
<contexts>
<context position="2051" citStr="Arai and Handayani, 2013" startWordPosition="296" endWordPosition="299">st his/her question on CQA portal and receive answers from other users. All users can post questions and answers on CQA portal freely. Although it makes CQA users to get answers easily, the answer quality evaluation becomes a challenge for questions with multiple answers. To reduce the inconvenient in going through plenty of candidate answers, it makes sense to evaluate the quality of answers and select high-quality answers automatically for CQA systems. As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years (Arai and Handayani, 2013; Shah and Pomerantz, 2010; Agichtein et al., 2008). The Answer Selection in CQA challenge was opened as one new task in SemEval-2015: SemEval-2015 Task 3 (Màrquez et al., 2015). It created a venue and provided annotated datasets for researchers to compare their methods for answer selection in CQA. This challenge consisted of Subtask A and Subtask B. Subtask A required participant system to classify answers as relevant, potentially useful and bad for each question. Subtask B required participant system to decide whether the answer to a YES_IO question should be Yes, Io or Unsure based on the a</context>
<context position="3873" citStr="Arai and Handayani (2013)" startWordPosition="588" endWordPosition="591">erent classification methods were tried by previous researchers for answer evaluation, prediction and selection in CQA. Jeon et al. (2006) designed a framework using non-textual features, most of which were user profile features, to predict the document quality and tried the framework on CQA. 196 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 196–202, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Shah and Pomerantz (2010) used text, user information and answer rank features to evaluate and predict answer quality. Arai and Handayani (2013) tried non-textual features mainly include nocontent features of text to train models to predict answer quality in CQA. For SemEval-2015 Task 3, we proposed ensemble learning method and hierarchical classification method to classify answers for each task. 2.1 English subtask A English subtask A required participant system to classify each answer of test questions as definitely relevant (good), potentially useful (potential) or bad (bad, dialog, non-English and other). Features employed to train classifiers for English subtask A include: Word length features: length of the max length word, aver</context>
</contexts>
<marker>Arai, Handayani, 2013</marker>
<rawString>Kohei Arai and Anik Nur Handayani. 2013. Predicting quality of answer in collaborative Q/A community. Society and culture, 2(3):21-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="14637" citStr="Breiman, 2001" startWordPosition="2277" endWordPosition="2278">The ensemble learning method for potential answers classification using 5 binary classifiers: 3 goodpotential classifiers trained using different training data; 1 bad-potential classifier and 1 dialoguepotential classifier. The training data for goodpotential classifiers was gotten by random splitting good answers into 3 parts. Classifiers used for the contrastive1 submission were SVM trained with toolkit LIBLINEAR. Steps for getting the contrastive2 submission were similar to the primary submission. The difference was that the first level classifier was trained using Random Forest algorithm (Breiman, 2001). The training data good-potential classifier was resampled to balance the instance distribution between good and potential class. Features employed for English subtask A includes 4044 features: the top 4000 frequent n-gram terms and the top 44 maximum gain ratio features of all the features described in section 2.1 except the “Frequent n-gram term features”. 4.3 Results of English subtask B Three submissions were submitted for English subtask B including primary submission, contrastive1 submission and contrastive2 submission. The evaluation results are presented in table 2. Submission Macro F</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine learning, 45(1):5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Màrquez</author>
<author>James Glass</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Bilal Randeree</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="2228" citStr="Màrquez et al., 2015" startWordPosition="326" endWordPosition="329">sily, the answer quality evaluation becomes a challenge for questions with multiple answers. To reduce the inconvenient in going through plenty of candidate answers, it makes sense to evaluate the quality of answers and select high-quality answers automatically for CQA systems. As a consequently, the task of answer quality evaluation and answer selection in CQA have attracted more and more attention in recent years (Arai and Handayani, 2013; Shah and Pomerantz, 2010; Agichtein et al., 2008). The Answer Selection in CQA challenge was opened as one new task in SemEval-2015: SemEval-2015 Task 3 (Màrquez et al., 2015). It created a venue and provided annotated datasets for researchers to compare their methods for answer selection in CQA. This challenge consisted of Subtask A and Subtask B. Subtask A required participant system to classify answers as relevant, potentially useful and bad for each question. Subtask B required participant system to decide whether the answer to a YES_IO question should be Yes, Io or Unsure based on the answer list. Subtask A was offered for two languages: English and Arabic. Data for the two languages was in different data set format. In remainder of this paper, Subtask A in En</context>
<context position="9896" citStr="Màrquez et al., 2015" startWordPosition="1533" endWordPosition="1536">mber, word “yes” number, word “no” number. Part-of-speech features: noun word number and ratio, verb word number and ratio, pronoun word number and ratio, WH word number and ratio. Methods proposed for Arabic task include: (1) two-level hierarchical classification method: classifying answers as irrelevant and not irrelevant in the first level and classifying not irrelevant answers as direct and related in the second level; (2) ensemble learning method: training and choosing top N best classifiers and using the results of those classifiers to vote final result. For English task, CQA-QL corpus (Màrquez et al., 2015) was provided. This corpus was gotten from the Qatar Living Forum2 and was filtered and annotated manually. Questions in the corpus were labeled into GENERAL and YES_NO class in QTYPE dimension, and yes, no, unsure and Not Applicable class in QGOLD_YN dimension. Answers were labeled into Good, Potential, Bad, Dialogue, Not English and Other class in CGOLD dimension, and Yes, No, Unsure and Not Applicable class in CGOLD_YN dimension. For Arabic task, Fatwa corpus (Màrquez et al., 2015) was provided, which was manually processed and annotated on source data from the Fatwa website3. Answers in th</context>
<context position="11551" citStr="Màrquez et al., 2015" startWordPosition="1797" endWordPosition="1800">009) toolkit was used to do feature selection and classifier training and choosing. LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) were used to train SVM classifier. Scikit-learn toolkit (Pedregosa et al., 2011) was used to train classifiers. We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official result, 2 contrastive results to compare effects of different methods. 4.1 Measures The official metric to evaluate results is the macroaveraged F1-score (Màrquez et al., 2015), which is calculated as: NumC  1i F i  1 1 3 Data Sets (1) macro F  NumC Data sets used for classifiers training includes the training and development data provided. No external data was used for classifiers training. 1 http://translate.google.com where NumC is the number of class in test set, F1i is the F1 value for class i in test set. F1 value is calculated as: 2 http://www.qatarliving.com/forum 3 http://fatwa.islamweb.net 198 F 1 2xPxR (2) P R  where P and R is the precision and recall of test results for a class in test set. The total accuracy for test result is used as secondary me</context>
</contexts>
<marker>Màrquez, Glass, Magdy, Moschitti, Nakov, Randeree, 2015</marker>
<rawString>Lluís Màrquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. 2015. SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Sumner</author>
<author>Eibe Frank</author>
<author>Mark Hall</author>
</authors>
<title>Speeding up logistic model tree induction.</title>
<date>2005</date>
<booktitle>In Knowledge Discovery in Databases: PKDD</booktitle>
<pages>3721--675</pages>
<contexts>
<context position="16977" citStr="Sumner et al., 2005" startWordPosition="2637" endWordPosition="2640">on and contrastive2 submission were based on the good answers in English subtask A primary submission. Only good answers of YES_NO question in subtask A primary submission were classified in CGOLD_YN dimension. Good answers of YES_NO question were classified into: yes, no and unsure. For the contrastive1 submission, good class answers were classified with ensemble learning method. Classifiers used for the ensemble learning method included the top 5 best classifiers for answer classification in CGOLD_YN dimension: SMO, Random Forest, DMNBtext, Logistic Regression and LMT (logistic model tree) (Sumner et al., 2005). For the contrastive2 submission, only classifier LMT, which was the best classifier of all classifiers tried based on 10 folds cross validation results on training data, was used to classify good answers. Features employed for the contrastive1 and contrastive2 submission include 110 features, which were the top 110 maximum gain ratio features of the 4400 features used in English task A. 4.4 Results of Arabic task Answers were classified into 3 classes in Arabic task: direct, related, and irrelevant. Evaluation results for Arabic task are presented in table 3. The primary submission was gotte</context>
</contexts>
<marker>Sumner, Frank, Hall, 2005</marker>
<rawString>Marc Sumner, Eibe Frank, and Mark Hall. 2005. Speeding up logistic model tree induction. In Knowledge Discovery in Databases: PKDD 2005, 3721:675-683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<pages>11--1</pages>
<contexts>
<context position="10934" citStr="Hall et al., 2009" startWordPosition="1700" endWordPosition="1703">LD_YN dimension. For Arabic task, Fatwa corpus (Màrquez et al., 2015) was provided, which was manually processed and annotated on source data from the Fatwa website3. Answers in this corpus were labeled into direct, related, and irrelevant class. The irrelevant class answers for each question were random selected from answers of other questions. 4 Results Evaluation Some toolkits were employed to extract features and train classifiers. NLTK (Bird et al., 2009) was used to extract features, include part-of-speech of question and answer, frequent n-gram terms, cosine similarity and so on. WEKA (Hall et al., 2009) toolkit was used to do feature selection and classifier training and choosing. LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) were used to train SVM classifier. Scikit-learn toolkit (Pedregosa et al., 2011) was used to train classifiers. We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official result, 2 contrastive results to compare effects of different methods. 4.1 Measures The official metric to evaluate results is the macroaveraged F1-score (Màrq</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. ACM SIGKDD Explorations Newsletter, 11(1):10-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nakatani Shuyo</author>
</authors>
<date>2010</date>
<institution>Language Detection Library for Java.</institution>
<contexts>
<context position="12939" citStr="Shuyo, 2010" startWordPosition="2033" endWordPosition="2034">different to other task. In CQA-QL corpus, all answers were labeled in fine-grained labels which include 6 classes: good, bad, potential, dialogue, “not English” and other. But in official evaluation, the macro-F1 score was calculated based on the coarse-grained labels which include 3 classes: good, bad, potential. The class dialogue, “not English” and other were merged with class bad. We considered English subtask A as a 5-class (good, potential, bad, dialogue, and “not English”) classification problem. The answers in “not English” class were firstly recognized by toolkit Language Detection (Shuyo, 2010). Other answers were classified with methods we proposed. The evaluation results for English subtask A submissions are shown in table 1. Submission Macro F1 Accuracy primary 56.41 68.67 contrastive1 56.44 69.43 contrastive2 55.22 67.91 Table 1. Macro F1 and accuracy of English subtask A. The primary submission was gotten by two-level hierarchical classification method: in the first level, answers were classified into good_potential and bad_dialogue. In the second level, good_potential answers and bad_dialogue answers were classified separately: good_potential answers were classified into good </context>
</contexts>
<marker>Shuyo, 2010</marker>
<rawString>Nakatani Shuyo. 2010. Language Detection Library for Java.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11075" citStr="Fan et al., 2008" startWordPosition="1724" endWordPosition="1727">from the Fatwa website3. Answers in this corpus were labeled into direct, related, and irrelevant class. The irrelevant class answers for each question were random selected from answers of other questions. 4 Results Evaluation Some toolkits were employed to extract features and train classifiers. NLTK (Bird et al., 2009) was used to extract features, include part-of-speech of question and answer, frequent n-gram terms, cosine similarity and so on. WEKA (Hall et al., 2009) toolkit was used to do feature selection and classifier training and choosing. LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) were used to train SVM classifier. Scikit-learn toolkit (Pedregosa et al., 2011) was used to train classifiers. We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official result, 2 contrastive results to compare effects of different methods. 4.1 Measures The official metric to evaluate results is the macroaveraged F1-score (Màrquez et al., 2015), which is calculated as: NumC  1i F i  1 1 3 Data Sets (1) macro F  NumC Data sets used for classifiers training include</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. The Journal of Machine Learning Research, 9:1871-1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sathiya Sathiya Keerthi</author>
<author>Shirish Krishnaj Shevade</author>
<author>Chiru Bhattacharyya</author>
<author>K R K Murthy</author>
</authors>
<date>2001</date>
<booktitle>Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Computation,</booktitle>
<pages>13--3</pages>
<contexts>
<context position="15827" citStr="Keerthi et al., 2001" startWordPosition="2459" endWordPosition="2462"> in table 2. Submission Macro F1 Accuracy primary 53.60 64.00 contrastive1 42.50 60.00 contrastive2 42.40 60.00 Table 2. Macro F1 and accuracy of English subtask B. For the primary submission, answers in YES_NO question answer list were classified into 5 classes. Steps to classify answers in CGOLD_YN dimension were: first, a rule based method was used to classify answers; second, ensemble learning method was used to classify the answers that cannot be classified by rule based method. Classifiers used in ensemble learning method include: SMO (sequential minimal optimization algorithm for SVM) (Keerthi et al., 2001), Random Forest, DMNBtext (Discriminative Multinomial Naïve Bayes) (Su et al., 2008), Logistic Regression (Le Cessie and Van Houwelingen, 1992) and RBFNetwork (normalized Gaussian radial basis function network). Those classifiers were the top 5 best of all classifiers have been tried based on 10 folds cross valida199 tion on training data. Features employed for the primary submission include 187 features, which were the top 187 maximum gain ratio features of the 4400 features used in English task A. The contrastive1 submission and contrastive2 submission were based on the good answers in Engli</context>
</contexts>
<marker>Keerthi, Shevade, Bhattacharyya, Murthy, 2001</marker>
<rawString>Sathiya Sathiya Keerthi, Shirish Krishnaj Shevade, Chiru Bhattacharyya, and K. R. K. Murthy. 2001. Improvements to Platt’s SMO Algorithm for SVM Classifier Design. Neural Computation, 13(3):637-649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saskia Le Cessie</author>
<author>Johannes C Van Houwelingen</author>
</authors>
<title>Ridge estimators in logistic regression.</title>
<date>1992</date>
<journal>Applied statistics,</journal>
<pages>191--201</pages>
<marker>Le Cessie, Van Houwelingen, 1992</marker>
<rawString>Saskia Le Cessie and Johannes C Van Houwelingen. 1992. Ridge estimators in logistic regression. Applied statistics, 191-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural language processing with Python.</title>
<date>2009</date>
<publisher>O’Reilly Media, Inc. .</publisher>
<contexts>
<context position="10780" citStr="Bird et al., 2009" startWordPosition="1675" endWordPosition="1678">wers were labeled into Good, Potential, Bad, Dialogue, Not English and Other class in CGOLD dimension, and Yes, No, Unsure and Not Applicable class in CGOLD_YN dimension. For Arabic task, Fatwa corpus (Màrquez et al., 2015) was provided, which was manually processed and annotated on source data from the Fatwa website3. Answers in this corpus were labeled into direct, related, and irrelevant class. The irrelevant class answers for each question were random selected from answers of other questions. 4 Results Evaluation Some toolkits were employed to extract features and train classifiers. NLTK (Bird et al., 2009) was used to extract features, include part-of-speech of question and answer, frequent n-gram terms, cosine similarity and so on. WEKA (Hall et al., 2009) toolkit was used to do feature selection and classifier training and choosing. LIBSVM (Chang and Lin, 2011) and LIBLINEAR (Fan et al., 2008) were used to train SVM classifier. Scikit-learn toolkit (Pedregosa et al., 2011) was used to train classifiers. We submitted 3 formal results for each subtask including English subtask A, English subtask B and Arabic task following task result submission requests: 1 primary result as team official resul</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python. O’Reilly Media, Inc. .</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>