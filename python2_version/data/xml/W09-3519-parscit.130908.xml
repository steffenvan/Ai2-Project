<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011278">
<title confidence="0.996709">
A Noisy Channel Model for Grapheme-based Machine Transliteration
</title>
<author confidence="0.998829">
Yuxiang Jia, Danqing Zhu, Shiwen Yu
</author>
<affiliation confidence="0.9651625">
Institute of Computational Linguistics, Peking University, Beijing, China
Key Laboratory of Computational Linguistics, Ministry of Education, China
</affiliation>
<email confidence="0.992904">
{yxjia,zhudanqing,yusw}@pku.edu.cn
</email>
<note confidence="0.448642">
C
</note>
<sectionHeader confidence="0.969711" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.978696">
Machine transliteration is an important Natu-
ral Language Processing task. This paper
proposes a Noisy Channel Model for Graph-
eme-based machine transliteration. Moses, a
phrase-based Statistical Machine Translation
tool, is employed for the implementation of
the system. Experiments are carried out on
the NEWS 2009 Machine Transliteration
Shared Task English-Chinese track. English-
Chinese back transliteration is studied as well.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999339">
Transliteration is defined as phonetic translation
of names across languages. Transliteration of
Named Entities is necessary in many applications,
such as machine translation, corpus alignment,
cross-language information retrieval, information
extraction and automatic lexicon acquisition.
The transliteration modeling approaches can
be classified into phoneme-based, grapheme-
based and hybrid approach of phoneme and
grapheme.
Many previous studies are devoted to the pho-
neme-based approach (Knight and Graehl, 1998;
Virga and Khudanpur, 2003). Suppose that E is
an English name and C is its Chinese translitera-
tion. The phoneme-based approach first converts
E into an intermediate phonemic representation p,
and then converts p into its Chinese counterpart
C. The idea is to transform both source and target
names into comparable phonemes so that the
phonetic similarity between two names can be
measured easily.
The grapheme-based approach has also at-
tracted much attention (Li et al., 2004). It treats
the transliteration as a statistical machine transla-
tion problem under monotonic constraint. The
idea is to obtain the bilingual orthographical cor-
respondence directly to reduce the possible errors
introduced in multiple conversions.
The hybrid approach attempts to utilize both
phoneme and grapheme information for translit-
eration. (Oh and Choi, 2006) proposed a way to
fuse both phoneme and grapheme features into a
single learning process.
The rest of this paper is organized as follows.
Section 2 briefly describes the noisy channel
model for machine transliteration. Section 3 in-
troduces the model’s implementation details. Ex-
periments and analysis are given in section 4.
Conclusions and future work are discussed in
section 5.
</bodyText>
<sectionHeader confidence="0.996931" genericHeader="method">
2 Noisy Channel Model
</sectionHeader>
<bodyText confidence="0.955806428571429">
Machine transliteration can be regarded as a
noisy channel problem. Take the English-
Chinese transliteration as an example. An Eng-
lish name E is considered as the output of the
noisy channel with its Chinese transliteration C
as the input. The transliteration process is as fol-
lows. The language model generates a Chinese
name C, and the transliteration model converts C
into its back-transliteration E. The channel de-
coder is used to find Ĉ that is the most likely to
the word C that gives rise to E. Ĉ is the result
transliteration of E.
The process can be formulated with equation 1.
arg max (  |) arg max
</bodyText>
<equation confidence="0.932073">
P C E =
P E
C C ( )
Since P(E) is constant for the given E, we can
rewrite equation 1 as follows:
C arg max P ( C ) P ( E  |C )
=
C
</equation>
<bodyText confidence="0.999947833333333">
The language model P(C) is simplified as n-
gram model of Chinese characters and is trained
with a Chinese name corpus. The transliteration
model P(E|C) is estimated from a parallel corpus
of English names and their Chinese translitera-
tions. The channel decoder combines the lan-
</bodyText>
<equation confidence="0.9851205">
P C P E C
( ) (  |)
</equation>
<page confidence="0.983671">
88
</page>
<note confidence="0.9833485">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 88–91,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.996389">
guage model and transliteration model to gener-
ate Chinese transliterations for given English
names.
</bodyText>
<sectionHeader confidence="0.965415" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9997355">
This section describes the data sets, experimental
setup, experiment results and analysis.
</bodyText>
<sectionHeader confidence="0.996126" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999594333333333">
Moses (Koehn et al., 2007), a phrase-based sta-
tistical machine translation tool, is leveraged to
implement the noisy channel model for graph-
eme-based machine transliteration without reor-
dering process (Matthews, 2007). Figure 1 is an
illustration of the phrase alignment result in ma-
chine transliteration of the name pairs “Clinton”
and “克林顿”, where characters are as words and
combinations of characters are as phrases.
</bodyText>
<figure confidence="0.570912">
c lin ton
克 林 顿
</figure>
<figureCaption confidence="0.999019">
Figure 1. Example phrase alignment
</figureCaption>
<bodyText confidence="0.999965681818182">
A collection of tools are used by Moses.
SRILM is used to build statistical language mod-
els. GIZA++ is used to perform word alignments
over parallel corpora. Mert is used for weight
optimization. It includes several improvements to
the basic training method including randomized
initial conditions and permuted model order and
dynamic parameter range expansion or restric-
tion. Bleu, an automatic machine translation
evaluation metric, is used during Mert optimiza-
tion. Moses’ beam-search decoding algorithm is
an efficient search algorithm that quickly finds
the highest probability translation among the ex-
ponential number of choices.
Moses automatically trains translation models
for any language pairs with only a collection of
parallel corpora. The parallel transliteration cor-
pora need to be preprocessed at first. English
names need to be lowercased. Both English
names and Chinese transliterations are space de-
limited. Samples of preprocessed input are
shown in figure 2.
</bodyText>
<figure confidence="0.9956808">
a a b y e 奥 比
a a g a a r d f 格 T
a a l l i b o n e 阿 利 本
a a l t o 阿 尔 托
a a m o d t 阿 莫 特
</figure>
<figureCaption confidence="0.999936">
Figure 2. Sample preprocessed name pairs
</figureCaption>
<subsectionHeader confidence="0.993805">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.99970805">
The training set contains 31961 paired names
between English and Chinese. The development
set has 2896 pairs. 2896 English names are given
to test the English-Chinese transliteration per-
formance.
Some statistics on the training data are shown
in table 1. All the English-Chinese transliteration
pairs are distinct. English names are unique
while some English names may share the same
Chinese transliteration. So the total number of
unique Chinese names is less than that of English
names. The Chinese characters composing the
Chinese transliterations are limited, where there
are only 370 unique characters in the 25033 Chi-
nese names. Supposing that the name length is
computed as the number of characters it contains,
the average length of English names is about
twice that of Chinese names. Name length is use-
ful when considering the order of the character n-
gram language model.
</bodyText>
<table confidence="0.997080333333333">
#unique transliteration pairs 31961
#unique English names 31961
#unique Chinese names 25033
#unique Chinese characters 370
Average number of English characters 6.8231
per name
Average number of Chinese characters 3.1665
per name
Maximum number of English charac- 15
ters per name
Maximum number of Chinese charac- 7
ters per name
</table>
<tableCaption confidence="0.999799">
Table 1. Training data statistics
</tableCaption>
<subsectionHeader confidence="0.949079">
4.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999975333333333">
Both English-Chinese forward transliteration and
back transliteration are studied. The process can
be divided into four steps: language model build-
ing, transliteration model training, weight tuning,
and decoding. When building language model,
data smoothing techniques Kneser-Ney and in-
terpolate are employed. In transliteration model
training step, the alignment heuristic is grow-
diag-final, while other parameters are default
settings. Tuning parameters are all defaults.
When decoding, the parameter distortion-limit is
set to 0, meaning that no reordering operation is
</bodyText>
<page confidence="0.999529">
89
</page>
<bodyText confidence="0.999858">
needed. The system outputs the 10-best distinct
transliterations.
The whole training set is used for language
model building and transliteration model training.
The development set is used for weight tuning
and system testing.
</bodyText>
<subsectionHeader confidence="0.999257">
4.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999825">
The following 6 metrics are used to measure the
quality of the transliteration results (Li et al.,
2009a): Word Accuracy in Top-1 (ACC), Fuzzi-
ness in Top-1 (Mean F-score), Mean Reciprocal
Rank (MRR), MAPref, MAP10, and MAPsys.
In the data of English-Chinese transliteration
track, each source name only has one reference
transliteration. Systems are required to output the
10-best unique transliterations for every source
name. Thus, MAPref equals ACC, and MAPsys is
the same or very close to MAP10. So we only
choose ACC, Mean F-score, MRR, and MAP10
to show the system performance.
</bodyText>
<subsectionHeader confidence="0.910588">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.996390857142857">
The language model n-gram order is an impor-
tant factor impacting transliteration performance,
so we experiment on both forward and back
transliteration tasks with increasing n-gram order,
trying to find the order giving the best perform-
ance. Here the development set is used for test-
ing.
Figure 3 and 4 show the results of forward and
back transliteration respectively, where the per-
formances become steady when the order reaches
6 and 11. The orders with the best performance
in all metrics for forward and back transliteration
are 2 and 5, which may relate to the average
length of Chinese and English names.
</bodyText>
<figure confidence="0.9835485">
1
0.8
0.6
0.4
0.2
0
1 2 3 4 5 6
Language Model N-Gram (Order)
</figure>
<figureCaption confidence="0.991168">
Figure 3. E2C language model n-gram (forward)
</figureCaption>
<figure confidence="0.530257">
Language Model N-Gram (Order)
</figure>
<figureCaption confidence="0.997183">
Figure 4. E2C language model n-gram (back)
</figureCaption>
<bodyText confidence="0.9998434375">
Weights generated in the training step can be
optimized through the tuning process. The de-
velopment set, 2896 name pairs, is divided into 4
equal parts, 1 for testing and other 3 for tuning.
We take the best settings as the baseline, and in-
crease tuning size by 1 part at one time. Table 2
and 3 show the tuning results of forward and
back transliteration, where the best results are
boldfaced. Tuning set size of 0 refers to the best
settings before tuning. Performances get im-
proved after tuning, among which the ACC of
forward transliteration gets improved by over
11%. The forward transliteration performance
gets improved steadily with the increase of tun-
ing set size, while the back transliteration per-
formance peaks at tuning set size of 2.
</bodyText>
<table confidence="0.999122666666667">
Tuning ACC Mean F-score MRR MAP10
size
0 0.543 0.797 0.669 0.209
1 0.645 0.851 0.752 0.231
2 0.645 0.850 0.749 0.230
3 0.655 0.854 0.758 0.233
</table>
<tableCaption confidence="0.996885">
Table 2. E2C tuning performance (forward)
</tableCaption>
<table confidence="0.999885833333333">
Tuning ACC Mean F-score MRR MAP10
size
0 0.166 0.790 0.278 0.092
1 0.181 0.801 0.306 0.102
2 0.190 0.806 0.314 0.104
3 0.187 0.801 0.312 0.104
</table>
<tableCaption confidence="0.999731">
Table 3. E2C tuning performance (back)
</tableCaption>
<bodyText confidence="0.467885833333333">
Table 2 shows that forward transliteration per-
formance gets improved with the increase of tun-
ing set size, so we use the whole development set
as the tuning set to tune the final system and the
final official results from the shared task report
(Li et al., 2009b) are shown in table 4.
</bodyText>
<figure confidence="0.905290818181818">
ACC Mean F-score
MRR MAP10
ACC Mean F-score
MRR MAP10
1
0.8
0.6
0.4
0.2
0
1 2 3 4 5 6 7 8 9 10 11
</figure>
<page confidence="0.939638">
90
</page>
<table confidence="0.996911">
ACC Mean MRR MAPref MAP10 MAPsys
F-score
0.652 0.858 0.755 0.652 0.232 0.232
</table>
<tableCaption confidence="0.999619">
Table 4. The final official results of E2C forward
</tableCaption>
<bodyText confidence="0.999897428571429">
Experiments show that forward transliteration
has better performance than back transliteration.
One reason may be that on average English name
is longer than Chinese name, thus need more
data to train a good character level language
model. Another reason is that some information
is lost during transliteration which can not be
recovered in back transliteration. One more very
important reason is as follows. Typically in back
transliteration, you have only one correct refer-
ence transliteration, and therefore, a wide cover-
age word level language model is very useful.
Without it, back transliteration may have a poor
performance.
</bodyText>
<sectionHeader confidence="0.99795" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999982388888889">
This paper proposes a Noisy Channel Model for
grapheme-based machine transliteration. The
phrase-based statistical machine translation tool,
Moses, is leveraged for system implementation.
We participate in the NEWS 2009 Machine
Transliteration Shared Task English-Chinese
track. English-Chinese back transliteration is also
studied. This model is language independent and
can be applied to transliteration of any language
pairs.
To improve system performance, extensive er-
ror analyses will be made in the future and meth-
ods will be proposed according to different error
types. We will pay much attention to back trans-
literation for its seemingly greater difficulty and
explore relations between forward and back
transliteration to seek a strategy solving the two
simultaneously.
</bodyText>
<sectionHeader confidence="0.998021" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99985425">
The authors are grateful to the organizers of the
NEWS 2009 Machine Transliteration Shared
Task for their hard work to provide such a good
research platform. The work in this paper is sup-
ported by a grant from the National Basic Re-
search Program of China (No.2004CB318102)
and a grant from the National Natural Science
Foundation of China (No.60773173).
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999958333333333">
K. Knight and J. Graehl. 1998. Machine Translitera-
tion. Computational Linguistics, Vol. 24, No. 4, pp.
599-612.
P. Virga and S. Khudanpur. 2003. Transliteration of
Proper Names in Cross-lingual Information Re-
trieval. In Proceedings of the ACL Workshop on
Multi-lingual Named Entity Recognition 2003.
H.Z. Li, M. Zhang and J. Su. 2004. A Joint Source
Channel Model for Machine Transliteration. In
Proceedings of the 42nd ACL, pp. 159-166.
J.H. Oh and K.S. Choi. 2006. An Ensemble of Trans-
literation Models for Information Retrieval. In In-
formation Processing and Management, Vol. 42,
pp. 980-1002.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th ACL Companion Volume of the Demo
and Poster Sessions, pp. 177-180.
D. Matthews. 2007. Machine Transliteration of Proper
Names. Master thesis. University of Edinburgh.
H.Z. Li, A. Kumaran, M. Zhang and V. Pervouchine.
2009a. Whitepaper of NEWS 2009 Machine Trans-
literation Shared Task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
H.Z. Li, A. Kumaran, V. Pervouchine and M. Zhang.
2009b. Report on NEWS 2009 Machine Translit-
eration Shared Task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
</reference>
<page confidence="0.999172">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.119111">
<title confidence="0.999939">A Noisy Channel Model for Grapheme-based Machine Transliteration</title>
<author confidence="0.999555">Yuxiang Jia</author>
<author confidence="0.999555">Danqing Zhu</author>
<author confidence="0.999555">Shiwen Yu</author>
<affiliation confidence="0.999989">Institute of Computational Linguistics, Peking University, Beijing,</affiliation>
<address confidence="0.871277">Key Laboratory of Computational Linguistics, Ministry of Education, China</address>
<email confidence="0.937723">yxjia@pku.edu.cn</email>
<email confidence="0.937723">zhudanqing@pku.edu.cn</email>
<email confidence="0.937723">yusw@pku.edu.cn</email>
<note confidence="0.287834">C</note>
<abstract confidence="0.9973805">Machine transliteration is an important Natural Language Processing task. This paper proposes a Noisy Channel Model for Grapheme-based machine transliteration. Moses, a phrase-based Statistical Machine Translation tool, is employed for the implementation of the system. Experiments are carried out on</abstract>
<note confidence="0.620967">the NEWS 2009 Machine Transliteration Shared Task English-Chinese track. English- Chinese back transliteration is studied as well.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration. Computational Linguistics,</journal>
<volume>24</volume>
<pages>599--612</pages>
<contexts>
<context position="1255" citStr="Knight and Graehl, 1998" startWordPosition="157" endWordPosition="160">n Shared Task English-Chinese track. EnglishChinese back transliteration is studied as well. 1 Introduction Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities is necessary in many applications, such as machine translation, corpus alignment, cross-language information retrieval, information extraction and automatic lexicon acquisition. The transliteration modeling approaches can be classified into phoneme-based, graphemebased and hybrid approach of phoneme and grapheme. Many previous studies are devoted to the phoneme-based approach (Knight and Graehl, 1998; Virga and Khudanpur, 2003). Suppose that E is an English name and C is its Chinese transliteration. The phoneme-based approach first converts E into an intermediate phonemic representation p, and then converts p into its Chinese counterpart C. The idea is to transform both source and target names into comparable phonemes so that the phonetic similarity between two names can be measured easily. The grapheme-based approach has also attracted much attention (Li et al., 2004). It treats the transliteration as a statistical machine translation problem under monotonic constraint. The idea is to ob</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine Transliteration. Computational Linguistics, Vol. 24, No. 4, pp. 599-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Virga</author>
<author>S Khudanpur</author>
</authors>
<title>Transliteration of Proper Names in Cross-lingual Information Retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Multi-lingual Named Entity Recognition</booktitle>
<contexts>
<context position="1283" citStr="Virga and Khudanpur, 2003" startWordPosition="161" endWordPosition="164">nese track. EnglishChinese back transliteration is studied as well. 1 Introduction Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities is necessary in many applications, such as machine translation, corpus alignment, cross-language information retrieval, information extraction and automatic lexicon acquisition. The transliteration modeling approaches can be classified into phoneme-based, graphemebased and hybrid approach of phoneme and grapheme. Many previous studies are devoted to the phoneme-based approach (Knight and Graehl, 1998; Virga and Khudanpur, 2003). Suppose that E is an English name and C is its Chinese transliteration. The phoneme-based approach first converts E into an intermediate phonemic representation p, and then converts p into its Chinese counterpart C. The idea is to transform both source and target names into comparable phonemes so that the phonetic similarity between two names can be measured easily. The grapheme-based approach has also attracted much attention (Li et al., 2004). It treats the transliteration as a statistical machine translation problem under monotonic constraint. The idea is to obtain the bilingual orthograp</context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>P. Virga and S. Khudanpur. 2003. Transliteration of Proper Names in Cross-lingual Information Retrieval. In Proceedings of the ACL Workshop on Multi-lingual Named Entity Recognition 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Z Li</author>
<author>M Zhang</author>
<author>J Su</author>
</authors>
<title>A Joint Source Channel Model for Machine Transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="1733" citStr="Li et al., 2004" startWordPosition="234" endWordPosition="237">ased and hybrid approach of phoneme and grapheme. Many previous studies are devoted to the phoneme-based approach (Knight and Graehl, 1998; Virga and Khudanpur, 2003). Suppose that E is an English name and C is its Chinese transliteration. The phoneme-based approach first converts E into an intermediate phonemic representation p, and then converts p into its Chinese counterpart C. The idea is to transform both source and target names into comparable phonemes so that the phonetic similarity between two names can be measured easily. The grapheme-based approach has also attracted much attention (Li et al., 2004). It treats the transliteration as a statistical machine translation problem under monotonic constraint. The idea is to obtain the bilingual orthographical correspondence directly to reduce the possible errors introduced in multiple conversions. The hybrid approach attempts to utilize both phoneme and grapheme information for transliteration. (Oh and Choi, 2006) proposed a way to fuse both phoneme and grapheme features into a single learning process. The rest of this paper is organized as follows. Section 2 briefly describes the noisy channel model for machine transliteration. Section 3 introd</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>H.Z. Li, M. Zhang and J. Su. 2004. A Joint Source Channel Model for Machine Transliteration. In Proceedings of the 42nd ACL, pp. 159-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Oh</author>
<author>K S Choi</author>
</authors>
<title>An Ensemble of Transliteration Models for Information Retrieval.</title>
<date>2006</date>
<booktitle>In Information Processing and Management,</booktitle>
<volume>42</volume>
<pages>980--1002</pages>
<contexts>
<context position="2097" citStr="Oh and Choi, 2006" startWordPosition="286" endWordPosition="289">hinese counterpart C. The idea is to transform both source and target names into comparable phonemes so that the phonetic similarity between two names can be measured easily. The grapheme-based approach has also attracted much attention (Li et al., 2004). It treats the transliteration as a statistical machine translation problem under monotonic constraint. The idea is to obtain the bilingual orthographical correspondence directly to reduce the possible errors introduced in multiple conversions. The hybrid approach attempts to utilize both phoneme and grapheme information for transliteration. (Oh and Choi, 2006) proposed a way to fuse both phoneme and grapheme features into a single learning process. The rest of this paper is organized as follows. Section 2 briefly describes the noisy channel model for machine transliteration. Section 3 introduces the model’s implementation details. Experiments and analysis are given in section 4. Conclusions and future work are discussed in section 5. 2 Noisy Channel Model Machine transliteration can be regarded as a noisy channel problem. Take the EnglishChinese transliteration as an example. An English name E is considered as the output of the noisy channel with i</context>
</contexts>
<marker>Oh, Choi, 2006</marker>
<rawString>J.H. Oh and K.S. Choi. 2006. An Ensemble of Transliteration Models for Information Retrieval. In Information Processing and Management, Vol. 42, pp. 980-1002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL Companion Volume of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="3919" citStr="Koehn et al., 2007" startWordPosition="609" endWordPosition="612">se characters and is trained with a Chinese name corpus. The transliteration model P(E|C) is estimated from a parallel corpus of English names and their Chinese transliterations. The channel decoder combines the lanP C P E C ( ) ( |) 88 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 88–91, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP guage model and transliteration model to generate Chinese transliterations for given English names. 4 Experiments This section describes the data sets, experimental setup, experiment results and analysis. 3 Implementation Moses (Koehn et al., 2007), a phrase-based statistical machine translation tool, is leveraged to implement the noisy channel model for grapheme-based machine transliteration without reordering process (Matthews, 2007). Figure 1 is an illustration of the phrase alignment result in machine transliteration of the name pairs “Clinton” and “克林顿”, where characters are as words and combinations of characters are as phrases. c lin ton 克 林 顿 Figure 1. Example phrase alignment A collection of tools are used by Moses. SRILM is used to build statistical language models. GIZA++ is used to perform word alignments over parallel corpo</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th ACL Companion Volume of the Demo and Poster Sessions, pp. 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Matthews</author>
</authors>
<title>Machine Transliteration of Proper Names. Master thesis.</title>
<date>2007</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4110" citStr="Matthews, 2007" startWordPosition="637" endWordPosition="638">der combines the lanP C P E C ( ) ( |) 88 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 88–91, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP guage model and transliteration model to generate Chinese transliterations for given English names. 4 Experiments This section describes the data sets, experimental setup, experiment results and analysis. 3 Implementation Moses (Koehn et al., 2007), a phrase-based statistical machine translation tool, is leveraged to implement the noisy channel model for grapheme-based machine transliteration without reordering process (Matthews, 2007). Figure 1 is an illustration of the phrase alignment result in machine transliteration of the name pairs “Clinton” and “克林顿”, where characters are as words and combinations of characters are as phrases. c lin ton 克 林 顿 Figure 1. Example phrase alignment A collection of tools are used by Moses. SRILM is used to build statistical language models. GIZA++ is used to perform word alignments over parallel corpora. Mert is used for weight optimization. It includes several improvements to the basic training method including randomized initial conditions and permuted model order and dynamic parameter </context>
</contexts>
<marker>Matthews, 2007</marker>
<rawString>D. Matthews. 2007. Machine Transliteration of Proper Names. Master thesis. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Z Li</author>
<author>A Kumaran</author>
<author>M Zhang</author>
<author>V Pervouchine</author>
</authors>
<title>Machine Transliteration Shared Task.</title>
<date>2009</date>
<journal>Whitepaper of NEWS</journal>
<booktitle>In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="7688" citStr="Li et al., 2009" startWordPosition="1212" endWordPosition="1215">ransliteration model training step, the alignment heuristic is growdiag-final, while other parameters are default settings. Tuning parameters are all defaults. When decoding, the parameter distortion-limit is set to 0, meaning that no reordering operation is 89 needed. The system outputs the 10-best distinct transliterations. The whole training set is used for language model building and transliteration model training. The development set is used for weight tuning and system testing. 4.3 Evaluation Metrics The following 6 metrics are used to measure the quality of the transliteration results (Li et al., 2009a): Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank (MRR), MAPref, MAP10, and MAPsys. In the data of English-Chinese transliteration track, each source name only has one reference transliteration. Systems are required to output the 10-best unique transliterations for every source name. Thus, MAPref equals ACC, and MAPsys is the same or very close to MAP10. So we only choose ACC, Mean F-score, MRR, and MAP10 to show the system performance. 4.4 Results The language model n-gram order is an important factor impacting transliteration performance, so we experim</context>
<context position="10349" citStr="Li et al., 2009" startWordPosition="1670" endWordPosition="1673">an F-score MRR MAP10 size 0 0.543 0.797 0.669 0.209 1 0.645 0.851 0.752 0.231 2 0.645 0.850 0.749 0.230 3 0.655 0.854 0.758 0.233 Table 2. E2C tuning performance (forward) Tuning ACC Mean F-score MRR MAP10 size 0 0.166 0.790 0.278 0.092 1 0.181 0.801 0.306 0.102 2 0.190 0.806 0.314 0.104 3 0.187 0.801 0.312 0.104 Table 3. E2C tuning performance (back) Table 2 shows that forward transliteration performance gets improved with the increase of tuning set size, so we use the whole development set as the tuning set to tune the final system and the final official results from the shared task report (Li et al., 2009b) are shown in table 4. ACC Mean F-score MRR MAP10 ACC Mean F-score MRR MAP10 1 0.8 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 11 90 ACC Mean MRR MAPref MAP10 MAPsys F-score 0.652 0.858 0.755 0.652 0.232 0.232 Table 4. The final official results of E2C forward Experiments show that forward transliteration has better performance than back transliteration. One reason may be that on average English name is longer than Chinese name, thus need more data to train a good character level language model. Another reason is that some information is lost during transliteration which can not be recovered in back </context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>H.Z. Li, A. Kumaran, M. Zhang and V. Pervouchine. 2009a. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Z Li</author>
<author>A Kumaran</author>
<author>V Pervouchine</author>
<author>M Zhang</author>
</authors>
<title>Machine Transliteration Shared Task.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<tech>2009b. Report on NEWS</tech>
<contexts>
<context position="7688" citStr="Li et al., 2009" startWordPosition="1212" endWordPosition="1215">ransliteration model training step, the alignment heuristic is growdiag-final, while other parameters are default settings. Tuning parameters are all defaults. When decoding, the parameter distortion-limit is set to 0, meaning that no reordering operation is 89 needed. The system outputs the 10-best distinct transliterations. The whole training set is used for language model building and transliteration model training. The development set is used for weight tuning and system testing. 4.3 Evaluation Metrics The following 6 metrics are used to measure the quality of the transliteration results (Li et al., 2009a): Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank (MRR), MAPref, MAP10, and MAPsys. In the data of English-Chinese transliteration track, each source name only has one reference transliteration. Systems are required to output the 10-best unique transliterations for every source name. Thus, MAPref equals ACC, and MAPsys is the same or very close to MAP10. So we only choose ACC, Mean F-score, MRR, and MAP10 to show the system performance. 4.4 Results The language model n-gram order is an important factor impacting transliteration performance, so we experim</context>
<context position="10349" citStr="Li et al., 2009" startWordPosition="1670" endWordPosition="1673">an F-score MRR MAP10 size 0 0.543 0.797 0.669 0.209 1 0.645 0.851 0.752 0.231 2 0.645 0.850 0.749 0.230 3 0.655 0.854 0.758 0.233 Table 2. E2C tuning performance (forward) Tuning ACC Mean F-score MRR MAP10 size 0 0.166 0.790 0.278 0.092 1 0.181 0.801 0.306 0.102 2 0.190 0.806 0.314 0.104 3 0.187 0.801 0.312 0.104 Table 3. E2C tuning performance (back) Table 2 shows that forward transliteration performance gets improved with the increase of tuning set size, so we use the whole development set as the tuning set to tune the final system and the final official results from the shared task report (Li et al., 2009b) are shown in table 4. ACC Mean F-score MRR MAP10 ACC Mean F-score MRR MAP10 1 0.8 0.6 0.4 0.2 0 1 2 3 4 5 6 7 8 9 10 11 90 ACC Mean MRR MAPref MAP10 MAPsys F-score 0.652 0.858 0.755 0.652 0.232 0.232 Table 4. The final official results of E2C forward Experiments show that forward transliteration has better performance than back transliteration. One reason may be that on average English name is longer than Chinese name, thus need more data to train a good character level language model. Another reason is that some information is lost during transliteration which can not be recovered in back </context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>H.Z. Li, A. Kumaran, V. Pervouchine and M. Zhang. 2009b. Report on NEWS 2009 Machine Transliteration Shared Task. In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>