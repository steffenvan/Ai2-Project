<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013965">
<title confidence="0.972142">
UCAM-CORE: Incorporating structured distributional similarity into STS
</title>
<author confidence="0.986996">
Tamara Polajnar Laura Rimell Douwe Kiela
</author>
<affiliation confidence="0.9904365">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.993876">
Cambridge CB3 0FD, UK
</address>
<email confidence="0.999756">
{tamara.polajnar,laura.rimell,douwe.kiela}@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999459285714286">
This paper describes methods that were sub-
mitted as part of the *SEM shared task on
Semantic Textual Similarity. Multiple kernels
provide different views of syntactic structure,
from both tree and dependency parses. The
kernels are then combined with simple lex-
ical features using Gaussian process regres-
sion, which is trained on different subsets of
training data for each run. We found that the
simplest combination has the highest consis-
tency across the different data sets, while in-
troduction of more training data and models
requires training and test data with matching
qualities.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870153846154">
The Semantic Textual Similarity (STS) shared task
consists of several data sets of paired passages of
text. The aim is to predict the similarity that hu-
man annotators have assigned to these aligned pairs.
Text length and grammatical quality vary between
the data sets, so our submissions to the task aimed to
investigate whether models that incorporate syntac-
tic structure in similarity calculation can be consis-
tently applied to diverse and noisy data.
We model the problem as a combination of ker-
nels (Shawe-Taylor and Cristianini, 2004), each of
which calculates similarity based on a different view
of the text. State-of-the-art results on text classifi-
cation have been achieved with kernel-based classi-
fication algorithms, such as the support vector ma-
chine (SVM) (Joachims, 1998), and the methods
here can be adapted for use in multiple kernel classi-
fication, as in Polajnar et al. (2011). The kernels are
combined using Gaussian process regression (GPR)
(Rasmussen and Williams, 2006). It is important
to note that the combination strategy described here
is only a different way of viewing the regression-
combined mixture of similarity measures approach
that is already popular in STS systems, including
several that participated in previous SemEval tasks
(Croce et al., 2012; B¨ar et al., 2012). Likewise, oth-
ers, such as Croce et al. (2012), have used tree and
dependency parse information as part of their sys-
tems; however, we use a tree kernel approach based
on a novel encoding method introduced by Zanzotto
et al. (2011) and from there derive two dependency-
based methods.
In the rest of this paper we will describe our sys-
tem, which consists of distributional similarity (Sec-
tion 2.1), several kernel measures (Section 2.2), and
a combination method (Section 2.3). This will be
followed by the description of our three submissions
(Section 3), and a discussion of the results (Sec-
tion 4).
</bodyText>
<sectionHeader confidence="0.994769" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999864333333333">
At the core of all the kernel methods is either sur-
face, distributional, or syntactic similarity between
sentence constituents. The methods themselves en-
code sentences into vectors or sets of vectors, while
the similarity between any two vectors is calculated
using cosine.
</bodyText>
<subsectionHeader confidence="0.989753">
2.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999675">
Target words are the non-stopwords that occur
within our training and test data. The two distri-
butional methods we use here both represent target
</bodyText>
<page confidence="0.995848">
85
</page>
<subsubsectionHeader confidence="0.253378">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.962512666666667">
and the Shared Task, pages 85–89, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
words as vectors that encode word occurrence within
a set of contexts. The first method is a variation on
BEAGLE (Jones and Mewhort, 2007), which con-
siders contexts to be words that surround targets.
The second method is based on ESA (Gabrilovich
and Markovitch, 2007), which considers contexts to
be Wikipedia documents that contain target words.
To gather the distributional data with both of
these approaches we used 316,305 documents from
the September 2012 snapshot of Wikipedia. The
training corpus for BEAGLE is generated by pool-
ing the top 20 documents retrieved by querying the
Wikipedia snapshot index for each target word in the
training and test data sets.
</bodyText>
<subsectionHeader confidence="0.520243">
2.1.1 BEAGLE
</subsectionHeader>
<bodyText confidence="0.999743181818182">
Random indexing (Kaski, 1998) is a technique for
dimensionality reduction where pseudo-orthogonal
bases are generated by randomly sampling a distri-
bution. BEAGLE is a model where random indexing
is used to represent word co-occurrence vectors in a
distributional model.
Each context word is represented as a D-
dimensional vector of normally distributed random
values drawn from the Gaussian distribution
N (0, σ�), where σ = � 1 D and D = 4096 (1)
A target word is represented as the sum of the
vectors of all the context words that occur within a
certain context window around the target word. In
BEAGLE this window is considered to be the sen-
tence in which the target word occurs; however, to
avoid segmenting the entire corpus, we assume the
window to include 5 words to either side of the tar-
get. This method has the advantage of keeping the
dimensionality of the context space constant even
if more context words are added, but we limit the
context words to the top 10,000 most frequent non-
stopwords in the corpus.
</bodyText>
<sectionHeader confidence="0.541682" genericHeader="method">
2.1.2 ESA
</sectionHeader>
<bodyText confidence="0.999510214285714">
ESA represents a target word as a weighted
ranked list of the top N documents that contain the
word, retrieved from a high quality collection. We
used the BM25F (Robertson et al., 2004) weighting
function and the top N = 700 documents. These pa-
rameters were chosen by testing on the WordSim353
dataset.1 The list of retrieved documents can be rep-
resented as a very sparse vector whose dimensions
match the number of documents in the collection,
or in a more computationally efficient manner as
a hash map linking document identifiers to the re-
trieval weights. Similarity between lists was calcu-
lated using the cosine measure augmented to work
on the hash map data type.
</bodyText>
<subsectionHeader confidence="0.997548">
2.2 Kernel Measures
</subsectionHeader>
<bodyText confidence="0.9999915">
In our experiments we use six basic kernel types,
which are described below. Effectively we have
eight kernels, because we also use the tree and de-
pendency kernels with and without distributional in-
formation. Each kernel is a function which is passed
a pair of short texts, which it then encodes into a spe-
cific format and compares using a defined similarity
function. LK uses the regular cosine similarity func-
tion, but LEK, TK, DK, MDK, DGK use the follow-
ing cosine similarity redefined for sets of vectors. If
the texts are represented as sets of vectors X and Y ,
the set similarity kernel function is:
</bodyText>
<equation confidence="0.9858205">
�κset(X,Y ) = � cos(~xi, ~yj) (2)
i j
</equation>
<bodyText confidence="0.991081">
and normalisation is accomplished in the standard
way for kernels by:
</bodyText>
<equation confidence="0.999029666666667">
κset(X, Y )
κset−n(X, Y ) = (3)
V/ (κset(X,X)κset(Y,Y ))
</equation>
<bodyText confidence="0.999200266666667">
LK - The lexical kernel calculates the overlap be-
tween the tokens that occur in each of the paired
texts, where the tokens consist of Porter stemmed
(Porter, 1980) non-stopwords. Each text is repre-
sented as a frequency vector of tokens that occur
within it and the similarity between the pair is cal-
culated using cosine.
LEK - The lexical ESA kernel represents each
example in the pair as the set of words that do not
occur in the intersection of the two texts. The simi-
larity is calculated as in Equation (3) with X and Y
being the ESA vectors of each word from the first
and second text representations, respectively.
TK - The tree kernel representation is based on
the definition by Zanzotto et al. (2011). Briefly,
</bodyText>
<footnote confidence="0.998357">
1http://www.cs.technion.ac.il/˜gabr/resources/
data/wordsim353/
</footnote>
<page confidence="0.997567">
86
</page>
<bodyText confidence="0.98543225">
each piece of text is parsed2; the non-terminal
nodes of the parse tree, stopwords, and out-of-
dictionary terms are all assigned a new random vec-
tor (Equation 1); while the leaves that occurred
in the BEAGLE training corpus are assigned their
learned distributional vectors (Section 2.1.1).
Each subtree of a tree is encoded recursively as
a vector, where the distributional vectors represent-
ing each node are combined using the circular con-
volution operator (Plate, 1994; Jones and Mewhort,
2007). The whole tree is represented as a set of vec-
tors, one for each subtree.
DK - The dependency kernel representation en-
codes each dependency pair as a separate vector, dis-
counting the labels. The non-stopword terminals are
represented as their distributional vectors, while the
stopwords and out-of-dictionary terms are given a
unique random vector. The vector for the depen-
dency pair is obtained via a circular convolution of
the individual word vectors.
MDK - The multiple dependency kernel is con-
structed like the dependency kernel, but similarity is
calculated separately between all the the pairs that
share the same dependency label. The combined
similarity for all dependency labels in the parse is
then calculated using least squares linear regression.
While at the later stage we use GPR to combine all
of the different kernels, for MDK we found that lin-
ear regression provided better performance.
DGK - The depgram kernel represents each de-
pendency pair as an ESA vector obtained by search-
ing the ESA collection for the two words in the
dependency pair joined by the AND operator. The
DGK representation only contains the dependencies
that occur in one similarity text or the other, but not
in both.
</bodyText>
<subsectionHeader confidence="0.979229">
2.3 Regression
</subsectionHeader>
<bodyText confidence="0.871692727272727">
Each of the kernel measures above is used to calcu-
late a similarity score between a pair of texts. The
different similarity scores are then combined using
2Because many of the datasets contained incomplete or un-
grammatical sentences, we had to approximate some parses.
The parsing was done using the Stanford parser (Klein and
Manning, 2003), which failed on some overly long sentences,
which we therefore segmented at conjunctions or commas.
Since our methods only compared subtrees of parses, we simply
took the union of all the partial parses for a given sentence.
Gaussian process regression (GPR) (Rasmussen and
Williams, 2006). GPR is a probabilistic regression
method where the weights are modelled as Gaussian
random variables. GPR is defined by a covariance
function, which is akin to the kernel function in the
support vector machine. We used the squared expo-
nential isotropic covariance function (also known as
the radial basis function):
with parameters p1 = 1, p2 = 1, and p3 = 0.01. We
found that training for parameters increased overfit-
ting and produced worse results in validation exper-
iments.
</bodyText>
<sectionHeader confidence="0.99768" genericHeader="method">
3 Submitted Runs
</sectionHeader>
<bodyText confidence="0.999951678571429">
We submitted three runs. This is not sufficient for
a full evaluation of the new methods we proposed
here, but it gives us an inkling of general trends. To
choose the composition of the submissions, we used
STS 2012 training data for training, and STS 2012
test data for validation (Agirre et al., 2012). The
final submitted runs also used some of the STS 2012
test data for training.
Basic - With this run we were examining if a sim-
ple introduction of syntactic structure can improve
over the baseline performance. We trained a GPR
combination of the linear and tree kernels (LK-TK)
on the MSRpar training data. In validation experi-
ments we found that this data set in general gave the
most consistent performance for regression training.
Custom - Here we tried to approximate the best
training setup for each type of data. We only had
training data for OnWN and for this dataset we were
able to improve over the LK-TK setup; however, the
settings for the rest of the data sets were guesses
based on observations from the validation experi-
ments and overall performed poorly. OnWN was
trained on MSRpar train with LK and DK. The head-
lines model was trained on MSRpar train and Eu-
roparl test, with LK-LEK-TK-DK-TKND-DKND-
MDK (trained on Europarl).3 FNWN was trained on
MSRpar train and OnWN test with LK-LEK-DGK-
TK-DK-TKND-DKND. Finally, the SMT model
</bodyText>
<footnote confidence="0.8723205">
3TKND and DKND are the versions of the tree and depen-
dency kernels where no distributional vectors were used.
</footnote>
<equation confidence="0.926307">
cov(xi, xj) = pie + p2
2 3�ij
(xz−xj)T ·(p2*I)−1·(xz−xj )
</equation>
<page confidence="0.951655">
87
</page>
<figure confidence="0.997575176470588">
60
50
40
30
20
Number of STS pairs
11
10
Gold Standard
Number of STS pairs
25
20
10
5
5
Basic Custom All
Number of STS pairs
30
25
20
15
10
5
Number of STS pairs
15
10
5
Score
0 1 2 3 4 5
Score
0 1 2 3 4 5
Score
2 2.5 3 3.5 4 4.5 5
Score
</figure>
<figureCaption confidence="0.999969">
Figure 1: Score distributions of different runs on the OnWN dataset
</figureCaption>
<bodyText confidence="0.9838473">
was trained on MSRpar train and Europarl test with
LK-LEK-TK-DK-TKND-DKND-MDK (trained on
MSRpar).
All - As in the LK-TK experiment, we used
the same model on all of the data sets. It was
trained on all of the training data except MSRvid,
using all eight kernel types defined above. In sum-
mary we used the LK-LEK-TK-TKND-DK-DKND-
MDK-DGK kernel combination. MDK was trained
on the 2012 training portion of MSRpar.
</bodyText>
<sectionHeader confidence="0.999042" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999772235294118">
From the shared task results in Table 1, we can see
that Basic is our highest ranked run. It has also
achieved the best performance on all data sets. The
LK on its own improves slightly on the task baseline
by removing stop words and using stemming, while
the introduction of TK contributes syntactic and dis-
tributional information. With the Custom run, we
were trying to manually estimate which training data
would best reflect properties of particular test data,
and to customise the kernel combination through
validation experiments. The only data set for which
this led to an improvement is OnWN, indicating
that customised settings can be beneficial, but that
a more scientific method for matching of training
and test data properties is required. In the All run,
we were examining the effects that maximising the
amount of training data and the number of kernel
</bodyText>
<table confidence="0.998518">
hdlns OnWN FNWN SMT mean rank
BL 0.5399 0.2828 0.2146 0.2861 0.3639 71
Basic 0.6399 0.4440 0.3995 0.3400 0.4709 51
Cstm 0.4962 0.5639 0.1724 0.3006 0.4207 60
All 0.5510 0.3099 0.2385 0.1171 0.3200 78
</table>
<tableCaption confidence="0.994109">
Table 1: Shared task results: Pearson correlation with the
gold standard
</tableCaption>
<bodyText confidence="0.999915666666667">
measures has on the output predictions. The results
show that swamping the regression with models and
training data leads to overly normalised output and
a decrease in performance.
While the evaluation measure, Pearson correla-
tion, does not take into account the shape of the out-
put distribution, Figure 1 shows that this informa-
tion may be a useful indicator of model quality and
behaviour. In particular, the role of the regression
component in our approach is to learn a transforma-
tion from the output distributions of the models to
the distribution of the training data gold standard.
This makes it sensitive to the choice of training data,
which ideally would have similar characteristics to
the individual kernels, as well as a similar gold stan-
dard distribution to the test data. We can see in Fig-
ure 1 that the training data and choice of kernels in-
fluence the output distribution.
Analysis of the minimum, first quartile, median,
third quartile, and maximum statistics of the distri-
butions in Figure 1 demonstrates that, while it is dif-
ficult to visually evaluate the similarities of the dif-
ferent distributions, the smallest squared error is be-
tween the gold standard and the Custom run. This
suggests that properties other than the rank order
may also be good indicators in training and testing
of STS methods.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.885425666666667">
Tamara Polajnar is supported by the ERC Starting
Grant, DisCoTex, awarded to Stephen Clark, and
Laura Rimell and Douwe Kiela by EPSRC grant
EP/I037512/1: A Unified Model of Compositional
and Distributional Semantics: Theory and Applica-
tions.
</bodyText>
<page confidence="0.998568">
88
</page>
<sectionHeader confidence="0.995872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999724038961039">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385–393,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435–440, Montr´eal, Canada,
June. Association for Computational Linguistics.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. 2012. UNITOR: Combining semantic text
similarity functions through sv regression. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation, held in conjunction with the 1st Joint
Conference on Lexical and Computational Semantics,
pages 597–602, Montr´eal, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI’07, pages 1606–1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In Proceedings of the 10th European Con-
ference on Machine Learning, ECML ’98, pages 137–
142, London, UK, UK. Springer-Verlag.
Michael N. Jones and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information in
a composite holographic lexicon. Psychological Re-
view, 114:1–37.
S. Kaski. 1998. Dimensionality reduction by random
mapping: fast similarity computation for clustering.
In Proceedings of the 1998 IEEE International Joint
Conference on Neural Networks, volume 1, pages
413–418 vol.1, May.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ’03, pages 423–430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. A. Plate. 1994. Distributed Representations and
Nested Compositional Structure. Ph.D. thesis, Univer-
sity of Toronto.
T Polajnar, T Damoulas, and M Girolami. 2011. Protein
interaction sentence detection using multiple semantic
kernels. JBiomed Semantics, 2(1):1–1.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137, July.
C. E. Rasmussen and C. K. I. Williams. 2006. Gaussian
Processes for Machine Learning. MIT Press.
Stephen Robertson, Hugo Zaragoza, and Michael Taylor.
2004. Simple BM25 extension to multiple weighted
fields. In Proceedings of the thirteenth ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ’04, pages 42–49, New York, NY,
USA. ACM.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete.
2011. Distributed structures and distributional mean-
ing. In Proceedings of the Workshop on Distributional
Semantics and Compositionality, DiSCo ’11, pages
10–15, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.999752">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753536">
<title confidence="0.999255">UCAM-CORE: Incorporating structured distributional similarity into STS</title>
<author confidence="0.996798">Tamara Polajnar Laura Rimell Douwe</author>
<affiliation confidence="0.992724">Computer University of</affiliation>
<address confidence="0.794796">Cambridge CB3 0FD,</address>
<abstract confidence="0.9972484">This paper describes methods that were submitted as part of the *SEM shared task on Semantic Textual Similarity. Multiple kernels provide different views of syntactic structure, from both tree and dependency parses. The kernels are then combined with simple lexical features using Gaussian process regression, which is trained on different subsets of training data for each run. We found that the simplest combination has the highest consistency across the different data sets, while introduction of more training data and models requires training and test data with matching qualities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="10555" citStr="Agirre et al., 2012" startWordPosition="1723" endWordPosition="1726">on in the support vector machine. We used the squared exponential isotropic covariance function (also known as the radial basis function): with parameters p1 = 1, p2 = 1, and p3 = 0.01. We found that training for parameters increased overfitting and produced worse results in validation experiments. 3 Submitted Runs We submitted three runs. This is not sufficient for a full evaluation of the new methods we proposed here, but it gives us an inkling of general trends. To choose the composition of the submissions, we used STS 2012 training data for training, and STS 2012 test data for validation (Agirre et al., 2012). The final submitted runs also used some of the STS 2012 test data for training. Basic - With this run we were examining if a simple introduction of syntactic structure can improve over the baseline performance. We trained a GPR combination of the linear and tree kernels (LK-TK) on the MSRpar training data. In validation experiments we found that this data set in general gave the most consistent performance for regression training. Custom - Here we tried to approximate the best training setup for each type of data. We only had training data for OnWN and for this dataset we were able to improv</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385–393, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, pages 435–440, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Paolo Annesi</author>
<author>Valerio Storch</author>
<author>Roberto Basili</author>
</authors>
<title>UNITOR: Combining semantic text similarity functions through sv regression.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>597--602</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="2126" citStr="Croce et al., 2012" startWordPosition="317" endWordPosition="320"> have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011). The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006). It is important to note that the combination strategy described here is only a different way of viewing the regressioncombined mixture of similarity measures approach that is already popular in STS systems, including several that participated in previous SemEval tasks (Croce et al., 2012; B¨ar et al., 2012). Likewise, others, such as Croce et al. (2012), have used tree and dependency parse information as part of their systems; however, we use a tree kernel approach based on a novel encoding method introduced by Zanzotto et al. (2011) and from there derive two dependencybased methods. In the rest of this paper we will describe our system, which consists of distributional similarity (Section 2.1), several kernel measures (Section 2.2), and a combination method (Section 2.3). This will be followed by the description of our three submissions (Section 3), and a discussion of the r</context>
</contexts>
<marker>Croce, Annesi, Storch, Basili, 2012</marker>
<rawString>Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto Basili. 2012. UNITOR: Combining semantic text similarity functions through sv regression. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, pages 597–602, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3712" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="572" endWordPosition="575">e non-stopwords that occur within our training and test data. The two distributional methods we use here both represent target 85 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 85–89, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics words as vectors that encode word occurrence within a set of contexts. The first method is a variation on BEAGLE (Jones and Mewhort, 2007), which considers contexts to be words that surround targets. The second method is based on ESA (Gabrilovich and Markovitch, 2007), which considers contexts to be Wikipedia documents that contain target words. To gather the distributional data with both of these approaches we used 316,305 documents from the September 2012 snapshot of Wikipedia. The training corpus for BEAGLE is generated by pooling the top 20 documents retrieved by querying the Wikipedia snapshot index for each target word in the training and test data sets. 2.1.1 BEAGLE Random indexing (Kaski, 1998) is a technique for dimensionality reduction where pseudo-orthogonal bases are generated by randomly sampling a distribution. BEAGLE is a model where random </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with suport vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning, ECML ’98,</booktitle>
<pages>137--142</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="1630" citStr="Joachims, 1998" startWordPosition="241" endWordPosition="242">e assigned to these aligned pairs. Text length and grammatical quality vary between the data sets, so our submissions to the task aimed to investigate whether models that incorporate syntactic structure in similarity calculation can be consistently applied to diverse and noisy data. We model the problem as a combination of kernels (Shawe-Taylor and Cristianini, 2004), each of which calculates similarity based on a different view of the text. State-of-the-art results on text classification have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011). The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006). It is important to note that the combination strategy described here is only a different way of viewing the regressioncombined mixture of similarity measures approach that is already popular in STS systems, including several that participated in previous SemEval tasks (Croce et al., 2012; B¨ar et al., 2012). Likewise, others, such as Croce et al. (2012), have used tree and dependency parse</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with suport vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, ECML ’98, pages 137– 142, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael N Jones</author>
<author>Douglas J K Mewhort</author>
</authors>
<title>Representing word meaning and order information in a composite holographic lexicon. Psychological Review,</title>
<date>2007</date>
<pages>114--1</pages>
<contexts>
<context position="3582" citStr="Jones and Mewhort, 2007" startWordPosition="551" endWordPosition="554">hile the similarity between any two vectors is calculated using cosine. 2.1 Distributional Similarity Target words are the non-stopwords that occur within our training and test data. The two distributional methods we use here both represent target 85 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 85–89, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics words as vectors that encode word occurrence within a set of contexts. The first method is a variation on BEAGLE (Jones and Mewhort, 2007), which considers contexts to be words that surround targets. The second method is based on ESA (Gabrilovich and Markovitch, 2007), which considers contexts to be Wikipedia documents that contain target words. To gather the distributional data with both of these approaches we used 316,305 documents from the September 2012 snapshot of Wikipedia. The training corpus for BEAGLE is generated by pooling the top 20 documents retrieved by querying the Wikipedia snapshot index for each target word in the training and test data sets. 2.1.1 BEAGLE Random indexing (Kaski, 1998) is a technique for dimensi</context>
<context position="7899" citStr="Jones and Mewhort, 2007" startWordPosition="1284" endWordPosition="1287"> based on the definition by Zanzotto et al. (2011). Briefly, 1http://www.cs.technion.ac.il/˜gabr/resources/ data/wordsim353/ 86 each piece of text is parsed2; the non-terminal nodes of the parse tree, stopwords, and out-ofdictionary terms are all assigned a new random vector (Equation 1); while the leaves that occurred in the BEAGLE training corpus are assigned their learned distributional vectors (Section 2.1.1). Each subtree of a tree is encoded recursively as a vector, where the distributional vectors representing each node are combined using the circular convolution operator (Plate, 1994; Jones and Mewhort, 2007). The whole tree is represented as a set of vectors, one for each subtree. DK - The dependency kernel representation encodes each dependency pair as a separate vector, discounting the labels. The non-stopword terminals are represented as their distributional vectors, while the stopwords and out-of-dictionary terms are given a unique random vector. The vector for the dependency pair is obtained via a circular convolution of the individual word vectors. MDK - The multiple dependency kernel is constructed like the dependency kernel, but similarity is calculated separately between all the the pair</context>
</contexts>
<marker>Jones, Mewhort, 2007</marker>
<rawString>Michael N. Jones and Douglas J. K. Mewhort. 2007. Representing word meaning and order information in a composite holographic lexicon. Psychological Review, 114:1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kaski</author>
</authors>
<title>Dimensionality reduction by random mapping: fast similarity computation for clustering.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,</booktitle>
<volume>1</volume>
<pages>413--418</pages>
<contexts>
<context position="4155" citStr="Kaski, 1998" startWordPosition="644" endWordPosition="645">ion on BEAGLE (Jones and Mewhort, 2007), which considers contexts to be words that surround targets. The second method is based on ESA (Gabrilovich and Markovitch, 2007), which considers contexts to be Wikipedia documents that contain target words. To gather the distributional data with both of these approaches we used 316,305 documents from the September 2012 snapshot of Wikipedia. The training corpus for BEAGLE is generated by pooling the top 20 documents retrieved by querying the Wikipedia snapshot index for each target word in the training and test data sets. 2.1.1 BEAGLE Random indexing (Kaski, 1998) is a technique for dimensionality reduction where pseudo-orthogonal bases are generated by randomly sampling a distribution. BEAGLE is a model where random indexing is used to represent word co-occurrence vectors in a distributional model. Each context word is represented as a Ddimensional vector of normally distributed random values drawn from the Gaussian distribution N (0, σ�), where σ = � 1 D and D = 4096 (1) A target word is represented as the sum of the vectors of all the context words that occur within a certain context window around the target word. In BEAGLE this window is considered</context>
</contexts>
<marker>Kaski, 1998</marker>
<rawString>S. Kaski. 1998. Dimensionality reduction by random mapping: fast similarity computation for clustering. In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks, volume 1, pages 413–418 vol.1, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9465" citStr="Klein and Manning, 2003" startWordPosition="1542" endWordPosition="1545"> each dependency pair as an ESA vector obtained by searching the ESA collection for the two words in the dependency pair joined by the AND operator. The DGK representation only contains the dependencies that occur in one similarity text or the other, but not in both. 2.3 Regression Each of the kernel measures above is used to calculate a similarity score between a pair of texts. The different similarity scores are then combined using 2Because many of the datasets contained incomplete or ungrammatical sentences, we had to approximate some parses. The parsing was done using the Stanford parser (Klein and Manning, 2003), which failed on some overly long sentences, which we therefore segmented at conjunctions or commas. Since our methods only compared subtrees of parses, we simply took the union of all the partial parses for a given sentence. Gaussian process regression (GPR) (Rasmussen and Williams, 2006). GPR is a probabilistic regression method where the weights are modelled as Gaussian random variables. GPR is defined by a covariance function, which is akin to the kernel function in the support vector machine. We used the squared exponential isotropic covariance function (also known as the radial basis fu</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423–430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Plate</author>
</authors>
<date>1994</date>
<booktitle>Distributed Representations and Nested Compositional Structure. Ph.D. thesis,</booktitle>
<institution>University of Toronto.</institution>
<contexts>
<context position="7873" citStr="Plate, 1994" startWordPosition="1282" endWordPosition="1283">esentation is based on the definition by Zanzotto et al. (2011). Briefly, 1http://www.cs.technion.ac.il/˜gabr/resources/ data/wordsim353/ 86 each piece of text is parsed2; the non-terminal nodes of the parse tree, stopwords, and out-ofdictionary terms are all assigned a new random vector (Equation 1); while the leaves that occurred in the BEAGLE training corpus are assigned their learned distributional vectors (Section 2.1.1). Each subtree of a tree is encoded recursively as a vector, where the distributional vectors representing each node are combined using the circular convolution operator (Plate, 1994; Jones and Mewhort, 2007). The whole tree is represented as a set of vectors, one for each subtree. DK - The dependency kernel representation encodes each dependency pair as a separate vector, discounting the labels. The non-stopword terminals are represented as their distributional vectors, while the stopwords and out-of-dictionary terms are given a unique random vector. The vector for the dependency pair is obtained via a circular convolution of the individual word vectors. MDK - The multiple dependency kernel is constructed like the dependency kernel, but similarity is calculated separatel</context>
</contexts>
<marker>Plate, 1994</marker>
<rawString>T. A. Plate. 1994. Distributed Representations and Nested Compositional Structure. Ph.D. thesis, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Polajnar</author>
<author>T Damoulas</author>
<author>M Girolami</author>
</authors>
<title>Protein interaction sentence detection using multiple semantic kernels. JBiomed Semantics,</title>
<date>2011</date>
<contexts>
<context position="1739" citStr="Polajnar et al. (2011)" startWordPosition="259" endWordPosition="262">our submissions to the task aimed to investigate whether models that incorporate syntactic structure in similarity calculation can be consistently applied to diverse and noisy data. We model the problem as a combination of kernels (Shawe-Taylor and Cristianini, 2004), each of which calculates similarity based on a different view of the text. State-of-the-art results on text classification have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011). The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006). It is important to note that the combination strategy described here is only a different way of viewing the regressioncombined mixture of similarity measures approach that is already popular in STS systems, including several that participated in previous SemEval tasks (Croce et al., 2012; B¨ar et al., 2012). Likewise, others, such as Croce et al. (2012), have used tree and dependency parse information as part of their systems; however, we use a tree kernel approach based on a novel encoding metho</context>
</contexts>
<marker>Polajnar, Damoulas, Girolami, 2011</marker>
<rawString>T Polajnar, T Damoulas, and M Girolami. 2011. Protein interaction sentence detection using multiple semantic kernels. JBiomed Semantics, 2(1):1–1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="6780" citStr="Porter, 1980" startWordPosition="1104" endWordPosition="1105"> compares using a defined similarity function. LK uses the regular cosine similarity function, but LEK, TK, DK, MDK, DGK use the following cosine similarity redefined for sets of vectors. If the texts are represented as sets of vectors X and Y , the set similarity kernel function is: �κset(X,Y ) = � cos(~xi, ~yj) (2) i j and normalisation is accomplished in the standard way for kernels by: κset(X, Y ) κset−n(X, Y ) = (3) V/ (κset(X,X)κset(Y,Y )) LK - The lexical kernel calculates the overlap between the tokens that occur in each of the paired texts, where the tokens consist of Porter stemmed (Porter, 1980) non-stopwords. Each text is represented as a frequency vector of tokens that occur within it and the similarity between the pair is calculated using cosine. LEK - The lexical ESA kernel represents each example in the pair as the set of words that do not occur in the intersection of the two texts. The similarity is calculated as in Equation (3) with X and Y being the ESA vectors of each word from the first and second text representations, respectively. TK - The tree kernel representation is based on the definition by Zanzotto et al. (2011). Briefly, 1http://www.cs.technion.ac.il/˜gabr/resource</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Rasmussen</author>
<author>C K I Williams</author>
</authors>
<title>Gaussian Processes for Machine Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1836" citStr="Rasmussen and Williams, 2006" startWordPosition="272" endWordPosition="275"> structure in similarity calculation can be consistently applied to diverse and noisy data. We model the problem as a combination of kernels (Shawe-Taylor and Cristianini, 2004), each of which calculates similarity based on a different view of the text. State-of-the-art results on text classification have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011). The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006). It is important to note that the combination strategy described here is only a different way of viewing the regressioncombined mixture of similarity measures approach that is already popular in STS systems, including several that participated in previous SemEval tasks (Croce et al., 2012; B¨ar et al., 2012). Likewise, others, such as Croce et al. (2012), have used tree and dependency parse information as part of their systems; however, we use a tree kernel approach based on a novel encoding method introduced by Zanzotto et al. (2011) and from there derive two dependencybased methods. In the </context>
<context position="9756" citStr="Rasmussen and Williams, 2006" startWordPosition="1587" endWordPosition="1590">h of the kernel measures above is used to calculate a similarity score between a pair of texts. The different similarity scores are then combined using 2Because many of the datasets contained incomplete or ungrammatical sentences, we had to approximate some parses. The parsing was done using the Stanford parser (Klein and Manning, 2003), which failed on some overly long sentences, which we therefore segmented at conjunctions or commas. Since our methods only compared subtrees of parses, we simply took the union of all the partial parses for a given sentence. Gaussian process regression (GPR) (Rasmussen and Williams, 2006). GPR is a probabilistic regression method where the weights are modelled as Gaussian random variables. GPR is defined by a covariance function, which is akin to the kernel function in the support vector machine. We used the squared exponential isotropic covariance function (also known as the radial basis function): with parameters p1 = 1, p2 = 1, and p3 = 0.01. We found that training for parameters increased overfitting and produced worse results in validation experiments. 3 Submitted Runs We submitted three runs. This is not sufficient for a full evaluation of the new methods we proposed her</context>
</contexts>
<marker>Rasmussen, Williams, 2006</marker>
<rawString>C. E. Rasmussen and C. K. I. Williams. 2006. Gaussian Processes for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
<author>Hugo Zaragoza</author>
<author>Michael Taylor</author>
</authors>
<title>Simple BM25 extension to multiple weighted fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the thirteenth ACM international conference on Information and knowledge management, CIKM ’04,</booktitle>
<pages>42--49</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5340" citStr="Robertson et al., 2004" startWordPosition="850" endWordPosition="853">d. In BEAGLE this window is considered to be the sentence in which the target word occurs; however, to avoid segmenting the entire corpus, we assume the window to include 5 words to either side of the target. This method has the advantage of keeping the dimensionality of the context space constant even if more context words are added, but we limit the context words to the top 10,000 most frequent nonstopwords in the corpus. 2.1.2 ESA ESA represents a target word as a weighted ranked list of the top N documents that contain the word, retrieved from a high quality collection. We used the BM25F (Robertson et al., 2004) weighting function and the top N = 700 documents. These parameters were chosen by testing on the WordSim353 dataset.1 The list of retrieved documents can be represented as a very sparse vector whose dimensions match the number of documents in the collection, or in a more computationally efficient manner as a hash map linking document identifiers to the retrieval weights. Similarity between lists was calculated using the cosine measure augmented to work on the hash map data type. 2.2 Kernel Measures In our experiments we use six basic kernel types, which are described below. Effectively we hav</context>
</contexts>
<marker>Robertson, Zaragoza, Taylor, 2004</marker>
<rawString>Stephen Robertson, Hugo Zaragoza, and Michael Taylor. 2004. Simple BM25 extension to multiple weighted fields. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, CIKM ’04, pages 42–49, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1384" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="202" endWordPosition="205">raining data and models requires training and test data with matching qualities. 1 Introduction The Semantic Textual Similarity (STS) shared task consists of several data sets of paired passages of text. The aim is to predict the similarity that human annotators have assigned to these aligned pairs. Text length and grammatical quality vary between the data sets, so our submissions to the task aimed to investigate whether models that incorporate syntactic structure in similarity calculation can be consistently applied to diverse and noisy data. We model the problem as a combination of kernels (Shawe-Taylor and Cristianini, 2004), each of which calculates similarity based on a different view of the text. State-of-the-art results on text classification have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011). The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006). It is important to note that the combination strategy described here is only a different way of viewing the regressioncombined mixture of similari</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Distributed structures and distributional meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Distributional Semantics and Compositionality, DiSCo ’11,</booktitle>
<pages>10--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Zanzotto, Dell’Arciprete, 2011</marker>
<rawString>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2011. Distributed structures and distributional meaning. In Proceedings of the Workshop on Distributional Semantics and Compositionality, DiSCo ’11, pages 10–15, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>