<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004392">
<title confidence="0.9486625">
LT3: Applying Hybrid Terminology Extraction to Aspect-Based Sentiment
Analysis
</title>
<author confidence="0.979615">
Orph´ee De Clercq, Marjan Van de Kauter, Els Lefever and V´eronique Hoste
</author>
<affiliation confidence="0.9731835">
LT3, Language and Translation Technology Team
Department of Translation, Interpreting and Communication – Ghent University
</affiliation>
<address confidence="0.936564">
Groot-Brittanni¨elaan 45, 9000 Ghent, Belgium
</address>
<email confidence="0.995853">
Firstname.Lastname@UGent.be
</email>
<sectionHeader confidence="0.995577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999584833333333">
The LT3 system perceives ABSA as a task
consisting of three main subtasks, which have
to be tackled incrementally, namely aspect
term extraction, classification and polarity
classification. For the first two steps, we see
that employing a hybrid terminology extrac-
tion system leads to promising results, espe-
cially when it comes to recall. For the polar-
ity classification, we show that it is possible
to gain satisfying accuracies, even on out-of-
domain data, with a basic model employing
only lexical information.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992829787234">
There exists a large interest in sentiment analysis
of user-generated content. Until recently, the main
research focus has been on discovering the overall
polarity of a certain text or phrase. A noticeable
shift has occurred to consider a more fine-grained
approach, known as aspect-based sentiment analysis
(ABSA). For this task the goal is to automatically
identify the aspects of given target entities and the
sentiment expressed towards each of them. In this
paper, we present the LT3 system that participated
in this year’s SemEval 2015 ABSA task. Though
the focus was on the same domains (restaurants and
laptops) as last year’s task (Pontiki et al., 2014), it
differed in two ways. This time, entire reviews were
to be annotated and for one subtask the systems were
confronted with an out-of-domain test set, unknown
to the participants.
The task ran in two phases. In the first phase
(Phase A), the participants were given two test sets
(one for the laptops and one for the restaurants do-
main). The restaurant sentences were to be anno-
tated with automatically identified &lt;target, aspect
category&gt; tuples, the laptop sentences only with the
identified aspect categories. In the second phase
(Phase B), the gold annotations for the above two
datasets, as well as for a hidden domain, were given
and the participants had to return the corresponding
polarities (positive, negative, neutral). For more in-
formation we refer to Pontiki et al. (2015).
We tackled the problem by dividing the ABSA
task into three incremental subtasks: (i) aspect term
extraction, (ii) aspect term classification and (iii) as-
pect term polarity estimation (Pavlopoulos and An-
droutsopoulos, 2014). The first two are at the basis
of Phase A, whereas the final one constitutes Phase
B. For the first step, viz. extracting terms (or tar-
gets), we wanted to test our in-house hybrid termi-
nology extraction system (Section 2). Next, we per-
formed a multiclass classification task relying on a
feature space containing both lexical and semantic
information to aggregate the previously identified
terms into the domain-specific and predefined as-
pects (or aspect categories) (Section 3). Finally, we
performed polarity classification by deriving both
general and domain-specific lexical features from
the reviews (Section 4). We finish with conclusions
and prospects for future work (Section 5).
</bodyText>
<sectionHeader confidence="0.9241" genericHeader="method">
2 Aspect Term Extraction
</sectionHeader>
<bodyText confidence="0.99822">
Before starting with any sort of classification, it
is essential to know which entities or concepts are
present in the reviews. According to Wright (1997),
these “words that are assigned to concepts used in
</bodyText>
<page confidence="0.979065">
719
</page>
<bodyText confidence="0.978600672727273">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 719–724,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
the special languages that occur in subject-field or
domain-related texts” are called terms. Translated to
the current challenge, we are thus looking for words
or terms specific to a specific domain or interest,
such as the restaurant domain.
In order to detect these terms, we tested
our in-house terminology extraction system TEx-
SIS (Macken et al., 2013), which is a hybrid
system combining linguistic and statistical infor-
mation. For the linguistic analysis, TExSIS re-
lies on tokenized, Part-of-Speech tagged, lemma-
tized and chunked data using the LeTs Preprocess
toolkit (Van de Kauter et al., 2013), which is in-
corporated in the architecture. Subsequently, all
words and chunks matching certain Part-of-Speech
patterns (i.e. nouns and noun phrases) were con-
sidered as candidate terms. In order to determine
the specificity of and cohesion between these can-
didate terms, we combine several statistical filters
to represent the termhood and unithood of the can-
didate terms (Kageura and Umino, 1996). To this
purpose, we employed Log-likelihood (Rayson and
Garside, 2000), C-value (Frantzi et al., 2000) and
termhood (Vintar, 2010). All these statistical fil-
ters were calculated using the Web 1T 5-gram cor-
pus (Brants and Franz, 2006) as a reference corpus.
After a manual inspection of the first output
for the training data, we formulated some filter-
ing heuristics. We filter out terms consisting of
more than six words, terms that refer to location
names or that contain sentiment words. Locations
are found using the Stanford CoreNLP toolkit (Man-
ning et al., 2014) and for the sentiment words, we
filter those terms occurring in one of the follow-
ing sentiment lexicons: AFINN (Nielsen, 2011),
General Inquirer (Stone et al., 1966), NRC Emo-
tion (Mohammad and Turney, 2010; Mohammad
and Yang, 2011), MPQA (Wilson et al., 2005) and
Bing Liu (Hu and Liu, 2004).
The terms that resulted from this filtered TExSIS
output, supplemented with those terms that were an-
notated in the training data but not recognized by our
terminology extraction system, were all considered
as candidate terms. Finally, this list of candidate tar-
gets was further extended by also including corefer-
ential links as null terms. Coreference resolution of
each individual review was performed with the Stan-
ford multi-pass sieve coreference resolution system
(Lee et al., 2011). We should also point out that we
only allowed terms to be identified in the test data
when a sentence contains a subjective opinion. This
was done by running it through the above-mentioned
sentiment lexicons.
</bodyText>
<sectionHeader confidence="0.961044" genericHeader="method">
3 Phase A
</sectionHeader>
<bodyText confidence="0.999963842105263">
Given a list of possible candidate terms, the next step
consists in aggregating these terms to broader aspect
categories. As our main focus was on combining as-
pect term extraction with classification and since no
targets were annotated for the laptops, we decided
to focus on the restaurants domain. The organizers
provided the participants with training data consist-
ing of 254 annotated restaurant reviews. The task
was then to assign each identified term to a correct
aspect category.
For the classification task, we relied on a rich
feature space for each of the candidate targets and
performed classification into the domain-specific
categories. Whereas the annotations allow for a
two-step classification procedure by first classify-
ing the main categories and afterwards the subcat-
egories, we chose to perform the joint classification
as this yielded better results in our exploratory ex-
periments.
</bodyText>
<subsectionHeader confidence="0.999196">
3.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.914256555555556">
For all candidate terms present in our data sets we
derived a number of lexical and semantic features.
For those candidate targets that have been recog-
nized as anaphors (see Section 2), these features
were derived based on the corresponding antecedent.
First of all, we derived bag-of-words token uni-
gram features of the sentence in which a term occurs
in order to represent some of the lexical information
present in each of the categories.
The main part of our feature vectors, however,
was made up of semantic features, which should
enable us to classify our aspect terms into the
predefined categories. These semantic features
consist of:
1. WordNet features: for each main category, a
value is derived indicating the number of (unique)
terms annotated as aspect terms from that cate-
gory in the training data that (1) co-occur in the
</bodyText>
<page confidence="0.968054">
720
</page>
<bodyText confidence="0.998499166666667">
synset of the candidate term or (2) which are a hy-
ponym/hypernym of a term in the synset. In case the
candidate term is a multi-word term whose full term
is not found, this value is calculated for all nouns in
the multi-word term and the resulting sum is divided
by the number of nouns.
</bodyText>
<listItem confidence="0.99980175">
2. Cluster features: using the implementa-
tion of the Brown hierarchical word clustering al-
gorithm (Brown et al., 1992) by Liang (2005), we
derived clusters from the Yelp dataset1. Then, we
derived for each main category a value indicating the
number of (unique) terms annotated as aspect terms
from that category in the training data that co-occur
with the candidate term in the same cluster. Since
clusters can only contain single words, we calculate
this value for all the nouns in a multi-word term and
take the mean of the resulting sum.
3. Linked Open Data (LOD) features: using
DBpedia (Lehmann et al., 2013), we included
binary values indicating whether a candidate
term occurs in one of the following DBpedia
categories: Foods, Cuisine, Alcoholic beverages,
</listItem>
<bodyText confidence="0.9043784">
Non-alcoholic beverages, Atmosphere, Peo-
ple in food and agriculture occupations or
Food services occupations. These features were
automatically derived using the RapidMiner Linked
Open Data Extension (Paulheim et al., 2014).
4. Training data features: number of annota-
tions in the training data for each of the main cate-
gories. We filtered out candidate terms for which all
of these feature values are “0”, but decided to keep
proper nouns and proper noun phrases.
</bodyText>
<subsectionHeader confidence="0.998797">
3.2 Classification and Results
</subsectionHeader>
<bodyText confidence="0.99998625">
For all our experiments, we used LIBSVM (Chang
and Lin, 2001). In order to tune our system, we
split the training data into a train (90%) and test fold
(10%) and ran various rounds of experiments, af-
ter which we manually analyzed the output. Based
on this analysis, we were able to derive some post-
processing heuristics to rule out some of the low-
hanging fruit (i.e. misclassification which could be
ruled out univocally). To do so, we built a dictio-
nary containing all targets annotated in the training
data, together with their associated category label(s).
In case our classifier assigns a main category to a
</bodyText>
<footnote confidence="0.983404">
1https://www.yelp.com/academic dataset
</footnote>
<bodyText confidence="0.9996404">
target term that is never associated with the respec-
tive target in the training dictionary, we overrule the
classification output and replace it by the (most fre-
quent) category-subcategory label that is associated
with this target in the training dictionary.
The results of our system on the final test set and
rank are presented in Table 1, where Slot 1 refers to
the aspect category classification and Slot 2 to the
task of finding the correct opinion target expressions
(or terms).
</bodyText>
<table confidence="0.99967225">
Slot Precision Recall F-score Rank
Slot 1 51.54 56.00 53.68 8/15
Slot 2 36.47 79.34 49.97 13/21
Slot 1,2 29.44 44.73 35.51 6/13
</table>
<tableCaption confidence="0.999952">
Table 1: Results of the LT3 system on Phase A
</tableCaption>
<bodyText confidence="0.999951157894737">
For the design of our system we wanted to focus
most on the combination of Slot 1 and 2, i.e. finding
the target terms and being able to classify them in the
correct category. This is the most difficult task of all
three, hence the lower F-scores in general (Pontiki et
al., 2015). Though there is much room for improve-
ment for our system, we do observe that our rank
increases for this more difficult task. Our precision
scores are rather low, but we obtain the best recall
scores for Slot 2 and Slot 1,2. For Slot 1,2 we are
able to find 378 of the 845 possible targets, resulting
in the best recall score of all participating systems
(e.g. 44.73 compared to a recall score of 41.73 ob-
tained by the winning team).
This leads us to conclude that there’s quite some
room for improvement for the aggregation phase.
Normally, the similarity between terms is first com-
puted after which some sort of clustering is per-
formed
</bodyText>
<sectionHeader confidence="0.992968" genericHeader="method">
4 Phase B
</sectionHeader>
<bodyText confidence="0.999670888888889">
In recent years, sentiment analysis has been a pop-
ular research strand. An example is last year’s Se-
mEval task 9 Sentiment Analysis in Twitter, which
drew over 45 participants. The competition revealed
that the best systems use supervised machine learn-
ing techniques and rely much on lexical features in
the form of n-grams and sentiment lexicons (Rosen-
thal et al., 2014). For Phase B, in which we had
all gold standard terms and aspect categories avail-
</bodyText>
<page confidence="0.994643">
721
</page>
<bodyText confidence="0.99947925">
able, we decided to extend our LT3 system with an-
other classification round where we classify every
aspect as positive, negative or neutral. All features
are derived from the sentence in which the terms
were found and we participated in all three domains.
data since we assumed hotels to be more similar to
restaurants than they are to laptops. The results of
our system are presented in Table 2.
</bodyText>
<figure confidence="0.429228333333333">
Domain Accuracy Rank
Restaurants
75.03 4/15
</figure>
<subsectionHeader confidence="0.98498">
4.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999982619047619">
We implemented a number of lexical features. First
of all, we derived bag-of-words token unigram fea-
tures. Then, we also generated features using two
of the more well-known sentiment lexicons: Gen-
eral Inquirer (Stone et al., 1966) and Bing Liu (Hu
and Liu, 2004) and a manually constructed list of
negation cues based on the training data of SemEval-
2014 task 9 (Van Hee et al., 2014). Moreover, for
both the restaurants and laptops domain we created a
list of all the domain-specific positive, negative and
neutral words based on the training data. For the ho-
tels we were not able to compile such a list.
Finally, we also included PMI features based
on three domain-specific datasets. PMI (pointwise
mutual information) values indicate the association
of a word with positive and negative sentiment:
the higher the PMI score, the stronger the word-
sentiment association. We calculated this for each
unigram based on the word-sentiment associations
found in the respective training dataset. PMI values
were calculated as follows:
</bodyText>
<equation confidence="0.9979355">
PMI(w) = PMI(w, positive) − PMI(w, negative)
(1)
</equation>
<bodyText confidence="0.999981916666667">
As the equation shows, the association score of a
word with negative sentiment is subtracted from
the word’s association score with positive senti-
ment. For the restaurants domain we relied on
the Yelp dataset (cfr. Section 3.1), for the lap-
tops domain on a subset of the Amazon electronics
dataset (McAuley and Leskovec, 2013), and for the
hidden – hotel – domain we worked with reviews
collected from TripAdvisor (Wang et al., 2011). All
datasets were filtered by only including reviews with
strong subjective ratings (e.g. we preferred a 5 star
rating for positive reviews over one of 3 stars).
</bodyText>
<subsectionHeader confidence="0.987313">
4.2 Classification and Results
</subsectionHeader>
<bodyText confidence="0.998649">
We again used LIBSVM as our learner. For the
restaurants and laptops domain, we used the re-
spective training data sets. For the hidden (ho-
tel) domain, we only used the restaurants training
</bodyText>
<table confidence="0.503238">
73.76 5/13
80.53 2/11
</table>
<tableCaption confidence="0.995755">
Table 2: Result of the LT3 system on Phase B
</tableCaption>
<bodyText confidence="0.999766222222222">
Our results show that using only lexical features
already results in quite satisfying accuracy scores for
all three domains. Considering the hotels dataset,
we can conclude that having training data available
from a very similar domain does already result in a
satisfying accuracy (our system has the second best
score on the hidden domain). In the future, we will
investigate the performance gain when also includ-
ing domain-specific training data.
</bodyText>
<sectionHeader confidence="0.989309" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999948384615384">
We presented the LT3 system, which is able to tackle
the aspect-based sentiment analysis task incremen-
tally by first deriving candidate terms, after which
these are classified into various categories and po-
larities. Applying a hybrid terminology extraction
system to the first phase seems to be a promising ap-
proach. Our experiments revealed that we are able
to receive high recall for the task of deriving tar-
gets and aspect categories using a variety of lexical
and semantic features. When it comes to the polar-
ity estimation, we see that a classifier mostly relying
on lexical information achieves a satisfying perfor-
mance, even on out-of-domain data.
Based on our results, we see different directions
for follow-up research. For the term extraction, we
will focus on more powerful filtering techniques.
With respect to term aggregation, we will explore
new techniques of clustering our list of candidate
terms in different manners. Furthemore, we will ex-
plore in future experiments to which extent deeper
syntactic, semantic and discourse modelling leads
to better polarity classification. Since the TEx-
SIS system was developed as a multilingual frame-
work (Macken et al., 2013), we are currently trans-
lating the LT3 system so that it can handle Dutch
reviews.
</bodyText>
<figure confidence="0.498572">
Laptops
Hotels
</figure>
<page confidence="0.985524">
722
</page>
<sectionHeader confidence="0.955934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998087333333333">
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1 LDC2006T13. Web Download.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms: the
C-value/NC-value method. International Journal on
Digital Libraries, 3(2):115–130.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD04, pages 168–
177, New York, NY. ACM.
Kyo Kageura and Bin Umino. 1996. Methods of au-
tomatic term recognition. A review. Terminology,
3(2):259–289.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the CoNLL-2011 Shared Task.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N. Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick van Kleef,
Soren Auer, and Christian Bizer. 2013. DBpedia – A
Large-scale, Multilingual Knowledge Base Extracted
from Wikipedia. Semantic Web Journal.
Percy Liang. 2005. Semi-supervised learning for natural
language. In MASTERS THESIS, MIT.
Lieve Macken, Els Lefever, and V´eronique Hoste. 2013.
TExSIS: Bilingual Terminology Extraction from Par-
allel Corpora Using Chunk-based Alignment. Termi-
nology, 19(1):1–30.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations.
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: Understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM Conference on Recommender Systems, RecSys
’13, pages 165–172.
Saif Mohammad and Peter Turney. 2010. Emotions
Evoked by Common Words and Phrases: Using Me-
chanical Turk to Create an Emotion Lexicon. In Pro-
ceedings of the NAACL-HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, LA, California.
Saif Mohammad and Tony Yang. 2011. Tracking Sen-
timent in Mail: How Genders Differ on Emotional
Axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2011), pages 70–79, Portland, Ore-
gon. ACL.
Finn Nielsen. 2011. A new anew: Evaluation of a word
list for sentiment analysis in microblogs. In Proceed-
ings of the ESWC2011 Workshop on Making Sense of
Microposts: Big things come in small packages.
Heiko Paulheim, Petar Ristoski, Evgeny Mitichkin, and
Christian Bizer. 2014. Data mining with background
knowledge from the web. In Proceedings of the 5th
RapidMiner World.
John Pavlopoulos and Ion Androutsopoulos. 2014. As-
pect term extraction for sentiment analysis: New
datasets, new evaluation measures and an improved
unsupervised method. In Proceedings of the 5th
Workshop on Language Analysis for Social Media
(LASM)@ EACL.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 27–35, Dublin, Ireland, August.
Association for Computational Linguistics and Dublin
City University.
Maria Pontiki, Dimitris Galanis, Harris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
Semeval-2015 task 12: Aspect based sentiment analy-
sis. In Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015), Denver, Col-
orado.
Paul Rayson and Roger Garside. 2000. Comparing cor-
pora using frequency profiling. In Proceedings of the
workshop on Comparing corpora, 38th annual meet-
ing of the Association for Computational Linguistics,
pages 1–6, Hong Kong, China.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 73–80, Dublin, Ireland, August. Associ-
ation for Computational Linguistics and Dublin City
University.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press.
Marjan Van de Kauter, Geert Coorman, Els Lefever, Bart
Desmet, Lieve Macken, and V´eronique Hoste. 2013.
</reference>
<page confidence="0.985133">
723
</page>
<reference confidence="0.998914451612903">
LeTs Preprocess: The multilingual LT3 linguistic pre-
processing toolkit. Computational Linguistics in the
Netherlands Journal, 3:103–120.
Cynthia Van Hee, Marjan Van de Kauter, Orphee
De Clercq, Els Lefever, and Veronique Hoste. 2014.
Lt3: Sentiment classification in user-generated con-
tent using a rich feature set. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval2014), pages 406–410, Dublin, Ireland, August.
Association for Computational Linguistics and Dublin
City University.
ˇSpela Vintar. 2010. Bilingual term recognition revisited:
The bag-of-equivalents term alignment approach and
its evaluation. Terminology, 16:141–158.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’11, pages 618–
626.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT05, pages
347–354, Stroudsburg, PA. ACL.
Sue Ellen Wright. 1997. Term selection: the initial phase
of terminology management. In Sue Ellen Wright
and Gerhard Budin, editors, Handbook of terminology
management, pages 13–23. John Benjamins, Amster-
dam.
</reference>
<page confidence="0.998228">
724
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431929">
<title confidence="0.961265">LT3: Applying Hybrid Terminology Extraction to Aspect-Based Sentiment Analysis Orph´ee De Clercq, Marjan Van de Kauter, Els Lefever and V´eronique</title>
<author confidence="0.896072">Language</author>
<author confidence="0.896072">Translation Technology</author>
<affiliation confidence="0.983601">Department of Translation, Interpreting and Communication – Ghent</affiliation>
<address confidence="0.583236">Groot-Brittanni¨elaan 45, 9000 Ghent,</address>
<email confidence="0.751109">Firstname.Lastname@UGent.be</email>
<abstract confidence="0.999491923076923">The LT3 system perceives ABSA as a task consisting of three main subtasks, which have to be tackled incrementally, namely aspect term extraction, classification and polarity classification. For the first two steps, we see that employing a hybrid terminology extraction system leads to promising results, especially when it comes to recall. For the polarity classification, we show that it is possible to gain satisfying accuracies, even on out-ofdomain data, with a basic model employing only lexical information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1 LDC2006T13. Web Download.</booktitle>
<contexts>
<context position="4908" citStr="Brants and Franz, 2006" startWordPosition="752" endWordPosition="755"> incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and B</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1 LDC2006T13. Web Download.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="8416" citStr="Brown et al., 1992" startWordPosition="1329" endWordPosition="1332">es consist of: 1. WordNet features: for each main category, a value is derived indicating the number of (unique) terms annotated as aspect terms from that category in the training data that (1) co-occur in the 720 synset of the candidate term or (2) which are a hyponym/hypernym of a term in the synset. In case the candidate term is a multi-word term whose full term is not found, this value is calculated for all nouns in the multi-word term and the resulting sum is divided by the number of nouns. 2. Cluster features: using the implementation of the Brown hierarchical word clustering algorithm (Brown et al., 1992) by Liang (2005), we derived clusters from the Yelp dataset1. Then, we derived for each main category a value indicating the number of (unique) terms annotated as aspect terms from that category in the training data that co-occur with the candidate term in the same cluster. Since clusters can only contain single words, we calculate this value for all the nouns in a multi-word term and take the mean of the resulting sum. 3. Linked Open Data (LOD) features: using DBpedia (Lehmann et al., 2013), we included binary values indicating whether a candidate term occurs in one of the following DBpedia c</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2001</date>
<contexts>
<context position="9621" citStr="Chang and Lin, 2001" startWordPosition="1523" endWordPosition="1526">owing DBpedia categories: Foods, Cuisine, Alcoholic beverages, Non-alcoholic beverages, Atmosphere, People in food and agriculture occupations or Food services occupations. These features were automatically derived using the RapidMiner Linked Open Data Extension (Paulheim et al., 2014). 4. Training data features: number of annotations in the training data for each of the main categories. We filtered out candidate terms for which all of these feature values are “0”, but decided to keep proper nouns and proper noun phrases. 3.2 Classification and Results For all our experiments, we used LIBSVM (Chang and Lin, 2001). In order to tune our system, we split the training data into a train (90%) and test fold (10%) and ran various rounds of experiments, after which we manually analyzed the output. Based on this analysis, we were able to derive some postprocessing heuristics to rule out some of the lowhanging fruit (i.e. misclassification which could be ruled out univocally). To do so, we built a dictionary containing all targets annotated in the training data, together with their associated category label(s). In case our classifier assigns a main category to a 1https://www.yelp.com/academic dataset target ter</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina Frantzi</author>
<author>Sophia Ananiadou</author>
<author>Hideki Mima</author>
</authors>
<title>Automatic recognition of multi-word terms: the C-value/NC-value method.</title>
<date>2000</date>
<journal>International Journal on Digital Libraries,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="4777" citStr="Frantzi et al., 2000" startWordPosition="730" endWordPosition="733">ized, Part-of-Speech tagged, lemmatized and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), Genera</context>
</contexts>
<marker>Frantzi, Ananiadou, Mima, 2000</marker>
<rawString>Katerina Frantzi, Sophia Ananiadou, and Hideki Mima. 2000. Automatic recognition of multi-word terms: the C-value/NC-value method. International Journal on Digital Libraries, 3(2):115–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="5534" citStr="Hu and Liu, 2004" startWordPosition="858" endWordPosition="861">erence corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system (Lee et al., 2011). We should also point out that we only allowed terms to be identified in the test data when a sentence contains</context>
<context position="12983" citStr="Hu and Liu, 2004" startWordPosition="2105" endWordPosition="2108"> positive, negative or neutral. All features are derived from the sentence in which the terms were found and we participated in all three domains. data since we assumed hotels to be more similar to restaurants than they are to laptops. The results of our system are presented in Table 2. Domain Accuracy Rank Restaurants 75.03 4/15 4.1 Feature Extraction We implemented a number of lexical features. First of all, we derived bag-of-words token unigram features. Then, we also generated features using two of the more well-known sentiment lexicons: General Inquirer (Stone et al., 1966) and Bing Liu (Hu and Liu, 2004) and a manually constructed list of negation cues based on the training data of SemEval2014 task 9 (Van Hee et al., 2014). Moreover, for both the restaurants and laptops domain we created a list of all the domain-specific positive, negative and neutral words based on the training data. For the hotels we were not able to compile such a list. Finally, we also included PMI features based on three domain-specific datasets. PMI (pointwise mutual information) values indicate the association of a word with positive and negative sentiment: the higher the PMI score, the stronger the wordsentiment assoc</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD04, pages 168– 177, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyo Kageura</author>
<author>Bin Umino</author>
</authors>
<title>Methods of automatic term recognition. A review.</title>
<date>1996</date>
<journal>Terminology,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="4673" citStr="Kageura and Umino, 1996" startWordPosition="715" endWordPosition="718">ystem combining linguistic and statistical information. For the linguistic analysis, TExSIS relies on tokenized, Part-of-Speech tagged, lemmatized and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, w</context>
</contexts>
<marker>Kageura, Umino, 1996</marker>
<rawString>Kyo Kageura and Bin Umino. 1996. Methods of automatic term recognition. A review. Terminology, 3(2):259–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the CoNLL-2011 Shared Task.</booktitle>
<contexts>
<context position="6022" citStr="Lee et al., 2011" startWordPosition="933" endWordPosition="936">66), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system (Lee et al., 2011). We should also point out that we only allowed terms to be identified in the test data when a sentence contains a subjective opinion. This was done by running it through the above-mentioned sentiment lexicons. 3 Phase A Given a list of possible candidate terms, the next step consists in aggregating these terms to broader aspect categories. As our main focus was on combining aspect term extraction with classification and since no targets were annotated for the laptops, we decided to focus on the restaurants domain. The organizers provided the participants with training data consisting of 254 a</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the CoNLL-2011 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Lehmann</author>
<author>Robert Isele</author>
<author>Max Jakob</author>
<author>Anja Jentzsch</author>
<author>Dimitris Kontokostas</author>
<author>Pablo N Mendes</author>
<author>Sebastian Hellmann</author>
<author>Mohamed Morsey</author>
<author>Patrick van Kleef</author>
<author>Soren Auer</author>
<author>Christian Bizer</author>
</authors>
<title>DBpedia – A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web Journal.</title>
<date>2013</date>
<marker>Lehmann, Isele, Jakob, Jentzsch, Kontokostas, Mendes, Hellmann, Morsey, van Kleef, Auer, Bizer, 2013</marker>
<rawString>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, Soren Auer, and Christian Bizer. 2013. DBpedia – A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. In</title>
<date>2005</date>
<tech>MASTERS THESIS, MIT.</tech>
<contexts>
<context position="8432" citStr="Liang (2005)" startWordPosition="1334" endWordPosition="1335">et features: for each main category, a value is derived indicating the number of (unique) terms annotated as aspect terms from that category in the training data that (1) co-occur in the 720 synset of the candidate term or (2) which are a hyponym/hypernym of a term in the synset. In case the candidate term is a multi-word term whose full term is not found, this value is calculated for all nouns in the multi-word term and the resulting sum is divided by the number of nouns. 2. Cluster features: using the implementation of the Brown hierarchical word clustering algorithm (Brown et al., 1992) by Liang (2005), we derived clusters from the Yelp dataset1. Then, we derived for each main category a value indicating the number of (unique) terms annotated as aspect terms from that category in the training data that co-occur with the candidate term in the same cluster. Since clusters can only contain single words, we calculate this value for all the nouns in a multi-word term and take the mean of the resulting sum. 3. Linked Open Data (LOD) features: using DBpedia (Lehmann et al., 2013), we included binary values indicating whether a candidate term occurs in one of the following DBpedia categories: Foods</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. In MASTERS THESIS, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lieve Macken</author>
<author>Els Lefever</author>
<author>V´eronique Hoste</author>
</authors>
<title>TExSIS: Bilingual Terminology Extraction from Parallel Corpora Using Chunk-based Alignment.</title>
<date>2013</date>
<tech>Terminology,</tech>
<contexts>
<context position="4028" citStr="Macken et al., 2013" startWordPosition="616" endWordPosition="619">cording to Wright (1997), these “words that are assigned to concepts used in 719 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 719–724, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics the special languages that occur in subject-field or domain-related texts” are called terms. Translated to the current challenge, we are thus looking for words or terms specific to a specific domain or interest, such as the restaurant domain. In order to detect these terms, we tested our in-house terminology extraction system TExSIS (Macken et al., 2013), which is a hybrid system combining linguistic and statistical information. For the linguistic analysis, TExSIS relies on tokenized, Part-of-Speech tagged, lemmatized and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of </context>
</contexts>
<marker>Macken, Lefever, Hoste, 2013</marker>
<rawString>Lieve Macken, Els Lefever, and V´eronique Hoste. 2013. TExSIS: Bilingual Terminology Extraction from Parallel Corpora Using Chunk-based Alignment. Terminology, 19(1):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</booktitle>
<contexts>
<context position="5242" citStr="Manning et al., 2014" startWordPosition="807" endWordPosition="811">ood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including c</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian McAuley</author>
<author>Jure Leskovec</author>
</authors>
<title>Hidden factors and hidden topics: Understanding rating dimensions with review text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13,</booktitle>
<pages>165--172</pages>
<contexts>
<context position="14124" citStr="McAuley and Leskovec, 2013" startWordPosition="2290" endWordPosition="2293">and negative sentiment: the higher the PMI score, the stronger the wordsentiment association. We calculated this for each unigram based on the word-sentiment associations found in the respective training dataset. PMI values were calculated as follows: PMI(w) = PMI(w, positive) − PMI(w, negative) (1) As the equation shows, the association score of a word with negative sentiment is subtracted from the word’s association score with positive sentiment. For the restaurants domain we relied on the Yelp dataset (cfr. Section 3.1), for the laptops domain on a subset of the Amazon electronics dataset (McAuley and Leskovec, 2013), and for the hidden – hotel – domain we worked with reviews collected from TripAdvisor (Wang et al., 2011). All datasets were filtered by only including reviews with strong subjective ratings (e.g. we preferred a 5 star rating for positive reviews over one of 3 stars). 4.2 Classification and Results We again used LIBSVM as our learner. For the restaurants and laptops domain, we used the respective training data sets. For the hidden (hotel) domain, we only used the restaurants training 73.76 5/13 80.53 2/11 Table 2: Result of the LT3 system on Phase B Our results show that using only lexical f</context>
</contexts>
<marker>McAuley, Leskovec, 2013</marker>
<rawString>Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13, pages 165–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
</authors>
<title>Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<contexts>
<context position="5448" citStr="Mohammad and Turney, 2010" startWordPosition="842" endWordPosition="845">tical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system (Lee et al., 2011). We should also point out</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif Mohammad and Peter Turney. 2010. Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking Sentiment in Mail: How Genders Differ on Emotional Axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2011),</booktitle>
<pages>70--79</pages>
<publisher>ACL.</publisher>
<location>Portland, Oregon.</location>
<contexts>
<context position="5474" citStr="Mohammad and Yang, 2011" startWordPosition="846" endWordPosition="849">ed using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system (Lee et al., 2011). We should also point out that we only allowed term</context>
</contexts>
<marker>Mohammad, Yang, 2011</marker>
<rawString>Saif Mohammad and Tony Yang. 2011. Tracking Sentiment in Mail: How Genders Differ on Emotional Axes. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2011), pages 70–79, Portland, Oregon. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn Nielsen</author>
</authors>
<title>A new anew: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on Making Sense of Microposts: Big things</booktitle>
<note>come in small packages.</note>
<contexts>
<context position="5369" citStr="Nielsen, 2011" startWordPosition="831" endWordPosition="832">(Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass s</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn Nielsen. 2011. A new anew: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on Making Sense of Microposts: Big things come in small packages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heiko Paulheim</author>
<author>Petar Ristoski</author>
<author>Evgeny Mitichkin</author>
<author>Christian Bizer</author>
</authors>
<title>Data mining with background knowledge from the web.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th RapidMiner World.</booktitle>
<contexts>
<context position="9287" citStr="Paulheim et al., 2014" startWordPosition="1465" endWordPosition="1468">term in the same cluster. Since clusters can only contain single words, we calculate this value for all the nouns in a multi-word term and take the mean of the resulting sum. 3. Linked Open Data (LOD) features: using DBpedia (Lehmann et al., 2013), we included binary values indicating whether a candidate term occurs in one of the following DBpedia categories: Foods, Cuisine, Alcoholic beverages, Non-alcoholic beverages, Atmosphere, People in food and agriculture occupations or Food services occupations. These features were automatically derived using the RapidMiner Linked Open Data Extension (Paulheim et al., 2014). 4. Training data features: number of annotations in the training data for each of the main categories. We filtered out candidate terms for which all of these feature values are “0”, but decided to keep proper nouns and proper noun phrases. 3.2 Classification and Results For all our experiments, we used LIBSVM (Chang and Lin, 2001). In order to tune our system, we split the training data into a train (90%) and test fold (10%) and ran various rounds of experiments, after which we manually analyzed the output. Based on this analysis, we were able to derive some postprocessing heuristics to rule</context>
</contexts>
<marker>Paulheim, Ristoski, Mitichkin, Bizer, 2014</marker>
<rawString>Heiko Paulheim, Petar Ristoski, Evgeny Mitichkin, and Christian Bizer. 2014. Data mining with background knowledge from the web. In Proceedings of the 5th RapidMiner World.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Pavlopoulos</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Aspect term extraction for sentiment analysis: New datasets, new evaluation measures and an improved unsupervised method.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL.</booktitle>
<contexts>
<context position="2563" citStr="Pavlopoulos and Androutsopoulos, 2014" startWordPosition="390" endWordPosition="394">e annotated with automatically identified &lt;target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as well as for a hidden domain, were given and the participants had to return the corresponding polarities (positive, negative, neutral). For more information we refer to Pontiki et al. (2015). We tackled the problem by dividing the ABSA task into three incremental subtasks: (i) aspect term extraction, (ii) aspect term classification and (iii) aspect term polarity estimation (Pavlopoulos and Androutsopoulos, 2014). The first two are at the basis of Phase A, whereas the final one constitutes Phase B. For the first step, viz. extracting terms (or targets), we wanted to test our in-house hybrid terminology extraction system (Section 2). Next, we performed a multiclass classification task relying on a feature space containing both lexical and semantic information to aggregate the previously identified terms into the domain-specific and predefined aspects (or aspect categories) (Section 3). Finally, we performed polarity classification by deriving both general and domain-specific lexical features from the r</context>
</contexts>
<marker>Pavlopoulos, Androutsopoulos, 2014</marker>
<rawString>John Pavlopoulos and Ion Androutsopoulos. 2014. Aspect term extraction for sentiment analysis: New datasets, new evaluation measures and an improved unsupervised method. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitris Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>27--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1550" citStr="Pontiki et al., 2014" startWordPosition="228" endWordPosition="231">t analysis of user-generated content. Until recently, the main research focus has been on discovering the overall polarity of a certain text or phrase. A noticeable shift has occurred to consider a more fine-grained approach, known as aspect-based sentiment analysis (ABSA). For this task the goal is to automatically identify the aspects of given target entities and the sentiment expressed towards each of them. In this paper, we present the LT3 system that participated in this year’s SemEval 2015 ABSA task. Though the focus was on the same domains (restaurants and laptops) as last year’s task (Pontiki et al., 2014), it differed in two ways. This time, entire reviews were to be annotated and for one subtask the systems were confronted with an out-of-domain test set, unknown to the participants. The task ran in two phases. In the first phase (Phase A), the participants were given two test sets (one for the laptops and one for the restaurants domain). The restaurant sentences were to be annotated with automatically identified &lt;target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as w</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitris Galanis</author>
<author>Harris Papageorgiou</author>
<author>Suresh Manandhar</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Semeval-2015 task 12: Aspect based sentiment analysis.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="2338" citStr="Pontiki et al. (2015)" startWordPosition="358" endWordPosition="361">e participants. The task ran in two phases. In the first phase (Phase A), the participants were given two test sets (one for the laptops and one for the restaurants domain). The restaurant sentences were to be annotated with automatically identified &lt;target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as well as for a hidden domain, were given and the participants had to return the corresponding polarities (positive, negative, neutral). For more information we refer to Pontiki et al. (2015). We tackled the problem by dividing the ABSA task into three incremental subtasks: (i) aspect term extraction, (ii) aspect term classification and (iii) aspect term polarity estimation (Pavlopoulos and Androutsopoulos, 2014). The first two are at the basis of Phase A, whereas the final one constitutes Phase B. For the first step, viz. extracting terms (or targets), we wanted to test our in-house hybrid terminology extraction system (Section 2). Next, we performed a multiclass classification task relying on a feature space containing both lexical and semantic information to aggregate the previ</context>
<context position="11150" citStr="Pontiki et al., 2015" startWordPosition="1787" endWordPosition="1790">e presented in Table 1, where Slot 1 refers to the aspect category classification and Slot 2 to the task of finding the correct opinion target expressions (or terms). Slot Precision Recall F-score Rank Slot 1 51.54 56.00 53.68 8/15 Slot 2 36.47 79.34 49.97 13/21 Slot 1,2 29.44 44.73 35.51 6/13 Table 1: Results of the LT3 system on Phase A For the design of our system we wanted to focus most on the combination of Slot 1 and 2, i.e. finding the target terms and being able to classify them in the correct category. This is the most difficult task of all three, hence the lower F-scores in general (Pontiki et al., 2015). Though there is much room for improvement for our system, we do observe that our rank increases for this more difficult task. Our precision scores are rather low, but we obtain the best recall scores for Slot 2 and Slot 1,2. For Slot 1,2 we are able to find 378 of the 845 possible targets, resulting in the best recall score of all participating systems (e.g. 44.73 compared to a recall score of 41.73 obtained by the winning team). This leads us to conclude that there’s quite some room for improvement for the aggregation phase. Normally, the similarity between terms is first computed after whi</context>
</contexts>
<marker>Pontiki, Galanis, Papageorgiou, Manandhar, Androutsopoulos, 2015</marker>
<rawString>Maria Pontiki, Dimitris Galanis, Harris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>Roger Garside</author>
</authors>
<title>Comparing corpora using frequency profiling.</title>
<date>2000</date>
<booktitle>In Proceedings of the workshop on Comparing corpora, 38th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--6</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="4745" citStr="Rayson and Garside, 2000" startWordPosition="725" endWordPosition="728">tic analysis, TExSIS relies on tokenized, Part-of-Speech tagged, lemmatized and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicon</context>
</contexts>
<marker>Rayson, Garside, 2000</marker>
<rawString>Paul Rayson and Roger Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the workshop on Comparing corpora, 38th annual meeting of the Association for Computational Linguistics, pages 1–6, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="12171" citStr="Rosenthal et al., 2014" startWordPosition="1966" endWordPosition="1970"> 41.73 obtained by the winning team). This leads us to conclude that there’s quite some room for improvement for the aggregation phase. Normally, the similarity between terms is first computed after which some sort of clustering is performed 4 Phase B In recent years, sentiment analysis has been a popular research strand. An example is last year’s SemEval task 9 Sentiment Analysis in Twitter, which drew over 45 participants. The competition revealed that the best systems use supervised machine learning techniques and rely much on lexical features in the form of n-grams and sentiment lexicons (Rosenthal et al., 2014). For Phase B, in which we had all gold standard terms and aspect categories avail721 able, we decided to extend our LT3 system with another classification round where we classify every aspect as positive, negative or neutral. All features are derived from the sentence in which the terms were found and we participated in all three domains. data since we assumed hotels to be more similar to restaurants than they are to laptops. The results of our system are presented in Table 2. Domain Accuracy Rank Restaurants 75.03 4/15 4.1 Feature Extraction We implemented a number of lexical features. First</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73–80, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5408" citStr="Stone et al., 1966" startWordPosition="835" endWordPosition="838">d (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system (Lee</context>
<context position="12951" citStr="Stone et al., 1966" startWordPosition="2098" endWordPosition="2101"> where we classify every aspect as positive, negative or neutral. All features are derived from the sentence in which the terms were found and we participated in all three domains. data since we assumed hotels to be more similar to restaurants than they are to laptops. The results of our system are presented in Table 2. Domain Accuracy Rank Restaurants 75.03 4/15 4.1 Feature Extraction We implemented a number of lexical features. First of all, we derived bag-of-words token unigram features. Then, we also generated features using two of the more well-known sentiment lexicons: General Inquirer (Stone et al., 1966) and Bing Liu (Hu and Liu, 2004) and a manually constructed list of negation cues based on the training data of SemEval2014 task 9 (Van Hee et al., 2014). Moreover, for both the restaurants and laptops domain we created a list of all the domain-specific positive, negative and neutral words based on the training data. For the hotels we were not able to compile such a list. Finally, we also included PMI features based on three domain-specific datasets. PMI (pointwise mutual information) values indicate the association of a word with positive and negative sentiment: the higher the PMI score, the </context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marjan Van de Kauter</author>
<author>Geert Coorman</author>
<author>Els Lefever</author>
<author>Bart Desmet</author>
<author>Lieve Macken</author>
<author>V´eronique Hoste</author>
</authors>
<date>2013</date>
<marker>Van de Kauter, Coorman, Lefever, Desmet, Macken, Hoste, 2013</marker>
<rawString>Marjan Van de Kauter, Geert Coorman, Els Lefever, Bart Desmet, Lieve Macken, and V´eronique Hoste. 2013.</rawString>
</citation>
<citation valid="false">
<authors>
<author>LeTs Preprocess</author>
</authors>
<title>The multilingual LT3 linguistic preprocessing toolkit.</title>
<journal>Computational Linguistics in the Netherlands Journal,</journal>
<pages>3--103</pages>
<marker>Preprocess, </marker>
<rawString>LeTs Preprocess: The multilingual LT3 linguistic preprocessing toolkit. Computational Linguistics in the Netherlands Journal, 3:103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Van Hee</author>
<author>Marjan Van de Kauter</author>
<author>Orphee De Clercq</author>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>Lt3: Sentiment classification in user-generated content using a rich feature set.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval2014),</booktitle>
<pages>406--410</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<marker>Van Hee, Van de Kauter, De Clercq, Lefever, Hoste, 2014</marker>
<rawString>Cynthia Van Hee, Marjan Van de Kauter, Orphee De Clercq, Els Lefever, and Veronique Hoste. 2014. Lt3: Sentiment classification in user-generated content using a rich feature set. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval2014), pages 406–410, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ˇSpela Vintar</author>
</authors>
<title>Bilingual term recognition revisited: The bag-of-equivalents term alignment approach and its evaluation.</title>
<date>2010</date>
<tech>Terminology,</tech>
<pages>16--141</pages>
<contexts>
<context position="4805" citStr="Vintar, 2010" startWordPosition="736" endWordPosition="737">zed and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 19</context>
</contexts>
<marker>Vintar, 2010</marker>
<rawString>ˇSpela Vintar. 2010. Bilingual term recognition revisited: The bag-of-equivalents term alignment approach and its evaluation. Terminology, 16:141–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongning Wang</author>
<author>Yue Lu</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Latent aspect rating analysis without aspect keyword supervision.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11,</booktitle>
<pages>618--626</pages>
<contexts>
<context position="14231" citStr="Wang et al., 2011" startWordPosition="2309" endWordPosition="2312"> each unigram based on the word-sentiment associations found in the respective training dataset. PMI values were calculated as follows: PMI(w) = PMI(w, positive) − PMI(w, negative) (1) As the equation shows, the association score of a word with negative sentiment is subtracted from the word’s association score with positive sentiment. For the restaurants domain we relied on the Yelp dataset (cfr. Section 3.1), for the laptops domain on a subset of the Amazon electronics dataset (McAuley and Leskovec, 2013), and for the hidden – hotel – domain we worked with reviews collected from TripAdvisor (Wang et al., 2011). All datasets were filtered by only including reviews with strong subjective ratings (e.g. we preferred a 5 star rating for positive reviews over one of 3 stars). 4.2 Classification and Results We again used LIBSVM as our learner. For the restaurants and laptops domain, we used the respective training data sets. For the hidden (hotel) domain, we only used the restaurants training 73.76 5/13 80.53 2/11 Table 2: Result of the LT3 system on Phase B Our results show that using only lexical features already results in quite satisfying accuracy scores for all three domains. Considering the hotels d</context>
</contexts>
<marker>Wang, Lu, Zhai, 2011</marker>
<rawString>Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’11, pages 618– 626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT05,</booktitle>
<pages>347--354</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="5502" citStr="Wilson et al., 2005" startWordPosition="851" endWordPosition="854">s (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system (Lee et al., 2011). We should also point out that we only allowed terms to be identified in the te</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT05, pages 347–354, Stroudsburg, PA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue Ellen Wright</author>
</authors>
<title>Term selection: the initial phase of terminology management.</title>
<date>1997</date>
<booktitle>Handbook of terminology management,</booktitle>
<pages>13--23</pages>
<editor>In Sue Ellen Wright and Gerhard Budin, editors,</editor>
<publisher>John Benjamins,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="3432" citStr="Wright (1997)" startWordPosition="528" endWordPosition="529"> classification task relying on a feature space containing both lexical and semantic information to aggregate the previously identified terms into the domain-specific and predefined aspects (or aspect categories) (Section 3). Finally, we performed polarity classification by deriving both general and domain-specific lexical features from the reviews (Section 4). We finish with conclusions and prospects for future work (Section 5). 2 Aspect Term Extraction Before starting with any sort of classification, it is essential to know which entities or concepts are present in the reviews. According to Wright (1997), these “words that are assigned to concepts used in 719 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 719–724, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics the special languages that occur in subject-field or domain-related texts” are called terms. Translated to the current challenge, we are thus looking for words or terms specific to a specific domain or interest, such as the restaurant domain. In order to detect these terms, we tested our in-house terminology extraction system TExSIS (Macken et al., 2013), wh</context>
</contexts>
<marker>Wright, 1997</marker>
<rawString>Sue Ellen Wright. 1997. Term selection: the initial phase of terminology management. In Sue Ellen Wright and Gerhard Budin, editors, Handbook of terminology management, pages 13–23. John Benjamins, Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>