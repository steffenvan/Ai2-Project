<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996973">
Looking Under the Hood: Tools for Diagnosing Your Question
Answering Engine
</title>
<author confidence="0.998524">
Eric Breck†, Marc Light†, Gideon S. Mann*, Ellen Riloff°,
Brianne Brown$, Pranav Anand*, Mats Rooth�, Michael Thelen°
</author>
<affiliation confidence="0.949608833333333">
† The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org
� Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu
° School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu
$ Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu
* Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu
T Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.edu
</affiliation>
<sectionHeader confidence="0.989177" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999563">
In this paper we analyze two question
answering tasks : the TREC-8 ques-
tion answering task and a set of reading
comprehension exams. First, we show
that Q/A systems perform better when
there are multiple answer opportunities
per question. Next, we analyze com-
mon approaches to two subproblems:
term overlap for answer sentence iden-
tification, and answer typing for short
answer extraction. We present general
tools for analyzing the strengths and
limitations of techniques for these sub-
problems. Our results quantify the limi-
tations of both term overlap and answer
typing to distinguish between compet-
ing answer candidates.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994140217391304">
When building a system to perform a task, the
most important statistic is the performance on
an end-to-end evaluation. For the task of open-
domain question answering against text collec-
tions, there have been two large-scale end-to-
end evaluations: (TREC-8 Proceedings, 1999)
and (TREC-9 Proceedings, 2000). In addition, a
number of researchers have built systems to take
reading comprehension examinations designed to
evaluate children’s reading levels (Charniak et al.,
2000; Hirschman et al., 1999; Ng et al., 2000;
Riloff and Thelen, 2000; Wang et al., 2000). The
performance statistics have been useful for deter-
mining how well techniques work.
However, raw performance statistics are not
enough. If the score is low, we need to under-
stand what went wrong and how to fix it. If the
score is high, it is important to understand why.
For example, performance may be dependent on
characteristics of the current test set and would
not carry over to a new domain. It would also be
useful to know if there is a particular character-
istic of the system that is central. If so, then the
system can be streamlined and simplified.
In this paper, we explore ways of gaining
insight into question answering system perfor-
mance. First, we analyze the impact of having
multiple answer opportunities for a question. We
found that TREC-8 Q/A systems performed bet-
ter on questions that had multiple answer oppor-
tunities in the document collection. Second, we
present a variety of graphs to visualize and ana-
lyze functions for ranking sentences. The graphs
revealed that relative score instead of absolute
score is paramount. Third, we introduce bounds
on functions that use term overlap&apos; to rank sen-
tences. Fourth, we compute the expected score of
a hypothetical Q/A system that correctly identifies
the answer type for a question and correctly iden-
tifies all entities of that type in answer sentences.
We found that a surprising amount of ambiguity
remains because sentences often contain multiple
entities of the same type.
&apos;Throughout the text, we use “overlap” to refer to the
intersection of sets of words, most often the words in the
question and the words in a sentence.
</bodyText>
<sectionHeader confidence="0.796389" genericHeader="introduction">
2 The data
</sectionHeader>
<bodyText confidence="0.999895547619048">
The experiments in Sections 3, 4, and 5 were per-
formed on two question answering data sets: (1)
the TREC-8 Question Answering Track data set
and (2) the CBC reading comprehension data set.
We will briefly describe each of these data sets
and their corresponding tasks.
The task of the TREC-8 Question Answering
track was to find the answer to 198 questions us-
ing a document collection consisting of roughly
500,000 newswire documents. For each question,
systems were allowed to return a ranked list of
5 short (either 50-character or 250-character) re-
sponses. As a service to track participants, AT&amp;T
provided top documents returned by their retrieval
engine for each of the TREC questions. Sec-
tions 4 and 5 present analyses that use all sen-
tences in the top 10 of these documents. Each
sentence is classified as correct or incorrect auto-
matically. This automatic classification judges a
sentence to be correct if it contains at least half
of the stemmed, content-words in the answer key.
We have compared this automatic evaluation to
the TREC-8 QA track assessors and found it to
agree 93-95% of the time (Breck et al., 2000).
The CBC data set was created for the Johns
Hopkins Summer 2000 Workshop on Reading
Comprehension. Texts were collected from the
Canadian Broadcasting Corporation web page for
kids (http://cbc4kids.ca/). They are an average
of 24 sentences long. The stories were adapted
from newswire texts to be appropriate for ado-
lescent children, and most fall into the follow-
ing domains: politics, health, education, science,
human interest, disaster, sports, business, crime,
war, entertainment, and environment. For each
CBC story, 8-12 questions and an answer key
were generated.2 We used a 650 question sub-
set of the data and their corresponding 75 stories.
The answer candidates for each question in this
data set were all sentences in the document. The
sentences were scored against the answer key by
the automatic method described previously.
</bodyText>
<footnote confidence="0.96570175">
2This work was performed by Lisa Ferro and Tim Bevins
of the MITRE Corporation. Dr. Ferro has professional expe-
rience writing questions for reading comprehension exams
and led the question writing effort.
</footnote>
<sectionHeader confidence="0.4528055" genericHeader="method">
3 Analyzing the number of answer
opportunities per question
</sectionHeader>
<bodyText confidence="0.9987718">
In this section we explore the impact of multiple
answer opportunities on end-to-end system per-
formance. A question may have multiple answers
for two reasons: (1) there is more than one differ-
ent answer to the question, and (2) there may be
multiple instances of each answer. For example,
“What does the Peugeot company manufacture?”
can be answered by trucks, cars, or motors and
each of these answers may occur in many sen-
tences that provide enough context to answer the
question. The table insert in Figure 1 shows that,
on average, there are 7 answer occurrences per
question in the TREC-8 collection.3 In contrast,
there are only 1.25 answer occurrences in a CBC
document. The number of answer occurrences
varies widely, as illustrated by the standard devia-
tions. The median shows an answer frequency of
3 for TREC and 1 for CBC, which perhaps gives
a more realistic sense of the degree of answer fre-
quency for most questions.
</bodyText>
<sectionHeader confidence="0.531979" genericHeader="method">
# Answers
</sectionHeader>
<figureCaption confidence="0.991416">
Figure 1: Frequency of answers in the TREC-8
(black bars) and CBC (white bars) data sets
</figureCaption>
<bodyText confidence="0.999904666666667">
To gather this data we manually reviewed 50
randomly chosen TREC-8 questions and identi-
fied all answers to these questions in our text col-
lection. We defined an “answer” as a text frag-
ment that contains the answer string in a context
sufficient to answer the question. Figure 1 shows
the resulting graph. The x-axis displays the num-
ber of answer occurrences found in the text col-
lection per question and the y-axis shows the per-
</bodyText>
<footnote confidence="0.621122">
3We would like to thank John Burger and John Aberdeen
for help preparing Figure 1.
</footnote>
<figure confidence="0.99640924137931">
1 2 3 4 5 6 7 9 12 14 18 27 28 61 67
% Questions
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
# Questions
# Answers
Mean
Median
Standard Dev.
TREC-8
5 0
352
7.04
3
12.94
CBC
219
274
1.25
1
0.61
</figure>
<bodyText confidence="0.997309272727273">
centage of questions that had x answers. For ex-
ample, 26% of the TREC-8 questions had only
1 answer occurrence, and 20% of the TREC-8
questions had exactly 2 answer occurrences (the
black bars). The most prolific question had 67
answer occurrences (the Peugeot example men-
tioned above). Figure 1 also shows the analysis
of 219 CBC questions. In contrast, 80% of the
CBC questions had only 1 answer occurrence in
the targeted document, and 16% had exactly 2 an-
swer occurrences.
</bodyText>
<figure confidence="0.9166452">
Point per question
correct per occurrence
Mean #
0 10 20 30 40 50 60 70
# answers occurences per question
</figure>
<figureCaption confidence="0.993216">
Figure 2: Answer repetition vs. system response
correctness for TREC-8
</figureCaption>
<bodyText confidence="0.928560736842105">
Figure 2 shows the effect that multiple answer
opportunities had on the performance of TREC-8
systems. Each solid dot in the scatter plot repre-
sents one of the 50 questions we examined.4 The
x-axis shows the number of answer opportunities
for the question, and the y-axis represents the per-
centage of systems that generated a correct an-
swer5 for the question. E.g., for the question with
67 answer occurrences, 80% of the systems pro-
duced a correct answer. In contrast, many ques-
tions had a single answer occurrence and the per-
centage of systems that got those correct varied
from about 2% to 60%.
The circles in Figure 2 represent the average
percentage of systems that answered questions
correctly for all questions with the same number
of answer occurrences. For example, on average
about 27% of the systems produced a correct an-
swer for questions that had exactly one answer oc-
</bodyText>
<footnote confidence="0.9940218">
4We would like to thank Lynette Hirschman for suggest-
ing the analysis behind Figure 2 and John Burger for help
with the analysis and presentation.
5For this analysis, we say that a system generated a cor-
rect answer if a correct answer was in its response set.
</footnote>
<bodyText confidence="0.999905">
currence, but about 50% of the systems produced
a correct answer for questions with 7 answer op-
portunities. Overall, a clear pattern emerges: the
performance of TREC-8 systems was strongly
correlated with the number of answer opportuni-
ties present in the document collection.
</bodyText>
<sectionHeader confidence="0.992752" genericHeader="method">
4 Graphs for analyzing scoring
</sectionHeader>
<subsectionHeader confidence="0.983431">
functions of answer candidates
</subsectionHeader>
<bodyText confidence="0.99980765">
Most question answering systems generate sev-
eral answer candidates and rank them by defin-
ing a scoring function that maps answer candi-
dates to a range of numbers. In this section,
we analyze one particular scoring function: term
overlap between the question and answer can-
didate. The techniques we use can be easily
applied to other scoring functions as well (e.g.,
weighted term overlap, partial unification of sen-
tence parses, weighted abduction score, etc.). The
answer candidates we consider are the sentences
from the documents.
The expected performance of a system that
ranks all sentences using term overlap is 35% for
the TREC-8 data. This number is an expected
score because of ties: correct and incorrect can-
didates may have the same term overlap score. If
ties are broken optimally, the best possible score
(maximum) would be 54%. If ties are broken
maximally suboptimally, the worst possible score
(minimum) would be 24%. The corresponding
scores on the CBC data are 58% expected, 69%
maximum, and 51% minimum. We would like to
understand why the term overlap scoring function
works as well as it does and what can be done to
improve it.
Figures 3 and 4 compare correct candidates and
incorrect candidates with respect to the scoring
function. The x-axis plots the range of the scor-
ing function, i.e., the amount of overlap. The
y-axis represents Pr(overlap=z I correct) and
Pr(overlap=z I incorrect), where separate curves
are plotted for correct and incorrect candidates.
The probabilities are generated by normalizing
the number of correct/incorrect answer candidates
with a particular overlap score by the total number
of correct/incorrect candidates, respectively.
Figure 3 illustrates that the correct candidates
for TREC-8 have term overlap scores distributed
between 0 and 10 with a peak of 24% at an over-
</bodyText>
<figure confidence="0.998969">
0.9
0.8
7
0.
0.6
0.5
4
0.
0.3
2
0.
1
0.
0
% of systems with at least one correct response
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Normalized (+/3087,-/57073) Count
0 2 4 6 8 10 12 14 16 18 20
overlap
</figure>
<figureCaption confidence="0.999969">
Figure 4: Pr(overlap=x|[in]correct) for CBC
</figureCaption>
<bodyText confidence="0.999793111111111">
lap of 2. However, the incorrect candidates have
a similar distribution between 0 and 8 with a peak
of 32% at an overlap of 0. The similarity of the
curves illustrates that it is unclear how to use the
score to decide if a candidate is correct or not.
Certainly no static threshold above which a can-
didate is deemed correct will work. Yet the ex-
pected score of our TREC term overlap system
was 35%, which is much higher than a random
baseline which would get an expected score of
less than 3% because there are over 40 sentences
on average in newswire documents.6
After inspecting some of the data directly, we
posited that it was not the absolute term overlap
that was important for judging candidate but how
the overlap score compares to the scores of other
candidates. To visualize this, we generated new
graphs by plotting the rank of a candidate’s score
</bodyText>
<footnote confidence="0.957971">
6We also tried dividing the term overlap score by the
length of the question to normalize for query length but did
not find that the graph was any more helpful.
</footnote>
<bodyText confidence="0.999539142857143">
on the x-axis. For example, the candidate with
the highest score would be ranked first, the can-
didate with the second highest score would be
ranked second, etc. Figures 5 and 6 show these
graphs, which display Pr(rank=z  |correct) and
Pr(rank=z  |incorrect) on the y-axis. The top-
ranked candidate has rank=0.
</bodyText>
<figure confidence="0.510697">
ranked overlap
</figure>
<figureCaption confidence="0.970696">
Figure 5: Pr(rank=x  |[in]correct) for TREC-8
</figureCaption>
<figure confidence="0.826399">
-45 -40 -35 -30 -25 -20 -15 -10 -5 0
ranked overlap
</figure>
<figureCaption confidence="0.998977">
Figure 6: Pr(rank=x  |[in]correct) for CBC
</figureCaption>
<bodyText confidence="0.999976357142857">
The ranked graphs are more revealing than the
graphs of absolute scores: the probability of a
high rank is greater for correct answers than in-
correct ones. Now we can begin to understand
why the term overlap scoring function worked as
well as it did. We see that, unlike classification
tasks, there is no good threshold for our scor-
ing function. Instead relative score is paramount.
Systems such as (Ng et al., 2000) make explicit
use of relative rank in their algorithms and now
we understand why this is effective.
Before we leave the topic of graphing scoring
functions, we want to introduce one other view of
the data. Figure 7 plots term overlap scores on
</bodyText>
<figure confidence="0.9476795">
incorrect
correct
</figure>
<figureCaption confidence="0.574824">
Figure 3: Pr(overlap=x|[in]correct) for TREC-8
</figureCaption>
<figure confidence="0.999768142857143">
incorrect
correct
0 5 10 15 20 25 30
overlap
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Normalized (+/1311,-/14610) Count
0.02
0.018
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
incorrect
correct
0.3
0.25
0.2
0.15
0.1
0.05
0
incorrect
correct
Normalized (+/3087,-/57073) Count
-1000
-900
-800
-700
-600
-500
-400
-300
-200
-100
0
Normalized (+/1311,-/14610) Count
20000 could differentiate them. In addition, subset rela-
18000 tions often hold between overlaps. A candidate
16000 whose overlap is a subset of a second candidate
cannot score higher regardless of the weighting
12000
10000 as scheme.&apos; We formalize these overlap set relations
8000 m and then calculate statistics based on them for the
6000 CBC and TREC data.
4000
14000
mass
1
0.5
0
-0.5
-1
-1.5
-2
-2.5
-3
log-odds of correctness
-3.5
log-odds
mass curve
2000
-4 0
0 2 4 6 8 10 12 14
overlap
</figure>
<figureCaption confidence="0.999985">
Figure 7: TREC-8 log odds correct given overlap
</figureCaption>
<bodyText confidence="0.9066205">
the x-axis and the log odds of being correct given
a score on the y-axis. The log odds formula is:
</bodyText>
<equation confidence="0.997159">
Pr(correct|overlap)
log
Pr(incorrect|overlap)
</equation>
<bodyText confidence="0.99996525">
Intuitively, this graph shows how much more
likely a sentence is to be correct versus incorrect
given a particular score. A second curve, labeled
“mass,” plots the number of answer candidates
with each score. Figure 7 shows that the odds of
being correct are negative until an overlap of 10,
but the mass curve reveals that few answer candi-
dates have an overlap score greater than 6.
</bodyText>
<sectionHeader confidence="0.5722875" genericHeader="method">
5 Bounds on scoring functions that use
term overlap
</sectionHeader>
<bodyText confidence="0.84420764516129">
The scoring function used in the previous sec-
tion simply counts the number of terms shared
by a question and a sentence. One obvious mod-
ification is to weight some terms more heavily
than others. We tried using inverse document fre-
quence based (IDF) term weighting on the CBC
data but found that it did not improve perfor-
mance. The graph analogous to Figure 6 but with
IDF term weighting was virtually identical.
Could another weighting scheme perform bet-
ter? How well could an optimal weighting
scheme do? How poorly would the maximally
suboptimal scheme do? The analysis in this sec-
tion addresses these questions. In essence the an-
swer is the following: the question and the can-
didate answers are typically short and thus the
number of overlapping terms is small – conse-
quently, many candidate answers have exactly the
same overlapping terms and no weighting scheme
Question: How much was Babe Belanger paid to play
amateur basketball?
S1: She was a member of the winningest
basketball team Canada ever had.
S2: Babe Belanger never made a cent for her
skills.
S3: They were just a group of young women
from the same school who liked to
play amateur basketball.
S4: Babe Belanger played with the Grads from
1929 to 1937.
S5: Babe never talked about her fabulous career.
</bodyText>
<equation confidence="0.52437">
MaxOsets : ( {S2, S4}, {S3} )
</equation>
<figureCaption confidence="0.999843">
Figure 8: Example of Overlap Sets from CBC
</figureCaption>
<bodyText confidence="0.920057405405405">
Figure 8 presents an example from the CBC
data. The four overlap sets are (i) Babe Belanger,
(ii) basketball, (iii) play amateur basketball, and
(iv) Babe. In any term-weighting scheme with
positive weights, a sentence containing the words
Babe Belanger will have a higher score than sen-
tences containing just Babe, and sentences with
play amateur basketball will have a higher score
than those with just basketball. However, we can-
not generalize with respect to the relative scores
of sentences containing Babe Belanger and those
containing play amateur basketball because some
terms may have higher weights than others.
The most we can say is that the highest scor-
ing candidate must be a member of {52, 54} or
{53}. S5 and S1 cannot be ranked highest be-
cause their overlap sets are a proper subset of
competing overlap sets. The correct answer is
S2 so an optimal weighting scheme would have
a 50% chance of ranking S2 first, assuming that
it identified the correct overlap set {52, 54} and
then randomly chose between S2 and S4. A max-
imally suboptimal weighting scheme could rank
S2 no lower than third.
We will formalize these concepts using the fol-
lowing variables:
Assuming that all term weights are positive.
q: a question (a set of words)
s: a sentence (a set of words)
w,v: sets of intersecting words
We define an overlap set (ow,q) to be a set of
sentences (answer candidates) that have the same
words overlapping with the question. We define a
maximal overlap set (Mq) as an overlap set that is
not a subset of any other overlap set for the ques-
tion. For simplicity, we will refer to a maximal
overlap set as a MaxOset.
</bodyText>
<equation confidence="0.9991388">
ow,q = {s|s ∩ q = w}
Qq = all unique overlap sets for q
maximal(ow,q) if ∀ov,q ∈ Qq, w ⊂6 v
Mq = {ow,q ∈ Qq  |maximal(ow,q)}
Cq = {s|s correctly answers q}
</equation>
<bodyText confidence="0.954549166666667">
We can use these definitions to give upper
and lower bounds on the performance of term-
weighting functions on our two data sets. Table 1
shows the results. The max statistic is the per-
centage of questions for which at least one mem-
ber of its MaxOsets is correct. The min statis-
tic is the percentage of questions for which all
candidates of all of its MaxOsets are correct (i.e.,
there is no way to pick a wrong answer). Finally
the expectedmax is a slightly more realistic up-
per bound. It is equivalent to randomly choosing
among members of the “best” maximal overlap
set, i.e., the MaxOset that has the highest percent-
age of correct members. Formally, the statistics
for a set of questions Q are computed as:
1 max|{s ∈ o and s ∈ Cq}|
exp. max = |Q|∗q ∈Qo∈ Mq |o|
The results for the TREC data are considerably
lower than the results for the CBC data. One ex-
planation may be that in the CBC data, only sen-
tences from one document containing the answer
are considered. In the TREC data, as in the TREC
task, it is not known beforehand which docu-
ments contain answers, so irrelevant documents
</bodyText>
<table confidence="0.951441666666667">
exp. max max min
CBC training 72.7% 79.0% 24.4%
TREC-8 48.8% 64.7% 10.1%
</table>
<tableCaption confidence="0.999749">
Table 1: Maximum overlap analysis of scores
</tableCaption>
<bodyText confidence="0.993819392857143">
may contain high-scoring sentences that distract
from the correct sentences.
In Table 2, we present a detailed breakdown
of the MaxOset results for the CBC data. (Note
that the classifications overlap, e.g., questions that
are in “there is always a chance to get it right”
are also in the class “there may be a chance to
get it right.”) 21% of the questions are literally
impossible to get right using only term weight-
ing because none of the correct sentences are in
the MaxOsets. This result illustrates that maxi-
mal overlap sets can identify the limitations of a
scoring function by recognizing that some candi-
dates will always be ranked higher than others.
Although our analysis only considered term over-
lap as a scoring function, maximal overlap sets
could be used to evaluate other scoring functions
as well, for example overlap sets based on seman-
tic classes rather than lexical items.
In sum, the upper bound for term weighting
schemes is quite low and the lower bound is
quite high. These results suggest that methods
such as query expansion are essential to increase
the feature sets used to score answer candidates.
Richer feature sets could distinguish candidates
that would otherwise be represented by the same
features and therefore would inevitably receive
the same score.
</bodyText>
<subsectionHeader confidence="0.406864">
6 Analyzing the effect of multiple
answer type occurrences in a sentence
</subsectionHeader>
<bodyText confidence="0.99806875">
In this section, we analyze the problem of extract-
ing short answers from a sentence. Many Q/A
systems first decide what answer type a question
expects and then identify instances of that type in
sentences. A scoring function ranks the possible
answers using additional criteria, which may in-
clude features of the surrounding sentence such
as term overlap with the question.
For our analysis, we will assume that two short
answers that have the same answer type and come
from the same sentence are indistinguishable to
the system. This assumption is made by many
</bodyText>
<equation confidence="0.9161712">
|{q|∃o ∈ Mq, ∃s ∈ o s.t. s ∈ Cq}|
|Q|
min = |{q|∀o ∈ Mq,∀s ∈ o s ∈ Cq}|
|Q|
max =
</equation>
<table confidence="0.918893071428571">
number of percentage
questions of questions
Impossible to get it wrong 159 24%
(Vow E Mq, Vs E ow, s E Cq)
There is always a chance to get it right 45 7%
(Vow E Mq, Els E ow s.t. s E Cq)
There may be a chance to get it right 310 48%
(Elow E Mq s.t. Els E ow s.t. s E Cq)
The wrong answers will always be weighted too highly 137 21%
(Vow E Mq, Vs E ow, s E� Cq)
There are no correct answers with any overlap with Q 66 10%
(Vs E d, s is incorrect or s has 0 overlap)
There are no correct answers (auto scoring error) 12 2%
(Vs E d, s is incorrect)
</table>
<tableCaption confidence="0.998126">
Table 2: Maximal Overlap Set Analysis for CBC data
</tableCaption>
<bodyText confidence="0.999775685714285">
Q/A systems: they do not have features that can
prefer one entity over another of the same type in
the same sentence.
We manually annotated data for 165 TREC-
9 questions and 186 CBC questions to indicate
perfect question typing, perfect answer sentence
identification, and perfect semantic tagging. Us-
ing these annotations, we measured how much
“answer confusion” remains if an oracle gives you
the correct question type, a sentence containing
the answer, and correctly tags all entities in the
sentence that match the question type. For exam-
ple, the oracle tells you that the question expects
a person, gives you a sentence containing the cor-
rect person, and tags all person entities in that sen-
tence. The one thing the oracle does not tell you
is which person is the correct one.
Table 3 shows the answer types that we used.
Most of the types are fairly standard, except for
the Defaultnp and Defaultvp which are default
tags for questions that desire a noun phrase or
verb phrase but cannot be more precisely typed.
We computed an expected score for this hy-
pothetical system as follows: for each question,
we divided the number of correct candidates (usu-
ally one) by the total number of candidates of the
same answer type in the sentence. For example,
if a question expects a Location as an answer and
the sentence contains three locations, then the ex-
pected accuracy of the system would be 1/3 be-
cause the system must choose among the loca-
tions randomly. When multiple sentences contain
a correct answer, we aggregated the sentences. Fi-
nally, we averaged this expected accuracy across
all questions for each answer type.
</bodyText>
<table confidence="0.999827678571429">
Answer Type TREC Freq CBC Freq
Score Score
defaultnp .33 47 .25 28
organization .50 1 .72 3
length .50 1 .75 2
thingname .58 14 .50 1
quantity .58 13 .77 14
agent .63 19 .40 23
location .70 24 .68 29
personname .72 11 .83 13
city .73 3 n/a 0
defaultvp .75 2 .42 15
temporal .78 16 .75 26
personnoun .79 7 .53 5
duration 1.0 3 .67 4
province 1.0 2 1.0 2
area 1.0 1 n/a 0
day 1.0 1 n/a 0
title n/a 0 .50 1
person n/a 0 .67 3
money n/a 0 .88 8
ambigbig n/a 0 .88 4
age n/a 0 1.0 2
comparison n/a 0 1.0 1
mass n/a 0 1.0 1
measure n/a 0 1.0 1
Overall .59 165 .61 186
Overall-dflts .69 116 .70 143
</table>
<tableCaption confidence="0.856252666666667">
Table 3: Expected scores and frequencies for each
answer type
Table 3 shows that a system with perfect ques-
</tableCaption>
<bodyText confidence="0.985390853658537">
tion typing, perfect answer sentence identifica-
tion, and perfect semantic tagging would still
achieve only 59% accuracy on the TREC-9 data.
These results reveal that there are often multi-
ple candidates of the same type in a sentence.
For example, Temporal questions received an ex-
pected score of 78% because there was usually
only one date expression per sentence (the correct
one), while Default NP questions yielded an ex-
pected score of 25% because there were four noun
phrases per question on average. Some common
types were particularly problematic. Agent ques-
tions (most Who questions) had an answer con-
fusability of 0.63, while Quantity questions had a
confusability of 0.58.
The CBC data showed a similar level of an-
swer confusion, with an expected score of 61%,
although the confusability of individual answer
types varied from TREC. For example, Agent
questions were even more difficult, receiving a
score of 40%, but Quantity questions were easier
receiving a score of 77%.
Perhaps a better question analyzer could assign
more specific types to the Default NP and De-
fault VP questions, which skew the results. The
Overall-dflts row of Table 3 shows the expected
scores without these types, which is still about
70% so a great deal of answer confusion remains
even without those questions. The confusability
analysis provides insight into the limitations of
the answer type set, and may be useful for com-
paring the effectiveness of different answer type
sets (somewhat analogous to the use of grammar
perplexity in speech research).
Q1: What city is Massachusetts General Hospital located
in?
A1: It was conducted by a cooperative group of on-
cologists from Hoag, Massachusetts General Hospital
in Boston, Dartmouth College in New Hampshire, UC
San Diego Medical Center, McGill University in Montreal
and the University of Missouri in Columbia.
</bodyText>
<figure confidence="0.298843333333333">
Q2: When was Nostradamus born?
A2: Mosley said followers of Nostradamus, who lived
from 1503 to 1566, have claimed ...
</figure>
<figureCaption confidence="0.644844">
Figure 9: Sentences with Multiple Items of the
Same Type
</figureCaption>
<bodyText confidence="0.999316777777778">
However, Figure 9 shows the fundamental
problem behind answer confusability. Many sen-
tences contain multiple instances of the same
type, such as lists and ranges. In Q1, recognizing
that the question expects a city rather than a gen-
eral location is still not enough because several
cities are in the answer sentence. To achieve bet-
ter performance, Q/A systems need use features
that can more precisely target an answer.
</bodyText>
<sectionHeader confidence="0.997637" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999791">
In this paper we have presented four analyses of
question answering system performance involv-
ing: multiple answer occurence, relative score for
candidate ranking, bounds on term overlap perfor-
mance, and limitations of answer typing for short
answer extraction. We hope that both the results
and the tools we describe will be useful to others.
In general, we feel that analysis of good perfor-
mance is nearly as important as the performance
itself and that the analysis of bad performance can
be equally important.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99834605">
E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House,
M. Light, and I. Mani. 2000. How to Evaluate your
Question Answering System Every Day and Still Get
Real Work Done. In Proceedings of the Second Con-
ference on Language Resources and Evaluation (LREC-
2000).
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-
mala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy,
Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Compre-
hension Programs in a Statistical-Language-Processing
Class. In ANLP/NAACL Workshop on Reading Com-
prehension Tests as Evaluation for Computer-Based Lan-
guage Understanding Systems.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep Read: A Reading Comprehension System. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine
Learning Approach to Answering Questions for Reading
Comprehension Tests. In Proceedings of EMNLP/VLC-
2000 at ACL-2000.
E. Riloff and M. Thelen. 2000. A Rule-based Question
Answering System for Reading Comprehension Tests.
In ANLP/NAACL Workshop on Reading Comprehension
Tests as Evaluation for Computer-Based Language Un-
derstanding Systems.
TREC-8 Proceedings. 1999. Proceedings of the Eighth
Text Retrieval Conference (TREC8). National Institute of
Standards and Technology, Special Publication 500-246,
Gaithersburg, MD.
TREC-9 Proceedings. 2000. Proceedings of the Ninth Text
Retrieval Conference (forthcoming). National Institute
of Standards and Technology, Special Publication 500-
XXX, Gaithersburg, MD.
W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-
berry, and M.P. Harper. 2000. A Question Answering
System Developed as a Project in a Natural Language
Processing Course. In ANLP/NAACL Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417972">
<title confidence="0.9991675">Looking Under the Hood: Tools for Diagnosing Your Answering Engine</title>
<author confidence="0.8757395">Marc Gideon S Ellen Pranav Mats Michael</author>
<affiliation confidence="0.515912">MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730,</affiliation>
<address confidence="0.9898875">of Computer Science, Johns Hopkins University, Baltimore, MD 21218, of Computing, University of Utah, Salt Lake City, UT 84112, Mawr College, Bryn Mawr, PA 19010, of Mathematics, Harvard University, Cambridge, MA 02138,</address>
<email confidence="0.972204">ofLinguistics,CornellUniversity,Ithaca,NY14853,mr249@cornell.edu</email>
<abstract confidence="0.998599277777778">In this paper we analyze two question answering tasks : the TREC-8 question answering task and a set of reading comprehension exams. First, we show that Q/A systems perform better when there are multiple answer opportunities per question. Next, we analyze common approaches to two subproblems: term overlap for answer sentence identification, and answer typing for short answer extraction. We present general tools for analyzing the strengths and limitations of techniques for these subproblems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E J Breck</author>
<author>J D Burger</author>
<author>L Ferro</author>
<author>L Hirschman</author>
<author>D House</author>
<author>M Light</author>
<author>I Mani</author>
</authors>
<title>How to Evaluate your Question Answering System Every Day and Still Get Real Work Done.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Conference on Language Resources and Evaluation (LREC2000).</booktitle>
<contexts>
<context position="4661" citStr="Breck et al., 2000" startWordPosition="740" endWordPosition="743">rt (either 50-character or 250-character) responses. As a service to track participants, AT&amp;T provided top documents returned by their retrieval engine for each of the TREC questions. Sections 4 and 5 present analyses that use all sentences in the top 10 of these documents. Each sentence is classified as correct or incorrect automatically. This automatic classification judges a sentence to be correct if it contains at least half of the stemmed, content-words in the answer key. We have compared this automatic evaluation to the TREC-8 QA track assessors and found it to agree 93-95% of the time (Breck et al., 2000). The CBC data set was created for the Johns Hopkins Summer 2000 Workshop on Reading Comprehension. Texts were collected from the Canadian Broadcasting Corporation web page for kids (http://cbc4kids.ca/). They are an average of 24 sentences long. The stories were adapted from newswire texts to be appropriate for adolescent children, and most fall into the following domains: politics, health, education, science, human interest, disaster, sports, business, crime, war, entertainment, and environment. For each CBC story, 8-12 questions and an answer key were generated.2 We used a 650 question subs</context>
</contexts>
<marker>Breck, Burger, Ferro, Hirschman, House, Light, Mani, 2000</marker>
<rawString>E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House, M. Light, and I. Mani. 2000. How to Evaluate your Question Answering System Every Day and Still Get Real Work Done. In Proceedings of the Second Conference on Language Resources and Evaluation (LREC2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>Y Altun</author>
<author>R de Salvo Braz</author>
<author>B Garrett</author>
<author>M Kosmala</author>
<author>T Moscovich</author>
<author>L Pang</author>
<author>C Pyo</author>
<author>Y Sun</author>
<author>W Wy</author>
<author>Z Yang</author>
<author>S Zeller</author>
<author>L Zorn</author>
</authors>
<title>Reading Comprehension Programs in a Statistical-Language-Processing Class. In ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.</title>
<date>2000</date>
<contexts>
<context position="1836" citStr="Charniak et al., 2000" startWordPosition="259" endWordPosition="262">ems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, th</context>
</contexts>
<marker>Charniak, Altun, Braz, Garrett, Kosmala, Moscovich, Pang, Pyo, Sun, Wy, Yang, Zeller, Zorn, 2000</marker>
<rawString>E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy, Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Comprehension Programs in a Statistical-Language-Processing Class. In ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>M Light</author>
<author>E Breck</author>
<author>J Burger</author>
</authors>
<title>Deep Read: A Reading Comprehension System.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1860" citStr="Hirschman et al., 1999" startWordPosition="263" endWordPosition="266">fy the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, then the system can be str</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>L. Hirschman, M. Light, E. Breck, and J. Burger. 1999. Deep Read: A Reading Comprehension System. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>L H Teo</author>
<author>J L P Kwan</author>
</authors>
<title>A Machine Learning Approach to Answering Questions for Reading Comprehension Tests.</title>
<date>2000</date>
<booktitle>In Proceedings of EMNLP/VLC2000 at ACL-2000.</booktitle>
<contexts>
<context position="1877" citStr="Ng et al., 2000" startWordPosition="267" endWordPosition="270">th term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, then the system can be streamlined and simp</context>
<context position="13587" citStr="Ng et al., 2000" startWordPosition="2271" endWordPosition="2274">e y-axis. The topranked candidate has rank=0. ranked overlap Figure 5: Pr(rank=x |[in]correct) for TREC-8 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 ranked overlap Figure 6: Pr(rank=x |[in]correct) for CBC The ranked graphs are more revealing than the graphs of absolute scores: the probability of a high rank is greater for correct answers than incorrect ones. Now we can begin to understand why the term overlap scoring function worked as well as it did. We see that, unlike classification tasks, there is no good threshold for our scoring function. Instead relative score is paramount. Systems such as (Ng et al., 2000) make explicit use of relative rank in their algorithms and now we understand why this is effective. Before we leave the topic of graphing scoring functions, we want to introduce one other view of the data. Figure 7 plots term overlap scores on incorrect correct Figure 3: Pr(overlap=x|[in]correct) for TREC-8 incorrect correct 0 5 10 15 20 25 30 overlap 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 Normalized (+/1311,-/14610) Count 0.02 0.018 0.016 0.014 0.012 0.01 0.008 0.006 0.004 0.002 0 incorrect correct 0.3 0.25 0.2 0.15 0.1 0.05 0 incorrect correct Normalized (+/3087,-/57073) Count -1000 -900 -80</context>
</contexts>
<marker>Ng, Teo, Kwan, 2000</marker>
<rawString>H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine Learning Approach to Answering Questions for Reading Comprehension Tests. In Proceedings of EMNLP/VLC2000 at ACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>M Thelen</author>
</authors>
<title>A Rule-based Question Answering System for Reading Comprehension Tests. In ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.</title>
<date>2000</date>
<contexts>
<context position="1902" citStr="Riloff and Thelen, 2000" startWordPosition="271" endWordPosition="274">nd answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, then the system can be streamlined and simplified. In this paper, we</context>
</contexts>
<marker>Riloff, Thelen, 2000</marker>
<rawString>E. Riloff and M. Thelen. 2000. A Rule-based Question Answering System for Reading Comprehension Tests. In ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TREC-8 Proceedings</author>
</authors>
<date>1999</date>
<booktitle>Proceedings of the Eighth Text Retrieval Conference (TREC8). National Institute of Standards and Technology, Special Publication 500-246,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="1635" citStr="Proceedings, 1999" startWordPosition="233" endWordPosition="234"> term overlap for answer sentence identification, and answer typing for short answer extraction. We present general tools for analyzing the strengths and limitations of techniques for these subproblems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be depend</context>
</contexts>
<marker>Proceedings, 1999</marker>
<rawString>TREC-8 Proceedings. 1999. Proceedings of the Eighth Text Retrieval Conference (TREC8). National Institute of Standards and Technology, Special Publication 500-246, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TREC-9 Proceedings</author>
</authors>
<date>2000</date>
<booktitle>Proceedings of the Ninth Text Retrieval Conference (forthcoming). National Institute of Standards and Technology, Special Publication 500-XXX,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="1666" citStr="Proceedings, 2000" startWordPosition="237" endWordPosition="238">ce identification, and answer typing for short answer extraction. We present general tools for analyzing the strengths and limitations of techniques for these subproblems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the c</context>
</contexts>
<marker>Proceedings, 2000</marker>
<rawString>TREC-9 Proceedings. 2000. Proceedings of the Ninth Text Retrieval Conference (forthcoming). National Institute of Standards and Technology, Special Publication 500-XXX, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Auer</author>
<author>R Parasuraman</author>
<author>I Zubarev</author>
<author>D Brandyberry</author>
<author>M P Harper</author>
</authors>
<title>A Question Answering System Developed as a Project in a Natural Language Processing Course. In ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for ComputerBased Language Understanding Systems.</title>
<date>2000</date>
<contexts>
<context position="1922" citStr="Wang et al., 2000" startWordPosition="275" endWordPosition="278">nguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, then the system can be streamlined and simplified. In this paper, we explore ways of gai</context>
</contexts>
<marker>Wang, Auer, Parasuraman, Zubarev, Brandyberry, Harper, 2000</marker>
<rawString>W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandyberry, and M.P. Harper. 2000. A Question Answering System Developed as a Project in a Natural Language Processing Course. In ANLP/NAACL Workshop on Reading Comprehension Tests as Evaluation for ComputerBased Language Understanding Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>