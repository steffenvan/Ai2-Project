<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000599">
<title confidence="0.9988975">
Dependency length minimisation effects in short spans: a large-scale
analysis of adjective placement in complex noun phrases
</title>
<author confidence="0.996351">
Kristina Gulordava Paola Merlo Benoit Crabb´e
</author>
<affiliation confidence="0.999846">
University of Geneva University of Geneva U. Paris Diderot/Inria/IUF
</affiliation>
<email confidence="0.689533">
Kristina.Gulordava, Paola.Merlo@unige.ch bcrabbe@
univ-paris-diderot.fr
</email>
<sectionHeader confidence="0.993009" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865666666667">
It has been extensively observed that lan-
guages minimise the distance between two
related words. Dependency length min-
imisation effects are explained as a means
to reduce memory load and for effective
communication. In this paper, we ask
whether they hold in typically short spans,
such as noun phrases, which could be
thought of being less subject to efficiency
pressure. We demonstrate that minimisa-
tion does occur in short spans, but also that
it is a complex effect: it is not only the
length of the dependency that is at stake,
but also the effect of the surrounding de-
pendencies.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999540677419355">
One of the main goals in the study of language is
to find explanations for those fundamental proper-
ties that are found in every human language. The
observation that human languages appear to min-
imise the distance between any two related words
– called the property of dependency length min-
imisation (DLM) — is a universal property that
has been documented in sentence processing (Gib-
son, 1998; Hawkins, 1994; Hawkins, 2004; Dem-
berg and Keller, 2008), in corpus properties of
treebanks (Temperley, 2007; Futrell et al., 2015),
in diachronic language change (Tily, 2010). Func-
tional explanations have been proposed for this
pervasive linguistic property. If speakers want to
reduce memory load and maximise efficiency of
processing, they will choose to produce and pref-
erentially analyse constructions where words are
linearised in such a way that minimises the total
distance of related words.
The DLM principle can be stated as follows:
if there exist possible alternative orderings of a
phrase, the one with the shortest overall depen-
dency length (DL) is preferred. We measure the
length of a dependency as the number of words
between the head and its dependent.
As an illustration, DLM principle is widely re-
ported in the literature to explain the alternation
of postverbal complements (Bresnan et al., 2007;
Wasow, 2002). Consider, for example, the case
when a verb has both a direct object (NP) and a
prepositional complement or adjunct (PP). Two al-
ternative orders of the verb complements are pos-
sible: VP1 = V NP PP, whose length is DL1 and
VP2 = V PP NP, whose length is DL2. DL1 is
DL(V-NP) + DL(V-PP) _ INPI + 1; DL2 is
DL(V-NP) + DL(V-PP) _ IPPI + 1. 1 If DL1 is
bigger than DL2, then VP2 is preferred over VP1,
despite the non-canonical V-PP-NP order.
While DLM has been demonstrated on a large
scale and explanations have been proposed based
on human sentence processing facts in the verbal
domain, it is not clear what the effects of DLM are
in the more limited nominal domain. If the expla-
nations are really rooted in memory and efficiency,
will they still hold in phrases that might span only
a few words?
In this paper, we look at the structural fac-
tors that play a role in adjective-noun word order
alternations in Romance languages. We choose
Romance languages because they show a good
amount of variation, making studies of DLM
meaningful. This would not be the case in En-
glish, for instance, as English has no variation of
word order placement in the noun phrase. Ad-
jective placement in Romance is often studied in
connection with semantic and lexical properties of
adjectives (Bouchard, 1998; Cinque, 2010). There
exists, however, a body of work which shows that
structural syntactic properties like the size of ad-
jective phrase also affect the adjective position
(Abeill´e and Godard, 2000; Thuilier, 2012).
We demonstrate that, unlike results for the ver-
</bodyText>
<footnote confidence="0.993968">
1The minimal dependency length is equal to one when the
head and its dependent are adjacent.
</footnote>
<page confidence="0.604894">
477
</page>
<bodyText confidence="0.271699333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 477–482,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<equation confidence="0.885216333333333">
RightNP=Yes RightNP=No
|0 |− |α|
−3|α |− 2
</equation>
<tableCaption confidence="0.886146666666667">
Table 1: Dependency length difference for differ-
ent types of noun phrases. By convention, we al-
ways calculate DL1 − DL2.
</tableCaption>
<bodyText confidence="0.995785333333333">
bal domain, it is not only the length of the depen-
dency that is at stake, but also the effect of the sur-
rounding dependencies.
</bodyText>
<sectionHeader confidence="0.8590135" genericHeader="method">
2 Dependency length minimisation in the
noun phrase
</sectionHeader>
<bodyText confidence="0.997291771428572">
In applying the general principle of DLM to the
dependency structure of noun phrases, our goal is
to test to what extent the DLM principle predicts
the observed adjective-noun word order alterna-
tion pattern in relatively short spans.
Consider a prototypical noun phrase with an ad-
jective phrase as a modifier. We assume two possi-
ble placements for an adjective phrase: postnom-
inal and prenominal. To simplify, we concentrate
on noun phrases with only one adjective modifier
adjacent to the noun. The adjective modifier can
be a complex phrase with both left and right de-
pendents ( α and 0, respectively, in Figure 1). The
noun phrase can have parents and right modifiers
(X and Y, respectively, in Figure 1). These alterna-
tive orderings yield different dependency lengths,
as can be seen from Figure 1. By convention, we
will always indicate the prenominal order as DL1,
and the postnominal order as DL2. Their differ-
ence is always calculated as DL1 − DL2.
We consider all dependencies in a noun phrase
and not only the length of the noun-adjective de-
pendency. This is because we assume, as previ-
ously done, that DLM is global, and not a local, ef-
fect. Our analysis is a faithful interpretation of the
very general DLM principle of Gildea and Tem-
perley (2010) which is based on the overall de-
pendency length of a sentence. We do no take
other dependencies in the sentence into account,
because their lengths are the same across DL1 and
DL2. The difference DL1 − DL2 is therefore the
difference between the overall dependency length
of two sentences that differ only in their placement
of one adjective.
The first panel, panel a, shows the case where
the parent of the NP is on the left of it. The depen-
dency length for the prenominal adjective struc-
ture is equal to DL1 = di + d2 = (|α |+ |0 |+
1) + |0 |and for the postnominal adjective struc-
ture is DL2 = d1 + d2 = |α|. The difference be-
tween these lengths is 2|0 |+ 1, which means that
DL1 &gt; DL2 and suggests that the postnominal
placement is always preferred.
Similarly, the second panel, panel b, in the
figure shows how we calculate the dependency
lengths when the parent of the NP is on its right.
The difference of lengths is equal to −2|α |− 1,
yielding a preference for prenominal adjectives.
We also consider more complex noun phrases
with at least one right dependent, which are very
common in Romance languages (around 50% of
noun phrases in our sample include, for instance,
a complement, such as a relative clause). The
third and fourth panels in Figure 1 illustrate the
case where three dependencies should be taken
into account. The calculations of these depen-
dency lengths for the prenominal and postnominal
alternatives yield the corresponding differences of
|0|−|α |(in the case of a left external dependency)
and −3|α |− 2 (in the case of a right external de-
pendency). These values are different from the
dependency length differences for noun phrases
without a right dependent (panel a and b). The
comparison of the values, where RightNP=Yes is
smaller than RightNP=No in both cases, suggests
that the presence of a right dependent favours the
prenominal placement of adjectives in comparison
to the case of a simple noun phrase.
The differences in dependency lengths are sum-
marized in Table 1. The expectations based on de-
</bodyText>
<listItem confidence="0.9060065">
pendency length minimisation are as indicated in
(1) below.
(1) a. the presence of a left dependent of an
adjective favours the adjective’s prenominal
placement;
b. the presence of a right dependent of an
adjective favours the adjective’s postnominal
placement;
</listItem>
<bodyText confidence="0.85696675">
c. when the external dependency is leftwards,
X = right, (for canonical subjects, for ex-
ample), then the adjective is prenominal, be-
cause the difference is negative and it is a
function of α;
d. when the noun has a right dependent, the
prenominal adjective position is more pre-
ferred than when there is no right dependent,
X=Left
X=Right
2|0 |+ 1
−2|α |− 1
</bodyText>
<page confidence="0.870295">
478
</page>
<figure confidence="0.97524025">
DL1: [NP [AP α Adj Q ] N ] X
d0
1 d0
2
d0
1
d0
2
DL1: X [NP [AP α Adj Q ] N ]
d00
1
d00
2
DL2: X [NP N [AP α Adj Q ] ]
DL2: [NP N [AP α Adj Q ] ] X
d00
d00 2
1
(a) Left external dependency (b) Right external dependency
d0
1
DL1: X [NP [AP α Adj Q ] N Y ]
DL1: [NP [AP α Adj Q ] N Y ] X
d0 d0
1 2
d0
3
d0
2 d0
3
DL2: X [NP N [AP α Adj Q ] Y ]
DL2: [NP N [AP α Adj Q ] Y ] X
d00
3
d00
d00 2
1
d00
1
d00
3
d00
2
(c) Right noun dependent Y, left external dependency (d) Right noun dependent Y, right external dependency
</figure>
<figureCaption confidence="0.99998">
Figure 1: Noun phrase structure variants and the dependencies relevant for the DLM calculation.
</figureCaption>
<bodyText confidence="0.999961470588235">
as evinced by the fact that the RightNP = Yes
column is always greater than the RightNP =
No column.
The predictions (1a) and (1b) are formulated
for an average case of adjective placement, across
nouns phrases with different values of X and
RightNP factors. Table 1 shows that for each com-
bination of these context factors the weight of α is
negative or zero and the weight of Q is positive or
zero. On average, therefore, we expect to see a
negative effect of α (1a) and a positive effect of Q
(1b).
We develop a model to test which of the fine-
grained predictions derived from DLM are con-
firmed by the data provided by the dependency an-
notated corpora of five of the main Romance lan-
guages.
</bodyText>
<sectionHeader confidence="0.9996245" genericHeader="method">
3 Identifying dependency minimisation
factors
</sectionHeader>
<subsectionHeader confidence="0.998222">
3.1 Materials: Dependency treebanks
</subsectionHeader>
<bodyText confidence="0.999961222222222">
We use the dependency annotated corpora of five
Romance languages: Catalan, Spanish, Italian
(Hajiˇc et al., 2009), French (Agi´c et al., 2015), and
Portuguese (Buchholz and Marsi, 2006).
We use part-of-speech information and depen-
dency arcs from the gold annotation to extract
noun phrases containing adjectives. Specifically,
we first convert all treebanks to coarse universal
part-of-speech tags, using existing conventional
mappings from the original tagset to the univer-
sal tagset (Petrov et al., 2012). We then identify
all adjectives (tagged using the universal PoS tag
‘ADJ’) whose dependency head is a noun (tagged
using the universal PoS tag ‘NOUN’). In addition,
we recover all elements of the noun phrase rooted
in this noun, that is, its dependency subtree. For
all languages where this information is available,
we extract lemmas of adjective and noun tokens
which are the features in our analysis. The only
treebank without lemma annotation is French, for
which we extract token forms.2 We extract a total
of around 64’000 instances of adjectives in noun
phrases, ranging from 2’800 for Italian to 20’000
for Spanish.
The data present a substantial amount of varia-
tion in the placement of the adjective: the ratio of
postnominal adjectives ranges from around 65%
</bodyText>
<footnote confidence="0.991947625">
2During preprocessing, we exclude all adjectives and
nouns with non-lower case and non-alphabetic symbols
which can include common names, compounds (in Spanish
and Catalan treebanks), and English borrowings. In addition,
we leave out noun phrases which have their elements sepa-
rated by punctuation (for example, commas or parentheses)
to ensure that the placement of adjective is not affected by an
unusual structure of the noun phrase.
</footnote>
<page confidence="0.999058">
479
</page>
<bodyText confidence="0.9999496">
for Italian to 78% for Catalan. Among all adjective
types, at least 10% in each language are observed
both prenominally and postnominally (ranging be-
tween 147 types for Italian and 445 types for Span-
ish).
</bodyText>
<subsectionHeader confidence="0.998001">
3.2 Method: Mixed Effects models
</subsectionHeader>
<bodyText confidence="0.999928692307693">
We analyse the interactions of several dependency
factors, using a logit mixed effect models (Bates et
al., 2014). Mixed-effect logistic regression mod-
els (logit models) are a type of Generalized Linear
Mixed Models with the logit link function and are
designed for binomially distributed outcomes such
as Order in our case.
More precisely, Generalized Linear Mixed
Models describe an outcome as the linear combi-
nation of fixed effects X and conditional random
effects Z associated with grouping of instances,
where Q and -y are the corresponding weights of
the effects.
</bodyText>
<listItem confidence="0.711379">
(2) y = XQ + Z-y + E
</listItem>
<bodyText confidence="0.999674666666667">
In logistic regression models, this linear combi-
nation is then transformed with the logit link func-
tion to predict the binomial output:
</bodyText>
<equation confidence="0.858247">
1
(3) Order =
1 + exp−y
</equation>
<bodyText confidence="0.998487666666667">
In our model, Order = 0 codes the prenominal
adjective order and Order = 1 codes the post-
nominal order.
</bodyText>
<subsectionHeader confidence="0.995894">
3.3 Factors
</subsectionHeader>
<bodyText confidence="0.9924195">
We define and test the following factors, corre-
sponding to the factors illustrated in Figure 1 and
example (1), represented as binary or real-valued
variables:
</bodyText>
<listItem confidence="0.990754642857143">
• LeftAP - the cumulative length (in words) of
all left dependents of the adjective, indicated
as α in Figure 1;
• RightAP - the cumulative length (in words)
of all right dependents of the adjective, indi-
cated as Q in Figure 1;
• RightNP - the indicator variable representing
the presence (RightNP = 1) or absence
(RightNP = 0) of the right dependent of
the noun, indicated as Y in Figure 1;
• ExtDep - the direction of the arc from the
noun to its parent X, an indicator variable.
ExtDep = 0 when X is on the left of the
noun, ExtDep = 1 when X is on the right.
</listItem>
<table confidence="0.96852">
Predictor Q SE Z p
Intercept -0.17 (0.117) -1.42 0.16
LeftAP 2.21 (0.101) 21.91 &lt; .001
RightAP 0.76 (0.054) 14.08 &lt; .001
ExtDep -0.06 (0.071) -0.85 0.40
RightNP -0.77 (0.050) -15.34 &lt; .001
Random effects Var
Adjective 1.989
Language 0.024
</table>
<tableCaption confidence="0.989869">
Table 2: Summary of the fixed and random effects
</tableCaption>
<bodyText confidence="0.94354425">
in the mixed-effects logit model (N = 15842),
shown in (4).
In addition, to account for lexical variation, we
include adjective lemmas (for French, we include
tokens) as grouping variables introducing random
effects. For example, the instances of adjective-
noun order for a particular adjective will share the
same weight value -y for the adjective variable, but
across different adjectives this value can vary.3
For a given example involving an adjective i and
belonging to language j, the linear component of
the model is shown in (4).
</bodyText>
<equation confidence="0.64053825">
(4)
yij = LeftAP · QLAP + RightAP · QRAP+
+ RightNP · QRNP + ExtDep · QED+
+ -yAdji + -yLangj
</equation>
<bodyText confidence="0.999944333333333">
By fitting the logit mixed-effect model to our
dataset, we find the fixed and random effects coef-
ficients which best explain the data. To show that a
factor has a statistically significant effect on adjec-
tive placement, we must show that its fixed effect
coefficient is significantly different from zero.
</bodyText>
<subsectionHeader confidence="0.686247">
3.4 Results
</subsectionHeader>
<bodyText confidence="0.999865285714286">
The logit mixed-effects model fitted to our data,
shown in (4), reveals the following picture (Table
2).
Both the LeftAP and RightAP factors favour
postnominal placement (QLAP = 2.21, QRAP =
0.76, p &lt; 0.001), however there are important dif-
ferences between the two.
</bodyText>
<footnote confidence="0.9664214">
3We include only random intercepts because the size of
the data is not sufficient to estimate the slope variables. In
addition, for a robust estimation of the random effects, we
include in our dataset only adjectives that are observed both
prenominally and postnominally.
</footnote>
<page confidence="0.997061">
480
</page>
<bodyText confidence="0.9999758125">
LeftAP shows a complex behavior. When Lef-
tAP is equal to one, it favors (slightly) the prenom-
inal placement and when LeftAP is greater than
one, it favors the postnominal placement. This re-
sult suggests that the adjective can behave differ-
ently depending on the size or type of its left pe-
riphery. For the moment it is not clear if the differ-
ence is due to length or type, as LeftAP of length
one are almost always adverbs. It is important to
notice that the results for LeftAP then do not en-
tirely pattern with the predictions of dependency
length minimisation, shown in (1a).
The RightAP factor shows a consistent post-
nominal preference, positively correlated to its
length. Consequently, we can say that the Righ-
tAP is a stronger indicator of the postnominal
placement than LeftAP, in agreement with the pre-
viously observed ordering patterns of adjective
phrases (Abeill´e and Godard, 2000) and the DLM
prediction.
The external dependency factor is not signifi-
cant (p &gt; 0.1). Moreover, the log likelihood ra-
tio between the full model and the model without
ExtDep is x2 distributed with 1 degree of free-
dom with x2 = 3.8,p = 0.052. This compar-
ison confirms that the introduction of the exter-
nal dependency does not help predicting the Order.
At first sight, this result suggests that this depen-
dency is not subject to the minimisation principle.
A plausible explanation claims that only the de-
pendencies between the head and the edge of the
dependent phrase are minimised (Hawkins, 1994).
In Romance languages, the majority of the noun
phrases take an article which unambiguously de-
fines the left edge of the noun phrase. There is
no need therefore to minimize the external depen-
dency to the noun, since the noun phrase can be
entirely predicted based on its left corner.
The RightNP factor is significant in the fitted
model (βRNP = −0.77, p &lt; 0.001).4 The pres-
ence of a noun dependent on the right of the noun
favours a prenominal placement, as predicted by
DLM (1d). This is a result which, to our knowl-
edge, was not previously observed in the literature,
and that clearly answers our initial question, con-
firming that DLM also applies to very short spans.
A much more detailed study of the lexical and
structural properties of this effect is developed in
</bodyText>
<footnote confidence="0.99448025">
4A log-likelihood test of the model including RightAP,
LeftAP and RightNP factors compared to the model including
only RightAP and LeftAP factors yields x2 = 107 and p &lt;
.001.
</footnote>
<note confidence="0.776992">
(Gulordava and Merlo, 2015).
</note>
<sectionHeader confidence="0.997531" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999979076923077">
In this paper, we have developed a model of de-
pendency length minimisation in the noun phrase
and shown subtle interactions among its subcom-
ponents. We show that most of DLM predictions
are confirmed, and that DLM also apply to short
spans. The fact that DLM effects also hold in
such short spans casts doubts, in our opinion, on
the grounding of this effect in memory limitations.
The subtle interactions also raise questions on the
exact definition of what dependencies are min-
imised and to what extent a given dependency an-
notation captures these distinctions, questions that
we reserve for future work.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999975666666667">
We gratefully acknowledge the partial funding of
this work by the Swiss National Science Foun-
dation, under grant 144362 to the second author.
Thanks also to the Labex EFL (ANR-10-LABX-
0083 EFL) in Paris for supporting the visit of the
first author.
</bodyText>
<sectionHeader confidence="0.993572" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.961955">
Anne Abeill´e and Daniele Godard. 2000. French word
order and lexical weight. In Robert D. Borsley, edi-
tor, The nature and function of Syntactic Categories,
volume 32 of Syntax and Semantics, pages 325–360.
BRILL.
ˇZeljko Agi´c, Maria Jesus Aranzabe, Aitziber Atutxa,
Cristina Bosco, Jinho Choi, Marie-Catherine
de Marneffe, Timothy Dozat, Rich´ard Farkas,
Jennifer Foster, Filip Ginter, Iakes Goenaga,
Koldo Gojenola, Yoav Goldberg, Jan Hajiˇc, An-
ders Trærup Johannsen, Jenna Kanerva, Juha
Kuokkala, Veronika Laippala, Alessandro Lenci,
Krister Lind´en, Nikola Ljubeˇsi´c, Teresa Lynn,
Christopher Manning, H´ector Alonso Martinez,
Ryan McDonald, Anna Missil¨a, Simonetta Monte-
magni, Joakim Nivre, Hanna Nurmi, Petya Osen-
ova, Slav Petrov, Jussi Piitulainen, Barbara Plank,
Prokopis Prokopidis, Sampo Pyysalo, Wolfgang
Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi,
Kiril Simov, Aaron Smith, Reut Tsarfaty, Veronika
Vincze, and Daniel Zeman. 2015. Universal depen-
dencies 1.1.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
7.
</reference>
<page confidence="0.992393">
481
</page>
<reference confidence="0.9998524">
Denis Bouchard. 1998. The distribution and inter-
pretation of adjectives in French: A consequence of
Bare Phrase Structure. Probus, 10(2):139–184.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Har-
ald Baayen. 2007. Predicting the dative alternation.
In G. Boume, I. Kraemer, and J. Zwarts, editors,
Cognitive Foundations of Interpretation, pages 69–
94. Royal Netherlands Academy of Science, Ams-
terdam.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL-X
’06, pages 149–164, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Guglielmo Cinque. 2010. The Syntax of Adjectives: A
Comparative Study. MIT Press.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210,
November.
Richard Futrell, Kyle Mahowald, and Edward Gibson.
2015. Large-Scale Evidence of Dependency Length
Minimization in 37 Languages. (Submitted to Pro-
ceedings of the National Academy of Sciences of the
United States of America).
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1–
76.
Daniel Gildea and David Temperley. 2010. Do Gram-
mars Minimize Dependency Length? Cognitive Sci-
ence, 34(2):286–310.
Kristina Gulordava and Paola Merlo. 2015. Structural
and lexical factors in adjective placement in com-
plex noun phrases across Romance languages. In
Proceedings of the Nineteenth Conference on Com-
putational Natural Language Learning (CoNLL’15).
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning: Shared Task, CoNLL ’09,
pages 1–18, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John A Hawkins. 1994. A performance theory of or-
der and constituency. Cambridge University Press,
Cambridge.
John A. Hawkins. 2004. Efficiency and Complexity in
Grammars. Oxford linguistics. Oxford University
Press, Oxford, UK.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2012. A Universal Part-of-Speech Tagset. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12),
pages 2089–2096, Istanbul, Turkey.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300–
333.
Juliette Thuilier. 2012. Contraintes pr´ef´erentielles et
ordre des mots en franc¸ais. Ph.D. Thesis, Universit´e
Paris-Diderot - Paris VII, Sep.
Harry Joel Tily. 2010. The role of processing com-
plexity in word order variation and change. Ph.D.
Thesis, Stanford University.
Thomas Wasow. 2002. Postverbal Behavior. CSLI
Publications.
</reference>
<page confidence="0.998453">
482
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.574017">
<title confidence="0.974336">Dependency length minimisation effects in short spans: a analysis of adjective placement in complex noun phrases</title>
<author confidence="0.999953">Kristina Gulordava Paola Merlo Benoit Crabb´e</author>
<affiliation confidence="0.999145">University of Geneva University of Geneva U. Paris Diderot/Inria/IUF</affiliation>
<address confidence="0.802696">Kristina.Gulordava, Paola.Merlo@unige.ch bcrabbe@</address>
<email confidence="0.963825">univ-paris-diderot.fr</email>
<abstract confidence="0.9853118125">It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efficiency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Daniele Godard</author>
</authors>
<title>French word order and lexical weight.</title>
<date>2000</date>
<booktitle>The nature and function of Syntactic Categories, volume 32 of Syntax and Semantics,</booktitle>
<pages>325--360</pages>
<editor>In Robert D. Borsley, editor,</editor>
<publisher>BRILL.</publisher>
<marker>Abeill´e, Godard, 2000</marker>
<rawString>Anne Abeill´e and Daniele Godard. 2000. French word order and lexical weight. In Robert D. Borsley, editor, The nature and function of Syntactic Categories, volume 32 of Syntax and Semantics, pages 325–360. BRILL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Maria Jesus Aranzabe</author>
<author>Aitziber Atutxa</author>
<author>Cristina Bosco</author>
<author>Jinho Choi</author>
<author>Marie-Catherine de Marneffe</author>
<author>Timothy Dozat</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
<author>Filip Ginter</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Jan Hajiˇc, Anders Trærup Johannsen, Jenna Kanerva, Juha Kuokkala, Veronika Laippala, Alessandro Lenci, Krister Lind´en, Nikola Ljubeˇsi´c,</title>
<date>2015</date>
<journal>Universal dependencies</journal>
<volume>1</volume>
<location>Teresa Lynn, Christopher Manning, H´ector Alonso Martinez, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Joakim Nivre, Hanna</location>
<marker>Agi´c, Aranzabe, Atutxa, Bosco, Choi, de Marneffe, Dozat, Farkas, Foster, Ginter, 2015</marker>
<rawString>ˇZeljko Agi´c, Maria Jesus Aranzabe, Aitziber Atutxa, Cristina Bosco, Jinho Choi, Marie-Catherine de Marneffe, Timothy Dozat, Rich´ard Farkas, Jennifer Foster, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Jan Hajiˇc, Anders Trærup Johannsen, Jenna Kanerva, Juha Kuokkala, Veronika Laippala, Alessandro Lenci, Krister Lind´en, Nikola Ljubeˇsi´c, Teresa Lynn, Christopher Manning, H´ector Alonso Martinez, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Joakim Nivre, Hanna Nurmi, Petya Osenova, Slav Petrov, Jussi Piitulainen, Barbara Plank, Prokopis Prokopidis, Sampo Pyysalo, Wolfgang Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi, Kiril Simov, Aaron Smith, Reut Tsarfaty, Veronika Vincze, and Daniel Zeman. 2015. Universal dependencies 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Ben Bolker</author>
<author>Steven Walker</author>
</authors>
<title>lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-7.</title>
<date>2014</date>
<contexts>
<context position="11890" citStr="Bates et al., 2014" startWordPosition="2046" endWordPosition="2049">reebanks), and English borrowings. In addition, we leave out noun phrases which have their elements separated by punctuation (for example, commas or parentheses) to ensure that the placement of adjective is not affected by an unusual structure of the noun phrase. 479 for Italian to 78% for Catalan. Among all adjective types, at least 10% in each language are observed both prenominally and postnominally (ranging between 147 types for Italian and 445 types for Spanish). 3.2 Method: Mixed Effects models We analyse the interactions of several dependency factors, using a logit mixed effect models (Bates et al., 2014). Mixed-effect logistic regression models (logit models) are a type of Generalized Linear Mixed Models with the logit link function and are designed for binomially distributed outcomes such as Order in our case. More precisely, Generalized Linear Mixed Models describe an outcome as the linear combination of fixed effects X and conditional random effects Z associated with grouping of instances, where Q and -y are the corresponding weights of the effects. (2) y = XQ + Z-y + E In logistic regression models, this linear combination is then transformed with the logit link function to predict the bi</context>
</contexts>
<marker>Bates, Maechler, Bolker, Walker, 2014</marker>
<rawString>Douglas Bates, Martin Maechler, Ben Bolker, and Steven Walker, 2014. lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Bouchard</author>
</authors>
<title>The distribution and interpretation of adjectives in French: A consequence of Bare Phrase Structure.</title>
<date>1998</date>
<journal>Probus,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="3534" citStr="Bouchard, 1998" startWordPosition="584" endWordPosition="585">explanations are really rooted in memory and efficiency, will they still hold in phrases that might span only a few words? In this paper, we look at the structural factors that play a role in adjective-noun word order alternations in Romance languages. We choose Romance languages because they show a good amount of variation, making studies of DLM meaningful. This would not be the case in English, for instance, as English has no variation of word order placement in the noun phrase. Adjective placement in Romance is often studied in connection with semantic and lexical properties of adjectives (Bouchard, 1998; Cinque, 2010). There exists, however, a body of work which shows that structural syntactic properties like the size of adjective phrase also affect the adjective position (Abeill´e and Godard, 2000; Thuilier, 2012). We demonstrate that, unlike results for the ver1The minimal dependency length is equal to one when the head and its dependent are adjacent. 477 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 477–482, Beijing, China, July 26-31, 2015. c�2015 Asso</context>
</contexts>
<marker>Bouchard, 1998</marker>
<rawString>Denis Bouchard. 1998. The distribution and interpretation of adjectives in French: A consequence of Bare Phrase Structure. Probus, 10(2):139–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Anna Cueni</author>
<author>Tatiana Nikitina</author>
<author>Harald Baayen</author>
</authors>
<title>Predicting the dative alternation.</title>
<date>2007</date>
<booktitle>Cognitive Foundations of Interpretation,</booktitle>
<pages>69</pages>
<editor>In G. Boume, I. Kraemer, and J. Zwarts, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="2231" citStr="Bresnan et al., 2007" startWordPosition="347" endWordPosition="350">ad and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest overall dependency length (DL) is preferred. We measure the length of a dependency as the number of words between the head and its dependent. As an illustration, DLM principle is widely reported in the literature to explain the alternation of postverbal complements (Bresnan et al., 2007; Wasow, 2002). Consider, for example, the case when a verb has both a direct object (NP) and a prepositional complement or adjunct (PP). Two alternative orders of the verb complements are possible: VP1 = V NP PP, whose length is DL1 and VP2 = V PP NP, whose length is DL2. DL1 is DL(V-NP) + DL(V-PP) _ INPI + 1; DL2 is DL(V-NP) + DL(V-PP) _ IPPI + 1. 1 If DL1 is bigger than DL2, then VP2 is preferred over VP1, despite the non-canonical V-PP-NP order. While DLM has been demonstrated on a large scale and explanations have been proposed based on human sentence processing facts in the verbal domain</context>
</contexts>
<marker>Bresnan, Cueni, Nikitina, Baayen, 2007</marker>
<rawString>Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald Baayen. 2007. Predicting the dative alternation. In G. Boume, I. Kraemer, and J. Zwarts, editors, Cognitive Foundations of Interpretation, pages 69– 94. Royal Netherlands Academy of Science, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10006" citStr="Buchholz and Marsi, 2006" startWordPosition="1748" endWordPosition="1751">f α is negative or zero and the weight of Q is positive or zero. On average, therefore, we expect to see a negative effect of α (1a) and a positive effect of Q (1b). We develop a model to test which of the finegrained predictions derived from DLM are confirmed by the data provided by the dependency annotated corpora of five of the main Romance languages. 3 Identifying dependency minimisation factors 3.1 Materials: Dependency treebanks We use the dependency annotated corpora of five Romance languages: Catalan, Spanish, Italian (Hajiˇc et al., 2009), French (Agi´c et al., 2015), and Portuguese (Buchholz and Marsi, 2006). We use part-of-speech information and dependency arcs from the gold annotation to extract noun phrases containing adjectives. Specifically, we first convert all treebanks to coarse universal part-of-speech tags, using existing conventional mappings from the original tagset to the universal tagset (Petrov et al., 2012). We then identify all adjectives (tagged using the universal PoS tag ‘ADJ’) whose dependency head is a noun (tagged using the universal PoS tag ‘NOUN’). In addition, we recover all elements of the noun phrase rooted in this noun, that is, its dependency subtree. For all languag</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 149–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guglielmo Cinque</author>
</authors>
<title>The Syntax of Adjectives: A Comparative Study.</title>
<date>2010</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3549" citStr="Cinque, 2010" startWordPosition="586" endWordPosition="587"> really rooted in memory and efficiency, will they still hold in phrases that might span only a few words? In this paper, we look at the structural factors that play a role in adjective-noun word order alternations in Romance languages. We choose Romance languages because they show a good amount of variation, making studies of DLM meaningful. This would not be the case in English, for instance, as English has no variation of word order placement in the noun phrase. Adjective placement in Romance is often studied in connection with semantic and lexical properties of adjectives (Bouchard, 1998; Cinque, 2010). There exists, however, a body of work which shows that structural syntactic properties like the size of adjective phrase also affect the adjective position (Abeill´e and Godard, 2000; Thuilier, 2012). We demonstrate that, unlike results for the ver1The minimal dependency length is equal to one when the head and its dependent are adjacent. 477 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 477–482, Beijing, China, July 26-31, 2015. c�2015 Association for Com</context>
</contexts>
<marker>Cinque, 2010</marker>
<rawString>Guglielmo Cinque. 2010. The Syntax of Adjectives: A Comparative Study. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>109</volume>
<issue>2</issue>
<contexts>
<context position="1371" citStr="Demberg and Keller, 2008" startWordPosition="210" endWordPosition="214"> spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest overall dependency length (</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Vera Demberg and Frank Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109(2):193–210, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Futrell</author>
<author>Kyle Mahowald</author>
<author>Edward Gibson</author>
</authors>
<date>2015</date>
<booktitle>Large-Scale Evidence of Dependency Length Minimization in 37 Languages. (Submitted to Proceedings of the National Academy of Sciences of the United States of America).</booktitle>
<contexts>
<context position="1446" citStr="Futrell et al., 2015" startWordPosition="222" endWordPosition="225">dependency that is at stake, but also the effect of the surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest overall dependency length (DL) is preferred. We measure the length of a dependency as the number of wo</context>
</contexts>
<marker>Futrell, Mahowald, Gibson, 2015</marker>
<rawString>Richard Futrell, Kyle Mahowald, and Edward Gibson. 2015. Large-Scale Evidence of Dependency Length Minimization in 37 Languages. (Submitted to Proceedings of the National Academy of Sciences of the United States of America).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: Locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>68</volume>
<issue>1</issue>
<pages>76</pages>
<contexts>
<context position="1314" citStr="Gibson, 1998" startWordPosition="203" endWordPosition="205">strate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phra</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson. 1998. Linguistic complexity: Locality of syntactic dependencies. Cognition, 68(1):1– 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>David Temperley</author>
</authors>
<title>Do Grammars Minimize Dependency Length?</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="5774" citStr="Gildea and Temperley (2010)" startWordPosition="954" endWordPosition="958">e can have parents and right modifiers (X and Y, respectively, in Figure 1). These alternative orderings yield different dependency lengths, as can be seen from Figure 1. By convention, we will always indicate the prenominal order as DL1, and the postnominal order as DL2. Their difference is always calculated as DL1 − DL2. We consider all dependencies in a noun phrase and not only the length of the noun-adjective dependency. This is because we assume, as previously done, that DLM is global, and not a local, effect. Our analysis is a faithful interpretation of the very general DLM principle of Gildea and Temperley (2010) which is based on the overall dependency length of a sentence. We do no take other dependencies in the sentence into account, because their lengths are the same across DL1 and DL2. The difference DL1 − DL2 is therefore the difference between the overall dependency length of two sentences that differ only in their placement of one adjective. The first panel, panel a, shows the case where the parent of the NP is on the left of it. The dependency length for the prenominal adjective structure is equal to DL1 = di + d2 = (|α |+ |0 |+ 1) + |0 |and for the postnominal adjective structure is DL2 = d1</context>
</contexts>
<marker>Gildea, Temperley, 2010</marker>
<rawString>Daniel Gildea and David Temperley. 2010. Do Grammars Minimize Dependency Length? Cognitive Science, 34(2):286–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Gulordava</author>
<author>Paola Merlo</author>
</authors>
<title>Structural and lexical factors in adjective placement in complex noun phrases across Romance languages.</title>
<date>2015</date>
<booktitle>In Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL’15).</booktitle>
<contexts>
<context position="17610" citStr="Gulordava and Merlo, 2015" startWordPosition="3039" endWordPosition="3042">.77, p &lt; 0.001).4 The presence of a noun dependent on the right of the noun favours a prenominal placement, as predicted by DLM (1d). This is a result which, to our knowledge, was not previously observed in the literature, and that clearly answers our initial question, confirming that DLM also applies to very short spans. A much more detailed study of the lexical and structural properties of this effect is developed in 4A log-likelihood test of the model including RightAP, LeftAP and RightNP factors compared to the model including only RightAP and LeftAP factors yields x2 = 107 and p &lt; .001. (Gulordava and Merlo, 2015). 4 Conclusion In this paper, we have developed a model of dependency length minimisation in the noun phrase and shown subtle interactions among its subcomponents. We show that most of DLM predictions are confirmed, and that DLM also apply to short spans. The fact that DLM effects also hold in such short spans casts doubts, in our opinion, on the grounding of this effect in memory limitations. The subtle interactions also raise questions on the exact definition of what dependencies are minimised and to what extent a given dependency annotation captures these distinctions, questions that we res</context>
</contexts>
<marker>Gulordava, Merlo, 2015</marker>
<rawString>Kristina Gulordava and Paola Merlo. 2015. Structural and lexical factors in adjective placement in complex noun phrases across Romance languages. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL’15).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09, pages 1–18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>A performance theory of order and constituency.</title>
<date>1994</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1329" citStr="Hawkins, 1994" startWordPosition="206" endWordPosition="207">nimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one wit</context>
<context position="16635" citStr="Hawkins, 1994" startWordPosition="2870" endWordPosition="2871"> and Godard, 2000) and the DLM prediction. The external dependency factor is not significant (p &gt; 0.1). Moreover, the log likelihood ratio between the full model and the model without ExtDep is x2 distributed with 1 degree of freedom with x2 = 3.8,p = 0.052. This comparison confirms that the introduction of the external dependency does not help predicting the Order. At first sight, this result suggests that this dependency is not subject to the minimisation principle. A plausible explanation claims that only the dependencies between the head and the edge of the dependent phrase are minimised (Hawkins, 1994). In Romance languages, the majority of the noun phrases take an article which unambiguously defines the left edge of the noun phrase. There is no need therefore to minimize the external dependency to the noun, since the noun phrase can be entirely predicted based on its left corner. The RightNP factor is significant in the fitted model (βRNP = −0.77, p &lt; 0.001).4 The presence of a noun dependent on the right of the noun favours a prenominal placement, as predicted by DLM (1d). This is a result which, to our knowledge, was not previously observed in the literature, and that clearly answers our</context>
</contexts>
<marker>Hawkins, 1994</marker>
<rawString>John A Hawkins. 1994. A performance theory of order and constituency. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>Efficiency and Complexity in Grammars. Oxford linguistics.</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="1344" citStr="Hawkins, 2004" startWordPosition="208" endWordPosition="209"> occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest </context>
</contexts>
<marker>Hawkins, 2004</marker>
<rawString>John A. Hawkins. 2004. Efficiency and Complexity in Grammars. Oxford linguistics. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan T McDonald</author>
</authors>
<title>A Universal Part-of-Speech Tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>2089--2096</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="10327" citStr="Petrov et al., 2012" startWordPosition="1794" endWordPosition="1797">of the main Romance languages. 3 Identifying dependency minimisation factors 3.1 Materials: Dependency treebanks We use the dependency annotated corpora of five Romance languages: Catalan, Spanish, Italian (Hajiˇc et al., 2009), French (Agi´c et al., 2015), and Portuguese (Buchholz and Marsi, 2006). We use part-of-speech information and dependency arcs from the gold annotation to extract noun phrases containing adjectives. Specifically, we first convert all treebanks to coarse universal part-of-speech tags, using existing conventional mappings from the original tagset to the universal tagset (Petrov et al., 2012). We then identify all adjectives (tagged using the universal PoS tag ‘ADJ’) whose dependency head is a noun (tagged using the universal PoS tag ‘NOUN’). In addition, we recover all elements of the noun phrase rooted in this noun, that is, its dependency subtree. For all languages where this information is available, we extract lemmas of adjective and noun tokens which are the features in our analysis. The only treebank without lemma annotation is French, for which we extract token forms.2 We extract a total of around 64’000 instances of adjectives in noun phrases, ranging from 2’800 for Itali</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan T. McDonald. 2012. A Universal Part-of-Speech Tagset. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 2089–2096, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Temperley</author>
</authors>
<title>Minimization of dependency length in written English.</title>
<date>2007</date>
<journal>Cognition,</journal>
<volume>105</volume>
<issue>2</issue>
<pages>333</pages>
<contexts>
<context position="1423" citStr="Temperley, 2007" startWordPosition="220" endWordPosition="221">he length of the dependency that is at stake, but also the effect of the surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest overall dependency length (DL) is preferred. We measure the length of a depende</context>
</contexts>
<marker>Temperley, 2007</marker>
<rawString>David Temperley. 2007. Minimization of dependency length in written English. Cognition, 105(2):300– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juliette Thuilier</author>
</authors>
<title>Contraintes pr´ef´erentielles et ordre des mots en franc¸ais.</title>
<date>2012</date>
<tech>Ph.D. Thesis,</tech>
<institution>Universit´e Paris-Diderot - Paris VII,</institution>
<contexts>
<context position="3750" citStr="Thuilier, 2012" startWordPosition="617" endWordPosition="618">er alternations in Romance languages. We choose Romance languages because they show a good amount of variation, making studies of DLM meaningful. This would not be the case in English, for instance, as English has no variation of word order placement in the noun phrase. Adjective placement in Romance is often studied in connection with semantic and lexical properties of adjectives (Bouchard, 1998; Cinque, 2010). There exists, however, a body of work which shows that structural syntactic properties like the size of adjective phrase also affect the adjective position (Abeill´e and Godard, 2000; Thuilier, 2012). We demonstrate that, unlike results for the ver1The minimal dependency length is equal to one when the head and its dependent are adjacent. 477 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 477–482, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics RightNP=Yes RightNP=No |0 |− |α| −3|α |− 2 Table 1: Dependency length difference for different types of noun phrases. By convention, we always calculate DL1 − DL2. bal domain, it</context>
</contexts>
<marker>Thuilier, 2012</marker>
<rawString>Juliette Thuilier. 2012. Contraintes pr´ef´erentielles et ordre des mots en franc¸ais. Ph.D. Thesis, Universit´e Paris-Diderot - Paris VII, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Joel Tily</author>
</authors>
<title>The role of processing complexity in word order variation and change.</title>
<date>2010</date>
<tech>Ph.D. Thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1490" citStr="Tily, 2010" startWordPosition="230" endWordPosition="231">e surrounding dependencies. 1 Introduction One of the main goals in the study of language is to find explanations for those fundamental properties that are found in every human language. The observation that human languages appear to minimise the distance between any two related words – called the property of dependency length minimisation (DLM) — is a universal property that has been documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004; Demberg and Keller, 2008), in corpus properties of treebanks (Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). Functional explanations have been proposed for this pervasive linguistic property. If speakers want to reduce memory load and maximise efficiency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest overall dependency length (DL) is preferred. We measure the length of a dependency as the number of words between the head and its dependent. As a</context>
</contexts>
<marker>Tily, 2010</marker>
<rawString>Harry Joel Tily. 2010. The role of processing complexity in word order variation and change. Ph.D. Thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Wasow</author>
</authors>
<title>Postverbal Behavior.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="2245" citStr="Wasow, 2002" startWordPosition="351" endWordPosition="352">ency of processing, they will choose to produce and preferentially analyse constructions where words are linearised in such a way that minimises the total distance of related words. The DLM principle can be stated as follows: if there exist possible alternative orderings of a phrase, the one with the shortest overall dependency length (DL) is preferred. We measure the length of a dependency as the number of words between the head and its dependent. As an illustration, DLM principle is widely reported in the literature to explain the alternation of postverbal complements (Bresnan et al., 2007; Wasow, 2002). Consider, for example, the case when a verb has both a direct object (NP) and a prepositional complement or adjunct (PP). Two alternative orders of the verb complements are possible: VP1 = V NP PP, whose length is DL1 and VP2 = V PP NP, whose length is DL2. DL1 is DL(V-NP) + DL(V-PP) _ INPI + 1; DL2 is DL(V-NP) + DL(V-PP) _ IPPI + 1. 1 If DL1 is bigger than DL2, then VP2 is preferred over VP1, despite the non-canonical V-PP-NP order. While DLM has been demonstrated on a large scale and explanations have been proposed based on human sentence processing facts in the verbal domain, it is not cl</context>
</contexts>
<marker>Wasow, 2002</marker>
<rawString>Thomas Wasow. 2002. Postverbal Behavior. CSLI Publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>