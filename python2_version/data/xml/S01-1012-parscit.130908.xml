<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008865">
<note confidence="0.733999">
The SENSEVAL-2 Panel on Domains, Topics and Senses
</note>
<footnote confidence="0.32020225">
Paul Buitelaar
DFKI GmbH
Stuhlsatzenhausweg 3
D-66123 Saarbruecken, Germany
</footnote>
<email confidence="0.86474">
paulb@dtki.de
</email>
<sectionHeader confidence="0.8770755" genericHeader="method">
1 Introduction: Why Domains
Matter in Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999945782608696">
An important aspect of sense disambiguation is
the wider semantic space (domain, topic) in
which the ambiguous word occurs. This may be
most clearly illustrated by some cross-lingual
examples, as they would appear in (machine)
translation. Consider for instance the English
word housing. In a more general &amp;quot;sense&amp;quot;, this
translates in German into Wohnung. In an
engineering setting however it translates into
Gehause. Also verbs may be translated
differently (i.e. have a different sense) according
to the semantic space in which they occur. For
instance, English warming up translates into
erhitzen in a more general sense, but into
aufwiirmen in the sports domain.
Because of the apparent relevance then of
domains or topics on sense disambiguation, a
panel was organized at SENSEVAL-2 to discuss
some current and previous work in this area. The
paper presents a more extended overview based
on the relevant literature, besides giving a
summary of the discussion that developed after
the panel presentations.
</bodyText>
<sectionHeader confidence="0.820177" genericHeader="method">
2 Domains, Topics and Senses
2.1 Subject Codes
</sectionHeader>
<bodyText confidence="0.949895857142857">
A semantic space may be indicated in a
dictionary by use of a so-called &amp;quot;subject code&amp;quot;.
In LDOCE for instance, subject codes like MD,
for the medical domain, or ML, for meteorology
are used to define which senses of a word are
used in which domains. Three of the senses of
the word high for instance correspond to three
different domains: music (a high tone), drugs
(the experience of being high) and meteorology
(a high pressure area).
Subject codes can be used to detect the topic of
a text segment by simply counting their
frequency over all content words (Walker and
Amsler 1986). At the same time, however,
subject codes can be used in sense
disambiguation by constructing topic specific
context models (Guthrie et. al 1991). Such
&amp;quot;neighborhoods&amp;quot; can be constructed by taking
into account all words in the definitions and in
sample sentences of all words in the dictionary
that share the same subject code. For instance,
the word bank has the following neighborhoods
for the financial and medical domains:
write safe sum
account person put
take money order
keep pay supply
paper draw cheque
</bodyText>
<tableCaption confidence="0.99902">
Table 1: Financial neighborhood of bank
</tableCaption>
<table confidence="0.9771644">
medicine product hold
origin place human
treatment blood hospital
use store
organ comb
</table>
<tableCaption confidence="0.999308">
Table 2: Medical neighborhood of bank
</tableCaption>
<bodyText confidence="0.999640153846154">
Using subject codes in sense disambiguation has
been shown to be fruitful, relative to using other
sources of knowledge. As reported in
(Stevenson and Wilks 1999), the performance of
using only subject codes (79% precision) was
much better than that of using only dictionary
definition words (65%), or selection restrictions
(44%). Given these results it seems worthwhile
to identify also the semantic space of WordNet
synsets more explicitly by the introduction of
subject codes (Magnini and Cavaglia 2000).
This allows for grouping together synsets across
part-of-speech, as in the medical domain
</bodyText>
<page confidence="0.975783">
49
</page>
<equation confidence="0.665948">
(doctor#1, hospital#1; operate#7) and across sub-
hierarchies, as in the sports domain (life_form#1:
athlete#1; physical_object#1: game_equipment#1;
act#2: sport#1; location#1: playing_field#1).
</equation>
<subsectionHeader confidence="0.99866">
2.2 Topic Signatures and Variation
</subsectionHeader>
<bodyText confidence="0.999901176470588">
The topic specific context models as constructed
by (Guthrie et al. 1991) can be viewed as
&amp;quot;signatures&amp;quot; of the topic in question. Such topic
signatures can, however, be constructed even
without the use of subject codes by generating
them (semi-) automatically from a lexical
resource and then validating them on topic
specific corpora (Hearst and Schtitze 1993).
An extension of this idea is to treat
senses, or rather WordNet synsets, as topics for
which a signature can be constructed. One
approach to this is to retrieve relevant
documents through search engines on the web
by defining queries for each synset (Agirre et al.
2000, Agirre et al. 2001). For instance, the
following query can be defined for the first
WordNet sense of boy:
</bodyText>
<equation confidence="0.93488325">
#1 (boy AND (altar boy OR ball boy OR ...)
#2 AND NOT (man OR ... OR broth of a boy OR
#3 son OR ... OR mama&apos;s boy OR
#4 nigger OR ... OR black)
</equation>
<bodyText confidence="0.999767019230769">
The document collections retrieved are then
analysed and a list of the most relevant words
for each synset is generated as its topic
signature. Examples (abridged) for the first three
senses of boy are:
Sense 1 Sense 2 Sense 3
child gay human
Child reference son
person tpd-results Human
Constructing topic signatures for senses implies
that a dominant sense can be identified given a
certain topic or domain. This may be true for
clearly ambiguous words (i.e in the case of
homonymy). For instance, sentence will be
dominant in the judicial sense in the law domain
and in the syntactic sense in the linguistics
domain. However, for words with related senses
(i.e in the case of systematic polysemy) the topic
signatures will overlap, as with the results on
boy in sense 1: young male person and sense 3:
son. This has been shown also from a somewhat
different viewpoint in reaction to (Gale et al.
1992), in which it was stated that one sense will
be uniquely used within a discourse (which we
can equate with a topic or domain for our
purposes here). Instead, many words have
overlapping senses that will be used
simultaneously throughout one discourse
(Krovetz 1998).
The main question that remains now is,
what exactly constitutes a discourse / subject /
topic / domain? We can get closer at answering
this question by looking at some empirical sense
disambiguation results that involve a variation
of topic. More specifically, we can observe
some effects of topic variation by training a
sense disambiguation system on one topic and
applying it to another. For instance, training on
Wall Street Journal while testing on SemCor and
vice versa shows a degrading of 12% and 19%
in precision (Escudero et al. 2000). On the other
hand, applying context information
(collocations) extracted from Wall Street Journal
to a financial text in SemCor shows significantly
higher precision than on texts in other domains
in SemCor (Martinez and Agirre 2000).
These results therefore suggest that a
discourse / subject / topic / domain corresponds
to a larger or smaller chunk of text (a corpus, a
text or a text segment) with a homogeneous
distribution of senses and corresponding
collocations.
</bodyText>
<subsectionHeader confidence="0.994025">
2.3 Tuning
</subsectionHeader>
<bodyText confidence="0.999982181818182">
But even with a clearly defined domain, it is far
from certain that any general sense inventory
will be appropriate. &amp;quot;The usual scenario ... has
been that the word senses are taken from a
general purpose dictionary, ... whereas the
material to be disambiguated is ... Wall Street
Journal. ... So, the profiles [Signatures,
Collocations] ... will be for general English
senses according to the WSJ ...&amp;quot; (Kilgarriff
1998). Instead, a general sense inventory needs
to be tuned to the domain at hand. This involves
selecting only those senses that are most
appropriate for the domain, as well as extending
the sense inventory with novel words (terms)
and novel senses, specific to the domain (Basili
et al. 1997; Cucchiarelli and Velardi 1998;
Turcato et at. 2000; Buitelaar and Sacaleanu
2001; Vossen 2001).
According to the method described in
(Cucchiarelli and Velardi 1998), a domain
specific sense inventory that is balanced (even
distribution of words to senses) and at the right
</bodyText>
<page confidence="0.980088">
50
</page>
<bodyText confidence="0.969551210526316">
level of abstraction (ambiguity vs.
generalization) can be selected automatically
given the following criteria: &amp;quot;Generality&amp;quot;,
&amp;quot;Discrimination Power&amp;quot;, &amp;quot;Domain Coverage&amp;quot;
and &amp;quot;Average Ambiguity.&amp;quot; Applying these
criteria in a quantitative way to a general sense
inventory (i.e the WordNet hierarchy) and a
given domain specific corpus automatically
selects a set of relevant categories (i.e. top level
synsets). For instance, this method selects
following categories for the financial domain:
person, individual,...
instrumentality,...
written_communication,...
possession,...
Only senses that are subsumed by these
categories are included in the domain specific
sense inventory. For instance, for the word
stock, only 5 out of 16 senses are selected:
</bodyText>
<figure confidence="0.9318844">
#1 capital &gt; asset &gt; possession
#2 support &gt; device &gt; instrumentality
#4 document &gt; &gt; written_communication
#5 accumulation &gt; asset &gt; possession
#6 ancestor &gt; relative &gt; person,individual
Senses that are discarded include:
#7 soup &gt;...
#9 plant_part &gt;
#12 lineage,line,line_of_descent &gt;
#14 lumber,timber &gt;
</figure>
<bodyText confidence="0.999868714285714">
The method described above uses a top down
approach that propagates the domain relevance
of certain top level synsets down through the
(WordNet) hierarchy. A somewhat different
approach would be to assign a domain relevance
to each concept (i.e. word sense, synset) from
the bottom up (Buitelaar and Sacaleanu 2001).
This method determines the domain specific
relevance of (WordNet, GermaNet) synsets on
the basis of the relevance of their constituent
synonyms that co-occur within representative
domain corpora.
Next to selecting domain relevant
concepts from the general sense inventory, novel
terms (those not covered by the sense inventory)
need to be accounted for also. This includes
adding morphological and syntactic variants of
known terms (Vossen 2001) as well as
extending the inventory with semantically
related terms through classification and/or
clustering.
</bodyText>
<sectionHeader confidence="0.991373" genericHeader="method">
3 Panel Discussion
</sectionHeader>
<bodyText confidence="0.999982666666667">
In the panel presentations most of the issues
discussed above were addressed. Central to the
discussion were the following two questions:
</bodyText>
<listItem confidence="0.9998845">
• Is generic sense disambiguation possible?
• Is sense disambiguation always necessary?
</listItem>
<bodyText confidence="0.999974365853659">
The first question concerns the influence of the
semantic space (topic, domain, etc.) on the
disambiguation process. Unlike with PoS
tagging, it seems hard and perhaps even
theoretically impossible to define a &apos;general&apos;
training corpus and sense inventory for sense
disambiguation. Instead, it seems necessary to
tightly connect sense disambiguation to topic
detection or text classification in order to
recognize the wider semantic space of
ambiguous words. The second question is
concerned with the even more fundamental
observation that sense disambiguation is
unneccessary if one sense (or more than one, in
the case of systematic polysemy) can be
assigned unambiguously within a certain
semantic space. The disambiguation problem
then shifts towards an appropriate modelling of
such semantic spaces (i.e. domain modelling). In
summary, it may not be feasible to separate
sense disambiguation from the domain in which
it operates, which in turn implies that modelling
this domain is the first priority for sense
disambiguation. In the discussion, however,
several arguments were raised against such a
view of sense disambiguation.
First of all, such an approach drives us
back to earlier domain specific methods. These
were not very robust and required major efforts
in adapting to new domains. As a counter
argument to this point, it was noted that there
are now many robust, machine-learning based
methods available for lexical acquisition, which
would allow for a rapid adaptation of the
disambiguation resources to a new domain. The
second main issue raised was that, from an
evaluation point of view, it is important to
evaluate the performance of different
algorithms, independent from a specific domain
or application. As a counter argument to this, the
question was asked what such an evaluation
</bodyText>
<page confidence="0.996567">
51
</page>
<bodyText confidence="0.99789325">
would then prove. Sense disambiguation
evaluated without a particular (application)
domain can only show an artificial result which
is hard to interpret and to generalize over. This
is illustrated in particular by low interannotator
agreement scores obtained when disambiguating
without the context of a certain domain.
The discussion did not reach a
consensus on these points, although there was
general agreement that future evaluation efforts
in sense disambiguation should take applications
(and hence certain domains) into account. The
following table gives an overview of those
teams that participated at SENSEVAL-2 and
declared to be using domains, topical context or
the ,,One Sense per Discourse&amp;quot; heuristic.
</bodyText>
<table confidence="0.995969384615385">
Team Domain Topical One Sense /
Information Context Discourse
Lexical Sample Task (English)
IRST .0
TALP ..0 .0
BCU-EHU .0
KUNLP ...,
All Words Task (English)
IRST .0
BCU-EHU .0
Sheffield .0
Sussex v
UCLA .0
</table>
<bodyText confidence="0.999858307692308">
On the lexical sample task, KUNLP and TALP
had both high precision and recall, while BCU-
EHU and IRST reached the highest precision of
all participating systems, but at a low recall. On
the all words task, all teams in the table scored
average to low, except for IRST, which reached
again a very high precision at a low recall.
These results are unfortunately still
inconclusive about the general merit of domain
and topic information. Only the anomalous
results of IRST may indicate the advantage of
domain information for reaching a high
precision in sense disambiguation.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="conclusions">
4 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999697428571429">
Many thanks to Eneko Agirre, Nancy Ide,
Bernardo Magnini and Piek Vossen for their
contributions to the panel, and to the
SENSEVAL-2 audience for their active
participation in the discussion. This research has
in part been supported by EC/NSF grant IST-
1999-11438 for the MUCHMORE project.
</bodyText>
<sectionHeader confidence="0.722954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.973753053571428">
Agirre E., Ansa 0., Hovy E., Martinez D.
Enriching very large ontologies using the WWW. In:
Proceedings of the Ontology Learning Workshop
ECAI 2000.
Agirre E., Ansa 0., Martinez D., Hovy E.
Enriching WordNet concepts with topic signatures.
In: Proceedings NAACL WordNet Workshop, 2001.
Basili R., Della Rocca M., Pazienza M.-T,
Contextual Word Sense Tuning and Disambiguation.
Applied Artificial Intelligence, vol. 11, 1997.
Buitelaar P., Sacaleanu B. Ranking and Selecting
Synsets by Domain Relevance. In: Proceedings
NAACL WordNet Workshop, 2001.
Cucchiarelli A., Velardi P. Finding a Domain-
Appropriate Sense Inventory for Semantically
Tagging a Corpus. In: Journal of Natural Language
Engineering, 1998
Escudero G., Marquez L., Rigau G. An Empirical
Study of the Domain Dependence of Supervised Word
Sense Disambiguation Systems. In: EMNLP 2000.
Gale W., Church K., Yarowsky D. One Sense per
Discourse. In: Proceedings of the 4th DARPA
Speech and Natural Language Workshop, 1992.
Hearst M., Schtitze H. Customizing a Lexicon to
Better Suit a Computational Task. In: Proceedings
ACL SIGLEX Workshop 1993.
Guthrie J. A., Guthrie I., Wilks Y., Aidinejad H.
Subject Dependent Co-Occurrence and Word Sense
Disambiguation. In: Proceedings of ACL 1991.
Kilgarriff A. Bridging the gap between lexicon
and corpus: convergence of formalisms. In:
Proceedings of LREC Workshop on Adapting
Lexical Resources, 1998.
Krovetz R. More than one sense per discourse.
NEC Research Memorandum, 1998.
Magnini B., Cavaglia G. Integrating Subject
Field Codes into WordNet. In: Proceedings LREC
2000.
Martinez D., Agirre E. One Sense per Collocation
and Genre/Topic Variations. In: Proceedings
EMNLP 2000.
Stevenson M., Wilks Y. Combining Weak
Knowledge Sources for Sense Disambiguation. In:
Proceedings IJCAI 1999.
Turcato D., Popowich F., Toole J., Fass D.,
Nicholson D., Tisher G. Adapting a synonym
database to specific domains. In: Proceedings of the
ACL workshop on recent advances in NLP and IR.
Hong Kong, 2000.
Vossen P. Extending, Trimming and Fusing
WordNet for Technical Documents. In: Proceedings
NAACL WordNet Workshop, 2001.
Walker D., Amsler R. The use of
-machinereadable dictionaries in sublanguage
analysis In: Analyzing Language in Restricted
Domains, 1986.
</reference>
<page confidence="0.998857">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425446">
<title confidence="0.999128">The SENSEVAL-2 Panel on Domains, Topics and Senses</title>
<author confidence="0.998232">Paul Buitelaar</author>
<affiliation confidence="0.67754">DFKI Stuhlsatzenhausweg</affiliation>
<address confidence="0.731832">D-66123 Saarbruecken,</address>
<email confidence="0.782876">paulb@dtki.de</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>D Martinez</author>
</authors>
<title>Enriching very large ontologies using the WWW. In:</title>
<date>2000</date>
<booktitle>Proceedings of the Ontology Learning Workshop ECAI</booktitle>
<marker>Hovy, Martinez, 2000</marker>
<rawString>Agirre E., Ansa 0., Hovy E., Martinez D. Enriching very large ontologies using the WWW. In: Proceedings of the Ontology Learning Workshop ECAI 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martinez</author>
<author>E Hovy</author>
</authors>
<title>Enriching WordNet concepts with topic signatures. In:</title>
<date>2001</date>
<booktitle>Proceedings NAACL WordNet Workshop,</booktitle>
<marker>Martinez, Hovy, 2001</marker>
<rawString>Agirre E., Ansa 0., Martinez D., Hovy E. Enriching WordNet concepts with topic signatures. In: Proceedings NAACL WordNet Workshop, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>Della Rocca M</author>
<author>M-T Pazienza</author>
</authors>
<title>Contextual Word Sense Tuning and Disambiguation.</title>
<date>1997</date>
<journal>Applied Artificial Intelligence,</journal>
<volume>11</volume>
<contexts>
<context position="7138" citStr="Basili et al. 1997" startWordPosition="1159" endWordPosition="1162">entory will be appropriate. &amp;quot;The usual scenario ... has been that the word senses are taken from a general purpose dictionary, ... whereas the material to be disambiguated is ... Wall Street Journal. ... So, the profiles [Signatures, Collocations] ... will be for general English senses according to the WSJ ...&amp;quot; (Kilgarriff 1998). Instead, a general sense inventory needs to be tuned to the domain at hand. This involves selecting only those senses that are most appropriate for the domain, as well as extending the sense inventory with novel words (terms) and novel senses, specific to the domain (Basili et al. 1997; Cucchiarelli and Velardi 1998; Turcato et at. 2000; Buitelaar and Sacaleanu 2001; Vossen 2001). According to the method described in (Cucchiarelli and Velardi 1998), a domain specific sense inventory that is balanced (even distribution of words to senses) and at the right 50 level of abstraction (ambiguity vs. generalization) can be selected automatically given the following criteria: &amp;quot;Generality&amp;quot;, &amp;quot;Discrimination Power&amp;quot;, &amp;quot;Domain Coverage&amp;quot; and &amp;quot;Average Ambiguity.&amp;quot; Applying these criteria in a quantitative way to a general sense inventory (i.e the WordNet hierarchy) and a given domain specifi</context>
</contexts>
<marker>Basili, M, Pazienza, 1997</marker>
<rawString>Basili R., Della Rocca M., Pazienza M.-T, Contextual Word Sense Tuning and Disambiguation. Applied Artificial Intelligence, vol. 11, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>B Sacaleanu</author>
</authors>
<title>Ranking and Selecting Synsets by Domain Relevance. In:</title>
<date>2001</date>
<booktitle>Proceedings NAACL WordNet Workshop,</booktitle>
<contexts>
<context position="7220" citStr="Buitelaar and Sacaleanu 2001" startWordPosition="1171" endWordPosition="1174">rd senses are taken from a general purpose dictionary, ... whereas the material to be disambiguated is ... Wall Street Journal. ... So, the profiles [Signatures, Collocations] ... will be for general English senses according to the WSJ ...&amp;quot; (Kilgarriff 1998). Instead, a general sense inventory needs to be tuned to the domain at hand. This involves selecting only those senses that are most appropriate for the domain, as well as extending the sense inventory with novel words (terms) and novel senses, specific to the domain (Basili et al. 1997; Cucchiarelli and Velardi 1998; Turcato et at. 2000; Buitelaar and Sacaleanu 2001; Vossen 2001). According to the method described in (Cucchiarelli and Velardi 1998), a domain specific sense inventory that is balanced (even distribution of words to senses) and at the right 50 level of abstraction (ambiguity vs. generalization) can be selected automatically given the following criteria: &amp;quot;Generality&amp;quot;, &amp;quot;Discrimination Power&amp;quot;, &amp;quot;Domain Coverage&amp;quot; and &amp;quot;Average Ambiguity.&amp;quot; Applying these criteria in a quantitative way to a general sense inventory (i.e the WordNet hierarchy) and a given domain specific corpus automatically selects a set of relevant categories (i.e. top level synset</context>
<context position="8786" citStr="Buitelaar and Sacaleanu 2001" startWordPosition="1399" endWordPosition="1402">ected: #1 capital &gt; asset &gt; possession #2 support &gt; device &gt; instrumentality #4 document &gt; &gt; written_communication #5 accumulation &gt; asset &gt; possession #6 ancestor &gt; relative &gt; person,individual Senses that are discarded include: #7 soup &gt;... #9 plant_part &gt; #12 lineage,line,line_of_descent &gt; #14 lumber,timber &gt; The method described above uses a top down approach that propagates the domain relevance of certain top level synsets down through the (WordNet) hierarchy. A somewhat different approach would be to assign a domain relevance to each concept (i.e. word sense, synset) from the bottom up (Buitelaar and Sacaleanu 2001). This method determines the domain specific relevance of (WordNet, GermaNet) synsets on the basis of the relevance of their constituent synonyms that co-occur within representative domain corpora. Next to selecting domain relevant concepts from the general sense inventory, novel terms (those not covered by the sense inventory) need to be accounted for also. This includes adding morphological and syntactic variants of known terms (Vossen 2001) as well as extending the inventory with semantically related terms through classification and/or clustering. 3 Panel Discussion In the panel presentatio</context>
</contexts>
<marker>Buitelaar, Sacaleanu, 2001</marker>
<rawString>Buitelaar P., Sacaleanu B. Ranking and Selecting Synsets by Domain Relevance. In: Proceedings NAACL WordNet Workshop, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cucchiarelli</author>
<author>P Velardi</author>
</authors>
<title>Finding a DomainAppropriate Sense Inventory for Semantically Tagging a Corpus. In:</title>
<date>1998</date>
<journal>Journal of Natural Language Engineering,</journal>
<contexts>
<context position="7169" citStr="Cucchiarelli and Velardi 1998" startWordPosition="1163" endWordPosition="1166">priate. &amp;quot;The usual scenario ... has been that the word senses are taken from a general purpose dictionary, ... whereas the material to be disambiguated is ... Wall Street Journal. ... So, the profiles [Signatures, Collocations] ... will be for general English senses according to the WSJ ...&amp;quot; (Kilgarriff 1998). Instead, a general sense inventory needs to be tuned to the domain at hand. This involves selecting only those senses that are most appropriate for the domain, as well as extending the sense inventory with novel words (terms) and novel senses, specific to the domain (Basili et al. 1997; Cucchiarelli and Velardi 1998; Turcato et at. 2000; Buitelaar and Sacaleanu 2001; Vossen 2001). According to the method described in (Cucchiarelli and Velardi 1998), a domain specific sense inventory that is balanced (even distribution of words to senses) and at the right 50 level of abstraction (ambiguity vs. generalization) can be selected automatically given the following criteria: &amp;quot;Generality&amp;quot;, &amp;quot;Discrimination Power&amp;quot;, &amp;quot;Domain Coverage&amp;quot; and &amp;quot;Average Ambiguity.&amp;quot; Applying these criteria in a quantitative way to a general sense inventory (i.e the WordNet hierarchy) and a given domain specific corpus automatically selects </context>
</contexts>
<marker>Cucchiarelli, Velardi, 1998</marker>
<rawString>Cucchiarelli A., Velardi P. Finding a DomainAppropriate Sense Inventory for Semantically Tagging a Corpus. In: Journal of Natural Language Engineering, 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L Marquez</author>
<author>G Rigau</author>
</authors>
<title>An Empirical Study of the Domain Dependence of Supervised Word Sense Disambiguation Systems. In: EMNLP</title>
<date>2000</date>
<booktitle>Proceedings of the 4th DARPA Speech and Natural Language Workshop,</booktitle>
<contexts>
<context position="5948" citStr="Escudero et al. 2000" startWordPosition="965" endWordPosition="968">es that will be used simultaneously throughout one discourse (Krovetz 1998). The main question that remains now is, what exactly constitutes a discourse / subject / topic / domain? We can get closer at answering this question by looking at some empirical sense disambiguation results that involve a variation of topic. More specifically, we can observe some effects of topic variation by training a sense disambiguation system on one topic and applying it to another. For instance, training on Wall Street Journal while testing on SemCor and vice versa shows a degrading of 12% and 19% in precision (Escudero et al. 2000). On the other hand, applying context information (collocations) extracted from Wall Street Journal to a financial text in SemCor shows significantly higher precision than on texts in other domains in SemCor (Martinez and Agirre 2000). These results therefore suggest that a discourse / subject / topic / domain corresponds to a larger or smaller chunk of text (a corpus, a text or a text segment) with a homogeneous distribution of senses and corresponding collocations. 2.3 Tuning But even with a clearly defined domain, it is far from certain that any general sense inventory will be appropriate. </context>
</contexts>
<marker>Escudero, Marquez, Rigau, 2000</marker>
<rawString>Escudero G., Marquez L., Rigau G. An Empirical Study of the Domain Dependence of Supervised Word Sense Disambiguation Systems. In: EMNLP 2000. Gale W., Church K., Yarowsky D. One Sense per Discourse. In: Proceedings of the 4th DARPA Speech and Natural Language Workshop, 1992. Hearst M., Schtitze H. Customizing a Lexicon to Better Suit a Computational Task. In: Proceedings ACL SIGLEX Workshop 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Guthrie</author>
<author>I Guthrie</author>
<author>Y Wilks</author>
<author>H Aidinejad</author>
</authors>
<title>Subject Dependent Co-Occurrence and Word Sense Disambiguation. In:</title>
<date>1991</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="3421" citStr="Guthrie et al. 1991" startWordPosition="527" endWordPosition="530">on words (65%), or selection restrictions (44%). Given these results it seems worthwhile to identify also the semantic space of WordNet synsets more explicitly by the introduction of subject codes (Magnini and Cavaglia 2000). This allows for grouping together synsets across part-of-speech, as in the medical domain 49 (doctor#1, hospital#1; operate#7) and across subhierarchies, as in the sports domain (life_form#1: athlete#1; physical_object#1: game_equipment#1; act#2: sport#1; location#1: playing_field#1). 2.2 Topic Signatures and Variation The topic specific context models as constructed by (Guthrie et al. 1991) can be viewed as &amp;quot;signatures&amp;quot; of the topic in question. Such topic signatures can, however, be constructed even without the use of subject codes by generating them (semi-) automatically from a lexical resource and then validating them on topic specific corpora (Hearst and Schtitze 1993). An extension of this idea is to treat senses, or rather WordNet synsets, as topics for which a signature can be constructed. One approach to this is to retrieve relevant documents through search engines on the web by defining queries for each synset (Agirre et al. 2000, Agirre et al. 2001). For instance, the </context>
</contexts>
<marker>Guthrie, Guthrie, Wilks, Aidinejad, 1991</marker>
<rawString>Guthrie J. A., Guthrie I., Wilks Y., Aidinejad H. Subject Dependent Co-Occurrence and Word Sense Disambiguation. In: Proceedings of ACL 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Bridging the gap between lexicon and corpus: convergence of formalisms. In:</title>
<date>1998</date>
<booktitle>Proceedings of LREC Workshop on Adapting Lexical Resources,</booktitle>
<contexts>
<context position="6850" citStr="Kilgarriff 1998" startWordPosition="1112" endWordPosition="1113"> subject / topic / domain corresponds to a larger or smaller chunk of text (a corpus, a text or a text segment) with a homogeneous distribution of senses and corresponding collocations. 2.3 Tuning But even with a clearly defined domain, it is far from certain that any general sense inventory will be appropriate. &amp;quot;The usual scenario ... has been that the word senses are taken from a general purpose dictionary, ... whereas the material to be disambiguated is ... Wall Street Journal. ... So, the profiles [Signatures, Collocations] ... will be for general English senses according to the WSJ ...&amp;quot; (Kilgarriff 1998). Instead, a general sense inventory needs to be tuned to the domain at hand. This involves selecting only those senses that are most appropriate for the domain, as well as extending the sense inventory with novel words (terms) and novel senses, specific to the domain (Basili et al. 1997; Cucchiarelli and Velardi 1998; Turcato et at. 2000; Buitelaar and Sacaleanu 2001; Vossen 2001). According to the method described in (Cucchiarelli and Velardi 1998), a domain specific sense inventory that is balanced (even distribution of words to senses) and at the right 50 level of abstraction (ambiguity vs</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Kilgarriff A. Bridging the gap between lexicon and corpus: convergence of formalisms. In: Proceedings of LREC Workshop on Adapting Lexical Resources, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
</authors>
<title>More than one sense per discourse.</title>
<date>1998</date>
<journal>NEC Research Memorandum,</journal>
<contexts>
<context position="5402" citStr="Krovetz 1998" startWordPosition="876" endWordPosition="877">aw domain and in the syntactic sense in the linguistics domain. However, for words with related senses (i.e in the case of systematic polysemy) the topic signatures will overlap, as with the results on boy in sense 1: young male person and sense 3: son. This has been shown also from a somewhat different viewpoint in reaction to (Gale et al. 1992), in which it was stated that one sense will be uniquely used within a discourse (which we can equate with a topic or domain for our purposes here). Instead, many words have overlapping senses that will be used simultaneously throughout one discourse (Krovetz 1998). The main question that remains now is, what exactly constitutes a discourse / subject / topic / domain? We can get closer at answering this question by looking at some empirical sense disambiguation results that involve a variation of topic. More specifically, we can observe some effects of topic variation by training a sense disambiguation system on one topic and applying it to another. For instance, training on Wall Street Journal while testing on SemCor and vice versa shows a degrading of 12% and 19% in precision (Escudero et al. 2000). On the other hand, applying context information (col</context>
</contexts>
<marker>Krovetz, 1998</marker>
<rawString>Krovetz R. More than one sense per discourse. NEC Research Memorandum, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavaglia</author>
</authors>
<title>Integrating Subject Field Codes into WordNet. In:</title>
<date>2000</date>
<booktitle>Proceedings LREC</booktitle>
<contexts>
<context position="3025" citStr="Magnini and Cavaglia 2000" startWordPosition="476" endWordPosition="479">ne product hold origin place human treatment blood hospital use store organ comb Table 2: Medical neighborhood of bank Using subject codes in sense disambiguation has been shown to be fruitful, relative to using other sources of knowledge. As reported in (Stevenson and Wilks 1999), the performance of using only subject codes (79% precision) was much better than that of using only dictionary definition words (65%), or selection restrictions (44%). Given these results it seems worthwhile to identify also the semantic space of WordNet synsets more explicitly by the introduction of subject codes (Magnini and Cavaglia 2000). This allows for grouping together synsets across part-of-speech, as in the medical domain 49 (doctor#1, hospital#1; operate#7) and across subhierarchies, as in the sports domain (life_form#1: athlete#1; physical_object#1: game_equipment#1; act#2: sport#1; location#1: playing_field#1). 2.2 Topic Signatures and Variation The topic specific context models as constructed by (Guthrie et al. 1991) can be viewed as &amp;quot;signatures&amp;quot; of the topic in question. Such topic signatures can, however, be constructed even without the use of subject codes by generating them (semi-) automatically from a lexical re</context>
</contexts>
<marker>Magnini, Cavaglia, 2000</marker>
<rawString>Magnini B., Cavaglia G. Integrating Subject Field Codes into WordNet. In: Proceedings LREC 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martinez</author>
<author>E Agirre</author>
</authors>
<title>One Sense per Collocation and Genre/Topic Variations. In:</title>
<date>2000</date>
<booktitle>Proceedings EMNLP</booktitle>
<contexts>
<context position="6182" citStr="Martinez and Agirre 2000" startWordPosition="1000" endWordPosition="1003">ooking at some empirical sense disambiguation results that involve a variation of topic. More specifically, we can observe some effects of topic variation by training a sense disambiguation system on one topic and applying it to another. For instance, training on Wall Street Journal while testing on SemCor and vice versa shows a degrading of 12% and 19% in precision (Escudero et al. 2000). On the other hand, applying context information (collocations) extracted from Wall Street Journal to a financial text in SemCor shows significantly higher precision than on texts in other domains in SemCor (Martinez and Agirre 2000). These results therefore suggest that a discourse / subject / topic / domain corresponds to a larger or smaller chunk of text (a corpus, a text or a text segment) with a homogeneous distribution of senses and corresponding collocations. 2.3 Tuning But even with a clearly defined domain, it is far from certain that any general sense inventory will be appropriate. &amp;quot;The usual scenario ... has been that the word senses are taken from a general purpose dictionary, ... whereas the material to be disambiguated is ... Wall Street Journal. ... So, the profiles [Signatures, Collocations] ... will be fo</context>
</contexts>
<marker>Martinez, Agirre, 2000</marker>
<rawString>Martinez D., Agirre E. One Sense per Collocation and Genre/Topic Variations. In: Proceedings EMNLP 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>Y Wilks</author>
</authors>
<title>Combining Weak Knowledge Sources for Sense Disambiguation. In:</title>
<date>1999</date>
<booktitle>Proceedings IJCAI</booktitle>
<contexts>
<context position="2680" citStr="Stevenson and Wilks 1999" startWordPosition="424" endWordPosition="427">words in the definitions and in sample sentences of all words in the dictionary that share the same subject code. For instance, the word bank has the following neighborhoods for the financial and medical domains: write safe sum account person put take money order keep pay supply paper draw cheque Table 1: Financial neighborhood of bank medicine product hold origin place human treatment blood hospital use store organ comb Table 2: Medical neighborhood of bank Using subject codes in sense disambiguation has been shown to be fruitful, relative to using other sources of knowledge. As reported in (Stevenson and Wilks 1999), the performance of using only subject codes (79% precision) was much better than that of using only dictionary definition words (65%), or selection restrictions (44%). Given these results it seems worthwhile to identify also the semantic space of WordNet synsets more explicitly by the introduction of subject codes (Magnini and Cavaglia 2000). This allows for grouping together synsets across part-of-speech, as in the medical domain 49 (doctor#1, hospital#1; operate#7) and across subhierarchies, as in the sports domain (life_form#1: athlete#1; physical_object#1: game_equipment#1; act#2: sport#</context>
</contexts>
<marker>Stevenson, Wilks, 1999</marker>
<rawString>Stevenson M., Wilks Y. Combining Weak Knowledge Sources for Sense Disambiguation. In: Proceedings IJCAI 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Turcato</author>
<author>F Popowich</author>
<author>J Toole</author>
<author>D Fass</author>
<author>D Nicholson</author>
<author>G Tisher</author>
</authors>
<title>Adapting a synonym database to specific domains. In:</title>
<date>2000</date>
<booktitle>Proceedings of the ACL workshop on recent advances in NLP</booktitle>
<marker>Turcato, Popowich, Toole, Fass, Nicholson, Tisher, 2000</marker>
<rawString>Turcato D., Popowich F., Toole J., Fass D., Nicholson D., Tisher G. Adapting a synonym database to specific domains. In: Proceedings of the ACL workshop on recent advances in NLP and IR. Hong Kong, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vossen P Extending</author>
</authors>
<title>Trimming and Fusing WordNet for Technical Documents. In:</title>
<date>2001</date>
<booktitle>Proceedings NAACL WordNet Workshop,</booktitle>
<marker>Extending, 2001</marker>
<rawString>Vossen P. Extending, Trimming and Fusing WordNet for Technical Documents. In: Proceedings NAACL WordNet Workshop, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Walker</author>
<author>R Amsler</author>
</authors>
<title>The use of -machinereadable dictionaries in sublanguage analysis In: Analyzing Language in Restricted Domains,</title>
<date>1986</date>
<contexts>
<context position="1840" citStr="Walker and Amsler 1986" startWordPosition="290" endWordPosition="293">ns, Topics and Senses 2.1 Subject Codes A semantic space may be indicated in a dictionary by use of a so-called &amp;quot;subject code&amp;quot;. In LDOCE for instance, subject codes like MD, for the medical domain, or ML, for meteorology are used to define which senses of a word are used in which domains. Three of the senses of the word high for instance correspond to three different domains: music (a high tone), drugs (the experience of being high) and meteorology (a high pressure area). Subject codes can be used to detect the topic of a text segment by simply counting their frequency over all content words (Walker and Amsler 1986). At the same time, however, subject codes can be used in sense disambiguation by constructing topic specific context models (Guthrie et. al 1991). Such &amp;quot;neighborhoods&amp;quot; can be constructed by taking into account all words in the definitions and in sample sentences of all words in the dictionary that share the same subject code. For instance, the word bank has the following neighborhoods for the financial and medical domains: write safe sum account person put take money order keep pay supply paper draw cheque Table 1: Financial neighborhood of bank medicine product hold origin place human treatm</context>
</contexts>
<marker>Walker, Amsler, 1986</marker>
<rawString>Walker D., Amsler R. The use of -machinereadable dictionaries in sublanguage analysis In: Analyzing Language in Restricted Domains, 1986.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>