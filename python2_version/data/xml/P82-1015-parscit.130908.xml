<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027663">
<title confidence="0.6045855">
Experience with an Easily Computed Metric
for Ranking Alternative Parses
</title>
<author confidence="0.563131">
George E. Heidorn
</author>
<affiliation confidence="0.389197">
Computer Sciences Department
IBM Thomas J. Watson Research Center
Yorktown Heights, New York 10598
</affiliation>
<sectionHeader confidence="0.858189" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846">
This brief paper, which is itself an extended abstract for a
forthcoming paper, describes a metric that can be easily com-
puted during either bottom-up or top-down construction of a
parse tree for ranking the desirability of alternative parses. In
its simplest form, the metric tends to prefer trees in which
constituents are pushed as far down as possible, but by appro-
priate modification of a constant in the formula other behavior
can be obtained also. This paper includes an introduction to
the EPISTLE system being developed at IBM Research and a
discussion of the results of using this metric with that system.
</bodyText>
<sectionHeader confidence="0.812417" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999906619047619">
Heidorn (1976) described a technique for computing a
number for each node during the bottom-up construction of a
parse tree, such that a node with a smaller number is to be
preferred to a node with a larger number covering the same
portion of text. At the time, this scheme was used primarily to
select among competing noun phrases in queries to a program
explanation system. Although it appeared to work well, it was
not extensively tested. Recently, as part of our research on
the EPISTLE system, this idea has been modified and extend-
ed to work over entire sentences and to provide for top-down
computation. Also, we have done an analysis of 80 sentences
with multiple parses from our data base to evaluate the per-
formance of this metric, and have found that it is producing
very good results.
This brief paper, which is actually an extended abstract
for a forthcoming paper, begins with an introduction to the
EPISTLE system, to set the stage for the current application of
this metric. Then the metric&apos;s computation is described, fol-
lowed by a discussion of the results of the 80-sentence analy-
sis. Finally, some comparisons are made to related work by
others.
</bodyText>
<subsectionHeader confidence="0.991257">
The EPISTLE System
</subsectionHeader>
<bodyText confidence="0.999963848484848">
In its current form, the EPISTLE system (Miller, Heidorn
and Jensen 1981) is intended to do critiquing of a writer&apos;s use
of English in business correspondence, and can do some
amount of grammar and style checking. The central compo-
nent of the system is a parser for assigning grammatical struc-
tures to input sentences. This is done with NLP, a LISP-based
natural language processing system which uses augmented
phrase structure grammar (APSG) rules (Heidorn 1975) to
specify how text is to be converted into a network of nodes
consisting of attribute-value pairs and how such a network can
be converted into text. The first process, decoding, is done in
a bottom-up, parallel processing fashion, and the inverse proc-
ess, encoding, is done in a top-down, serial manner. In the
current application the network which is constructed is simply
a decorated parse tree, rather than a meaning representation.
Because EPISTLE must deal with unrestricted input (both
in terms of vocabulary and syntactic constructions), we are
trying to see how far we can get initially with almost no se-
mantic information. In particular, our information about
words is pretty much limited to parts-of-speech that come from
an on-line version of a standard dictionary of over 100,000
entries, and the conditions in our 250 decoding rules are based
primarily on syntactic cues. We strive for what we call a
unique approximate parse for each sentence, a parse that is not
necessarily semantically accurate (e.g., prepositional phrase
attachments are not always done right) but one which is ade-
quate for the text critiquing tasks, nevertheless.
One of the things we do periodically to test the perform-
ance of our parsing component is to run it on a set of 400
actual business letters, consisting of almost 2,300 sentences
which range in length up to 63 words, averaging 19 words per
sentence. In two recent runs of this data base, the following
results were obtained:
</bodyText>
<table confidence="0.7796348">
No. of parses June 1981 Dec. 1981
0 57% 36%
1 31% 41%
2 6% 11%
&gt;2 6% 12%
</table>
<bodyText confidence="0.998271666666667">
The improvement in performance from June to December
can be attributed both to writing additional grammar rules and
to relaxing overly restrictive conditions in other rules. It can
be seen that this not only had the desirable effect of reducing
the percentage of no-parse sentences (from 57% to 36%) and
increasing the percentage of single-parse sentences (from 31%
to 41%), but it also had the undesirable side effect of incrm
ing the multiple-parse sentences (from 12% to 23%). Be-
cause we expect th*, .1,,..41.ion to continue as we further in-
crease our grammatical coverage, the need for a method of
ranking multiple parses in order to select the best one on
which to base our grammar and style critiques is acutely felt.
</bodyText>
<page confidence="0.992543">
82
</page>
<subsectionHeader confidence="0.618608">
The Metric and Its Computation
</subsectionHeader>
<bodyText confidence="0.994777">
The metric can be stated by the following recursive for-
mula:
</bodyText>
<equation confidence="0.751152">
ScorePhrase = KMod(ScoreMod+1)
Mods
</equation>
<bodyText confidence="0.999976287671233">
where the lowest score is considered to be the best. This for-
mula says that the score associated with a phrase is equal to
the sum of the scores of the modifying phrases of that phrase
adjusted in a particular way, namely that the score of each
modifier is increased by 1 and then multiplied by a constant K
appropriate for that type of modifier. A phrase with no modi-
fiers, such as an individual word, has a score of 0. This metric
is based on a flat view of syntactic structure which says that
each phrase consists of a head word and zero or more pre- and
post-modifying phrases. (In this view a sentence is just a big
verb phrase, with modifiers such as subject, objects, adverbs,
and subordinate clauses.)
In its simplest form this metric can be considered to be
nothing more than the numerical realization of Kimball&apos;s Prin-
ciple Number Two (Kimball 1972): &amp;quot;Terminal symbols opti-
mally associate to the lowest nonterminal node.&amp;quot; (Although
Kimball calls this principle right association and illustrates it
with right-branching examples, it can often apply equally well
to left-branching structures.) One way to achieve this simplest
form is to use a K of 0.1 for all types of modifiers.
An example of the application of the metric in this sim-
plest form is given in Figure 1. Two parse trees are shown for
the sentence, &amp;quot;See the man with the telescope,&amp;quot; with a score
attached to each node (other than those that are zero). A
node marked with an asterisk is the head of its respective
phrase. In this form of flat parse tree a prepositional phrase is
displayed as a noun phrase with the preposition as an addition-
al premodifier. As an example of the calculation, the score of
the PP here is computed as 0.1(0+1)+0.1(0+1), because the
scores of its modifiers — the ADJ and the PREP — are each
0. Similarly, the score of the NP in the second parse tree is
computed as 0.1(0+1)+0.1(0.2+1), where the 0.2 within it is
the score of the PP.
It can be seen from the example that in this simplest form
the individual digits of the score after the decimal point tell
how many modifiers appear at each level in the phrase (as long
as there are no more than nine modifiers at any level). The
farther down in the parse tree a constituent is pushed, the
farther to the right in the final score its contribution will ap-
pear. Hence, a deeper structure will tend to have a smaller
score than a shallower structure, and, therefore, be preferred.
In the example, this is the second tree, with a score of 0.122
vs. 0.23. That is not to say that this would be the semantically
correct tree for this sentence in all contexts, but only that if a
choice cannot be made on any other grounds, this tree is to be
preferred.
Applying the metric in its simplest form does not produce
the desired result for all grammatical constructions, so that
values for K other than 0.1 must be used for some types of
modifiers. It basically boils down to a system of rewards and
penalties to make the metric reflect preferences determined
heuristically. For example, the preference that a potential
auxiliary verb is to be used as an auxiliary rather than as a
main verb when both parses are possible can be realized by
using a K of 0, a reward, when picking up an auxiliary verb.
Similarly, a K of 2, a penalty, can be used to increase the score
(thereby lessening the preference) when attaching an adverbial
phrase as a premodifier in a lower level clause (rather than as
a postmodifier in a higher level clause). When semantic infor-
mation is available, it can be used to select appropriate values
for K, too, such as using 100 for an anomalous combination.
Straightforward application of the formula given above
implies that the computation of the score can be done in a
bottom-up fashion, as the modifiers of each phrase are picked
up. However, it can also be done in a top-down manner after
doing a little bit of algebra on the formula to expand it and
regroup the terms. In the EPISTLE application it is the latter
approach that is being used. There is actually a set of ten
NLP encoding rules that do the computation in a downward
traversal of a completed parse tree, determining the appropri-
ate constant to use at each node. The top-down method of
computation could be done during top-down parsing of the
sort typically used with ATN&apos;s, also.
</bodyText>
<table confidence="0.987822333333333">
SENT(0.23)1---- VERB* &amp;quot;SEE&amp;quot;
NP(0.1)1 ADJ &amp;quot;THE&amp;quot;
NOUN* &amp;quot;MAN&amp;quot;
PP(0.2)I---- PREP &amp;quot;WITH&amp;quot;
ADJ &amp;quot;THE&amp;quot;
NOUN* &amp;quot;TELESCOPE&amp;quot;
SENT(0.122)1--- VERB* &amp;quot;SEE&amp;quot;
NP(0.22)1 ADJ &amp;quot;THE&amp;quot;
NOUN* &amp;quot;MAN&amp;quot;
PP(0.2)1--- PREP &amp;quot;WITH&amp;quot;
ADJ &amp;quot;THE&amp;quot;
1--- NOUN* &amp;quot;TELESCOPE&amp;quot;
</table>
<figureCaption confidence="0.98872">
Figure 1. Two alternative parses with their scores.
</figureCaption>
<page confidence="0.995575">
83
</page>
<subsectionHeader confidence="0.879103">
Performance of the Metric
</subsectionHeader>
<bodyText confidence="0.999984714285714">
To test the performance of the metric in our EPISTLE
application, the parse trees of 80 multiple-parse sentences were
analyzed to determine if the metric favored what we consid-
ered to be the best tree for our purposes. A raw calculation
said it was right in 65% of the cases. However, further analy-
sis of those cases where it was wrong showed that in half of
them the parse that it favored was one which will not even be
produced when we further refine our grammar rules. If we
eliminate these from consideration, our success rate increases
to 80%. Out of the remaining &amp;quot;failures,&amp;quot; more than half are
cases where semantic information is required to make the
correct choice, and our system simply does not yet have
enough such information to deal with these. The others, about
7%, will require further tuning of the constant K in the for-
mula. (In fact, they all seem to involve VP conjunction, for
which the metric has not been tuned at all yet.)
The analysis just described was based on multiple parses
of order 2 through 6. Another analysis was done separately on
the double parses (i.e. order 2). The results were similar, but
with an adjusted success rate of 85%, and with almost all of
the remainder due to the need for more semantic information.
It is also of interest to note that significant right-
branching occurred in about 75% of the cases for which the
metric selected the best parse. Most of these were situations
in which the grammar rules would allow a constituent to be
attached at more than one level, but simply pushing it down to
the lowest possible level with the metric turned out to produce
the best parse.
</bodyText>
<subsectionHeader confidence="0.855635">
Related Research
</subsectionHeader>
<bodyText confidence="0.99997059375">
There has not been much in the literature about using
numerical scores to rank alternative analyses of segments of
text. One notable exception to this is the work at SRI (e.g.,
Paxton 1975 and Robinson 1975, 1980), where factor
statements may be attached to an APSG rule to aid in the
calculation of a score for a phrase formed by applying the rule.
The score of a phrase is intended to express the likelihood that
the phrase is a correct interpretation of the input. These
scores apparently can be integers in the range 0 to 100 or
symbols such as GOOD or POOR. This method of scoring
phrases provides more flexibility than the metric of this paper,
but also puts more of a burden on the grammar writer.
Another place in which scoring played an important role is
the syntactic component of the BBN SPEECHLIS system
(Bates 1976), where ,an integer score is assigned to each
configuration during the processing of a sentence to reflect the
likelihood that the path which terminates on that configuration
is correct. The grammar writer must assign weights to each arc
of the ATN grammar, but the rest of the computation appears
to be done by the system, utilizing such information as the
number of words in a constituent. Although this scoring
mechanism worked very well for its intended purpose, it may
not be more generally applicable.
A very specialized scoring scheme was used in the
JIMMY3 system (Maxwell and Tuggle 1977), where each
parse network is given an integer score calculated by rewarding
the finding of the actor, object, modifiers, and prepositional
phrases and punishing the ignoring of words and terms. Final-
ly, there is Wilks&apos; counting of dependencies to find the analysis
with the greatest semantic density in his Preference Semantics
work (eg., Wilks 1975). Neither of these purports to propose
scoring methods that are more generally applicable, either.
</bodyText>
<sectionHeader confidence="0.990745" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99967925">
I would like to thank Karen Jensen, Martin Chodorow and
Lance Miller for the help that they have given me in the devel-
opment and testing of this parsing metric, and John Sowa for
his comments on an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.996714" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999772178571429">
Bates, M. 1976. &amp;quot;Syntax in Automatic Speech Understanding&amp;quot;
Am. J. Comp. Ling. Microfiche 45.
Heidorn, G.E. 1975. &amp;quot;Augmented Phrase Structure Gram-
mars&amp;quot; Theoretical Issues in Natural Language Processing,
B.L. Webber and R.C. Schank (Eds.), Assoc. for Comp.
Ling., June 1975, 1-5.
Heidorn, G.E. 1976. &amp;quot;An Easily Computed Metric for Rank-
ing Alternative Parses&amp;quot; Presented at the Fourteenth Annual
Meeting of the Assoc. for Comp. Ling., San Francisco, Octo-
ber 1976.
Kimball, J. 1972. &amp;quot;Seven Principles of Surface Structure Pars-
ing in Natural Language&amp;quot; Cognition 2, 1, 15-47.
Maxwell, B.D. and F.D. Tuggle 1977. &amp;quot;Toward a &apos;Natural&apos;
Language Question-Answering Facility&amp;quot; Am. J. Comp. Ling.
Microfiche 61.
Miller, L.A., G.E. Heidorn and K. Jensen 1981. &amp;quot;Text-
Critiquing with the EPISTLE System: An Author&apos;s Aid to
Better Syntax&amp;quot; AFIPS - Conference Proceedings, Vol. 50,
May 1981, 649-655.
Paxton, W.H. 1975. &amp;quot;The Definition System&amp;quot; in Speech Un-
derstanding Research, SRI Annual Technical Report, June
1975, 20-25.
Robinson, J.J. 1975. &amp;quot;A Tuneable Performance Grammar&amp;quot;
Am. J. Comp. Ling., Microfiche 34, 19-33.
Robinson, J.J. 1980. &amp;quot;DIAGRAM: A Grammar for Dia-
logues&amp;quot; SRI Technical Note 205, Feb. 1980.
Wilks, Y. 1975. &amp;quot;An Intelligent Analyzer and Understander of
English&amp;quot; Comm. ACM 18, 5 (May 1975), 264-274.
</reference>
<page confidence="0.99924">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.703549">
<title confidence="0.9982245">Experience with an Easily Computed Metric for Ranking Alternative Parses</title>
<author confidence="0.999996">George E Heidorn</author>
<affiliation confidence="0.999345">Computer Sciences Department IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.971681">Yorktown Heights, New York 10598</address>
<abstract confidence="0.999575857142857">This brief paper, which is itself an extended abstract for a forthcoming paper, describes a metric that can be easily computed during either bottom-up or top-down construction of a parse tree for ranking the desirability of alternative parses. In its simplest form, the metric tends to prefer trees in which constituents are pushed as far down as possible, but by appro-</abstract>
<intro confidence="0.72928">priate modification of a constant in the formula other behavior</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bates</author>
</authors>
<title>Syntax in Automatic Speech Understanding&amp;quot;</title>
<date>1976</date>
<journal>Am. J. Comp. Ling. Microfiche</journal>
<volume>45</volume>
<contexts>
<context position="11972" citStr="Bates 1976" startWordPosition="2075" endWordPosition="2076">ere factor statements may be attached to an APSG rule to aid in the calculation of a score for a phrase formed by applying the rule. The score of a phrase is intended to express the likelihood that the phrase is a correct interpretation of the input. These scores apparently can be integers in the range 0 to 100 or symbols such as GOOD or POOR. This method of scoring phrases provides more flexibility than the metric of this paper, but also puts more of a burden on the grammar writer. Another place in which scoring played an important role is the syntactic component of the BBN SPEECHLIS system (Bates 1976), where ,an integer score is assigned to each configuration during the processing of a sentence to reflect the likelihood that the path which terminates on that configuration is correct. The grammar writer must assign weights to each arc of the ATN grammar, but the rest of the computation appears to be done by the system, utilizing such information as the number of words in a constituent. Although this scoring mechanism worked very well for its intended purpose, it may not be more generally applicable. A very specialized scoring scheme was used in the JIMMY3 system (Maxwell and Tuggle 1977), w</context>
</contexts>
<marker>Bates, 1976</marker>
<rawString>Bates, M. 1976. &amp;quot;Syntax in Automatic Speech Understanding&amp;quot; Am. J. Comp. Ling. Microfiche 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Augmented Phrase Structure Grammars&amp;quot; Theoretical Issues</title>
<date>1975</date>
<booktitle>in Natural Language Processing, B.L. Webber and R.C. Schank (Eds.), Assoc. for Comp. Ling.,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="2476" citStr="Heidorn 1975" startWordPosition="409" endWordPosition="410">followed by a discussion of the results of the 80-sentence analysis. Finally, some comparisons are made to related work by others. The EPISTLE System In its current form, the EPISTLE system (Miller, Heidorn and Jensen 1981) is intended to do critiquing of a writer&apos;s use of English in business correspondence, and can do some amount of grammar and style checking. The central component of the system is a parser for assigning grammatical structures to input sentences. This is done with NLP, a LISP-based natural language processing system which uses augmented phrase structure grammar (APSG) rules (Heidorn 1975) to specify how text is to be converted into a network of nodes consisting of attribute-value pairs and how such a network can be converted into text. The first process, decoding, is done in a bottom-up, parallel processing fashion, and the inverse process, encoding, is done in a top-down, serial manner. In the current application the network which is constructed is simply a decorated parse tree, rather than a meaning representation. Because EPISTLE must deal with unrestricted input (both in terms of vocabulary and syntactic constructions), we are trying to see how far we can get initially wit</context>
</contexts>
<marker>Heidorn, 1975</marker>
<rawString>Heidorn, G.E. 1975. &amp;quot;Augmented Phrase Structure Grammars&amp;quot; Theoretical Issues in Natural Language Processing, B.L. Webber and R.C. Schank (Eds.), Assoc. for Comp. Ling., June 1975, 1-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>An Easily Computed Metric for Ranking Alternative Parses&amp;quot; Presented at the Fourteenth Annual Meeting of the Assoc. for Comp. Ling.,</title>
<date>1976</date>
<location>San Francisco,</location>
<contexts>
<context position="842" citStr="Heidorn (1976)" startWordPosition="132" endWordPosition="133">self an extended abstract for a forthcoming paper, describes a metric that can be easily computed during either bottom-up or top-down construction of a parse tree for ranking the desirability of alternative parses. In its simplest form, the metric tends to prefer trees in which constituents are pushed as far down as possible, but by appropriate modification of a constant in the formula other behavior can be obtained also. This paper includes an introduction to the EPISTLE system being developed at IBM Research and a discussion of the results of using this metric with that system. Introduction Heidorn (1976) described a technique for computing a number for each node during the bottom-up construction of a parse tree, such that a node with a smaller number is to be preferred to a node with a larger number covering the same portion of text. At the time, this scheme was used primarily to select among competing noun phrases in queries to a program explanation system. Although it appeared to work well, it was not extensively tested. Recently, as part of our research on the EPISTLE system, this idea has been modified and extended to work over entire sentences and to provide for top-down computation. Als</context>
</contexts>
<marker>Heidorn, 1976</marker>
<rawString>Heidorn, G.E. 1976. &amp;quot;An Easily Computed Metric for Ranking Alternative Parses&amp;quot; Presented at the Fourteenth Annual Meeting of the Assoc. for Comp. Ling., San Francisco, October 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<title>Seven Principles of Surface Structure Parsing in Natural Language&amp;quot;</title>
<date>1972</date>
<journal>Cognition</journal>
<volume>2</volume>
<pages>15--47</pages>
<contexts>
<context position="5738" citStr="Kimball 1972" startWordPosition="972" endWordPosition="973">odifier is increased by 1 and then multiplied by a constant K appropriate for that type of modifier. A phrase with no modifiers, such as an individual word, has a score of 0. This metric is based on a flat view of syntactic structure which says that each phrase consists of a head word and zero or more pre- and post-modifying phrases. (In this view a sentence is just a big verb phrase, with modifiers such as subject, objects, adverbs, and subordinate clauses.) In its simplest form this metric can be considered to be nothing more than the numerical realization of Kimball&apos;s Principle Number Two (Kimball 1972): &amp;quot;Terminal symbols optimally associate to the lowest nonterminal node.&amp;quot; (Although Kimball calls this principle right association and illustrates it with right-branching examples, it can often apply equally well to left-branching structures.) One way to achieve this simplest form is to use a K of 0.1 for all types of modifiers. An example of the application of the metric in this simplest form is given in Figure 1. Two parse trees are shown for the sentence, &amp;quot;See the man with the telescope,&amp;quot; with a score attached to each node (other than those that are zero). A node marked with an asterisk is t</context>
</contexts>
<marker>Kimball, 1972</marker>
<rawString>Kimball, J. 1972. &amp;quot;Seven Principles of Surface Structure Parsing in Natural Language&amp;quot; Cognition 2, 1, 15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B D Maxwell</author>
<author>F D Tuggle</author>
</authors>
<title>Toward a &apos;Natural&apos; Language Question-Answering Facility&amp;quot;</title>
<date>1977</date>
<journal>Am. J. Comp. Ling. Microfiche</journal>
<volume>61</volume>
<contexts>
<context position="12569" citStr="Maxwell and Tuggle 1977" startWordPosition="2172" endWordPosition="2175">EECHLIS system (Bates 1976), where ,an integer score is assigned to each configuration during the processing of a sentence to reflect the likelihood that the path which terminates on that configuration is correct. The grammar writer must assign weights to each arc of the ATN grammar, but the rest of the computation appears to be done by the system, utilizing such information as the number of words in a constituent. Although this scoring mechanism worked very well for its intended purpose, it may not be more generally applicable. A very specialized scoring scheme was used in the JIMMY3 system (Maxwell and Tuggle 1977), where each parse network is given an integer score calculated by rewarding the finding of the actor, object, modifiers, and prepositional phrases and punishing the ignoring of words and terms. Finally, there is Wilks&apos; counting of dependencies to find the analysis with the greatest semantic density in his Preference Semantics work (eg., Wilks 1975). Neither of these purports to propose scoring methods that are more generally applicable, either. Acknowledgements I would like to thank Karen Jensen, Martin Chodorow and Lance Miller for the help that they have given me in the development and test</context>
</contexts>
<marker>Maxwell, Tuggle, 1977</marker>
<rawString>Maxwell, B.D. and F.D. Tuggle 1977. &amp;quot;Toward a &apos;Natural&apos; Language Question-Answering Facility&amp;quot; Am. J. Comp. Ling. Microfiche 61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Miller</author>
<author>G E Heidorn</author>
<author>K Jensen</author>
</authors>
<title>TextCritiquing with the EPISTLE System: An Author&apos;s Aid to Better Syntax&amp;quot;</title>
<date>1981</date>
<booktitle>AFIPS - Conference Proceedings,</booktitle>
<volume>50</volume>
<pages>649--655</pages>
<marker>Miller, Heidorn, Jensen, 1981</marker>
<rawString>Miller, L.A., G.E. Heidorn and K. Jensen 1981. &amp;quot;TextCritiquing with the EPISTLE System: An Author&apos;s Aid to Better Syntax&amp;quot; AFIPS - Conference Proceedings, Vol. 50, May 1981, 649-655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Paxton</author>
</authors>
<title>The Definition System&amp;quot;</title>
<date>1975</date>
<booktitle>in Speech Understanding Research, SRI Annual Technical Report,</booktitle>
<contexts>
<context position="11332" citStr="Paxton 1975" startWordPosition="1960" endWordPosition="1961">or more semantic information. It is also of interest to note that significant rightbranching occurred in about 75% of the cases for which the metric selected the best parse. Most of these were situations in which the grammar rules would allow a constituent to be attached at more than one level, but simply pushing it down to the lowest possible level with the metric turned out to produce the best parse. Related Research There has not been much in the literature about using numerical scores to rank alternative analyses of segments of text. One notable exception to this is the work at SRI (e.g., Paxton 1975 and Robinson 1975, 1980), where factor statements may be attached to an APSG rule to aid in the calculation of a score for a phrase formed by applying the rule. The score of a phrase is intended to express the likelihood that the phrase is a correct interpretation of the input. These scores apparently can be integers in the range 0 to 100 or symbols such as GOOD or POOR. This method of scoring phrases provides more flexibility than the metric of this paper, but also puts more of a burden on the grammar writer. Another place in which scoring played an important role is the syntactic component </context>
</contexts>
<marker>Paxton, 1975</marker>
<rawString>Paxton, W.H. 1975. &amp;quot;The Definition System&amp;quot; in Speech Understanding Research, SRI Annual Technical Report, June 1975, 20-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Robinson</author>
</authors>
<title>A Tuneable Performance Grammar&amp;quot;</title>
<date>1975</date>
<journal>Am. J. Comp. Ling., Microfiche</journal>
<volume>34</volume>
<contexts>
<context position="11350" citStr="Robinson 1975" startWordPosition="1963" endWordPosition="1964"> information. It is also of interest to note that significant rightbranching occurred in about 75% of the cases for which the metric selected the best parse. Most of these were situations in which the grammar rules would allow a constituent to be attached at more than one level, but simply pushing it down to the lowest possible level with the metric turned out to produce the best parse. Related Research There has not been much in the literature about using numerical scores to rank alternative analyses of segments of text. One notable exception to this is the work at SRI (e.g., Paxton 1975 and Robinson 1975, 1980), where factor statements may be attached to an APSG rule to aid in the calculation of a score for a phrase formed by applying the rule. The score of a phrase is intended to express the likelihood that the phrase is a correct interpretation of the input. These scores apparently can be integers in the range 0 to 100 or symbols such as GOOD or POOR. This method of scoring phrases provides more flexibility than the metric of this paper, but also puts more of a burden on the grammar writer. Another place in which scoring played an important role is the syntactic component of the BBN SPEECHL</context>
</contexts>
<marker>Robinson, 1975</marker>
<rawString>Robinson, J.J. 1975. &amp;quot;A Tuneable Performance Grammar&amp;quot; Am. J. Comp. Ling., Microfiche 34, 19-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Robinson</author>
</authors>
<title>DIAGRAM: A Grammar for Dialogues&amp;quot;</title>
<date>1980</date>
<tech>SRI Technical Note 205,</tech>
<marker>Robinson, 1980</marker>
<rawString>Robinson, J.J. 1980. &amp;quot;DIAGRAM: A Grammar for Dialogues&amp;quot; SRI Technical Note 205, Feb. 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>An Intelligent Analyzer and Understander of English&amp;quot;</title>
<date>1975</date>
<journal>Comm. ACM</journal>
<volume>18</volume>
<pages>264--274</pages>
<marker>Wilks, 1975</marker>
<rawString>Wilks, Y. 1975. &amp;quot;An Intelligent Analyzer and Understander of English&amp;quot; Comm. ACM 18, 5 (May 1975), 264-274.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>