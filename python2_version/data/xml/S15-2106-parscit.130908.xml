<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006696">
<title confidence="0.973686">
SWATAC: A Sentiment Analyzer using One-Vs-Rest Logistic Regression
</title>
<author confidence="0.970599">
Yousef Alhessi and Richard Wicentowski
</author>
<affiliation confidence="0.7099265">
Swarthmore College
Swarthmore, PA 19081 USA
</affiliation>
<email confidence="0.995741">
{yalhess1, richardw}@cs.swarthmore.edu
</email>
<sectionHeader confidence="0.99554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999515294117647">
This paper describes SWATAC, a system
built for SemEval-2015’s Task 10 Subtask B,
namely the Message Polarity Classification
Task. Given a tweet, the system classifies the
sentiment as either positive, negative, or neu-
tral. Several preprocessing tasks such as nega-
tion detection, spell checking, and tokeniza-
tion are performed to enhance lexical infor-
mation. The features are then augmented with
external sentiment lexicons. Classification is
done with Logistic Regression using a one-vs-
rest configuration. For the test runs, the sys-
tem was trained using only the provided train-
ing tweets. The classifier was successful, with
an F1 score of 58.43 on the official 2015 test
data, and an F1 score of 66.64 on the Twitter
2014 progress data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876733333333">
Since 2006, Twitter has grown into a ubiquitous
global social platform. Millions of users compose
Twitter messages, which are known as “tweets”,
to express their opinions and sentiments about the
world around them. These tweets turn into valu-
able resources for sentiment analysis, a field that fo-
cuses on analyzing the attitude of speakers or writ-
ers towards a certain topic. Working with this infor-
mal text genre opens up a new realm of challenges
in the natural language processing world. This pa-
per describes a tweet sentiment classifier which has
been applied to Subtask B of SemEval-2015 Task
10 (Rosenthal et al., 2015). The tweets generated by
users contain Internet slang, unconventional punctu-
ation and spelling, and typos, which require a differ-
ent set of preprocessing tools than traditional genres
like newswire text.
After preprocessing the tweets, classifying them
into categories of positive, negative, and neutral
presents another challenge. Many sentiment appli-
cations make use of lexicons to supply features to
the system, populating a list of positive and neg-
ative types. Some publicly available sources in-
clude the MPQA Subjectivity Lexicon (Wilson et
al., 2005), the Opinion Lexicon (Liu et al., 2005),
and the Sentiment140 Lexicon (Mohammad et al.,
2013). While some of these lexicons do not target
tweets as their analysis subject, they each provide
a mapping from n-grams to sentiment labels, which
proves to be helpful in building our tweet sentiment
analyzer.
After preprocessing, the system performs the clas-
sification task. The classifier we use is a one-
vs-rest logistic regression classifier, so the sys-
tem uses three binary classifiers: positive/not-
positive, negative/not-negative, and neutral/not-
neutral. The classifier also over-samples the low-
frequency classes, learning from the same number
of examples of each class overall.
The accompanying sections of the papers are or-
ganized as follows: Section 2 describes resources
such as the lexicons used in the system. It also out-
lines the system design and the APIs that the system
adopts. Section 3 describes the test runs and evalu-
ates the system. Section 4 concludes the paper.
</bodyText>
<page confidence="0.98527">
636
</page>
<note confidence="0.610478">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 636–639,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.961181" genericHeader="method">
2 System Details
</sectionHeader>
<bodyText confidence="0.999975142857143">
The main objective of our system is to determine if
a tweet conveys a positive, negative, or neutral sen-
timent. To achieve this goal, the system first em-
ploys some preprocessing tools to enhance the lex-
ical information. Then it relies on various senti-
ment lexicons to help with the classification of senti-
ments. For preprocessing, the system performs case-
folding, detects negation, optionally uses a spell
checker, performs tokenization, and makes use of
unigrams, bigrams, and pairs of n-grams.
In addition to features extracted from the tweets,
the system relies on four external sentiment lexi-
cons. Three of them are pre-existing resources: the
MPQA Subjectivity Lexicon (Wilson et al., 2005),
the Opinion Lexicon (Liu et al., 2005), and the Sen-
timent140 Lexicon (Mohammad et al., 2013). The
final lexicon is a manually created Emoji lexicon
compiled by the authors.
After extracting features, a Logistics Regression
classifier using a one-vs-rest setup is used to label
each of the tweets.
</bodyText>
<subsectionHeader confidence="0.998409">
2.1 Preprocessing
2.1.1 Case Folding
</subsectionHeader>
<bodyText confidence="0.999979">
We use case folding to make every letter of every
word in both the training and the test data lowercase.
This helps in dimensionality reduction.
</bodyText>
<subsectionHeader confidence="0.798709">
2.1.2 Negation Detector
</subsectionHeader>
<bodyText confidence="0.999987">
The system includes a negation detector. Similar
to (Pang et al., 2002), in this detector, we append a
negation suffix to words that occur within a nega-
tion window between a negation key word and some
punctuation. For example, the word “great”, which
is considered a positive word, is treated and learned
as a different token if it is preceded by “not” as in
“this pasta is not very great”. This sentence would
become “this pasta is not NOT_very NOT_great”.
</bodyText>
<subsectionHeader confidence="0.796297">
2.1.3 Jazzy
</subsectionHeader>
<bodyText confidence="0.999734">
Jazzy is the Java Open Source Spell Checker1.
Previous work had shown Jazzy to be effec-
tive (Miura et al., 2014). Though this was used dur-
ing the development of the system, time constraints
didn’t allow its use in the final submission. Using
</bodyText>
<footnote confidence="0.921258">
1http://jazzy.sourceforge.net/
</footnote>
<bodyText confidence="0.961640333333333">
five-fold crossvalidation, including Jazzy improved
performance slightly, from an F1 score of 63.8 to
64.75.
</bodyText>
<subsectionHeader confidence="0.621072">
2.1.4 Twokenizer
</subsectionHeader>
<bodyText confidence="0.99989575">
Twokenizer is a tokenizer designed specifically
for tweets (Gimpel et al., 2011). Twokenizer prop-
erly handles the tokenization of tweets without man-
gling URLs, mentions, or hashtags.
</bodyText>
<subsectionHeader confidence="0.9971525">
2.2 Sentiment Lexicons
2.2.1 MPQA
</subsectionHeader>
<bodyText confidence="0.999979666666667">
We make use of the MPQA Subjectivity Lexi-
con (Wilson et al., 2005). The lexicon is generated
from the MPQA Opinion Corpus, which incorpo-
rates a wide range of news articles manually anno-
tated for opinions and other private states. Although
the MPQA lexicon list mainly targets news articles,
it improved our system’s classifications. The MPQA
subjectivity lexicon provides a list of words with
both their polarity (positive, negative, and neutral)
and their strength (strong subjective, weak subjec-
tive). Our system made use of the polarity, but not
the strength.
</bodyText>
<subsubsectionHeader confidence="0.493516">
2.2.2 Opinion Lexicon
</subsubsectionHeader>
<bodyText confidence="0.9999852">
The Opinion Lexicon provided by Liu et al.
(2005) consists of a list of positive words and a list
of negative words. Because the lexicon is automat-
ically generated from social media content, it con-
tains misspelled lemmas, which could be beneficial
to tweet analysis as tweets tend to include erroneous
spellings and Internet slang (Liu, 2010). For exam-
ple, we can find both words “awesome” and “aw-
some” in the list of positive words. In the negative
list, we find “awful” as well as “aweful”.
</bodyText>
<subsubsectionHeader confidence="0.504287">
2.2.3 Sentiment140 Lexicon
</subsubsectionHeader>
<bodyText confidence="0.999924818181818">
The Sentiment140-Lexicon is a list of features
with associations to positive and negative senti-
ments (Mohammad et al., 2013). The lexicon
was created from the automatically-labeled senti-
ment140 corpus of 1.6 million tweets. The la-
beled features are unigrams, bigrams, and pairs
of n-grams (unigrams-unigrams, unigrams-bigrams,
bigrams-unigrams, and bigrams-bigrams). For ex-
ample, some of the features we could see in the list
are: the unigrams “@jeffery_donovan” and “xox-
oxo”, the bigrams “yeh yeh” and “praise !”, and the
</bodyText>
<page confidence="0.992788">
637
</page>
<bodyText confidence="0.999941090909091">
pairs “done—had”, “i—, drinking”, “thank you—
lovely”, and “good morning—can be”. Each feature
has a score that reflects how positive or negative the
feature is. If the word was seen in more positive
contexts than negative contexts, it’s score is posi-
tive. The magnitude of the score is highest when
the distribution is overwhelmingly positive, and the
magnitude is closest to zero when the word appears
equally in both positive and negative contexts. Neg-
ative words are scored similarly using negative val-
ues instead of positive values.
</bodyText>
<subsectionHeader confidence="0.540557">
2.2.4 Emoji Lexicon
</subsectionHeader>
<bodyText confidence="0.999974846153846">
Our system uses a hand-created Emoji dictionary
comprised of 16 positive2 and 7 negative3 emoti-
cons. Only the most common Emoji in the training
set were added to the lexicon. However, we chose to
some exclude some emoticons because they portray
a wide range of sentiments. For example, emoticons
like &amp;quot;:-|&amp;quot; and &amp;quot;:|&amp;quot; were seen in both neutral and nega-
tive tweets. Using this specific set of emoticons im-
proved the results when using cross-validation from
an F1 score of around 62.5 to 64.8. A more exten-
sive list might improve results, but given the time
constraints, these 23 emoticons covered the test set
adequately.
</bodyText>
<subsectionHeader confidence="0.982835">
2.3 Classifier
</subsectionHeader>
<bodyText confidence="0.9999921">
Our system uses a one-vs-rest logistic regression
classifier to analyze the sentiment of each tweet. Be-
fore the tweets get passed to the classifier, an over-
sampling process takes place to ensure equal num-
bers of each sentiment class during training. The
classifier uses a one-vs-rest scheme, breaking down
the classification process into three tasks: positive,
negative, and neutral. Our classification task as-
sumes that each sample is assigned to one and only
one label.
</bodyText>
<subsectionHeader confidence="0.974309">
2.3.1 One-Vs-Rest
</subsectionHeader>
<bodyText confidence="0.999927166666667">
We use a one-vs-rest strategy, building a classi-
fier for each sentiment label (Hong and Cho, 2008).
This means our system is comprised of three clas-
sifiers: positive/not-positive, negative/not-negative,
and neutral/not-neutral. For each classifier, the class
is compared against all the other classes. In other
</bodyText>
<footnote confidence="0.9736955">
2Positive :) :D :-) :-D :] :-] :’) :’-) ;) =) (: ;-) XD =D =] ;D
3Negative :( :-( :[ :-[ =( =/ :/
</footnote>
<bodyText confidence="0.999850235294118">
words, the features are screened to determine if they
are positive, negative, or neutral in three separate
stages: positive vs. non-positive, negative vs. non-
negative, and neutral vs. non-neutral.
During testing, each instance is labeled by each
of the three classifiers. When determining the la-
bel for a test instance, we would ideally like to have
only one of the binary classifiers find a match. This
usually happens when a tweet has many features ex-
pressing the same sentiment. However, when a tweet
has contradicting features, the classifiers may con-
tradict each either, either finding no matching class,
or having multiple classifiers match a class. In cases
of uncertainty, we use the labeling returned by the
classifier with the highest confidence. Removing
the one-vs-rest strategy decreases the score on cross-
validation from 64.8 to 64.0.
</bodyText>
<subsectionHeader confidence="0.944399">
2.3.2 Oversampling
</subsectionHeader>
<bodyText confidence="0.999950647058824">
In our classifier, we over-sample classes accord-
ing to the number of examples we have in the train-
ing data. This means no matter what the distribu-
tion of our underlying training data is, the system
learns from an equal number of examples of each
class label. For example, if we have 100 negative
instances in the training data and 200 non-negative
instances, the negative instances would be sampled
twice, whereas every non-negative example would
be sampled only once. This way, a negative fea-
ture that is seen once is twice as strong or informa-
tive to our system as a non-negative feature that is
seen once, and it would have the same weight as a
non-negative feature that had been seen twice. This
method decreased the system’s bias towards posi-
tive features. Removing oversampling decreases the
score on cross-validation from 64.8 to 62.3.
</bodyText>
<subsectionHeader confidence="0.8987">
2.3.3 Logistic Regression Model
</subsectionHeader>
<bodyText confidence="0.9998345">
The system uses the scikit-learn (Pedregosa et al.,
2011) implementation of a Logistic Regression clas-
sifier. In this system, we use a simple logistic re-
gression, where the model has one nominal variable
(a class or non-class), and the features are used as
measurement variables.
</bodyText>
<sectionHeader confidence="0.826288" genericHeader="method">
3 Test Runs
</sectionHeader>
<bodyText confidence="0.9987505">
The final classifier included in the submitted sys-
tem is an L2 regularized logistic regression algo-
</bodyText>
<page confidence="0.995744">
638
</page>
<table confidence="0.999633833333333">
Live Twitter 2014
System Journal 2014 SMS 2013 Twitter 2013 Twitter 2014 Sarcasm Twitter 2015
SWATAC 68.67 61.30 65.86 66.64 39.45 58.43
Webis 71.64 63.92 68.49 70.86 49.33 64.84
Splusplus 75.34 67.16 72.80 74.42 42.86 63.73
Average 68.13 60.21 63.88 64.90 47.06 57.13
</table>
<tableCaption confidence="0.989587">
Table 1: Official results comparing the SWATAC system to the best performing systems on the Twitter 2015 and
Twitter 2014 datasets, as well as the average performance on each dataset.
</tableCaption>
<bodyText confidence="0.999935833333333">
rithm, with a C value (the inverse of regulariza-
tion strength) set to 1, and the tolerance for stop-
ping criteria set to 0.0001, which are the default val-
ues provided by the scikit-learn library (Pedregosa
et al., 2011). This system is stochastic and returns
slightly different labellings on each run. Using five-
fold cross-validation, the final classifier had an F1
score between 64.0 and 65.0.
The official results for our system are in Ta-
ble 1. Our system has successfully scored a bet-
ter than average F1 in all of the test sets, except
for Twitter 2014 Sarcasm dataset. The table com-
pares our system to two other submitted systems:
Webis, the best scoring system on the Twitter 2015
dataset, Splusplus, the best scoring system on the
Twitter 2014 progress test data, as well as the aver-
age scores of all submitted systems in each test data
set.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999948545454545">
This paper describes our submission to SemEval-
2015’s Task 10 subtask B. Our system uses sev-
eral preprocessing tools, which includes case fold-
ing, negation, and tokenization. Several sentiment
lexicons and a manually created Emoji lexicon are
employed to help with classifying message polari-
ties. The system uses a logistic regression classifier
along with a one-vs-rest scheme to perform a three-
stage classification. The results indicate that our sys-
tem generally performs well, with an F1 score of
58.43 on the 2015 test data.
</bodyText>
<sectionHeader confidence="0.998959" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853733333333">
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of HLT ’11: Short Papers, volume 2,
pages 42–47.
Jin-Hyuk Hong and Sung-Bae Cho. 2008. A probabilis-
tic multi-class strategy of one-vs.-rest support vector
machines for cancer classification. Neurocomputing,
71(16-18):3275–3281.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the Web. In WWW ’05, pages 342–351.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2:627–666.
Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. TeamX: A sentiment an-
alyzer with enhanced lexicon mapping and weight-
ing scheme for unbalanced data. In Proceedings of
SemEval-2014, pages 628–632.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings of
SemEval-2013, pages 321–327.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP
’02, pages 79–86.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and Edouard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825–2830.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in Twitter. In Proceedings of SemEval-2015.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HTL-EMNLP
’05, pages 347–354.
</reference>
<page confidence="0.998866">
639
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479433">
<title confidence="0.999864">SWATAC: A Sentiment Analyzer using One-Vs-Rest Logistic Regression</title>
<author confidence="0.985005">Yousef Alhessi</author>
<author confidence="0.985005">Richard</author>
<affiliation confidence="0.91328">Swarthmore</affiliation>
<address confidence="0.999166">Swarthmore, PA 19081</address>
<email confidence="0.998482">yalhess1@cs.swarthmore.edu</email>
<email confidence="0.998482">richardw@cs.swarthmore.edu</email>
<abstract confidence="0.969796388888889">This paper describes SWATAC, a system built for SemEval-2015’s Task 10 Subtask B, namely the Message Polarity Classification Task. Given a tweet, the system classifies the sentiment as either positive, negative, or neutral. Several preprocessing tasks such as negation detection, spell checking, and tokenization are performed to enhance lexical information. The features are then augmented with external sentiment lexicons. Classification is done with Logistic Regression using a one-vsrest configuration. For the test runs, the system was trained using only the provided training tweets. The classifier was successful, with an F1 score of 58.43 on the official 2015 test data, and an F1 score of 66.64 on the Twitter 2014 progress data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
</authors>
<location>and</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, </marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of HLT ’11: Short Papers,</booktitle>
<volume>2</volume>
<pages>42--47</pages>
<marker>Smith, 2011</marker>
<rawString>Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of HLT ’11: Short Papers, volume 2, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Hyuk Hong</author>
<author>Sung-Bae Cho</author>
</authors>
<title>A probabilistic multi-class strategy of one-vs.-rest support vector machines for cancer classification.</title>
<date>2008</date>
<journal>Neurocomputing,</journal>
<pages>71--16</pages>
<contexts>
<context position="9014" citStr="Hong and Cho, 2008" startWordPosition="1426" endWordPosition="1429">set adequately. 2.3 Classifier Our system uses a one-vs-rest logistic regression classifier to analyze the sentiment of each tweet. Before the tweets get passed to the classifier, an oversampling process takes place to ensure equal numbers of each sentiment class during training. The classifier uses a one-vs-rest scheme, breaking down the classification process into three tasks: positive, negative, and neutral. Our classification task assumes that each sample is assigned to one and only one label. 2.3.1 One-Vs-Rest We use a one-vs-rest strategy, building a classifier for each sentiment label (Hong and Cho, 2008). This means our system is comprised of three classifiers: positive/not-positive, negative/not-negative, and neutral/not-neutral. For each classifier, the class is compared against all the other classes. In other 2Positive :) :D :-) :-D :] :-] :’) :’-) ;) =) (: ;-) XD =D =] ;D 3Negative :( :-( :[ :-[ =( =/ :/ words, the features are screened to determine if they are positive, negative, or neutral in three separate stages: positive vs. non-positive, negative vs. nonnegative, and neutral vs. non-neutral. During testing, each instance is labeled by each of the three classifiers. When determining </context>
</contexts>
<marker>Hong, Cho, 2008</marker>
<rawString>Jin-Hyuk Hong and Sung-Bae Cho. 2008. A probabilistic multi-class strategy of one-vs.-rest support vector machines for cancer classification. Neurocomputing, 71(16-18):3275–3281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the Web. In</title>
<date>2005</date>
<booktitle>WWW ’05,</booktitle>
<pages>342--351</pages>
<contexts>
<context position="2174" citStr="Liu et al., 2005" startWordPosition="332" endWordPosition="335">enthal et al., 2015). The tweets generated by users contain Internet slang, unconventional punctuation and spelling, and typos, which require a different set of preprocessing tools than traditional genres like newswire text. After preprocessing the tweets, classifying them into categories of positive, negative, and neutral presents another challenge. Many sentiment applications make use of lexicons to supply features to the system, populating a list of positive and negative types. Some publicly available sources include the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). While some of these lexicons do not target tweets as their analysis subject, they each provide a mapping from n-grams to sentiment labels, which proves to be helpful in building our tweet sentiment analyzer. After preprocessing, the system performs the classification task. The classifier we use is a onevs-rest logistic regression classifier, so the system uses three binary classifiers: positive/notpositive, negative/not-negative, and neutral/notneutral. The classifier also over-samples the lowfrequency classes, learning from the same numb</context>
<context position="4051" citStr="Liu et al., 2005" startWordPosition="625" endWordPosition="628">ieve this goal, the system first employs some preprocessing tools to enhance the lexical information. Then it relies on various sentiment lexicons to help with the classification of sentiments. For preprocessing, the system performs casefolding, detects negation, optionally uses a spell checker, performs tokenization, and makes use of unigrams, bigrams, and pairs of n-grams. In addition to features extracted from the tweets, the system relies on four external sentiment lexicons. Three of them are pre-existing resources: the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). The final lexicon is a manually created Emoji lexicon compiled by the authors. After extracting features, a Logistics Regression classifier using a one-vs-rest setup is used to label each of the tweets. 2.1 Preprocessing 2.1.1 Case Folding We use case folding to make every letter of every word in both the training and the test data lowercase. This helps in dimensionality reduction. 2.1.2 Negation Detector The system includes a negation detector. Similar to (Pang et al., 2002), in this detector, we append a negation suffix to words that oc</context>
<context position="6236" citStr="Liu et al. (2005)" startWordPosition="976" endWordPosition="979">e MPQA Subjectivity Lexicon (Wilson et al., 2005). The lexicon is generated from the MPQA Opinion Corpus, which incorporates a wide range of news articles manually annotated for opinions and other private states. Although the MPQA lexicon list mainly targets news articles, it improved our system’s classifications. The MPQA subjectivity lexicon provides a list of words with both their polarity (positive, negative, and neutral) and their strength (strong subjective, weak subjective). Our system made use of the polarity, but not the strength. 2.2.2 Opinion Lexicon The Opinion Lexicon provided by Liu et al. (2005) consists of a list of positive words and a list of negative words. Because the lexicon is automatically generated from social media content, it contains misspelled lemmas, which could be beneficial to tweet analysis as tweets tend to include erroneous spellings and Internet slang (Liu, 2010). For example, we can find both words “awesome” and “awsome” in the list of positive words. In the negative list, we find “awful” as well as “aweful”. 2.2.3 Sentiment140 Lexicon The Sentiment140-Lexicon is a list of features with associations to positive and negative sentiments (Mohammad et al., 2013). The</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the Web. In WWW ’05, pages 342–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of natural language processing,</booktitle>
<pages>2--627</pages>
<contexts>
<context position="6529" citStr="Liu, 2010" startWordPosition="1026" endWordPosition="1027">lassifications. The MPQA subjectivity lexicon provides a list of words with both their polarity (positive, negative, and neutral) and their strength (strong subjective, weak subjective). Our system made use of the polarity, but not the strength. 2.2.2 Opinion Lexicon The Opinion Lexicon provided by Liu et al. (2005) consists of a list of positive words and a list of negative words. Because the lexicon is automatically generated from social media content, it contains misspelled lemmas, which could be beneficial to tweet analysis as tweets tend to include erroneous spellings and Internet slang (Liu, 2010). For example, we can find both words “awesome” and “awsome” in the list of positive words. In the negative list, we find “awful” as well as “aweful”. 2.2.3 Sentiment140 Lexicon The Sentiment140-Lexicon is a list of features with associations to positive and negative sentiments (Mohammad et al., 2013). The lexicon was created from the automatically-labeled sentiment140 corpus of 1.6 million tweets. The labeled features are unigrams, bigrams, and pairs of n-grams (unigrams-unigrams, unigrams-bigrams, bigrams-unigrams, and bigrams-bigrams). For example, some of the features we could see in the l</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. Handbook of natural language processing, 2:627–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Miura</author>
<author>Shigeyuki Sakaki</author>
<author>Keigo Hattori</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>TeamX: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval-2014,</booktitle>
<pages>628--632</pages>
<contexts>
<context position="5098" citStr="Miura et al., 2014" startWordPosition="802" endWordPosition="805">y reduction. 2.1.2 Negation Detector The system includes a negation detector. Similar to (Pang et al., 2002), in this detector, we append a negation suffix to words that occur within a negation window between a negation key word and some punctuation. For example, the word “great”, which is considered a positive word, is treated and learned as a different token if it is preceded by “not” as in “this pasta is not very great”. This sentence would become “this pasta is not NOT_very NOT_great”. 2.1.3 Jazzy Jazzy is the Java Open Source Spell Checker1. Previous work had shown Jazzy to be effective (Miura et al., 2014). Though this was used during the development of the system, time constraints didn’t allow its use in the final submission. Using 1http://jazzy.sourceforge.net/ five-fold crossvalidation, including Jazzy improved performance slightly, from an F1 score of 63.8 to 64.75. 2.1.4 Twokenizer Twokenizer is a tokenizer designed specifically for tweets (Gimpel et al., 2011). Twokenizer properly handles the tokenization of tweets without mangling URLs, mentions, or hashtags. 2.2 Sentiment Lexicons 2.2.1 MPQA We make use of the MPQA Subjectivity Lexicon (Wilson et al., 2005). The lexicon is generated fro</context>
</contexts>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and Tomoko Ohkuma. 2014. TeamX: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. In Proceedings of SemEval-2014, pages 628–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of SemEval-2013,</booktitle>
<pages>321--327</pages>
<contexts>
<context position="2228" citStr="Mohammad et al., 2013" startWordPosition="340" endWordPosition="343">rs contain Internet slang, unconventional punctuation and spelling, and typos, which require a different set of preprocessing tools than traditional genres like newswire text. After preprocessing the tweets, classifying them into categories of positive, negative, and neutral presents another challenge. Many sentiment applications make use of lexicons to supply features to the system, populating a list of positive and negative types. Some publicly available sources include the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). While some of these lexicons do not target tweets as their analysis subject, they each provide a mapping from n-grams to sentiment labels, which proves to be helpful in building our tweet sentiment analyzer. After preprocessing, the system performs the classification task. The classifier we use is a onevs-rest logistic regression classifier, so the system uses three binary classifiers: positive/notpositive, negative/not-negative, and neutral/notneutral. The classifier also over-samples the lowfrequency classes, learning from the same number of examples of each class overall. The accompanying</context>
<context position="4105" citStr="Mohammad et al., 2013" startWordPosition="634" endWordPosition="637">processing tools to enhance the lexical information. Then it relies on various sentiment lexicons to help with the classification of sentiments. For preprocessing, the system performs casefolding, detects negation, optionally uses a spell checker, performs tokenization, and makes use of unigrams, bigrams, and pairs of n-grams. In addition to features extracted from the tweets, the system relies on four external sentiment lexicons. Three of them are pre-existing resources: the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). The final lexicon is a manually created Emoji lexicon compiled by the authors. After extracting features, a Logistics Regression classifier using a one-vs-rest setup is used to label each of the tweets. 2.1 Preprocessing 2.1.1 Case Folding We use case folding to make every letter of every word in both the training and the test data lowercase. This helps in dimensionality reduction. 2.1.2 Negation Detector The system includes a negation detector. Similar to (Pang et al., 2002), in this detector, we append a negation suffix to words that occur within a negation window between a negation key wo</context>
<context position="6831" citStr="Mohammad et al., 2013" startWordPosition="1075" endWordPosition="1078">ovided by Liu et al. (2005) consists of a list of positive words and a list of negative words. Because the lexicon is automatically generated from social media content, it contains misspelled lemmas, which could be beneficial to tweet analysis as tweets tend to include erroneous spellings and Internet slang (Liu, 2010). For example, we can find both words “awesome” and “awsome” in the list of positive words. In the negative list, we find “awful” as well as “aweful”. 2.2.3 Sentiment140 Lexicon The Sentiment140-Lexicon is a list of features with associations to positive and negative sentiments (Mohammad et al., 2013). The lexicon was created from the automatically-labeled sentiment140 corpus of 1.6 million tweets. The labeled features are unigrams, bigrams, and pairs of n-grams (unigrams-unigrams, unigrams-bigrams, bigrams-unigrams, and bigrams-bigrams). For example, some of the features we could see in the list are: the unigrams “@jeffery_donovan” and “xoxoxo”, the bigrams “yeh yeh” and “praise !”, and the 637 pairs “done—had”, “i—, drinking”, “thank you— lovely”, and “good morning—can be”. Each feature has a score that reflects how positive or negative the feature is. If the word was seen in more positi</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of SemEval-2013, pages 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP ’02,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="4587" citStr="Pang et al., 2002" startWordPosition="711" endWordPosition="714">Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). The final lexicon is a manually created Emoji lexicon compiled by the authors. After extracting features, a Logistics Regression classifier using a one-vs-rest setup is used to label each of the tweets. 2.1 Preprocessing 2.1.1 Case Folding We use case folding to make every letter of every word in both the training and the test data lowercase. This helps in dimensionality reduction. 2.1.2 Negation Detector The system includes a negation detector. Similar to (Pang et al., 2002), in this detector, we append a negation suffix to words that occur within a negation window between a negation key word and some punctuation. For example, the word “great”, which is considered a positive word, is treated and learned as a different token if it is preceded by “not” as in “this pasta is not very great”. This sentence would become “this pasta is not NOT_very NOT_great”. 2.1.3 Jazzy Jazzy is the Java Open Source Spell Checker1. Previous work had shown Jazzy to be effective (Miura et al., 2014). Though this was used during the development of the system, time constraints didn’t allo</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP ’02, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gael Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<contexts>
<context position="11125" citStr="Pedregosa et al., 2011" startWordPosition="1773" endWordPosition="1776">the training data and 200 non-negative instances, the negative instances would be sampled twice, whereas every non-negative example would be sampled only once. This way, a negative feature that is seen once is twice as strong or informative to our system as a non-negative feature that is seen once, and it would have the same weight as a non-negative feature that had been seen twice. This method decreased the system’s bias towards positive features. Removing oversampling decreases the score on cross-validation from 64.8 to 62.3. 2.3.3 Logistic Regression Model The system uses the scikit-learn (Pedregosa et al., 2011) implementation of a Logistic Regression classifier. In this system, we use a simple logistic regression, where the model has one nominal variable (a class or non-class), and the features are used as measurement variables. 3 Test Runs The final classifier included in the submitted system is an L2 regularized logistic regression algo638 Live Twitter 2014 System Journal 2014 SMS 2013 Twitter 2013 Twitter 2014 Sarcasm Twitter 2015 SWATAC 68.67 61.30 65.86 66.64 39.45 58.43 Webis 71.64 63.92 68.49 70.86 49.33 64.84 Splusplus 75.34 67.16 72.80 74.42 42.86 63.73 Average 68.13 60.21 63.88 64.90 47.06</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval-2015.</booktitle>
<contexts>
<context position="1577" citStr="Rosenthal et al., 2015" startWordPosition="242" endWordPosition="245">nce 2006, Twitter has grown into a ubiquitous global social platform. Millions of users compose Twitter messages, which are known as “tweets”, to express their opinions and sentiments about the world around them. These tweets turn into valuable resources for sentiment analysis, a field that focuses on analyzing the attitude of speakers or writers towards a certain topic. Working with this informal text genre opens up a new realm of challenges in the natural language processing world. This paper describes a tweet sentiment classifier which has been applied to Subtask B of SemEval-2015 Task 10 (Rosenthal et al., 2015). The tweets generated by users contain Internet slang, unconventional punctuation and spelling, and typos, which require a different set of preprocessing tools than traditional genres like newswire text. After preprocessing the tweets, classifying them into categories of positive, negative, and neutral presents another challenge. Many sentiment applications make use of lexicons to supply features to the system, populating a list of positive and negative types. Some publicly available sources include the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), a</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in Twitter. In Proceedings of SemEval-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HTL-EMNLP ’05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="2134" citStr="Wilson et al., 2005" startWordPosition="325" endWordPosition="328">d to Subtask B of SemEval-2015 Task 10 (Rosenthal et al., 2015). The tweets generated by users contain Internet slang, unconventional punctuation and spelling, and typos, which require a different set of preprocessing tools than traditional genres like newswire text. After preprocessing the tweets, classifying them into categories of positive, negative, and neutral presents another challenge. Many sentiment applications make use of lexicons to supply features to the system, populating a list of positive and negative types. Some publicly available sources include the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). While some of these lexicons do not target tweets as their analysis subject, they each provide a mapping from n-grams to sentiment labels, which proves to be helpful in building our tweet sentiment analyzer. After preprocessing, the system performs the classification task. The classifier we use is a onevs-rest logistic regression classifier, so the system uses three binary classifiers: positive/notpositive, negative/not-negative, and neutral/notneutral. The classifier also over-samples the lowfreque</context>
<context position="4011" citStr="Wilson et al., 2005" startWordPosition="618" endWordPosition="621">ive, negative, or neutral sentiment. To achieve this goal, the system first employs some preprocessing tools to enhance the lexical information. Then it relies on various sentiment lexicons to help with the classification of sentiments. For preprocessing, the system performs casefolding, detects negation, optionally uses a spell checker, performs tokenization, and makes use of unigrams, bigrams, and pairs of n-grams. In addition to features extracted from the tweets, the system relies on four external sentiment lexicons. Three of them are pre-existing resources: the MPQA Subjectivity Lexicon (Wilson et al., 2005), the Opinion Lexicon (Liu et al., 2005), and the Sentiment140 Lexicon (Mohammad et al., 2013). The final lexicon is a manually created Emoji lexicon compiled by the authors. After extracting features, a Logistics Regression classifier using a one-vs-rest setup is used to label each of the tweets. 2.1 Preprocessing 2.1.1 Case Folding We use case folding to make every letter of every word in both the training and the test data lowercase. This helps in dimensionality reduction. 2.1.2 Negation Detector The system includes a negation detector. Similar to (Pang et al., 2002), in this detector, we a</context>
<context position="5668" citStr="Wilson et al., 2005" startWordPosition="887" endWordPosition="890">d shown Jazzy to be effective (Miura et al., 2014). Though this was used during the development of the system, time constraints didn’t allow its use in the final submission. Using 1http://jazzy.sourceforge.net/ five-fold crossvalidation, including Jazzy improved performance slightly, from an F1 score of 63.8 to 64.75. 2.1.4 Twokenizer Twokenizer is a tokenizer designed specifically for tweets (Gimpel et al., 2011). Twokenizer properly handles the tokenization of tweets without mangling URLs, mentions, or hashtags. 2.2 Sentiment Lexicons 2.2.1 MPQA We make use of the MPQA Subjectivity Lexicon (Wilson et al., 2005). The lexicon is generated from the MPQA Opinion Corpus, which incorporates a wide range of news articles manually annotated for opinions and other private states. Although the MPQA lexicon list mainly targets news articles, it improved our system’s classifications. The MPQA subjectivity lexicon provides a list of words with both their polarity (positive, negative, and neutral) and their strength (strong subjective, weak subjective). Our system made use of the polarity, but not the strength. 2.2.2 Opinion Lexicon The Opinion Lexicon provided by Liu et al. (2005) consists of a list of positive </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HTL-EMNLP ’05, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>