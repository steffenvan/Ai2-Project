<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9996545">
Non-projective Dependency-based Pre-Reordering with Recurrent
Neural Network for Machine Translation
</title>
<author confidence="0.96452">
Antonio Valerio Miceli-Barone
</author>
<affiliation confidence="0.891586">
Universit`a di Pisa
</affiliation>
<address confidence="0.8743895">
Largo B. Pontecorvo, 3
56127 Pisa, Italy
</address>
<email confidence="0.997235">
miceli@di.unipi.it
</email>
<author confidence="0.837845">
Giuseppe Attardi
</author>
<affiliation confidence="0.780537">
Universit`a di Pisa
</affiliation>
<address confidence="0.88404">
Largo B. Pontecorvo, 3
56127 Pisa, Italy
</address>
<email confidence="0.997311">
attardi@di.unipi.it
</email>
<sectionHeader confidence="0.993853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915157894737">
The quality of statistical machine
translation performed with phrase
based approaches can be increased by
permuting the words in the source
sentences in an order which resem-
bles that of the target language. We
propose a class of recurrent neu-
ral models which exploit source-side
dependency syntax features to re-
order the words into a target-like or-
der. We evaluate these models on
the German-to-English and Italian-to-
English language pairs, showing sig-
nificant improvements over a phrase-
based Moses baseline. We also com-
pare with state of the art German-to-
English pre-reordering rules, showing
that our method obtains similar or bet-
ter results.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999756056603774">
Statistical machine translation is typically
performed using phrase-based systems
(Koehn et al., 2007). These systems can
usually produce accurate local reordering
but they have difficulties dealing with the
long-distance reordering that tends to occur
between certain language pairs (Birch et al.,
2008).
The quality of phrase-based machine trans-
lation can be improved by reordering the
words in each sentence of source-side of the
parallel training corpus in a ”target-like” or-
der and then applying the same transforma-
tion as a pre-processing step to input strings
during execution.
When the source-side sentences can be ac-
curately parsed, pre-reordering can be per-
formed using hand-coded rules. This ap-
proach has been successfully applied to
German-to-English (Collins et al., 2005) and
other languages. The main issue with it is that
these rules must be designed for each spe-
cific language pair, which requires consider-
able linguistic expertise.
Fully statistical approaches, on the other
hand, learn the reordering relation from word
alignments. Some of them learn reordering
rules on the constituency (Dyer and Resnik,
2010) (Khalilov and Fonollosa, 2011) or pro-
jective dependency (Genzel, 2010), (Lerner
and Petrov, 2013) parse trees of source sen-
tences. The permutations that these meth-
ods can learn can be generally non-local
(i.e. high distance) on the sentences but lo-
cal (parent-child or sibling-sibling swaps) on
the parse trees. Moreover, constituency or
projective dependency trees may not be the
ideal way of representing the syntax of non-
analytic languages such as German or Ital-
ian, which could be better described using
non-projective dependency trees (Bosco and
Lombardo, 2004). Other methods, based on
recasting reordering as a combinatorial opti-
mization problem (Tromble and Eisner, 2009),
(Visweswariah et al., 2011), can learn to gen-
erate in principle arbitrary permutations, but
they can only make use of minimal syntactic
information (part-of-speech tags) and there-
fore can’t exploit the potentially valuable
structural syntactic information provided by
a parser.
In this work we propose a class of reorder-
ing models which attempt to close this gap by
</bodyText>
<page confidence="0.988551">
846
</page>
<note confidence="0.978398">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 846–856,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.983276977777778">
exploiting rich dependency syntax features
and at the same time being able to process
non-projective dependency parse trees and
generate permutations which may be non-
local both on the sentences and on the parse
trees.
We represent these problems as sequence pre-
diction machine learning tasks, which we ad-
dress using recurrent neural networks.
We applied our model to reorder German
sentences into an English-like word order as
a pre-processing step for phrase-based ma-
chine translation, obtaining significant im-
provements over the unreordered baseline
system and quality comparable to the hand-
coded rules introduced by Collins et al.
(2005). We also applied our model to Italian-
to-English pre-reordering, obtaining a con-
siderable improvement over the unreordered
baseline.
2 Reordering as a walk on a
dependency tree
In order to describe the non-local reordering
phenomena that can occur between language
pairs such as German-to-English and Italian-
to-English, we introduce a reordering frame-
work similar to (Miceli Barone and Attardi,
2013), based on a graph walk of the depen-
dency parse tree of the source sentence. This
framework doesn’t restrict the parse tree to be
projective, and allows the generation of arbi-
trary permutations.
Let f ≡ (f1, f2, . . . , fLf) be a source sen-
tence, annotated by a rooted dependency
parse tree: ∀j ∈ 1, ... , Lf, hj ≡ PARENT(j)
We define a walker process that walks from
word to word across the edges of the parse
tree, and at each steps optionally emits the
current word, with the constraint that each
word must be eventually emitted exactly
once.
Therefore, the final string of emitted words f0
is a permutation of the original sentence f,
and any permutation can be generated by a
suitable walk on the parse tree.
</bodyText>
<subsectionHeader confidence="0.999791">
2.1 Reordering automaton
</subsectionHeader>
<bodyText confidence="0.998851818181818">
We formalize the walker process as a non-
deterministic finite-state automaton.
The state v of the automaton is the tuple v ≡
(j, E, a) where j ∈ 1, . . . , Lf is the current ver-
tex (word index), E is the set of emitted ver-
tices, a is the last action taken by the automa-
ton.
The initial state is: v(0) ≡ (rootf, {}, null)
where rootf is the root vertex of the parse tree.
At each step t, the automaton chooses one
of the following actions:
</bodyText>
<listItem confidence="0.997017571428571">
• EMIT: emit the word fj at the current
vertex j. This action is enabled only if the
current vertex has not been already emit-
ted:
• UP: move to the parent of the current
vertex. Enabled if there is a parent and
we did not just come down from it:
</listItem>
<equation confidence="0.6806485">
hj =6 null, a =6 DOWNj
(j, E, a) UP→ (hj, E, UPj) (2)
</equation>
<listItem confidence="0.9990816">
• DOWNj0: move to the child j0 of the cur-
rent vertex. Enabled if the subtree s(j0)
rooted at j0 contains vertices that have
not been already emitted and if we did
not just come up from it:
</listItem>
<equation confidence="0.9983075">
hj0 = j, a =6 UPj0, ∃k ∈ s(j0) : k ∈/ E (3)
(j0, E, DOWNj0)
</equation>
<bodyText confidence="0.999900055555556">
The execution continues until all the vertices
have been emitted.
We define the sequence of states of the
walker automaton during one run as an execu-
tion v¯ ∈ GEN(f ). An execution also uniquely
specifies the sequence of actions performed
by the automation.
The preconditions make sure that all execu-
tion of the automaton always end generating
a permutation of the source sentence. Fur-
thermore, no cycles are possible: progress is
made at every step, and it is not possible to
enter in an execution that later turns out to be
invalid.
Every permutation of the source sentence can
be generated by some execution. In fact, each
permutation f0 can be generated by exactly
one execution, which we denote as ¯v(f0).
</bodyText>
<equation confidence="0.99444">
(j, E, a) EMIT
→ (j, E ∪ {j}, EMIT)
j ∈/ E (1)
DOWNj0
→
(j, E, a)
</equation>
<page confidence="0.979372">
847
</page>
<bodyText confidence="0.99963525">
We can split the execution ¯v(f&apos;) into a se-
quence of Lf emission fragments ¯vj(f&apos;), each
ending with an EMIT action.
The first fragment has zero or more DOWN*
actions followed by one EMIT action, while
each other fragment has a non-empty se-
quence of UP and DOWN* actions (always
zero or more UPs followed by zero or more
DOWNs) followed by one EMIT action.
Finally, we define an action in an execution
as forced if it was the only action enabled at
the step where it occurred.
</bodyText>
<subsectionHeader confidence="0.999869">
2.2 Application
</subsectionHeader>
<bodyText confidence="0.999772225">
Suppose we perform reordering using a
typical syntax-based system which pro-
cesses source-side projective dependency
parse trees and is limited to swaps between
pair of vertices which are either in a parent-
child relation or in a sibling relation. In such
execution the UP actions are always forced,
since the ”walker” process never leaves a sub-
tree before all its vertices have been emitted.
Suppose instead that we could perform re-
ordering according to an ”oracle”. The ex-
ecutions of our automaton corresponding to
these permutations will in general contain
unforced UP actions. We define these ac-
tions, and the execution fragments that ex-
hibit them, as non-tree-local.
In practice we don’t have access to a
reordering ”oracle”, but for sentences pairs
in a parallel corpus we can compute heuristic
”pseudo-oracle” reference permutations of
the source sentences from word-alignments.
Following (Al-Onaizan and Pap-
ineni, 2006), (Tromble and Eisner, 2009),
(Visweswariah et al., 2011), (Navratil et al.,
2012), we generate word alignments in both
the source-to-target and the target-to-source
directions using IBM model 4 as imple-
mented in GIZA++ (Och et al., 1999) and then
we combine them into a symmetrical word
alignment using the ”grow-diag-final-and”
heuristic implemented in Moses (Koehn et
al., 2007).
Given the symmetric word-aligned corpus,
we assign to each source-side word an in-
teger index corresponding to the position of
the leftmost target-side word it is aligned to
(attaching unaligned words to the following
aligned word) and finally we perform a sta-
ble sort of source-side words according to this
index.
</bodyText>
<subsectionHeader confidence="0.999964">
2.3 Reordering example
</subsectionHeader>
<bodyText confidence="0.999419272727273">
Consider the segment of a German sentence
shown in fig. 1. The English-reordered
segment ”die W¨ahrungsreserven anfangs
lediglich dienen sollten zur Verteidigung”
corresponds to the English: ”the reserve as-
sets were originally intended to provide
protection”.
In order to compose this segment from the
original German, the reordering automaton
described in our framework must perform a
complex sequence of moves on the parse tree:
</bodyText>
<listItem confidence="0.951098">
• Starting from ”sollten”, de-
scend to ”dienen”, descent to
”W¨ahrungsreserven” and finally
</listItem>
<bodyText confidence="0.696389">
to ”die”. Emit it, then go up to
”W¨ahrungsreserven”, emit it and
go up to ”dienen” and up again to
”sollten”. Note that the last UP is
unforced since ”dienen” has not been
emitted at that point and has also un-
emitted children. This unforced action
indicates non-tree-local reordering.
</bodyText>
<listItem confidence="0.922385833333333">
• Go down to ”anfangs”. Note that the
in the parse tree edge crosses another
edge, indicating non-projectivity. Emit
”anfangs” and go up (forced) back to
”sollten”.
• Go down to ”dienen”, down to ”zur”,
down to ”lediglich” and emit it. Go
up (forced) to ”zur”, up (unforced) to
”dienen”, emit it, go up (unforced) to
”sollten”, emit it. Go down to ”dienen”,
down to ”zur” emit it, go down to
”Verteidigung” and emit it.
</listItem>
<bodyText confidence="0.996761285714286">
Correct reordering of this segment would
be difficult both for a phrase-based system
(since the words are further apart than both
the typical maximum distortion distance and
maximum phrase length) and for a syntax-
based system (due to the presence of non-
projectivity and non-tree-locality).
</bodyText>
<page confidence="0.994853">
848
</page>
<figureCaption confidence="0.999833">
Figure 1: Section of the dependency parse tree of a German sentence.
</figureCaption>
<sectionHeader confidence="0.902695" genericHeader="method">
3 Recurrent Neural Network
</sectionHeader>
<subsectionHeader confidence="0.530135">
reordering models
</subsectionHeader>
<bodyText confidence="0.9999142">
Given the reordering framework described
above, we could try to directly predict the ex-
ecutions as Miceli Barone and Attardi (2013)
attempted with their version of the frame-
work. However, the executions of a given
sentence can have widely different lengths,
which could make incremental inexact decod-
ing such as beam search difficult due to the
need to prune over partial hypotheses that
have different numbers of emitted words.
Therefore, we decided to investigate a dif-
ferent class of models which have the prop-
erty that state transition happen only in corre-
spondence with word emission. This enables
us to leverage the technology of incremental
language models.
Using language models for reordering is
not something new (Feng et al., 2010), (Dur-
rani et al., 2011), (Bisazza and Federico, 2013),
but instead of using a more or less standard
n-gram language model, we are going to base
our model on recurrent neural network language
models (Mikolov et al., 2010).
Neural networks allow easy incorpora-
tion of multiple types of features and can
be trained more specifically on the types
of sequences that will occur during decod-
ing, hence they can avoid wasting model
space to represent the probabilities of non-
permutations.
</bodyText>
<subsectionHeader confidence="0.999877">
3.1 Base RNN-RM
</subsectionHeader>
<bodyText confidence="0.9996825">
Let f ≡ (f1, f2, . . . , fLf ) be a source sentence.
We model the reordering system as a deter-
ministic single hidden layer recurrent neural
network:
</bodyText>
<equation confidence="0.999651">
v(t) = T(Θ(1) · x(t) + ΘREC · v(t − 1)) (4)
</equation>
<bodyText confidence="0.999633875">
where x(t) ∈ Rn is a feature vector associated
to the t-th word in a permutation f0, v(0) ≡
vinit, Θ(1) and ΘREC are parameters1 and T(·)
is the hyperbolic tangent function.
If we know the first t − 1 words of the per-
mutation f0 in order to compute the proba-
bility distribution of the t-th word we do the
following:
</bodyText>
<listItem confidence="0.999247545454545">
• Iteratively compute the state v(t − 1)
from the feature vectors x(1), ... , x(t −
1).
• For the all the indices of the words that
haven’t occurred in the permutation so
far j ∈ J(t) ≡ ([1, Lf ] − ¯it−1:), compute
a score hj,t ≡ ho(v(t − 1), xo(j)), where
xo(·) is the feature vector of the candidate
target word.
• Normalize the scores using the logistic
softmax function: P(¯It = j |f, ¯it−1:, t) =
</listItem>
<equation confidence="0.979549">
exp(hj,t)
∑j0∈J(t) exp(hj0,t).
</equation>
<bodyText confidence="0.998666111111111">
The scoring function ho(v(t − 1), xo(j)) ap-
plies a feed-forward hidden layer to the fea-
ture inputs xo(j), and then takes a weighed
inner product between the activation of this
layer and the state v(t − 1). The result is
then linearly combined to an additional fea-
ture equal to the logarithm of the remaining
words in the permutation (Lf − t),2 and to a
bias feature:
</bodyText>
<equation confidence="0.716678">
hj,t ≡&lt; T(Θ(o) · xo(j)), 0(2) o v(t − 1) &gt;
</equation>
<bodyText confidence="0.637105166666667">
1we don’t use a bias feature since it is redundant
when the layer has input features encoded with the
”one-hot” encoding
2since we are then passing this score to a softmax of
variable size (Lf − t), this feature helps the model to
keep the score already approximately scaled.
</bodyText>
<equation confidence="0.595022">
(5)
+ 0(&amp;quot;) · log(Lf − t) + 0(bias)
</equation>
<bodyText confidence="0.804537">
where hj,t ≡ ho(v(t − 1), xo(j)).
</bodyText>
<page confidence="0.933378">
849
</page>
<bodyText confidence="0.999104333333333">
We can compute the probability of an entire
permutation f0 just by multiplying the proba-
bilities for each word: P(f0|f) = P( I¯ = ¯i|f ) =
</bodyText>
<equation confidence="0.7753805">
∏Lf
t=1 P(¯It = ¯it|f, t)
</equation>
<subsectionHeader confidence="0.889429">
3.1.1 Training
</subsectionHeader>
<bodyText confidence="0.997760147058824">
Given a training set of pairs of sentences and
reference permutations, the training problem
is defined as finding the set of parameters
B ≡ (vinit, Θ(1), B(2), ΘREC, Θ(o), B(a), B(bias))
which minimize the per-word empirical
cross-entropy of the model w.r.t. the reference
permutations in the training set. Gradients
can be efficiently computed using backpropa-
gation through time (BPTT).
In practice we used the following training
architecture:
Stochastic gradient descent, with each train-
ing pair (f, f0) considered as a single mini-
batch for updating purposes. Gradients com-
puted using the automatic differentiation fa-
cilities of Theano (Bergstra et al., 2010) (which
implements a generalized BPTT). No trun-
cation is used. L2-regularization 3. Learn-
ing rates dynamically adjusted per scalar pa-
rameter using the AdaDelta heuristic (Zeiler,
2012). Gradient clipping heuristic to prevent
the ”exploding gradient” problem (Graves,
2013). Early stopping w.r.t. a validation set to
prevent overfitting. Uniform random initial-
ization for parameters other than the recur-
rent parameter matrix ΘREC. Random initial-
ization with echo state property for ΘREC, with
contraction coefficient v = 0.99 (Jaeger, 2001),
(Gallicchio and Micheli, 2011).
Training time complexity is O(L2f) per sen-
tence, which could be reduced to O(Lf ) using
truncated BTTP at the expense of update ac-
curacy and hence convergence speed. Space
complexity is O(Lf ) per sentence.
</bodyText>
<subsectionHeader confidence="0.977751">
3.1.2 Decoding
</subsectionHeader>
<bodyText confidence="0.9999655">
In order to use the RNN-RM model for pre-
reordering we need to compute the most
</bodyText>
<equation confidence="0.854465285714286">
∗
likely permutation f0 of the source sentence
f:
∗
f0 ≡ argmax P(f 0|f) (6)
f 0∈GEN(f )
3A = 10−4 on the recurrent matrix, A = 10−6 on the
</equation>
<bodyText confidence="0.969867315789474">
final layer, per minibatch.
Solving this problem to the global optimum is
computationally hard4, hence we solve it to a
local optimum using a beam search strategy.
We generate the permutation incrementally
from left to right. Starting from an initial
state consisting of an empty string and the ini-
tial state vector vinit, at each step we generate
all possible successor states and retain the B-
most probable of them (histogram pruning),
according to the probability of the entire pre-
fix of permutation they represent.
Since RNN state vectors do not decompose
in a meaningful way, we don’t use any hy-
pothesis recombination.
At step t there are Lf − t possible succes-
sor states, and the process always takes ex-
actly Lf steps5, therefore time complexity is
O(B · L2f) and space complexity is O(B).
</bodyText>
<subsectionHeader confidence="0.447036">
3.1.3 Features
</subsectionHeader>
<bodyText confidence="0.999309">
We use two different feature configurations:
unlexicalized and lexicalized.
In the unlexicalized configuration, the state
transition input feature function x(j) is com-
posed by the following features, all encoded
using the ”one-hot” encoding scheme:
</bodyText>
<listItem confidence="0.985855666666667">
• Unigram: POS(j), DEPREL(j), POS(j) ∗
DEPREL(j). Left, right and parent un-
igram: POS(k), DEPREL(k), POS(k) ∗
DEPREL(k), where k is the index of re-
spectively the word at the left (in the
original sentence), at the right and the
dependency parent of word j. Unique
tags are used for padding.
• Pair features: POS(j) ∗ POS(k), POS(j) ∗
DEPREL(k), DEPREL(j) ∗ POS(k),
DEPREL(j) ∗ DEPREL(k), for k defined
as above.
• Triple features POS(j) ∗ POS(leftj) ∗
POS(rightj), POS(j) ∗ POS(leftj) ∗
POS(parentj), POS(j) ∗ POS(rightj) ∗
POS(parentj).
• Bigram: POS(j) ∗ POS(k), POS(j) ∗
DEPREL(k), DEPREL(j) ∗ POS(k)
</listItem>
<bodyText confidence="0.6437655">
where k is the previous emitted word in
the permutation.
</bodyText>
<footnote confidence="0.563711">
4NP-hard for at least certain choices of features and
parameters
5actually, Lf − 1, since the last choice is forced
</footnote>
<page confidence="0.978423">
850
</page>
<listItem confidence="0.950295">
• Topological features: three binary fea-
</listItem>
<bodyText confidence="0.885351333333333">
tures which indicate whether word j
and the previously emitted word are in
a parent-child, child-parent or sibling-
sibling relation, respectively.
The target word feature function xo(j) is
the same as x(j) except that each feature is
also conjoined with a quantized signed dis-
tance6 between word j and the previous emit-
ted word. Feature value combinations that
appear less than 100 times in the training set
are replaced by a distinguished ”rare” tag.
The lexicalized configuration is equivalent
to the unlexicalized one except that x(j) and
xo(j) also have the surface form of word j (not
conjoined with the signed distance).
</bodyText>
<subsectionHeader confidence="0.998788">
3.2 Fragment RNN-RM
</subsectionHeader>
<bodyText confidence="0.999959965517241">
The Base RNN-RM described in the previ-
ous section includes dependency informa-
tion, but not the full information of reorder-
ing fragments as defined by our automa-
ton model (sec. 2). In order to determine
whether this rich information is relevant to
machine translation pre-reordering, we pro-
pose an extension, denoted as Fragment RNN-
RM, which includes reordering fragment fea-
tures, at expense of a significant increase of
time complexity.
We consider a hierarchical recurrent neural
network. At top level, this is defined as the
previous RNN. However, the x(j) and xo(j)
vectors, in addition to the feature vectors de-
scribed as above now contain also the final
states of another recurrent neural network.
This internal RNN has a separate clock and
a separate state vector. For each step t of
the top-level RNN which transitions between
word f0(t − 1) and f0(t), the internal RNN is
reinitialized to its own initial state and per-
forms multiple internal steps, one for each ac-
tion in the fragment of the execution that the
walker automaton must perform to walk be-
tween words f0(t − 1) and f0(t) in the depen-
dency parse (with a special shortcut of length
one if they are adjacent in f with monotonic
relative order).
</bodyText>
<footnote confidence="0.979908666666667">
6values greater than 5 and smaller than 10 are quan-
tized as 5, values greater or equal to 10 are quantized as
10. Negative values are treated similarly.
</footnote>
<bodyText confidence="0.9700815">
The state transition of the inner RNN is de-
fined as:
</bodyText>
<equation confidence="0.9999">
vr(t) = τ(O(r1) · xr(tr) +OrREC · vr(tr − 1))(7)
</equation>
<bodyText confidence="0.999899">
where xr(tr) is the feature function for the
word traversed at inner time tr in the execu-
tion fragment. vr(0)
are parameters.
Evaluation and decoding are performed es-
sentially in the same was as in Base RNN-
RM, except that the time complexity is now
O(L3f) since the length of execution fragments
</bodyText>
<equation confidence="0.457373">
is O(Lf
</equation>
<bodyText confidence="0.976096">
).
Training is also essentially performed in the
same way, though gradient computation is
much more involved since gradients prop-
agate from the top-level RNN to the inner
RNN. In our implementation we just used the
automatic differentiation facilities of Theano.
</bodyText>
<subsectionHeader confidence="0.508236">
3.2.1 Features
</subsectionHeader>
<bodyText confidence="0.969967111111111">
The unlexicalized features for the inner RNN
input vector xr(tr) depend on the current
word in the execution fragment (at index tr),
the previous one and the action label: UP,
DOWN or RIGHT (shortcut). EMIT actions
are not included as they always implicitly oc-
cur at the end of each fragment.
Specifically the features, encoded with the
”one-hot” encoding are: A ∗ POS(tr) ∗
</bodyText>
<equation confidence="0.979068">
POS(tr − 1), A ∗ POS(tr) ∗ DEPREL(tr −
1), A ∗ DEPREL(tr) ∗ POS(tr − 1), A ∗
DEPREL(tr) ∗ DEPREL(tr − 1).
</equation>
<bodyText confidence="0.9999868">
These features are also conjoined with the
quantized signed distance (in the original
sentence) between each pair of words.
The lexicalized features just include the surface
form of each visited word at tr.
</bodyText>
<subsectionHeader confidence="0.999872">
3.3 Base GRU-RM
</subsectionHeader>
<bodyText confidence="0.965513071428571">
We also propose a variant of the Base RNN-
RM where the standard recurrent hidden
layer is replaced by a Gated Recurrent Unit
layer, recently proposed by Cho et al. (2014)
for machine translation applications.
The Base GRU-RM is defined as the Base
RNN-RM of sec. 3.1, except that the recur-
rence relation 4 is replaced by fig. 2
Features are the same of unlexicalized Base
RNN-RM (we experienced difficulties train-
ing the Base GRU-RM with lexicalized fea-
tures).
= vinit
r , O(r1) and OrREC
</bodyText>
<page confidence="0.744279">
851
</page>
<equation confidence="0.997171625">
vrst(t) = π(Θ(1) (8)
rst · x(t) + ΘREC
rst · v(t − 1))
vupd(t) = π(Θ(1)
upd · x(t) upd
+ ΘREC · v(t − 1))
vraw(t) = τ(Θ(1) · x(t) + ΘREC · v(t − 1) O vupd(t))
v(t) = vrst(t) O v(t − 1) + (1 − vrst(t)) o vraw(t)
</equation>
<figureCaption confidence="0.89067">
Figure 2: GRU recurrence equations. vrst(t) and vupd(t) are the activation vectors of the ”reset”
and ”update” gates, respectively, and π(·) is the logistic sigmoid function.
</figureCaption>
<bodyText confidence="0.9919926">
.
Training is also performed in the same
way except that we found more benefi-
cial to convergence speed to optimize using
Adam (Kingma and Ba, 2014) 7 rather than
AdaDelta.
In principle we could also extend the Frag-
ment RNN-RM into a Fragment GRU-RM,
but we did not investigate that model in this
work.
</bodyText>
<sectionHeader confidence="0.999741" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999985363636364">
We performed German-to-English pre-
reordering experiments with Base RNN-RM
(both unlexicalized and lexicalized), Frag-
ment RNN-RM and Base GRU-RM.
In order to validate the experimental re-
sults on a different language pair, we addi-
tionally performed an Italian-to-English pre-
reordering experiment with the Base GRU-
RM, after assessing that this was the model
that obtained the largest improvement on
German-to-English.
</bodyText>
<subsectionHeader confidence="0.998003">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.994996618181818">
The German-to-English baseline phrase-
based system was trained on the Europarl v7
corpus (Koehn, 2005). We randomly split it
in a 1,881,531 sentence pairs training set, a
2,000 sentence pairs development set (used
for tuning) and a 2,000 sentence pairs test
set. The English language model was trained
on the English side of the parallel corpus
augmented with a corpus of sentences from
AP News, for a total of 22,891,001 sentences.
The baseline system is phrase-based Moses
in a default configuration with maximum
distortion distance equal to 6 and lexicalized
reordering enabled. Maximum phrase size is
7with learning rate 2 · 10−5 and all the other hyper-
parameters equal to the default values in the article.
equal to 7.
The language model is a 5-gram
IRSTLM/KenLM.
The pseudo-oracle system was trained on
the training and tuning corpus obtained by
permuting the German source side using
the heuristic described in section 2.2 and is
otherwise equal to the baseline system.
In addition to the test set extracted from
Europarl, we also used a 2,525 sentence
pairs test set (”news2009”) a 3,000 sentence
pairs ”challenge” set used for the WMT 2013
translation task (”news2013”).
The Italian-to-English baseline system was
trained on a parallel corpus assembled from
Europarl v7, JRC-ACQUIS v2.2 (Steinberger
et al., 2006) and additional bilingual articles
crawled from online newspaper websites8, to-
taling 3,081,700 sentence pairs, which were
split into a 3,075,777 sentence pairs phrase-
table training corpus, a 3,923 sentence pairs
tuning corpus, and a 2,000 sentence pairs test
corpus.
Non-projective dependency parsing for our
models, both for German and Italian was
performed with the DeSR transition-based
parser (Attardi, 2006).
We also trained a German-to-English
Moses system with pre-reordering performed
by Collins et al. (2005) rules, implemented by
Howlett and Dras (2011).
Constituency parsing for Collins et al. (2005)
rules was performed with the Berkeley parser
(Petrov et al., 2006). For Italian-to-English
we did not compare with a hand-coded
reordering system as we are not aware of
any strong pre-reordering baseline for this
language pair.
For our experiments, we extract approxi-
</bodyText>
<footnote confidence="0.6406">
8Corriere.it and Asianews.it
</footnote>
<page confidence="0.996361">
852
</page>
<bodyText confidence="0.99997325">
mately 300,000 sentence pairs from the Moses
training set based on a heuristic confidence
measure of word-alignment quality (Huang,
2009), (Navratil et al., 2012). We randomly
removed 2,000 sentences from this filtered
dataset to form a validation set for early stop-
ping, the rest were used for training the pre-
reordering models.
</bodyText>
<subsectionHeader confidence="0.966392">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.995550148148148">
The hidden state size s of the RNNs was set to
100 while it was set to 30 for the GRU model,
validation was performed every 2,000 train-
ing examples. After 50 consecutive validation
rounds without improvement, training was
stopped and the set of training parameters
that resulted in the lowest validation cross-
entropy were saved.
Training took approximately 1.5 days for the
unlexicalized Base RNN-RM, 2.5 days for the
lexicalized Base RNN-RM and for the unlexi-
calized Base GRU-RM and 5 days for the un-
lexicalized Fragment RNN-RM on a 24-core
machine without GPU (CPU load never rose
to more than 400%).
Decoding was performed with a beam size
of 4. Decoding the whole German corpus
took about 1.0-1.2 days for all the models ex-
cept Fragment RNN-RM for which it took
about 3 days. Decoding for the Italian corpus
for the Base GRU-RM took approximately 1.5
days.
Effects on monolingual reordering score
are shown in fig. 3 (German) and fig. 4
(Italian), effects on translation quality are
shown in fig. 5 (German-to-English) and fig.
6 (Italian-to-English)9.
</bodyText>
<subsectionHeader confidence="0.999784">
4.3 Discussion and analysis
</subsectionHeader>
<bodyText confidence="0.998454875">
All our German-to-English models signifi-
cantly improved over the phrase-based base-
line, performing as well as or almost as well
as (Collins et al., 2005), which is an interesting
result since our models doesn’t require any
specific linguistic expertise.
Surprisingly, the lexicalized version of Base
RNN-RM performed worse than the unlexi-
</bodyText>
<footnote confidence="0.89484775">
9Although the baseline systems were trained on the
same datasets used in Miceli Barone and Attardi (2013),
the results are different since we used a different ver-
sion of Moses
</footnote>
<bodyText confidence="0.999792846153846">
calized one. This goes contrary to expectation
as neural language models are usually lexical-
ized and in fact often use nothing but lexical
features.
The unlexicalized Fragment RNN-RM was
quite accurate but very expensive both dur-
ing training and decoding, thus it may not be
practical.
The unlexicalized Base GRU-RM per-
formed very well, especially on the Europarl
dataset (where all the scores are much higher
than the other datasets) and it never per-
formed significantly worse than the unlexi-
calized Fragment RNN-RM which is much
slower.
We also performed exploratory experi-
ments with different feature sets (such as
lexical-only features) but we couldn’t obtain
a good training error. Larger network sizes
should increase model capacity and may pos-
sibly enable training on simpler feature sets.
The Italian-to-English experiment with
Base GRU-RM confirmed that this model per-
forms very well on a language pair with dif-
ferent reordering phenomena than German-
to-English.
</bodyText>
<sectionHeader confidence="0.996564" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999961782608696">
We presented a class of statistical syntax-
based, non-projective, non-tree-local pre-
reordering systems for machine translation.
Our systems processes source sentences
parsed with non-projective dependency
parsers and permutes them into a target-
like word order, suitable for translation
by an appropriately trained downstream
phrase-based system.
The models we proposed are completely
trained with machine learning approaches
and is, in principle, capable of generating ar-
bitrary permutations, without the hard con-
straints that are commonly present in other
statistical syntax-based pre-reordering meth-
ods.
Practical constraints depend on the choice
of features and are therefore quite flexible,
allowing a trade-off between accuracy and
speed.
In our experiments with the RNN-RM and
GRU-RM models we managed to achieve
translation quality improvements compara-
</bodyText>
<page confidence="0.997015">
853
</page>
<table confidence="0.995883">
Reordering BLEU improvement
none 62.10
unlex. Base RNN-RM 64.03 +1.93
lex. Base RNN-RM 63.99 +1.89
unlex. Fragment RNN-RM 64.43 +2.33
unlex. Base GRU-RM 64.78 +2.68
</table>
<figureCaption confidence="0.8918125">
Figure 3: German ”Monolingual” reordering scores (upstream system output vs. ”oracle”-
permuted German) on the Europarl test set. All improvements are significant at 1% level.
</figureCaption>
<table confidence="0.642460666666667">
Reordering BLEU improvement
none 73.11
unlex. Base GRU-RM 81.09 +7.98
</table>
<figureCaption confidence="0.782936">
Figure 4: Italian ”Monolingual” reordering scores on the Europarl test set. All improvements
are significant at 1% level.
</figureCaption>
<table confidence="0.88130692">
Test set system BLEU improvement
Europarl baseline 33.00
Europarl ”oracle” 41.80 +8.80
Europarl Collins 33.52 +0.52
Europarl unlex. Base RNN-RM 33.41 +0.41
Europarl lex. Base RNN-RM 33.38 +0.38
Europarl unlex. Fragment RNN-RM 33.54 +0.54
Europarl unlex. Base GRU-RM 34.15 +1.15
news2013 baseline 18.80
news2013 Collins NA NA
news2013 unlex. Base RNN-RM 19.19 +0.39
news2013 lex. Base RNN-RM 19.01 +0.21
news2013 unlex. Fragment RNN-RM 19.27 +0.47
news2013 unlex. Base GRU-RM 19.28 +0.48
news2009 baseline 18.09
news2009 Collins 18.74 +0.65
news2009 unlex. Base RNN-RM 18.50 +0.41
news2009 lex. Base RNN-RM 18.44 +0.35
news2009 unlex. Fragment RNN-RM 18.60 +0.51
news2009 unlex. Base GRU-RM 18.58 +0.49
Figure 5: German-to-English RNN-RM translation scores. All improvements are significant at
1% level.
Test set system BLEU improvement
Europarl baseline 29.58
Europarl unlex. Base GRU-RM 30.84 +1.26
</table>
<figureCaption confidence="0.945018">
Figure 6: Italian-to-English RNN-RM translation scores. Improvement is significant at 1% level.
</figureCaption>
<page confidence="0.995284">
854
</page>
<bodyText confidence="0.9976055">
ble to those of the best hand-coded pre-
reordering rules.
</bodyText>
<sectionHeader confidence="0.989093" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998898130841122">
Yaser Al-Onaizan and Kishore Papineni. 2006.
Distortion models for statistical machine trans-
lation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Com-
putational Linguistics, ACL-44, pages 529–536,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Giuseppe Attardi. 2006. Experiments with a mul-
tilanguage non-projective dependency parser.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06,
pages 166–170, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
James Bergstra, Olivier Breuleux, Fr´ed´eric
Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David
Warde-Farley, and Yoshua Bengio. 2010.
Theano: a CPU and GPU math expression
compiler. In Proceedings of the Python for Scien-
tific Computing Conference (SciPy), June. Oral
Presentation.
Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2008. Predicting success in machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 745–754, Stroudsburg,
PA, USA. Association for Computational Lin-
guistics.
Arianna Bisazza and Marcello Federico. 2013.
Efficient solutions for word reordering in
German-English phrase-based statistical ma-
chine translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 440–451, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Cristina Bosco and Vincenzo Lombardo. 2004.
Dependency and relational structure in tree-
bank annotation. In COLING 2004 Recent Ad-
vances in Dependency Grammar, pages 1–8.
Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry
Bahdanau, and Yoshua Bengio. 2014. On
the properties of neural machine translation:
Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259.
Michael Collins, Philipp Koehn, and Ivona
Kuˇcerov´a. 2005. Clause restructuring for sta-
tistical machine translation. In Proceedings of
the 43rd annual meeting on association for computa-
tional linguistics, pages 531–540. Association for
Computational Linguistics.
Nadir Durrani, Helmut Schmid, and Alexander
Fraser. 2011. A joint sequence translation
model with integrated reordering. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1045–1054. Associ-
ation for Computational Linguistics.
Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
858–866, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney.
2010. A source-side decoding sequence model
for statistical machine translation. In Conference
of the Association for Machine Translation in the
Americas (AMTA).
C. Gallicchio and A. Micheli. 2011. Architectural
and markovian factors of echo state networks.
Neural Networks, 24(5):440 – 456.
Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale ma-
chine translation. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, COLING ’10, pages 376–384, Stroudsburg,
PA, USA. Association for Computational Lin-
guistics.
Alex Graves. 2013. Generating sequences
with recurrent neural networks. arXiv preprint
arXiv:1308.0850.
Susan Howlett and Mark Dras. 2011. Clause
restructuring for SMT not absolutely helpful.
In Proceedings of the 49th Annual Meeting of the
Assocation for Computational Linguistics: Human
Language Technologies, pages 384–388.
Fei Huang. 2009. Confidence measure for word
alignment. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 932–940. Association for Com-
putational Linguistics.
Herbert Jaeger. 2001. The echo state ap-
proach to analysing and training recurrent neu-
ral networks-with an erratum note. Bonn, Ger-
many: German National Research Center for Infor-
mation Technology GMD Technical Report, 148:34.
Maxim Khalilov and Jos´e AR Fonollosa. 2011.
Syntax-based reordering for statistical ma-
chine translation. Computer speech &amp; language,
25(4):761–788.
Diederik Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. arXiv
preprint arXiv:1412.6980.
</reference>
<page confidence="0.9876">
855
</page>
<reference confidence="0.999676064102565">
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ondˇrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings
of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL
’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Con-
ference Proceedings: the tenth Machine Translation
Summit, pages 79–86, Phuket, Thailand. AAMT,
AAMT.
Uri Lerner and Slav Petrov. 2013. Source-side
classifier preordering for machine translation.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
’13).
Antonio Valerio Miceli Barone and Giuseppe At-
tardi. 2013. Pre-reordering for machine
translation using transition-based walks on de-
pendency parse trees. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 164–169, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Tomas Mikolov, Martin Karafi´at, Lukas Bur-
get, Jan Cernock`y, and Sanjeev Khudanpur.
2010. Recurrent neural network based lan-
guage model. In INTERSPEECH, pages 1045–
1048.
Jiri Navratil, Karthik Visweswariah, and Anan-
thakrishnan Ramanathan. 2012. A com-
parison of syntactic reordering methods for
english-german machine translation. In COL-
ING, pages 2043–2058.
Franz Josef Och, Christoph Tillmann, Hermann
Ney, et al. 1999. Improved alignment models
for statistical machine translation. In Proc. of the
Joint SIGDAT Conf. on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora,
pages 20–28.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 433–440. Association for Computational
Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Dniel Varga. 2006. The jrc-acquis: A multi-
lingual aligned parallel corpus with 20+ lan-
guages. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC’2006), Genoa, Italy.
Roy Tromble and Jason Eisner. 2009. Learning
linear ordering problems for better translation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 2 - Volume 2, EMNLP ’09, pages 1007–1016,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajku-
mar, Ankur Gandhe, Ananthakrishnan Ra-
manathan, and Jiri Navratil. 2011. A word
reordering model for improved machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 486–496, Stroudsburg,
PA, USA. Association for Computational
Linguistics.
Matthew D Zeiler. 2012. Adadelta: An adap-
tive learning rate method. arXiv preprint
arXiv:1212.5701.
</reference>
<page confidence="0.999001">
856
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.462759">
<title confidence="0.9996475">Non-projective Dependency-based Pre-Reordering with Neural Network for Machine Translation</title>
<author confidence="0.999972">Antonio Valerio</author>
<affiliation confidence="0.974974">Universit`a di Largo B. Pontecorvo,</affiliation>
<address confidence="0.998887">56127 Pisa,</address>
<email confidence="0.999518">miceli@di.unipi.it</email>
<affiliation confidence="0.826057666666667">Giuseppe Universit`a di Largo B. Pontecorvo,</affiliation>
<address confidence="0.99959">56127 Pisa,</address>
<email confidence="0.999096">attardi@di.unipi.it</email>
<abstract confidence="0.9980352">The quality of statistical machine translation performed with phrase based approaches can be increased by permuting the words in the source sentences in an order which resembles that of the target language. We propose a class of recurrent neural models which exploit source-side dependency syntax features to reorder the words into a target-like order. We evaluate these models on the German-to-English and Italian-to- English language pairs, showing significant improvements over a phrasebased Moses baseline. We also compare with state of the art German-to- English pre-reordering rules, showing that our method obtains similar or better results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>529--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8386" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="1388" endWordPosition="1392">d, since the ”walker” process never leaves a subtree before all its vertices have been emitted. Suppose instead that we could perform reordering according to an ”oracle”. The executions of our automaton corresponding to these permutations will in general contain unforced UP actions. We define these actions, and the execution fragments that exhibit them, as non-tree-local. In practice we don’t have access to a reordering ”oracle”, but for sentences pairs in a parallel corpus we can compute heuristic ”pseudo-oracle” reference permutations of the source sentences from word-alignments. Following (Al-Onaizan and Papineni, 2006), (Tromble and Eisner, 2009), (Visweswariah et al., 2011), (Navratil et al., 2012), we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as implemented in GIZA++ (Och et al., 1999) and then we combine them into a symmetrical word alignment using the ”grow-diag-final-and” heuristic implemented in Moses (Koehn et al., 2007). Given the symmetric word-aligned corpus, we assign to each source-side word an integer index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following </context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 529–536, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>166--170</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24169" citStr="Attardi, 2006" startWordPosition="4054" endWordPosition="4055">e” set used for the WMT 2013 translation task (”news2013”). The Italian-to-English baseline system was trained on a parallel corpus assembled from Europarl v7, JRC-ACQUIS v2.2 (Steinberger et al., 2006) and additional bilingual articles crawled from online newspaper websites8, totaling 3,081,700 sentence pairs, which were split into a 3,075,777 sentence pairs phrasetable training corpus, a 3,923 sentence pairs tuning corpus, and a 2,000 sentence pairs test corpus. Non-projective dependency parsing for our models, both for German and Italian was performed with the DeSR transition-based parser (Attardi, 2006). We also trained a German-to-English Moses system with pre-reordering performed by Collins et al. (2005) rules, implemented by Howlett and Dras (2011). Constituency parsing for Collins et al. (2005) rules was performed with the Berkeley parser (Petrov et al., 2006). For Italian-to-English we did not compare with a hand-coded reordering system as we are not aware of any strong pre-reordering baseline for this language pair. For our experiments, we extract approxi8Corriere.it and Asianews.it 852 mately 300,000 sentence pairs from the Moses training set based on a heuristic confidence measure of</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 166–170, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
</authors>
<location>Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David</location>
<marker>Bergstra, Breuleux, Bastien, Lamblin, </marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David</rawString>
</citation>
<citation valid="true">
<authors>
<author>Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy),</booktitle>
<tech>Oral Presentation.</tech>
<marker>Warde-Farley, Bengio, 2010</marker>
<rawString>Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Predicting success in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>745--754</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1282" citStr="Birch et al., 2008" startWordPosition="181" endWordPosition="184"> order. We evaluate these models on the German-to-English and Italian-toEnglish language pairs, showing significant improvements over a phrasebased Moses baseline. We also compare with state of the art German-toEnglish pre-reordering rules, showing that our method obtains similar or better results. 1 Introduction Statistical machine translation is typically performed using phrase-based systems (Koehn et al., 2007). These systems can usually produce accurate local reordering but they have difficulties dealing with the long-distance reordering that tends to occur between certain language pairs (Birch et al., 2008). The quality of phrase-based machine translation can be improved by reordering the words in each sentence of source-side of the parallel training corpus in a ”target-like” order and then applying the same transformation as a pre-processing step to input strings during execution. When the source-side sentences can be accurately parsed, pre-reordering can be performed using hand-coded rules. This approach has been successfully applied to German-to-English (Collins et al., 2005) and other languages. The main issue with it is that these rules must be designed for each specific language pair, whic</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2008</marker>
<rawString>Alexandra Birch, Miles Osborne, and Philipp Koehn. 2008. Predicting success in machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 745–754, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Marcello Federico</author>
</authors>
<title>Efficient solutions for word reordering in German-English phrase-based statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>440--451</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11538" citStr="Bisazza and Federico, 2013" startWordPosition="1895" endWordPosition="1898">ramework. However, the executions of a given sentence can have widely different lengths, which could make incremental inexact decoding such as beam search difficult due to the need to prune over partial hypotheses that have different numbers of emitted words. Therefore, we decided to investigate a different class of models which have the property that state transition happen only in correspondence with word emission. This enables us to leverage the technology of incremental language models. Using language models for reordering is not something new (Feng et al., 2010), (Durrani et al., 2011), (Bisazza and Federico, 2013), but instead of using a more or less standard n-gram language model, we are going to base our model on recurrent neural network language models (Mikolov et al., 2010). Neural networks allow easy incorporation of multiple types of features and can be trained more specifically on the types of sequences that will occur during decoding, hence they can avoid wasting model space to represent the probabilities of nonpermutations. 3.1 Base RNN-RM Let f ≡ (f1, f2, . . . , fLf ) be a source sentence. We model the reordering system as a deterministic single hidden layer recurrent neural network: v(t) = </context>
</contexts>
<marker>Bisazza, Federico, 2013</marker>
<rawString>Arianna Bisazza and Marcello Federico. 2013. Efficient solutions for word reordering in German-English phrase-based statistical machine translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 440–451, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Bosco</author>
<author>Vincenzo Lombardo</author>
</authors>
<title>Dependency and relational structure in treebank annotation.</title>
<date>2004</date>
<booktitle>In COLING 2004 Recent Advances in Dependency Grammar,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2675" citStr="Bosco and Lombardo, 2004" startWordPosition="398" endWordPosition="401">ordering rules on the constituency (Dyer and Resnik, 2010) (Khalilov and Fonollosa, 2011) or projective dependency (Genzel, 2010), (Lerner and Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasting reordering as a combinatorial optimization problem (Tromble and Eisner, 2009), (Visweswariah et al., 2011), can learn to generate in principle arbitrary permutations, but they can only make use of minimal syntactic information (part-of-speech tags) and therefore can’t exploit the potentially valuable structural syntactic information provided by a parser. In this work we propose a class of reordering models which attempt to close this gap by 846 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International J</context>
</contexts>
<marker>Bosco, Lombardo, 2004</marker>
<rawString>Cristina Bosco and Vincenzo Lombardo. 2004. Dependency and relational structure in treebank annotation. In COLING 2004 Recent Advances in Dependency Grammar, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merri¨enboer</author>
<author>Dzmitry Bahdanau</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</title>
<date>2014</date>
<marker>Cho, van Merri¨enboer, Bahdanau, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting on association for computational linguistics,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531–540. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>1045--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11508" citStr="Durrani et al., 2011" startWordPosition="1890" endWordPosition="1894">h their version of the framework. However, the executions of a given sentence can have widely different lengths, which could make incremental inexact decoding such as beam search difficult due to the need to prune over partial hypotheses that have different numbers of emitted words. Therefore, we decided to investigate a different class of models which have the property that state transition happen only in correspondence with word emission. This enables us to leverage the technology of incremental language models. Using language models for reordering is not something new (Feng et al., 2010), (Durrani et al., 2011), (Bisazza and Federico, 2013), but instead of using a more or less standard n-gram language model, we are going to base our model on recurrent neural network language models (Mikolov et al., 2010). Neural networks allow easy incorporation of multiple types of features and can be trained more specifically on the types of sequences that will occur during decoding, hence they can avoid wasting model space to represent the probabilities of nonpermutations. 3.1 Base RNN-RM Let f ≡ (f1, f2, . . . , fLf ) be a source sentence. We model the reordering system as a deterministic single hidden layer rec</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1045–1054. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Context-free reordering, finite-state translation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>858--866</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2108" citStr="Dyer and Resnik, 2010" startWordPosition="310" endWordPosition="313">e transformation as a pre-processing step to input strings during execution. When the source-side sentences can be accurately parsed, pre-reordering can be performed using hand-coded rules. This approach has been successfully applied to German-to-English (Collins et al., 2005) and other languages. The main issue with it is that these rules must be designed for each specific language pair, which requires considerable linguistic expertise. Fully statistical approaches, on the other hand, learn the reordering relation from word alignments. Some of them learn reordering rules on the constituency (Dyer and Resnik, 2010) (Khalilov and Fonollosa, 2011) or projective dependency (Genzel, 2010), (Lerner and Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasti</context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>Chris Dyer and Philip Resnik. 2010. Context-free reordering, finite-state translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 858–866, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>A source-side decoding sequence model for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="11484" citStr="Feng et al., 2010" startWordPosition="1886" endWordPosition="1889"> (2013) attempted with their version of the framework. However, the executions of a given sentence can have widely different lengths, which could make incremental inexact decoding such as beam search difficult due to the need to prune over partial hypotheses that have different numbers of emitted words. Therefore, we decided to investigate a different class of models which have the property that state transition happen only in correspondence with word emission. This enables us to leverage the technology of incremental language models. Using language models for reordering is not something new (Feng et al., 2010), (Durrani et al., 2011), (Bisazza and Federico, 2013), but instead of using a more or less standard n-gram language model, we are going to base our model on recurrent neural network language models (Mikolov et al., 2010). Neural networks allow easy incorporation of multiple types of features and can be trained more specifically on the types of sequences that will occur during decoding, hence they can avoid wasting model space to represent the probabilities of nonpermutations. 3.1 Base RNN-RM Let f ≡ (f1, f2, . . . , fLf ) be a source sentence. We model the reordering system as a deterministic</context>
</contexts>
<marker>Feng, Mauser, Ney, 2010</marker>
<rawString>Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A source-side decoding sequence model for statistical machine translation. In Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gallicchio</author>
<author>A Micheli</author>
</authors>
<title>Architectural and markovian factors of echo state networks.</title>
<date>2011</date>
<journal>Neural Networks,</journal>
<volume>24</volume>
<issue>5</issue>
<pages>456</pages>
<contexts>
<context position="15095" citStr="Gallicchio and Micheli, 2011" startWordPosition="2519" endWordPosition="2522">fferentiation facilities of Theano (Bergstra et al., 2010) (which implements a generalized BPTT). No truncation is used. L2-regularization 3. Learning rates dynamically adjusted per scalar parameter using the AdaDelta heuristic (Zeiler, 2012). Gradient clipping heuristic to prevent the ”exploding gradient” problem (Graves, 2013). Early stopping w.r.t. a validation set to prevent overfitting. Uniform random initialization for parameters other than the recurrent parameter matrix ΘREC. Random initialization with echo state property for ΘREC, with contraction coefficient v = 0.99 (Jaeger, 2001), (Gallicchio and Micheli, 2011). Training time complexity is O(L2f) per sentence, which could be reduced to O(Lf ) using truncated BTTP at the expense of update accuracy and hence convergence speed. Space complexity is O(Lf ) per sentence. 3.1.2 Decoding In order to use the RNN-RM model for prereordering we need to compute the most ∗ likely permutation f0 of the source sentence f: ∗ f0 ≡ argmax P(f 0|f) (6) f 0∈GEN(f ) 3A = 10−4 on the recurrent matrix, A = 10−6 on the final layer, per minibatch. Solving this problem to the global optimum is computationally hard4, hence we solve it to a local optimum using a beam search str</context>
</contexts>
<marker>Gallicchio, Micheli, 2011</marker>
<rawString>C. Gallicchio and A. Micheli. 2011. Architectural and markovian factors of echo state networks. Neural Networks, 24(5):440 – 456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning source-side reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>376--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2179" citStr="Genzel, 2010" startWordPosition="322" endWordPosition="323">n the source-side sentences can be accurately parsed, pre-reordering can be performed using hand-coded rules. This approach has been successfully applied to German-to-English (Collins et al., 2005) and other languages. The main issue with it is that these rules must be designed for each specific language pair, which requires considerable linguistic expertise. Fully statistical approaches, on the other hand, learn the reordering relation from word alignments. Some of them learn reordering rules on the constituency (Dyer and Resnik, 2010) (Khalilov and Fonollosa, 2011) or projective dependency (Genzel, 2010), (Lerner and Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasting reordering as a combinatorial optimization problem (Tromble and Eisn</context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning source-side reordering rules for large scale machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 376–384, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</title>
<date>2013</date>
<contexts>
<context position="14796" citStr="Graves, 2013" startWordPosition="2477" endWordPosition="2478"> be efficiently computed using backpropagation through time (BPTT). In practice we used the following training architecture: Stochastic gradient descent, with each training pair (f, f0) considered as a single minibatch for updating purposes. Gradients computed using the automatic differentiation facilities of Theano (Bergstra et al., 2010) (which implements a generalized BPTT). No truncation is used. L2-regularization 3. Learning rates dynamically adjusted per scalar parameter using the AdaDelta heuristic (Zeiler, 2012). Gradient clipping heuristic to prevent the ”exploding gradient” problem (Graves, 2013). Early stopping w.r.t. a validation set to prevent overfitting. Uniform random initialization for parameters other than the recurrent parameter matrix ΘREC. Random initialization with echo state property for ΘREC, with contraction coefficient v = 0.99 (Jaeger, 2001), (Gallicchio and Micheli, 2011). Training time complexity is O(L2f) per sentence, which could be reduced to O(Lf ) using truncated BTTP at the expense of update accuracy and hence convergence speed. Space complexity is O(Lf ) per sentence. 3.1.2 Decoding In order to use the RNN-RM model for prereordering we need to compute the mos</context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Howlett</author>
<author>Mark Dras</author>
</authors>
<title>Clause restructuring for SMT not absolutely helpful.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Assocation for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>384--388</pages>
<contexts>
<context position="24320" citStr="Howlett and Dras (2011)" startWordPosition="4074" endWordPosition="4077">from Europarl v7, JRC-ACQUIS v2.2 (Steinberger et al., 2006) and additional bilingual articles crawled from online newspaper websites8, totaling 3,081,700 sentence pairs, which were split into a 3,075,777 sentence pairs phrasetable training corpus, a 3,923 sentence pairs tuning corpus, and a 2,000 sentence pairs test corpus. Non-projective dependency parsing for our models, both for German and Italian was performed with the DeSR transition-based parser (Attardi, 2006). We also trained a German-to-English Moses system with pre-reordering performed by Collins et al. (2005) rules, implemented by Howlett and Dras (2011). Constituency parsing for Collins et al. (2005) rules was performed with the Berkeley parser (Petrov et al., 2006). For Italian-to-English we did not compare with a hand-coded reordering system as we are not aware of any strong pre-reordering baseline for this language pair. For our experiments, we extract approxi8Corriere.it and Asianews.it 852 mately 300,000 sentence pairs from the Moses training set based on a heuristic confidence measure of word-alignment quality (Huang, 2009), (Navratil et al., 2012). We randomly removed 2,000 sentences from this filtered dataset to form a validation set</context>
</contexts>
<marker>Howlett, Dras, 2011</marker>
<rawString>Susan Howlett and Mark Dras. 2011. Clause restructuring for SMT not absolutely helpful. In Proceedings of the 49th Annual Meeting of the Assocation for Computational Linguistics: Human Language Technologies, pages 384–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Confidence measure for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>932--940</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24806" citStr="Huang, 2009" startWordPosition="4149" endWordPosition="4150">n-to-English Moses system with pre-reordering performed by Collins et al. (2005) rules, implemented by Howlett and Dras (2011). Constituency parsing for Collins et al. (2005) rules was performed with the Berkeley parser (Petrov et al., 2006). For Italian-to-English we did not compare with a hand-coded reordering system as we are not aware of any strong pre-reordering baseline for this language pair. For our experiments, we extract approxi8Corriere.it and Asianews.it 852 mately 300,000 sentence pairs from the Moses training set based on a heuristic confidence measure of word-alignment quality (Huang, 2009), (Navratil et al., 2012). We randomly removed 2,000 sentences from this filtered dataset to form a validation set for early stopping, the rest were used for training the prereordering models. 4.2 Results The hidden state size s of the RNNs was set to 100 while it was set to 30 for the GRU model, validation was performed every 2,000 training examples. After 50 consecutive validation rounds without improvement, training was stopped and the set of training parameters that resulted in the lowest validation crossentropy were saved. Training took approximately 1.5 days for the unlexicalized Base RN</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Fei Huang. 2009. Confidence measure for word alignment. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 932–940. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Jaeger</author>
</authors>
<title>The echo state approach to analysing and training recurrent neural networks-with an erratum note.</title>
<date>2001</date>
<tech>Technical Report, 148:34.</tech>
<institution>German National Research Center for Information Technology GMD</institution>
<location>Bonn, Germany:</location>
<contexts>
<context position="15063" citStr="Jaeger, 2001" startWordPosition="2517" endWordPosition="2518">the automatic differentiation facilities of Theano (Bergstra et al., 2010) (which implements a generalized BPTT). No truncation is used. L2-regularization 3. Learning rates dynamically adjusted per scalar parameter using the AdaDelta heuristic (Zeiler, 2012). Gradient clipping heuristic to prevent the ”exploding gradient” problem (Graves, 2013). Early stopping w.r.t. a validation set to prevent overfitting. Uniform random initialization for parameters other than the recurrent parameter matrix ΘREC. Random initialization with echo state property for ΘREC, with contraction coefficient v = 0.99 (Jaeger, 2001), (Gallicchio and Micheli, 2011). Training time complexity is O(L2f) per sentence, which could be reduced to O(Lf ) using truncated BTTP at the expense of update accuracy and hence convergence speed. Space complexity is O(Lf ) per sentence. 3.1.2 Decoding In order to use the RNN-RM model for prereordering we need to compute the most ∗ likely permutation f0 of the source sentence f: ∗ f0 ≡ argmax P(f 0|f) (6) f 0∈GEN(f ) 3A = 10−4 on the recurrent matrix, A = 10−6 on the final layer, per minibatch. Solving this problem to the global optimum is computationally hard4, hence we solve it to a local</context>
</contexts>
<marker>Jaeger, 2001</marker>
<rawString>Herbert Jaeger. 2001. The echo state approach to analysing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148:34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Khalilov</author>
<author>Jos´e AR Fonollosa</author>
</authors>
<title>Syntax-based reordering for statistical machine translation.</title>
<date>2011</date>
<journal>Computer speech &amp; language,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2139" citStr="Khalilov and Fonollosa, 2011" startWordPosition="314" endWordPosition="317">e-processing step to input strings during execution. When the source-side sentences can be accurately parsed, pre-reordering can be performed using hand-coded rules. This approach has been successfully applied to German-to-English (Collins et al., 2005) and other languages. The main issue with it is that these rules must be designed for each specific language pair, which requires considerable linguistic expertise. Fully statistical approaches, on the other hand, learn the reordering relation from word alignments. Some of them learn reordering rules on the constituency (Dyer and Resnik, 2010) (Khalilov and Fonollosa, 2011) or projective dependency (Genzel, 2010), (Lerner and Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasting reordering as a combinatoria</context>
</contexts>
<marker>Khalilov, Fonollosa, 2011</marker>
<rawString>Maxim Khalilov and Jos´e AR Fonollosa. 2011. Syntax-based reordering for statistical machine translation. Computer speech &amp; language, 25(4):761–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diederik Kingma</author>
<author>Jimmy Ba</author>
</authors>
<title>Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</title>
<date>2014</date>
<contexts>
<context position="21842" citStr="Kingma and Ba, 2014" startWordPosition="3689" endWordPosition="3692">s training the Base GRU-RM with lexicalized features). = vinit r , O(r1) and OrREC 851 vrst(t) = π(Θ(1) (8) rst · x(t) + ΘREC rst · v(t − 1)) vupd(t) = π(Θ(1) upd · x(t) upd + ΘREC · v(t − 1)) vraw(t) = τ(Θ(1) · x(t) + ΘREC · v(t − 1) O vupd(t)) v(t) = vrst(t) O v(t − 1) + (1 − vrst(t)) o vraw(t) Figure 2: GRU recurrence equations. vrst(t) and vupd(t) are the activation vectors of the ”reset” and ”update” gates, respectively, and π(·) is the logistic sigmoid function. . Training is also performed in the same way except that we found more beneficial to convergence speed to optimize using Adam (Kingma and Ba, 2014) 7 rather than AdaDelta. In principle we could also extend the Fragment RNN-RM into a Fragment GRU-RM, but we did not investigate that model in this work. 4 Experiments We performed German-to-English prereordering experiments with Base RNN-RM (both unlexicalized and lexicalized), Fragment RNN-RM and Base GRU-RM. In order to validate the experimental results on a different language pair, we additionally performed an Italian-to-English prereordering experiment with the Base GRURM, after assessing that this was the model that obtained the largest improvement on German-to-English. 4.1 Setup The Ge</context>
</contexts>
<marker>Kingma, Ba, 2014</marker>
<rawString>Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1080" citStr="Koehn et al., 2007" startWordPosition="152" endWordPosition="155">tences in an order which resembles that of the target language. We propose a class of recurrent neural models which exploit source-side dependency syntax features to reorder the words into a target-like order. We evaluate these models on the German-to-English and Italian-toEnglish language pairs, showing significant improvements over a phrasebased Moses baseline. We also compare with state of the art German-toEnglish pre-reordering rules, showing that our method obtains similar or better results. 1 Introduction Statistical machine translation is typically performed using phrase-based systems (Koehn et al., 2007). These systems can usually produce accurate local reordering but they have difficulties dealing with the long-distance reordering that tends to occur between certain language pairs (Birch et al., 2008). The quality of phrase-based machine translation can be improved by reordering the words in each sentence of source-side of the parallel training corpus in a ”target-like” order and then applying the same transformation as a pre-processing step to input strings during execution. When the source-side sentences can be accurately parsed, pre-reordering can be performed using hand-coded rules. This</context>
<context position="8767" citStr="Koehn et al., 2007" startWordPosition="1447" endWordPosition="1450">n’t have access to a reordering ”oracle”, but for sentences pairs in a parallel corpus we can compute heuristic ”pseudo-oracle” reference permutations of the source sentences from word-alignments. Following (Al-Onaizan and Papineni, 2006), (Tromble and Eisner, 2009), (Visweswariah et al., 2011), (Navratil et al., 2012), we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as implemented in GIZA++ (Och et al., 1999) and then we combine them into a symmetrical word alignment using the ”grow-diag-final-and” heuristic implemented in Moses (Koehn et al., 2007). Given the symmetric word-aligned corpus, we assign to each source-side word an integer index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following aligned word) and finally we perform a stable sort of source-side words according to this index. 2.3 Reordering example Consider the segment of a German sentence shown in fig. 1. The English-reordered segment ”die W¨ahrungsreserven anfangs lediglich dienen sollten zur Verteidigung” corresponds to the English: ”the reserve assets were originally intended to provide protection”. I</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand. AAMT, AAMT.</location>
<contexts>
<context position="22537" citStr="Koehn, 2005" startWordPosition="3798" endWordPosition="3799"> a Fragment GRU-RM, but we did not investigate that model in this work. 4 Experiments We performed German-to-English prereordering experiments with Base RNN-RM (both unlexicalized and lexicalized), Fragment RNN-RM and Base GRU-RM. In order to validate the experimental results on a different language pair, we additionally performed an Italian-to-English prereordering experiment with the Base GRURM, after assessing that this was the model that obtained the largest improvement on German-to-English. 4.1 Setup The German-to-English baseline phrasebased system was trained on the Europarl v7 corpus (Koehn, 2005). We randomly split it in a 1,881,531 sentence pairs training set, a 2,000 sentence pairs development set (used for tuning) and a 2,000 sentence pairs test set. The English language model was trained on the English side of the parallel corpus augmented with a corpus of sentences from AP News, for a total of 22,891,001 sentences. The baseline system is phrase-based Moses in a default configuration with maximum distortion distance equal to 6 and lexicalized reordering enabled. Maximum phrase size is 7with learning rate 2 · 10−5 and all the other hyperparameters equal to the default values in the</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-side classifier preordering for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP ’13).</booktitle>
<contexts>
<context position="2206" citStr="Lerner and Petrov, 2013" startWordPosition="324" endWordPosition="327">e sentences can be accurately parsed, pre-reordering can be performed using hand-coded rules. This approach has been successfully applied to German-to-English (Collins et al., 2005) and other languages. The main issue with it is that these rules must be designed for each specific language pair, which requires considerable linguistic expertise. Fully statistical approaches, on the other hand, learn the reordering relation from word alignments. Some of them learn reordering rules on the constituency (Dyer and Resnik, 2010) (Khalilov and Fonollosa, 2011) or projective dependency (Genzel, 2010), (Lerner and Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasting reordering as a combinatorial optimization problem (Tromble and Eisner, 2009), (Visweswariah et</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP ’13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Valerio Miceli Barone</author>
<author>Giuseppe Attardi</author>
</authors>
<title>Pre-reordering for machine translation using transition-based walks on dependency parse trees.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>164--169</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4461" citStr="Barone and Attardi, 2013" startWordPosition="666" endWordPosition="669">rd order as a pre-processing step for phrase-based machine translation, obtaining significant improvements over the unreordered baseline system and quality comparable to the handcoded rules introduced by Collins et al. (2005). We also applied our model to Italianto-English pre-reordering, obtaining a considerable improvement over the unreordered baseline. 2 Reordering as a walk on a dependency tree In order to describe the non-local reordering phenomena that can occur between language pairs such as German-to-English and Italianto-English, we introduce a reordering framework similar to (Miceli Barone and Attardi, 2013), based on a graph walk of the dependency parse tree of the source sentence. This framework doesn’t restrict the parse tree to be projective, and allows the generation of arbitrary permutations. Let f ≡ (f1, f2, . . . , fLf) be a source sentence, annotated by a rooted dependency parse tree: ∀j ∈ 1, ... , Lf, hj ≡ PARENT(j) We define a walker process that walks from word to word across the edges of the parse tree, and at each steps optionally emits the current word, with the constraint that each word must be eventually emitted exactly once. Therefore, the final string of emitted words f0 is a p</context>
<context position="10873" citStr="Barone and Attardi (2013)" startWordPosition="1787" endWordPosition="1790">t. Go down to ”dienen”, down to ”zur” emit it, go down to ”Verteidigung” and emit it. Correct reordering of this segment would be difficult both for a phrase-based system (since the words are further apart than both the typical maximum distortion distance and maximum phrase length) and for a syntaxbased system (due to the presence of nonprojectivity and non-tree-locality). 848 Figure 1: Section of the dependency parse tree of a German sentence. 3 Recurrent Neural Network reordering models Given the reordering framework described above, we could try to directly predict the executions as Miceli Barone and Attardi (2013) attempted with their version of the framework. However, the executions of a given sentence can have widely different lengths, which could make incremental inexact decoding such as beam search difficult due to the need to prune over partial hypotheses that have different numbers of emitted words. Therefore, we decided to investigate a different class of models which have the property that state transition happen only in correspondence with word emission. This enables us to leverage the technology of incremental language models. Using language models for reordering is not something new (Feng et</context>
<context position="26539" citStr="Barone and Attardi (2013)" startWordPosition="4431" endWordPosition="4434">e shown in fig. 3 (German) and fig. 4 (Italian), effects on translation quality are shown in fig. 5 (German-to-English) and fig. 6 (Italian-to-English)9. 4.3 Discussion and analysis All our German-to-English models significantly improved over the phrase-based baseline, performing as well as or almost as well as (Collins et al., 2005), which is an interesting result since our models doesn’t require any specific linguistic expertise. Surprisingly, the lexicalized version of Base RNN-RM performed worse than the unlexi9Although the baseline systems were trained on the same datasets used in Miceli Barone and Attardi (2013), the results are different since we used a different version of Moses calized one. This goes contrary to expectation as neural language models are usually lexicalized and in fact often use nothing but lexical features. The unlexicalized Fragment RNN-RM was quite accurate but very expensive both during training and decoding, thus it may not be practical. The unlexicalized Base GRU-RM performed very well, especially on the Europarl dataset (where all the scores are much higher than the other datasets) and it never performed significantly worse than the unlexicalized Fragment RNN-RM which is muc</context>
</contexts>
<marker>Barone, Attardi, 2013</marker>
<rawString>Antonio Valerio Miceli Barone and Giuseppe Attardi. 2013. Pre-reordering for machine translation using transition-based walks on dependency parse trees. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 164–169, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045– 1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Navratil</author>
<author>Karthik Visweswariah</author>
<author>Ananthakrishnan Ramanathan</author>
</authors>
<title>A comparison of syntactic reordering methods for english-german machine translation.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>2043--2058</pages>
<contexts>
<context position="8468" citStr="Navratil et al., 2012" startWordPosition="1401" endWordPosition="1404">tted. Suppose instead that we could perform reordering according to an ”oracle”. The executions of our automaton corresponding to these permutations will in general contain unforced UP actions. We define these actions, and the execution fragments that exhibit them, as non-tree-local. In practice we don’t have access to a reordering ”oracle”, but for sentences pairs in a parallel corpus we can compute heuristic ”pseudo-oracle” reference permutations of the source sentences from word-alignments. Following (Al-Onaizan and Papineni, 2006), (Tromble and Eisner, 2009), (Visweswariah et al., 2011), (Navratil et al., 2012), we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as implemented in GIZA++ (Och et al., 1999) and then we combine them into a symmetrical word alignment using the ”grow-diag-final-and” heuristic implemented in Moses (Koehn et al., 2007). Given the symmetric word-aligned corpus, we assign to each source-side word an integer index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following aligned word) and finally we perform a stable sort of source-side words according </context>
<context position="24831" citStr="Navratil et al., 2012" startWordPosition="4151" endWordPosition="4154">ses system with pre-reordering performed by Collins et al. (2005) rules, implemented by Howlett and Dras (2011). Constituency parsing for Collins et al. (2005) rules was performed with the Berkeley parser (Petrov et al., 2006). For Italian-to-English we did not compare with a hand-coded reordering system as we are not aware of any strong pre-reordering baseline for this language pair. For our experiments, we extract approxi8Corriere.it and Asianews.it 852 mately 300,000 sentence pairs from the Moses training set based on a heuristic confidence measure of word-alignment quality (Huang, 2009), (Navratil et al., 2012). We randomly removed 2,000 sentences from this filtered dataset to form a validation set for early stopping, the rest were used for training the prereordering models. 4.2 Results The hidden state size s of the RNNs was set to 100 while it was set to 30 for the GRU model, validation was performed every 2,000 training examples. After 50 consecutive validation rounds without improvement, training was stopped and the set of training parameters that resulted in the lowest validation crossentropy were saved. Training took approximately 1.5 days for the unlexicalized Base RNN-RM, 2.5 days for the le</context>
</contexts>
<marker>Navratil, Visweswariah, Ramanathan, 2012</marker>
<rawString>Jiri Navratil, Karthik Visweswariah, and Ananthakrishnan Ramanathan. 2012. A comparison of syntactic reordering methods for english-german machine translation. In COLING, pages 2043–2058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="8624" citStr="Och et al., 1999" startWordPosition="1426" endWordPosition="1429">ral contain unforced UP actions. We define these actions, and the execution fragments that exhibit them, as non-tree-local. In practice we don’t have access to a reordering ”oracle”, but for sentences pairs in a parallel corpus we can compute heuristic ”pseudo-oracle” reference permutations of the source sentences from word-alignments. Following (Al-Onaizan and Papineni, 2006), (Tromble and Eisner, 2009), (Visweswariah et al., 2011), (Navratil et al., 2012), we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as implemented in GIZA++ (Och et al., 1999) and then we combine them into a symmetrical word alignment using the ”grow-diag-final-and” heuristic implemented in Moses (Koehn et al., 2007). Given the symmetric word-aligned corpus, we assign to each source-side word an integer index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following aligned word) and finally we perform a stable sort of source-side words according to this index. 2.3 Reordering example Consider the segment of a German sentence shown in fig. 1. The English-reordered segment ”die W¨ahrungsreserven anfang</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, Hermann Ney, et al. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24435" citStr="Petrov et al., 2006" startWordPosition="4092" endWordPosition="4095">spaper websites8, totaling 3,081,700 sentence pairs, which were split into a 3,075,777 sentence pairs phrasetable training corpus, a 3,923 sentence pairs tuning corpus, and a 2,000 sentence pairs test corpus. Non-projective dependency parsing for our models, both for German and Italian was performed with the DeSR transition-based parser (Attardi, 2006). We also trained a German-to-English Moses system with pre-reordering performed by Collins et al. (2005) rules, implemented by Howlett and Dras (2011). Constituency parsing for Collins et al. (2005) rules was performed with the Berkeley parser (Petrov et al., 2006). For Italian-to-English we did not compare with a hand-coded reordering system as we are not aware of any strong pre-reordering baseline for this language pair. For our experiments, we extract approxi8Corriere.it and Asianews.it 852 mately 300,000 sentence pairs from the Moses training set based on a heuristic confidence measure of word-alignment quality (Huang, 2009), (Navratil et al., 2012). We randomly removed 2,000 sentences from this filtered dataset to form a validation set for early stopping, the rest were used for training the prereordering models. 4.2 Results The hidden state size s </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
</authors>
<title>Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel Varga.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Steinberger, Pouliquen, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Dniel Varga. 2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>1007--1016</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2788" citStr="Tromble and Eisner, 2009" startWordPosition="414" endWordPosition="417">y (Genzel, 2010), (Lerner and Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasting reordering as a combinatorial optimization problem (Tromble and Eisner, 2009), (Visweswariah et al., 2011), can learn to generate in principle arbitrary permutations, but they can only make use of minimal syntactic information (part-of-speech tags) and therefore can’t exploit the potentially valuable structural syntactic information provided by a parser. In this work we propose a class of reordering models which attempt to close this gap by 846 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 846–856, Beijing, China, July 26-31, 2015. c�2015 Associati</context>
<context position="8414" citStr="Tromble and Eisner, 2009" startWordPosition="1393" endWordPosition="1396">er leaves a subtree before all its vertices have been emitted. Suppose instead that we could perform reordering according to an ”oracle”. The executions of our automaton corresponding to these permutations will in general contain unforced UP actions. We define these actions, and the execution fragments that exhibit them, as non-tree-local. In practice we don’t have access to a reordering ”oracle”, but for sentences pairs in a parallel corpus we can compute heuristic ”pseudo-oracle” reference permutations of the source sentences from word-alignments. Following (Al-Onaizan and Papineni, 2006), (Tromble and Eisner, 2009), (Visweswariah et al., 2011), (Navratil et al., 2012), we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as implemented in GIZA++ (Och et al., 1999) and then we combine them into a symmetrical word alignment using the ”grow-diag-final-and” heuristic implemented in Moses (Koehn et al., 2007). Given the symmetric word-aligned corpus, we assign to each source-side word an integer index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following aligned word) and finally we</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 1007–1016, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Rajakrishnan Rajkumar</author>
<author>Ankur Gandhe</author>
<author>Ananthakrishnan Ramanathan</author>
<author>Jiri Navratil</author>
</authors>
<title>A word reordering model for improved machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>486--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2817" citStr="Visweswariah et al., 2011" startWordPosition="418" endWordPosition="421">d Petrov, 2013) parse trees of source sentences. The permutations that these methods can learn can be generally non-local (i.e. high distance) on the sentences but local (parent-child or sibling-sibling swaps) on the parse trees. Moreover, constituency or projective dependency trees may not be the ideal way of representing the syntax of nonanalytic languages such as German or Italian, which could be better described using non-projective dependency trees (Bosco and Lombardo, 2004). Other methods, based on recasting reordering as a combinatorial optimization problem (Tromble and Eisner, 2009), (Visweswariah et al., 2011), can learn to generate in principle arbitrary permutations, but they can only make use of minimal syntactic information (part-of-speech tags) and therefore can’t exploit the potentially valuable structural syntactic information provided by a parser. In this work we propose a class of reordering models which attempt to close this gap by 846 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 846–856, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguist</context>
<context position="8443" citStr="Visweswariah et al., 2011" startWordPosition="1397" endWordPosition="1400">ll its vertices have been emitted. Suppose instead that we could perform reordering according to an ”oracle”. The executions of our automaton corresponding to these permutations will in general contain unforced UP actions. We define these actions, and the execution fragments that exhibit them, as non-tree-local. In practice we don’t have access to a reordering ”oracle”, but for sentences pairs in a parallel corpus we can compute heuristic ”pseudo-oracle” reference permutations of the source sentences from word-alignments. Following (Al-Onaizan and Papineni, 2006), (Tromble and Eisner, 2009), (Visweswariah et al., 2011), (Navratil et al., 2012), we generate word alignments in both the source-to-target and the target-to-source directions using IBM model 4 as implemented in GIZA++ (Och et al., 1999) and then we combine them into a symmetrical word alignment using the ”grow-diag-final-and” heuristic implemented in Moses (Koehn et al., 2007). Given the symmetric word-aligned corpus, we assign to each source-side word an integer index corresponding to the position of the leftmost target-side word it is aligned to (attaching unaligned words to the following aligned word) and finally we perform a stable sort of sou</context>
</contexts>
<marker>Visweswariah, Rajkumar, Gandhe, Ramanathan, Navratil, 2011</marker>
<rawString>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 486–496, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701.</title>
<date>2012</date>
<contexts>
<context position="14708" citStr="Zeiler, 2012" startWordPosition="2466" endWordPosition="2467">ntropy of the model w.r.t. the reference permutations in the training set. Gradients can be efficiently computed using backpropagation through time (BPTT). In practice we used the following training architecture: Stochastic gradient descent, with each training pair (f, f0) considered as a single minibatch for updating purposes. Gradients computed using the automatic differentiation facilities of Theano (Bergstra et al., 2010) (which implements a generalized BPTT). No truncation is used. L2-regularization 3. Learning rates dynamically adjusted per scalar parameter using the AdaDelta heuristic (Zeiler, 2012). Gradient clipping heuristic to prevent the ”exploding gradient” problem (Graves, 2013). Early stopping w.r.t. a validation set to prevent overfitting. Uniform random initialization for parameters other than the recurrent parameter matrix ΘREC. Random initialization with echo state property for ΘREC, with contraction coefficient v = 0.99 (Jaeger, 2001), (Gallicchio and Micheli, 2011). Training time complexity is O(L2f) per sentence, which could be reduced to O(Lf ) using truncated BTTP at the expense of update accuracy and hence convergence speed. Space complexity is O(Lf ) per sentence. 3.1.</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D Zeiler. 2012. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>