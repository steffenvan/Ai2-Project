<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000296">
<title confidence="0.996844">
Automated Collocation Suggestion for Japanese Second Language
Learners
</title>
<author confidence="0.988417">
Lis W. Kanashiro Pereira Erlyn Manguilimotan Yuji Matsumoto
</author>
<affiliation confidence="0.9997385">
Nara Institute of Science and Technology
Department of Information Science
</affiliation>
<email confidence="0.994028">
{lis-k, erlyn-m, matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995471875">
This study addresses issues of Japanese lan-
guage learning concerning word combinations
(collocations). Japanese learners may be able
to construct grammatically correct sentences,
however, these may sound “unnatural”. In this
work, we analyze correct word combinations
using different collocation measures and
word similarity methods. While other methods
use well-formed text, our approach makes use
of a large Japanese language learner corpus for
generating collocation candidates, in order to
build a system that is more sensitive to con-
structions that are difficult for learners. Our
results show that we get better results com-
pared to other methods that use only well-
formed text.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993265">
Automated grammatical error correction is
emerging as an interesting topic of natural lan-
guage processing (NLP). However, previous re-
search in second language learning are focused
on restricted types of learners’ errors, such as
article and preposition errors (Gamon, 2010;
Rozovskaya and Roth, 2010; Tetreault et al.,
2010). For example, research for Japanese lan-
guage mainly focuses on Japanese case particles
(Suzuki and Toutanova, 2006; Oyama and
Matsumoto, 2010). It is only recently that NLP
research has addressed issues of collocation er-
rors.
Collocations are conventional word combina-
tions in a language. In Japanese, ocha wo ireru
“ れる1 [to make tea]” and yume wo
miru ” V;jq22 [to have a dream]” are exam-
ples of collocations. Even though their accurate
use is crucial to make communication precise
and to sound like a native speaker, learning them
</bodyText>
<page confidence="0.4783">
1 lit. to put in tea
2 lit. to see a dream
</page>
<bodyText confidence="0.9998975">
is one of the most difficult tasks for second lan-
guage learners. For instance, the Japanese collo-
cation yume wo miru [lit. to see a dream] is un-
predictable, at least, for native speakers of Eng-
lish, because its constituents are different from
those in the Japanese language. A learner might
create the unnatural combination yume wo suru,
using the verb suru (a general light verb meaning
“do” in English) instead of miru “to see”.
In this work, we analyze various Japanese
corpora using a number of collocation and word
similarity measures to deduce and suggest the
best collocations for Japanese second language
learners. In order to build a system that is more
sensitive to constructions that are difficult for
learners, we use word similarity measures that
generate collocation candidates using a large
Japanese language learner corpus. By employing
this approach, we could obtain a better result
compared to other methods that use only well-
formed text.
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce related work on
collocation error correction. Section 3 explains
our method, based on word similarity and
association measures, for suggesting collocations.
In Section 4, we describe different word
similarity and association measures, as well as
the corpora used in our experiments. The exper-
imental setup and the results are described in
Sections 5 and 6, respectively. Section 7 points
out the future directions for our research.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999852">
Collocation correction currently follows a similar
approach used in article and preposition correc-
tion. The general strategy compares the learner&apos;s
word choice to a confusion set generated from
well-formed text during the training phase. If
one or more alternatives are more appropriate to
the context, the learner&apos;s word is flagged as an
error and the alternatives are suggested as correc-
tions. To constrain the size of the confusion set,
</bodyText>
<page confidence="0.983405">
52
</page>
<note confidence="0.512713">
Proceedings of the ACL Student Research Workshop, pages 52–58,
</note>
<page confidence="0.362881">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.999885159090909">
similarity measures are used. To rank the best
candidates, the strength of association in the
learner’s construction and in each of the
generated alternative construction are measured.
For example, Futagi et al. (2008) generated
synonyms for each candidate string using
WordNet and Roget’s Thesaurus and used the
rank ratio measure to score them by their
semantic similarity. Liu et al. (2009) also used
WordNet to generate synonyms, but used
Pointwise Mutual Information as association
measure to rank the candidates. Chang et al.
(2008) used bilingual dictionaries to derive
collocation candidates and used the log-
likelihood measure to rank them. One drawback
of these approaches is that they rely on resources
of limited coverage, such as dictionaries, thesau-
rus or manually constructed databases to gener-
ate the candidates. Other studies have tried to
offer better coverage by automatically deriving
paraphrases from parallel corpora (Dahlmeier
and Ng, 2011), but similar to Chang et al. (2008),
it is essential to identify the learner’s first lan-
guage and to have bilingual dictionaries and par-
allel corpora for every first language (L1) in or-
der to extend the resulting system. Another prob-
lem is that most research does not actually take
the learners&apos; tendency of collocation errors into
account; instead, their systems are trained only
on well-formed text corpora. Our work follows
the general approach, that is, uses similarity
measures for generating the confusion set and
association measures for ranking the best candi-
dates. However, instead of using only well-
formed text for generating the confusion set, we
use a large learner corpus created by crawling the
revision log of a language learning social net-
working service (SNS), Lang-83. Another work
that also uses data from Lang-8 is Mizumoto et
al. (2011), which uses Lang-8 in creating a large-
scale Japanese learner’s corpus. The biggest ben-
efit of using such kind of data is that we can ob-
tain in large scale pairs of learners’ sentences and
their corrections assigned by native speakers.
</bodyText>
<sectionHeader confidence="0.978866333333333" genericHeader="method">
3 Combining Word Similarity and As-
sociation Measures to Suggest Collo-
cations
</sectionHeader>
<bodyText confidence="0.98219225">
In our work, we focus on suggestions for noun
and verb collocation errors in “noun wo verb
(noun-を-verb)” constructions, where noun is the
direct object of verb. Our approach consists of
</bodyText>
<footnote confidence="0.889755">
3www.lang-8.com
</footnote>
<bodyText confidence="0.987732892857143">
three steps: 1) for each extracted tuple in the se-
cond learner’s composition, we created a set of
candidates by substituting words generated using
word similarity algorithms; 2) then, we measured
the strength of association in the writer’s phrase
and in each generated candidate phrase using
association measures to compute collocation
scores; 3) the highest ranking alternatives are
suggested as corrections. In our evaluation, we
checked if the correction given in the learner
corpus matches one of the suggestions given by
the system. Figure 1 illustrates the method used
in this study.
Figure 1 Word Similarity and Association
Measures combination method for suggesting
col-locations.
We considered only the tuples that contain
noun or verb error. A real application, however,
should also deal with error detection. For each
example of the construction on the writer’s text,
the system should create the confusion set with
alternative phrases, measure the strength of asso-
ciation in the writer’s phrase and in each gener-
ated alternative phrase and flag as error only if
the association score of the writer’s phrase is
lower than one or more of the alternatives gener-
ated and suggest the higher-ranking alternatives
as corrections.
</bodyText>
<sectionHeader confidence="0.890934" genericHeader="method">
4 Approaches to Word Similarity and
Word Association Strength
</sectionHeader>
<subsectionHeader confidence="0.99977">
4.1 Word Similarity
</subsectionHeader>
<bodyText confidence="0.99989225">
Similarity measures are used to generate the col-
location candidates that are later ranked using
association measures. In our work, we used the
following three measures to analyze word simila-
</bodyText>
<page confidence="0.997534">
53
</page>
<table confidence="0.999549714285714">
Confusion Set
Word t;6 �NA1;6 mlvb;6 1if;6 aく A_ ��;6 �;6 W-)
Meaning do accept begin make write say eat do carry
Word tAL t-AL J-Zt-AL �3k +L k m&apos;0- P!k ���
Meaning building beer draft beer money bill amount scenery fee building
of
money
</table>
<tableCaption confidence="0.993741">
Table 1 Confusion Set example for the words suru (する) and biru (ビル)
</tableCaption>
<table confidence="0.697301">
aく nL _A1;6
write read put on
qaZ;�- 15 11 8
diary
</table>
<tableCaption confidence="0.51044">
Table 2 Context of a particular noun represented
</tableCaption>
<bodyText confidence="0.994980253164557">
as a co-occurrence vector
rity: 1) thesaurus-based word similarity, 2) dis-
tributional similarity and 3) confusion set derived
from learner corpus. The first two measures gen-
erate the collocation candidates by finding words
that are analogous to the writer’s choice, a com-
mon approach used in the related work on
collocation error correction (Liu et al., 2009;
Östling and O. Knutsson, 2009; Wu et al., 2010)
and the third measure generates the candidates
based on the corrections given by native speakers
in the learner corpus.
Thesaurus-based word similarity: The intui-
tion of this measure is to check if the given
words have similar glosses (definitions). Two
words are considered similar if they are near
each other in the thesaurus hierarchy (have a path
within a pre-defined threshold length).
Distributional Similarity: Thesaurus-based
methods produce weak recall since many words,
phrases and semantic connections are not cov-
ered by hand-built thesauri, especially for verbs
and adjectives. As an alternative, distributional
similarity models are often used since it gives
higher recall. On the other hand, distributional
similarity models tend to have lower precision
(Jurafsky et al., 2009), because the candidate set
is larger. The intuition of this measure is that two
words are similar if they have similar word con-
texts. In our task, context will be defined by
some grammatical dependency relation, specifi-
cally, ‘object-verb’ relation. Context is repre-
sented as co-occurrence vectors that are based on
syntactic dependencies. We are interested in
computing similarity of nouns and verbs and
hence the context of a particular noun is a vector
of verbs that are in an object relation with that
noun. The context of a particular verb is a vector
ご飯を ラーメンを カレーを
rice ramen noodle soup curry
食べる 164 53 39
eat
Table 3 Context of a particular noun represented
as a co-occurrence vector
of nouns that are in an object relation with that
verb. Table 2 and Table 3 show examples of part
of co-occurrence vectors for the noun “qaZ [dia-
ry]” and the verb “食べる [eat]”, respectively.
The numbers indicate the co-occurrence frequen-
cy in the BCCWJ corpus (Maekawa, 2008). We
computed the similarity between co-occurrence
vectors using different metrics: Cosine Similarity,
Dice coefficient (Curran, 2004), Kullback-
Leibler divergence or KL divergence or relative
entropy (Kullback and Leibler, 1951) and the
Jenson-Shannon divergence (Lee, 1999).
Confusion Set derived from learner corpus:
In order to build a module that can “guess”
common construction errors, we created a confu-
sion set using Lang-8 corpus. Instead of generat-
ing words that have similar meaning to the learn-
er’s written construction, we extracted all the
possible noun and verb corrections for each of
the nouns and verbs found in the data. Table 1
shows some examples extracted. For instance,
the confusion set of the verb suru “する [to do]”
is composed of verbs such as ukeru “受ける [to
accept]”, which does not necessarily have similar
meaning with suru. The confusion set means that
in the corpus, suru was corrected to either one of
these verbs, i.e., when the learner writes the verb
suru, he/she might actually mean to write one of
the verbs in the confusion set. For the noun
biru“ビル [building]”, the learner may have, for
example, misspelled the word baru “ ビ ー ル
[beer]”, or may have got confused with the trans-
lation of the English words bill (“お金[money]”,
“札 [bill]”, “金額 [amount of money]”, “料金
[fee]”) or view (“景色 [scenery]”) to Japanese.
</bodyText>
<page confidence="0.991433">
54
</page>
<subsectionHeader confidence="0.813994">
4.2 Word Association Strength
</subsectionHeader>
<bodyText confidence="0.985379647058823">
After generating the collocation candidates using
word similarity, the next step is to identify the
“true collocations” among them. Here, the
association strength was measured, in such a way
that word pairs generated by chance from the
sampling process can be excluded. An
association measure assigns an association score
to each word pair. High scores indicate strong
association, and can be used to select the “true
collocations”. We adopted the Weighted Dice
coefficient (Kitamura and Matsumoto, 1997) as
our association measurement. We also tested us-
ing other association measures (results are omit-
ted): Pointwise Mutual Information (Church and
Hanks, 1990), log-likelihood ratio (Dunning,
1993) and Dice coefficient (Smadja et al., 1996),
but Weighted Dice performed best.
</bodyText>
<sectionHeader confidence="0.998133" genericHeader="method">
5 Experiment setup
</sectionHeader>
<bodyText confidence="0.9999797">
We divided our experiments into two parts: verb
suggestion and noun suggestion. For verb sug-
gestion, given the learners’ “noun wo verb” con-
struction, our focus is to suggest “noun wo verb”
collocations with alternative verbs other than the
learner’s written verb. For noun suggestion, giv-
en the learners’ “noun wo verb” construction, our
focus is to suggest “noun wo verb” collocations
with alternative nouns other than the learner’s
written noun.
</bodyText>
<subsectionHeader confidence="0.988279">
5.1 Data Set
</subsectionHeader>
<bodyText confidence="0.859245384615384">
For computing word similarity and association
scores for verb suggestion, the following re-
sources were used:
1) Bunrui Goi Hyo Thesaurus (The National
Institute for Japanese Language, 1964): a Japa-
nese thesaurus, which has a vocabulary size of
around 100,000 words, organized into 32,600
unique semantic classes. This thesaurus was used
to compute word similarity, taking the words that
are in the same subtree as the candidate word. By
subtree, we mean the tree with distance 2 from
the leaf node (learner’s written word) doing the
pre-order tree traversal.
</bodyText>
<listItem confidence="0.837661">
2) Mainichi Shimbun Corpus (Mainichi
Newspaper Co., 1991): one of the ma-
jor newspapers in Japan that provides raw text of
newspaper articles used as linguistic resource.
One year data (1991) were used to extract the
“noun wo verb” tuples to compute word similari-
ty (using cosine similarity metric) and colloca-
tion scores. We extracted 224,185 tuples com-
posed of 16,781 unique verbs and 37,300 unique
nouns.
3) Balanced Corpus of Contemporary Written
</listItem>
<bodyText confidence="0.946889615384615">
Japanese, BCCWJ Corpus (Maekawa, 2008):
composed of one hundred million words, por-
tions of this corpus used in our experiments in-
clude magazine, newspaper, textbooks, and blog
data4. Incorporating a variety of topics and styles
in the training data helps minimize the domain
gap problem between the learner’s vocabulary
and newspaper vocabulary found in the Mainichi
Shimbun data. We extracted 194,036 “noun wo
verb” tuples composed of 43,243 unique nouns
and 18,212 unique verbs. These data are neces-
sary to compute the word similarity (using cosine
similarity metric) and collocation scores.
</bodyText>
<listItem confidence="0.982460666666667">
4) Lang-8 Corpus: Consisted of two year data
(2010 and 2011):
A) Year 2010 data, which contain
</listItem>
<bodyText confidence="0.990024692307692">
1,288,934 pairs of learner’s sentence and its
correction, was used to: i) Compute word sim-
ilarity (using cosine similarity metric) and col-
location scores: We took out the learners’ sen-
tences and used only the corrected sentences.
We extracted 163,880 “noun wo verb” tuples
composed of 38,999 unique nouns and 16,086
unique verbs. ii) Construct the confusion set
(explained in Section 4.1): We constructed the
confusion set for all the 16,086 verbs and
38,999 nouns that appeared in the data.
B) Year 2011 data were used to con-
struct the test set (described in Section 5.2).
</bodyText>
<subsectionHeader confidence="0.999876">
5.2 Test set selection
</subsectionHeader>
<bodyText confidence="0.999684857142857">
We used Lang-8 (2011 data) for selecting our
test set. For the verb suggestion task, we extract-
ed all the “noun wo verb” tuples with incorrect
verbs and their correction. From the tuples ex-
tracted, we selected the ones where the verbs
were corrected to the same verb 5 or more times
by the native speakers. Similarly, for the noun
suggestion task, we extracted all the “noun wo
verb” tuples with incorrect nouns and their cor-
rection. There are cases where the learner’s con-
struction sounds more acceptable than its correc-
tion, cases where in the corpus, they were cor-
rected due to some contextual information. For
our application, since we are only considering
4 Although the language used in blog data is usually
more informal than the one used in newspaper,
maganizes, etc., and might contain errors like spelling
and grammar, collocation errors are much less fre-
quent compared to spelling and grammar errors, since
combining words appropriately is one the vital com-
petencies of a native speaker of a language.
</bodyText>
<page confidence="0.996915">
55
</page>
<bodyText confidence="0.9999445">
the noun, particle and verb that the learner wrote,
there was a need to filter out such contextually
induced corrections. To solve this problem, we
used the Weighted Dice coefficient to compute
the association strength between the noun and all
the verbs, filtering out the pairs where the learn-
er’s construction has a higher score than the cor-
rection. After applying those conditions, we ob-
tained 185 tuples for the verb suggestion test set
and 85 tuples for the noun suggestion test set.
</bodyText>
<subsectionHeader confidence="0.990401">
5.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999535">
We compared the verbs in the confusion set
ranked by collocation score suggested by the sys-
tem with the human correction verb and noun in
the Lang-8 data. A match would be counted as a
true positive (tp). A false negative (fn) occurs
when the system cannot offer any suggestion.
The metrics we used for the evaluation are:
precision, recall and the mean reciprocal rank
(MRR). We report precision at rank k, k=1, 5,
computing the rank of the correction when a true
positive occurs. The MRR was used to assess
whether the suggestion list contains the correc-
tion and how far up it is in the list. It is calculat-
ed as follows:
</bodyText>
<equation confidence="0.962566333333333">
tp fn
MRR = 1 � 1 (1)
N i1 rank(i)
</equation>
<bodyText confidence="0.873646">
where N is the size of the test set. If the system
did not return the correction for a test instance,
1
to zero. Recall rate is calculated
rank(i)
with the formula below:
</bodyText>
<sectionHeader confidence="0.999866" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99998735483871">
Table 4 shows the ten models derived from com-
bining different word similarity measures and the
Weighted Dice measure as association measure,
using different corpora. In this table, for instance,
we named M1 the model that uses thesaurus for
computing word similarity and uses Mainichi
Shimbun corpus when computing collocation
scores using the association measure adopted,
Weighted Dice. M2 uses Mainichi Shimbun cor-
pus for computing both word similarity and col-
location scores. M10 computes word similarity
using the confusing set from Lang-8 corpus and
uses BCCWJ and Lang-8 corpus when compu-
ting collocation scores.
Considering that the size of the candidate set
generated by different word similarity measures
vary considerably, we limit the size of the confu-
sion set to 270 for verbs and 160 for nouns,
which correspond to the maximum values of the
confusion set size for nouns and verbs when us-
ing Lang-8 for generating the candidate set. Set-
ting up a threshold was necessary since the size
of the candidate set generated when using Distri-
butional Similarity methods may be quite large,
affecting the system performance. When compu-
ting Distributional Similarity, scores are also as-
signed to each candidate, thus, when we set up a
threshold value n, we consider the list of n can-
didates with highest scores. Table 4 reports the
precision of the k-best suggestions, the recall rate
and the MRR for verb and noun suggestion.
</bodyText>
<subsectionHeader confidence="0.996242">
6.1 Verb Suggestion
</subsectionHeader>
<bodyText confidence="0.999812515151515">
Table 4 shows that the model using thesaurus
(M1) achieved the highest precision rate among
the other models; however, it had the lowest re-
call. The model could suggest for cases where
the wrong verb written by the learner and the
correction suggested in Lang-8 data have similar
meaning, as they are near to each other in the
thesaurus hierarchy. However, for cases where
the wrong verb written by the learner and the
correction suggested in Lang-8 data do not have
similar meaning, M1 could not suggest the cor-
rection.
In order to improve the recall rate, we generat-
ed models M2-M6, which use distributional simi-
larity (cosine similarity) and also use corpora
other than Mainichi Shimbun corpus to minimize
the domain gap problem between the learner’s
vocabulary and the newspaper vocabulary found
in the Mainichi Shimbun data. The recall rate
improved significantly but the precision rate de-
creased. In order to compare it with other distri-
butional similarity metrics (Dice, KL-Divergence
and Jenson-Shannon Divergence) and with the
method that uses Lang-8 for generating the con-
fusion set, we chose the model with the highest
recall value as baseline, which is the one that
uses BCCWJ and Lang-8 (M6) and generated
other models (M7-M10). The best MRR value
obtained among all the Distributional Similarity
methods was obtained by Jenson-Shannon diver-
gence. The highest recall and MRR values are
achieved when Lang-8 data were used to gener-
ate the confusion set (M10).
</bodyText>
<table confidence="0.925677105263158">
we set
tp (2)

56
Similarity used Thesaurus Cosine Similarity Dice KL Jenson- Confusion Set
for Confusion Coefficient Divergence Shannon from
Sets Divergence Lang-8
K-Best Mainichi Mainichi BCCWJ Lang-8 Mainichi BCCWJ + BCCWJ + BCCWJ + BCCWJ + BCCWJ +
Shimbun Shimbun Shimbun + Lang-8 Lang-8 Lang-8 Lang-8 Lang-8
BCCWJ
M1 M2 M3 M4 MS M6 M7 M8 M9 M10
Verb 1 0.94 0.48 0.42 0.60 0.62 0.56 0.59 0.63 0.60 0.64
Suggestion S 1 0.91 0.94 0.90 0.90 0.86 0.86 0.84 0.88 0.95
Recall 0.20 0.40 0.30 0.68 0.49 0.71 0.81 0.35 0.74 0.97
MRR 0.19 0.26 0.19 0.49 0.36 0.50 0.58 0.26 0.53 0.75
Noun 1 0.16 0.20 0.42 0.58 0.50 0.55 0.30 0.63 0.57 0.73
Suggestion S 1 0.66 0.94 0.89 1 0.91 0.83 1 0.84 0.98
Recall 0.07 0.17 0.22 0.45 0.04 0.42 0.35 0.12 0.38 0.98
MRR 0.03 0.06 0.13 0.33 0.02 0.29 0.18 0.10 0.26 0.83
</table>
<tableCaption confidence="0.9825975">
Table 4 The precision and recall rate and MRR of the Models of Word Similarity and Association
Strength method combination.
</tableCaption>
<subsectionHeader confidence="0.993596">
6.2 Noun Suggestion
</subsectionHeader>
<bodyText confidence="0.999995894736842">
Similar to the verb suggestion experiments, the
best recall and MRR values are achieved when
Lang-8 data were used to generate the confusion
set (M10).
For noun suggestion, our automatically con-
structed test set includes a number of spelling
correction cases, such as cases for the combina-
tion eat ice cream, where the learner wrote
aisukurimu wo taberu “アイスクリムを食べ
る” and the correction is aisukuramu wo taberu ”
アイスクリームを食べる”. Such phenomena
did not occur with the test set for verb suggestion.
For those cases, the fact that only spelling cor-
rection is necessary in order to have the right
collocation may also indicate that the learner is
more confident regarding the choice of the noun
than the verb. This also justifies the even lower
recall rate obtained (0.07) when using a thesau-
rus for generating the candidates
</bodyText>
<sectionHeader confidence="0.997076" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999974740740741">
We analyzed various Japanese corpora using a
number of collocation and word similarity
measures to deduce and suggest the best colloca-
tions for Japanese second language learners. In
order to build a system that is more sensitive to
constructions that are difficult for learners, we
use word similarity measures that generate collo-
cation candidates using a large Japanese lan-
guage learner corpus, instead of only using well-
formed text. By employing this approach, we
could obtain better recall and MRR values com-
pared to thesaurus based method and distribu-
tional similarity methods.
Although only noun-wo-verb construction is
examined, the model is designed to be applicable
to other types of constructions, such as adjective-
noun and adverb-noun. Another straightforward
extension is to pursue constructions with other
particles, such as “noun ga verb (subject-verb)”,
“noun ni verb (dative-verb)”, etc. In our experi-
ments, only a small context information is con-
sidered (only the noun, the particle wo (を) and
the verb written by the learner). In order to verify
our approach and to improve our current results,
considering a wider context size and other types
of constructions will be the next steps of this re-
search.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<page confidence="0.994832">
57
</page>
<bodyText confidence="0.9308155">
Special thanks to Yangyang Xi for maintaining Lang-
8.
</bodyText>
<sectionHeader confidence="0.99416" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991412745098">
Y. C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou.
2008. An automatic collocation writing assistant for
Taiwanese EFL learners: A case of corpus-based
NLP technology. Computer Assisted Language
Learning, 21(3):283–299.
K. Church, and P. Hanks. 1990. Word Association
Norms, Mutual Information and Lexicogra-
phy, Computational Linguistics, Vol. 16:1, pp. 22-
29.
J. R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
D. Dahlmeier, H. T. Ng. 2011. Correcting Semantic
Collocation Errors with L1-induced Paraphrases. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
107–117, Edinburgh, Scotland, UK, July. Associa-
tion for Computational Linguistics
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics 19.1 (Mar. 1993), 61-74.
Y. Futagi, P. Deane, M. Chodorow, and J. Tetreault.
2008. A computational approach to detecting collo-
cation errors in the writing of non-native speakers
of English. Computer Assisted Language Learning,
21, 4 (October 2008), 353-367.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing: A meta-classifier ap-
proach. In Proceedings of Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 163–
171, Los Angeles, California, June. Association for
Computational Linguistics.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing: An Introduction to Natural Lan-
guage Processing, Speech Recognition, and Com-
putational Linguistics. 2nd edition. Prentice-Hall.
K. Maekawa. 2008. Balanced corpus of contemporary
written japanese. In Proceedings of the 6th
Workshop on Asian Language Resources (ALR),
pages 101–102.
M. Kitamura, Y. Matsumoto. 1997. Automatic extrac-
tion of translation patterns in parallel corpora. In
IPSJ, Vol. 38(4), pp.108-117, April. In Japanese.
S. Kullback, R.A. Leibler. 1951. On Information and
Sufficiency. Annals of Mathematical Statis-
tics 22 (1): 79–86.
L. Lee. 1999. Measures of Distributional Similarity.
In Proc of the 37th annual meeting of the ACL,
Stroudsburg, PA, USA, 25.
A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated
suggestions for miscollocations. In Proceedings of
the NAACL HLT Workshop on Innovative Use of
NLP for Building Educational Applications, pages
47–50, Boulder, Colorado, June. Association for
Computational Linguistics.
Mainichi Newspaper Co. 1991. Mainichi Shimbun
CD-ROM 1991.
T. Mizumoto, K. Mamoru, M. Nagata, Y. Matsumoto.
2011. Mining Revision Log of Language Learning
SNS for Automated Japanese Error Correction of
Second Language Learners. In Proceedings of the
5th International Joint Conference on Natural
Language Processing, pp.147-155. Chiang Mai,
Thailand, November. AFNLP.
R. Östling and O. Knutsson. 2009. A corpus-based
tool for helping writers with Swedish collocations.
In Proceedings of the Workshop on Extracting and
Using Constructions in NLP, Nodalida, Odense,
Denmark. 70, 77.
H. Oyama and Y. Matsumoto. 2010. Automatic Error
Detection Method for Japanese Case Particles in
Japanese Language Learners. In Corpus, ICT, and
Language Education, pages 235–245.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
961–970, MIT, Massachusetts, USA, October. As-
sociation for Computational Linguistics.
F. Smadja, K. R. Mckeown, V. Hatzivassiloglou. 1996.
Translation collocations for bilingual lexicons: a
statistical approach. Computational Linguistics,
22:1-38.
H. Suzuki and K. Toutanova. 2006. Learning to Pre-
dict Case Markers in Japanese. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
ACL, pages 1049–1056 , Sydney, July. Association
for Computational Linguistics.J. Tetreault, J. Foster,
and M. Chodorow. 2010. Using parse features for
preposition selection and error detection. In Pro-
ceedings of ACL 2010 Conference Short Papers,
pages 353-358, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
The National Institute for Japanese Language, editor.
1964. Bunrui-Goi-Hyo. Shuei shuppan. In Japanese.
J. C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang.
2010. Automatic collocation suggestion in academ-
ic writing. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 115-119, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999261">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.736476">
<title confidence="0.9977395">Automated Collocation Suggestion for Japanese Second Language Learners</title>
<author confidence="0.986602">Lis W Kanashiro Pereira Erlyn Manguilimotan Yuji Matsumoto</author>
<affiliation confidence="0.997842">Nara Institute of Science and Department of Information</affiliation>
<email confidence="0.882607">lis-k@is.naist.jp</email>
<email confidence="0.882607">erlyn-m@is.naist.jp</email>
<email confidence="0.882607">matsu@is.naist.jp</email>
<abstract confidence="0.989133882352941">This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, these may sound In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y C Chang</author>
<author>J S Chang</author>
<author>H J Chen</author>
<author>H C Liou</author>
</authors>
<title>An automatic collocation writing assistant for Taiwanese EFL learners: A case of corpus-based NLP technology.</title>
<date>2008</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="4461" citStr="Chang et al. (2008)" startWordPosition="692" endWordPosition="695">fia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics similarity measures are used. To rank the best candidates, the strength of association in the learner’s construction and in each of the generated alternative construction are measured. For example, Futagi et al. (2008) generated synonyms for each candidate string using WordNet and Roget’s Thesaurus and used the rank ratio measure to score them by their semantic similarity. Liu et al. (2009) also used WordNet to generate synonyms, but used Pointwise Mutual Information as association measure to rank the candidates. Chang et al. (2008) used bilingual dictionaries to derive collocation candidates and used the loglikelihood measure to rank them. One drawback of these approaches is that they rely on resources of limited coverage, such as dictionaries, thesaurus or manually constructed databases to generate the candidates. Other studies have tried to offer better coverage by automatically deriving paraphrases from parallel corpora (Dahlmeier and Ng, 2011), but similar to Chang et al. (2008), it is essential to identify the learner’s first language and to have bilingual dictionaries and parallel corpora for every first language </context>
</contexts>
<marker>Chang, Chang, Chen, Liou, 2008</marker>
<rawString>Y. C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou. 2008. An automatic collocation writing assistant for Taiwanese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language Learning, 21(3):283–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information and Lexicography, Computational Linguistics,</journal>
<volume>16</volume>
<pages>22--29</pages>
<contexts>
<context position="12387" citStr="Church and Hanks, 1990" startWordPosition="1973" endWordPosition="1976">dates using word similarity, the next step is to identify the “true collocations” among them. Here, the association strength was measured, in such a way that word pairs generated by chance from the sampling process can be excluded. An association measure assigns an association score to each word pair. High scores indicate strong association, and can be used to select the “true collocations”. We adopted the Weighted Dice coefficient (Kitamura and Matsumoto, 1997) as our association measurement. We also tested using other association measures (results are omitted): Pointwise Mutual Information (Church and Hanks, 1990), log-likelihood ratio (Dunning, 1993) and Dice coefficient (Smadja et al., 1996), but Weighted Dice performed best. 5 Experiment setup We divided our experiments into two parts: verb suggestion and noun suggestion. For verb suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative verbs other than the learner’s written verb. For noun suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative nouns other than the learner’s written noun. 5.1 Data Set For</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. Church, and P. Hanks. 1990. Word Association Norms, Mutual Information and Lexicography, Computational Linguistics, Vol. 16:1, pp. 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="10478" citStr="Curran, 2004" startWordPosition="1664" endWordPosition="1665">lation with that noun. The context of a particular verb is a vector ご飯を ラーメンを カレーを rice ramen noodle soup curry 食べる 164 53 39 eat Table 3 Context of a particular noun represented as a co-occurrence vector of nouns that are in an object relation with that verb. Table 2 and Table 3 show examples of part of co-occurrence vectors for the noun “qaZ [diary]” and the verb “食べる [eat]”, respectively. The numbers indicate the co-occurrence frequency in the BCCWJ corpus (Maekawa, 2008). We computed the similarity between co-occurrence vectors using different metrics: Cosine Similarity, Dice coefficient (Curran, 2004), KullbackLeibler divergence or KL divergence or relative entropy (Kullback and Leibler, 1951) and the Jenson-Shannon divergence (Lee, 1999). Confusion Set derived from learner corpus: In order to build a module that can “guess” common construction errors, we created a confusion set using Lang-8 corpus. Instead of generating words that have similar meaning to the learner’s written construction, we extracted all the possible noun and verb corrections for each of the nouns and verbs found in the data. Table 1 shows some examples extracted. For instance, the confusion set of the verb suru “する [to</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>J. R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Correcting Semantic Collocation Errors with L1-induced Paraphrases.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>107--117</pages>
<publisher>Association for Computational Linguistics</publisher>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="4885" citStr="Dahlmeier and Ng, 2011" startWordPosition="755" endWordPosition="758">e them by their semantic similarity. Liu et al. (2009) also used WordNet to generate synonyms, but used Pointwise Mutual Information as association measure to rank the candidates. Chang et al. (2008) used bilingual dictionaries to derive collocation candidates and used the loglikelihood measure to rank them. One drawback of these approaches is that they rely on resources of limited coverage, such as dictionaries, thesaurus or manually constructed databases to generate the candidates. Other studies have tried to offer better coverage by automatically deriving paraphrases from parallel corpora (Dahlmeier and Ng, 2011), but similar to Chang et al. (2008), it is essential to identify the learner’s first language and to have bilingual dictionaries and parallel corpora for every first language (L1) in order to extend the resulting system. Another problem is that most research does not actually take the learners&apos; tendency of collocation errors into account; instead, their systems are trained only on well-formed text corpora. Our work follows the general approach, that is, uses similarity measures for generating the confusion set and association measures for ranking the best candidates. However, instead of using</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>D. Dahlmeier, H. T. Ng. 2011. Correcting Semantic Collocation Errors with L1-induced Paraphrases. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Edinburgh, Scotland, UK, July. Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>61--74</pages>
<contexts>
<context position="12425" citStr="Dunning, 1993" startWordPosition="1979" endWordPosition="1980">o identify the “true collocations” among them. Here, the association strength was measured, in such a way that word pairs generated by chance from the sampling process can be excluded. An association measure assigns an association score to each word pair. High scores indicate strong association, and can be used to select the “true collocations”. We adopted the Weighted Dice coefficient (Kitamura and Matsumoto, 1997) as our association measurement. We also tested using other association measures (results are omitted): Pointwise Mutual Information (Church and Hanks, 1990), log-likelihood ratio (Dunning, 1993) and Dice coefficient (Smadja et al., 1996), but Weighted Dice performed best. 5 Experiment setup We divided our experiments into two parts: verb suggestion and noun suggestion. For verb suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative verbs other than the learner’s written verb. For noun suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative nouns other than the learner’s written noun. 5.1 Data Set For computing word similarity and associa</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19.1 (Mar. 1993), 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Futagi</author>
<author>P Deane</author>
<author>M Chodorow</author>
<author>J Tetreault</author>
</authors>
<title>A computational approach to detecting collocation errors in the writing of non-native speakers of English.</title>
<date>2008</date>
<journal>Computer Assisted Language Learning,</journal>
<volume>21</volume>
<pages>353--367</pages>
<contexts>
<context position="4141" citStr="Futagi et al. (2008)" startWordPosition="642" endWordPosition="645">d from well-formed text during the training phase. If one or more alternatives are more appropriate to the context, the learner&apos;s word is flagged as an error and the alternatives are suggested as corrections. To constrain the size of the confusion set, 52 Proceedings of the ACL Student Research Workshop, pages 52–58, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics similarity measures are used. To rank the best candidates, the strength of association in the learner’s construction and in each of the generated alternative construction are measured. For example, Futagi et al. (2008) generated synonyms for each candidate string using WordNet and Roget’s Thesaurus and used the rank ratio measure to score them by their semantic similarity. Liu et al. (2009) also used WordNet to generate synonyms, but used Pointwise Mutual Information as association measure to rank the candidates. Chang et al. (2008) used bilingual dictionaries to derive collocation candidates and used the loglikelihood measure to rank them. One drawback of these approaches is that they rely on resources of limited coverage, such as dictionaries, thesaurus or manually constructed databases to generate the ca</context>
</contexts>
<marker>Futagi, Deane, Chodorow, Tetreault, 2008</marker>
<rawString>Y. Futagi, P. Deane, M. Chodorow, and J. Tetreault. 2008. A computational approach to detecting collocation errors in the writing of non-native speakers of English. Computer Assisted Language Learning, 21, 4 (October 2008), 353-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing: A meta-classifier approach.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1221" citStr="Gamon, 2010" startWordPosition="172" endWordPosition="173">ell-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “ れる1 [to make tea]” and yume wo miru ” V;jq22 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 lit. to put</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing: A meta-classifier approach. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163– 171, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. 2nd edition.</title>
<date>2009</date>
<publisher>Prentice-Hall.</publisher>
<marker>Jurafsky, Martin, 2009</marker>
<rawString>D. Jurafsky and J. H. Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. 2nd edition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Maekawa</author>
</authors>
<title>Balanced corpus of contemporary written japanese.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Workshop on Asian Language Resources (ALR),</booktitle>
<pages>101--102</pages>
<contexts>
<context position="10344" citStr="Maekawa, 2008" startWordPosition="1648" endWordPosition="1649">ted in computing similarity of nouns and verbs and hence the context of a particular noun is a vector of verbs that are in an object relation with that noun. The context of a particular verb is a vector ご飯を ラーメンを カレーを rice ramen noodle soup curry 食べる 164 53 39 eat Table 3 Context of a particular noun represented as a co-occurrence vector of nouns that are in an object relation with that verb. Table 2 and Table 3 show examples of part of co-occurrence vectors for the noun “qaZ [diary]” and the verb “食べる [eat]”, respectively. The numbers indicate the co-occurrence frequency in the BCCWJ corpus (Maekawa, 2008). We computed the similarity between co-occurrence vectors using different metrics: Cosine Similarity, Dice coefficient (Curran, 2004), KullbackLeibler divergence or KL divergence or relative entropy (Kullback and Leibler, 1951) and the Jenson-Shannon divergence (Lee, 1999). Confusion Set derived from learner corpus: In order to build a module that can “guess” common construction errors, we created a confusion set using Lang-8 corpus. Instead of generating words that have similar meaning to the learner’s written construction, we extracted all the possible noun and verb corrections for each of </context>
<context position="14031" citStr="Maekawa, 2008" startWordPosition="2235" endWordPosition="2236"> By subtree, we mean the tree with distance 2 from the leaf node (learner’s written word) doing the pre-order tree traversal. 2) Mainichi Shimbun Corpus (Mainichi Newspaper Co., 1991): one of the major newspapers in Japan that provides raw text of newspaper articles used as linguistic resource. One year data (1991) were used to extract the “noun wo verb” tuples to compute word similarity (using cosine similarity metric) and collocation scores. We extracted 224,185 tuples composed of 16,781 unique verbs and 37,300 unique nouns. 3) Balanced Corpus of Contemporary Written Japanese, BCCWJ Corpus (Maekawa, 2008): composed of one hundred million words, portions of this corpus used in our experiments include magazine, newspaper, textbooks, and blog data4. Incorporating a variety of topics and styles in the training data helps minimize the domain gap problem between the learner’s vocabulary and newspaper vocabulary found in the Mainichi Shimbun data. We extracted 194,036 “noun wo verb” tuples composed of 43,243 unique nouns and 18,212 unique verbs. These data are necessary to compute the word similarity (using cosine similarity metric) and collocation scores. 4) Lang-8 Corpus: Consisted of two year data</context>
</contexts>
<marker>Maekawa, 2008</marker>
<rawString>K. Maekawa. 2008. Balanced corpus of contemporary written japanese. In Proceedings of the 6th Workshop on Asian Language Resources (ALR), pages 101–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kitamura</author>
<author>Y Matsumoto</author>
</authors>
<title>Automatic extraction of translation patterns in parallel corpora.</title>
<date>1997</date>
<booktitle>In IPSJ,</booktitle>
<volume>38</volume>
<issue>4</issue>
<pages>108--117</pages>
<note>In Japanese.</note>
<contexts>
<context position="12230" citStr="Kitamura and Matsumoto, 1997" startWordPosition="1950" endWordPosition="1953">ey]”, “札 [bill]”, “金額 [amount of money]”, “料金 [fee]”) or view (“景色 [scenery]”) to Japanese. 54 4.2 Word Association Strength After generating the collocation candidates using word similarity, the next step is to identify the “true collocations” among them. Here, the association strength was measured, in such a way that word pairs generated by chance from the sampling process can be excluded. An association measure assigns an association score to each word pair. High scores indicate strong association, and can be used to select the “true collocations”. We adopted the Weighted Dice coefficient (Kitamura and Matsumoto, 1997) as our association measurement. We also tested using other association measures (results are omitted): Pointwise Mutual Information (Church and Hanks, 1990), log-likelihood ratio (Dunning, 1993) and Dice coefficient (Smadja et al., 1996), but Weighted Dice performed best. 5 Experiment setup We divided our experiments into two parts: verb suggestion and noun suggestion. For verb suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative verbs other than the learner’s written verb. For noun suggestion, given the learners’ “n</context>
</contexts>
<marker>Kitamura, Matsumoto, 1997</marker>
<rawString>M. Kitamura, Y. Matsumoto. 1997. Automatic extraction of translation patterns in parallel corpora. In IPSJ, Vol. 38(4), pp.108-117, April. In Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kullback</author>
<author>R A Leibler</author>
</authors>
<title>On Information and Sufficiency.</title>
<date>1951</date>
<journal>Annals of Mathematical Statistics</journal>
<volume>22</volume>
<issue>1</issue>
<pages>79--86</pages>
<contexts>
<context position="10572" citStr="Kullback and Leibler, 1951" startWordPosition="1675" endWordPosition="1678">ーを rice ramen noodle soup curry 食べる 164 53 39 eat Table 3 Context of a particular noun represented as a co-occurrence vector of nouns that are in an object relation with that verb. Table 2 and Table 3 show examples of part of co-occurrence vectors for the noun “qaZ [diary]” and the verb “食べる [eat]”, respectively. The numbers indicate the co-occurrence frequency in the BCCWJ corpus (Maekawa, 2008). We computed the similarity between co-occurrence vectors using different metrics: Cosine Similarity, Dice coefficient (Curran, 2004), KullbackLeibler divergence or KL divergence or relative entropy (Kullback and Leibler, 1951) and the Jenson-Shannon divergence (Lee, 1999). Confusion Set derived from learner corpus: In order to build a module that can “guess” common construction errors, we created a confusion set using Lang-8 corpus. Instead of generating words that have similar meaning to the learner’s written construction, we extracted all the possible noun and verb corrections for each of the nouns and verbs found in the data. Table 1 shows some examples extracted. For instance, the confusion set of the verb suru “する [to do]” is composed of verbs such as ukeru “受ける [to accept]”, which does not necessarily have si</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>S. Kullback, R.A. Leibler. 1951. On Information and Sufficiency. Annals of Mathematical Statistics 22 (1): 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of Distributional Similarity.</title>
<date>1999</date>
<booktitle>In Proc of the 37th annual meeting of the ACL,</booktitle>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="10618" citStr="Lee, 1999" startWordPosition="1683" endWordPosition="1684">xt of a particular noun represented as a co-occurrence vector of nouns that are in an object relation with that verb. Table 2 and Table 3 show examples of part of co-occurrence vectors for the noun “qaZ [diary]” and the verb “食べる [eat]”, respectively. The numbers indicate the co-occurrence frequency in the BCCWJ corpus (Maekawa, 2008). We computed the similarity between co-occurrence vectors using different metrics: Cosine Similarity, Dice coefficient (Curran, 2004), KullbackLeibler divergence or KL divergence or relative entropy (Kullback and Leibler, 1951) and the Jenson-Shannon divergence (Lee, 1999). Confusion Set derived from learner corpus: In order to build a module that can “guess” common construction errors, we created a confusion set using Lang-8 corpus. Instead of generating words that have similar meaning to the learner’s written construction, we extracted all the possible noun and verb corrections for each of the nouns and verbs found in the data. Table 1 shows some examples extracted. For instance, the confusion set of the verb suru “する [to do]” is composed of verbs such as ukeru “受ける [to accept]”, which does not necessarily have similar meaning with suru. The confusion set mea</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee. 1999. Measures of Distributional Similarity. In Proc of the 37th annual meeting of the ACL, Stroudsburg, PA, USA, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Liu</author>
<author>D Wible</author>
<author>N L Tsao</author>
</authors>
<title>Automated suggestions for miscollocations.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>47--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="4316" citStr="Liu et al. (2009)" startWordPosition="670" endWordPosition="673"> are suggested as corrections. To constrain the size of the confusion set, 52 Proceedings of the ACL Student Research Workshop, pages 52–58, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics similarity measures are used. To rank the best candidates, the strength of association in the learner’s construction and in each of the generated alternative construction are measured. For example, Futagi et al. (2008) generated synonyms for each candidate string using WordNet and Roget’s Thesaurus and used the rank ratio measure to score them by their semantic similarity. Liu et al. (2009) also used WordNet to generate synonyms, but used Pointwise Mutual Information as association measure to rank the candidates. Chang et al. (2008) used bilingual dictionaries to derive collocation candidates and used the loglikelihood measure to rank them. One drawback of these approaches is that they rely on resources of limited coverage, such as dictionaries, thesaurus or manually constructed databases to generate the candidates. Other studies have tried to offer better coverage by automatically deriving paraphrases from parallel corpora (Dahlmeier and Ng, 2011), but similar to Chang et al. (</context>
<context position="8534" citStr="Liu et al., 2009" startWordPosition="1352" endWordPosition="1355"> P!k ��� Meaning building beer draft beer money bill amount scenery fee building of money Table 1 Confusion Set example for the words suru (する) and biru (ビル) aく nL _A1;6 write read put on qaZ;�- 15 11 8 diary Table 2 Context of a particular noun represented as a co-occurrence vector rity: 1) thesaurus-based word similarity, 2) distributional similarity and 3) confusion set derived from learner corpus. The first two measures generate the collocation candidates by finding words that are analogous to the writer’s choice, a common approach used in the related work on collocation error correction (Liu et al., 2009; Östling and O. Knutsson, 2009; Wu et al., 2010) and the third measure generates the candidates based on the corrections given by native speakers in the learner corpus. Thesaurus-based word similarity: The intuition of this measure is to check if the given words have similar glosses (definitions). Two words are considered similar if they are near each other in the thesaurus hierarchy (have a path within a pre-defined threshold length). Distributional Similarity: Thesaurus-based methods produce weak recall since many words, phrases and semantic connections are not covered by hand-built thesaur</context>
</contexts>
<marker>Liu, Wible, Tsao, 2009</marker>
<rawString>A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated suggestions for miscollocations. In Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 47–50, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mainichi Newspaper Co</author>
</authors>
<date>1991</date>
<booktitle>Mainichi Shimbun CD-ROM</booktitle>
<marker>Co, 1991</marker>
<rawString>Mainichi Newspaper Co. 1991. Mainichi Shimbun CD-ROM 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mizumoto</author>
<author>K Mamoru</author>
<author>M Nagata</author>
<author>Y Matsumoto</author>
</authors>
<title>Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>147--155</pages>
<publisher>AFNLP.</publisher>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="5743" citStr="Mizumoto et al. (2011)" startWordPosition="896" endWordPosition="899">m is that most research does not actually take the learners&apos; tendency of collocation errors into account; instead, their systems are trained only on well-formed text corpora. Our work follows the general approach, that is, uses similarity measures for generating the confusion set and association measures for ranking the best candidates. However, instead of using only wellformed text for generating the confusion set, we use a large learner corpus created by crawling the revision log of a language learning social networking service (SNS), Lang-83. Another work that also uses data from Lang-8 is Mizumoto et al. (2011), which uses Lang-8 in creating a largescale Japanese learner’s corpus. The biggest benefit of using such kind of data is that we can obtain in large scale pairs of learners’ sentences and their corrections assigned by native speakers. 3 Combining Word Similarity and Association Measures to Suggest Collocations In our work, we focus on suggestions for noun and verb collocation errors in “noun wo verb (noun-を-verb)” constructions, where noun is the direct object of verb. Our approach consists of 3www.lang-8.com three steps: 1) for each extracted tuple in the second learner’s composition, we cre</context>
</contexts>
<marker>Mizumoto, Mamoru, Nagata, Matsumoto, 2011</marker>
<rawString>T. Mizumoto, K. Mamoru, M. Nagata, Y. Matsumoto. 2011. Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pp.147-155. Chiang Mai, Thailand, November. AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Östling</author>
<author>O Knutsson</author>
</authors>
<title>A corpus-based tool for helping writers with Swedish collocations.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Extracting and Using Constructions in NLP, Nodalida,</booktitle>
<volume>70</volume>
<pages>77</pages>
<location>Odense,</location>
<marker>Östling, Knutsson, 2009</marker>
<rawString>R. Östling and O. Knutsson. 2009. A corpus-based tool for helping writers with Swedish collocations. In Proceedings of the Workshop on Extracting and Using Constructions in NLP, Nodalida, Odense, Denmark. 70, 77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Oyama</author>
<author>Y Matsumoto</author>
</authors>
<title>Automatic Error Detection Method for Japanese Case Particles in Japanese Language Learners.</title>
<date>2010</date>
<booktitle>In Corpus, ICT, and Language Education,</booktitle>
<pages>235--245</pages>
<contexts>
<context position="1416" citStr="Oyama and Matsumoto, 2010" startWordPosition="199" endWordPosition="202">tructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “ れる1 [to make tea]” and yume wo miru ” V;jq22 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 lit. to put in tea 2 lit. to see a dream is one of the most difficult tasks for second language learners. For instance, the Japanese collocation yume wo miru [lit. to see a dream] is unpredictable, at least</context>
</contexts>
<marker>Oyama, Matsumoto, 2010</marker>
<rawString>H. Oyama and Y. Matsumoto. 2010. Automatic Error Detection Method for Japanese Case Particles in Japanese Language Learners. In Corpus, ICT, and Language Education, pages 235–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Generating confusion sets for context-sensitive error correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>961--970</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>MIT, Massachusetts, USA,</location>
<contexts>
<context position="1248" citStr="Rozovskaya and Roth, 2010" startWordPosition="174" endWordPosition="177">xt, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “ れる1 [to make tea]” and yume wo miru ” V;jq22 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 lit. to put in tea 2 lit. to see a dre</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010. Generating confusion sets for context-sensitive error correction. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 961–970, MIT, Massachusetts, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K R Mckeown</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Translation collocations for bilingual lexicons: a statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="12468" citStr="Smadja et al., 1996" startWordPosition="1984" endWordPosition="1987">ong them. Here, the association strength was measured, in such a way that word pairs generated by chance from the sampling process can be excluded. An association measure assigns an association score to each word pair. High scores indicate strong association, and can be used to select the “true collocations”. We adopted the Weighted Dice coefficient (Kitamura and Matsumoto, 1997) as our association measurement. We also tested using other association measures (results are omitted): Pointwise Mutual Information (Church and Hanks, 1990), log-likelihood ratio (Dunning, 1993) and Dice coefficient (Smadja et al., 1996), but Weighted Dice performed best. 5 Experiment setup We divided our experiments into two parts: verb suggestion and noun suggestion. For verb suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative verbs other than the learner’s written verb. For noun suggestion, given the learners’ “noun wo verb” construction, our focus is to suggest “noun wo verb” collocations with alternative nouns other than the learner’s written noun. 5.1 Data Set For computing word similarity and association scores for verb suggestion, the follow</context>
</contexts>
<marker>Smadja, Mckeown, Hatzivassiloglou, 1996</marker>
<rawString>F. Smadja, K. R. Mckeown, V. Hatzivassiloglou. 1996. Translation collocations for bilingual lexicons: a statistical approach. Computational Linguistics, 22:1-38.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Suzuki</author>
<author>K Toutanova</author>
</authors>
<title>Learning to Predict Case Markers in Japanese.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>1049--1056</pages>
<publisher>Association for</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1388" citStr="Suzuki and Toutanova, 2006" startWordPosition="195" endWordPosition="198">at is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “ れる1 [to make tea]” and yume wo miru ” V;jq22 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 lit. to put in tea 2 lit. to see a dream is one of the most difficult tasks for second language learners. For instance, the Japanese collocation yume wo miru [lit. to see a dream</context>
</contexts>
<marker>Suzuki, Toutanova, 2006</marker>
<rawString>H. Suzuki and K. Toutanova. 2006. Learning to Predict Case Markers in Japanese. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1049–1056 , Sydney, July. Association for Computational Linguistics.J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings of ACL 2010 Conference Short Papers, pages 353-358, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<booktitle>The National Institute for Japanese Language, editor. 1964. Bunrui-Goi-Hyo. Shuei shuppan. In Japanese.</booktitle>
<marker></marker>
<rawString>The National Institute for Japanese Language, editor. 1964. Bunrui-Goi-Hyo. Shuei shuppan. In Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Wu</author>
<author>Y C Chang</author>
<author>T Mitamura</author>
<author>J S Chang</author>
</authors>
<title>Automatic collocation suggestion in academic writing.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>115--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8583" citStr="Wu et al., 2010" startWordPosition="1361" endWordPosition="1364">ill amount scenery fee building of money Table 1 Confusion Set example for the words suru (する) and biru (ビル) aく nL _A1;6 write read put on qaZ;�- 15 11 8 diary Table 2 Context of a particular noun represented as a co-occurrence vector rity: 1) thesaurus-based word similarity, 2) distributional similarity and 3) confusion set derived from learner corpus. The first two measures generate the collocation candidates by finding words that are analogous to the writer’s choice, a common approach used in the related work on collocation error correction (Liu et al., 2009; Östling and O. Knutsson, 2009; Wu et al., 2010) and the third measure generates the candidates based on the corrections given by native speakers in the learner corpus. Thesaurus-based word similarity: The intuition of this measure is to check if the given words have similar glosses (definitions). Two words are considered similar if they are near each other in the thesaurus hierarchy (have a path within a pre-defined threshold length). Distributional Similarity: Thesaurus-based methods produce weak recall since many words, phrases and semantic connections are not covered by hand-built thesauri, especially for verbs and adjectives. As an alt</context>
</contexts>
<marker>Wu, Chang, Mitamura, Chang, 2010</marker>
<rawString>J. C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang. 2010. Automatic collocation suggestion in academic writing. In Proceedings of the ACL 2010 Conference Short Papers, pages 115-119, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>