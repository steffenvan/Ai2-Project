<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000712">
<title confidence="0.9970145">
Chinese word segmentation and named entity recognition based
on a context-dependent Mutual Information Independence Model
</title>
<author confidence="0.999377">
Zhang Min Zhou GuoDong Yang LingPeng Ji DongHong
</author>
<affiliation confidence="0.997185">
Institute for Infocomm Research
</affiliation>
<address confidence="0.982522">
21 Heng Mui Keng Terrace
Singapore, 119613
</address>
<email confidence="0.994447">
Email: (mzhang, zhougd, lpyang, dhji)@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.99561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796823529412">
This paper briefly describes our system in the
third SIGHAN bakeoff on Chinese word
segmentation and named entity recognition.
This is done via a word chunking strategy
using a context-dependent Mutual
Information Independence Model.
Evaluation shows that our system performs
well on all the word segmentation closed
tracks and achieves very good scalability
across different corpora. It also shows that
the use of the same strategy in named entity
recognition shows promising performance
given the fact that we only spend less than
three days in total on extending the system in
word segmentation to incorporate named
entity recognition, including training and
formal testing.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969016949153">
Word segmentation and named entity recognition
aim at recognizing the implicit word boundaries
and proper nouns, such as names of persons,
locations and organizations, respectively in plain
Chinese text, and are critical in Chinese
information processing. However, there exist two
problems when developing a practical word
segmentation or named entity recognition system
for large open applications, i.e. the resolution of
ambiguous segmentations and the identification
of OOV words or OOV entity names.
In order to resolve above problems, we
developed a purely statistical Chinese word
segmentation system and a named entity
recognition system using a three-stage strategy
under an unified framework.
The first stage is called known word
segmentation, which aims to segment an input
sequence of Chinese characters into a sequence of
known words (called word atoms in this paper). In
this paper, all Chinese characters are regarded as
known words and a word unigram model is
applied to perform this task for efficiency. Also,
for convenience, all the English characters are
transformed into the Chinese counterparts in
preprocessing, which will be recovered just
before outputting results.
The second stage is the word and/or named
entity identification and classification on the
sequence of atomic words in the first step. Here, a
word chunking strategy is applied to detect words
and/or entity names by chunking one or more
atomic words together according to the word
formation patterns of the word atoms and optional
entity name formation patterns for named entity
recognition. The problem of word segmentation
and/or entity name recognition are re-cast as
chunking one or more word atoms together to
form a new word and/or entity name, and a
discriminative Markov model, named Mutual
Information Independence Model (MIIM), is
adopted in chunking. Besides, a SVM plus
sigmoid model is applied to integrate various
types of contexts and implement the
discriminative modeling in MIIM.
The third step is post processing, which tries
to further resolve ambiguous segmentations and
unknown word segmentation. Due to time limit,
this is only done in Chinese word segmentation.
No post processing is done on Chinese named
entity recognition.
The rest of this paper is as follows: Section 2
describes the context-dependent Mutual
Information Independence Model in details while
purely statistical post-processing in Chinese word
segmentation is presented in Section 3. Finally,
we report the results of our system in Chinese
word segmentation and named entity recognition
in Section 4 and conclude our work in Section 5.
</bodyText>
<page confidence="0.996689">
154
</page>
<bodyText confidence="0.72407">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 154–157,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.8517385" genericHeader="method">
2 Mutual Information Independence
Model
</sectionHeader>
<bodyText confidence="0.951289428571428">
In this paper, we use a discriminative Markov
model, called Mutual Information Independence
Model (MIIM) as proposed by Zhou et al (2002),
for Chinese word segmentation and named entity
recognition. MIIM is derived from a conditional
probability model. Given an observation sequence
O1n = o1o2 L o , MIIM finds a stochastic optimal
</bodyText>
<equation confidence="0.998069733333333">
n
state(tag) sequence S1n = s1s2 L s that
n
maximizes:
n n
log P(
S O
1 1 1 1
n 1
 |) PMI s S
n = − + n
∑ ( , ) log (  |)
i i
i ∑ P s O
i=2 i=1
</equation>
<bodyText confidence="0.963683571428572">
We call the above model the Mutual
Information Independence Model due to its
Pair-wise Mutual Information (PMI) assumption
(Zhou et al 2002). The above model consists of
two sub-models: the state transition model
1) , which can be computed by
applying ngram modeling, and the output model
</bodyText>
<equation confidence="0.59015">
log (  |1 ), which can be estimated by any
P si O n
</equation>
<bodyText confidence="0.99811888">
probability-based classifier, such as a maximum
entropy classifier or a SVM plus sigmoid
classifier (Zhou et al 2006). In this competition,
the SVM plus sigmoid classifier is used in
Chinese word segmentation while a simple
backoff approach as described in Zhou et al
(2002) is used in named entity recognition.
Here, a variant of the Viterbi algorithm
(Viterbi 1967) in decoding the standard Hidden
Markov Model (HMM) (Rabiner 1989) is
implemented to find the most likely state
sequence by replacing the state transition model
and the output model of the standard HMM with
the state transition model and the output model of
the MIIM, respectively. The above MIIM has
been successfully applied in many applications,
such as text chunking (Zhou 2004), Chinese word
segmentation ( Zhou 2005), English named entity
recognition in the newswire domain (Zhou et al
2002) and the biomedical domain (Zhou et al
2004; Zhou et al 2006).
For Chinese word segmentation and named
entity recognition by chunking, a word or a entity
name is regarded as a chunk of one or more word
atoms and we have:
</bodyText>
<listItem confidence="0.973412">
• oi =&lt; pi, wi &gt; ; wi is the i − th word atom in
</listItem>
<bodyText confidence="0.883923066666667">
the sequence of word atoms W1n = w1w2 L w ;
npi is the word formation pattern of the word
atom wi . Here pi measures the word
formation power of the word atom wi and
consists of:
o The percentage of wi occurring as a whole
word (round to 10%)
o The percentage of wi occurring at the
beginning of other words (round to 10%)
o The percentage of wi occurring at the end
of other words (round to 10%)
o The length of wi
o Especially for named entity recognition,
the percentages of a word occurring in
different entity types (round to 10%).
</bodyText>
<listItem confidence="0.973917">
• si : the states are used to bracket and
</listItem>
<bodyText confidence="0.981319428571429">
differentiate various types of words and
optional entity types for named entity
recognition. In this way, Chinese word
segmentation and named entity recognition
can be regarded as a bracketing and
classification process. si is structural and
consists of two parts:
</bodyText>
<listItem confidence="0.84058825">
o Boundary category (B): it includes four
values: {O, B, M, E}, where O means that
current word atom is a whOle word or
entity name and B/M/E means that current
word atom is at the Beginning/in the
Middle/at the End of a word or entity name.
o Unit category (W): It is used to denote the
type of the word or entity name.
</listItem>
<bodyText confidence="0.998937333333333">
Because of the limited number of boundary
and unit categories, the current word atom
formation pattern pi described above is added
into the state transition model in MIIM. This
makes the above MIIM context dependent as
follows:
</bodyText>
<equation confidence="0.9973106">
log (  |)
P S O
n n
1 1
n n
i
∑ PMI(si,S1−1  |pi−1pi)+∑
i
=2 i=
1
</equation>
<sectionHeader confidence="0.808221" genericHeader="method">
3 Post Processing
Segmentation
</sectionHeader>
<bodyText confidence="0.999786571428571">
The third step is post processing, which tries to
resolve ambiguous segmentations and false
unknown word generation raised in the second
step. Due to time limit, this is only done in
Chinese word segmentation, i.e. no post
processing is done on Chinese named entity
recognition.
</bodyText>
<equation confidence="0.947451153846154">
n
∑PMI si S
( , 1 i −
i =2
n
∑=
i 1
log ( |
P s O
i
n )
1
in Word
</equation>
<page confidence="0.993993">
155
</page>
<bodyText confidence="0.97272505882353">
A simple pattern-based method is employed to
capture context information to correct the
segmentation errors generated in the second steps.
The pattern is designed as follows:
&lt;Ambiguous Entry (AE)&gt;  |&lt;Left Context,
Right Context&gt; =&gt; &lt;Proper Segmentation&gt;
The ambiguity entry (AE) means ambiguous
segmentations or forced-generated unknown
words. We use the 1st and 2nd words before AE as
the left context and the 1st and 2nd words after AE
as the right context. To reduce sparseness, we also
only use the 1st left and right words as context.
This means that there are two patterns generated
for the same context. All the patterns are
automatically learned from training corpus using
the following algorithm.
LearningPatterns()
</bodyText>
<listItem confidence="0.965795375">
// Input: training corpus
// Output: patterns
BEGIN
(1) Training a MIIM model using training
corpus
(2) Using the MIIM model to segment training
corpus
(3) Aligning the training corpus with the
segmented training corpus
(4) Extracting error segmentations
(5) Generating disambiguation patterns using
the left and right context
(6) Removing the conflicting entries if two
patterns have the same left hand side but
different right hand side.
END
</listItem>
<sectionHeader confidence="0.994101" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999939195121952">
We first develop our system using the PKU data
released in the Second SIGHAN Bakeoff last
year. Then, we train and evaluate it on the Third
SIGHAN Bakeoff corpora without any
fine-tuning. We only carry out our evaluation on
the closed tracks. It means that we do not use any
additional knowledge beyond the training corpus.
Precision (P), Recall (R), F-measure (F), OOV
Recall and IV Recall are adopted to measure the
performance of word segmentation. Accuracy
(A), Precision (P), Recall (R) and F-measure (F)
are adopted to measure the performance of NER.
Tables 1, 2 and 3 in the next page report the
performance of our algorithm on different corpus
in the SIGHAN Bakeoff 02 and Bakeoff 03,
respectively. For the performance of other
systems, please refer to
http://sighan.cs.uchicago.edu/bakeoff2005/data/r
esults.php.htm for the Chinese bakeoff 2005 and
http://sighan.cs.uchicago.edu/bakeoff2006/longst
ats.html for the Chinese bakeoff 2006.
Comparison against other systems shows that
our system achieves the state-of-the-art
performance on all Chinese word segmentation
closed tracks and shows good scalability across
different corpora. The small performance gap
should be able to overcome by replacing the word
unigram model with the more powerful word
bigram model. Due to very limited time of less
than three days, although our NER system under
the unified framework as Chinese word
segmentation does not achieve the
state-of-the-art, its performance in NER is quite
promising and provides a good platform for
further improvement. Error analysis reveals that
OOV is still an open problem that is far from to
resolve. In addition, different corpus defines
different segmentation principles. This will stress
OOV handling in the extreme. Therefore a system
trained on one genre usually performances worse
when faced with text from a different register.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999881">
This paper proposes a purely unified statistical
three-stage strategy in Chinese word
segmentation and named entity recognition,
which are based on a context-dependent Mutual
Information Independence Model. Evaluation
shows that our system achieves the
states-of-the-art segmentation performance and
provides a good platform for further performance
improvement of Chinese NER.
</bodyText>
<sectionHeader confidence="0.987789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.645399785714286">
Rabiner L. 1989. A Tutorial on Hidden Markov
Models and Selected Applications in Speech
Recognition. IEEE 77(2), pages257-285.
Viterbi A.J. 1967. Error Bounds for
Convolutional Codes and an Asymptotically
Optimum Decoding Algorithm. IEEE
Transactions on Information Theory, IT 13(2),
260-269.
Zhou GuoDong and Su Jain. 2002. Named Entity
Recognition Using a HMM-based Chunk
Tagger, Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL’2002). Philadelphia. July
2002. pp473-480.
</reference>
<page confidence="0.990006">
156
</page>
<reference confidence="0.998875739130435">
Zhou GuoDong, Zhang Jie, Su Jian, Shen Dan and
Tan ChewLim. 2004. Recognizing Names in
Biomedical Texts: a Machine Learning
Approach. Bioinformatics. 20(7): 1178-1190.
DOI: 10.1093/bioinformatics/bth060. 2004.
ISSN: 1460-2059
Zhou GuoDong. 2004. Discriminative hidden
Markov modeling with long state dependence
using a kNN ensemble. Proceedings of 20th
International Conference on Computational
Linguistics (COLING’2004). 23-27 Aug, 2004,
Geneva, Switzerland.
Zhou GuoDong. 2005. A chunking strategy
towards unknown word detection in Chinese
word segmentation. Proceedings of 2nd
International Joint Conference on Natural
Language Processing (IJCNLP’2005), Lecture
Notes in Computer Science (LNCS 3651)
Zhou GuoDong. 2006. Recognizing names in
biomedical texts using Mutual Information
Independence Model and SVM plus Sigmod.
International Journal of Medical Informatics
(Article in Press). ISSN 1386-5056
</reference>
<tableCaption confidence="0.407543">
Tables
</tableCaption>
<table confidence="0.996045">
Task P R F OOV Recall IV Recall
CityU 0.9 38 0.952 94.5 0.578 0.967
MSRA 0.952 0.962 95.7 0.51 0.98
CKIP 0.94 0.957 94.8 0.502 0.976
PKU 0.952 0.952 95.2 0.71 0.967
</table>
<tableCaption confidence="0.998109">
Table 1: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 02
</tableCaption>
<table confidence="0.9991114">
Task P R F OOV Recall IV Recall
CityU 0.968 0.961 96.5 0.633 0.983
MSRA 0.961 0.953 95.7 0.499 0.977
CKIP 0.958 0.941 94.9 0.554 0.976
UPUC 0.936 0.917 92.6 0.617 0.966
</table>
<tableCaption confidence="0.988866">
Table 2: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 03
</tableCaption>
<table confidence="0.996637666666667">
Task A P R F
MSRA 0.9743 0.8150 0.7882 79.92
CityU 0.9725 0.8466 0.8061 82.59
</table>
<tableCaption confidence="0.99981">
Table 3: Performance of NER on Closed Tracks in the SIGHAN Bakeoff 03
</tableCaption>
<page confidence="0.995277">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907900">
<title confidence="0.994495">Chinese word segmentation and named entity recognition on a context-dependent Mutual Information Independence Model</title>
<author confidence="0.991992">Zhang Min Zhou GuoDong Yang LingPeng Ji DongHong</author>
<affiliation confidence="0.997741">Institute for Infocomm</affiliation>
<address confidence="0.9961065">21 Heng Mui Keng Singapore,</address>
<email confidence="0.9965">mzhang,zhougd,lpyang,dhji)@i2r.a-star.edu.sg</email>
<abstract confidence="0.996167222222222">This paper briefly describes our system in the third SIGHAN bakeoff on Chinese word segmentation and named entity recognition. This is done via a word chunking strategy using a context-dependent Mutual Information Independence Evaluation shows that our system performs well on all the word segmentation closed tracks and achieves very good scalability across different corpora. It also shows that the use of the same strategy in named entity recognition shows promising performance given the fact that we only spend less than three days in total on extending the system in word segmentation to incorporate named entity recognition, including training and formal testing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.</title>
<date>1989</date>
<journal>IEEE</journal>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<contexts>
<context position="5067" citStr="Rabiner 1989" startWordPosition="801" endWordPosition="802">nsists of two sub-models: the state transition model 1) , which can be computed by applying ngram modeling, and the output model log ( |1 ), which can be estimated by any P si O n probability-based classifier, such as a maximum entropy classifier or a SVM plus sigmoid classifier (Zhou et al 2006). In this competition, the SVM plus sigmoid classifier is used in Chinese word segmentation while a simple backoff approach as described in Zhou et al (2002) is used in named entity recognition. Here, a variant of the Viterbi algorithm (Viterbi 1967) in decoding the standard Hidden Markov Model (HMM) (Rabiner 1989) is implemented to find the most likely state sequence by replacing the state transition model and the output model of the standard HMM with the state transition model and the output model of the MIIM, respectively. The above MIIM has been successfully applied in many applications, such as text chunking (Zhou 2004), Chinese word segmentation ( Zhou 2005), English named entity recognition in the newswire domain (Zhou et al 2002) and the biomedical domain (Zhou et al 2004; Zhou et al 2006). For Chinese word segmentation and named entity recognition by chunking, a word or a entity name is regarde</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner L. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. IEEE 77(2), pages257-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory, IT</journal>
<volume>13</volume>
<issue>2</issue>
<pages>260--269</pages>
<contexts>
<context position="5001" citStr="Viterbi 1967" startWordPosition="791" endWordPosition="792">Information (PMI) assumption (Zhou et al 2002). The above model consists of two sub-models: the state transition model 1) , which can be computed by applying ngram modeling, and the output model log ( |1 ), which can be estimated by any P si O n probability-based classifier, such as a maximum entropy classifier or a SVM plus sigmoid classifier (Zhou et al 2006). In this competition, the SVM plus sigmoid classifier is used in Chinese word segmentation while a simple backoff approach as described in Zhou et al (2002) is used in named entity recognition. Here, a variant of the Viterbi algorithm (Viterbi 1967) in decoding the standard Hidden Markov Model (HMM) (Rabiner 1989) is implemented to find the most likely state sequence by replacing the state transition model and the output model of the standard HMM with the state transition model and the output model of the MIIM, respectively. The above MIIM has been successfully applied in many applications, such as text chunking (Zhou 2004), Chinese word segmentation ( Zhou 2005), English named entity recognition in the newswire domain (Zhou et al 2002) and the biomedical domain (Zhou et al 2004; Zhou et al 2006). For Chinese word segmentation and named </context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Viterbi A.J. 1967. Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. IEEE Transactions on Information Theory, IT 13(2), 260-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jain</author>
</authors>
<title>Named Entity Recognition Using a HMM-based Chunk Tagger,</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’2002).</booktitle>
<pages>473--480</pages>
<location>Philadelphia.</location>
<marker>GuoDong, Jain, 2002</marker>
<rawString>Zhou GuoDong and Su Jain. 2002. Named Entity Recognition Using a HMM-based Chunk Tagger, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’2002). Philadelphia. July 2002. pp473-480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Zhang Jie</author>
<author>Su Jian</author>
<author>Shen Dan</author>
<author>Tan ChewLim</author>
</authors>
<title>Recognizing Names in Biomedical Texts: a Machine Learning Approach.</title>
<date>2004</date>
<journal>Bioinformatics.</journal>
<volume>20</volume>
<issue>7</issue>
<pages>1178--1190</pages>
<marker>GuoDong, Jie, Jian, Dan, ChewLim, 2004</marker>
<rawString>Zhou GuoDong, Zhang Jie, Su Jian, Shen Dan and Tan ChewLim. 2004. Recognizing Names in Biomedical Texts: a Machine Learning Approach. Bioinformatics. 20(7): 1178-1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DOI 10 1093bioinformaticsbth060</author>
</authors>
<date>2004</date>
<volume>ISSN:</volume>
<pages>1460--2059</pages>
<marker>1093bioinformaticsbth060, 2004</marker>
<rawString>DOI: 10.1093/bioinformatics/bth060. 2004. ISSN: 1460-2059</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
</authors>
<title>Discriminative hidden Markov modeling with long state dependence using a kNN ensemble.</title>
<date>2004</date>
<booktitle>Proceedings of 20th International Conference on Computational Linguistics (COLING’2004).</booktitle>
<pages>23--27</pages>
<location>Geneva, Switzerland.</location>
<marker>GuoDong, 2004</marker>
<rawString>Zhou GuoDong. 2004. Discriminative hidden Markov modeling with long state dependence using a kNN ensemble. Proceedings of 20th International Conference on Computational Linguistics (COLING’2004). 23-27 Aug, 2004, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
</authors>
<title>A chunking strategy towards unknown word detection in Chinese word segmentation.</title>
<date>2005</date>
<booktitle>Proceedings of 2nd International Joint Conference on Natural Language Processing (IJCNLP’2005), Lecture Notes in Computer Science (LNCS</booktitle>
<pages>3651</pages>
<marker>GuoDong, 2005</marker>
<rawString>Zhou GuoDong. 2005. A chunking strategy towards unknown word detection in Chinese word segmentation. Proceedings of 2nd International Joint Conference on Natural Language Processing (IJCNLP’2005), Lecture Notes in Computer Science (LNCS 3651)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
</authors>
<title>Recognizing names in biomedical texts using Mutual Information Independence Model and SVM plus Sigmod.</title>
<date>2006</date>
<journal>International Journal of Medical Informatics (Article in Press). ISSN</journal>
<pages>1386--5056</pages>
<marker>GuoDong, 2006</marker>
<rawString>Zhou GuoDong. 2006. Recognizing names in biomedical texts using Mutual Information Independence Model and SVM plus Sigmod. International Journal of Medical Informatics (Article in Press). ISSN 1386-5056</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>