<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.984581">
Discontinuous Incremental Shift-Reduce Parsing
</title>
<author confidence="0.986329">
Wolfgang Maier
</author>
<affiliation confidence="0.833222">
Universit¨at D¨usseldorf
</affiliation>
<address confidence="0.765544">
Institut f¨ur Sprache und Information
Universit¨atsstr. 1, 40225 D¨usseldorf, Germany
</address>
<email confidence="0.998458">
maierw@hhu.de
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796285714286">
We present an extension to incremental
shift-reduce parsing that handles discon-
tinuous constituents, using a linear clas-
sifier and beam search. We achieve very
high parsing speeds (up to 640 sent./sec.)
and accurate results (up to 79.52 F1 on
TiGer).
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999774">
Discontinuous constituents consist of more than
one continuous block of tokens. They arise
through phenomena which traditionally in linguis-
tics would be analyzed as being the result of some
kind of “movement”, such as extraposition or top-
icalization. The occurrence of discontinuous con-
stituents does not necessarily depend on the de-
gree of freedom in word order that a language al-
lows for. They can be found, e.g., in almost equal
proportions in English and German treebank data
(Evang and Kallmeyer, 2011).
Generally, discontinuous constituents are ac-
counted for in treebank annotation. One annota-
tion method consists of using trace nodes that de-
note the source of a movement and are co-indexed
with the moved constituent. Another method is
to annotate discontinuities directly by allowing for
crossing branches. Fig. 1 shows an example for
the latter approach with which we are concerned
in this paper, namely, the annotation of (1). The
tree contains a discontinuous VP due to the fact
that the fronted pronoun is directly attached.
</bodyText>
<listItem confidence="0.674523">
(1) Das
</listItem>
<bodyText confidence="0.298209">
That
</bodyText>
<footnote confidence="0.519958666666667">
“This is what we want to reverse”
Several methods have been proposed for pars-
ing such structures. Trace recovery can been
</footnote>
<figureCaption confidence="0.9931675">
Figure 1: Example annotation with discontinuous
constituents from TiGer
</figureCaption>
<bodyText confidence="0.999324655172414">
framed as a separate pre-, post- or in-processing
task to PCFG parsing (Johnson, 2002; Dienes and
Dubey, 2003; Jijkoun, 2003; Levy and Manning,
2004; Schmid, 2006; Cai et al., 2011, among
others); see particularly Schmid (2006) for more
details. Directly annotated discontinuous con-
stituents can be parsed with a dependency parser,
given a reversible transformation from discontin-
uous constituency trees to non-projective depen-
dency structures. Transformations have been pro-
posed by Hall and Nivre (2008), who use com-
plex edge labels that encode paths between lexical
heads, and recently by Fern´andez-Gonz´alez and
Martins (2015), who use edge labels to encode the
attachment order of modifiers to heads.
Direct parsing of discontinuous constituents can
be done with Linear Context-Free Rewriting Sys-
tem (LCFRS), an extension of CFG which allows
its non-terminals to cover more than one contin-
uous block (Vijay-Shanker et al., 1987). LCFRS
parsing is expensive: CYK chart parsing with a
binarized grammar can be done in O(n3k) where
k is the block degree, the maximal number of con-
tinuous blocks a non-terminal can cover (Seki et
al., 1991). For a typical treebank LCFRS (Maier
and Søgaard, 2008), k ≈ 3, instead of k = 1 for
PCFG. In order to improve on otherwise imprac-
tical parsing times, LCFRS chart parsers employ
different strategies to speed up search: Kallmeyer
</bodyText>
<figure confidence="0.991788166666667">
Das
PDS
wollen
VMFIN
wir
PPER
umkehren
VVINF
VP
S
wollen wir umkehren
want we reverse
</figure>
<page confidence="0.958137">
1202
</page>
<note confidence="0.976566">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.997052818181818">
and Maier (2013) use A∗ search; van Cranenburgh
(2012) and van Cranenburgh and Bod (2013) use a
coarse-to-fine strategy in combination with Data-
Oriented Parsing; Angelov and Ljungl¨of (2014)
use a novel cost estimation to rank parser items.
Maier et al. (2012) apply a treebank transforma-
tion which limits the block degree and therewith
also the parsing complexity.
Recently Versley (2014) achieved a break-
through with a EaFi, a classifier-based parser that
uses an “easy-first” approach in the style of Gold-
berg and Elhadad (2010). In order to obtain dis-
continuous constituents, the parser uses a strat-
egy known from non-projective dependency pars-
ing (Nivre, 2009; Nivre et al., 2009): For every
non-projective dependency tree, there is a projec-
tive dependency tree which can be obtained by
reordering the input words. Non-projective de-
pendency parsing can therefore be viewed as pro-
jective dependency parsing with an additional re-
ordering of the input words. The reordering can
be done online during parsing with a “swap” op-
eration that allows to process input words out of
order. This idea can be transferred, because also
for every discontinuous constituency tree, one can
find a continuous tree by reordering the terminals.
Versley (2014) uses an adaptive gradient method
to train his parser. He reports a parsing speed of
40-55 sent./sec. and results that surpass those re-
ported for the above mentioned chart parsers.
In (continuous) constituency parsing, incremen-
tal shift-reduce parsing using the structured per-
ceptron is an established technique. While the
structured perceptron for parsing has first been
used by Collins and Roark (2004), classifier-based
incremental shift-reduce parsing has been taken up
by Sagae and Lavie (2005). A general formula-
tion for the application of the perceptron algorithm
to various problems, including shift-reduce con-
stituency parsing, has been introduced by Zhang
and Clark (2011b). Improvements have followed
(Zhu et al., 2012; Zhu et al., 2013). A similar strat-
egy has been shown to work well for CCG parsing
(Zhang and Clark, 2011a), too.
In this paper, we contribute a perceptron-based
shift-reduce parsing architecture with beam search
(following Zhu et al. (2013) and Bauer (2014))
and extend it such that it can create trees with
crossing branches (following Versley (2014)). We
present strategies to improve performance on dis-
continuous structures, such as a new feature set.
Our parser is very fast (up to 640 sent./sec.),
and produces accurate results. In our evaluation,
where we pay particular attention to the parser
performance on discontinuous structures, we show
among other things that surprisingly, a grammar-
based parser has an edge over a shift-reduce ap-
proach concerning the reconstruction of discontin-
uous constituents.
The remainder of the paper is structured as fol-
lows. In subsection 2.1, we introduce the gen-
eral parser architecture; the subsections 2.2 and
2.3 introduce the features we use and our strat-
egy for handling discontinuous structures. Section
3 presents and discusses the experimental results,
section 4 concludes the article.
</bodyText>
<sectionHeader confidence="0.945821" genericHeader="method">
2 Discontinuous Shift-Reduce Parsing
</sectionHeader>
<bodyText confidence="0.986495">
Our parser architecture follows previous work,
particularly Zhu et al. (2013) and Bauer (2014).
</bodyText>
<subsectionHeader confidence="0.8937815">
2.1 Shift-reduce parsing with perceptron
training
</subsectionHeader>
<bodyText confidence="0.999877925925926">
An item in our parser consists of a queue q of
token/POS-pairs to be processed, and a stack s,
which holds completed constituents.1 The parser
uses different transitions: SHIFT shifts a termi-
nal from the queue on to the stack. UNARY-X
reduces the first element on the stack to a new
constituent labeled X. BINARY-X-L and BINARY-
X-R reduce the first two elements on the stack to
a new X constituent, with the lexical head com-
ing from the left or the right child, respectively.
FINISH removes the last element from the stack.
We additionally use an IDLE transition, which can
be applied any number of times after FINISH, to
improve the comparability of analyses of different
lengths (Zhu et al., 2013).
The application of a transition is subject to re-
strictions. UNARY-X, e.g., can only be applied
when there is at least a single item on the stack.
We implement all restrictions listed in the ap-
pendix of Zhang and Clark (2009), and add addi-
tional restrictions that block transitions involving
the root label when not having arrived at the end of
a derivation. We do not use an underlying gram-
mar to filter out transitions which have not been
seen during training.
For decoding, we use beam search (Zhang and
Clark, 2011b). Decoding is started by putting the
</bodyText>
<footnote confidence="0.991993">
1As in other shift-reduce approaches, we assume that POS
tagging is done outside of the parser.
</footnote>
<page confidence="0.964475">
1203
</page>
<figureCaption confidence="0.999845">
Figure 2: Binarization example
</figureCaption>
<bodyText confidence="0.999112193548387">
start item (empty stack, full queue) on the beam.
Then, repeatedly, a candidate list is filled with all
items that result from applying legal transitions to
the items on the beam, followed by putting the
highest scoring n of them back on the beam (given
a beam size of n). Parsing is finished if the high-
est scoring item on the beam is a final item (stack
holds one item labeled with the root label, queue
is empty), which can be popped. Item scores are
computed as in Zhang and Clark (2011b): The
score of the i+1th item is computed as the sum of
the score of the ith item and the dot product of a
global feature weight vector and the local weight
vector resulting from the changes induced by the
corresponding transition to the i + 1th item. The
start item has score 0. We train the global weight
vector with an averaged Perceptron with early up-
date (Collins and Roark, 2004).
Parsing relies on binary trees. As in previ-
ous work, we binarize the incoming trees head-
outward with binary top and bottom productions.
Given a constituent X which is to be binarized,
all intermediate nodes which are introduced will
be labeled @X. Lexical heads are marked with
Collins-style head rules. As an example, Fig. 2
shows the binarized version of the tree of Fig. 1.
Finally, since we are learning a sparse model,
we also exploit the work of Goldberg and Elhadad
(2011) who propose to include a feature in the cal-
culation of a score only if it has been observed ≥
MINUPDATE times.
</bodyText>
<subsectionHeader confidence="0.982622">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999913444444445">
Features are generated by applying templates to
parser items. They reflect different configurations
of stack and queue. As BASELINE features, we
use the feature set from Zhang and Clark (2009)
without the bracketing features (as used in Zhu et
al. (2013)). We furthermore experiment with fea-
tures that reflect the presence of separating punctu-
ation “,”, “:”, “;” (SEPARATOR) (Zhang and Clark,
2009), and with the EXTENDED features of Zhu et
</bodyText>
<equation confidence="0.9847005">
unigrams
s0tc, s0wc, s1tc, s1wc, s2tc, s2wc, s3tc, s3wc,
q0wt, q1wt, q2wt, q3wt,
s0lwc, s0rwc, s0uwc, s1lwc, s1rwc, s1uwc
bigrams
s0ws1w, s0ws1c, s0cs1w, s0cs1c, s0wq0w, s0wq0t,
s0cq0w, s0cq0t, s1wq0w, s1wq0t, s1cq0w, s1cq0t,
q0wq1w, q0wq1t, q0tq1w, q0tq1t
trigrams
s0cs1cs2w, s0cs1cs2c, s0cs1cq0w, s0cs1cq0t,
s0cs1wq0w, s0cs1wq0t, s0ws1cs2c, s0ws1cq0t
extended
s0llwc, s0lrwc, s0luwc, s0rlwc, s0rrwc,
s0ruwc, s0ulwc, s0urwc, s0uuwc, s1llwc,
s1lrwc, s1luwc, s1rlwc, s1rrwc, s1ruwc
separator
s0wp, s0wcp, s0wq, s0wcq, s0cs1cp, s0cs1cq
s1wp, s1wcp, s1wq, s1wcq
</equation>
<figureCaption confidence="0.993279">
Figure 3: Feature templates
</figureCaption>
<bodyText confidence="0.985230357142857">
al. (2013), which look deeper into the trees on the
stack, i.e., up to the grand-children instead of only
to children.
Fig. 3 shows all the feature templates. Note that
si and qi stands for the ith stack and queue item,
w stands for the head word, t for the head tag and
c for the constituent label (w, t and c are identi-
cal on POS-level). l and r (ll and rr) represent
the left and right children (grand-children) of the
element on the stack; u handles the unary case.
Concerning the separator features, p is a unique
separator punctuation between the head words of
so and sl, q is the count of any separator punctua-
tion between so and sl.
</bodyText>
<subsectionHeader confidence="0.999697">
2.3 Handling Discontinuities
</subsectionHeader>
<bodyText confidence="0.9955566">
In order to handle discontinuities, we use two
variants of a swap transition which are similar
to swap-eager and swap-lazy from Nivre (2009)
and Nivre et al. (2009). The first variant, SIN-
GLESWAP, swaps the second item of the stack
back on the queue. The second variant COM-
POUNDSWAPi bundles a maximal number of ad-
jacent swaps. It swaps i items starting from the
second item on the stack, with 1 ≤ i &lt; |s|. Both
swap operations can only be applied if
</bodyText>
<listItem confidence="0.994371333333333">
1. the item has not yet been FINISHed and the
last transition has not been a transition with
the root category,
2. the queue is not empty,
3. all elements to be swapped are pre-terminals,
and
</listItem>
<figure confidence="0.998779">
Das
PDS
wollen
VMFIN
wir
PPER
umkehren
VVINF
@S
VP
S
</figure>
<page confidence="0.9905">
1204
</page>
<bodyText confidence="0.987749235294118">
4. if the first item of the stack has a lower index
than the second (this inhibits swap loops).
SINGLESWAP can only been applied if there
are at least two items on the stack. For COM-
POUNDSWAPi, there must be at least i + 1 items.
Transition sequences are extracted from tree-
bank trees with an algorithm that traverses the tree
bottom-up and collects the transitions. For a given
tree T, intuitively, the algorithm works as follows.
We start out with a queue t containing the pre-
terminals of T, a stack Q that receives finished con-
stituents, a counter s that keeps track of the num-
ber of terminals to be swapped, and an empty se-
quence r that holds the result. First, the first ele-
ment of t is pushed on Q and removed from t.
While |Q |&gt; 0 or |t |&gt; 0, we repeat the follow-
ing two steps.
</bodyText>
<listItem confidence="0.9762565">
1. Repeat while transitions can be added:
(a) if the top two elements on Q, l and r,
have the same parent p labeled X and
l/r is the head of p, add BINARY-X-l/r
to r, pop two elements from Q and push
p;
(b) if the top element on Q is the only child
of its parent p labeled X, add UNARY-
X, pop an element of Q and push p.
2. If |t |&gt; 0, while the first element of t is not
</listItem>
<bodyText confidence="0.9949845">
equal to the leftmost pre-terminal dominated
by the right child of the parent of the top el-
ement on Q (i.e., while there are terminals
that must be swapped), add SHIFT to r, in-
crement s, push the first element of t on Q
and remove it from t. Finally, add another
SHIFT to r, push first element of t to Q and
remove it from t (this will contribute to the
next reduction). If s &gt; 0, we must swap. Ei-
ther we add s many SWAP transitions or one
COMPOUNDSWAPs to r. Then we move s
many elements from Q to the front of t, start-
ing with the second element of Q. Finally we
set s = 0.
As an example, consider the transition sequence
we would extract from the tree in Fig. 2. Using
SINGLESWAP, we would obtain SHIFT, SHIFT,
SHIFT, SHIFT, SINGLESWAP, SINGLESWAP,
BINARY-VP-R, SHIFT, BINARY-@S-R, SHIFT,
BINARY-S-L, FINISH. Using COMPOUNDSWAPi,
instead of two SINGLESWAPs, we would just ob-
tain a single COMPOUNDSWAP2.
</bodyText>
<equation confidence="0.9454995">
unigrams
s0xwc, s1xwc, s2xwc, s3xwc,
s0xtc, s1xwc, s2xtc, s3xwc,
s0xy, s1xy, s2xy, s3xy
bigrams
s0xs1c, s0xs1w, s0xs1x, s0ws1x, s0cs1x,
s0xs2c, s0xs2w, s0xs2x, s0ws2x, s0cs2x,
s0ys1y, s0ys2y, s0xq0t, s0xq0w
</equation>
<figureCaption confidence="0.982003">
Figure 4: Features for discontinuous structures
</figureCaption>
<bodyText confidence="0.999988458333333">
We explore two methods which improve the
performance on discontinuous structures. Even
though almost a third of all sentences in the Ger-
man NeGra and TiGer treebanks contains at least
one discontinuous constituent, among all con-
stituents, the discontinuous ones are rare, making
up only around 2%. The first, simple method ad-
dresses this sparseness by raising the importance
of the features that model the actual discontinu-
ities by counting all feature occurrences at a gold
swap transition twice (IMPORTANCE).
Secondly, we use a new feature set (DISCO)
with bigram and unigram features that conveys in-
formation about discontinuities. The features con-
dition the possible occurrence of a gap on previ-
ous gaps and their properties.2 The feature tem-
plates are shown in Fig. 4. x denotes the gap
type of a tree on the stack. There are three possi-
ble values, either “none” (tree is fully continuous),
“pass” (there is a gap at the root, i.e., this gap must
be filled later further up in the tree), or “gap” (the
root of this tree fills a gap, i.e., its children have
gaps, but the root does not). Finally, y is the sum
of all gap lengths.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995207">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999293222222222">
We use the TiGer treebank release 2.2 (TIGER),
and the NeGra treebank (NEGRA). For TIGER,
we use the first half of the last 10,000 sentences
for development and the second half for testing.3
We also recreate the split of Hall and Nivre (2008)
(TIGERHN), for which we split TiGer in 10 parts,
assigning sentence i to part imod10. The first of
those parts is used for testing, the concatenation of
the rest for training.
</bodyText>
<footnote confidence="0.991017714285714">
2See Maier and Lichte (2011) for a formal account on
gaps in treebanks.
3This split, which corresponds to the split used in the
SPMRL 2013 shared task (Seddah et al., 2013), was proposed
in Farkas and Schmid (2012). We exclude sentences 46,234
and 50,224, because of annotation errors. Both contain nodes
with more than one parent node.
</footnote>
<page confidence="0.968861">
1205
</page>
<bodyText confidence="0.97474255">
F1 on dev. set
From NeGra, we exclude all sentences longer
than 30 words (in order to make a comparison
with rparse possible, see below), and split off
the last 10% of the treebank for testing, as well
as the previous 10% for development. As a pre-
processing step, in both treebanks we remove spu-
rious discontinuities that are caused by material
which is attached to the virtual root node (mainly
punctuation). All such elements are attached to the
least common ancestor node of their left and right
terminal neighbors (as proposed by Levy (2005),
p. 163). We furthermore create a continuous vari-
ant NEGRACF of NEGRA with the method usu-
ally used for PCFG parsing: For all maximal con-
tinuous parts of a discontinuous constituent, a sep-
arate node is introduced (Boyd, 2007). Subse-
quently, all nodes that do not cover the head child
of the discontinuous constituent are removed.
No further preprocessing or cleanup is applied.
</bodyText>
<subsectionHeader confidence="0.991212">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9684756">
Our parser is implemented in Java. We run all our
experiments with Java 8 on an Intel Core i5, al-
locating 15 GB per experiment. All experiments
are carried out with gold POS tags, as in previous
work on shift-reduce constituency parsing (Zhang
and Clark, 2009). Grammatical function labels are
discarded.
For the evaluation, we use the corresponding
module of discodop.4 We report several metrics
(as implemented in discodop):
</bodyText>
<listItem confidence="0.995183">
• Extended labeled bracketing, in which a
bracket for a single node consists of its la-
bel and a set of pairs of indices, delimiting
the continuous blocks it covers. We do not
include the root node in the evaluation and
ignore punctuation. We report labeled preci-
sion, recall and Fl, as well as exact match (all
brackets correct).
• Leaf-ancestor (Sampson and Babarczy,
2003), for which we consider all paths from
leaves to the root.
• Tree edit distance (Emms, 2008), which con-
sists of the minimum edit distance between
gold tree and parser output.
</listItem>
<bodyText confidence="0.594139">
Aside from a full evaluation, we also evaluate only
the constituents that are discontinuous.
</bodyText>
<footnote confidence="0.746768">
4http://github.com/andreasvc/discodop
</footnote>
<figure confidence="0.7276955">
2 4 6 8 10 12 14 16 18 20
Iteration
</figure>
<figureCaption confidence="0.98312">
Figure 5: NEGRA dev results (Fi) for different
beam sizes
</figureCaption>
<bodyText confidence="0.996574666666667">
We perform 20 training iterations unless indi-
cated otherwise. When training stops, we average
the model (as in Daum´e III (2006)).
We run further experiments with rparse5
(Kallmeyer and Maier, 2013) to facilitate a com-
parison with a grammar-based parser.
</bodyText>
<subsectionHeader confidence="0.746469">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999707">
We start with discontinuous parsing experiments
on NEGRA and TIGER, followed by continu-
ous parsing experiments, and a comparison to
grammar-based parsing.
</bodyText>
<subsectionHeader confidence="0.599383">
3.3.1 Discontinuous Parsing
</subsectionHeader>
<bodyText confidence="0.99995095">
NeGra The first goal is to determine the effect
of different beam sizes with BASELINE features
and the COMPOUNDSWAPZ operation. We run ex-
periments with beam sizes 1, 2, 4 and 8; Fig. 5
shows the results obtained on the dev set after
each iteration. Fig. 6 shows the average decod-
ing speed during each iteration for each beam size
(both smoothed).
Tracking two items instead of one results in a
large improvement. Raising the beam size from
2 to 4 results in a smaller improvement. The im-
provement obtained by augmenting the beam size
from 4 to 8 is even smaller. This behavior is mir-
rored by the parsing speeds during training: The
differences in parsing speed roughly align with the
result differences. Note that fast parsing during
training means that the parser does not perform
well (yet) and that therefore, early update is done
more often. Note finally that the average parsing
speeds on the test set after the last training iteration
</bodyText>
<footnote confidence="0.739272">
5http://github.com/wmaier/rparse
</footnote>
<figure confidence="0.969319583333333">
74
72
70
68
66
64
62
60
1
2
4
8
</figure>
<page confidence="0.89918">
1206
</page>
<table confidence="0.998371666666667">
LR LP All E LR Discont. only E
LF, LP LF,
BASELINE combined with
SWAP 74.74 75.60 75.17 43.54 15.70 15.82 15.76 12.31
COMPOUNDSWAPi 75.60 76.37 75.98 43.04 16.46 19.96 18.05 12.05
BASELINE + COMPOUNDSWAP{ combined with
SEPARATOR 75.20 75.74 75.47 42.61 13.11 16.73 14.70 9.89
EXTENDED 76.15 76.92 76.53 44.46 15.09 20.62 17.43 12.70
DISCO 75.86 76.39 76.12 43.94 15.42 22.95 18.45 12.86
IMPORTANCE 75.72 76.61 76.16 43.86 16.16 20.42 18.04 12.38
BASELINE + COMPOUNDSWAP{ + DISCO combined with
EXTENDED 76.68 77.19 76.93 44.10 15.27 26.88 19.47 13.61
EXTENDED + SEPARATOR 76.21 76.45 76.33 43.29 15.57 26.56 19.63 13.52
IMPORTANCE 76.22 76.86 76.54 43.75 16.01 29.41 20.73 13.89
EXTENDED + IMPORTANCE 76.76 77.13 76.95 44.30 15.09 28.86 19.82 13.23
</table>
<tableCaption confidence="0.999865">
Table 1: Results NEGRA, beam size 8
</tableCaption>
<figure confidence="0.7595965">
2 4 6 8 10 12 14 16 18 20
Iteration
</figure>
<figureCaption confidence="0.917899">
Figure 6: NEGRA dev average parsing speeds per
</figureCaption>
<bodyText confidence="0.943955444444444">
sentence for different beam sizes
range from 640 sent./sec. (greedy) to 80 sent./sec.
(beam size 8).
For further experiments on NeGra, we choose
a beam size of 8. Tab. 1 shows the bracketing
scores for various parser setups. In Tab. 2, the
corresponding TED and Leaf-Ancestor scores are
shown.
In the first block of the tables, we com-
</bodyText>
<table confidence="0.999630642857143">
pare SWAP with COMPOUNDSWAPi. TED On all
LA
BASELINE combined with
SWAP 89.19 91.62
COMPOUNDSWAPi 89.60 91.93
BASELINE + COMPOUNDSWAP{ combined with
SEPARATOR 89.41 91.77
EXTENDED 89.68 91.99
DISCO 89.42 91.83
BASELINE + COMPOUNDSWAP{ + DISCO combined with
IMPORTANCE 89.64 91.90
EXTENDED 89.68 91.99
EXTENDED + SEPARATOR 89.52 91.86
EXTENDED + IMPORTANCE 89.80 91.98
</table>
<tableCaption confidence="0.999692">
Table 2: Results NEGRA TED and Leaf-Ancestor
</tableCaption>
<bodyText confidence="0.999942675675676">
constituents, the latter beats the former by 0.8
(F1). On discontinuous constituents, using COM-
POUNDSWAPi gives an improvement of more than
four points in precision and of about 0.8 points
in recall. A manual analysis confirms that as
expected, particularly discontinuous constituents
with large gaps profit from bundling swap transi-
tions.
In the second block, we run the BASELINE
features with COMPOUNDSWAPi combined with
SEPARATOR, EXTENDED and DISCO. The SEP-
ARATOR features were not as successful as they
were for Zhang and Clark (2009). All scores for
discontinuous constituents drop (compared to the
baseline). The EXTENDED features are more ef-
fective and give an improvement of about half a
point F1 on all constituents, as well as the highest
exact match among all experiments. On discontin-
uous constituents, precision raises slightly but we
loose about 1.4% in recall (compared to the base-
line). The latter seems to be due to the fact that
in comparison to the baseline, with EXTENDED,
more sentences get erroneously analyzed as not
containing any crossing branches. This effect can
be explained with data sparseness and is less pro-
nounced when more training data is available (see
below). Similarly to EXTENDED, the new DISCO
features lead to a slight gain over the baseline (on
all constituents). As with EXTENDED, on discon-
tinuous constituents, we again gain precision (3%)
but loose recall (0.5%), because more sentences
wrongly analyzed as not having discontinuities
than in the BASELINE. A category-based evalua-
tion of discontinuous constituents reveals that EX-
TENDED has an advantage over DISCO when con-
sidering all constituents. However, we can also see
that the DISCO features yield better results than
</bodyText>
<figure confidence="0.9907074">
Parsing speeds (sent./sec.)
2000
1500
1000
500
0
1
2
4
8
</figure>
<page confidence="0.988863">
1207
</page>
<bodyText confidence="0.999938088235294">
EXTENDED particularly on the frequent discontin-
uous categories (NP, VP, AP, PP), which indicates
that the information about gap type and gap length
is useful for the recovery of discontinuities. IM-
PORTANCE (see Sec. 2.3) is not very successful,
yielding results which lie in the vicinity of those
of the BASELINE.
In the third block of the tables, we test the per-
formance of the DISCO features in combination
with other techniques, i.e., we use the BASELINE
and DISCO features with COMPOUNDSWAPZ and
combine it with EXTENDED and SEPARATOR fea-
tures as well as with the IMPORTANCE strategy.
All experiments beat the BASELINE/DISCO com-
bination in terms of F1. EXTENDED and DISCO
give a cumulative advantage, resulting in an in-
crease of precision of almost 4%, resp. over 6% on
discontinuous constituents, compared to the use
of DISCO, resp. EXTENDED alone. Adding the
SEPARATOR features to this combination does not
bring an advantage. The IMPORTANCE strategy
is the most successful one in combination with
DISCO, causing a boost of almost 10% on preci-
sion of discontinuous constituents, leading to the
highest overall discontinuous F1 of 29.41 (notably
more than 12 points higher than the baseline); also
on all constituents we obtain the third-highest F1.
Combining DISCO with IMPORTANCE and EX-
TENDED leads to the highest overall F1 on all con-
stituents of 76.95, however, the results on discon-
tinuous constituents are slightly lower than for IM-
PORTANCE alone. This confirms the previously
observed behavior: The EXTENDED features help
when considering all constituents, but they do not
seem to be effective for the recovery of disconti-
nuities in particular.
In the TED and LA scores (Tab. 2), we see much
less variation than in the bracketing scores. As re-
ported in the literature (e.g., Rehbein and van Gen-
abith (2007)), this is because of the fact that with
bracketing evaluation, a single wrong attachment
can “break” brackets which otherwise would be
counted as correct. Nevertheless, the trends from
bracketing evaluation repeat.
To sum up, the COMPOUNDSWAPZ operation
works better than SWAP because the latter misses
long gaps. The most useful feature sets were EX-
TENDED and DISCO, both when used indepen-
dently and when used together. DISCO was partic-
ularly useful for discontinuous constituents. SEP-
ARATOR yielded no usable improvements. IM-
PORTANCE has also proven to be effective, yield-
ing the best results on discontinuous constituents
(in combination with DISCO). Over almost all ex-
periments, a common error is that on root level,
CS and S get confused, indicating that the present
features do not provide sufficient information for
disambiguation of those categories. We can also
confirm the tendency that discontinuous VPs in
relatively short sentences are recognized correctly,
as reported by Versley (2014).
TiGer We now repeat the most successful exper-
iments on TIGER. Tab. 3 shows the parsing results
for the test set.
Some of the trends seen on the experiments with
NEGRA are repeated. EXTENDED and DISCO
yields an improvement on all constituents. How-
ever, now not only DISCO, but also EXTENDED
lead to improved scores on discontinuous con-
stituents. As mentioned above, this can be ex-
plained with the fact that for the EXTENDED fea-
tures to be effective, the amount of training data
available in NEGRA was not enough. Other than
in NEGRA, the DISCO features are now more ef-
fective when used alone, leading to the highest
overall F1 on discontinuous constituents of 19.45.
They are, however, less effective in combination
with EXTENDED. This is partially remedied by
giving the swap transitions more IMPORTANCE,
which leads to the highest overall F1 on all con-
stituents of 74.71.
The models we learn are sparse, therefore, as
mentioned above, we can exploit the work of
Goldberg and Elhadad (2011). They propose to
only include the weight of a feature in the compu-
tation of a score if it has been seen more than MIN-
UPDATE times. We repeat the BASELINE experi-
ment with two different MINUPDATE settings (see
Tab. 3). As expected, the MINUPDATE models are
much smaller. The final model with the baseline
experiment uses 8.3m features (parsing speed on
test set 73 sent./sec.), with MINUPDATE 5 3.3m
features (121 sent./sec.) and with MINUPDATE
10 1.8m features (124 sent./sec.). With MINUP-
DATE 10, the results do degrade. However, with
MINUPDATE 5 in addition to the faster parsing we
consistently improve over the baseline.
Finally, in order to check the convergence, we
run a further experiment in which we limit train-
ing iterations to 40 instead of 20, together with
beam size 4. We use the BASELINE features with
COMPOUNDSWAPZ combined with DISCO, EX-
</bodyText>
<page confidence="0.964157">
1208
</page>
<table confidence="0.9837975">
LR LP All E LR Discont. only E
LF1 LP LF1
BASELINE + COMPOUNDSWAPi 72.69 74.77 73.71 36.47 16.08 18.72 17.30 12.96
+ EXTENDED 73.52 75.50 74.50 37.26 15.86 20.04 17.71 13.20
+ DISCO 73.77 75.35 74.55 37.08 16.68 23.32 19.45 14.43
+ DISCO + EXTENDED 73.97 75.29 74.62 37.54 15.56 22.21 18.30 13.64
+ DISCO + EXTENDED + IMPORTANCE 74.01 75.41 74.71 37.20 15.61 23.53 18.77 13.84
BASELINE + COMPOUNDSWAPi with
MINUPDATE 5 73.04 75.03 74.03 37.36 16.25 19.72 17.82 13.28
MINUPDATE 10 72.71 74.55 73.62 36.85 15.78 18.56 17.06 13.07
</table>
<tableCaption confidence="0.99927">
Table 3: Results TIGER, beam size 4
</tableCaption>
<table confidence="0.999651333333333">
LR LP LF1 E LR LP LF1 E
BASELINE 81.89 82.49 82.19 49.05 All constituents 69.72 68.85 69.28 33.89
EXTENDED 82.20 82.70 82.45 49.54 Disc. only 25.77 27.51 26.61 17.77
</table>
<tableCaption confidence="0.999882">
Table 4: Results NEGRACF Table 5: Results NEGRA rparse
</tableCaption>
<bodyText confidence="0.9993998">
TENDED, and IMPORTANCE. The parsing speed
on the test set drops to around 39 sentences per
second. However, we achieve 75.10 F1, i.e., a
slight improvement over the experiments in Tab. 3
that confirms the tendencies visible in Fig. 5.
</bodyText>
<subsectionHeader confidence="0.895722">
3.3.2 Continuous Parsing
</subsectionHeader>
<bodyText confidence="0.999867027777778">
We investigate the impact of the swap transitions
on both speed and parsing results by running an
experiment with NEGRACF using the BASELINE
and EXTENDED features. The corresponding re-
sults are shown in Tab. 4.
Particularly high frequency categories (NP, VP,
S) are much easier to find in the continuous case
and show large improvements. This explains why
without the swap transition, F1 with BASELINE
features is 6.9 points higher than the F1 on discon-
tinuous constituents (with COMPOUNDSWAPZ).
With the EXTENDED features, we obtain a small
improvement.
Note that with the shift-reduce approach, the
difference between the computational cost of pro-
ducing discontinuous constituents vs. the cost of
producing continuous constituents is much lower
than for a grammar-based approach. When pro-
ducing continuous constituents, parsing is only
20% faster than with the swap transition, namely
97 instead of 81 sentences per second.
In order to give a different perspective on the
role of discontinuous constituents, we perform two
further evaluations. First, we remove the dis-
continuities from the output of the discontinuous
baseline parser using the procedure described in
Sec. 3.1 and evaluate the result against the con-
tinuous gold data. We obtain an F1 of 76.70,
5.5 points lower than the continuous baseline.
Secondly, we evaluate the output of the continu-
ous baseline parser against the discontinuous gold
data. This leads to an F1 78.89, 2.9 point more
than the discontinuous baseline. Both evaluations
confirm the intuition that parsing is much easier
when discontinuities (i.e., in our case the swap
transition) do not have to be considered.
</bodyText>
<subsectionHeader confidence="0.989814">
3.3.3 Comparison with other Parsers
</subsectionHeader>
<bodyText confidence="0.999900703703703">
rparse In order to compare our parser with a
grammar-based approach, we now parse NEGRA
with rparse, with the same training and test sets as
before (i.e., we do not use the development set).
We employ markovization with v = 1, h = 2 and
head driven binarization with binary top and bot-
tom productions.
The first thing to notice is that rparse is much
slower than our parser. The average parsing speed
is about 0.3 sent./sec.; very long sentences require
over a minute to be parsed. The parsing results
are shown in Tab. 5. They are about 5 points
worse than those reported by Kallmeyer and Maier
(2013). This is due to the fact that they train on the
first 90% of the treebank, and not on the first 80%
as we do, which leads to an increased number of
unparsed sentences. In comparison to the baseline
setting of the shift-reduce parser with beam size
8, the results are around 10 points worse. How-
ever, rparse reaches an F1 of 26.61 on discontinu-
ous constituents, which is 5.9 points more than we
achieved with the best setting with our parser.
In order to investigate why the grammar-based
approach outperforms our parser on discontinuous
constituents, we count the frequency of LCFRS
productions of a certain gap degree in the bina-
rized grammar used in the rparse experiment. The
</bodyText>
<page confidence="0.977311">
1209
</page>
<table confidence="0.9968836">
LF1 E
Versley (2014) 74.23 37.32
this work 79.52 44.32
H&amp;N (2008) 79.93 37.78
F&amp;M (2015) 85.53 51.21
</table>
<tableCaption confidence="0.999662">
Table 6: Results TIGERHN, sentence length ≤ 40
</tableCaption>
<bodyText confidence="0.971638761904762">
average occurrence count of rules with gap degree
0 is 12.18. Discontinuous rules have a much lower
frequency, the average count of productions with
one, two and three gaps being 3.09, 2.09, and 1.06,
respectively. In PCFG parsing, excluding low fre-
quency productions does not have a large effect
(Charniak, 1996); however, this does not hold for
LCFRS parsing, where they have a major influ-
ence (cf. Maier (2013, p. 205)): This means that
removing low frequency productions has a nega-
tive impact on the parser performance particularly
concerning discontinuous structures; however, it
also means that low frequency discontinuous pro-
ductions get triggered reliably. This hypothesis
is confirmed by the fact that the our parser per-
forms much worse on discontinuous constituents
with a very low frequency (such as CS, making
up only 0.62% of all discontinuous constituents)
than it performs on those with a high frequency
(such as VP, making up 60.65% of all discontin-
uous constituents), while rparse performs well on
the low frequency constituents.
EaFi and Dependency Parsers We run an ex-
periment with 40 iterations on TIGERHN, using
DISCO, EXTENDED and IMPORTANCE. Tab. 6
lists the results, together with the correspond-
ing results of Versley (2014), Hall and Nivre
(2008) (H&amp;N) and Fern´andez-Gonz´alez and Mar-
tins (2015) (F&amp;M).
Our results exceed those of EaFi6 and the ex-
act match score of H&amp;N. We are outperformed by
the F&amp;M parser. Note, that particularly the com-
parison to EaFi must be handled with care, since
Versley (2014) uses additional preprocessing: PP-
internal NPs are annotated explicitly, and the par-
enthetical sentences are changed to be embedded
by their enclosing sentence (instead of vice versa).
We postpone a thorough comparison with both
EaFi and the dependency parsers to future work.
6Note that Versley (2014) reports a parsing speed of 40-
55 sent./sec.; depending on the beam size and the training set
size, per second, our parser parses 39-640 sentences.
</bodyText>
<subsectionHeader confidence="0.595503">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999586315789474">
To our knowledge, surprisingly, numerical scores
for discontinuous constituents have not been re-
ported anywhere in previous work. The relatively
low overall performance with both grammar-based
and shift-reduce based parsing, along with the fact
that the grammar-based approach outperforms the
shift-reduce approach, is striking. We have shown
that it is possible to push the precision on discon-
tinuous constituents, but not the recall, to the level
of what can be achieved with a grammar-based ap-
proach.
Particularly the outcome of the experiments
involving the EXTENDED features and IMPOR-
TANCE drives us to the conclusion that the major
problem when parsing discontinuous constituents
is data sparseness. More features cannot be the
only solution: A more reliable recognition of dis-
continuous constituents requires a more robust
learning from larger amounts of data.
</bodyText>
<sectionHeader confidence="0.999295" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999995363636364">
We have presented a shift-reduce parser for dis-
continuous constituents which combines previous
work in shift-reduce parsing for continuous con-
stituents with recent work in easy-first parsing of
discontinuous constituents. Our experiments con-
firm that an incremental shift-reduce architecture
with a swap transition can indeed be used to parse
discontinuous constituents. The swap transition is
associated with a low computational cost. We have
obtained a speed-up of up to 2,000% in compar-
ison to the grammar-based rparse, and we have
shown that we obtain better results than with the
grammar-based parser, even though the grammar-
based strategy does better at the reconstruction of
discontinuous constituents.
In future work, we will concentrate on methods
that could remedy the data sparseness concerning
discontinuous constituents, such as self-training.
Furthermore, we will experiment with larger fea-
ture sets that add lexical information. An for-
mal investigation of the expressivity of our parsing
model is currently under way.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992798">
I wish to thank Miriam Kaeshammer for enlight-
ening discussions and the three anonymous re-
viewers for helpful comments and suggestions.
This work was partially funded by Deutsche
Forschungsgemeinschaft (DFG).
</bodyText>
<page confidence="0.983867">
1210
</page>
<sectionHeader confidence="0.98963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998768486238532">
Krasimir Angelov and Peter Ljungl¨of. 2014. Fast
statistical parsing with parallel multiple context-free
grammars. In Proceedings of the 14th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 368–376, Gothenburg,
Sweden.
John Bauer. 2014. Stanford shift-reduce parser.
http://nlp.stanford.edu/software/
srparser.shtml.
Adriane Boyd. 2007. Discontinuity revisited: An
improved conversion to context-free representations.
In Proceedings of The Linguistic Annotation Work-
shop (LAW) at ACL 2007, pages 41–44, Prague,
Czech Republic.
Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 212–
216, Portland, OR.
Eugene Charniak. 1996. Tree-bank grammars. Tech-
nical Report CS-96-02, Department of Computer
Science, Brown University, Providence, RI.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain.
Hal Daum´e III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA.
P´eter Dienes and Amit Dubey. 2003. Antecedent re-
covery: Experiments with a trace tagger. In Pro-
ceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 33–40,
Sapporo, Japan.
Martin Emms. 2008. Tree distance and some
other variants of Evalb. In Proceedings of the
Sixth International Language Resources and Eval-
uation (LREC’08), pages 1373–1379, Marrakech,
Morocco.
Kilian Evang and Laura Kallmeyer. 2011. PLCFRS
parsing of English discontinuous constituents. In
Proceedings of the 12th International Conference on
Parsing Technologies (IWPT 2011), pages 104–116,
Dublin, Ireland.
Richard Farkas and Helmut Schmid. 2012. Forest
reranking through subtree ranking. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1038–1047, Jeju
Island, Korea.
Daniel Fern´andez-Gonz´alez and Andr´e F. T. Martins.
2015. Parsing as reduction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and Teh 7th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing,
Beijing, China. To appear.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 742–750, Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2011. Learning
sparser perceptron models. Technical report, Ben
Gurion University of the Negev.
Johan Hall and Joakim Nivre. 2008. Parsing dis-
continuous phrase structure with grammatical func-
tions. In Bengt Nordstr¨om and Aarne Ranta, editors,
Advances in Natural Language Processing, volume
5221 of Lecture Notes in Computer Science, pages
169–180. Springer, Gothenburg, Sweden.
Valentin Jijkoun. 2003. Finding non-local dependen-
cies: Beyond pattern matching. In The Compan-
ion Volume to the Proceedings of 41st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 37–43, Sapporo, Japan.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 136–143, Philadelphia, PA.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics,
39(1):87–119.
Roger Levy and Christopher Manning. 2004. Deep
dependencies from context-free statistical parsers:
Correcting the surface dependency approximation.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics (ACL’04), Main
Volume, pages 327–334, Barcelona, Spain.
Roger Levy. 2005. Probabilistic Models of Word Or-
der and Syntactic Discontinuity. Ph.D. thesis, Stan-
ford University.
Wolfgang Maier and Timm Lichte. 2011. Characteriz-
ing discontinuity in constituent treebanks. In For-
mal Grammar. 14th International Conference, FG
2009. Bordeaux, France, July 25-26, 2009. Revised
Selected Papers, volume 5591 of LNCS/LNAI, pages
167–182. Springer-Verlag.
Wolfgang Maier and Anders Søgaard. 2008. Tree-
banks and mild context-sensitivity. In Philippe
de Groote, editor, Proceedings of the 13th Confer-
ence on Formal Grammar (FG-2008), pages 61–76,
Hamburg, Germany. CSLI Publications.
</reference>
<page confidence="0.779147">
1211
</page>
<reference confidence="0.99980262">
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceed-
ings of the Eleventh International Conference on
Tree Adjoining Grammars and Related Formalisms
(TAG+11), pages 126–134, Paris, France.
Wolfgang Maier. 2013. Parsing Discontinuous Struc-
tures. Dissertation, University of T¨ubingen.
Joakim Nivre, Marco Kuhlmann, and Johan Hall.
2009. An improved oracle for dependency parsing
with online reordering. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT’09), pages 73–76, Paris, France.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351–359, Singapore.
Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of the
16th Nordic Conference of Computational Linguis-
tics NODALIDA-2007, pages 372–379, Tartu, Esto-
nia.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the Ninth International Workshop on Parsing
Technology, pages 125–132, Vancouver, BC.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Journal
of Natural Language Engineering, 9:365–380.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized PCFGs and slash features. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 177–184, Sydney, Australia.
Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie
Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer
Foster, Iakes Goenaga, Koldo Gojenola Gallete-
beitia, Yoav Goldberg, Spence Green, Nizar Habash,
Marco Kuhlmann, Wolfgang Maier, Yuval Mar-
ton, Joakim Nivre, Adam Przepi´orkowski, Ryan
Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Woli´nski, and Alina Wr´oblewska.
2013. Overview of the SPMRL 2013 shared
task: A cross-framework evaluation of parsing
morphologically rich languages. In Proceedings
of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 146–182,
Seattle, WA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191–229.
Andreas van Cranenburgh and Rens Bod. 2013. Dis-
continuous parsing with an efficient and accurate
DOP model. In Proceedings of The 13th Interna-
tional Conference on Parsing Technologies, Nara,
Japan.
Andreas van Cranenburgh. 2012. Efficient parsing
with linear context-free rewriting systems. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 460–470, Avignon, France.
Yannick Versley. 2014. Experiments with easy-first
nonprojective constituent parsing. In Proceedings
of the First Joint Workshop on Statistical Parsing
of Morphologically Rich Languages and Syntactic
Analysis of Non-Canonical Languages, pages 39–
53, Dublin, Ireland.
K. Vijay-Shanker, David Weir, and Aravind K. Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of the 25th An-
nual Meeting of the Association for Computational
Linguistics, pages 104–111, Stanford, CA.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the Chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT’09), pages 162–171, Paris, France.
Yue Zhang and Stephen Clark. 2011a. Shift-reduce
CCG parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
683–692, Portland, OR.
Yue Zhang and Stephen Clark. 2011b. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012.
Exploiting lexical dependencies from large-scale
data for better shift-reduce constituency parsing. In
Proceedings of COLING 2012, pages 3171–3186,
Mumbai, India.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434–443, Sofia, Bulgaria.
</reference>
<page confidence="0.993341">
1212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.116940">
<title confidence="0.996635">Discontinuous Incremental Shift-Reduce Parsing</title>
<author confidence="0.777288">Wolfgang</author>
<affiliation confidence="0.876563">Universit¨at Institut f¨ur Sprache und</affiliation>
<address confidence="0.362832">Universit¨atsstr. 1, 40225 D¨usseldorf,</address>
<email confidence="0.976934">maierw@hhu.de</email>
<abstract confidence="0.994885857142857">We present an extension to incremental shift-reduce parsing that handles discontinuous constituents, using a linear classifier and beam search. We achieve very high parsing speeds (up to 640 sent./sec.) accurate results (up to 79.52</abstract>
<intro confidence="0.409557">TiGer).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Krasimir Angelov</author>
<author>Peter Ljungl¨of</author>
</authors>
<title>Fast statistical parsing with parallel multiple context-free grammars.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>368--376</pages>
<location>Gothenburg,</location>
<marker>Angelov, Ljungl¨of, 2014</marker>
<rawString>Krasimir Angelov and Peter Ljungl¨of. 2014. Fast statistical parsing with parallel multiple context-free grammars. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 368–376, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bauer</author>
</authors>
<title>Stanford shift-reduce parser.</title>
<date>2014</date>
<note>http://nlp.stanford.edu/software/ srparser.shtml.</note>
<contexts>
<context position="5662" citStr="Bauer (2014)" startWordPosition="881" endWordPosition="882"> by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluation, where we pay particular attention to the parser performance on discontinuous structures, we show among other things that surprisingly, a grammarbased parser has an edge over a shift-reduce approach concerning the reconstruction of discontinuous constituents. The remainder of the paper is structured as f</context>
</contexts>
<marker>Bauer, 2014</marker>
<rawString>John Bauer. 2014. Stanford shift-reduce parser. http://nlp.stanford.edu/software/ srparser.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
</authors>
<title>Discontinuity revisited: An improved conversion to context-free representations.</title>
<date>2007</date>
<booktitle>In Proceedings of The Linguistic Annotation Workshop (LAW) at ACL</booktitle>
<pages>41--44</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="16999" citStr="Boyd, 2007" startWordPosition="2863" endWordPosition="2864">t 10% of the treebank for testing, as well as the previous 10% for development. As a preprocessing step, in both treebanks we remove spurious discontinuities that are caused by material which is attached to the virtual root node (mainly punctuation). All such elements are attached to the least common ancestor node of their left and right terminal neighbors (as proposed by Levy (2005), p. 163). We furthermore create a continuous variant NEGRACF of NEGRA with the method usually used for PCFG parsing: For all maximal continuous parts of a discontinuous constituent, a separate node is introduced (Boyd, 2007). Subsequently, all nodes that do not cover the head child of the discontinuous constituent are removed. No further preprocessing or cleanup is applied. 3.2 Experimental Setup Our parser is implemented in Java. We run all our experiments with Java 8 on an Intel Core i5, allocating 15 GB per experiment. All experiments are carried out with gold POS tags, as in previous work on shift-reduce constituency parsing (Zhang and Clark, 2009). Grammatical function labels are discarded. For the evaluation, we use the corresponding module of discodop.4 We report several metrics (as implemented in discodop</context>
</contexts>
<marker>Boyd, 2007</marker>
<rawString>Adriane Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In Proceedings of The Linguistic Annotation Workshop (LAW) at ACL 2007, pages 41–44, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>David Chiang</author>
<author>Yoav Goldberg</author>
</authors>
<title>Language-independent parsing with empty elements.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>212--216</pages>
<location>Portland, OR.</location>
<contexts>
<context position="1886" citStr="Cai et al., 2011" startWordPosition="288" endWordPosition="291">nches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done with Linear Conte</context>
</contexts>
<marker>Cai, Chiang, Goldberg, 2011</marker>
<rawString>Shu Cai, David Chiang, and Yoav Goldberg. 2011. Language-independent parsing with empty elements. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 212– 216, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<tech>Technical Report CS-96-02,</tech>
<institution>Department of Computer Science, Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="32378" citStr="Charniak, 1996" startWordPosition="5406" endWordPosition="5407">inuous constituents, we count the frequency of LCFRS productions of a certain gap degree in the binarized grammar used in the rparse experiment. The 1209 LF1 E Versley (2014) 74.23 37.32 this work 79.52 44.32 H&amp;N (2008) 79.93 37.78 F&amp;M (2015) 85.53 51.21 Table 6: Results TIGERHN, sentence length ≤ 40 average occurrence count of rules with gap degree 0 is 12.18. Discontinuous rules have a much lower frequency, the average count of productions with one, two and three gaps being 3.09, 2.09, and 1.06, respectively. In PCFG parsing, excluding low frequency productions does not have a large effect (Charniak, 1996); however, this does not hold for LCFRS parsing, where they have a major influence (cf. Maier (2013, p. 205)): This means that removing low frequency productions has a negative impact on the parser performance particularly concerning discontinuous structures; however, it also means that low frequency discontinuous productions get triggered reliably. This hypothesis is confirmed by the fact that the our parser performs much worse on discontinuous constituents with a very low frequency (such as CS, making up only 0.62% of all discontinuous constituents) than it performs on those with a high freq</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. Technical Report CS-96-02, Department of Computer Science, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="5078" citStr="Collins and Roark (2004)" startWordPosition="789" endWordPosition="792"> operation that allows to process input words out of order. This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gradient method to train his parser. He reports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it </context>
<context position="8989" citStr="Collins and Roark, 2004" startWordPosition="1444" endWordPosition="1447"> a beam size of n). Parsing is finished if the highest scoring item on the beam is a final item (stack holds one item labeled with the root label, queue is empty), which can be popped. Item scores are computed as in Zhang and Clark (2011b): The score of the i+1th item is computed as the sum of the score of the ith item and the dot product of a global feature weight vector and the local weight vector resulting from the changes induced by the corresponding transition to the i + 1th item. The start item has score 0. We train the global weight vector with an averaged Perceptron with early update (Collins and Roark, 2004). Parsing relies on binary trees. As in previous work, we binarize the incoming trees headoutward with binary top and bottom productions. Given a constituent X which is to be binarized, all intermediate nodes which are introduced will be labeled @X. Lexical heads are marked with Collins-style head rules. As an example, Fig. 2 shows the binarized version of the tree of Fig. 1. Finally, since we are learning a sparse model, we also exploit the work of Goldberg and Elhadad (2011) who propose to include a feature in the calculation of a score only if it has been observed ≥ MINUPDATE times. 2.2 Fea</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern</institution>
<location>California, Los Angeles, CA.</location>
<marker>Daum´e, 2006</marker>
<rawString>Hal Daum´e III. 2006. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. thesis, University of Southern California, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P´eter Dienes</author>
<author>Amit Dubey</author>
</authors>
<title>Antecedent recovery: Experiments with a trace tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>33--40</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1815" citStr="Dienes and Dubey, 2003" startWordPosition="276" endWordPosition="279">r method is to annotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Dir</context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>P´eter Dienes and Amit Dubey. 2003. Antecedent recovery: Experiments with a trace tagger. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 33–40, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emms</author>
</authors>
<title>Tree distance and some other variants of Evalb.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<pages>1373--1379</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="18068" citStr="Emms, 2008" startWordPosition="3041" endWordPosition="3042"> labels are discarded. For the evaluation, we use the corresponding module of discodop.4 We report several metrics (as implemented in discodop): • Extended labeled bracketing, in which a bracket for a single node consists of its label and a set of pairs of indices, delimiting the continuous blocks it covers. We do not include the root node in the evaluation and ignore punctuation. We report labeled precision, recall and Fl, as well as exact match (all brackets correct). • Leaf-ancestor (Sampson and Babarczy, 2003), for which we consider all paths from leaves to the root. • Tree edit distance (Emms, 2008), which consists of the minimum edit distance between gold tree and parser output. Aside from a full evaluation, we also evaluate only the constituents that are discontinuous. 4http://github.com/andreasvc/discodop 2 4 6 8 10 12 14 16 18 20 Iteration Figure 5: NEGRA dev results (Fi) for different beam sizes We perform 20 training iterations unless indicated otherwise. When training stops, we average the model (as in Daum´e III (2006)). We run further experiments with rparse5 (Kallmeyer and Maier, 2013) to facilitate a comparison with a grammar-based parser. 3.3 Results We start with discontinuo</context>
</contexts>
<marker>Emms, 2008</marker>
<rawString>Martin Emms. 2008. Tree distance and some other variants of Evalb. In Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), pages 1373–1379, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Evang</author>
<author>Laura Kallmeyer</author>
</authors>
<title>PLCFRS parsing of English discontinuous constituents.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies (IWPT 2011),</booktitle>
<pages>104--116</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="969" citStr="Evang and Kallmeyer, 2011" startWordPosition="140" endWordPosition="143">achieve very high parsing speeds (up to 640 sent./sec.) and accurate results (up to 79.52 F1 on TiGer). 1 Introduction Discontinuous constituents consist of more than one continuous block of tokens. They arise through phenomena which traditionally in linguistics would be analyzed as being the result of some kind of “movement”, such as extraposition or topicalization. The occurrence of discontinuous constituents does not necessarily depend on the degree of freedom in word order that a language allows for. They can be found, e.g., in almost equal proportions in English and German treebank data (Evang and Kallmeyer, 2011). Generally, discontinuous constituents are accounted for in treebank annotation. One annotation method consists of using trace nodes that denote the source of a movement and are co-indexed with the moved constituent. Another method is to annotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have be</context>
</contexts>
<marker>Evang, Kallmeyer, 2011</marker>
<rawString>Kilian Evang and Laura Kallmeyer. 2011. PLCFRS parsing of English discontinuous constituents. In Proceedings of the 12th International Conference on Parsing Technologies (IWPT 2011), pages 104–116, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Farkas</author>
<author>Helmut Schmid</author>
</authors>
<title>Forest reranking through subtree ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1038--1047</pages>
<location>Jeju Island,</location>
<contexts>
<context position="16101" citStr="Farkas and Schmid (2012)" startWordPosition="2707" endWordPosition="2710">e TiGer treebank release 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER, we use the first half of the last 10,000 sentences for development and the second half for testing.3 We also recreate the split of Hall and Nivre (2008) (TIGERHN), for which we split TiGer in 10 parts, assigning sentence i to part imod10. The first of those parts is used for testing, the concatenation of the rest for training. 2See Maier and Lichte (2011) for a formal account on gaps in treebanks. 3This split, which corresponds to the split used in the SPMRL 2013 shared task (Seddah et al., 2013), was proposed in Farkas and Schmid (2012). We exclude sentences 46,234 and 50,224, because of annotation errors. Both contain nodes with more than one parent node. 1205 F1 on dev. set From NeGra, we exclude all sentences longer than 30 words (in order to make a comparison with rparse possible, see below), and split off the last 10% of the treebank for testing, as well as the previous 10% for development. As a preprocessing step, in both treebanks we remove spurious discontinuities that are caused by material which is attached to the virtual root node (mainly punctuation). All such elements are attached to the least common ancestor no</context>
</contexts>
<marker>Farkas, Schmid, 2012</marker>
<rawString>Richard Farkas and Helmut Schmid. 2012. Forest reranking through subtree ranking. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1038–1047, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Fern´andez-Gonz´alez</author>
<author>Andr´e F T Martins</author>
</authors>
<title>Parsing as reduction.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and Teh 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,</booktitle>
<location>Beijing, China.</location>
<note>To appear.</note>
<marker>Fern´andez-Gonz´alez, Martins, 2015</marker>
<rawString>Daniel Fern´andez-Gonz´alez and Andr´e F. T. Martins. 2015. Parsing as reduction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and Teh 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Beijing, China. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="3962" citStr="Goldberg and Elhadad (2010)" startWordPosition="612" endWordPosition="616">s 1202–1212, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done online during parsing with a “swap” operation that allows to process input words out of order. This idea can be transferred, because also for e</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742–750, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Learning sparser perceptron models.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>Ben Gurion University of the Negev.</institution>
<contexts>
<context position="9470" citStr="Goldberg and Elhadad (2011)" startWordPosition="1527" endWordPosition="1530"> + 1th item. The start item has score 0. We train the global weight vector with an averaged Perceptron with early update (Collins and Roark, 2004). Parsing relies on binary trees. As in previous work, we binarize the incoming trees headoutward with binary top and bottom productions. Given a constituent X which is to be binarized, all intermediate nodes which are introduced will be labeled @X. Lexical heads are marked with Collins-style head rules. As an example, Fig. 2 shows the binarized version of the tree of Fig. 1. Finally, since we are learning a sparse model, we also exploit the work of Goldberg and Elhadad (2011) who propose to include a feature in the calculation of a score only if it has been observed ≥ MINUPDATE times. 2.2 Features Features are generated by applying templates to parser items. They reflect different configurations of stack and queue. As BASELINE features, we use the feature set from Zhang and Clark (2009) without the bracketing features (as used in Zhu et al. (2013)). We furthermore experiment with features that reflect the presence of separating punctuation “,”, “:”, “;” (SEPARATOR) (Zhang and Clark, 2009), and with the EXTENDED features of Zhu et unigrams s0tc, s0wc, s1tc, s1wc, s</context>
<context position="27044" citStr="Goldberg and Elhadad (2011)" startWordPosition="4507" endWordPosition="4510">, this can be explained with the fact that for the EXTENDED features to be effective, the amount of training data available in NEGRA was not enough. Other than in NEGRA, the DISCO features are now more effective when used alone, leading to the highest overall F1 on discontinuous constituents of 19.45. They are, however, less effective in combination with EXTENDED. This is partially remedied by giving the swap transitions more IMPORTANCE, which leads to the highest overall F1 on all constituents of 74.71. The models we learn are sparse, therefore, as mentioned above, we can exploit the work of Goldberg and Elhadad (2011). They propose to only include the weight of a feature in the computation of a score if it has been seen more than MINUPDATE times. We repeat the BASELINE experiment with two different MINUPDATE settings (see Tab. 3). As expected, the MINUPDATE models are much smaller. The final model with the baseline experiment uses 8.3m features (parsing speed on test set 73 sent./sec.), with MINUPDATE 5 3.3m features (121 sent./sec.) and with MINUPDATE 10 1.8m features (124 sent./sec.). With MINUPDATE 10, the results do degrade. However, with MINUPDATE 5 in addition to the faster parsing we consistently im</context>
</contexts>
<marker>Goldberg, Elhadad, 2011</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2011. Learning sparser perceptron models. Technical report, Ben Gurion University of the Negev.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>Parsing discontinuous phrase structure with grammatical functions.</title>
<date>2008</date>
<booktitle>In Bengt Nordstr¨om and Aarne Ranta, editors, Advances in Natural Language Processing,</booktitle>
<volume>5221</volume>
<pages>169--180</pages>
<publisher>Springer,</publisher>
<location>Gothenburg, Sweden.</location>
<contexts>
<context position="2210" citStr="Hall and Nivre (2008)" startWordPosition="333" endWordPosition="336">ing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done with Linear Context-Free Rewriting System (LCFRS), an extension of CFG which allows its non-terminals to cover more than one continuous block (Vijay-Shanker et al., 1987). LCFRS parsing is expensive: CYK chart parsing with a binarized grammar can be done in O(n3k) where k is the block degree, the maximal number of continuous blocks a non-t</context>
<context position="15710" citStr="Hall and Nivre (2008)" startWordPosition="2638" endWordPosition="2641">notes the gap type of a tree on the stack. There are three possible values, either “none” (tree is fully continuous), “pass” (there is a gap at the root, i.e., this gap must be filled later further up in the tree), or “gap” (the root of this tree fills a gap, i.e., its children have gaps, but the root does not). Finally, y is the sum of all gap lengths. 3 Experiments 3.1 Data We use the TiGer treebank release 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER, we use the first half of the last 10,000 sentences for development and the second half for testing.3 We also recreate the split of Hall and Nivre (2008) (TIGERHN), for which we split TiGer in 10 parts, assigning sentence i to part imod10. The first of those parts is used for testing, the concatenation of the rest for training. 2See Maier and Lichte (2011) for a formal account on gaps in treebanks. 3This split, which corresponds to the split used in the SPMRL 2013 shared task (Seddah et al., 2013), was proposed in Farkas and Schmid (2012). We exclude sentences 46,234 and 50,224, because of annotation errors. Both contain nodes with more than one parent node. 1205 F1 on dev. set From NeGra, we exclude all sentences longer than 30 words (in orde</context>
<context position="33336" citStr="Hall and Nivre (2008)" startWordPosition="5557" endWordPosition="5560">ered reliably. This hypothesis is confirmed by the fact that the our parser performs much worse on discontinuous constituents with a very low frequency (such as CS, making up only 0.62% of all discontinuous constituents) than it performs on those with a high frequency (such as VP, making up 60.65% of all discontinuous constituents), while rparse performs well on the low frequency constituents. EaFi and Dependency Parsers We run an experiment with 40 iterations on TIGERHN, using DISCO, EXTENDED and IMPORTANCE. Tab. 6 lists the results, together with the corresponding results of Versley (2014), Hall and Nivre (2008) (H&amp;N) and Fern´andez-Gonz´alez and Martins (2015) (F&amp;M). Our results exceed those of EaFi6 and the exact match score of H&amp;N. We are outperformed by the F&amp;M parser. Note, that particularly the comparison to EaFi must be handled with care, since Versley (2014) uses additional preprocessing: PPinternal NPs are annotated explicitly, and the parenthetical sentences are changed to be embedded by their enclosing sentence (instead of vice versa). We postpone a thorough comparison with both EaFi and the dependency parsers to future work. 6Note that Versley (2014) reports a parsing speed of 40- 55 sent</context>
</contexts>
<marker>Hall, Nivre, 2008</marker>
<rawString>Johan Hall and Joakim Nivre. 2008. Parsing discontinuous phrase structure with grammatical functions. In Bengt Nordstr¨om and Aarne Ranta, editors, Advances in Natural Language Processing, volume 5221 of Lecture Notes in Computer Science, pages 169–180. Springer, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
</authors>
<title>Finding non-local dependencies: Beyond pattern matching.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>37--43</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1830" citStr="Jijkoun, 2003" startWordPosition="280" endWordPosition="281">discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of </context>
</contexts>
<marker>Jijkoun, 2003</marker>
<rawString>Valentin Jijkoun. 2003. Finding non-local dependencies: Beyond pattern matching. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 37–43, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>136--143</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1791" citStr="Johnson, 2002" startWordPosition="274" endWordPosition="275">tituent. Another method is to annotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of</context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Mark Johnson. 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 136–143, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Wolfgang Maier</author>
</authors>
<title>Datadriven parsing using probabilistic linear contextfree rewriting systems.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="18574" citStr="Kallmeyer and Maier, 2013" startWordPosition="3120" endWordPosition="3123">(Sampson and Babarczy, 2003), for which we consider all paths from leaves to the root. • Tree edit distance (Emms, 2008), which consists of the minimum edit distance between gold tree and parser output. Aside from a full evaluation, we also evaluate only the constituents that are discontinuous. 4http://github.com/andreasvc/discodop 2 4 6 8 10 12 14 16 18 20 Iteration Figure 5: NEGRA dev results (Fi) for different beam sizes We perform 20 training iterations unless indicated otherwise. When training stops, we average the model (as in Daum´e III (2006)). We run further experiments with rparse5 (Kallmeyer and Maier, 2013) to facilitate a comparison with a grammar-based parser. 3.3 Results We start with discontinuous parsing experiments on NEGRA and TIGER, followed by continuous parsing experiments, and a comparison to grammar-based parsing. 3.3.1 Discontinuous Parsing NeGra The first goal is to determine the effect of different beam sizes with BASELINE features and the COMPOUNDSWAPZ operation. We run experiments with beam sizes 1, 2, 4 and 8; Fig. 5 shows the results obtained on the dev set after each iteration. Fig. 6 shows the average decoding speed during each iteration for each beam size (both smoothed). T</context>
<context position="31232" citStr="Kallmeyer and Maier (2013)" startWordPosition="5207" endWordPosition="5210">ison with other Parsers rparse In order to compare our parser with a grammar-based approach, we now parse NEGRA with rparse, with the same training and test sets as before (i.e., we do not use the development set). We employ markovization with v = 1, h = 2 and head driven binarization with binary top and bottom productions. The first thing to notice is that rparse is much slower than our parser. The average parsing speed is about 0.3 sent./sec.; very long sentences require over a minute to be parsed. The parsing results are shown in Tab. 5. They are about 5 points worse than those reported by Kallmeyer and Maier (2013). This is due to the fact that they train on the first 90% of the treebank, and not on the first 80% as we do, which leads to an increased number of unparsed sentences. In comparison to the baseline setting of the shift-reduce parser with beam size 8, the results are around 10 points worse. However, rparse reaches an F1 of 26.61 on discontinuous constituents, which is 5.9 points more than we achieved with the best setting with our parser. In order to investigate why the grammar-based approach outperforms our parser on discontinuous constituents, we count the frequency of LCFRS productions of a</context>
</contexts>
<marker>Kallmeyer, Maier, 2013</marker>
<rawString>Laura Kallmeyer and Wolfgang Maier. 2013. Datadriven parsing using probabilistic linear contextfree rewriting systems. Computational Linguistics, 39(1):87–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>327--334</pages>
<location>Barcelona,</location>
<contexts>
<context position="1854" citStr="Levy and Manning, 2004" startWordPosition="282" endWordPosition="285"> directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituen</context>
</contexts>
<marker>Levy, Manning, 2004</marker>
<rawString>Roger Levy and Christopher Manning. 2004. Deep dependencies from context-free statistical parsers: Correcting the surface dependency approximation. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 327–334, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Probabilistic Models of Word Order and Syntactic Discontinuity.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="16774" citStr="Levy (2005)" startWordPosition="2824" endWordPosition="2825">n errors. Both contain nodes with more than one parent node. 1205 F1 on dev. set From NeGra, we exclude all sentences longer than 30 words (in order to make a comparison with rparse possible, see below), and split off the last 10% of the treebank for testing, as well as the previous 10% for development. As a preprocessing step, in both treebanks we remove spurious discontinuities that are caused by material which is attached to the virtual root node (mainly punctuation). All such elements are attached to the least common ancestor node of their left and right terminal neighbors (as proposed by Levy (2005), p. 163). We furthermore create a continuous variant NEGRACF of NEGRA with the method usually used for PCFG parsing: For all maximal continuous parts of a discontinuous constituent, a separate node is introduced (Boyd, 2007). Subsequently, all nodes that do not cover the head child of the discontinuous constituent are removed. No further preprocessing or cleanup is applied. 3.2 Experimental Setup Our parser is implemented in Java. We run all our experiments with Java 8 on an Intel Core i5, allocating 15 GB per experiment. All experiments are carried out with gold POS tags, as in previous work</context>
</contexts>
<marker>Levy, 2005</marker>
<rawString>Roger Levy. 2005. Probabilistic Models of Word Order and Syntactic Discontinuity. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Timm Lichte</author>
</authors>
<title>Characterizing discontinuity in constituent treebanks.</title>
<date>2011</date>
<booktitle>In Formal Grammar. 14th International Conference, FG 2009.</booktitle>
<volume>5591</volume>
<pages>167--182</pages>
<publisher>Springer-Verlag.</publisher>
<location>Bordeaux, France,</location>
<contexts>
<context position="15915" citStr="Maier and Lichte (2011)" startWordPosition="2674" endWordPosition="2677"> the tree), or “gap” (the root of this tree fills a gap, i.e., its children have gaps, but the root does not). Finally, y is the sum of all gap lengths. 3 Experiments 3.1 Data We use the TiGer treebank release 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER, we use the first half of the last 10,000 sentences for development and the second half for testing.3 We also recreate the split of Hall and Nivre (2008) (TIGERHN), for which we split TiGer in 10 parts, assigning sentence i to part imod10. The first of those parts is used for testing, the concatenation of the rest for training. 2See Maier and Lichte (2011) for a formal account on gaps in treebanks. 3This split, which corresponds to the split used in the SPMRL 2013 shared task (Seddah et al., 2013), was proposed in Farkas and Schmid (2012). We exclude sentences 46,234 and 50,224, because of annotation errors. Both contain nodes with more than one parent node. 1205 F1 on dev. set From NeGra, we exclude all sentences longer than 30 words (in order to make a comparison with rparse possible, see below), and split off the last 10% of the treebank for testing, as well as the previous 10% for development. As a preprocessing step, in both treebanks we r</context>
</contexts>
<marker>Maier, Lichte, 2011</marker>
<rawString>Wolfgang Maier and Timm Lichte. 2011. Characterizing discontinuity in constituent treebanks. In Formal Grammar. 14th International Conference, FG 2009. Bordeaux, France, July 25-26, 2009. Revised Selected Papers, volume 5591 of LNCS/LNAI, pages 167–182. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Anders Søgaard</author>
</authors>
<title>Treebanks and mild context-sensitivity.</title>
<date>2008</date>
<booktitle>Proceedings of the 13th Conference on Formal Grammar (FG-2008),</booktitle>
<pages>61--76</pages>
<editor>In Philippe de Groote, editor,</editor>
<publisher>CSLI Publications.</publisher>
<location>Hamburg, Germany.</location>
<contexts>
<context position="2903" citStr="Maier and Søgaard, 2008" startWordPosition="446" endWordPosition="449">, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done with Linear Context-Free Rewriting System (LCFRS), an extension of CFG which allows its non-terminals to cover more than one continuous block (Vijay-Shanker et al., 1987). LCFRS parsing is expensive: CYK chart parsing with a binarized grammar can be done in O(n3k) where k is the block degree, the maximal number of continuous blocks a non-terminal can cover (Seki et al., 1991). For a typical treebank LCFRS (Maier and Søgaard, 2008), k ≈ 3, instead of k = 1 for PCFG. In order to improve on otherwise impractical parsing times, LCFRS chart parsers employ different strategies to speed up search: Kallmeyer Das PDS wollen VMFIN wir PPER umkehren VVINF VP S wollen wir umkehren want we reverse 1202 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenbur</context>
</contexts>
<marker>Maier, Søgaard, 2008</marker>
<rawString>Wolfgang Maier and Anders Søgaard. 2008. Treebanks and mild context-sensitivity. In Philippe de Groote, editor, Proceedings of the 13th Conference on Formal Grammar (FG-2008), pages 61–76, Hamburg, Germany. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Miriam Kaeshammer</author>
<author>Laura Kallmeyer</author>
</authors>
<title>Data-driven PLCFRS parsing revisited: Restricting the fan-out to two.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eleventh International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+11),</booktitle>
<pages>126--134</pages>
<location>Paris, France.</location>
<contexts>
<context position="3691" citStr="Maier et al. (2012)" startWordPosition="570" endWordPosition="573">eyer Das PDS wollen VMFIN wir PPER umkehren VVINF VP S wollen wir umkehren want we reverse 1202 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can th</context>
</contexts>
<marker>Maier, Kaeshammer, Kallmeyer, 2012</marker>
<rawString>Wolfgang Maier, Miriam Kaeshammer, and Laura Kallmeyer. 2012. Data-driven PLCFRS parsing revisited: Restricting the fan-out to two. In Proceedings of the Eleventh International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 126–134, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
</authors>
<title>Parsing Discontinuous Structures. Dissertation,</title>
<date>2013</date>
<institution>University of T¨ubingen.</institution>
<contexts>
<context position="3447" citStr="Maier (2013)" startWordPosition="533" endWordPosition="534">t al., 1991). For a typical treebank LCFRS (Maier and Søgaard, 2008), k ≈ 3, instead of k = 1 for PCFG. In order to improve on otherwise impractical parsing times, LCFRS chart parsers employ different strategies to speed up search: Kallmeyer Das PDS wollen VMFIN wir PPER umkehren VVINF VP S wollen wir umkehren want we reverse 1202 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known fro</context>
<context position="18574" citStr="Maier, 2013" startWordPosition="3122" endWordPosition="3123">abarczy, 2003), for which we consider all paths from leaves to the root. • Tree edit distance (Emms, 2008), which consists of the minimum edit distance between gold tree and parser output. Aside from a full evaluation, we also evaluate only the constituents that are discontinuous. 4http://github.com/andreasvc/discodop 2 4 6 8 10 12 14 16 18 20 Iteration Figure 5: NEGRA dev results (Fi) for different beam sizes We perform 20 training iterations unless indicated otherwise. When training stops, we average the model (as in Daum´e III (2006)). We run further experiments with rparse5 (Kallmeyer and Maier, 2013) to facilitate a comparison with a grammar-based parser. 3.3 Results We start with discontinuous parsing experiments on NEGRA and TIGER, followed by continuous parsing experiments, and a comparison to grammar-based parsing. 3.3.1 Discontinuous Parsing NeGra The first goal is to determine the effect of different beam sizes with BASELINE features and the COMPOUNDSWAPZ operation. We run experiments with beam sizes 1, 2, 4 and 8; Fig. 5 shows the results obtained on the dev set after each iteration. Fig. 6 shows the average decoding speed during each iteration for each beam size (both smoothed). T</context>
<context position="31232" citStr="Maier (2013)" startWordPosition="5209" endWordPosition="5210">r Parsers rparse In order to compare our parser with a grammar-based approach, we now parse NEGRA with rparse, with the same training and test sets as before (i.e., we do not use the development set). We employ markovization with v = 1, h = 2 and head driven binarization with binary top and bottom productions. The first thing to notice is that rparse is much slower than our parser. The average parsing speed is about 0.3 sent./sec.; very long sentences require over a minute to be parsed. The parsing results are shown in Tab. 5. They are about 5 points worse than those reported by Kallmeyer and Maier (2013). This is due to the fact that they train on the first 90% of the treebank, and not on the first 80% as we do, which leads to an increased number of unparsed sentences. In comparison to the baseline setting of the shift-reduce parser with beam size 8, the results are around 10 points worse. However, rparse reaches an F1 of 26.61 on discontinuous constituents, which is 5.9 points more than we achieved with the best setting with our parser. In order to investigate why the grammar-based approach outperforms our parser on discontinuous constituents, we count the frequency of LCFRS productions of a</context>
<context position="32477" citStr="Maier (2013" startWordPosition="5424" endWordPosition="5425">ed grammar used in the rparse experiment. The 1209 LF1 E Versley (2014) 74.23 37.32 this work 79.52 44.32 H&amp;N (2008) 79.93 37.78 F&amp;M (2015) 85.53 51.21 Table 6: Results TIGERHN, sentence length ≤ 40 average occurrence count of rules with gap degree 0 is 12.18. Discontinuous rules have a much lower frequency, the average count of productions with one, two and three gaps being 3.09, 2.09, and 1.06, respectively. In PCFG parsing, excluding low frequency productions does not have a large effect (Charniak, 1996); however, this does not hold for LCFRS parsing, where they have a major influence (cf. Maier (2013, p. 205)): This means that removing low frequency productions has a negative impact on the parser performance particularly concerning discontinuous structures; however, it also means that low frequency discontinuous productions get triggered reliably. This hypothesis is confirmed by the fact that the our parser performs much worse on discontinuous constituents with a very low frequency (such as CS, making up only 0.62% of all discontinuous constituents) than it performs on those with a high frequency (such as VP, making up 60.65% of all discontinuous constituents), while rparse performs well </context>
</contexts>
<marker>Maier, 2013</marker>
<rawString>Wolfgang Maier. 2013. Parsing Discontinuous Structures. Dissertation, University of T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Marco Kuhlmann</author>
<author>Johan Hall</author>
</authors>
<title>An improved oracle for dependency parsing with online reordering.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>73--76</pages>
<location>Paris, France.</location>
<contexts>
<context position="4116" citStr="Nivre et al., 2009" startWordPosition="638" endWordPosition="641">nenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done online during parsing with a “swap” operation that allows to process input words out of order. This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gradient method to train</context>
<context position="11457" citStr="Nivre et al. (2009)" startWordPosition="1851" endWordPosition="1854">tem, w stands for the head word, t for the head tag and c for the constituent label (w, t and c are identical on POS-level). l and r (ll and rr) represent the left and right children (grand-children) of the element on the stack; u handles the unary case. Concerning the separator features, p is a unique separator punctuation between the head words of so and sl, q is the count of any separator punctuation between so and sl. 2.3 Handling Discontinuities In order to handle discontinuities, we use two variants of a swap transition which are similar to swap-eager and swap-lazy from Nivre (2009) and Nivre et al. (2009). The first variant, SINGLESWAP, swaps the second item of the stack back on the queue. The second variant COMPOUNDSWAPi bundles a maximal number of adjacent swaps. It swaps i items starting from the second item on the stack, with 1 ≤ i &lt; |s|. Both swap operations can only be applied if 1. the item has not yet been FINISHed and the last transition has not been a transition with the root category, 2. the queue is not empty, 3. all elements to be swapped are pre-terminals, and Das PDS wollen VMFIN wir PPER umkehren VVINF @S VP S 1204 4. if the first item of the stack has a lower index than the se</context>
</contexts>
<marker>Nivre, Kuhlmann, Hall, 2009</marker>
<rawString>Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009. An improved oracle for dependency parsing with online reordering. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 73–76, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>351--359</pages>
<contexts>
<context position="4095" citStr="Nivre, 2009" startWordPosition="636" endWordPosition="637">) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done online during parsing with a “swap” operation that allows to process input words out of order. This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gra</context>
<context position="11433" citStr="Nivre (2009)" startWordPosition="1848" endWordPosition="1849">stack and queue item, w stands for the head word, t for the head tag and c for the constituent label (w, t and c are identical on POS-level). l and r (ll and rr) represent the left and right children (grand-children) of the element on the stack; u handles the unary case. Concerning the separator features, p is a unique separator punctuation between the head words of so and sl, q is the count of any separator punctuation between so and sl. 2.3 Handling Discontinuities In order to handle discontinuities, we use two variants of a swap transition which are similar to swap-eager and swap-lazy from Nivre (2009) and Nivre et al. (2009). The first variant, SINGLESWAP, swaps the second item of the stack back on the queue. The second variant COMPOUNDSWAPi bundles a maximal number of adjacent swaps. It swaps i items starting from the second item on the stack, with 1 ≤ i &lt; |s|. Both swap operations can only be applied if 1. the item has not yet been FINISHed and the last transition has not been a transition with the root category, 2. the queue is not empty, 3. all elements to be swapped are pre-terminals, and Das PDS wollen VMFIN wir PPER umkehren VVINF @S VP S 1204 4. if the first item of the stack has a</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351–359, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Evaluating evaluation measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference of Computational Linguistics NODALIDA-2007,</booktitle>
<pages>372--379</pages>
<location>Tartu, Estonia.</location>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith. 2007. Evaluating evaluation measures. In Proceedings of the 16th Nordic Conference of Computational Linguistics NODALIDA-2007, pages 372–379, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>125--132</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="5173" citStr="Sagae and Lavie (2005)" startWordPosition="802" endWordPosition="805"> also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gradient method to train his parser. He reports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present str</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifier-based parser with linear run-time complexity. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 125–132, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
<author>Anna Babarczy</author>
</authors>
<title>A test of the leaf-ancestor metric for parse accuracy.</title>
<date>2003</date>
<journal>Journal of Natural Language Engineering,</journal>
<pages>9--365</pages>
<contexts>
<context position="17976" citStr="Sampson and Babarczy, 2003" startWordPosition="3022" endWordPosition="3025">tags, as in previous work on shift-reduce constituency parsing (Zhang and Clark, 2009). Grammatical function labels are discarded. For the evaluation, we use the corresponding module of discodop.4 We report several metrics (as implemented in discodop): • Extended labeled bracketing, in which a bracket for a single node consists of its label and a set of pairs of indices, delimiting the continuous blocks it covers. We do not include the root node in the evaluation and ignore punctuation. We report labeled precision, recall and Fl, as well as exact match (all brackets correct). • Leaf-ancestor (Sampson and Babarczy, 2003), for which we consider all paths from leaves to the root. • Tree edit distance (Emms, 2008), which consists of the minimum edit distance between gold tree and parser output. Aside from a full evaluation, we also evaluate only the constituents that are discontinuous. 4http://github.com/andreasvc/discodop 2 4 6 8 10 12 14 16 18 20 Iteration Figure 5: NEGRA dev results (Fi) for different beam sizes We perform 20 training iterations unless indicated otherwise. When training stops, we average the model (as in Daum´e III (2006)). We run further experiments with rparse5 (Kallmeyer and Maier, 2013) t</context>
</contexts>
<marker>Sampson, Babarczy, 2003</marker>
<rawString>Geoffrey Sampson and Anna Babarczy. 2003. A test of the leaf-ancestor metric for parse accuracy. Journal of Natural Language Engineering, 9:365–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Trace prediction and recovery with unlexicalized PCFGs and slash features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--184</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1868" citStr="Schmid, 2006" startWordPosition="286" endWordPosition="287">r crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) Das That “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been Figure 1: Example annotation with discontinuous constituents from TiGer framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done</context>
</contexts>
<marker>Schmid, 2006</marker>
<rawString>Helmut Schmid. 2006. Trace prediction and recovery with unlexicalized PCFGs and slash features. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 177–184, Sydney, Australia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jinho D Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash,</title>
<date>2013</date>
<journal>Overview of the SPMRL</journal>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>146--182</pages>
<institution>Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, and Alina Wr´oblewska.</institution>
<location>Marco Kuhlmann, Wolfgang Maier, Yuval Marton, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang</location>
<marker>Seddah, Tsarfaty, K¨ubler, Candito, Choi, Farkas, Foster, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Yuval Marton, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, and Alina Wr´oblewska. 2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146–182, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On Multiple ContextFree Grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="2847" citStr="Seki et al., 1991" startWordPosition="437" endWordPosition="440">dge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done with Linear Context-Free Rewriting System (LCFRS), an extension of CFG which allows its non-terminals to cover more than one continuous block (Vijay-Shanker et al., 1987). LCFRS parsing is expensive: CYK chart parsing with a binarized grammar can be done in O(n3k) where k is the block degree, the maximal number of continuous blocks a non-terminal can cover (Seki et al., 1991). For a typical treebank LCFRS (Maier and Søgaard, 2008), k ≈ 3, instead of k = 1 for PCFG. In order to improve on otherwise impractical parsing times, LCFRS chart parsers employ different strategies to speed up search: Kallmeyer Das PDS wollen VMFIN wir PPER umkehren VVINF VP S wollen wir umkehren want we reverse 1202 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and Maier (2013)</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On Multiple ContextFree Grammars. Theoretical Computer Science, 88(2):191–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas van Cranenburgh</author>
<author>Rens Bod</author>
</authors>
<title>Discontinuous parsing with an efficient and accurate DOP model.</title>
<date>2013</date>
<booktitle>In Proceedings of The 13th International Conference on Parsing Technologies,</booktitle>
<location>Nara, Japan.</location>
<marker>van Cranenburgh, Bod, 2013</marker>
<rawString>Andreas van Cranenburgh and Rens Bod. 2013. Discontinuous parsing with an efficient and accurate DOP model. In Proceedings of The 13th International Conference on Parsing Technologies, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas van Cranenburgh</author>
</authors>
<title>Efficient parsing with linear context-free rewriting systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>460--470</pages>
<location>Avignon, France.</location>
<marker>van Cranenburgh, 2012</marker>
<rawString>Andreas van Cranenburgh. 2012. Efficient parsing with linear context-free rewriting systems. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 460–470, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
</authors>
<title>Experiments with easy-first nonprojective constituent parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,</booktitle>
<pages>39--53</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="3820" citStr="Versley (2014)" startWordPosition="591" endWordPosition="592"> of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done o</context>
<context position="5756" citStr="Versley (2014)" startWordPosition="896" endWordPosition="897">en up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluation, where we pay particular attention to the parser performance on discontinuous structures, we show among other things that surprisingly, a grammarbased parser has an edge over a shift-reduce approach concerning the reconstruction of discontinuous constituents. The remainder of the paper is structured as follows. In subsection 2.1, we introduce the general parser architecture; the subsections 2.2 a</context>
<context position="26052" citStr="Versley (2014)" startWordPosition="4340" endWordPosition="4341">ependently and when used together. DISCO was particularly useful for discontinuous constituents. SEPARATOR yielded no usable improvements. IMPORTANCE has also proven to be effective, yielding the best results on discontinuous constituents (in combination with DISCO). Over almost all experiments, a common error is that on root level, CS and S get confused, indicating that the present features do not provide sufficient information for disambiguation of those categories. We can also confirm the tendency that discontinuous VPs in relatively short sentences are recognized correctly, as reported by Versley (2014). TiGer We now repeat the most successful experiments on TIGER. Tab. 3 shows the parsing results for the test set. Some of the trends seen on the experiments with NEGRA are repeated. EXTENDED and DISCO yields an improvement on all constituents. However, now not only DISCO, but also EXTENDED lead to improved scores on discontinuous constituents. As mentioned above, this can be explained with the fact that for the EXTENDED features to be effective, the amount of training data available in NEGRA was not enough. Other than in NEGRA, the DISCO features are now more effective when used alone, leadin</context>
<context position="31937" citStr="Versley (2014)" startWordPosition="5333" endWordPosition="5334">first 80% as we do, which leads to an increased number of unparsed sentences. In comparison to the baseline setting of the shift-reduce parser with beam size 8, the results are around 10 points worse. However, rparse reaches an F1 of 26.61 on discontinuous constituents, which is 5.9 points more than we achieved with the best setting with our parser. In order to investigate why the grammar-based approach outperforms our parser on discontinuous constituents, we count the frequency of LCFRS productions of a certain gap degree in the binarized grammar used in the rparse experiment. The 1209 LF1 E Versley (2014) 74.23 37.32 this work 79.52 44.32 H&amp;N (2008) 79.93 37.78 F&amp;M (2015) 85.53 51.21 Table 6: Results TIGERHN, sentence length ≤ 40 average occurrence count of rules with gap degree 0 is 12.18. Discontinuous rules have a much lower frequency, the average count of productions with one, two and three gaps being 3.09, 2.09, and 1.06, respectively. In PCFG parsing, excluding low frequency productions does not have a large effect (Charniak, 1996); however, this does not hold for LCFRS parsing, where they have a major influence (cf. Maier (2013, p. 205)): This means that removing low frequency productio</context>
<context position="33313" citStr="Versley (2014)" startWordPosition="5555" endWordPosition="5556">ctions get triggered reliably. This hypothesis is confirmed by the fact that the our parser performs much worse on discontinuous constituents with a very low frequency (such as CS, making up only 0.62% of all discontinuous constituents) than it performs on those with a high frequency (such as VP, making up 60.65% of all discontinuous constituents), while rparse performs well on the low frequency constituents. EaFi and Dependency Parsers We run an experiment with 40 iterations on TIGERHN, using DISCO, EXTENDED and IMPORTANCE. Tab. 6 lists the results, together with the corresponding results of Versley (2014), Hall and Nivre (2008) (H&amp;N) and Fern´andez-Gonz´alez and Martins (2015) (F&amp;M). Our results exceed those of EaFi6 and the exact match score of H&amp;N. We are outperformed by the F&amp;M parser. Note, that particularly the comparison to EaFi must be handled with care, since Versley (2014) uses additional preprocessing: PPinternal NPs are annotated explicitly, and the parenthetical sentences are changed to be embedded by their enclosing sentence (instead of vice versa). We postpone a thorough comparison with both EaFi and the dependency parsers to future work. 6Note that Versley (2014) reports a parsi</context>
</contexts>
<marker>Versley, 2014</marker>
<rawString>Yannick Versley. 2014. Experiments with easy-first nonprojective constituent parsing. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 39– 53, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterising structural descriptions used by various formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="2639" citStr="Vijay-Shanker et al., 1987" startWordPosition="400" endWordPosition="403">with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done with Linear Context-Free Rewriting System (LCFRS), an extension of CFG which allows its non-terminals to cover more than one continuous block (Vijay-Shanker et al., 1987). LCFRS parsing is expensive: CYK chart parsing with a binarized grammar can be done in O(n3k) where k is the block degree, the maximal number of continuous blocks a non-terminal can cover (Seki et al., 1991). For a typical treebank LCFRS (Maier and Søgaard, 2008), k ≈ 3, instead of k = 1 for PCFG. In order to improve on otherwise impractical parsing times, LCFRS chart parsers employ different strategies to speed up search: Kallmeyer Das PDS wollen VMFIN wir PPER umkehren VVINF VP S wollen wir umkehren want we reverse 1202 Proceedings of the 53rd Annual Meeting of the Association for Computati</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David Weir, and Aravind K. Joshi. 1987. Characterising structural descriptions used by various formalisms. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 104–111, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transitionbased parsing of the Chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>162--171</pages>
<location>Paris, France.</location>
<contexts>
<context position="7648" citStr="Zhang and Clark (2009)" startWordPosition="1202" endWordPosition="1205"> X. BINARY-X-L and BINARYX-R reduce the first two elements on the stack to a new X constituent, with the lexical head coming from the left or the right child, respectively. FINISH removes the last element from the stack. We additionally use an IDLE transition, which can be applied any number of times after FINISH, to improve the comparability of analyses of different lengths (Zhu et al., 2013). The application of a transition is subject to restrictions. UNARY-X, e.g., can only be applied when there is at least a single item on the stack. We implement all restrictions listed in the appendix of Zhang and Clark (2009), and add additional restrictions that block transitions involving the root label when not having arrived at the end of a derivation. We do not use an underlying grammar to filter out transitions which have not been seen during training. For decoding, we use beam search (Zhang and Clark, 2011b). Decoding is started by putting the 1As in other shift-reduce approaches, we assume that POS tagging is done outside of the parser. 1203 Figure 2: Binarization example start item (empty stack, full queue) on the beam. Then, repeatedly, a candidate list is filled with all items that result from applying </context>
<context position="9787" citStr="Zhang and Clark (2009)" startWordPosition="1581" endWordPosition="1584">inarized, all intermediate nodes which are introduced will be labeled @X. Lexical heads are marked with Collins-style head rules. As an example, Fig. 2 shows the binarized version of the tree of Fig. 1. Finally, since we are learning a sparse model, we also exploit the work of Goldberg and Elhadad (2011) who propose to include a feature in the calculation of a score only if it has been observed ≥ MINUPDATE times. 2.2 Features Features are generated by applying templates to parser items. They reflect different configurations of stack and queue. As BASELINE features, we use the feature set from Zhang and Clark (2009) without the bracketing features (as used in Zhu et al. (2013)). We furthermore experiment with features that reflect the presence of separating punctuation “,”, “:”, “;” (SEPARATOR) (Zhang and Clark, 2009), and with the EXTENDED features of Zhu et unigrams s0tc, s0wc, s1tc, s1wc, s2tc, s2wc, s3tc, s3wc, q0wt, q1wt, q2wt, q3wt, s0lwc, s0rwc, s0uwc, s1lwc, s1rwc, s1uwc bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c, s0wq0w, s0wq0t, s0cq0w, s0cq0t, s1wq0w, s1wq0t, s1cq0w, s1cq0t, q0wq1w, q0wq1t, q0tq1w, q0tq1t trigrams s0cs1cs2w, s0cs1cs2c, s0cs1cq0w, s0cs1cq0t, s0cs1wq0w, s0cs1wq0t, s0ws1cs2c, s0ws1cq0</context>
<context position="17435" citStr="Zhang and Clark, 2009" startWordPosition="2934" endWordPosition="2937">inuous variant NEGRACF of NEGRA with the method usually used for PCFG parsing: For all maximal continuous parts of a discontinuous constituent, a separate node is introduced (Boyd, 2007). Subsequently, all nodes that do not cover the head child of the discontinuous constituent are removed. No further preprocessing or cleanup is applied. 3.2 Experimental Setup Our parser is implemented in Java. We run all our experiments with Java 8 on an Intel Core i5, allocating 15 GB per experiment. All experiments are carried out with gold POS tags, as in previous work on shift-reduce constituency parsing (Zhang and Clark, 2009). Grammatical function labels are discarded. For the evaluation, we use the corresponding module of discodop.4 We report several metrics (as implemented in discodop): • Extended labeled bracketing, in which a bracket for a single node consists of its label and a set of pairs of indices, delimiting the continuous blocks it covers. We do not include the root node in the evaluation and ignore punctuation. We report labeled precision, recall and Fl, as well as exact match (all brackets correct). • Leaf-ancestor (Sampson and Babarczy, 2003), for which we consider all paths from leaves to the root. </context>
<context position="21990" citStr="Zhang and Clark (2009)" startWordPosition="3683" endWordPosition="3686">.86 EXTENDED + IMPORTANCE 89.80 91.98 Table 2: Results NEGRA TED and Leaf-Ancestor constituents, the latter beats the former by 0.8 (F1). On discontinuous constituents, using COMPOUNDSWAPi gives an improvement of more than four points in precision and of about 0.8 points in recall. A manual analysis confirms that as expected, particularly discontinuous constituents with large gaps profit from bundling swap transitions. In the second block, we run the BASELINE features with COMPOUNDSWAPi combined with SEPARATOR, EXTENDED and DISCO. The SEPARATOR features were not as successful as they were for Zhang and Clark (2009). All scores for discontinuous constituents drop (compared to the baseline). The EXTENDED features are more effective and give an improvement of about half a point F1 on all constituents, as well as the highest exact match among all experiments. On discontinuous constituents, precision raises slightly but we loose about 1.4% in recall (compared to the baseline). The latter seems to be due to the fact that in comparison to the baseline, with EXTENDED, more sentences get erroneously analyzed as not containing any crossing branches. This effect can be explained with data sparseness and is less pr</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transitionbased parsing of the Chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 162–171, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-reduce CCG parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>683--692</pages>
<location>Portland, OR.</location>
<contexts>
<context position="5355" citStr="Zhang and Clark (2011" startWordPosition="829" endWordPosition="832">ports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluatio</context>
<context position="7941" citStr="Zhang and Clark, 2011" startWordPosition="1253" endWordPosition="1256">f times after FINISH, to improve the comparability of analyses of different lengths (Zhu et al., 2013). The application of a transition is subject to restrictions. UNARY-X, e.g., can only be applied when there is at least a single item on the stack. We implement all restrictions listed in the appendix of Zhang and Clark (2009), and add additional restrictions that block transitions involving the root label when not having arrived at the end of a derivation. We do not use an underlying grammar to filter out transitions which have not been seen during training. For decoding, we use beam search (Zhang and Clark, 2011b). Decoding is started by putting the 1As in other shift-reduce approaches, we assume that POS tagging is done outside of the parser. 1203 Figure 2: Binarization example start item (empty stack, full queue) on the beam. Then, repeatedly, a candidate list is filled with all items that result from applying legal transitions to the items on the beam, followed by putting the highest scoring n of them back on the beam (given a beam size of n). Parsing is finished if the highest scoring item on the beam is a final item (stack holds one item labeled with the root label, queue is empty), which can be</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011a. Shift-reduce CCG parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 683–692, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="5355" citStr="Zhang and Clark (2011" startWordPosition="829" endWordPosition="832">ports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluatio</context>
<context position="7941" citStr="Zhang and Clark, 2011" startWordPosition="1253" endWordPosition="1256">f times after FINISH, to improve the comparability of analyses of different lengths (Zhu et al., 2013). The application of a transition is subject to restrictions. UNARY-X, e.g., can only be applied when there is at least a single item on the stack. We implement all restrictions listed in the appendix of Zhang and Clark (2009), and add additional restrictions that block transitions involving the root label when not having arrived at the end of a derivation. We do not use an underlying grammar to filter out transitions which have not been seen during training. For decoding, we use beam search (Zhang and Clark, 2011b). Decoding is started by putting the 1As in other shift-reduce approaches, we assume that POS tagging is done outside of the parser. 1203 Figure 2: Binarization example start item (empty stack, full queue) on the beam. Then, repeatedly, a candidate list is filled with all items that result from applying legal transitions to the items on the beam, followed by putting the highest scoring n of them back on the beam (given a beam size of n). Parsing is finished if the highest scoring item on the beam is a final item (stack holds one item labeled with the root label, queue is empty), which can be</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011b. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Jingbo Zhu</author>
<author>Huizhen Wang</author>
</authors>
<title>Exploiting lexical dependencies from large-scale data for better shift-reduce constituency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>3171--3186</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="5403" citStr="Zhu et al., 2012" startWordPosition="836" endWordPosition="839"> that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluation, where we pay particular attention to the pars</context>
</contexts>
<marker>Zhu, Zhu, Wang, 2012</marker>
<rawString>Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012. Exploiting lexical dependencies from large-scale data for better shift-reduce constituency parsing. In Proceedings of COLING 2012, pages 3171–3186, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>434--443</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5422" citStr="Zhu et al., 2013" startWordPosition="840" endWordPosition="843">e reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluation, where we pay particular attention to the parser performance on d</context>
<context position="6655" citStr="Zhu et al. (2013)" startWordPosition="1030" endWordPosition="1033">uctures, we show among other things that surprisingly, a grammarbased parser has an edge over a shift-reduce approach concerning the reconstruction of discontinuous constituents. The remainder of the paper is structured as follows. In subsection 2.1, we introduce the general parser architecture; the subsections 2.2 and 2.3 introduce the features we use and our strategy for handling discontinuous structures. Section 3 presents and discusses the experimental results, section 4 concludes the article. 2 Discontinuous Shift-Reduce Parsing Our parser architecture follows previous work, particularly Zhu et al. (2013) and Bauer (2014). 2.1 Shift-reduce parsing with perceptron training An item in our parser consists of a queue q of token/POS-pairs to be processed, and a stack s, which holds completed constituents.1 The parser uses different transitions: SHIFT shifts a terminal from the queue on to the stack. UNARY-X reduces the first element on the stack to a new constituent labeled X. BINARY-X-L and BINARYX-R reduce the first two elements on the stack to a new X constituent, with the lexical head coming from the left or the right child, respectively. FINISH removes the last element from the stack. We addit</context>
<context position="9849" citStr="Zhu et al. (2013)" startWordPosition="1592" endWordPosition="1595">ed @X. Lexical heads are marked with Collins-style head rules. As an example, Fig. 2 shows the binarized version of the tree of Fig. 1. Finally, since we are learning a sparse model, we also exploit the work of Goldberg and Elhadad (2011) who propose to include a feature in the calculation of a score only if it has been observed ≥ MINUPDATE times. 2.2 Features Features are generated by applying templates to parser items. They reflect different configurations of stack and queue. As BASELINE features, we use the feature set from Zhang and Clark (2009) without the bracketing features (as used in Zhu et al. (2013)). We furthermore experiment with features that reflect the presence of separating punctuation “,”, “:”, “;” (SEPARATOR) (Zhang and Clark, 2009), and with the EXTENDED features of Zhu et unigrams s0tc, s0wc, s1tc, s1wc, s2tc, s2wc, s3tc, s3wc, q0wt, q1wt, q2wt, q3wt, s0lwc, s0rwc, s0uwc, s1lwc, s1rwc, s1uwc bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c, s0wq0w, s0wq0t, s0cq0w, s0cq0t, s1wq0w, s1wq0t, s1cq0w, s1cq0t, q0wq1w, q0wq1t, q0tq1w, q0tq1t trigrams s0cs1cs2w, s0cs1cs2c, s0cs1cq0w, s0cs1cq0t, s0cs1wq0w, s0cs1wq0t, s0ws1cs2c, s0ws1cq0t extended s0llwc, s0lrwc, s0luwc, s0rlwc, s0rrwc, s0ruwc, s0u</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 434–443, Sofia, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>